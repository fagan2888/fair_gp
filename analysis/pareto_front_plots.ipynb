{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "from utils import pareto_compare_plots, pareto_multicompare_plots\n",
    "from glob import glob\n",
    "import pdb\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "from collections import namedtuple\n",
    "perf = namedtuple('perf',['method','dataset','seed'])\n",
    "rdir = 'results_r7'\n",
    "performances = {} \n",
    "methods = set()\n",
    "datasets = set()\n",
    "seeds = set()\n",
    "\n",
    "for f in tqdm(glob(rdir+'/*.json')):\n",
    "    with open(f) as fh:\n",
    "        a = json.load(fh)\n",
    "        method = a[0]['method']\n",
    "        dataset = a[0]['dataset']\n",
    "        seed = a[0]['seed']\n",
    "        performances.update({perf(method,dataset,seed): a})\n",
    "        methods.add(method)\n",
    "        datasets.add(dataset)\n",
    "        seeds.add(seed)\n",
    "        \n",
    "print('methods:',methods)\n",
    "print('datasets:',datasets)\n",
    "print('seeds:',seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "for m1,m2 in combinations(methods,2):\n",
    "    for d in datasets:\n",
    "        for s in seeds:\n",
    "            if perf(m1,d,s) in performances.keys() and perf(m2,d,s) in performances.keys():\n",
    "                h = pareto_compare_plots(performances[perf(m1,d,s)],\n",
    "                                     performances[perf(m2,d,s)],\n",
    "                                     d)\n",
    "                plt.close(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "# metrics                                                                       \n",
    "fair_metrics = {                                                                \n",
    "    'auditor_fp_violation':'Audit FP Violation $\\gamma$',                       \n",
    "#     'auditor_fn_violation':'Audit FN Violation $\\gamma$',                       \n",
    "#     'mean_subgroup_unfairness':'Mean Subgroup Unfairness',                    \n",
    "#     'max_subgroup_unfairness':'Max Subgroup Unfairness',                      \n",
    "#     'max_marginal_unfairness':'Max Marginal Unfairness',                      \n",
    "#     'mean_marginal_unfairness':'Mean Marginal Unfairness',                    \n",
    "    }                                                                           \n",
    "loss_metrics = {                                                                \n",
    "#     'fpr':'False Positive Rate',                                              \n",
    "    'accuracy':'1-Accuracy',                                                    \n",
    "#     'logloss':'Log Loss',                                                     \n",
    "#     'mae':'Mean Absolute Error',                                              \n",
    "#     'precision':'1-Precision',                                                \n",
    "#     'recall':'1-Recall',                                                      \n",
    "#     'ave_precision_score':'1-Average Precision Score',                          \n",
    "#     'auc_prc':'1 - Area Under Precision-Recall Curve'                           \n",
    "    }            \n",
    "for d in datasets:\n",
    "    for s in seeds:\n",
    "        keepgoing = True\n",
    "        for m in methods:\n",
    "            if perf(m,d,s) not in performances.keys(): \n",
    "                keepgoing=False\n",
    "        if keepgoing:\n",
    "            h = pareto_multicompare_plots([performances[perf(m,d,s)] for m in methods], d, rdir=rdir)\n",
    "            plt.close(h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

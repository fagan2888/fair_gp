[
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(0)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8488124300692039,
            "auditor_fn_violation": 0.02166833667334669,
            "auditor_fp_violation": 0.014949980884414425,
            "ave_precision_score": 0.8490695448893717,
            "fpr": 0.11732456140350878,
            "logloss": 0.8247567254054312,
            "mae": 0.26072308562967855,
            "precision": 0.782520325203252,
            "recall": 0.7715430861723447
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8115177003952595,
            "auditor_fn_violation": 0.01617592067646953,
            "auditor_fp_violation": 0.03312823771833536,
            "ave_precision_score": 0.8120296889304632,
            "fpr": 0.14709110867178923,
            "logloss": 0.8235004590032736,
            "mae": 0.27928502617538553,
            "precision": 0.7202505219206681,
            "recall": 0.7582417582417582
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(1)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6129385964912281,
            "auc_prc": 0.6868370273152603,
            "auditor_fn_violation": 0.016181485778574697,
            "auditor_fp_violation": 0.00833386432182151,
            "ave_precision_score": 0.6833486675923557,
            "fpr": 0.16666666666666666,
            "logloss": 0.6501039023147623,
            "mae": 0.4399027643692598,
            "precision": 0.6622222222222223,
            "recall": 0.5971943887775552
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.6599389387631449,
            "auditor_fn_violation": 0.019029927262638583,
            "auditor_fp_violation": 0.015803435592273776,
            "ave_precision_score": 0.6505231535487284,
            "fpr": 0.18990120746432493,
            "logloss": 0.6362384367783304,
            "mae": 0.4344281609017815,
            "precision": 0.6189427312775331,
            "recall": 0.6175824175824176
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(2)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7730263157894737,
            "auc_prc": 0.8681651003310651,
            "auditor_fn_violation": 0.006458091621840169,
            "auditor_fp_violation": 0.005880697506478061,
            "ave_precision_score": 0.8609362515727108,
            "fpr": 0.07456140350877193,
            "logloss": 1.9613889889726361,
            "mae": 0.2339485388849185,
            "precision": 0.8411214953271028,
            "recall": 0.7214428857715431
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8365003563893124,
            "auditor_fn_violation": 0.003944463878602191,
            "auditor_fp_violation": 0.011232114314325883,
            "ave_precision_score": 0.8275025896211037,
            "fpr": 0.1163556531284303,
            "logloss": 1.9072790289682744,
            "mae": 0.24306618571268956,
            "precision": 0.7623318385650224,
            "recall": 0.7472527472527473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(3)",
        "seed": 4866,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7938724896358427,
            "auditor_fn_violation": 0.019668723411735756,
            "auditor_fp_violation": 0.011726880761225095,
            "ave_precision_score": 0.7529431560185583,
            "fpr": 0.11513157894736842,
            "logloss": 2.7421454438744153,
            "mae": 0.3079225903642302,
            "precision": 0.7558139534883721,
            "recall": 0.6513026052104208
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7478409212643243,
            "auditor_fn_violation": 0.01587435615975682,
            "auditor_fp_violation": 0.02205500028886707,
            "ave_precision_score": 0.697744850892186,
            "fpr": 0.12403951701427003,
            "logloss": 3.0986469559200187,
            "mae": 0.32856307198702683,
            "precision": 0.712468193384224,
            "recall": 0.6153846153846154
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(4)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.5644147341147445,
            "auditor_fn_violation": 0.018871075484301936,
            "auditor_fp_violation": 0.0178040440083259,
            "ave_precision_score": 0.6572509842862279,
            "fpr": 0.15350877192982457,
            "logloss": 0.6503139679505276,
            "mae": 0.4344333623323524,
            "precision": 0.7222222222222222,
            "recall": 0.7294589178356713
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.5367316836924346,
            "auditor_fn_violation": 0.01649678532225184,
            "auditor_fp_violation": 0.027890114969091225,
            "ave_precision_score": 0.5999372579122435,
            "fpr": 0.1734357848518112,
            "logloss": 0.6630406704740869,
            "mae": 0.43909859001374796,
            "precision": 0.6645435244161358,
            "recall": 0.6879120879120879
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(5)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5997807017543859,
            "auc_prc": 0.7024993031637501,
            "auditor_fn_violation": 0.018284375769082036,
            "auditor_fp_violation": 0.013625164606431336,
            "ave_precision_score": 0.7018688811824653,
            "fpr": 0.07894736842105263,
            "logloss": 0.7299232961935646,
            "mae": 0.44061555267769126,
            "precision": 0.7410071942446043,
            "recall": 0.41282565130260523
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.6251430568535796,
            "auditor_fn_violation": 0.006366630076838644,
            "auditor_fp_violation": 0.03307527875671616,
            "ave_precision_score": 0.6237029580177342,
            "fpr": 0.10757409440175632,
            "logloss": 0.7312195364910298,
            "mae": 0.4400585279661986,
            "precision": 0.65,
            "recall": 0.4
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(6)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8268500196426415,
            "auditor_fn_violation": 0.0013118341947051968,
            "auditor_fp_violation": 0.004460303300624443,
            "ave_precision_score": 0.7187867344146723,
            "fpr": 0.08881578947368421,
            "logloss": 0.5721185338707507,
            "mae": 0.38910532089179023,
            "precision": 0.8043478260869565,
            "recall": 0.6673346693386774
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.795903858897379,
            "auditor_fn_violation": 0.005114534203447489,
            "auditor_fp_violation": 0.009181158164346103,
            "ave_precision_score": 0.6719756887815875,
            "fpr": 0.10757409440175632,
            "logloss": 0.5883938101923413,
            "mae": 0.39626554155192706,
            "precision": 0.7568238213399504,
            "recall": 0.6703296703296703
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(7)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5570175438596491,
            "auc_prc": 0.7751158067280844,
            "auditor_fn_violation": 0.00187875751503006,
            "auditor_fp_violation": 0.001858459708593513,
            "ave_precision_score": 0.5529270614948226,
            "fpr": 0.43969298245614036,
            "logloss": 0.6870223035093382,
            "mae": 0.49394750832825113,
            "precision": 0.5529542920847269,
            "recall": 0.9939879759519038
        },
        "train": {
            "accuracy": 0.5027442371020856,
            "auc_prc": 0.7483589674914874,
            "auditor_fn_violation": 0.0018359247777469513,
            "auditor_fp_violation": 0.0025251795790244076,
            "ave_precision_score": 0.5011037929794523,
            "fpr": 0.49286498353457736,
            "logloss": 0.6966147901887277,
            "mae": 0.49851366700083183,
            "precision": 0.5011111111111111,
            "recall": 0.9912087912087912
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(8)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8191958668373239,
            "auditor_fn_violation": 0.031796048236824526,
            "auditor_fp_violation": 0.026485705789898485,
            "ave_precision_score": 0.8196697550514747,
            "fpr": 0.14144736842105263,
            "logloss": 0.6224226137875857,
            "mae": 0.3206470384645575,
            "precision": 0.7490272373540856,
            "recall": 0.7715430861723447
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7724758120434759,
            "auditor_fn_violation": 0.03558943800436666,
            "auditor_fp_violation": 0.0274423710208562,
            "ave_precision_score": 0.7728932405252356,
            "fpr": 0.1668496158068057,
            "logloss": 0.6541300001516226,
            "mae": 0.3401282116852912,
            "precision": 0.6947791164658634,
            "recall": 0.7604395604395604
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(9)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5504385964912281,
            "auc_prc": 0.7740776583326344,
            "auditor_fn_violation": 0.0006108708645360899,
            "auditor_fp_violation": 0.0008575464083938761,
            "ave_precision_score": 0.5490614192995802,
            "fpr": 0.44846491228070173,
            "logloss": 15.492814886151379,
            "mae": 0.45009469090585125,
            "precision": 0.5490628445424476,
            "recall": 0.9979959919839679
        },
        "train": {
            "accuracy": 0.5060373216245884,
            "auc_prc": 0.7513812154696133,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0026142469235657904,
            "ave_precision_score": 0.5027624309392266,
            "fpr": 0.49396267837541163,
            "logloss": 17.061848273030098,
            "mae": 0.4945161256767425,
            "precision": 0.5027624309392266,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(10)",
        "seed": 4866,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.7990724539778428,
            "auditor_fn_violation": 0.01957863094610273,
            "auditor_fp_violation": 0.005118729025954718,
            "ave_precision_score": 0.8007882283458738,
            "fpr": 0.0625,
            "logloss": 0.5845060802384036,
            "mae": 0.33638342534597315,
            "precision": 0.8519480519480519,
            "recall": 0.657314629258517
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.7441514509669754,
            "auditor_fn_violation": 0.025654696565783286,
            "auditor_fp_violation": 0.010721782502359082,
            "ave_precision_score": 0.7432396823423343,
            "fpr": 0.09110867178924259,
            "logloss": 0.5886561175020598,
            "mae": 0.33758662390557415,
            "precision": 0.7904040404040404,
            "recall": 0.6879120879120879
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(11)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.817419845433604,
            "auditor_fn_violation": 0.009053194107513273,
            "auditor_fp_violation": 0.01878371776899877,
            "ave_precision_score": 0.8144312441137544,
            "fpr": 0.3059210526315789,
            "logloss": 0.6918506342920191,
            "mae": 0.38464352389105205,
            "precision": 0.6081460674157303,
            "recall": 0.8677354709418837
        },
        "train": {
            "accuracy": 0.5817782656421515,
            "auc_prc": 0.7595414877550254,
            "auditor_fn_violation": 0.008528244532635313,
            "auditor_fp_violation": 0.025121805611724165,
            "ave_precision_score": 0.7557625849681484,
            "fpr": 0.36443468715697036,
            "logloss": 0.7367674897297295,
            "mae": 0.4083724074166894,
            "precision": 0.5501355013550135,
            "recall": 0.8923076923076924
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(12)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8126161960100635,
            "auditor_fn_violation": 0.014775164363815355,
            "auditor_fp_violation": 0.0073940146977613565,
            "ave_precision_score": 0.8040783285899515,
            "fpr": 0.10526315789473684,
            "logloss": 0.6050711293388267,
            "mae": 0.3551675924304219,
            "precision": 0.7762237762237763,
            "recall": 0.6673346693386774
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7665447729514264,
            "auditor_fn_violation": 0.012817698218356845,
            "auditor_fp_violation": 0.011658193232807597,
            "ave_precision_score": 0.7574054539131377,
            "fpr": 0.11086717892425905,
            "logloss": 0.6024304480745286,
            "mae": 0.35394722355772973,
            "precision": 0.749379652605459,
            "recall": 0.6637362637362637
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(13)",
        "seed": 4866,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7834124496587253,
            "auditor_fn_violation": 0.013986305945223789,
            "auditor_fp_violation": 0.004229323308270682,
            "ave_precision_score": 0.7822452711059336,
            "fpr": 0.12938596491228072,
            "logloss": 0.5929570612546001,
            "mae": 0.4053107948581639,
            "precision": 0.7412280701754386,
            "recall": 0.6773547094188377
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7387467483550815,
            "auditor_fn_violation": 0.003338922329043073,
            "auditor_fp_violation": 0.005820671327055296,
            "ave_precision_score": 0.7398024907936794,
            "fpr": 0.150384193194292,
            "logloss": 0.592595920594762,
            "mae": 0.407191894887144,
            "precision": 0.6914414414414415,
            "recall": 0.6747252747252748
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(14)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8050847888657143,
            "auditor_fn_violation": 0.0019073234187673663,
            "auditor_fp_violation": 0.007712607790663101,
            "ave_precision_score": 0.7863160606250945,
            "fpr": 0.08771929824561403,
            "logloss": 0.5437620115136873,
            "mae": 0.3702920643089895,
            "precision": 0.8148148148148148,
            "recall": 0.7054108216432866
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.7549744662852493,
            "auditor_fn_violation": 0.013857492671982248,
            "auditor_fp_violation": 0.009677046623144031,
            "ave_precision_score": 0.7375775165061191,
            "fpr": 0.11855104281009879,
            "logloss": 0.5635135320294392,
            "mae": 0.3777388800528518,
            "precision": 0.76,
            "recall": 0.7516483516483516
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(15)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5822368421052632,
            "auc_prc": 0.6771842755578047,
            "auditor_fn_violation": 0.05303589635411173,
            "auditor_fp_violation": 0.08425990824518925,
            "ave_precision_score": 0.6497058891638635,
            "fpr": 0.26644736842105265,
            "logloss": 0.6799917318089178,
            "mae": 0.47214193228762924,
            "precision": 0.597682119205298,
            "recall": 0.7234468937875751
        },
        "train": {
            "accuracy": 0.5510428100987925,
            "auc_prc": 0.6374508855479375,
            "auditor_fn_violation": 0.046009095185824064,
            "auditor_fp_violation": 0.07746692472124328,
            "ave_precision_score": 0.6088044907986601,
            "fpr": 0.3194291986827662,
            "logloss": 0.6760109073636505,
            "mae": 0.4720533159934598,
            "precision": 0.5366242038216561,
            "recall": 0.7406593406593407
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(16)",
        "seed": 4866,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8464343933412876,
            "auditor_fn_violation": 0.004025595049748619,
            "auditor_fp_violation": 0.0018584597085935198,
            "ave_precision_score": 0.8421103186200384,
            "fpr": 0.0537280701754386,
            "logloss": 0.5245704467405234,
            "mae": 0.3289884972435079,
            "precision": 0.8693333333333333,
            "recall": 0.6533066132264529
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8216667526842005,
            "auditor_fn_violation": 0.006460718206053004,
            "auditor_fp_violation": 0.004910740077416376,
            "ave_precision_score": 0.8166445602507519,
            "fpr": 0.07903402854006586,
            "logloss": 0.5226204051083875,
            "mae": 0.33100636928259475,
            "precision": 0.8125,
            "recall": 0.6857142857142857
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(17)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.6647646600058803,
            "auditor_fn_violation": 0.008048992722286678,
            "auditor_fp_violation": 0.025293636633957787,
            "ave_precision_score": 0.6666603421200926,
            "fpr": 0.12609649122807018,
            "logloss": 0.7213416156580946,
            "mae": 0.3969271485693753,
            "precision": 0.7589098532494759,
            "recall": 0.7254509018036072
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.6188789672077855,
            "auditor_fn_violation": 0.0031362709738121442,
            "auditor_fp_violation": 0.02671057446030004,
            "ave_precision_score": 0.6202595207878061,
            "fpr": 0.1668496158068057,
            "logloss": 0.7228991634282755,
            "mae": 0.4023568474418769,
            "precision": 0.6941649899396378,
            "recall": 0.7582417582417582
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(18)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5833333333333334,
            "auc_prc": 0.7685049428166557,
            "auditor_fn_violation": 0.009413563970045355,
            "auditor_fp_violation": 0.016187184061849547,
            "ave_precision_score": 0.766662976415846,
            "fpr": 0.3815789473684211,
            "logloss": 0.6927507712956248,
            "mae": 0.43491772965430037,
            "precision": 0.5730061349693252,
            "recall": 0.935871743486974
        },
        "train": {
            "accuracy": 0.5411635565312843,
            "auc_prc": 0.7119824447254977,
            "auditor_fn_violation": 0.013485965187392193,
            "auditor_fp_violation": 0.016754289675891165,
            "ave_precision_score": 0.7159753294898079,
            "fpr": 0.43029637760702527,
            "logloss": 0.7204127166388059,
            "mae": 0.45051187271115284,
            "precision": 0.5225334957369062,
            "recall": 0.9428571428571428
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(19)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5646929824561403,
            "auc_prc": 0.8542868154091858,
            "auditor_fn_violation": 0.0013799528882326058,
            "auditor_fp_violation": 0.0092312348668281,
            "ave_precision_score": 0.8545162677479715,
            "fpr": 0.4331140350877193,
            "logloss": 1.9266981091704485,
            "mae": 0.42669335826643684,
            "precision": 0.5571748878923767,
            "recall": 0.9959919839679359
        },
        "train": {
            "accuracy": 0.5181119648737651,
            "auc_prc": 0.8007744893508172,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.01113823251872822,
            "ave_precision_score": 0.8017260899656456,
            "fpr": 0.4818880351262349,
            "logloss": 2.2169996384801514,
            "mae": 0.47157802005829413,
            "precision": 0.5089485458612976,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(20)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8356229533158852,
            "auditor_fn_violation": 0.010382607319902969,
            "auditor_fp_violation": 0.015916379932883053,
            "ave_precision_score": 0.8275669297994229,
            "fpr": 0.07456140350877193,
            "logloss": 0.557231782428472,
            "mae": 0.3265892358115372,
            "precision": 0.8325123152709359,
            "recall": 0.6773547094188377
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8166776856538938,
            "auditor_fn_violation": 0.00979481550282868,
            "auditor_fp_violation": 0.010471431047431975,
            "ave_precision_score": 0.8162480450463966,
            "fpr": 0.09879253567508232,
            "logloss": 0.5098300158009244,
            "mae": 0.3199867082856346,
            "precision": 0.7820823244552058,
            "recall": 0.7098901098901099
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(21)",
        "seed": 4866,
        "test": {
            "accuracy": 0.45285087719298245,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.897861207303105,
            "mae": 0.5471491228070176,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5005488474204172,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.25043167912663,
            "mae": 0.4994511525795829,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(22)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7076243115983512,
            "auditor_fn_violation": 0.018488731849664243,
            "auditor_fp_violation": 0.04239146595301815,
            "ave_precision_score": 0.7090089018973476,
            "fpr": 0.25,
            "logloss": 0.6050565533350764,
            "mae": 0.4144888594333009,
            "precision": 0.6602086438152012,
            "recall": 0.8877755511022044
        },
        "train": {
            "accuracy": 0.6597145993413831,
            "auc_prc": 0.6711425212132486,
            "auditor_fn_violation": 0.01923981616627061,
            "auditor_fp_violation": 0.059547537889729815,
            "ave_precision_score": 0.6731445651310264,
            "fpr": 0.29527991218441274,
            "logloss": 0.6261093089008908,
            "mae": 0.42379996467802905,
            "precision": 0.6061493411420205,
            "recall": 0.9098901098901099
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(23)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.8312538084955621,
            "auditor_fn_violation": 0.01015627746721513,
            "auditor_fp_violation": 0.004088611358905741,
            "ave_precision_score": 0.8315589830942828,
            "fpr": 0.03289473684210526,
            "logloss": 0.6323249033430668,
            "mae": 0.3911150022544093,
            "precision": 0.868421052631579,
            "recall": 0.3967935871743487
        },
        "train": {
            "accuracy": 0.6498353457738749,
            "auc_prc": 0.7910300745174074,
            "auditor_fn_violation": 0.0021785020687326057,
            "auditor_fp_violation": 0.009147457006952067,
            "ave_precision_score": 0.7914004439299718,
            "fpr": 0.04171240395170143,
            "logloss": 0.6072059971242603,
            "mae": 0.38305346456137374,
            "precision": 0.8207547169811321,
            "recall": 0.3824175824175824
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(24)",
        "seed": 4866,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8348771100315527,
            "auditor_fn_violation": 0.03102037408149633,
            "auditor_fp_violation": 0.020684656556645854,
            "ave_precision_score": 0.854237483508117,
            "fpr": 0.09539473684210527,
            "logloss": 0.5382966935461667,
            "mae": 0.3286733857921239,
            "precision": 0.8044943820224719,
            "recall": 0.717434869739479
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8105917744603917,
            "auditor_fn_violation": 0.02771257282783079,
            "auditor_fp_violation": 0.020032930845225033,
            "ave_precision_score": 0.803583105405429,
            "fpr": 0.12184412733260154,
            "logloss": 0.542263882806596,
            "mae": 0.33868300764536247,
            "precision": 0.74364896073903,
            "recall": 0.7076923076923077
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(25)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.6172409760700135,
            "auditor_fn_violation": 0.011668072988081443,
            "auditor_fp_violation": 0.007014357928720109,
            "ave_precision_score": 0.6192187974905915,
            "fpr": 0.07785087719298246,
            "logloss": 1.367322401150354,
            "mae": 0.49141778058456986,
            "precision": 0.6377551020408163,
            "recall": 0.250501002004008
        },
        "train": {
            "accuracy": 0.5378704720087816,
            "auc_prc": 0.5415545874755596,
            "auditor_fn_violation": 0.01325677615469054,
            "auditor_fp_violation": 0.015603635873437712,
            "ave_precision_score": 0.5433799755916556,
            "fpr": 0.10757409440175632,
            "logloss": 1.2820851206499895,
            "mae": 0.47448964847686687,
            "precision": 0.5739130434782609,
            "recall": 0.29010989010989013
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(26)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7872807017543859,
            "auc_prc": 0.8031179359923983,
            "auditor_fn_violation": 0.009224589529937068,
            "auditor_fp_violation": 0.010479057814026591,
            "ave_precision_score": 0.8041206204036597,
            "fpr": 0.11732456140350878,
            "logloss": 0.5866731840778349,
            "mae": 0.30715314735058813,
            "precision": 0.7938342967244701,
            "recall": 0.8256513026052105
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.7638500843941451,
            "auditor_fn_violation": 0.005814163882220965,
            "auditor_fp_violation": 0.010011650971556225,
            "ave_precision_score": 0.7656869811793248,
            "fpr": 0.1394072447859495,
            "logloss": 0.6555757801972228,
            "mae": 0.3164731007917233,
            "precision": 0.744466800804829,
            "recall": 0.8131868131868132
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(27)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7841201032809666,
            "auditor_fn_violation": 0.00666464507963295,
            "auditor_fp_violation": 0.039083407671721686,
            "ave_precision_score": 0.7732362962259597,
            "fpr": 0.24561403508771928,
            "logloss": 0.6020958675120278,
            "mae": 0.38942955128298135,
            "precision": 0.6574923547400612,
            "recall": 0.8617234468937875
        },
        "train": {
            "accuracy": 0.6663007683863886,
            "auc_prc": 0.7448790634079668,
            "auditor_fn_violation": 0.0064172929156463745,
            "auditor_fp_violation": 0.021491709515281083,
            "ave_precision_score": 0.73570806276857,
            "fpr": 0.25686059275521406,
            "logloss": 0.6228351895809722,
            "mae": 0.39551822745453863,
            "precision": 0.6219709208400647,
            "recall": 0.8461538461538461
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(28)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7681151749685506,
            "auditor_fn_violation": 0.018624969236719056,
            "auditor_fp_violation": 0.022784716027356527,
            "ave_precision_score": 0.7603432037532986,
            "fpr": 0.2149122807017544,
            "logloss": 0.5989877285266416,
            "mae": 0.40630820808572726,
            "precision": 0.679214402618658,
            "recall": 0.8316633266533067
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.7641787586062099,
            "auditor_fn_violation": 0.011085511634359058,
            "auditor_fp_violation": 0.03195110443507231,
            "ave_precision_score": 0.7438447799642239,
            "fpr": 0.24259055982436883,
            "logloss": 0.6010980588292643,
            "mae": 0.4045040723014087,
            "precision": 0.632890365448505,
            "recall": 0.8373626373626374
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(29)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5877192982456141,
            "auc_prc": 0.7076520325169926,
            "auditor_fn_violation": 0.09396204690081919,
            "auditor_fp_violation": 0.05400683913172762,
            "ave_precision_score": 0.7085672472525475,
            "fpr": 0.24561403508771928,
            "logloss": 1.0831678320712643,
            "mae": 0.4429988140805611,
            "precision": 0.6077057793345009,
            "recall": 0.6953907815631263
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.7009860122366067,
            "auditor_fn_violation": 0.0705878095559764,
            "auditor_fp_violation": 0.06653571359793557,
            "ave_precision_score": 0.7017311872852893,
            "fpr": 0.287596048298573,
            "logloss": 0.9339975012782898,
            "mae": 0.4277114091729538,
            "precision": 0.569078947368421,
            "recall": 0.7604395604395604
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(30)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8342545165927534,
            "auditor_fn_violation": 0.008174243223288689,
            "auditor_fp_violation": 0.004869164436515016,
            "ave_precision_score": 0.8345458144449849,
            "fpr": 0.04057017543859649,
            "logloss": 0.5470025394076741,
            "mae": 0.3677484791750382,
            "precision": 0.8875379939209727,
            "recall": 0.5851703406813628
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.8069810056106096,
            "auditor_fn_violation": 0.0028564191023027494,
            "auditor_fp_violation": 0.005623278833747376,
            "ave_precision_score": 0.8073076973254488,
            "fpr": 0.06147091108671789,
            "logloss": 0.551411822021904,
            "mae": 0.3752614873758917,
            "precision": 0.8175895765472313,
            "recall": 0.5516483516483517
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(31)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8074889013867919,
            "auditor_fn_violation": 0.014614755827444367,
            "auditor_fp_violation": 0.008984325219829234,
            "ave_precision_score": 0.8080789894689568,
            "fpr": 0.08114035087719298,
            "logloss": 0.6423128391782174,
            "mae": 0.34830356759505715,
            "precision": 0.8097686375321337,
            "recall": 0.6312625250501002
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7460287384384652,
            "auditor_fn_violation": 0.007683863885839743,
            "auditor_fp_violation": 0.013793402276272458,
            "ave_precision_score": 0.7466403189912046,
            "fpr": 0.09440175631174534,
            "logloss": 0.6803763683351509,
            "mae": 0.3605072770084203,
            "precision": 0.7604456824512534,
            "recall": 0.6
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(32)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8104373457244729,
            "auditor_fn_violation": 0.001935889322504659,
            "auditor_fp_violation": 0.013601270124463704,
            "ave_precision_score": 0.7661211140454407,
            "fpr": 0.08333333333333333,
            "logloss": 0.5655275929967821,
            "mae": 0.376874163420054,
            "precision": 0.8104738154613467,
            "recall": 0.6513026052104208
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.7969193329222027,
            "auditor_fn_violation": 0.007148285304157975,
            "auditor_fp_violation": 0.013851175688947951,
            "ave_precision_score": 0.7459978067451702,
            "fpr": 0.09330406147091108,
            "logloss": 0.555118107182827,
            "mae": 0.3716451032853938,
            "precision": 0.7809278350515464,
            "recall": 0.6659340659340659
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(33)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7719298245614035,
            "auc_prc": 0.8603747169904247,
            "auditor_fn_violation": 0.013557817389164301,
            "auditor_fp_violation": 0.016824370247653035,
            "ave_precision_score": 0.8589054268497373,
            "fpr": 0.08114035087719298,
            "logloss": 0.5087661560942247,
            "mae": 0.2977863389594284,
            "precision": 0.8314350797266514,
            "recall": 0.7314629258517034
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8448106370873136,
            "auditor_fn_violation": 0.009879253567508246,
            "auditor_fp_violation": 0.00942188071716063,
            "ave_precision_score": 0.8417060710172835,
            "fpr": 0.09440175631174534,
            "logloss": 0.5016612151670408,
            "mae": 0.29735972633243885,
            "precision": 0.7971698113207547,
            "recall": 0.7428571428571429
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(34)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8538648873591598,
            "auditor_fn_violation": 0.030361160918327898,
            "auditor_fp_violation": 0.009772843124761057,
            "ave_precision_score": 0.8491630296753044,
            "fpr": 0.0625,
            "logloss": 0.5367538229812628,
            "mae": 0.3526902203448117,
            "precision": 0.8475935828877005,
            "recall": 0.6352705410821643
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7767195814971428,
            "auditor_fn_violation": 0.028303639280587694,
            "auditor_fp_violation": 0.009828701831417184,
            "ave_precision_score": 0.7698251933053376,
            "fpr": 0.10208562019758508,
            "logloss": 0.5659387043987976,
            "mae": 0.3634842511196953,
            "precision": 0.7552631578947369,
            "recall": 0.6307692307692307
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(35)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7752192982456141,
            "auc_prc": 0.8625928070950472,
            "auditor_fn_violation": 0.00791495271244243,
            "auditor_fp_violation": 0.013996856548150041,
            "ave_precision_score": 0.8629061779435665,
            "fpr": 0.10416666666666667,
            "logloss": 0.48835076894932933,
            "mae": 0.3130632886793791,
            "precision": 0.8037190082644629,
            "recall": 0.779559118236473
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8328782107056169,
            "auditor_fn_violation": 0.0021664394880640766,
            "auditor_fp_violation": 0.010976948408342489,
            "ave_precision_score": 0.8333338231960106,
            "fpr": 0.14050493962678376,
            "logloss": 0.500792535138206,
            "mae": 0.3208498217811569,
            "precision": 0.7445109780439122,
            "recall": 0.8197802197802198
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(36)",
        "seed": 4866,
        "test": {
            "accuracy": 0.4583333333333333,
            "auc_prc": 0.5392383732951016,
            "auditor_fn_violation": 0.009367419048623564,
            "auditor_fp_violation": 0.01256318763009218,
            "ave_precision_score": 0.5391322487752278,
            "fpr": 0.12828947368421054,
            "logloss": 1.2346152571738165,
            "mae": 0.5376408001644897,
            "precision": 0.5104602510460251,
            "recall": 0.24448897795591182
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0.46572188955910626,
            "auditor_fn_violation": 0.004914295364350261,
            "auditor_fp_violation": 0.01316030196237025,
            "ave_precision_score": 0.4882404947770411,
            "fpr": 0.150384193194292,
            "logloss": 1.1148043809811252,
            "mae": 0.5104867072671685,
            "precision": 0.4849624060150376,
            "recall": 0.2835164835164835
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(37)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8590323386899501,
            "auditor_fn_violation": 0.007385384804697117,
            "auditor_fp_violation": 0.01042064908032794,
            "ave_precision_score": 0.8589599247568858,
            "fpr": 0.08991228070175439,
            "logloss": 0.5028338620497381,
            "mae": 0.3252624064201914,
            "precision": 0.8165548098434005,
            "recall": 0.7314629258517034
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8335102048856595,
            "auditor_fn_violation": 0.0035608738133436303,
            "auditor_fp_violation": 0.015695110443507236,
            "ave_precision_score": 0.8332738101622303,
            "fpr": 0.12294182217343579,
            "logloss": 0.5016046827745806,
            "mae": 0.33059728531111704,
            "precision": 0.7533039647577092,
            "recall": 0.7516483516483516
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(38)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8132674427680348,
            "auditor_fn_violation": 0.0106265161902753,
            "auditor_fp_violation": 0.012433095450490641,
            "ave_precision_score": 0.7811312283933574,
            "fpr": 0.11732456140350878,
            "logloss": 3.915901165011531,
            "mae": 0.2754166233595752,
            "precision": 0.7770833333333333,
            "recall": 0.7474949899799599
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7663515584086473,
            "auditor_fn_violation": 0.015097525964704891,
            "auditor_fp_violation": 0.02438038014905541,
            "ave_precision_score": 0.724099524070084,
            "fpr": 0.13172338090010977,
            "logloss": 4.037160935993928,
            "mae": 0.2937340266665165,
            "precision": 0.732739420935412,
            "recall": 0.7230769230769231
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(39)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.751457778629488,
            "auditor_fn_violation": 0.00916745772246247,
            "auditor_fp_violation": 0.008304659954972178,
            "ave_precision_score": 0.6945966415374132,
            "fpr": 0.1875,
            "logloss": 3.9074157091548654,
            "mae": 0.3002926603147953,
            "precision": 0.7126050420168067,
            "recall": 0.8496993987975952
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7218094696564372,
            "auditor_fn_violation": 0.0016766987129226462,
            "auditor_fp_violation": 0.004564099601363464,
            "ave_precision_score": 0.6678805891787731,
            "fpr": 0.21075740944017562,
            "logloss": 3.515045607861924,
            "mae": 0.3213257625513727,
            "precision": 0.6577540106951871,
            "recall": 0.810989010989011
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(40)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.838525471193674,
            "auditor_fn_violation": 0.016869264845480444,
            "auditor_fp_violation": 0.004569155940699206,
            "ave_precision_score": 0.8387442639927147,
            "fpr": 0.027412280701754384,
            "logloss": 1.6400252096830394,
            "mae": 0.37597199033463025,
            "precision": 0.8803827751196173,
            "recall": 0.3687374749498998
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.8012912107582011,
            "auditor_fn_violation": 0.016270008805683894,
            "auditor_fp_violation": 0.006920773393417683,
            "ave_precision_score": 0.8016901136536195,
            "fpr": 0.03402854006586169,
            "logloss": 1.5201698404842483,
            "mae": 0.35232583090857256,
            "precision": 0.8393782383419689,
            "recall": 0.35604395604395606
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(41)",
        "seed": 4866,
        "test": {
            "accuracy": 0.4473684210526316,
            "auc_prc": 0.3454568912260611,
            "auditor_fn_violation": 0.0006108708645360903,
            "auditor_fp_violation": 0.002312454865978506,
            "ave_precision_score": 0.5463389184383804,
            "fpr": 0.006578947368421052,
            "logloss": 0.7246645172343401,
            "mae": 0.5084571258017891,
            "precision": 0.14285714285714285,
            "recall": 0.002004008016032064
        },
        "train": {
            "accuracy": 0.49396267837541163,
            "auc_prc": 0.24972557628979145,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0026142469235657757,
            "ave_precision_score": 0.4994511525795829,
            "fpr": 0.006586169045005488,
            "logloss": 0.7085592764218309,
            "mae": 0.5004742591755319,
            "precision": 0.0,
            "recall": 0.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(42)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.865974684502804,
            "auditor_fn_violation": 0.014267570228175649,
            "auditor_fp_violation": 0.013914553332483754,
            "ave_precision_score": 0.8661788065586954,
            "fpr": 0.13486842105263158,
            "logloss": 0.632936321418592,
            "mae": 0.2675813764152528,
            "precision": 0.7607003891050583,
            "recall": 0.7835671342685371
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8222575472941946,
            "auditor_fn_violation": 0.011712765829121485,
            "auditor_fp_violation": 0.023629325784274076,
            "ave_precision_score": 0.8228438954717922,
            "fpr": 0.14489571899012074,
            "logloss": 0.678706243282141,
            "mae": 0.2878616580515476,
            "precision": 0.7232704402515723,
            "recall": 0.7582417582417582
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(43)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6118421052631579,
            "auc_prc": 0.7504961287150927,
            "auditor_fn_violation": 0.018708469570720407,
            "auditor_fp_violation": 0.005259440975319658,
            "ave_precision_score": 0.7510343628384595,
            "fpr": 0.05701754385964912,
            "logloss": 2.95284679594962,
            "mae": 0.4099109741181894,
            "precision": 0.7911646586345381,
            "recall": 0.39478957915831664
        },
        "train": {
            "accuracy": 0.6256860592755215,
            "auc_prc": 0.6991618059386383,
            "auditor_fn_violation": 0.021266329718580012,
            "auditor_fp_violation": 0.012941244439309035,
            "ave_precision_score": 0.6995724125725706,
            "fpr": 0.07025246981339188,
            "logloss": 3.048363409946745,
            "mae": 0.39668328375417455,
            "precision": 0.7355371900826446,
            "recall": 0.3912087912087912
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(44)",
        "seed": 4866,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0.3704214860992995,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0006796652648570579,
            "ave_precision_score": 0.5701409066874978,
            "fpr": 0.0010964912280701754,
            "logloss": 0.7152167670728787,
            "mae": 0.5027972363066255,
            "precision": 0.0,
            "recall": 0.0
        },
        "train": {
            "accuracy": 0.5005488474204172,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.7004377762611582,
            "mae": 0.4957624705902962,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(45)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8655252003379988,
            "auditor_fn_violation": 0.0034345005801075827,
            "auditor_fp_violation": 0.007030287583365198,
            "ave_precision_score": 0.814317655036472,
            "fpr": 0.0800438596491228,
            "logloss": 0.513226472668206,
            "mae": 0.3429195643320941,
            "precision": 0.8310185185185185,
            "recall": 0.7194388777555111
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8285648710461944,
            "auditor_fn_violation": 0.01117477473130602,
            "auditor_fp_violation": 0.00881044543301173,
            "ave_precision_score": 0.7662076639929631,
            "fpr": 0.10757409440175632,
            "logloss": 0.5268548874605431,
            "mae": 0.3481871381352945,
            "precision": 0.772093023255814,
            "recall": 0.7296703296703296
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(46)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.8785631006704577,
            "auditor_fn_violation": 0.01697913370600851,
            "auditor_fp_violation": 0.026254725797544703,
            "ave_precision_score": 0.8787233009938185,
            "fpr": 0.31798245614035087,
            "logloss": 0.8162139352473758,
            "mae": 0.3455665137847104,
            "precision": 0.6164021164021164,
            "recall": 0.9338677354709419
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.8361064503723116,
            "auditor_fn_violation": 0.016501610354519247,
            "auditor_fp_violation": 0.027796233173493584,
            "ave_precision_score": 0.8365978104940022,
            "fpr": 0.3589462129527991,
            "logloss": 0.8785444314486182,
            "mae": 0.37592352719234934,
            "precision": 0.5651595744680851,
            "recall": 0.9340659340659341
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(47)",
        "seed": 4866,
        "test": {
            "accuracy": 0.4868421052631579,
            "auc_prc": 0.7856115046322513,
            "auditor_fn_violation": 0.004671623949653692,
            "auditor_fp_violation": 0.0013540206448324201,
            "ave_precision_score": 0.7860071238849298,
            "fpr": 0.003289473684210526,
            "logloss": 1.0372650294383157,
            "mae": 0.47838525335668747,
            "precision": 0.918918918918919,
            "recall": 0.06813627254509018
        },
        "train": {
            "accuracy": 0.5301866081229418,
            "auc_prc": 0.7542997347187371,
            "auditor_fn_violation": 0.005534312010711585,
            "auditor_fp_violation": 0.0012036127640726404,
            "ave_precision_score": 0.7547083404854993,
            "fpr": 0.0021953896816684962,
            "logloss": 1.007239393109854,
            "mae": 0.4496661784287346,
            "precision": 0.9354838709677419,
            "recall": 0.06373626373626373
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(48)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.7022981266432847,
            "auditor_fn_violation": 0.05145158738529691,
            "auditor_fp_violation": 0.04017458901491016,
            "ave_precision_score": 0.7029206088095077,
            "fpr": 0.14912280701754385,
            "logloss": 1.0073166353704708,
            "mae": 0.42139454114000224,
            "precision": 0.6957494407158836,
            "recall": 0.6232464929859719
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.6427573509248222,
            "auditor_fn_violation": 0.04281974885707049,
            "auditor_fp_violation": 0.050436189265699934,
            "ave_precision_score": 0.643471535076433,
            "fpr": 0.1964873765093304,
            "logloss": 1.0217754451019465,
            "mae": 0.4314961567146887,
            "precision": 0.6117136659436009,
            "recall": 0.6197802197802198
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(49)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7295194255167756,
            "auditor_fn_violation": 0.053115001933691945,
            "auditor_fp_violation": 0.041717110573042775,
            "ave_precision_score": 0.729552919889285,
            "fpr": 0.12609649122807018,
            "logloss": 2.0453737864047903,
            "mae": 0.3685674966734381,
            "precision": 0.7415730337078652,
            "recall": 0.6613226452905812
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.692576855170843,
            "auditor_fn_violation": 0.06157223676433338,
            "auditor_fp_violation": 0.049868084041057636,
            "ave_precision_score": 0.6927546335585161,
            "fpr": 0.1602634467618002,
            "logloss": 1.8148214074857687,
            "mae": 0.3811648275611741,
            "precision": 0.683982683982684,
            "recall": 0.6945054945054945
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(50)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.7681677189590098,
            "auditor_fn_violation": 0.03889797138135921,
            "auditor_fp_violation": 0.014044645512085301,
            "ave_precision_score": 0.7697540412907716,
            "fpr": 0.06578947368421052,
            "logloss": 0.6926061906515352,
            "mae": 0.40307040871304806,
            "precision": 0.7849462365591398,
            "recall": 0.43887775551102204
        },
        "train": {
            "accuracy": 0.6641053787047201,
            "auc_prc": 0.7285056971899397,
            "auditor_fn_violation": 0.017102326871810958,
            "auditor_fp_violation": 0.012784774779979588,
            "ave_precision_score": 0.7292921587460918,
            "fpr": 0.07354555433589462,
            "logloss": 0.6512480553845741,
            "mae": 0.3894702537191273,
            "precision": 0.7632508833922261,
            "recall": 0.4747252747252747
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(51)",
        "seed": 4866,
        "test": {
            "accuracy": 0.45285087719298245,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.897861207303105,
            "mae": 0.5471491228070176,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5005488474204172,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.25043167912663,
            "mae": 0.4994511525795829,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(52)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.71817663551289,
            "auditor_fn_violation": 0.007732570403965826,
            "auditor_fp_violation": 0.009379911643515568,
            "ave_precision_score": 0.7195746756671237,
            "fpr": 0.13596491228070176,
            "logloss": 0.5631468574401602,
            "mae": 0.36630849929101633,
            "precision": 0.7633587786259542,
            "recall": 0.8016032064128257
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.7296504831949191,
            "auditor_fn_violation": 0.0029625698121856234,
            "auditor_fp_violation": 0.014154486105494255,
            "ave_precision_score": 0.6917197518081859,
            "fpr": 0.15806805708013172,
            "logloss": 0.5426585874585568,
            "mae": 0.358330188415184,
            "precision": 0.7267552182163188,
            "recall": 0.8417582417582418
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(53)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7708333333333334,
            "auc_prc": 0.8540560548568314,
            "auditor_fn_violation": 0.004875980030235915,
            "auditor_fp_violation": 0.004420479164011724,
            "ave_precision_score": 0.7799202385202519,
            "fpr": 0.0712719298245614,
            "logloss": 0.5269871786025596,
            "mae": 0.35887036214402895,
            "precision": 0.8452380952380952,
            "recall": 0.7114228456913828
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.45377510313567293,
            "auditor_fn_violation": 0.0031242083931436347,
            "auditor_fp_violation": 0.01067604521732432,
            "ave_precision_score": 0.7216949117928522,
            "fpr": 0.11306256860592755,
            "logloss": 0.5516692908382892,
            "mae": 0.36667965185694584,
            "precision": 0.765375854214123,
            "recall": 0.7384615384615385
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(54)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.873219590212637,
            "auditor_fn_violation": 0.01991043490489752,
            "auditor_fp_violation": 0.015292468459283805,
            "ave_precision_score": 0.871744354347669,
            "fpr": 0.0800438596491228,
            "logloss": 0.5048756714900617,
            "mae": 0.3143264885048235,
            "precision": 0.8282352941176471,
            "recall": 0.7054108216432866
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8339251152306988,
            "auditor_fn_violation": 0.016151795515132513,
            "auditor_fp_violation": 0.011205634833516283,
            "ave_precision_score": 0.8302842527545913,
            "fpr": 0.09769484083424808,
            "logloss": 0.5121122221113577,
            "mae": 0.3138695565797593,
            "precision": 0.7813267813267813,
            "recall": 0.6989010989010989
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(55)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5657894736842105,
            "auc_prc": 0.7267405952207692,
            "auditor_fn_violation": 0.019152339767253817,
            "auditor_fp_violation": 0.014979185251263755,
            "ave_precision_score": 0.7304837929833689,
            "fpr": 0.043859649122807015,
            "logloss": 0.6820874777601745,
            "mae": 0.49080153074311583,
            "precision": 0.7814207650273224,
            "recall": 0.2865731462925852
        },
        "train": {
            "accuracy": 0.5762897914379802,
            "auc_prc": 0.6716424384166916,
            "auditor_fn_violation": 0.013645191252216516,
            "auditor_fp_violation": 0.0160441581450883,
            "ave_precision_score": 0.6710355131737991,
            "fpr": 0.05817782656421515,
            "logloss": 0.6798833639824977,
            "mae": 0.4905355298558391,
            "precision": 0.6971428571428572,
            "recall": 0.2681318681318681
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(56)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8517946633443845,
            "auditor_fn_violation": 0.014608163695812684,
            "auditor_fp_violation": 0.010165774606006544,
            "ave_precision_score": 0.8521843266337109,
            "fpr": 0.13267543859649122,
            "logloss": 0.549556462336609,
            "mae": 0.31690780495671533,
            "precision": 0.7599206349206349,
            "recall": 0.7675350701402806
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7912112579931132,
            "auditor_fn_violation": 0.008202554854585594,
            "auditor_fp_violation": 0.0205095614997978,
            "ave_precision_score": 0.7916883793347183,
            "fpr": 0.15806805708013172,
            "logloss": 0.5638717980738825,
            "mae": 0.33714005212646087,
            "precision": 0.6993736951983298,
            "recall": 0.7362637362637363
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(57)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5307017543859649,
            "auc_prc": 0.7205832843137225,
            "auditor_fn_violation": 0.008380796681081475,
            "auditor_fp_violation": 0.0028938872605241937,
            "ave_precision_score": 0.6197691451381134,
            "fpr": 0.025219298245614034,
            "logloss": 0.6774997113185627,
            "mae": 0.46164940062322113,
            "precision": 0.8034188034188035,
            "recall": 0.18837675350701402
        },
        "train": {
            "accuracy": 0.5609220636663008,
            "auc_prc": 0.6738965457731376,
            "auditor_fn_violation": 0.011145824537701604,
            "auditor_fp_violation": 0.005271823906638167,
            "ave_precision_score": 0.5682802723274548,
            "fpr": 0.029637760702524697,
            "logloss": 0.6812010313436263,
            "mae": 0.4625870562296537,
            "precision": 0.7522935779816514,
            "recall": 0.18021978021978022
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(58)",
        "seed": 4866,
        "test": {
            "accuracy": 0.2598684210526316,
            "auc_prc": 0.4192309878136929,
            "auditor_fn_violation": 0.005119888900608245,
            "auditor_fp_violation": 0.007240028036192192,
            "ave_precision_score": 0.38039444612703266,
            "fpr": 0.2708333333333333,
            "logloss": 10.55590708461307,
            "mae": 0.7371587460998342,
            "precision": 0.22327044025157233,
            "recall": 0.14228456913827656
        },
        "train": {
            "accuracy": 0.27661909989023054,
            "auc_prc": 0.37943511041827555,
            "auditor_fn_violation": 0.0033027345870375573,
            "auditor_fp_violation": 0.007900514183372809,
            "ave_precision_score": 0.3419617772544521,
            "fpr": 0.283205268935236,
            "logloss": 10.64321822850898,
            "mae": 0.7233760500564823,
            "precision": 0.17307692307692307,
            "recall": 0.11868131868131868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(59)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8178793341331491,
            "auditor_fn_violation": 0.003994831768800763,
            "auditor_fp_violation": 0.012034854084363455,
            "ave_precision_score": 0.8131137815292111,
            "fpr": 0.06798245614035088,
            "logloss": 0.6170298152456848,
            "mae": 0.35311918409046994,
            "precision": 0.8333333333333334,
            "recall": 0.6212424849699398
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.7958308945019215,
            "auditor_fn_violation": 0.007232723368837537,
            "auditor_fp_violation": 0.012652377375931601,
            "ave_precision_score": 0.792450510303235,
            "fpr": 0.07903402854006586,
            "logloss": 0.5589936823546732,
            "mae": 0.33465511830263944,
            "precision": 0.8120104438642297,
            "recall": 0.6835164835164835
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(60)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5657894736842105,
            "auc_prc": 0.703302343358938,
            "auditor_fn_violation": 0.1336884294905601,
            "auditor_fp_violation": 0.10253918695042692,
            "ave_precision_score": 0.5869317677384303,
            "fpr": 0.17214912280701755,
            "logloss": 0.6832691694527186,
            "mae": 0.49007723598103775,
            "precision": 0.6235011990407674,
            "recall": 0.5210420841683366
        },
        "train": {
            "accuracy": 0.5686059275521405,
            "auc_prc": 0.6691932747091912,
            "auditor_fn_violation": 0.11919035958552975,
            "auditor_fp_violation": 0.11547942303618541,
            "ave_precision_score": 0.5388365755826945,
            "fpr": 0.1964873765093304,
            "logloss": 0.6836677893528316,
            "mae": 0.49026055236548416,
            "precision": 0.5738095238095238,
            "recall": 0.5296703296703297
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(61)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7741228070175439,
            "auc_prc": 0.8711299987912058,
            "auditor_fn_violation": 0.007833649755651652,
            "auditor_fp_violation": 0.012544603033006245,
            "ave_precision_score": 0.8713417906407289,
            "fpr": 0.07675438596491228,
            "logloss": 0.4879321200867562,
            "mae": 0.3299962315757416,
            "precision": 0.8383371824480369,
            "recall": 0.7274549098196392
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8281160445879341,
            "auditor_fn_violation": 0.012212156668797726,
            "auditor_fp_violation": 0.010976948408342487,
            "ave_precision_score": 0.8284829207662062,
            "fpr": 0.10537870472008781,
            "logloss": 0.4909802539117315,
            "mae": 0.33244147361895127,
            "precision": 0.7803203661327232,
            "recall": 0.7494505494505495
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(62)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8151985763880232,
            "auditor_fn_violation": 0.008747758675245229,
            "auditor_fp_violation": 0.019014697761352537,
            "ave_precision_score": 0.7955796390662153,
            "fpr": 0.17105263157894737,
            "logloss": 0.5712712095519249,
            "mae": 0.3707942727364992,
            "precision": 0.7214285714285714,
            "recall": 0.8096192384769539
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.804439821496895,
            "auditor_fn_violation": 0.0029625698121856234,
            "auditor_fp_violation": 0.017772546074296618,
            "ave_precision_score": 0.7766269240569857,
            "fpr": 0.1942919868276619,
            "logloss": 0.556096489055957,
            "mae": 0.3628978953174388,
            "precision": 0.6839285714285714,
            "recall": 0.8417582417582418
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(63)",
        "seed": 4866,
        "test": {
            "accuracy": 0.49890350877192985,
            "auc_prc": 0.6836313512794646,
            "auditor_fn_violation": 0.004381570157859576,
            "auditor_fp_violation": 0.0005628477974597512,
            "ave_precision_score": 0.5771352696371544,
            "fpr": 0.01206140350877193,
            "logloss": 0.683118942615474,
            "mae": 0.4919982820487859,
            "precision": 0.828125,
            "recall": 0.1062124248496994
        },
        "train": {
            "accuracy": 0.5367727771679474,
            "auc_prc": 0.6055824746593187,
            "auditor_fn_violation": 0.0027164931665480414,
            "auditor_fp_violation": 0.0018583781077281564,
            "ave_precision_score": 0.5213742858108833,
            "fpr": 0.027442371020856202,
            "logloss": 0.6853598148923776,
            "mae": 0.4920943980028548,
            "precision": 0.6987951807228916,
            "recall": 0.12747252747252746
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(64)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8514505567186716,
            "auditor_fn_violation": 0.005845023380093523,
            "auditor_fp_violation": 0.007032942525806046,
            "ave_precision_score": 0.8502800564085629,
            "fpr": 0.07785087719298246,
            "logloss": 0.5491832056638422,
            "mae": 0.32465802859683446,
            "precision": 0.8259803921568627,
            "recall": 0.6753507014028056
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8421215205855103,
            "auditor_fn_violation": 0.008241155112724819,
            "auditor_fp_violation": 0.01034144086891213,
            "ave_precision_score": 0.8409251673774591,
            "fpr": 0.10537870472008781,
            "logloss": 0.496122956691305,
            "mae": 0.31391456641249654,
            "precision": 0.7686746987951807,
            "recall": 0.701098901098901
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(65)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.6347251061934562,
            "auditor_fn_violation": 0.006233959146362929,
            "auditor_fp_violation": 0.006549743001571728,
            "ave_precision_score": 0.5930430563280515,
            "fpr": 0.09539473684210527,
            "logloss": 0.6899037042166734,
            "mae": 0.4830426057885409,
            "precision": 0.618421052631579,
            "recall": 0.28256513026052105
        },
        "train": {
            "accuracy": 0.5433589462129528,
            "auc_prc": 0.6246119366570811,
            "auditor_fn_violation": 0.008127766854440846,
            "auditor_fp_violation": 0.007770524004852966,
            "ave_precision_score": 0.5758985555798278,
            "fpr": 0.09330406147091108,
            "logloss": 0.6730098172605915,
            "mae": 0.47547031655697036,
            "precision": 0.5933014354066986,
            "recall": 0.2725274725274725
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(66)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.850162228468424,
            "auditor_fn_violation": 0.0014414794501283273,
            "auditor_fp_violation": 0.010842784928422753,
            "ave_precision_score": 0.850502078623422,
            "fpr": 0.13267543859649122,
            "logloss": 0.5367973551048397,
            "mae": 0.34696178614959744,
            "precision": 0.7655038759689923,
            "recall": 0.7915831663326653
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8049647716039294,
            "auditor_fn_violation": 0.0027550934246872807,
            "auditor_fp_violation": 0.023239355248714545,
            "ave_precision_score": 0.8055151919408738,
            "fpr": 0.16794731064763996,
            "logloss": 0.5385199606527264,
            "mae": 0.35457261137755197,
            "precision": 0.7063339731285988,
            "recall": 0.8087912087912088
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(67)",
        "seed": 4866,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8527244959291914,
            "auditor_fn_violation": 0.00967505185810217,
            "auditor_fp_violation": 0.00833386432182151,
            "ave_precision_score": 0.8530015496811674,
            "fpr": 0.12609649122807018,
            "logloss": 0.6518513626763762,
            "mae": 0.31838268523237595,
            "precision": 0.7731755424063116,
            "recall": 0.7855711422845691
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8368551170518309,
            "auditor_fn_violation": 0.002714080650414352,
            "auditor_fp_violation": 0.01735850328345562,
            "ave_precision_score": 0.8370930382498734,
            "fpr": 0.15697036223929747,
            "logloss": 0.5272450269165432,
            "mae": 0.32424230208978144,
            "precision": 0.7265774378585086,
            "recall": 0.8351648351648352
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(68)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5515350877192983,
            "auc_prc": 0.7479114500360942,
            "auditor_fn_violation": 0.010738582428013925,
            "auditor_fp_violation": 0.003485939424833284,
            "ave_precision_score": 0.5568800793658403,
            "fpr": 0.37280701754385964,
            "logloss": 0.7132302876286696,
            "mae": 0.48252887801643,
            "precision": 0.5584415584415584,
            "recall": 0.8617234468937875
        },
        "train": {
            "accuracy": 0.5082327113062569,
            "auc_prc": 0.7175988705077618,
            "auditor_fn_violation": 0.0034450730389259505,
            "auditor_fp_violation": 0.012156488917133675,
            "ave_precision_score": 0.5038048096225922,
            "fpr": 0.4226125137211855,
            "logloss": 0.7456262050394642,
            "mae": 0.4978563224183218,
            "precision": 0.5045045045045045,
            "recall": 0.8615384615384616
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(69)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7730263157894737,
            "auc_prc": 0.8563129410051687,
            "auditor_fn_violation": 0.007539201209436424,
            "auditor_fp_violation": 0.008979015334947544,
            "ave_precision_score": 0.8565404676740603,
            "fpr": 0.09100877192982457,
            "logloss": 0.5262970341036691,
            "mae": 0.33926836594024246,
            "precision": 0.8187772925764192,
            "recall": 0.751503006012024
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8267105554751423,
            "auditor_fn_violation": 0.013213350864283904,
            "auditor_fp_violation": 0.010002022069443646,
            "ave_precision_score": 0.8270349239584847,
            "fpr": 0.12184412733260154,
            "logloss": 0.5116275295016209,
            "mae": 0.33525489204441544,
            "precision": 0.7633262260127932,
            "recall": 0.7868131868131868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(70)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0.5754187455119399,
            "auditor_fn_violation": 0.010419962732482541,
            "auditor_fp_violation": 0.006703729663140905,
            "ave_precision_score": 0.5773940504924089,
            "fpr": 0.047149122807017545,
            "logloss": 1.3356581125383276,
            "mae": 0.5037950009369013,
            "precision": 0.6666666666666666,
            "recall": 0.17234468937875752
        },
        "train": {
            "accuracy": 0.5005488474204172,
            "auc_prc": 0.48949235916022976,
            "auditor_fn_violation": 0.006554806335267373,
            "auditor_fp_violation": 0.01746682843222216,
            "ave_precision_score": 0.49003209903990624,
            "fpr": 0.07464324917672886,
            "logloss": 1.2372946953502455,
            "mae": 0.48935890556992934,
            "precision": 0.5,
            "recall": 0.14945054945054945
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(71)",
        "seed": 4866,
        "test": {
            "accuracy": 0.543859649122807,
            "auc_prc": 0.7502340859867769,
            "auditor_fn_violation": 0.0017095594698168265,
            "auditor_fp_violation": 0.0013195063931014045,
            "ave_precision_score": 0.7507030812573219,
            "fpr": 0.4473684210526316,
            "logloss": 3.5053895014105527,
            "mae": 0.45506272208468435,
            "precision": 0.5461624026696329,
            "recall": 0.9839679358717435
        },
        "train": {
            "accuracy": 0.5071350164654226,
            "auc_prc": 0.7213702442491567,
            "auditor_fn_violation": 0.00159949819664419,
            "auditor_fp_violation": 0.0035241781732046947,
            "ave_precision_score": 0.7219622554238945,
            "fpr": 0.48957189901207465,
            "logloss": 3.8417738775920647,
            "mae": 0.4926320974000461,
            "precision": 0.5033407572383074,
            "recall": 0.9934065934065934
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(72)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8199139424789131,
            "auditor_fn_violation": 0.0037685019161129317,
            "auditor_fp_violation": 0.008400237882842701,
            "ave_precision_score": 0.8204832401724813,
            "fpr": 0.08442982456140351,
            "logloss": 0.5388493587792195,
            "mae": 0.3241060468319215,
            "precision": 0.8242009132420092,
            "recall": 0.7234468937875751
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.7812465233763027,
            "auditor_fn_violation": 0.00231360297221988,
            "auditor_fp_violation": 0.018574152175168984,
            "ave_precision_score": 0.7821112506190652,
            "fpr": 0.1207464324917673,
            "logloss": 0.5830420993451094,
            "mae": 0.33844383358628244,
            "precision": 0.755011135857461,
            "recall": 0.7450549450549451
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(73)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.5949552948249391,
            "auditor_fn_violation": 0.0030104067784692185,
            "auditor_fp_violation": 0.017878382396669654,
            "ave_precision_score": 0.5615554461443866,
            "fpr": 0.3026315789473684,
            "logloss": 4.02008021118682,
            "mae": 0.41255100064147865,
            "precision": 0.6096181046676096,
            "recall": 0.8637274549098196
        },
        "train": {
            "accuracy": 0.5773874862788145,
            "auc_prc": 0.576427154180632,
            "auditor_fn_violation": 0.009859953438438625,
            "auditor_fp_violation": 0.016044158145088312,
            "ave_precision_score": 0.534670374659366,
            "fpr": 0.35016465422612514,
            "logloss": 4.533545030916981,
            "mae": 0.44025942621596303,
            "precision": 0.5494350282485876,
            "recall": 0.8549450549450549
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(74)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7996392505228465,
            "auditor_fn_violation": 0.016809935660795288,
            "auditor_fp_violation": 0.020265175650991896,
            "ave_precision_score": 0.8000589660109723,
            "fpr": 0.16666666666666666,
            "logloss": 0.5860243384642236,
            "mae": 0.3859797099874796,
            "precision": 0.7110266159695817,
            "recall": 0.749498997995992
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7715039398774769,
            "auditor_fn_violation": 0.013478727638991085,
            "auditor_fp_violation": 0.02202370635700118,
            "ave_precision_score": 0.7719674513415107,
            "fpr": 0.1986827661909989,
            "logloss": 0.6012043955891657,
            "mae": 0.3946469188897853,
            "precision": 0.6552380952380953,
            "recall": 0.756043956043956
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(75)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.6942942893686044,
            "auditor_fn_violation": 0.015122349963084075,
            "auditor_fp_violation": 0.010157809778684003,
            "ave_precision_score": 0.6753604531505114,
            "fpr": 0.11074561403508772,
            "logloss": 0.6691421134335266,
            "mae": 0.45203822947581085,
            "precision": 0.7055393586005831,
            "recall": 0.4849699398797595
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.6739589033586793,
            "auditor_fn_violation": 0.01707578919434026,
            "auditor_fp_violation": 0.0118339206963622,
            "ave_precision_score": 0.6526751207469252,
            "fpr": 0.10318331503841932,
            "logloss": 0.6539120026126974,
            "mae": 0.44337860008233465,
            "precision": 0.6877076411960132,
            "recall": 0.45494505494505494
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(76)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.768599538720529,
            "auditor_fn_violation": 0.05877764300530886,
            "auditor_fp_violation": 0.07269232403041502,
            "ave_precision_score": 0.7690213222828138,
            "fpr": 0.2138157894736842,
            "logloss": 0.696074670065069,
            "mae": 0.3877799061752019,
            "precision": 0.6608695652173913,
            "recall": 0.7615230460921844
        },
        "train": {
            "accuracy": 0.6355653128430296,
            "auc_prc": 0.7549862403873099,
            "auditor_fn_violation": 0.060103014438909065,
            "auditor_fp_violation": 0.07985729967069156,
            "ave_precision_score": 0.7554704093380489,
            "fpr": 0.24807903402854006,
            "logloss": 0.6911894140449143,
            "mae": 0.39612044054816076,
            "precision": 0.6069565217391304,
            "recall": 0.7670329670329671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(77)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6567982456140351,
            "auc_prc": 0.7616821413213649,
            "auditor_fn_violation": 0.014454347291073378,
            "auditor_fp_violation": 0.030818571853362217,
            "ave_precision_score": 0.7620712609835838,
            "fpr": 0.18092105263157895,
            "logloss": 0.6274178102088758,
            "mae": 0.43744375027324023,
            "precision": 0.6802325581395349,
            "recall": 0.7034068136272545
        },
        "train": {
            "accuracy": 0.6520307354555434,
            "auc_prc": 0.7269921437990715,
            "auditor_fn_violation": 0.00805539137042979,
            "auditor_fp_violation": 0.022286093939569016,
            "ave_precision_score": 0.7276299407809385,
            "fpr": 0.21734357848518113,
            "logloss": 0.6146913486899904,
            "mae": 0.4302600798341236,
            "precision": 0.6292134831460674,
            "recall": 0.7384615384615385
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(78)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8371465570702605,
            "auditor_fn_violation": 0.005082533488028691,
            "auditor_fp_violation": 0.002214221995667145,
            "ave_precision_score": 0.837403863531643,
            "fpr": 0.20394736842105263,
            "logloss": 0.648569345677807,
            "mae": 0.31973423219792413,
            "precision": 0.7024,
            "recall": 0.8797595190380761
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7997952663938119,
            "auditor_fn_violation": 0.006299079625094996,
            "auditor_fp_violation": 0.021376162689930096,
            "ave_precision_score": 0.8002759271337807,
            "fpr": 0.2349066959385291,
            "logloss": 0.7020327469454691,
            "mae": 0.3453328245729372,
            "precision": 0.6548387096774193,
            "recall": 0.8923076923076924
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(79)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.6689709903581108,
            "auditor_fn_violation": 0.01187902120029534,
            "auditor_fp_violation": 0.004569155940699206,
            "ave_precision_score": 0.6559341151285836,
            "fpr": 0.01206140350877193,
            "logloss": 10.86714750161374,
            "mae": 0.4946442895863228,
            "precision": 0.8552631578947368,
            "recall": 0.13026052104208416
        },
        "train": {
            "accuracy": 0.535675082327113,
            "auc_prc": 0.6083047737438998,
            "auditor_fn_violation": 0.011495639377088345,
            "auditor_fp_violation": 0.0014732220232249126,
            "ave_precision_score": 0.5982222020619078,
            "fpr": 0.02305159165751921,
            "logloss": 9.16700903413589,
            "mae": 0.4608154660323795,
            "precision": 0.7162162162162162,
            "recall": 0.11648351648351649
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(80)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.8583390128661411,
            "auditor_fn_violation": 0.0031861969553141378,
            "auditor_fp_violation": 0.010428613907650496,
            "ave_precision_score": 0.8585559549775793,
            "fpr": 0.3256578947368421,
            "logloss": 1.066013675552099,
            "mae": 0.3538647758341959,
            "precision": 0.6137841352405722,
            "recall": 0.9458917835671342
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.8447093838128699,
            "auditor_fn_violation": 0.004053027104618763,
            "auditor_fp_violation": 0.019835538351917127,
            "ave_precision_score": 0.8448494480903834,
            "fpr": 0.3600439077936334,
            "logloss": 0.932160294078214,
            "mae": 0.3702858329615444,
            "precision": 0.5734720416124838,
            "recall": 0.9692307692307692
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(81)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7719298245614035,
            "auc_prc": 0.884275367078921,
            "auditor_fn_violation": 0.003816844214745282,
            "auditor_fp_violation": 0.004632874559279558,
            "ave_precision_score": 0.8704902009264909,
            "fpr": 0.07785087719298246,
            "logloss": 0.4821272623905183,
            "mae": 0.3247976787224935,
            "precision": 0.836027713625866,
            "recall": 0.7254509018036072
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8340592740701189,
            "auditor_fn_violation": 0.0066923197548883605,
            "auditor_fp_violation": 0.009879253567508236,
            "ave_precision_score": 0.8102882399169294,
            "fpr": 0.10757409440175632,
            "logloss": 0.5143257478053197,
            "mae": 0.33605327593769907,
            "precision": 0.772093023255814,
            "recall": 0.7296703296703296
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(82)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5482456140350878,
            "auc_prc": 0.5375078063109244,
            "auditor_fn_violation": 0.025889498294835295,
            "auditor_fp_violation": 0.046960621893717344,
            "ave_precision_score": 0.5381969142398575,
            "fpr": 0.27850877192982454,
            "logloss": 0.9310316470389839,
            "mae": 0.49471372568555044,
            "precision": 0.573109243697479,
            "recall": 0.6833667334669339
        },
        "train": {
            "accuracy": 0.5345773874862788,
            "auc_prc": 0.5042189101066327,
            "auditor_fn_violation": 0.02946164702476448,
            "auditor_fp_violation": 0.04997400196429602,
            "ave_precision_score": 0.5061058004383601,
            "fpr": 0.3029637760702525,
            "logloss": 0.9293806254753332,
            "mae": 0.49582701227929277,
            "precision": 0.5265866209262435,
            "recall": 0.6747252747252748
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(83)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7708333333333334,
            "auc_prc": 0.8454509608062534,
            "auditor_fn_violation": 0.013122736701473122,
            "auditor_fp_violation": 0.012154326494201607,
            "ave_precision_score": 0.8458137927581705,
            "fpr": 0.08333333333333333,
            "logloss": 0.4959247061719634,
            "mae": 0.33858860123967915,
            "precision": 0.8280542986425339,
            "recall": 0.7334669338677354
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.7926787914667829,
            "auditor_fn_violation": 0.010373819374917067,
            "auditor_fp_violation": 0.008266412463650894,
            "ave_precision_score": 0.7922266095591691,
            "fpr": 0.10757409440175632,
            "logloss": 0.5156757748732973,
            "mae": 0.3438232981576354,
            "precision": 0.7736720554272517,
            "recall": 0.7362637362637363
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(84)",
        "seed": 4866,
        "test": {
            "accuracy": 0.45285087719298245,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.897861207303105,
            "mae": 0.5471491228070176,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5005488474204172,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.25043167912663,
            "mae": 0.4994511525795829,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(85)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8197298092934928,
            "auditor_fn_violation": 0.007732570403965826,
            "auditor_fp_violation": 0.009379911643515568,
            "ave_precision_score": 0.7770758951794583,
            "fpr": 0.13596491228070176,
            "logloss": 0.5531996910189098,
            "mae": 0.35802875312960203,
            "precision": 0.7633587786259542,
            "recall": 0.8016032064128257
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.801110895943997,
            "auditor_fn_violation": 0.0029625698121856234,
            "auditor_fp_violation": 0.014154486105494255,
            "ave_precision_score": 0.7521358058280564,
            "fpr": 0.15806805708013172,
            "logloss": 0.5276943515898841,
            "mae": 0.3487575394229229,
            "precision": 0.7267552182163188,
            "recall": 0.8417582417582418
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(86)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8257376954223167,
            "auditor_fn_violation": 0.0013887423970748618,
            "auditor_fp_violation": 0.009759568412556817,
            "ave_precision_score": 0.819381927692117,
            "fpr": 0.08991228070175439,
            "logloss": 0.5894847796641732,
            "mae": 0.3829002678945759,
            "precision": 0.8097447795823666,
            "recall": 0.6993987975951904
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7714982732148771,
            "auditor_fn_violation": 0.006277366979891689,
            "auditor_fp_violation": 0.018675255647351088,
            "ave_precision_score": 0.7645458314968937,
            "fpr": 0.12184412733260154,
            "logloss": 0.5765165934569294,
            "mae": 0.38137716660940557,
            "precision": 0.7442396313364056,
            "recall": 0.7098901098901099
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(87)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8446198963203397,
            "auditor_fn_violation": 0.01238002320430335,
            "auditor_fp_violation": 0.017291640117242258,
            "ave_precision_score": 0.8444361979513442,
            "fpr": 0.0800438596491228,
            "logloss": 0.5550658180354939,
            "mae": 0.32732752855793623,
            "precision": 0.8228155339805825,
            "recall": 0.6793587174348698
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8029922267934321,
            "auditor_fn_violation": 0.006149503624805493,
            "auditor_fp_violation": 0.01600804976216612,
            "ave_precision_score": 0.8040991956696883,
            "fpr": 0.09001097694840834,
            "logloss": 0.5348130340025139,
            "mae": 0.3212303285406234,
            "precision": 0.7913486005089059,
            "recall": 0.6835164835164835
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(88)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5032894736842105,
            "auc_prc": 0.5494153976300815,
            "auditor_fn_violation": 0.023498751889744415,
            "auditor_fp_violation": 0.03969138949067585,
            "ave_precision_score": 0.545066474046476,
            "fpr": 0.1524122807017544,
            "logloss": 0.749740257448963,
            "mae": 0.4971331270705712,
            "precision": 0.5709876543209876,
            "recall": 0.37074148296593185
        },
        "train": {
            "accuracy": 0.5323819978046103,
            "auc_prc": 0.5276444866946333,
            "auditor_fn_violation": 0.009995054341925939,
            "auditor_fp_violation": 0.03371078629614652,
            "ave_precision_score": 0.5152900821907949,
            "fpr": 0.16465422612513722,
            "logloss": 0.7509023895698649,
            "mae": 0.49460582212676857,
            "precision": 0.5440729483282675,
            "recall": 0.3934065934065934
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(89)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.8822059686295324,
            "auditor_fn_violation": 0.00436838589459621,
            "auditor_fp_violation": 0.025431693640881867,
            "ave_precision_score": 0.8823810670831851,
            "fpr": 0.31359649122807015,
            "logloss": 0.6819987977052722,
            "mae": 0.32062575156624806,
            "precision": 0.6246719160104987,
            "recall": 0.9539078156312625
        },
        "train": {
            "accuracy": 0.6311745334796927,
            "auc_prc": 0.8517930264262935,
            "auditor_fn_violation": 0.009264061953414315,
            "auditor_fp_violation": 0.024486298072293796,
            "ave_precision_score": 0.852353706966872,
            "fpr": 0.34796926454445665,
            "logloss": 0.7896445522213834,
            "mae": 0.35342474492178855,
            "precision": 0.5790172642762285,
            "recall": 0.9582417582417583
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(90)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8639327325446987,
            "auditor_fn_violation": 0.012412983862461775,
            "auditor_fp_violation": 0.0050045665009982605,
            "ave_precision_score": 0.8642254995368996,
            "fpr": 0.07894736842105263,
            "logloss": 0.5061716361883558,
            "mae": 0.3228820452094895,
            "precision": 0.8285714285714286,
            "recall": 0.6973947895791583
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8413743691275087,
            "auditor_fn_violation": 0.01869217500392034,
            "auditor_fp_violation": 0.005488474204171241,
            "ave_precision_score": 0.841757319684146,
            "fpr": 0.09659714599341383,
            "logloss": 0.49586229983250296,
            "mae": 0.32341543395117206,
            "precision": 0.7899761336515513,
            "recall": 0.7274725274725274
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(91)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6129385964912281,
            "auc_prc": 0.6420164860804736,
            "auditor_fn_violation": 0.009453116759835465,
            "auditor_fp_violation": 0.0123587570621469,
            "ave_precision_score": 0.6433562176533616,
            "fpr": 0.30701754385964913,
            "logloss": 0.6917835215325387,
            "mae": 0.4463816022402362,
            "precision": 0.603399433427762,
            "recall": 0.8537074148296593
        },
        "train": {
            "accuracy": 0.5938529088913282,
            "auc_prc": 0.6157642652878702,
            "auditor_fn_violation": 0.011324350731595519,
            "auditor_fp_violation": 0.02035068461494023,
            "ave_precision_score": 0.6174471219346358,
            "fpr": 0.3194291986827662,
            "logloss": 0.7304321472276615,
            "mae": 0.45799144492944355,
            "precision": 0.5637181409295352,
            "recall": 0.8263736263736263
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(92)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8071293069277159,
            "auditor_fn_violation": 0.009442129873782656,
            "auditor_fp_violation": 0.012786202795123408,
            "ave_precision_score": 0.8092300561144349,
            "fpr": 0.16557017543859648,
            "logloss": 0.5335446479971947,
            "mae": 0.34822118482470776,
            "precision": 0.7355516637478109,
            "recall": 0.8416833667334669
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.8023687193208208,
            "auditor_fn_violation": 0.013710329187826447,
            "auditor_fp_violation": 0.023564330695014155,
            "ave_precision_score": 0.797665299201497,
            "fpr": 0.20087815587266739,
            "logloss": 0.5523265797833332,
            "mae": 0.36302738257251377,
            "precision": 0.6783831282952548,
            "recall": 0.8483516483516483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(93)",
        "seed": 4866,
        "test": {
            "accuracy": 0.5197368421052632,
            "auc_prc": 0.5494397805445554,
            "auditor_fn_violation": 0.06398982174876068,
            "auditor_fp_violation": 0.05851227645384648,
            "ave_precision_score": 0.5509084927263754,
            "fpr": 0.17434210526315788,
            "logloss": 0.6937865672922771,
            "mae": 0.49894284682446405,
            "precision": 0.5804749340369393,
            "recall": 0.4408817635270541
        },
        "train": {
            "accuracy": 0.5543358946212953,
            "auc_prc": 0.5265906576583519,
            "auditor_fn_violation": 0.05417787481453783,
            "auditor_fp_violation": 0.058531688716852506,
            "ave_precision_score": 0.5284419467935559,
            "fpr": 0.18990120746432493,
            "logloss": 0.6892757285428749,
            "mae": 0.4967434549240066,
            "precision": 0.5620253164556962,
            "recall": 0.4879120879120879
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(94)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.6817636616550522,
            "auditor_fn_violation": 0.014979520444397572,
            "auditor_fp_violation": 0.018454504906333637,
            "ave_precision_score": 0.6616439551347182,
            "fpr": 0.23793859649122806,
            "logloss": 2.5393728248897265,
            "mae": 0.3483843033424623,
            "precision": 0.6588050314465409,
            "recall": 0.8396793587174348
        },
        "train": {
            "accuracy": 0.6564215148188803,
            "auc_prc": 0.6355743727750147,
            "auditor_fn_violation": 0.007104860013751343,
            "auditor_fp_violation": 0.02849432857665569,
            "ave_precision_score": 0.6103893960134645,
            "fpr": 0.27771679473106475,
            "logloss": 2.957894390169091,
            "mae": 0.36635084015521086,
            "precision": 0.6095679012345679,
            "recall": 0.8681318681318682
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(95)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8627270215715543,
            "auditor_fn_violation": 0.006451499490208489,
            "auditor_fp_violation": 0.01634648060830041,
            "ave_precision_score": 0.8629660229385583,
            "fpr": 0.16447368421052633,
            "logloss": 0.5875776384005673,
            "mae": 0.27817828271521683,
            "precision": 0.7418244406196214,
            "recall": 0.8637274549098196
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.802750047938092,
            "auditor_fn_violation": 0.007418487111132558,
            "auditor_fp_violation": 0.025709168640591605,
            "ave_precision_score": 0.8036705157695156,
            "fpr": 0.18880351262349068,
            "logloss": 0.6494304260851241,
            "mae": 0.2992912163343546,
            "precision": 0.6928571428571428,
            "recall": 0.8527472527472527
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(96)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6282894736842105,
            "auc_prc": 0.6521175430025694,
            "auditor_fn_violation": 0.008919154097669026,
            "auditor_fp_violation": 0.008381653285756773,
            "ave_precision_score": 0.6391883313125617,
            "fpr": 0.28399122807017546,
            "logloss": 3.2794009328626657,
            "mae": 0.37436146266949477,
            "precision": 0.6179941002949852,
            "recall": 0.8396793587174348
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.6094766191782537,
            "auditor_fn_violation": 0.003971001556072907,
            "auditor_fp_violation": 0.01323011150268648,
            "ave_precision_score": 0.5978719159048549,
            "fpr": 0.31613611416026344,
            "logloss": 3.315137343486631,
            "mae": 0.40872254966352,
            "precision": 0.5596330275229358,
            "recall": 0.8043956043956044
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(97)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8561403153373622,
            "auditor_fn_violation": 0.007763333684913686,
            "auditor_fp_violation": 0.019731532220381466,
            "ave_precision_score": 0.8562986244106506,
            "fpr": 0.18201754385964913,
            "logloss": 0.5932507253229736,
            "mae": 0.4365923184443984,
            "precision": 0.717206132879046,
            "recall": 0.843687374749499
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.8323768674690404,
            "auditor_fn_violation": 0.0013871967768784503,
            "auditor_fp_violation": 0.019522599033258228,
            "ave_precision_score": 0.8323984562975151,
            "fpr": 0.2217343578485181,
            "logloss": 0.5986883804935559,
            "mae": 0.4395685393023831,
            "precision": 0.661641541038526,
            "recall": 0.8681318681318682
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(98)",
        "seed": 4866,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8145544416912522,
            "auditor_fn_violation": 0.007842439264493899,
            "auditor_fp_violation": 0.008227666624187596,
            "ave_precision_score": 0.7775033571981491,
            "fpr": 0.13157894736842105,
            "logloss": 0.5498629063184284,
            "mae": 0.3529025568063126,
            "precision": 0.7678916827852998,
            "recall": 0.7955911823647295
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.7986056351689848,
            "auditor_fn_violation": 0.003811775491248602,
            "auditor_fp_violation": 0.01375007221676585,
            "ave_precision_score": 0.7543625246550579,
            "fpr": 0.14928649835345773,
            "logloss": 0.5390594593440204,
            "mae": 0.35056207988218996,
            "precision": 0.7364341085271318,
            "recall": 0.8351648351648352
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(99)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.7500888439706468,
            "auditor_fn_violation": 0.0023929437823014483,
            "auditor_fp_violation": 0.008641837644959858,
            "ave_precision_score": 0.7509326000553044,
            "fpr": 0.04276315789473684,
            "logloss": 0.9205156918284606,
            "mae": 0.3973204653974263,
            "precision": 0.827433628318584,
            "recall": 0.374749498997996
        },
        "train": {
            "accuracy": 0.6388583973655324,
            "auc_prc": 0.7189973948432022,
            "auditor_fn_violation": 0.00680570801317234,
            "auditor_fp_violation": 0.003581951585880178,
            "ave_precision_score": 0.7199959390433661,
            "fpr": 0.04610318331503842,
            "logloss": 0.841856631635403,
            "mae": 0.3737581520068732,
            "precision": 0.8,
            "recall": 0.36923076923076925
        }
    }
]
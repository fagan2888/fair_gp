[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 18313,
        "test": {
            "accuracy": 0.46381578947368424,
            "auc_prc": 0.6362224836099795,
            "auditor_fn_violation": 0.015742481203007527,
            "auditor_fp_violation": 0.015197469016578143,
            "ave_precision_score": 0.6209399879286912,
            "fpr": 0.051535087719298246,
            "logloss": 17.85107000788101,
            "mae": 0.5355557680381415,
            "precision": 0.41975308641975306,
            "recall": 0.07142857142857142
        },
        "train": {
            "accuracy": 0.45554335894621295,
            "auc_prc": 0.6205192060912135,
            "auditor_fn_violation": 0.013893417964533906,
            "auditor_fp_violation": 0.01556039476452798,
            "ave_precision_score": 0.6081824408276687,
            "fpr": 0.05159165751920966,
            "logloss": 18.075131654798977,
            "mae": 0.5463682918938028,
            "precision": 0.3815789473684211,
            "recall": 0.060669456066945605
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4692982456140351,
            "auc_prc": 0.6416385572687555,
            "auditor_fn_violation": 0.009320175438596503,
            "auditor_fp_violation": 0.011151014002897152,
            "ave_precision_score": 0.6361688691817483,
            "fpr": 0.027412280701754384,
            "logloss": 17.86492951780337,
            "mae": 0.531409372961217,
            "precision": 0.40476190476190477,
            "recall": 0.03571428571428571
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.64310974053531,
            "auditor_fn_violation": 0.007073012781944534,
            "auditor_fp_violation": 0.01120510668934729,
            "ave_precision_score": 0.6372735572058279,
            "fpr": 0.029637760702524697,
            "logloss": 18.08337972963716,
            "mae": 0.5386754652325826,
            "precision": 0.37209302325581395,
            "recall": 0.03347280334728033
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 18313,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.6889861062605303,
            "auditor_fn_violation": 0.006224200206398348,
            "auditor_fp_violation": 0.022960928697891514,
            "ave_precision_score": 0.677941951365231,
            "fpr": 0.2050438596491228,
            "logloss": 1.9062694186947058,
            "mae": 0.30390849770782025,
            "precision": 0.6883333333333334,
            "recall": 0.8676470588235294
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7026292542708628,
            "auditor_fn_violation": 0.008866526737366175,
            "auditor_fp_violation": 0.02586047360588953,
            "ave_precision_score": 0.6904987316726752,
            "fpr": 0.1942919868276619,
            "logloss": 1.7485025056862353,
            "mae": 0.3083487831767848,
            "precision": 0.6916376306620209,
            "recall": 0.8305439330543933
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.637162512453189,
            "auditor_fn_violation": 0.010117204776647501,
            "auditor_fp_violation": 0.012964248350233383,
            "ave_precision_score": 0.6308127845385715,
            "fpr": 0.03179824561403509,
            "logloss": 17.777258432703327,
            "mae": 0.5323299512988986,
            "precision": 0.40816326530612246,
            "recall": 0.04201680672268908
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6420518944661605,
            "auditor_fn_violation": 0.009047944922357614,
            "auditor_fp_violation": 0.011306510369793871,
            "ave_precision_score": 0.6355011308303188,
            "fpr": 0.03293084522502744,
            "logloss": 17.965743943795616,
            "mae": 0.5385516311024778,
            "precision": 0.3877551020408163,
            "recall": 0.0397489539748954
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.6426353179410971,
            "auditor_fn_violation": 0.01032222099366062,
            "auditor_fp_violation": 0.01224750523096733,
            "ave_precision_score": 0.6364519946727,
            "fpr": 0.027412280701754384,
            "logloss": 17.749819609068755,
            "mae": 0.5296480451726595,
            "precision": 0.4318181818181818,
            "recall": 0.03991596638655462
        },
        "train": {
            "accuracy": 0.4643249176728869,
            "auc_prc": 0.6425676746570662,
            "auditor_fn_violation": 0.007633342365968703,
            "auditor_fp_violation": 0.01141044914225162,
            "ave_precision_score": 0.6366074650360527,
            "fpr": 0.029637760702524697,
            "logloss": 17.939880582937977,
            "mae": 0.5368174801066927,
            "precision": 0.38636363636363635,
            "recall": 0.03556485355648536
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6375820187328628,
            "auditor_fn_violation": 0.009269497272593248,
            "auditor_fp_violation": 0.010117394978271368,
            "ave_precision_score": 0.6332254189823947,
            "fpr": 0.027412280701754384,
            "logloss": 17.878410612527826,
            "mae": 0.531842616031151,
            "precision": 0.3902439024390244,
            "recall": 0.03361344537815126
        },
        "train": {
            "accuracy": 0.4643249176728869,
            "auc_prc": 0.639401035426262,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.009851367555385423,
            "ave_precision_score": 0.6340729017014153,
            "fpr": 0.027442371020856202,
            "logloss": 18.127593741391628,
            "mae": 0.5376634143911694,
            "precision": 0.375,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.642628951433021,
            "auditor_fn_violation": 0.009914492112634543,
            "auditor_fp_violation": 0.011151014002897152,
            "ave_precision_score": 0.6371698968966077,
            "fpr": 0.027412280701754384,
            "logloss": 17.865944682630165,
            "mae": 0.5315644363317117,
            "precision": 0.4186046511627907,
            "recall": 0.037815126050420166
        },
        "train": {
            "accuracy": 0.4610318331503842,
            "auc_prc": 0.6443154542340799,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.010776676139460484,
            "ave_precision_score": 0.6384776242276305,
            "fpr": 0.030735455543358946,
            "logloss": 18.084869835801406,
            "mae": 0.5393273224255732,
            "precision": 0.3488372093023256,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.7505681397207475,
            "auditor_fn_violation": 0.0013867389060887693,
            "auditor_fp_violation": 0.0007066835667149528,
            "ave_precision_score": 0.7515311074447163,
            "fpr": 0.009868421052631578,
            "logloss": 17.759038838484482,
            "mae": 0.5277570568253858,
            "precision": 0.4,
            "recall": 0.012605042016806723
        },
        "train": {
            "accuracy": 0.47200878155872666,
            "auc_prc": 0.7430743759899414,
            "auditor_fn_violation": 0.001451345479931492,
            "auditor_fp_violation": 0.003371672374848845,
            "ave_precision_score": 0.7441885039055323,
            "fpr": 0.009879253567508232,
            "logloss": 17.88799548084749,
            "mae": 0.5308435878948473,
            "precision": 0.4,
            "recall": 0.012552301255230125
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 18313,
        "test": {
            "accuracy": 0.46271929824561403,
            "auc_prc": 0.6446782313185672,
            "auditor_fn_violation": 0.0038814868052484326,
            "auditor_fp_violation": 0.008346913729277322,
            "ave_precision_score": 0.6438518311588968,
            "fpr": 0.025219298245614034,
            "logloss": 18.17036107223626,
            "mae": 0.5378797046266801,
            "precision": 0.28125,
            "recall": 0.018907563025210083
        },
        "train": {
            "accuracy": 0.45773874862788144,
            "auc_prc": 0.633677858808222,
            "auditor_fn_violation": 0.0016763958866297171,
            "auditor_fp_violation": 0.008269470140418747,
            "ave_precision_score": 0.6328021436007917,
            "fpr": 0.02854006586169045,
            "logloss": 18.404042064309053,
            "mae": 0.5450089345974575,
            "precision": 0.2777777777777778,
            "recall": 0.02092050209205021
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6392235499800096,
            "auditor_fn_violation": 0.01032222099366062,
            "auditor_fp_violation": 0.012182118139385162,
            "ave_precision_score": 0.6328838030671242,
            "fpr": 0.03070175438596491,
            "logloss": 17.840294084128956,
            "mae": 0.5316699698537233,
            "precision": 0.40425531914893614,
            "recall": 0.03991596638655462
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6393797786274887,
            "auditor_fn_violation": 0.007633342365968703,
            "auditor_fp_violation": 0.010989623868398305,
            "ave_precision_score": 0.6333014092301223,
            "fpr": 0.030735455543358946,
            "logloss": 18.033535207222258,
            "mae": 0.5389287942742784,
            "precision": 0.37777777777777777,
            "recall": 0.03556485355648536
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.6470312838583144,
            "auditor_fn_violation": 0.010752985404688211,
            "auditor_fp_violation": 0.015157230806373732,
            "ave_precision_score": 0.6397142960172647,
            "fpr": 0.03179824561403509,
            "logloss": 10.379303465840794,
            "mae": 0.5268531391160547,
            "precision": 0.46296296296296297,
            "recall": 0.052521008403361345
        },
        "train": {
            "accuracy": 0.4676180021953897,
            "auc_prc": 0.651705034004004,
            "auditor_fn_violation": 0.011877150035135434,
            "auditor_fp_violation": 0.012556310731297995,
            "ave_precision_score": 0.6431795436739189,
            "fpr": 0.03293084522502744,
            "logloss": 10.398746750764628,
            "mae": 0.5303494891948916,
            "precision": 0.4339622641509434,
            "recall": 0.04811715481171548
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.6600282149792966,
            "auditor_fn_violation": 0.014963880288957718,
            "auditor_fp_violation": 0.01963876146788991,
            "ave_precision_score": 0.6503272031262175,
            "fpr": 0.047149122807017545,
            "logloss": 4.2564714955458625,
            "mae": 0.5187524042385612,
            "precision": 0.4819277108433735,
            "recall": 0.08403361344537816
        },
        "train": {
            "accuracy": 0.46871569703622395,
            "auc_prc": 0.6672008193400853,
            "auditor_fn_violation": 0.018665405159625042,
            "auditor_fp_violation": 0.015083797466429046,
            "ave_precision_score": 0.6573869007818767,
            "fpr": 0.04610318331503842,
            "logloss": 4.128179562177608,
            "mae": 0.5172102653530017,
            "precision": 0.46153846153846156,
            "recall": 0.07531380753138076
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4967105263157895,
            "auc_prc": 0.6716560659661919,
            "auditor_fn_violation": 0.01707393483709273,
            "auditor_fp_violation": 0.02171605906969258,
            "ave_precision_score": 0.6597976426018676,
            "fpr": 0.05592105263157895,
            "logloss": 3.287739418987212,
            "mae": 0.4881361552501694,
            "precision": 0.5714285714285714,
            "recall": 0.14285714285714285
        },
        "train": {
            "accuracy": 0.5115257958287596,
            "auc_prc": 0.6683622479914131,
            "auditor_fn_violation": 0.022872469905249198,
            "auditor_fp_violation": 0.013823856736880268,
            "ave_precision_score": 0.656397976847538,
            "fpr": 0.052689352360043906,
            "logloss": 3.284214257282848,
            "mae": 0.479797131396381,
            "precision": 0.627906976744186,
            "recall": 0.1694560669456067
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 18313,
        "test": {
            "accuracy": 0.46710526315789475,
            "auc_prc": 0.6392510598902561,
            "auditor_fn_violation": 0.009269497272593248,
            "auditor_fp_violation": 0.010763721229679704,
            "ave_precision_score": 0.6342170676202528,
            "fpr": 0.02850877192982456,
            "logloss": 17.897582289196087,
            "mae": 0.5326259226743705,
            "precision": 0.38095238095238093,
            "recall": 0.03361344537815126
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6407801359466004,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.01020121025292613,
            "ave_precision_score": 0.6349452475125488,
            "fpr": 0.02854006586169045,
            "logloss": 18.11342821405268,
            "mae": 0.5379052055147909,
            "precision": 0.36585365853658536,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 18313,
        "test": {
            "accuracy": 0.46271929824561403,
            "auc_prc": 0.6261300781330637,
            "auditor_fn_violation": 0.010117204776647501,
            "auditor_fp_violation": 0.012051343956220829,
            "ave_precision_score": 0.6193745252219796,
            "fpr": 0.03728070175438596,
            "logloss": 17.7110722749353,
            "mae": 0.5349365406508609,
            "precision": 0.37037037037037035,
            "recall": 0.04201680672268908
        },
        "train": {
            "accuracy": 0.4566410537870472,
            "auc_prc": 0.6403733538238607,
            "auditor_fn_violation": 0.009667981757138466,
            "auditor_fp_violation": 0.011111308284934202,
            "ave_precision_score": 0.6332383096852099,
            "fpr": 0.04061470911086718,
            "logloss": 17.918519769231942,
            "mae": 0.542153371738798,
            "precision": 0.3508771929824561,
            "recall": 0.04184100418410042
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 18313,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.6892108402856818,
            "auditor_fn_violation": 0.005579205366357076,
            "auditor_fp_violation": 0.02573233542572027,
            "ave_precision_score": 0.6783887785604558,
            "fpr": 0.12280701754385964,
            "logloss": 1.9595843896266822,
            "mae": 0.3118092709802141,
            "precision": 0.748314606741573,
            "recall": 0.6995798319327731
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7016624456201068,
            "auditor_fn_violation": 0.021885003835042646,
            "auditor_fp_violation": 0.023277214846512857,
            "ave_precision_score": 0.6900542829941309,
            "fpr": 0.1394072447859495,
            "logloss": 1.811568278638722,
            "mae": 0.3183777857748301,
            "precision": 0.7171492204899778,
            "recall": 0.6736401673640168
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.6426853827943335,
            "auditor_fn_violation": 0.009914492112634543,
            "auditor_fp_violation": 0.011151014002897152,
            "ave_precision_score": 0.6372261636558163,
            "fpr": 0.027412280701754384,
            "logloss": 17.867281146449958,
            "mae": 0.5315820480487,
            "precision": 0.4186046511627907,
            "recall": 0.037815126050420166
        },
        "train": {
            "accuracy": 0.4610318331503842,
            "auc_prc": 0.6444633312453885,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.010776676139460484,
            "ave_precision_score": 0.6386252435958164,
            "fpr": 0.030735455543358946,
            "logloss": 18.085933256324406,
            "mae": 0.5393258646989594,
            "precision": 0.3488372093023256,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 18313,
        "test": {
            "accuracy": 0.32785087719298245,
            "auc_prc": 0.5106636422498296,
            "auditor_fn_violation": 0.010098776352646319,
            "auditor_fp_violation": 0.022993622243682607,
            "ave_precision_score": 0.45234356231499756,
            "fpr": 0.23793859649122806,
            "logloss": 22.968680322474253,
            "mae": 0.6711863896003272,
            "precision": 0.26936026936026936,
            "recall": 0.16806722689075632
        },
        "train": {
            "accuracy": 0.31833150384193193,
            "auc_prc": 0.5071851986026279,
            "auditor_fn_violation": 0.011946043016777737,
            "auditor_fp_violation": 0.01912219904021416,
            "ave_precision_score": 0.45214774477392805,
            "fpr": 0.24039517014270034,
            "logloss": 23.25191495328055,
            "mae": 0.6820756335682455,
            "precision": 0.2576271186440678,
            "recall": 0.1589958158995816
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.646077079303357,
            "auditor_fn_violation": 0.010172490048651048,
            "auditor_fp_violation": 0.014772452921294062,
            "ave_precision_score": 0.6388108294173145,
            "fpr": 0.03289473684210526,
            "logloss": 12.628441366693128,
            "mae": 0.5276057233221625,
            "precision": 0.4444444444444444,
            "recall": 0.05042016806722689
        },
        "train": {
            "accuracy": 0.4654226125137212,
            "auc_prc": 0.6453787049945667,
            "auditor_fn_violation": 0.010673819289116291,
            "auditor_fp_violation": 0.012612082755543612,
            "ave_precision_score": 0.6382699963722687,
            "fpr": 0.031833150384193196,
            "logloss": 12.685516356680534,
            "mae": 0.5337823800828138,
            "precision": 0.40816326530612246,
            "recall": 0.04184100418410042
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 18313,
        "test": {
            "accuracy": 0.5603070175438597,
            "auc_prc": 0.7660814308977645,
            "auditor_fn_violation": 0.0008661359280554331,
            "auditor_fp_violation": 0.009875965717044901,
            "ave_precision_score": 0.5438177112415178,
            "fpr": 0.4309210526315789,
            "logloss": 14.983804808899015,
            "mae": 0.439693399110255,
            "precision": 0.5435540069686411,
            "recall": 0.9831932773109243
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.771966792206904,
            "auditor_fn_violation": 0.0021517574599616953,
            "auditor_fp_violation": 0.010170789148792173,
            "ave_precision_score": 0.5515447045244051,
            "fpr": 0.424807903402854,
            "logloss": 14.58451573716991,
            "mae": 0.43022518950789496,
            "precision": 0.55,
            "recall": 0.9895397489539749
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4649122807017544,
            "auc_prc": 0.6166472493449953,
            "auditor_fn_violation": 0.0033631873802152613,
            "auditor_fp_violation": 0.00541203927249316,
            "ave_precision_score": 0.6170484671502987,
            "fpr": 0.021929824561403508,
            "logloss": 18.003049563549556,
            "mae": 0.5372904157337383,
            "precision": 0.2857142857142857,
            "recall": 0.01680672268907563
        },
        "train": {
            "accuracy": 0.45773874862788144,
            "auc_prc": 0.6255419464716446,
            "auditor_fn_violation": 0.0014926812689168686,
            "auditor_fp_violation": 0.0006287028187688072,
            "ave_precision_score": 0.6238853102189803,
            "fpr": 0.02854006586169045,
            "logloss": 18.03502420406719,
            "mae": 0.5411151908358163,
            "precision": 0.2777777777777778,
            "recall": 0.02092050209205021
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 18313,
        "test": {
            "accuracy": 0.46710526315789475,
            "auc_prc": 0.6166611559045133,
            "auditor_fn_violation": 0.003003833112192258,
            "auditor_fp_violation": 0.005090133590857879,
            "ave_precision_score": 0.6171058501231655,
            "fpr": 0.019736842105263157,
            "logloss": 17.935019617429546,
            "mae": 0.5348598229620442,
            "precision": 0.3076923076923077,
            "recall": 0.01680672268907563
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6237197964356119,
            "auditor_fn_violation": 0.0016029100395445694,
            "auditor_fp_violation": 0.0057191675771872145,
            "ave_precision_score": 0.6221901165671837,
            "fpr": 0.021953896816684963,
            "logloss": 17.99463416899899,
            "mae": 0.5387266471534483,
            "precision": 0.3103448275862069,
            "recall": 0.01882845188284519
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4692982456140351,
            "auc_prc": 0.6416385572687555,
            "auditor_fn_violation": 0.009320175438596503,
            "auditor_fp_violation": 0.011151014002897152,
            "ave_precision_score": 0.6361688691817483,
            "fpr": 0.027412280701754384,
            "logloss": 17.86495403136035,
            "mae": 0.5314095398817984,
            "precision": 0.40476190476190477,
            "recall": 0.03571428571428571
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.64310974053531,
            "auditor_fn_violation": 0.007073012781944534,
            "auditor_fp_violation": 0.01120510668934729,
            "ave_precision_score": 0.6372735572058279,
            "fpr": 0.029637760702524697,
            "logloss": 18.083401890999898,
            "mae": 0.5386764946210186,
            "precision": 0.37209302325581395,
            "recall": 0.03347280334728033
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.6456054422937213,
            "auditor_fn_violation": 0.008976946041574523,
            "auditor_fp_violation": 0.015157230806373732,
            "ave_precision_score": 0.6382866014771882,
            "fpr": 0.03179824561403509,
            "logloss": 9.168752466688591,
            "mae": 0.5264093416955524,
            "precision": 0.48214285714285715,
            "recall": 0.05672268907563025
        },
        "train": {
            "accuracy": 0.47091108671789245,
            "auc_prc": 0.6530227067696927,
            "auditor_fn_violation": 0.012901359028884535,
            "auditor_fp_violation": 0.012556310731297995,
            "ave_precision_score": 0.6444971771054757,
            "fpr": 0.03293084522502744,
            "logloss": 9.135285048141768,
            "mae": 0.530153303482395,
            "precision": 0.4642857142857143,
            "recall": 0.05439330543933055
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6424702385581581,
            "auditor_fn_violation": 0.009202694235588976,
            "auditor_fp_violation": 0.010763721229679704,
            "ave_precision_score": 0.6374191720175609,
            "fpr": 0.02850877192982456,
            "logloss": 17.89106395832315,
            "mae": 0.5323754780788525,
            "precision": 0.3953488372093023,
            "recall": 0.03571428571428571
        },
        "train": {
            "accuracy": 0.4621295279912184,
            "auc_prc": 0.6451318975173832,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.010107411848513043,
            "ave_precision_score": 0.6392927649309997,
            "fpr": 0.029637760702524697,
            "logloss": 18.103582817961573,
            "mae": 0.5390086265522726,
            "precision": 0.35714285714285715,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 18313,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.6622011579307727,
            "auditor_fn_violation": 0.025878114403656206,
            "auditor_fp_violation": 0.044669443103170774,
            "ave_precision_score": 0.6463257992937784,
            "fpr": 0.16337719298245615,
            "logloss": 2.4753968068213386,
            "mae": 0.3225267522495571,
            "precision": 0.7066929133858267,
            "recall": 0.7542016806722689
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.680270870676244,
            "auditor_fn_violation": 0.029816882454794722,
            "auditor_fp_violation": 0.044693672156830934,
            "ave_precision_score": 0.6662636049036064,
            "fpr": 0.16355653128430298,
            "logloss": 2.1342123226009186,
            "mae": 0.31759656650149176,
            "precision": 0.703187250996016,
            "recall": 0.7384937238493724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6423966273296142,
            "auditor_fn_violation": 0.009202694235588976,
            "auditor_fp_violation": 0.010763721229679704,
            "ave_precision_score": 0.6373600807093625,
            "fpr": 0.02850877192982456,
            "logloss": 17.90376209032045,
            "mae": 0.532833065599579,
            "precision": 0.3953488372093023,
            "recall": 0.03571428571428571
        },
        "train": {
            "accuracy": 0.4621295279912184,
            "auc_prc": 0.6454316112576666,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.010107411848513043,
            "ave_precision_score": 0.6395928308393473,
            "fpr": 0.029637760702524697,
            "logloss": 18.117508114999612,
            "mae": 0.5390743157750351,
            "precision": 0.35714285714285715,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.7510933340935362,
            "auditor_fn_violation": 0.0013867389060887693,
            "auditor_fp_violation": 0.0007066835667149528,
            "ave_precision_score": 0.7520049629089904,
            "fpr": 0.009868421052631578,
            "logloss": 17.76219505826629,
            "mae": 0.5284025886488196,
            "precision": 0.4,
            "recall": 0.012605042016806723
        },
        "train": {
            "accuracy": 0.47091108671789245,
            "auc_prc": 0.741195352919717,
            "auditor_fn_violation": 0.001451345479931492,
            "auditor_fp_violation": 0.004910473225625725,
            "ave_precision_score": 0.7423091668029009,
            "fpr": 0.010976948408342482,
            "logloss": 17.890565358413106,
            "mae": 0.5318573412825849,
            "precision": 0.375,
            "recall": 0.012552301255230125
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6373734183928679,
            "auditor_fn_violation": 0.010117204776647501,
            "auditor_fp_violation": 0.012964248350233383,
            "ave_precision_score": 0.6306182956084879,
            "fpr": 0.03179824561403509,
            "logloss": 17.832710382502583,
            "mae": 0.5320372843182961,
            "precision": 0.40816326530612246,
            "recall": 0.04201680672268908
        },
        "train": {
            "accuracy": 0.4621295279912184,
            "auc_prc": 0.6365751632682874,
            "auditor_fn_violation": 0.007633342365968703,
            "auditor_fp_violation": 0.012176046929623312,
            "ave_precision_score": 0.6304988621758649,
            "fpr": 0.031833150384193196,
            "logloss": 18.022331625145593,
            "mae": 0.5395169036000885,
            "precision": 0.3695652173913043,
            "recall": 0.03556485355648536
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6350609245518338,
            "auditor_fn_violation": 0.009914492112634543,
            "auditor_fp_violation": 0.011472919684532434,
            "ave_precision_score": 0.6287167722525181,
            "fpr": 0.029605263157894735,
            "logloss": 17.946046169161626,
            "mae": 0.5337299401492632,
            "precision": 0.4,
            "recall": 0.037815126050420166
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.635128605422249,
            "auditor_fn_violation": 0.007073012781944534,
            "auditor_fp_violation": 0.01120510668934729,
            "ave_precision_score": 0.6291737939695166,
            "fpr": 0.029637760702524697,
            "logloss": 18.113767481682242,
            "mae": 0.5404387480606094,
            "precision": 0.37209302325581395,
            "recall": 0.03347280334728033
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.6416922517599876,
            "auditor_fn_violation": 0.011821833996756612,
            "auditor_fp_violation": 0.012569410912602608,
            "ave_precision_score": 0.6355068381529023,
            "fpr": 0.029605263157894735,
            "logloss": 17.584682016752748,
            "mae": 0.5299726153659332,
            "precision": 0.425531914893617,
            "recall": 0.04201680672268908
        },
        "train": {
            "accuracy": 0.4654226125137212,
            "auc_prc": 0.6443234503183477,
            "auditor_fn_violation": 0.008841265977430662,
            "auditor_fp_violation": 0.012176046929623312,
            "ave_precision_score": 0.6373326757413434,
            "fpr": 0.031833150384193196,
            "logloss": 17.770939300405246,
            "mae": 0.5353808785284552,
            "precision": 0.40816326530612246,
            "recall": 0.04184100418410042
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 18313,
        "test": {
            "accuracy": 0.5635964912280702,
            "auc_prc": 0.7834573442025874,
            "auditor_fn_violation": 0.0008661359280554331,
            "auditor_fp_violation": 0.010039433446000329,
            "ave_precision_score": 0.6333003504182606,
            "fpr": 0.4276315789473684,
            "logloss": 11.241718465675877,
            "mae": 0.43665336162411494,
            "precision": 0.5454545454545454,
            "recall": 0.9831932773109243
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.7829594065010225,
            "auditor_fn_violation": 0.0021517574599616953,
            "auditor_fp_violation": 0.010170789148792173,
            "ave_precision_score": 0.6309618513944575,
            "fpr": 0.424807903402854,
            "logloss": 11.089204841288234,
            "mae": 0.42938674851520037,
            "precision": 0.55,
            "recall": 0.9895397489539749
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6376000140734972,
            "auditor_fn_violation": 0.009202694235588976,
            "auditor_fp_violation": 0.010763721229679704,
            "ave_precision_score": 0.6324247106888252,
            "fpr": 0.02850877192982456,
            "logloss": 17.949395834670682,
            "mae": 0.5355458997849533,
            "precision": 0.3953488372093023,
            "recall": 0.03571428571428571
        },
        "train": {
            "accuracy": 0.4588364434687157,
            "auc_prc": 0.6434591090098191,
            "auditor_fn_violation": 0.005828346246940006,
            "auditor_fp_violation": 0.01032289466946203,
            "ave_precision_score": 0.6376211885714365,
            "fpr": 0.03402854006586169,
            "logloss": 18.149198441566234,
            "mae": 0.5422720415782506,
            "precision": 0.3404255319148936,
            "recall": 0.03347280334728033
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6374464391288128,
            "auditor_fn_violation": 0.010117204776647501,
            "auditor_fp_violation": 0.012964248350233383,
            "ave_precision_score": 0.63068809238316,
            "fpr": 0.03179824561403509,
            "logloss": 17.82699613897998,
            "mae": 0.5324284991225379,
            "precision": 0.40816326530612246,
            "recall": 0.04201680672268908
        },
        "train": {
            "accuracy": 0.4610318331503842,
            "auc_prc": 0.6406664814320011,
            "auditor_fn_violation": 0.007633342365968703,
            "auditor_fp_violation": 0.011306510369793871,
            "ave_precision_score": 0.6345924132672917,
            "fpr": 0.03293084522502744,
            "logloss": 18.014140542990983,
            "mae": 0.5396443923015568,
            "precision": 0.3617021276595745,
            "recall": 0.03556485355648536
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6373700255037725,
            "auditor_fn_violation": 0.009914492112634543,
            "auditor_fp_violation": 0.011472919684532434,
            "ave_precision_score": 0.6313488117272084,
            "fpr": 0.029605263157894735,
            "logloss": 17.939363028512332,
            "mae": 0.5344151838933947,
            "precision": 0.4,
            "recall": 0.037815126050420166
        },
        "train": {
            "accuracy": 0.4610318331503842,
            "auc_prc": 0.6359388199542593,
            "auditor_fn_violation": 0.005828346246940006,
            "auditor_fp_violation": 0.011587905583033138,
            "ave_precision_score": 0.6301058594443723,
            "fpr": 0.031833150384193196,
            "logloss": 18.117335333605205,
            "mae": 0.5420765858336074,
            "precision": 0.35555555555555557,
            "recall": 0.03347280334728033
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 18313,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.6518549555959945,
            "auditor_fn_violation": 0.0007739938080495393,
            "auditor_fp_violation": 0.022332206663447617,
            "ave_precision_score": 0.6403129721279951,
            "fpr": 0.22478070175438597,
            "logloss": 2.109991701686306,
            "mae": 0.3404054813509007,
            "precision": 0.652542372881356,
            "recall": 0.8088235294117647
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.6561043059827443,
            "auditor_fn_violation": 0.00893312328628708,
            "auditor_fp_violation": 0.03432007564714561,
            "ave_precision_score": 0.6435281897633525,
            "fpr": 0.22063666300768386,
            "logloss": 1.9876363577299072,
            "mae": 0.34164516836992803,
            "precision": 0.660472972972973,
            "recall": 0.8179916317991632
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.6468196615413322,
            "auditor_fn_violation": 0.010752985404688211,
            "auditor_fp_violation": 0.015157230806373732,
            "ave_precision_score": 0.6395028268101628,
            "fpr": 0.03179824561403509,
            "logloss": 10.394188567681818,
            "mae": 0.5268364418861335,
            "precision": 0.46296296296296297,
            "recall": 0.052521008403361345
        },
        "train": {
            "accuracy": 0.4676180021953897,
            "auc_prc": 0.6516425601396129,
            "auditor_fn_violation": 0.011877150035135434,
            "auditor_fp_violation": 0.012556310731297995,
            "ave_precision_score": 0.6431170733029464,
            "fpr": 0.03293084522502744,
            "logloss": 10.413407450882023,
            "mae": 0.5302998294294476,
            "precision": 0.4339622641509434,
            "recall": 0.04811715481171548
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4692982456140351,
            "auc_prc": 0.6439380056212396,
            "auditor_fn_violation": 0.009269497272593248,
            "auditor_fp_violation": 0.009345324319974249,
            "ave_precision_score": 0.6399653617953995,
            "fpr": 0.02631578947368421,
            "logloss": 17.89810439679758,
            "mae": 0.5309999469787726,
            "precision": 0.4,
            "recall": 0.03361344537815126
        },
        "train": {
            "accuracy": 0.4643249176728869,
            "auc_prc": 0.6458970581169905,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.009851367555385423,
            "ave_precision_score": 0.6405308992008361,
            "fpr": 0.027442371020856202,
            "logloss": 18.154484916065105,
            "mae": 0.5372592925446367,
            "precision": 0.375,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.6482396442881488,
            "auditor_fn_violation": 0.0116905314757482,
            "auditor_fp_violation": 0.015484166264284565,
            "ave_precision_score": 0.6401568681762716,
            "fpr": 0.03399122807017544,
            "logloss": 8.439465272439923,
            "mae": 0.5255750244559869,
            "precision": 0.46551724137931033,
            "recall": 0.05672268907563025
        },
        "train": {
            "accuracy": 0.47200878155872666,
            "auc_prc": 0.6508574714441507,
            "auditor_fn_violation": 0.014504269068429112,
            "auditor_fp_violation": 0.013225575022245433,
            "ave_precision_score": 0.6423346527500645,
            "fpr": 0.03402854006586169,
            "logloss": 8.451474106612707,
            "mae": 0.5272455822661936,
            "precision": 0.4745762711864407,
            "recall": 0.058577405857740586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 18313,
        "test": {
            "accuracy": 0.46271929824561403,
            "auc_prc": 0.6336663493123526,
            "auditor_fn_violation": 0.009914492112634543,
            "auditor_fp_violation": 0.009697408659262838,
            "ave_precision_score": 0.6278944813553623,
            "fpr": 0.03508771929824561,
            "logloss": 17.880990767667292,
            "mae": 0.5361122163339029,
            "precision": 0.36,
            "recall": 0.037815126050420166
        },
        "train": {
            "accuracy": 0.4610318331503842,
            "auc_prc": 0.6343320526304492,
            "auditor_fn_violation": 0.006627504833990879,
            "auditor_fp_violation": 0.011369887670072987,
            "ave_precision_score": 0.6283758975241273,
            "fpr": 0.03512623490669594,
            "logloss": 18.070441320332773,
            "mae": 0.5413290945757235,
            "precision": 0.37254901960784315,
            "recall": 0.0397489539748954
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4649122807017544,
            "auc_prc": 0.6317389870517744,
            "auditor_fn_violation": 0.019534129441250207,
            "auditor_fp_violation": 0.02481691614356994,
            "ave_precision_score": 0.613047988101186,
            "fpr": 0.05701754385964912,
            "logloss": 18.014578916667208,
            "mae": 0.536297365826178,
            "precision": 0.43478260869565216,
            "recall": 0.08403361344537816
        },
        "train": {
            "accuracy": 0.45773874862788144,
            "auc_prc": 0.6249164637804608,
            "auditor_fn_violation": 0.01747585300993437,
            "auditor_fp_violation": 0.02125674651361471,
            "ave_precision_score": 0.6092543899286536,
            "fpr": 0.05378704720087816,
            "logloss": 18.08009426250011,
            "mae": 0.5447851164882177,
            "precision": 0.4024390243902439,
            "recall": 0.06903765690376569
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4692982456140351,
            "auc_prc": 0.6732552732960568,
            "auditor_fn_violation": 0.0018635743771192774,
            "auditor_fp_violation": 0.002381599066473524,
            "ave_precision_score": 0.6747751510040603,
            "fpr": 0.01425438596491228,
            "logloss": 17.73151764554955,
            "mae": 0.5296581878512816,
            "precision": 0.2777777777777778,
            "recall": 0.01050420168067227
        },
        "train": {
            "accuracy": 0.47530186608122943,
            "auc_prc": 0.688765398476982,
            "auditor_fn_violation": 0.0009415374157783282,
            "auditor_fp_violation": 0.003531383171552212,
            "ave_precision_score": 0.6890542099364675,
            "fpr": 0.009879253567508232,
            "logloss": 17.788477785061996,
            "mae": 0.5276149868682453,
            "precision": 0.5,
            "recall": 0.01882845188284519
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.6305862522717722,
            "auditor_fn_violation": 0.008550788736547259,
            "auditor_fp_violation": 0.009405681635280863,
            "ave_precision_score": 0.6245023643486433,
            "fpr": 0.02631578947368421,
            "logloss": 17.72131465899154,
            "mae": 0.5288011332785758,
            "precision": 0.42857142857142855,
            "recall": 0.037815126050420166
        },
        "train": {
            "accuracy": 0.4665203073545554,
            "auc_prc": 0.6537157275043148,
            "auditor_fn_violation": 0.008848155275594898,
            "auditor_fp_violation": 0.009661235654548081,
            "ave_precision_score": 0.6473186528732863,
            "fpr": 0.027442371020856202,
            "logloss": 17.867818100523376,
            "mae": 0.5339681390370745,
            "precision": 0.40476190476190477,
            "recall": 0.03556485355648536
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 18313,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.6955170469695943,
            "auditor_fn_violation": 0.006625018428424002,
            "auditor_fp_violation": 0.022271849348141,
            "ave_precision_score": 0.6852256652773957,
            "fpr": 0.16666666666666666,
            "logloss": 1.8171006734038793,
            "mae": 0.2986444686201841,
            "precision": 0.7142857142857143,
            "recall": 0.7983193277310925
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7099414764807017,
            "auditor_fn_violation": 0.007637935231411528,
            "auditor_fp_violation": 0.027432230652811543,
            "ave_precision_score": 0.6988685804964465,
            "fpr": 0.16465422612513722,
            "logloss": 1.6714613674233392,
            "mae": 0.30081782715894384,
            "precision": 0.7137404580152672,
            "recall": 0.7824267782426778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 18313,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.7905581974478728,
            "auditor_fn_violation": 0.0013061145510835917,
            "auditor_fp_violation": 0.00939310719459199,
            "ave_precision_score": 0.6467268989753836,
            "fpr": 0.4375,
            "logloss": 10.721725581762488,
            "mae": 0.444968572351921,
            "precision": 0.5403225806451613,
            "recall": 0.9852941176470589
        },
        "train": {
            "accuracy": 0.5598243688254665,
            "auc_prc": 0.8029228278994783,
            "auditor_fn_violation": 0.0018945569951637127,
            "auditor_fp_violation": 0.009286042036895745,
            "ave_precision_score": 0.6597206655535635,
            "fpr": 0.43688254665203075,
            "logloss": 10.51068852179906,
            "mae": 0.440728800894068,
            "precision": 0.5441008018327605,
            "recall": 0.9937238493723849
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.6372914894925765,
            "auditor_fn_violation": 0.009914492112634543,
            "auditor_fp_violation": 0.011151014002897152,
            "ave_precision_score": 0.6309473852822838,
            "fpr": 0.027412280701754384,
            "logloss": 17.91867254062039,
            "mae": 0.5318016938542587,
            "precision": 0.4186046511627907,
            "recall": 0.037815126050420166
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6398060969094614,
            "auditor_fn_violation": 0.007633342365968703,
            "auditor_fp_violation": 0.010776676139460484,
            "ave_precision_score": 0.6338449820472205,
            "fpr": 0.030735455543358946,
            "logloss": 18.09931788579719,
            "mae": 0.5384540800207618,
            "precision": 0.37777777777777777,
            "recall": 0.03556485355648536
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.641421405714768,
            "auditor_fn_violation": 0.009269497272593248,
            "auditor_fp_violation": 0.011151014002897152,
            "ave_precision_score": 0.6363861977953609,
            "fpr": 0.027412280701754384,
            "logloss": 17.903291959886833,
            "mae": 0.5323479468270522,
            "precision": 0.3902439024390244,
            "recall": 0.03361344537815126
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6439854979041526,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.01020121025292613,
            "ave_precision_score": 0.6381469037674176,
            "fpr": 0.02854006586169045,
            "logloss": 18.11811359218733,
            "mae": 0.538145858627512,
            "precision": 0.36585365853658536,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 18313,
        "test": {
            "accuracy": 0.45614035087719296,
            "auc_prc": 0.6106948117946961,
            "auditor_fn_violation": 0.02277292495945747,
            "auditor_fp_violation": 0.03320155319491389,
            "ave_precision_score": 0.5934244584251923,
            "fpr": 0.0581140350877193,
            "logloss": 12.76044522803857,
            "mae": 0.543948582007104,
            "precision": 0.38372093023255816,
            "recall": 0.06932773109243698
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6082556319244067,
            "auditor_fn_violation": 0.02046580841321093,
            "auditor_fp_violation": 0.028291626844596327,
            "ave_precision_score": 0.5921015325970194,
            "fpr": 0.048298572996706916,
            "logloss": 12.459378037697599,
            "mae": 0.5373559798074029,
            "precision": 0.42857142857142855,
            "recall": 0.06903765690376569
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4692982456140351,
            "auc_prc": 0.6418384977290695,
            "auditor_fn_violation": 0.009914492112634543,
            "auditor_fp_violation": 0.010763721229679704,
            "ave_precision_score": 0.6363833696219556,
            "fpr": 0.02850877192982456,
            "logloss": 17.87333009476284,
            "mae": 0.5319030682148979,
            "precision": 0.4090909090909091,
            "recall": 0.037815126050420166
        },
        "train": {
            "accuracy": 0.4610318331503842,
            "auc_prc": 0.6441065194710938,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.010776676139460484,
            "ave_precision_score": 0.6382686775275083,
            "fpr": 0.030735455543358946,
            "logloss": 18.089450771494793,
            "mae": 0.5396991305769461,
            "precision": 0.3488372093023256,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.6358035235888934,
            "auditor_fn_violation": 0.010177097154651345,
            "auditor_fp_violation": 0.014060739578303559,
            "ave_precision_score": 0.6283182593378533,
            "fpr": 0.03179824561403509,
            "logloss": 15.59257490404389,
            "mae": 0.5285060578052277,
            "precision": 0.46296296296296297,
            "recall": 0.052521008403361345
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6431440389161663,
            "auditor_fn_violation": 0.009222473809184834,
            "auditor_fp_violation": 0.012556310731297995,
            "ave_precision_score": 0.6360006448174133,
            "fpr": 0.03293084522502744,
            "logloss": 15.678841138768687,
            "mae": 0.5371735654780672,
            "precision": 0.3877551020408163,
            "recall": 0.0397489539748954
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 18313,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.6729162780192208,
            "auditor_fn_violation": 0.017672858617131065,
            "auditor_fp_violation": 0.014787542250120718,
            "ave_precision_score": 0.6601964410872495,
            "fpr": 0.17653508771929824,
            "logloss": 2.8942916323055425,
            "mae": 0.32145500511512837,
            "precision": 0.6973684210526315,
            "recall": 0.7794117647058824
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7090704488820592,
            "auditor_fn_violation": 0.0009622053102710259,
            "auditor_fp_violation": 0.009187173448460314,
            "ave_precision_score": 0.6966315888120203,
            "fpr": 0.16465422612513722,
            "logloss": 2.360853138248181,
            "mae": 0.3014182259741471,
            "precision": 0.7175141242937854,
            "recall": 0.797071129707113
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.7519589682817505,
            "auditor_fn_violation": 0.0013867389060887693,
            "auditor_fp_violation": 0.0022533397714469662,
            "ave_precision_score": 0.7529196035320594,
            "fpr": 0.008771929824561403,
            "logloss": 17.753145152830808,
            "mae": 0.5256617638065986,
            "precision": 0.42857142857142855,
            "recall": 0.012605042016806723
        },
        "train": {
            "accuracy": 0.47420417124039516,
            "auc_prc": 0.7424070856950465,
            "auditor_fn_violation": 0.002560522484372787,
            "auditor_fp_violation": 0.003026899861330467,
            "ave_precision_score": 0.7435247492047323,
            "fpr": 0.006586169045005488,
            "logloss": 17.881890194714426,
            "mae": 0.528745528206262,
            "precision": 0.45454545454545453,
            "recall": 0.010460251046025104
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 18313,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7503675868770809,
            "auditor_fn_violation": 0.016705366357069148,
            "auditor_fp_violation": 0.023788326895219702,
            "ave_precision_score": 0.7511848502421073,
            "fpr": 0.1875,
            "logloss": 1.5669602272282341,
            "mae": 0.3172704603203264,
            "precision": 0.6850828729281768,
            "recall": 0.7815126050420168
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7524275145826105,
            "auditor_fn_violation": 0.014061057553196867,
            "auditor_fp_violation": 0.029320874201129138,
            "ave_precision_score": 0.7529206234312227,
            "fpr": 0.1942919868276619,
            "logloss": 1.4792814525759244,
            "mae": 0.31407570051012956,
            "precision": 0.6839285714285714,
            "recall": 0.801255230125523
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.7416239514777331,
            "auditor_fn_violation": 0.0013867389060887693,
            "auditor_fp_violation": 1.5089328826654156e-05,
            "ave_precision_score": 0.7433902692288062,
            "fpr": 0.006578947368421052,
            "logloss": 17.757820014154177,
            "mae": 0.524640548545377,
            "precision": 0.5,
            "recall": 0.012605042016806723
        },
        "train": {
            "accuracy": 0.47310647639956094,
            "auc_prc": 0.7328999362610815,
            "auditor_fn_violation": 0.002560522484372787,
            "auditor_fp_violation": 0.0034426549511614524,
            "ave_precision_score": 0.7340271858117509,
            "fpr": 0.007683863885839737,
            "logloss": 17.885596193703474,
            "mae": 0.5282114435331824,
            "precision": 0.4166666666666667,
            "recall": 0.010460251046025104
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 18313,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.7019680034385269,
            "auditor_fn_violation": 0.010246203744655754,
            "auditor_fp_violation": 0.024927571221632073,
            "ave_precision_score": 0.6919994220596593,
            "fpr": 0.16666666666666666,
            "logloss": 1.7815506275791457,
            "mae": 0.2963630389755979,
            "precision": 0.7153558052434457,
            "recall": 0.8025210084033614
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7123328715968636,
            "auditor_fn_violation": 0.011192813084155073,
            "auditor_fp_violation": 0.027323221696331473,
            "ave_precision_score": 0.7024662162222707,
            "fpr": 0.16136114160263446,
            "logloss": 1.6211840998205025,
            "mae": 0.2986075505516842,
            "precision": 0.7183908045977011,
            "recall": 0.7845188284518828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6322134337527463,
            "auditor_fn_violation": 0.010117204776647501,
            "auditor_fp_violation": 0.012964248350233383,
            "ave_precision_score": 0.6258694657972349,
            "fpr": 0.03179824561403509,
            "logloss": 17.78364448516368,
            "mae": 0.5323241085473965,
            "precision": 0.40816326530612246,
            "recall": 0.04201680672268908
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.642569846510639,
            "auditor_fn_violation": 0.009047944922357614,
            "auditor_fp_violation": 0.011306510369793871,
            "ave_precision_score": 0.6360177424794791,
            "fpr": 0.03293084522502744,
            "logloss": 17.97151969657894,
            "mae": 0.5387552937500522,
            "precision": 0.3877551020408163,
            "recall": 0.0397489539748954
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.6425969530981104,
            "auditor_fn_violation": 0.009914492112634543,
            "auditor_fp_violation": 0.011151014002897152,
            "ave_precision_score": 0.6371381194958454,
            "fpr": 0.027412280701754384,
            "logloss": 17.872599632811596,
            "mae": 0.5316791928930231,
            "precision": 0.4186046511627907,
            "recall": 0.037815126050420166
        },
        "train": {
            "accuracy": 0.4610318331503842,
            "auc_prc": 0.6444380870028465,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.010776676139460484,
            "ave_precision_score": 0.6385992907797395,
            "fpr": 0.030735455543358946,
            "logloss": 18.090473579428263,
            "mae": 0.539452709499902,
            "precision": 0.3488372093023256,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 18313,
        "test": {
            "accuracy": 0.5668859649122807,
            "auc_prc": 0.7920447357249705,
            "auditor_fn_violation": 0.0008661359280554331,
            "auditor_fp_violation": 0.007730766135522315,
            "ave_precision_score": 0.758087497305266,
            "fpr": 0.4243421052631579,
            "logloss": 5.162024492527011,
            "mae": 0.4308929247667429,
            "precision": 0.5473684210526316,
            "recall": 0.9831932773109243
        },
        "train": {
            "accuracy": 0.5729967069154775,
            "auc_prc": 0.8068191621685871,
            "auditor_fn_violation": 0.0018784819661138389,
            "auditor_fp_violation": 0.01188451134833939,
            "ave_precision_score": 0.7722381270972484,
            "fpr": 0.42041712403951703,
            "logloss": 4.855094467393353,
            "mae": 0.4215319188514525,
            "precision": 0.552046783625731,
            "recall": 0.9874476987447699
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 18313,
        "test": {
            "accuracy": 0.46710526315789475,
            "auc_prc": 0.6288574788282544,
            "auditor_fn_violation": 0.008550788736547259,
            "auditor_fp_violation": 0.01005955255110253,
            "ave_precision_score": 0.6222815243863868,
            "fpr": 0.03070175438596491,
            "logloss": 17.80479974385365,
            "mae": 0.53356094427929,
            "precision": 0.391304347826087,
            "recall": 0.037815126050420166
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6543260636365853,
            "auditor_fn_violation": 0.009309738252598421,
            "auditor_fp_violation": 0.012480257970963058,
            "ave_precision_score": 0.6473774863870005,
            "fpr": 0.03293084522502744,
            "logloss": 17.881253877414114,
            "mae": 0.53699781799002,
            "precision": 0.3877551020408163,
            "recall": 0.0397489539748954
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4649122807017544,
            "auc_prc": 0.6252898852961697,
            "auditor_fn_violation": 0.009403103346601811,
            "auditor_fp_violation": 0.012272654112345086,
            "ave_precision_score": 0.6193615149555187,
            "fpr": 0.03399122807017544,
            "logloss": 17.885679987047002,
            "mae": 0.5367001738362642,
            "precision": 0.38,
            "recall": 0.03991596638655462
        },
        "train": {
            "accuracy": 0.45773874862788144,
            "auc_prc": 0.6300065127204578,
            "auditor_fn_violation": 0.008588658378075513,
            "auditor_fp_violation": 0.014363831335258315,
            "ave_precision_score": 0.6239037636681795,
            "fpr": 0.03732162458836443,
            "logloss": 18.037606758288987,
            "mae": 0.5430392522901409,
            "precision": 0.34615384615384615,
            "recall": 0.03765690376569038
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.6427117270885223,
            "auditor_fn_violation": 0.009914492112634543,
            "auditor_fp_violation": 0.011151014002897152,
            "ave_precision_score": 0.637252609782299,
            "fpr": 0.027412280701754384,
            "logloss": 17.866547232793046,
            "mae": 0.5315741575361087,
            "precision": 0.4186046511627907,
            "recall": 0.037815126050420166
        },
        "train": {
            "accuracy": 0.4610318331503842,
            "auc_prc": 0.6444831613360387,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.010776676139460484,
            "ave_precision_score": 0.6386452113909735,
            "fpr": 0.030735455543358946,
            "logloss": 18.085343475255456,
            "mae": 0.5393063740504631,
            "precision": 0.3488372093023256,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 18313,
        "test": {
            "accuracy": 0.46381578947368424,
            "auc_prc": 0.6486717880693952,
            "auditor_fn_violation": 0.003298687896211117,
            "auditor_fp_violation": 0.008289071302108483,
            "ave_precision_score": 0.6463363141109144,
            "fpr": 0.025219298245614034,
            "logloss": 18.119153819202673,
            "mae": 0.5381414338044777,
            "precision": 0.30303030303030304,
            "recall": 0.02100840336134454
        },
        "train": {
            "accuracy": 0.4588364434687157,
            "auc_prc": 0.6195276153764264,
            "auditor_fn_violation": 0.0020644930165480916,
            "auditor_fp_violation": 0.0105966846066678,
            "ave_precision_score": 0.61647991810425,
            "fpr": 0.02854006586169045,
            "logloss": 18.279379516113924,
            "mae": 0.5446514031863933,
            "precision": 0.2972972972972973,
            "recall": 0.02301255230125523
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 18313,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.6830155302893166,
            "auditor_fn_violation": 0.009426138876603277,
            "auditor_fp_violation": 0.021001830838564294,
            "ave_precision_score": 0.6724415458101611,
            "fpr": 0.2138157894736842,
            "logloss": 1.9029184456720414,
            "mae": 0.32143707025649126,
            "precision": 0.6728187919463087,
            "recall": 0.842436974789916
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7002071118924152,
            "auditor_fn_violation": 0.007224577341557626,
            "auditor_fp_violation": 0.02613933372711763,
            "ave_precision_score": 0.6885987799152611,
            "fpr": 0.20417124039517015,
            "logloss": 1.7290907621609046,
            "mae": 0.3172559923708092,
            "precision": 0.6782006920415224,
            "recall": 0.8200836820083682
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 18313,
        "test": {
            "accuracy": 0.48464912280701755,
            "auc_prc": 0.6516462013509097,
            "auditor_fn_violation": 0.0080578283945157,
            "auditor_fp_violation": 0.011538306776114601,
            "ave_precision_score": 0.6489404255125139,
            "fpr": 0.02631578947368421,
            "logloss": 5.002721973564335,
            "mae": 0.512072211296577,
            "precision": 0.5555555555555556,
            "recall": 0.06302521008403361
        },
        "train": {
            "accuracy": 0.48737650933040616,
            "auc_prc": 0.6499768461552045,
            "auditor_fn_violation": 0.01415521129477471,
            "auditor_fp_violation": 0.011856625336216578,
            "ave_precision_score": 0.6452043525906039,
            "fpr": 0.027442371020856202,
            "logloss": 4.9229896032979354,
            "mae": 0.5103087814775452,
            "precision": 0.5901639344262295,
            "recall": 0.07531380753138076
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.6205719935293332,
            "auditor_fn_violation": 0.009605816010614783,
            "auditor_fp_violation": 0.012634798004184775,
            "ave_precision_score": 0.6143971393271301,
            "fpr": 0.02631578947368421,
            "logloss": 17.786180986595966,
            "mae": 0.5291109718122862,
            "precision": 0.42857142857142855,
            "recall": 0.037815126050420166
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6294163620853402,
            "auditor_fn_violation": 0.007633342365968703,
            "auditor_fp_violation": 0.010989623868398305,
            "ave_precision_score": 0.6234628845653022,
            "fpr": 0.030735455543358946,
            "logloss": 17.98309452106405,
            "mae": 0.5368509720066457,
            "precision": 0.37777777777777777,
            "recall": 0.03556485355648536
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.6499567113367424,
            "auditor_fn_violation": 0.003902218782249754,
            "auditor_fp_violation": 0.00704168678577177,
            "ave_precision_score": 0.6503562477993736,
            "fpr": 0.017543859649122806,
            "logloss": 6.063522540498126,
            "mae": 0.5279516382215439,
            "precision": 0.36,
            "recall": 0.018907563025210083
        },
        "train": {
            "accuracy": 0.4665203073545554,
            "auc_prc": 0.645023322134688,
            "auditor_fn_violation": 0.003931492819054877,
            "auditor_fp_violation": 0.006380826592101161,
            "ave_precision_score": 0.6433832236717776,
            "fpr": 0.020856201975850714,
            "logloss": 5.9561972914349495,
            "mae": 0.527900148001673,
            "precision": 0.36666666666666664,
            "recall": 0.02301255230125523
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4725877192982456,
            "auc_prc": 0.5983835755326907,
            "auditor_fn_violation": 0.013360607400855089,
            "auditor_fp_violation": 0.016371921776919367,
            "ave_precision_score": 0.588719792930744,
            "fpr": 0.04057017543859649,
            "logloss": 6.422842237354099,
            "mae": 0.5294941010393355,
            "precision": 0.463768115942029,
            "recall": 0.06722689075630252
        },
        "train": {
            "accuracy": 0.4676180021953897,
            "auc_prc": 0.6071343649583598,
            "auditor_fn_violation": 0.013810746386563103,
            "auditor_fp_violation": 0.012931504348950346,
            "ave_precision_score": 0.5984717077510886,
            "fpr": 0.04061470911086718,
            "logloss": 6.374357347188201,
            "mae": 0.5328384358444096,
            "precision": 0.44776119402985076,
            "recall": 0.06276150627615062
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4605263157894737,
            "auc_prc": 0.38722397498731165,
            "auditor_fn_violation": 0.002377266696152161,
            "auditor_fp_violation": 0.006951150812811847,
            "ave_precision_score": 0.5219411139068629,
            "fpr": 0.02631578947368421,
            "logloss": 17.80729747521947,
            "mae": 0.5368413113449749,
            "precision": 0.25,
            "recall": 0.01680672268907563
        },
        "train": {
            "accuracy": 0.4566410537870472,
            "auc_prc": 0.3788765507581282,
            "auditor_fn_violation": 0.002101235940090671,
            "auditor_fp_violation": 0.0069208011904792095,
            "ave_precision_score": 0.5264267304360386,
            "fpr": 0.026344676180021953,
            "logloss": 17.938780814552707,
            "mae": 0.5404736116348585,
            "precision": 0.22580645161290322,
            "recall": 0.014644351464435146
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.64269364505372,
            "auditor_fn_violation": 0.009914492112634543,
            "auditor_fp_violation": 0.011151014002897152,
            "ave_precision_score": 0.6372344281732677,
            "fpr": 0.027412280701754384,
            "logloss": 17.86729391562764,
            "mae": 0.5315821708529891,
            "precision": 0.4186046511627907,
            "recall": 0.037815126050420166
        },
        "train": {
            "accuracy": 0.4610318331503842,
            "auc_prc": 0.6444633312453885,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.010776676139460484,
            "ave_precision_score": 0.6386252435958164,
            "fpr": 0.030735455543358946,
            "logloss": 18.08594558427815,
            "mae": 0.539326142991004,
            "precision": 0.3488372093023256,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6388882791873599,
            "auditor_fn_violation": 0.010117204776647501,
            "auditor_fp_violation": 0.012964248350233383,
            "ave_precision_score": 0.6321319603092838,
            "fpr": 0.03179824561403509,
            "logloss": 17.797872362746045,
            "mae": 0.5321870157769477,
            "precision": 0.40816326530612246,
            "recall": 0.04201680672268908
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6384433239283243,
            "auditor_fn_violation": 0.009438338484997427,
            "auditor_fp_violation": 0.012176046929623312,
            "ave_precision_score": 0.6318944283257129,
            "fpr": 0.031833150384193196,
            "logloss": 17.98615255853212,
            "mae": 0.5385397822237497,
            "precision": 0.3829787234042553,
            "recall": 0.03765690376569038
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 18313,
        "test": {
            "accuracy": 0.5625,
            "auc_prc": 0.7003307908847219,
            "auditor_fn_violation": 0.0008661359280554331,
            "auditor_fp_violation": 0.010381458232737811,
            "ave_precision_score": 0.6815998468082046,
            "fpr": 0.42872807017543857,
            "logloss": 5.803199341048332,
            "mae": 0.4370420276347687,
            "precision": 0.5448195576251456,
            "recall": 0.9831932773109243
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.7195153562346617,
            "auditor_fn_violation": 0.0021517574599616953,
            "auditor_fp_violation": 0.008888032591142916,
            "ave_precision_score": 0.7051058842532278,
            "fpr": 0.424807903402854,
            "logloss": 5.398184401356481,
            "mae": 0.4289789532277129,
            "precision": 0.55,
            "recall": 0.9895397489539749
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6333037234808514,
            "auditor_fn_violation": 0.010117204776647501,
            "auditor_fp_violation": 0.012964248350233383,
            "ave_precision_score": 0.6265512522760894,
            "fpr": 0.03179824561403509,
            "logloss": 17.827042480322245,
            "mae": 0.5322440200561601,
            "precision": 0.40816326530612246,
            "recall": 0.04201680672268908
        },
        "train": {
            "accuracy": 0.4621295279912184,
            "auc_prc": 0.6329821146623316,
            "auditor_fn_violation": 0.007633342365968703,
            "auditor_fp_violation": 0.012176046929623312,
            "ave_precision_score": 0.6269100157246943,
            "fpr": 0.031833150384193196,
            "logloss": 18.013898097982267,
            "mae": 0.5393342525524523,
            "precision": 0.3695652173913043,
            "recall": 0.03556485355648536
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.64269364505372,
            "auditor_fn_violation": 0.009914492112634543,
            "auditor_fp_violation": 0.011151014002897152,
            "ave_precision_score": 0.6372344281732677,
            "fpr": 0.027412280701754384,
            "logloss": 17.867302837933277,
            "mae": 0.5315822601893125,
            "precision": 0.4186046511627907,
            "recall": 0.037815126050420166
        },
        "train": {
            "accuracy": 0.4610318331503842,
            "auc_prc": 0.6444633312453885,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.010776676139460484,
            "ave_precision_score": 0.6386252435958164,
            "fpr": 0.030735455543358946,
            "logloss": 18.085954470860326,
            "mae": 0.5393263441455711,
            "precision": 0.3488372093023256,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.7536294532858354,
            "auditor_fn_violation": 0.0013867389060887693,
            "auditor_fp_violation": 0.0022533397714469662,
            "ave_precision_score": 0.7545884130671283,
            "fpr": 0.008771929824561403,
            "logloss": 17.752439440031342,
            "mae": 0.5256048661387084,
            "precision": 0.42857142857142855,
            "recall": 0.012605042016806723
        },
        "train": {
            "accuracy": 0.47310647639956094,
            "auc_prc": 0.743255456681997,
            "auditor_fn_violation": 0.0018371461771284534,
            "auditor_fp_violation": 0.003026899861330467,
            "ave_precision_score": 0.7442927499900169,
            "fpr": 0.006586169045005488,
            "logloss": 17.881436990793553,
            "mae": 0.5285974982062724,
            "precision": 0.4,
            "recall": 0.008368200836820083
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.6448003056000102,
            "auditor_fn_violation": 0.012383900928792569,
            "auditor_fp_violation": 0.014714610494125223,
            "ave_precision_score": 0.6374876584069283,
            "fpr": 0.03618421052631579,
            "logloss": 5.694210152682168,
            "mae": 0.5271452168969449,
            "precision": 0.47619047619047616,
            "recall": 0.06302521008403361
        },
        "train": {
            "accuracy": 0.4698133918770582,
            "auc_prc": 0.6621908364327984,
            "auditor_fn_violation": 0.014504269068429112,
            "auditor_fp_violation": 0.014046944833862743,
            "ave_precision_score": 0.6536599525453883,
            "fpr": 0.036223929747530186,
            "logloss": 5.634454599473145,
            "mae": 0.5309121866479957,
            "precision": 0.45901639344262296,
            "recall": 0.058577405857740586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4769736842105263,
            "auc_prc": 0.6916005757468074,
            "auditor_fn_violation": 0.0013867389060887693,
            "auditor_fp_violation": 0.0027060196362465798,
            "ave_precision_score": 0.6934261890030057,
            "fpr": 0.007675438596491228,
            "logloss": 17.74481483558611,
            "mae": 0.5246430372885991,
            "precision": 0.46153846153846156,
            "recall": 0.012605042016806723
        },
        "train": {
            "accuracy": 0.4698133918770582,
            "auc_prc": 0.702988541732085,
            "auditor_fn_violation": 0.0008496801069219,
            "auditor_fp_violation": 0.0038761556850705897,
            "ave_precision_score": 0.7043376750081354,
            "fpr": 0.009879253567508232,
            "logloss": 17.810353926027854,
            "mae": 0.5302916820136612,
            "precision": 0.3076923076923077,
            "recall": 0.008368200836820083
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.6328433983208877,
            "auditor_fn_violation": 0.009926009877635266,
            "auditor_fp_violation": 0.009732617093191695,
            "ave_precision_score": 0.6258391753920765,
            "fpr": 0.025219298245614034,
            "logloss": 17.67561166406259,
            "mae": 0.527793642360399,
            "precision": 0.4523809523809524,
            "recall": 0.03991596638655462
        },
        "train": {
            "accuracy": 0.4676180021953897,
            "auc_prc": 0.6536188008885289,
            "auditor_fn_violation": 0.009438338484997427,
            "auditor_fp_violation": 0.009661235654548081,
            "ave_precision_score": 0.6466479040621995,
            "fpr": 0.027442371020856202,
            "logloss": 17.810655229979904,
            "mae": 0.5320740124313992,
            "precision": 0.4186046511627907,
            "recall": 0.03765690376569038
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4692982456140351,
            "auc_prc": 0.6404951139316686,
            "auditor_fn_violation": 0.009320175438596503,
            "auditor_fp_violation": 0.011151014002897152,
            "ave_precision_score": 0.635041944387621,
            "fpr": 0.027412280701754384,
            "logloss": 17.889811104928796,
            "mae": 0.5317926407797451,
            "precision": 0.40476190476190477,
            "recall": 0.03571428571428571
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6410652431983322,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.01020121025292613,
            "ave_precision_score": 0.6352283253363401,
            "fpr": 0.02854006586169045,
            "logloss": 18.101367568845653,
            "mae": 0.5381555550816705,
            "precision": 0.36585365853658536,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 18313,
        "test": {
            "accuracy": 0.46381578947368424,
            "auc_prc": 0.5477638226152302,
            "auditor_fn_violation": 0.007822865988500693,
            "auditor_fp_violation": 0.008065246257846452,
            "ave_precision_score": 0.5383644317581452,
            "fpr": 0.04057017543859649,
            "logloss": 18.02118692749056,
            "mae": 0.5371562526209771,
            "precision": 0.39344262295081966,
            "recall": 0.05042016806722689
        },
        "train": {
            "accuracy": 0.44017563117453345,
            "auc_prc": 0.5445955630155257,
            "auditor_fn_violation": 0.005166973623173768,
            "auditor_fp_violation": 0.011993520304819465,
            "ave_precision_score": 0.5344540463260506,
            "fpr": 0.05817782656421515,
            "logloss": 18.81131927704575,
            "mae": 0.5634630995132788,
            "precision": 0.28378378378378377,
            "recall": 0.043933054393305436
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 18313,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.6647222798066559,
            "auditor_fn_violation": 0.01049959457467198,
            "auditor_fp_violation": 0.028156687590535977,
            "ave_precision_score": 0.6539433153278116,
            "fpr": 0.10964912280701754,
            "logloss": 2.0696711693970467,
            "mae": 0.3602479932782835,
            "precision": 0.732620320855615,
            "recall": 0.5756302521008403
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.6721672408424023,
            "auditor_fn_violation": 0.014890069765626084,
            "auditor_fp_violation": 0.020006946152110593,
            "ave_precision_score": 0.6603276065173297,
            "fpr": 0.11306256860592755,
            "logloss": 1.9345662272872917,
            "mae": 0.36585909993307875,
            "precision": 0.7162534435261708,
            "recall": 0.5439330543933054
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.6540164982034957,
            "auditor_fn_violation": 0.013054234851835479,
            "auditor_fp_violation": 0.016309049573474975,
            "ave_precision_score": 0.6448873876626914,
            "fpr": 0.03618421052631579,
            "logloss": 6.188374100288013,
            "mae": 0.5239772816104394,
            "precision": 0.484375,
            "recall": 0.06512605042016807
        },
        "train": {
            "accuracy": 0.4698133918770582,
            "auc_prc": 0.6573574884205166,
            "auditor_fn_violation": 0.014325147316159087,
            "auditor_fp_violation": 0.01427763820687872,
            "ave_precision_score": 0.6488324549893953,
            "fpr": 0.03732162458836443,
            "logloss": 6.077279660045268,
            "mae": 0.5274312540579837,
            "precision": 0.4603174603174603,
            "recall": 0.060669456066945605
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 18313,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.6918160903851407,
            "auditor_fn_violation": 0.009522888102609466,
            "auditor_fp_violation": 0.017400511025269592,
            "ave_precision_score": 0.6804422012173857,
            "fpr": 0.20065789473684212,
            "logloss": 1.9831185512530625,
            "mae": 0.30312634227767266,
            "precision": 0.6822916666666666,
            "recall": 0.8256302521008403
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7054734088725829,
            "auditor_fn_violation": 0.007842317743617069,
            "auditor_fp_violation": 0.029343690029229617,
            "ave_precision_score": 0.6933213564316324,
            "fpr": 0.18880351262349068,
            "logloss": 1.8191829393064893,
            "mae": 0.3000437300034887,
            "precision": 0.6977152899824253,
            "recall": 0.8305439330543933
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4967105263157895,
            "auc_prc": 0.6290213940480356,
            "auditor_fn_violation": 0.005666740380362683,
            "auditor_fp_violation": 0.02368773136970868,
            "ave_precision_score": 0.5142049803029346,
            "fpr": 0.26206140350877194,
            "logloss": 16.582469899003016,
            "mae": 0.5030315602391097,
            "precision": 0.5171717171717172,
            "recall": 0.5378151260504201
        },
        "train": {
            "accuracy": 0.47530186608122943,
            "auc_prc": 0.6182575930813135,
            "auditor_fn_violation": 0.008956087613501189,
            "auditor_fp_violation": 0.017332424080332004,
            "ave_precision_score": 0.5053118564690554,
            "fpr": 0.2678375411635565,
            "logloss": 17.338303719822363,
            "mae": 0.5243904558489508,
            "precision": 0.5,
            "recall": 0.5104602510460251
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.6573819917247677,
            "auditor_fn_violation": 0.014712793011941625,
            "auditor_fp_violation": 0.017561463866087236,
            "ave_precision_score": 0.6479094778212047,
            "fpr": 0.04057017543859649,
            "logloss": 4.995363486362653,
            "mae": 0.5213063563823483,
            "precision": 0.5,
            "recall": 0.07773109243697479
        },
        "train": {
            "accuracy": 0.47310647639956094,
            "auc_prc": 0.6601851089466135,
            "auditor_fn_violation": 0.017062495120080463,
            "auditor_fp_violation": 0.014756770596988818,
            "ave_precision_score": 0.6511037557938291,
            "fpr": 0.03951701427003293,
            "logloss": 4.895541623118625,
            "mae": 0.5238290856703187,
            "precision": 0.4857142857142857,
            "recall": 0.07112970711297072
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.7440196288618642,
            "auditor_fn_violation": 0.0012623470440808052,
            "auditor_fp_violation": 0.0011266698857234831,
            "ave_precision_score": 0.7458995730937586,
            "fpr": 0.0043859649122807015,
            "logloss": 5.175938063203164,
            "mae": 0.5206728562752485,
            "precision": 0.5,
            "recall": 0.008403361344537815
        },
        "train": {
            "accuracy": 0.47310647639956094,
            "auc_prc": 0.7305520904419363,
            "auditor_fn_violation": 0.0018371461771284534,
            "auditor_fp_violation": 0.003026899861330467,
            "ave_precision_score": 0.7317579782852965,
            "fpr": 0.006586169045005488,
            "logloss": 5.130691081961542,
            "mae": 0.5234969530778151,
            "precision": 0.4,
            "recall": 0.008368200836820083
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 18313,
        "test": {
            "accuracy": 0.30372807017543857,
            "auc_prc": 0.4673783929421602,
            "auditor_fn_violation": 0.009949045407636756,
            "auditor_fp_violation": 0.0007242877836793951,
            "ave_precision_score": 0.4256197931461959,
            "fpr": 0.2916666666666667,
            "logloss": 22.111277252709467,
            "mae": 0.6965778744049295,
            "precision": 0.2868632707774799,
            "recall": 0.22478991596638656
        },
        "train": {
            "accuracy": 0.2711306256860593,
            "auc_prc": 0.4416352381751555,
            "auditor_fn_violation": 0.0023285827795103166,
            "auditor_fp_violation": 0.0032651985103799327,
            "ave_precision_score": 0.42433265662984354,
            "fpr": 0.3040614709110867,
            "logloss": 23.08656547190488,
            "mae": 0.7277193487545119,
            "precision": 0.24728260869565216,
            "recall": 0.1903765690376569
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.6385508203671336,
            "auditor_fn_violation": 0.011273588382721512,
            "auditor_fp_violation": 0.012182118139385162,
            "ave_precision_score": 0.6308482691747628,
            "fpr": 0.03070175438596491,
            "logloss": 17.680503948793106,
            "mae": 0.5297099304141346,
            "precision": 0.44,
            "recall": 0.046218487394957986
        },
        "train": {
            "accuracy": 0.4643249176728869,
            "auc_prc": 0.6433847024121555,
            "auditor_fn_violation": 0.009222473809184834,
            "auditor_fp_violation": 0.012176046929623312,
            "ave_precision_score": 0.6362507779850264,
            "fpr": 0.031833150384193196,
            "logloss": 17.872976701772515,
            "mae": 0.5364749142015808,
            "precision": 0.3958333333333333,
            "recall": 0.0397489539748954
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.6474830191877883,
            "auditor_fn_violation": 0.010752985404688211,
            "auditor_fp_violation": 0.015157230806373732,
            "ave_precision_score": 0.6397719294214474,
            "fpr": 0.03179824561403509,
            "logloss": 9.657727539593917,
            "mae": 0.5262231141906077,
            "precision": 0.46296296296296297,
            "recall": 0.052521008403361345
        },
        "train": {
            "accuracy": 0.4698133918770582,
            "auc_prc": 0.6514727961787099,
            "auditor_fn_violation": 0.013514506565501157,
            "auditor_fp_violation": 0.012556310731297995,
            "ave_precision_score": 0.6429484328099199,
            "fpr": 0.03293084522502744,
            "logloss": 9.657457066990307,
            "mae": 0.528710242412638,
            "precision": 0.45454545454545453,
            "recall": 0.05230125523012552
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 18313,
        "test": {
            "accuracy": 0.5526315789473685,
            "auc_prc": 0.7662072041999541,
            "auditor_fn_violation": 0.0023542311661506714,
            "auditor_fp_violation": 0.007434009335264769,
            "ave_precision_score": 0.5449974614064403,
            "fpr": 0.43640350877192985,
            "logloss": 14.877017645669563,
            "mae": 0.4453437515175003,
            "precision": 0.5393518518518519,
            "recall": 0.9789915966386554
        },
        "train": {
            "accuracy": 0.5587266739846323,
            "auc_prc": 0.7718156731912751,
            "auditor_fn_violation": 0.00042713648618236455,
            "auditor_fp_violation": 0.007154029655506348,
            "ave_precision_score": 0.5522039957983055,
            "fpr": 0.43468715697036225,
            "logloss": 14.524271839642585,
            "mae": 0.43803033525477914,
            "precision": 0.543778801843318,
            "recall": 0.9874476987447699
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 18313,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.6371706129815903,
            "auditor_fn_violation": 0.01040284534866579,
            "auditor_fp_violation": 0.012116731047802996,
            "ave_precision_score": 0.6290947017132825,
            "fpr": 0.03399122807017544,
            "logloss": 17.822676531731585,
            "mae": 0.530167960861604,
            "precision": 0.43636363636363634,
            "recall": 0.05042016806722689
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6424703531980743,
            "auditor_fn_violation": 0.0084784296074478,
            "auditor_fp_violation": 0.011501712454653543,
            "ave_precision_score": 0.6348946832946291,
            "fpr": 0.03402854006586169,
            "logloss": 18.073801493164954,
            "mae": 0.5399515845675285,
            "precision": 0.39215686274509803,
            "recall": 0.04184100418410042
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 18313,
        "test": {
            "accuracy": 0.46710526315789475,
            "auc_prc": 0.638648328628605,
            "auditor_fn_violation": 0.009202694235588976,
            "auditor_fp_violation": 0.009279937228392083,
            "ave_precision_score": 0.6339476576244882,
            "fpr": 0.029605263157894735,
            "logloss": 17.962546439900997,
            "mae": 0.5343163486274436,
            "precision": 0.38636363636363635,
            "recall": 0.03571428571428571
        },
        "train": {
            "accuracy": 0.45993413830954993,
            "auc_prc": 0.6434733583462052,
            "auditor_fn_violation": 0.005828346246940006,
            "auditor_fp_violation": 0.010284868289294563,
            "ave_precision_score": 0.6381087453118517,
            "fpr": 0.03293084522502744,
            "logloss": 18.172399965524086,
            "mae": 0.5410965276833312,
            "precision": 0.34782608695652173,
            "recall": 0.03347280334728033
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6335431069367968,
            "auditor_fn_violation": 0.009202694235588976,
            "auditor_fp_violation": 0.010763721229679704,
            "ave_precision_score": 0.6283419459161453,
            "fpr": 0.02850877192982456,
            "logloss": 17.967476023890054,
            "mae": 0.5331551461154439,
            "precision": 0.3953488372093023,
            "recall": 0.03571428571428571
        },
        "train": {
            "accuracy": 0.4621295279912184,
            "auc_prc": 0.6355161275602407,
            "auditor_fn_violation": 0.0062531863004009714,
            "auditor_fp_violation": 0.010244306817115927,
            "ave_precision_score": 0.6296586814343937,
            "fpr": 0.029637760702524697,
            "logloss": 18.172473043856474,
            "mae": 0.5397993059205146,
            "precision": 0.35714285714285715,
            "recall": 0.03138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6344828128504167,
            "auditor_fn_violation": 0.0093155683325962,
            "auditor_fp_violation": 0.01102023981973282,
            "ave_precision_score": 0.6269389390375539,
            "fpr": 0.03399122807017544,
            "logloss": 17.889830032710616,
            "mae": 0.5329972466304856,
            "precision": 0.41509433962264153,
            "recall": 0.046218487394957986
        },
        "train": {
            "accuracy": 0.4621295279912184,
            "auc_prc": 0.6395895667927483,
            "auditor_fn_violation": 0.007518520729898174,
            "auditor_fp_violation": 0.010677807551025065,
            "ave_precision_score": 0.6319753658959679,
            "fpr": 0.03512623490669594,
            "logloss": 18.15864608163498,
            "mae": 0.5411010449555781,
            "precision": 0.38461538461538464,
            "recall": 0.04184100418410042
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4692982456140351,
            "auc_prc": 0.6428695646912712,
            "auditor_fn_violation": 0.010117204776647501,
            "auditor_fp_violation": 0.012182118139385162,
            "ave_precision_score": 0.6366846301785491,
            "fpr": 0.03070175438596491,
            "logloss": 17.809410824523418,
            "mae": 0.5310953096941791,
            "precision": 0.4166666666666667,
            "recall": 0.04201680672268908
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6463457002229938,
            "auditor_fn_violation": 0.007633342365968703,
            "auditor_fp_violation": 0.010989623868398305,
            "ave_precision_score": 0.6403817094398419,
            "fpr": 0.030735455543358946,
            "logloss": 18.00091724764696,
            "mae": 0.5389766008408806,
            "precision": 0.37777777777777777,
            "recall": 0.03556485355648536
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 18313,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.6775555102866877,
            "auditor_fn_violation": 0.008191434468524253,
            "auditor_fp_violation": 0.025236902462578467,
            "ave_precision_score": 0.6640828240595384,
            "fpr": 0.17434210526315788,
            "logloss": 2.0594041405500665,
            "mae": 0.3107974435373316,
            "precision": 0.7114337568058077,
            "recall": 0.8235294117647058
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.6839347692444351,
            "auditor_fn_violation": 0.014040389658704171,
            "auditor_fp_violation": 0.027751652246218277,
            "ave_precision_score": 0.6707848723175026,
            "fpr": 0.17014270032930845,
            "logloss": 1.8593823605457713,
            "mae": 0.314835189704954,
            "precision": 0.7113594040968343,
            "recall": 0.799163179916318
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6372295574052683,
            "auditor_fn_violation": 0.010117204776647501,
            "auditor_fp_violation": 0.012964248350233383,
            "ave_precision_score": 0.6308877145958343,
            "fpr": 0.03179824561403509,
            "logloss": 17.816637321777563,
            "mae": 0.5324383078066566,
            "precision": 0.40816326530612246,
            "recall": 0.04201680672268908
        },
        "train": {
            "accuracy": 0.4610318331503842,
            "auc_prc": 0.6407045229871151,
            "auditor_fn_violation": 0.007633342365968703,
            "auditor_fp_violation": 0.011306510369793871,
            "ave_precision_score": 0.6346245666767187,
            "fpr": 0.03293084522502744,
            "logloss": 18.004269524806183,
            "mae": 0.5397531640452081,
            "precision": 0.3617021276595745,
            "recall": 0.03556485355648536
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.6338997605735013,
            "auditor_fn_violation": 0.003918343653250772,
            "auditor_fp_violation": 0.016889988733301147,
            "ave_precision_score": 0.5219624303788122,
            "fpr": 0.23793859649122806,
            "logloss": 17.83266228465955,
            "mae": 0.5204879597017918,
            "precision": 0.5,
            "recall": 0.45588235294117646
        },
        "train": {
            "accuracy": 0.4522502744237102,
            "auc_prc": 0.6184117882932519,
            "auditor_fn_violation": 0.013264195398867414,
            "auditor_fp_violation": 0.017613819293571278,
            "ave_precision_score": 0.5092039123854909,
            "fpr": 0.24807903402854006,
            "logloss": 18.615021558400116,
            "mae": 0.5476648635168357,
            "precision": 0.4756380510440835,
            "recall": 0.42887029288702927
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 18313,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6367481376035157,
            "auditor_fn_violation": 0.010117204776647501,
            "auditor_fp_violation": 0.012964248350233383,
            "ave_precision_score": 0.6304060747292101,
            "fpr": 0.03179824561403509,
            "logloss": 17.81955485309706,
            "mae": 0.5321519847124752,
            "precision": 0.40816326530612246,
            "recall": 0.04201680672268908
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.6380086318894485,
            "auditor_fn_violation": 0.007633342365968703,
            "auditor_fp_violation": 0.010989623868398305,
            "ave_precision_score": 0.6319315782120581,
            "fpr": 0.030735455543358946,
            "logloss": 18.009399379074857,
            "mae": 0.5395428326823439,
            "precision": 0.37777777777777777,
            "recall": 0.03556485355648536
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 18313,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.6943149213145652,
            "auditor_fn_violation": 0.011324266548724754,
            "auditor_fp_violation": 0.023187268630291333,
            "ave_precision_score": 0.6834278636455882,
            "fpr": 0.18859649122807018,
            "logloss": 1.8401685540302533,
            "mae": 0.3018854659294767,
            "precision": 0.7013888888888888,
            "recall": 0.8487394957983193
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7090476091134608,
            "auditor_fn_violation": 0.0112846703930115,
            "auditor_fp_violation": 0.02766545911783868,
            "ave_precision_score": 0.6974203770824228,
            "fpr": 0.1800219538968167,
            "logloss": 1.678298755716944,
            "mae": 0.3040620714623346,
            "precision": 0.7076648841354723,
            "recall": 0.8305439330543933
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 18313,
        "test": {
            "accuracy": 0.5844298245614035,
            "auc_prc": 0.6257234695013973,
            "auditor_fn_violation": 0.013194751584844469,
            "auditor_fp_violation": 0.02141678738129728,
            "ave_precision_score": 0.6104169247565046,
            "fpr": 0.32785087719298245,
            "logloss": 3.7964363153151957,
            "mae": 0.4148294699311061,
            "precision": 0.5697841726618705,
            "recall": 0.8319327731092437
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.612634363892933,
            "auditor_fn_violation": 0.008483022472890612,
            "auditor_fp_violation": 0.020184402592892114,
            "ave_precision_score": 0.5974423364426521,
            "fpr": 0.3260153677277717,
            "logloss": 3.7023923647715247,
            "mae": 0.4136721408820903,
            "precision": 0.5708092485549133,
            "recall": 0.8263598326359832
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8076265008890982,
            "auditor_fn_violation": 0.0010428949154964793,
            "auditor_fp_violation": 0.006815956161186818,
            "ave_precision_score": 0.807160494820428,
            "fpr": 0.2138157894736842,
            "logloss": 0.9525372066047455,
            "mae": 0.30832512843668763,
            "precision": 0.6953125,
            "recall": 0.9063136456211812
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8207938405832054,
            "auditor_fn_violation": 0.00640124421220836,
            "auditor_fp_violation": 0.017661125921279597,
            "ave_precision_score": 0.8204368470962955,
            "fpr": 0.21953896816684962,
            "logloss": 0.9452091126809311,
            "mae": 0.3006099743438671,
            "precision": 0.68,
            "recall": 0.91792656587473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8269971999755099,
            "auditor_fn_violation": 0.012782720548826242,
            "auditor_fp_violation": 0.014900300037504694,
            "ave_precision_score": 0.8280361466815036,
            "fpr": 0.1699561403508772,
            "logloss": 0.8150229355227592,
            "mae": 0.27238114777380695,
            "precision": 0.7299651567944251,
            "recall": 0.8533604887983707
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8385375422886399,
            "auditor_fn_violation": 0.015818185697723766,
            "auditor_fp_violation": 0.021556962521561862,
            "ave_precision_score": 0.8388299282335421,
            "fpr": 0.1734357848518112,
            "logloss": 0.7981768771791186,
            "mae": 0.26488679953008615,
            "precision": 0.7178571428571429,
            "recall": 0.8682505399568035
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 20300,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.823993086288647,
            "auditor_fn_violation": 0.005361864437060068,
            "auditor_fp_violation": 0.010556007000875113,
            "ave_precision_score": 0.8244841233131905,
            "fpr": 0.1611842105263158,
            "logloss": 0.8708202440753854,
            "mae": 0.27315222472440853,
            "precision": 0.7332123411978222,
            "recall": 0.8228105906313645
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.833171711635931,
            "auditor_fn_violation": 0.01374845006910973,
            "auditor_fp_violation": 0.023605339501332916,
            "ave_precision_score": 0.8334947832223687,
            "fpr": 0.15916575192096596,
            "logloss": 0.8694837951110495,
            "mae": 0.2655070505272924,
            "precision": 0.7279549718574109,
            "recall": 0.838012958963283
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8295929982141086,
            "auditor_fn_violation": 0.01224229106370815,
            "auditor_fp_violation": 0.01665051881485186,
            "ave_precision_score": 0.830364208477363,
            "fpr": 0.16885964912280702,
            "logloss": 0.7788488995453955,
            "mae": 0.2729718474265833,
            "precision": 0.7335640138408305,
            "recall": 0.8635437881873728
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8415603416963532,
            "auditor_fn_violation": 0.013316958792583091,
            "auditor_fp_violation": 0.02171867649364905,
            "ave_precision_score": 0.8418433946818394,
            "fpr": 0.1756311745334797,
            "logloss": 0.7662693710556758,
            "mae": 0.2656056522275298,
            "precision": 0.7163120567375887,
            "recall": 0.8725701943844493
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8217374911631634,
            "auditor_fn_violation": 0.010480312287847932,
            "auditor_fp_violation": 0.007133704213026634,
            "ave_precision_score": 0.8220138211815184,
            "fpr": 0.15570175438596492,
            "logloss": 0.8396001871097328,
            "mae": 0.279690560689859,
            "precision": 0.737037037037037,
            "recall": 0.8105906313645621
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8295423531980801,
            "auditor_fn_violation": 0.011479564620560325,
            "auditor_fp_violation": 0.02470303434216717,
            "ave_precision_score": 0.8298572741245762,
            "fpr": 0.15916575192096596,
            "logloss": 0.8507783008757035,
            "mae": 0.271388507581862,
            "precision": 0.7238095238095238,
            "recall": 0.8207343412526998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8389031001603354,
            "auditor_fn_violation": 0.01422088826955372,
            "auditor_fp_violation": 0.014272617410509647,
            "ave_precision_score": 0.8394673874622574,
            "fpr": 0.16666666666666666,
            "logloss": 0.7603942677783706,
            "mae": 0.27066123915128987,
            "precision": 0.7304964539007093,
            "recall": 0.8391038696537678
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8435349959110151,
            "auditor_fn_violation": 0.012819084242744664,
            "auditor_fp_violation": 0.024046377607025247,
            "ave_precision_score": 0.8438186193254925,
            "fpr": 0.1690450054884742,
            "logloss": 0.7487421543518448,
            "mae": 0.2640927956385445,
            "precision": 0.716390423572744,
            "recall": 0.8401727861771058
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8295102261189482,
            "auditor_fn_violation": 0.010703630256904994,
            "auditor_fp_violation": 0.01582489477851399,
            "ave_precision_score": 0.8297462640635183,
            "fpr": 0.16666666666666666,
            "logloss": 0.7742680182797502,
            "mae": 0.27505352232156266,
            "precision": 0.7275985663082437,
            "recall": 0.8268839103869654
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8348445443831174,
            "auditor_fn_violation": 0.011154760747570491,
            "auditor_fp_violation": 0.023340716637917525,
            "ave_precision_score": 0.8351311831615096,
            "fpr": 0.16575192096597147,
            "logloss": 0.7994762365290007,
            "mae": 0.2711502338872062,
            "precision": 0.7166979362101313,
            "recall": 0.8250539956803455
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8234849583773254,
            "auditor_fn_violation": 0.004640547397005754,
            "auditor_fp_violation": 0.01166552069008626,
            "ave_precision_score": 0.8240174668923999,
            "fpr": 0.15899122807017543,
            "logloss": 0.845253703429766,
            "mae": 0.27730629209546975,
            "precision": 0.7358834244080146,
            "recall": 0.8228105906313645
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8301874365831339,
            "auditor_fn_violation": 0.012100722392263506,
            "auditor_fp_violation": 0.024041477183628673,
            "ave_precision_score": 0.8304187509580494,
            "fpr": 0.16245883644346873,
            "logloss": 0.8606647641821545,
            "mae": 0.27090145015305594,
            "precision": 0.7212806026365348,
            "recall": 0.8272138228941684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8226681595859454,
            "auditor_fn_violation": 0.011981009039911391,
            "auditor_fp_violation": 0.01162384881443514,
            "ave_precision_score": 0.8223612764569403,
            "fpr": 0.17653508771929824,
            "logloss": 0.8567832453963373,
            "mae": 0.27211775053606874,
            "precision": 0.7238421955403087,
            "recall": 0.8594704684317719
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.827143178332501,
            "auditor_fn_violation": 0.01464699509000861,
            "auditor_fp_violation": 0.015970479849458995,
            "ave_precision_score": 0.8268906050644396,
            "fpr": 0.18221734357848518,
            "logloss": 0.8598726653261155,
            "mae": 0.2680850644086982,
            "precision": 0.7072310405643739,
            "recall": 0.8660907127429806
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8334783747421266,
            "auditor_fn_violation": 0.01023689570157573,
            "auditor_fp_violation": 0.01575457348835272,
            "ave_precision_score": 0.8339820301927813,
            "fpr": 0.12938596491228072,
            "logloss": 0.8500778898705089,
            "mae": 0.2724309699681984,
            "precision": 0.7620967741935484,
            "recall": 0.769857433808554
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8362508127532688,
            "auditor_fn_violation": 0.011173727397088143,
            "auditor_fp_violation": 0.02130214050493963,
            "ave_precision_score": 0.8365546300501049,
            "fpr": 0.13721185510428102,
            "logloss": 0.7538648290356095,
            "mae": 0.26535146504917495,
            "precision": 0.7438524590163934,
            "recall": 0.7840172786177105
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8266153137604797,
            "auditor_fn_violation": 0.005960356594132991,
            "auditor_fp_violation": 0.011165458182272789,
            "ave_precision_score": 0.8271028912168652,
            "fpr": 0.1611842105263158,
            "logloss": 0.7973302185693811,
            "mae": 0.2721868910615155,
            "precision": 0.7365591397849462,
            "recall": 0.8370672097759674
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8374907086068262,
            "auditor_fn_violation": 0.009056575144679975,
            "auditor_fp_violation": 0.020094186137682303,
            "ave_precision_score": 0.837781878533178,
            "fpr": 0.17014270032930845,
            "logloss": 0.7824349254751063,
            "mae": 0.26735966362066477,
            "precision": 0.7171532846715328,
            "recall": 0.8488120950323974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8303657550368423,
            "auditor_fn_violation": 0.01306186801014757,
            "auditor_fp_violation": 0.011566549985414851,
            "ave_precision_score": 0.8309126999438506,
            "fpr": 0.1611842105263158,
            "logloss": 0.7927489468543358,
            "mae": 0.27128090735706883,
            "precision": 0.7393617021276596,
            "recall": 0.8492871690427699
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.841452264682325,
            "auditor_fn_violation": 0.010988802564291019,
            "auditor_fp_violation": 0.022424337462756787,
            "ave_precision_score": 0.8417320217061532,
            "fpr": 0.1756311745334797,
            "logloss": 0.7784184107940014,
            "mae": 0.2635553108214937,
            "precision": 0.7117117117117117,
            "recall": 0.8531317494600432
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 20300,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8240099746745765,
            "auditor_fn_violation": 0.005361864437060068,
            "auditor_fp_violation": 0.010556007000875113,
            "ave_precision_score": 0.8244779825979147,
            "fpr": 0.1611842105263158,
            "logloss": 0.8708214331462741,
            "mae": 0.2731512105011482,
            "precision": 0.7332123411978222,
            "recall": 0.8228105906313645
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8331676309615933,
            "auditor_fn_violation": 0.01374845006910973,
            "auditor_fp_violation": 0.023605339501332916,
            "ave_precision_score": 0.8334888320863922,
            "fpr": 0.15916575192096596,
            "logloss": 0.8694950354914862,
            "mae": 0.2655145251188946,
            "precision": 0.7279549718574109,
            "recall": 0.838012958963283
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8350632894638326,
            "auditor_fn_violation": 0.015174455997427378,
            "auditor_fp_violation": 0.014322102762845356,
            "ave_precision_score": 0.8358269948074549,
            "fpr": 0.13486842105263158,
            "logloss": 0.7470240439151147,
            "mae": 0.27038541547406936,
            "precision": 0.7588235294117647,
            "recall": 0.7881873727087576
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8455418836919385,
            "auditor_fn_violation": 0.008480463165581225,
            "auditor_fp_violation": 0.020841500705660973,
            "ave_precision_score": 0.8457954423094193,
            "fpr": 0.13391877058177826,
            "logloss": 0.7173607680446181,
            "mae": 0.25999355095490156,
            "precision": 0.7530364372469636,
            "recall": 0.8034557235421166
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8194808272593874,
            "auditor_fn_violation": 0.010828688319576944,
            "auditor_fp_violation": 0.014454931866483315,
            "ave_precision_score": 0.819816431919523,
            "fpr": 0.15350877192982457,
            "logloss": 0.7598083554679098,
            "mae": 0.2794446301284625,
            "precision": 0.7397769516728625,
            "recall": 0.8105906313645621
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8236555988427611,
            "auditor_fn_violation": 0.005886773844041984,
            "auditor_fp_violation": 0.020270601379959233,
            "ave_precision_score": 0.8242439183605184,
            "fpr": 0.150384193194292,
            "logloss": 0.7447100002705668,
            "mae": 0.27432887431710945,
            "precision": 0.7318982387475538,
            "recall": 0.8077753779697624
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 20300,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.8245348670604908,
            "auditor_fn_violation": 0.0041805123807482055,
            "auditor_fp_violation": 0.012399987498437305,
            "ave_precision_score": 0.8252892856868724,
            "fpr": 0.28289473684210525,
            "logloss": 0.7431158686188104,
            "mae": 0.34100329632622706,
            "precision": 0.6371308016877637,
            "recall": 0.9226069246435845
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.8434139697922491,
            "auditor_fn_violation": 0.004618379157548846,
            "auditor_fp_violation": 0.006618021797083288,
            "ave_precision_score": 0.84470514365449,
            "fpr": 0.3106476399560922,
            "logloss": 0.7335532976925632,
            "mae": 0.34173869176073046,
            "precision": 0.6101928374655647,
            "recall": 0.9568034557235421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 20300,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8302525023575706,
            "auditor_fn_violation": 0.012628631150176872,
            "auditor_fp_violation": 0.013532941617702215,
            "ave_precision_score": 0.830519922103947,
            "fpr": 0.16557017543859648,
            "logloss": 0.7876221046373003,
            "mae": 0.27262086143450504,
            "precision": 0.7327433628318584,
            "recall": 0.8431771894093686
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8347407357066694,
            "auditor_fn_violation": 0.00783559708198097,
            "auditor_fp_violation": 0.02771679473106476,
            "ave_precision_score": 0.8350472366907546,
            "fpr": 0.1690450054884742,
            "logloss": 0.7833855983281799,
            "mae": 0.27064188085657004,
            "precision": 0.7142857142857143,
            "recall": 0.8315334773218143
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8345555689925359,
            "auditor_fn_violation": 0.01265766248615429,
            "auditor_fp_violation": 0.012876609576197032,
            "ave_precision_score": 0.8351340095650708,
            "fpr": 0.15679824561403508,
            "logloss": 0.7770740050666897,
            "mae": 0.27182491801735814,
            "precision": 0.7342007434944238,
            "recall": 0.8044806517311609
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8392747065232093,
            "auditor_fn_violation": 0.011569656205769182,
            "auditor_fp_violation": 0.02260320291673201,
            "ave_precision_score": 0.8395617214385459,
            "fpr": 0.150384193194292,
            "logloss": 0.7612742659299859,
            "mae": 0.2634057329825597,
            "precision": 0.732943469785575,
            "recall": 0.8120950323974082
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8132048071416101,
            "auditor_fn_violation": 0.011683996141065502,
            "auditor_fp_violation": 0.016861482685335674,
            "ave_precision_score": 0.813608111058534,
            "fpr": 0.16666666666666666,
            "logloss": 0.8565049404683642,
            "mae": 0.2826199373765868,
            "precision": 0.7295373665480427,
            "recall": 0.835030549898167
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8259230905144718,
            "auditor_fn_violation": 0.014559274335989456,
            "auditor_fp_violation": 0.02494805551199624,
            "ave_precision_score": 0.8266892923116202,
            "fpr": 0.17014270032930845,
            "logloss": 0.8236032116618746,
            "mae": 0.27196728514440954,
            "precision": 0.716636197440585,
            "recall": 0.8466522678185745
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8196900924793719,
            "auditor_fn_violation": 0.013044002572623005,
            "auditor_fp_violation": 0.02075259407425929,
            "ave_precision_score": 0.8200773895127444,
            "fpr": 0.16447368421052633,
            "logloss": 0.8041602842200675,
            "mae": 0.27826830729977203,
            "precision": 0.7297297297297297,
            "recall": 0.824847250509165
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8257037470249347,
            "auditor_fn_violation": 0.012048564106089952,
            "auditor_fp_violation": 0.026001646542261253,
            "ave_precision_score": 0.8269966638383177,
            "fpr": 0.1690450054884742,
            "logloss": 0.7859538276933777,
            "mae": 0.272254220134075,
            "precision": 0.7153419593345656,
            "recall": 0.8358531317494601
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8259894187506583,
            "auditor_fn_violation": 0.012396380462357522,
            "auditor_fp_violation": 0.014246572488227695,
            "ave_precision_score": 0.8267675185081194,
            "fpr": 0.17105263157894737,
            "logloss": 0.7956992625811379,
            "mae": 0.2741334899060075,
            "precision": 0.7282229965156795,
            "recall": 0.8513238289205702
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8378453611665133,
            "auditor_fn_violation": 0.015737577437273737,
            "auditor_fp_violation": 0.02218666692802258,
            "ave_precision_score": 0.8381357786031978,
            "fpr": 0.17453347969264543,
            "logloss": 0.7818574041964146,
            "mae": 0.266726284832484,
            "precision": 0.7155635062611807,
            "recall": 0.8639308855291576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8247058430983283,
            "auditor_fn_violation": 0.00394826169292886,
            "auditor_fp_violation": 0.015585281493520026,
            "ave_precision_score": 0.8253219157073566,
            "fpr": 0.16447368421052633,
            "logloss": 0.8232397744480024,
            "mae": 0.27755464222623194,
            "precision": 0.7311827956989247,
            "recall": 0.8309572301425662
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8334066495004471,
            "auditor_fn_violation": 0.013184192245959514,
            "auditor_fp_violation": 0.02468833307197742,
            "ave_precision_score": 0.8337154269677367,
            "fpr": 0.1712403951701427,
            "logloss": 0.8391416925137,
            "mae": 0.2722429751292197,
            "precision": 0.7132352941176471,
            "recall": 0.838012958963283
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 20300,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.8267451725561958,
            "auditor_fn_violation": 0.009919784185514703,
            "auditor_fp_violation": 0.01244165937408845,
            "ave_precision_score": 0.8275012273725919,
            "fpr": 0.2675438596491228,
            "logloss": 0.6907544817941156,
            "mae": 0.3362150884813832,
            "precision": 0.6453488372093024,
            "recall": 0.9042769857433809
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.8429580994178577,
            "auditor_fn_violation": 0.011716647739531004,
            "auditor_fp_violation": 0.008806060843656895,
            "ave_precision_score": 0.8442576363960002,
            "fpr": 0.283205268935236,
            "logloss": 0.6715679651911888,
            "mae": 0.3340992303831264,
            "precision": 0.625,
            "recall": 0.9287257019438445
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8296240922477829,
            "auditor_fn_violation": 0.012691160181512848,
            "auditor_fp_violation": 0.01665051881485186,
            "ave_precision_score": 0.8304013830676614,
            "fpr": 0.16885964912280702,
            "logloss": 0.7822908880586141,
            "mae": 0.27296789942250943,
            "precision": 0.7335640138408305,
            "recall": 0.8635437881873728
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8417439084281971,
            "auditor_fn_violation": 0.013518479443708173,
            "auditor_fp_violation": 0.021953896816684963,
            "ave_precision_score": 0.8420279117649047,
            "fpr": 0.17672886937431395,
            "logloss": 0.7628784226910169,
            "mae": 0.2658991282558335,
            "precision": 0.715547703180212,
            "recall": 0.8747300215982722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8258638723864731,
            "auditor_fn_violation": 0.010667899381855863,
            "auditor_fp_violation": 0.016168687752635743,
            "ave_precision_score": 0.8264407733605112,
            "fpr": 0.16885964912280702,
            "logloss": 0.8295961920316951,
            "mae": 0.2800895555474977,
            "precision": 0.7230215827338129,
            "recall": 0.8187372708757638
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8324638350454405,
            "auditor_fn_violation": 0.010502782170401122,
            "auditor_fp_violation": 0.024556021640269723,
            "ave_precision_score": 0.8325671849256193,
            "fpr": 0.16465422612513722,
            "logloss": 0.8198039665354795,
            "mae": 0.27074644004940845,
            "precision": 0.7159090909090909,
            "recall": 0.816414686825054
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 20300,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.8194337967592138,
            "auditor_fn_violation": 0.011483009968914142,
            "auditor_fp_violation": 0.0074853106638329845,
            "ave_precision_score": 0.820198918123124,
            "fpr": 0.2576754385964912,
            "logloss": 0.689574872577339,
            "mae": 0.3361064449336801,
            "precision": 0.6539027982326951,
            "recall": 0.9042769857433809
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.8363944627092308,
            "auditor_fn_violation": 0.005203974461406426,
            "auditor_fp_violation": 0.009176042810098796,
            "ave_precision_score": 0.8377035062613548,
            "fpr": 0.2711306256860593,
            "logloss": 0.6697019554139322,
            "mae": 0.33296142486225005,
            "precision": 0.6351550960118169,
            "recall": 0.9287257019438445
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8319807878719976,
            "auditor_fn_violation": 0.012592900275127739,
            "auditor_fp_violation": 0.014928949452014837,
            "ave_precision_score": 0.8326088787144212,
            "fpr": 0.16557017543859648,
            "logloss": 0.7758345713572868,
            "mae": 0.27170836963866274,
            "precision": 0.736013986013986,
            "recall": 0.8574338085539714
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8433280965955801,
            "auditor_fn_violation": 0.013568266898692016,
            "auditor_fp_violation": 0.023122647796769643,
            "ave_precision_score": 0.8436000957653854,
            "fpr": 0.17233809001097694,
            "logloss": 0.7582373502726407,
            "mae": 0.2641993786540816,
            "precision": 0.7186379928315412,
            "recall": 0.8660907127429806
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8256881937529377,
            "auditor_fn_violation": 0.011634866187872945,
            "auditor_fp_violation": 0.01085812809934575,
            "ave_precision_score": 0.8262784183789113,
            "fpr": 0.14473684210526316,
            "logloss": 0.7957803517081318,
            "mae": 0.27434667549531133,
            "precision": 0.7461538461538462,
            "recall": 0.790224032586558
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8354548124474349,
            "auditor_fn_violation": 0.013352521260428695,
            "auditor_fp_violation": 0.020542574878469506,
            "ave_precision_score": 0.8357584816111077,
            "fpr": 0.141602634467618,
            "logloss": 0.7640475879249212,
            "mae": 0.261903889298684,
            "precision": 0.7445544554455445,
            "recall": 0.8120950323974082
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8336728875940421,
            "auditor_fn_violation": 0.010370886483009968,
            "auditor_fp_violation": 0.013910592990790517,
            "ave_precision_score": 0.8340672830759182,
            "fpr": 0.16776315789473684,
            "logloss": 0.7764105628912875,
            "mae": 0.270516641114636,
            "precision": 0.7325174825174825,
            "recall": 0.8533604887983707
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8381492663053203,
            "auditor_fn_violation": 0.01171190607715159,
            "auditor_fp_violation": 0.027045436725733115,
            "ave_precision_score": 0.8384830706506297,
            "fpr": 0.1734357848518112,
            "logloss": 0.7734352581595387,
            "mae": 0.266378232152572,
            "precision": 0.7132486388384754,
            "recall": 0.8488120950323974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8281912389829204,
            "auditor_fn_violation": 0.010866652374316652,
            "auditor_fp_violation": 0.015554027586781686,
            "ave_precision_score": 0.8287607797950229,
            "fpr": 0.16885964912280702,
            "logloss": 0.8191767616965941,
            "mae": 0.2732865571181174,
            "precision": 0.7326388888888888,
            "recall": 0.8594704684317719
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8391591630728279,
            "auditor_fn_violation": 0.014288999580362878,
            "auditor_fp_violation": 0.020314705190528463,
            "ave_precision_score": 0.839449064895993,
            "fpr": 0.17892425905598244,
            "logloss": 0.8068662060186867,
            "mae": 0.2672876075620981,
            "precision": 0.7104795737122558,
            "recall": 0.8639308855291576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8251971217518078,
            "auditor_fn_violation": 0.013204791510344089,
            "auditor_fp_violation": 0.021283910488811102,
            "ave_precision_score": 0.8262261041633694,
            "fpr": 0.16228070175438597,
            "logloss": 0.7923052007406062,
            "mae": 0.273488107555102,
            "precision": 0.7318840579710145,
            "recall": 0.8228105906313645
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8295870548632999,
            "auditor_fn_violation": 0.007996813602881037,
            "auditor_fp_violation": 0.02397532146777482,
            "ave_precision_score": 0.8299507131190639,
            "fpr": 0.16794731064763996,
            "logloss": 0.8177701732124886,
            "mae": 0.2689239775088215,
            "precision": 0.7140186915887851,
            "recall": 0.8250539956803455
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 20300,
        "test": {
            "accuracy": 0.6348684210526315,
            "auc_prc": 0.664011736764998,
            "auditor_fn_violation": 0.010109604459213207,
            "auditor_fp_violation": 0.0032972871608951226,
            "ave_precision_score": 0.6461458686509056,
            "fpr": 0.3201754385964912,
            "logloss": 2.633713825767067,
            "mae": 0.38418911698065966,
            "precision": 0.6064690026954178,
            "recall": 0.9164969450101833
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.6875717343621931,
            "auditor_fn_violation": 0.002927976519287897,
            "auditor_fp_violation": 0.021652520777795222,
            "ave_precision_score": 0.6679461545738727,
            "fpr": 0.3260153677277717,
            "logloss": 2.6179752472073345,
            "mae": 0.37950837278528476,
            "precision": 0.5886426592797784,
            "recall": 0.91792656587473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8190841556752088,
            "auditor_fn_violation": 0.018546557330189017,
            "auditor_fp_violation": 0.01228018085594033,
            "ave_precision_score": 0.8194838018834921,
            "fpr": 0.14473684210526316,
            "logloss": 0.8123770970367867,
            "mae": 0.2787257201148565,
            "precision": 0.7441860465116279,
            "recall": 0.7820773930753564
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8238326617477849,
            "auditor_fn_violation": 0.017411384257206737,
            "auditor_fp_violation": 0.025090167790497105,
            "ave_precision_score": 0.8251412355197425,
            "fpr": 0.14818880351262348,
            "logloss": 0.7802670739259207,
            "mae": 0.2679802271308153,
            "precision": 0.733201581027668,
            "recall": 0.8012958963282938
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7993900193019202,
            "auditor_fn_violation": 0.010888984171222358,
            "auditor_fp_violation": 0.019327936825436518,
            "ave_precision_score": 0.7997598125560448,
            "fpr": 0.18421052631578946,
            "logloss": 0.9864242887281501,
            "mae": 0.2922834358493083,
            "precision": 0.708838821490468,
            "recall": 0.8329938900203666
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.807473828194287,
            "auditor_fn_violation": 0.011237739839210233,
            "auditor_fp_violation": 0.023803806648894473,
            "ave_precision_score": 0.8078657832425415,
            "fpr": 0.1964873765093304,
            "logloss": 0.9641112790048078,
            "mae": 0.29284221068891847,
            "precision": 0.6831858407079646,
            "recall": 0.8336933045356372
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8362628476251213,
            "auditor_fn_violation": 0.0111212348590417,
            "auditor_fp_violation": 0.015111263907988505,
            "ave_precision_score": 0.83703400535391,
            "fpr": 0.16776315789473684,
            "logloss": 0.7211237400412813,
            "mae": 0.26982724578934797,
            "precision": 0.7352941176470589,
            "recall": 0.8655804480651731
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8458281223551057,
            "auditor_fn_violation": 0.012762184294191703,
            "auditor_fp_violation": 0.021157578014740483,
            "ave_precision_score": 0.8460895347552198,
            "fpr": 0.16575192096597147,
            "logloss": 0.7160511142204897,
            "mae": 0.26445482912782226,
            "precision": 0.7269439421338155,
            "recall": 0.8682505399568035
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8248678437808298,
            "auditor_fn_violation": 0.00394826169292886,
            "auditor_fp_violation": 0.015585281493520026,
            "ave_precision_score": 0.8255529819176952,
            "fpr": 0.16447368421052633,
            "logloss": 0.8230982784735152,
            "mae": 0.2775299698273411,
            "precision": 0.7311827956989247,
            "recall": 0.8309572301425662
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.833162486154553,
            "auditor_fn_violation": 0.011104973292586652,
            "auditor_fp_violation": 0.025156323506350952,
            "ave_precision_score": 0.8334632073505106,
            "fpr": 0.17014270032930845,
            "logloss": 0.8392715509933635,
            "mae": 0.2722928529665088,
            "precision": 0.7150735294117647,
            "recall": 0.8401727861771058
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8280426363668383,
            "auditor_fn_violation": 0.00393262943509487,
            "auditor_fp_violation": 0.01557746801683545,
            "ave_precision_score": 0.8286258630393647,
            "fpr": 0.15899122807017543,
            "logloss": 0.783716518443796,
            "mae": 0.2773972278667671,
            "precision": 0.7358834244080146,
            "recall": 0.8228105906313645
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8350015381295868,
            "auditor_fn_violation": 0.013674954302228821,
            "auditor_fp_violation": 0.0245094676180022,
            "ave_precision_score": 0.8353024355328109,
            "fpr": 0.16136114160263446,
            "logloss": 0.7986453797126127,
            "mae": 0.2701402674359729,
            "precision": 0.7210626185958254,
            "recall": 0.8207343412526998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8241901913294134,
            "auditor_fn_violation": 0.005390895773037485,
            "auditor_fp_violation": 0.013811622286119096,
            "ave_precision_score": 0.8248042327816576,
            "fpr": 0.1611842105263158,
            "logloss": 0.8180512806215139,
            "mae": 0.2776921479220248,
            "precision": 0.7341772151898734,
            "recall": 0.8268839103869654
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8325814395008936,
            "auditor_fn_violation": 0.012233488938887089,
            "auditor_fp_violation": 0.02373520072134233,
            "ave_precision_score": 0.8328905809365863,
            "fpr": 0.16575192096597147,
            "logloss": 0.8321488724006951,
            "mae": 0.27157674980238833,
            "precision": 0.7177570093457943,
            "recall": 0.8293736501079914
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8272189319141046,
            "auditor_fn_violation": 0.01386357951906242,
            "auditor_fp_violation": 0.011743655456932123,
            "ave_precision_score": 0.8274885528133344,
            "fpr": 0.16666666666666666,
            "logloss": 0.8148631560715299,
            "mae": 0.27265129697946533,
            "precision": 0.7333333333333333,
            "recall": 0.8513238289205702
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8385087332281735,
            "auditor_fn_violation": 0.015818185697723766,
            "auditor_fp_violation": 0.02218666692802258,
            "ave_precision_score": 0.8387981731106374,
            "fpr": 0.17453347969264543,
            "logloss": 0.7974894598202124,
            "mae": 0.2650146423880981,
            "precision": 0.7165775401069518,
            "recall": 0.8682505399568035
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8227576178290166,
            "auditor_fn_violation": 0.014178457855432883,
            "auditor_fp_violation": 0.018682022752844116,
            "ave_precision_score": 0.8236963508175803,
            "fpr": 0.16557017543859648,
            "logloss": 0.8521311658848956,
            "mae": 0.2759384525149719,
            "precision": 0.7313167259786477,
            "recall": 0.8370672097759674
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8240651248127929,
            "auditor_fn_violation": 0.012980300763644727,
            "auditor_fp_violation": 0.02517837541163557,
            "ave_precision_score": 0.8245390153789032,
            "fpr": 0.16794731064763996,
            "logloss": 0.843378158957204,
            "mae": 0.2693412906725688,
            "precision": 0.7145522388059702,
            "recall": 0.8272138228941684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8201829224294522,
            "auditor_fn_violation": 0.010634401686497298,
            "auditor_fp_violation": 0.009597553860899284,
            "ave_precision_score": 0.8206516355052754,
            "fpr": 0.1337719298245614,
            "logloss": 0.8129663837063845,
            "mae": 0.28093084943420316,
            "precision": 0.7525354969574036,
            "recall": 0.7556008146639511
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8307536473107231,
            "auditor_fn_violation": 0.011683456102875108,
            "auditor_fp_violation": 0.017063274266896668,
            "ave_precision_score": 0.8310580427762236,
            "fpr": 0.12733260153677278,
            "logloss": 0.7746698089413072,
            "mae": 0.2660427301547223,
            "precision": 0.7578288100208769,
            "recall": 0.7840172786177105
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 20300,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.8213752623225721,
            "auditor_fn_violation": 0.009707632114910495,
            "auditor_fp_violation": 0.015332645747385094,
            "ave_precision_score": 0.8221451852507954,
            "fpr": 0.26644736842105265,
            "logloss": 0.7071758066212537,
            "mae": 0.33579533431631653,
            "precision": 0.6473149492017417,
            "recall": 0.9083503054989817
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.8396082291944078,
            "auditor_fn_violation": 0.00708878525722333,
            "auditor_fp_violation": 0.01271169829073234,
            "ave_precision_score": 0.8409144132486517,
            "fpr": 0.2854006586169045,
            "logloss": 0.6892467519529887,
            "mae": 0.3338239485236027,
            "precision": 0.6258992805755396,
            "recall": 0.9395248380129589
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8289157766681075,
            "auditor_fn_violation": 0.008622306785293172,
            "auditor_fp_violation": 0.01332979122390299,
            "ave_precision_score": 0.8291708712112984,
            "fpr": 0.1600877192982456,
            "logloss": 0.8070826267183011,
            "mae": 0.2767063300695019,
            "precision": 0.7326007326007326,
            "recall": 0.814663951120163
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8289655617976348,
            "auditor_fn_violation": 0.008672500491947477,
            "auditor_fp_violation": 0.019974125764466055,
            "ave_precision_score": 0.8292660570355046,
            "fpr": 0.1668496158068057,
            "logloss": 0.8734993936505174,
            "mae": 0.2761474174691236,
            "precision": 0.7137476459510358,
            "recall": 0.8185745140388769
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.840623879600725,
            "auditor_fn_violation": 0.010750527030406978,
            "auditor_fp_violation": 0.01148060174188441,
            "ave_precision_score": 0.8411352148295692,
            "fpr": 0.17543859649122806,
            "logloss": 0.5772904423791065,
            "mae": 0.315336601996428,
            "precision": 0.7217391304347827,
            "recall": 0.845213849287169
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.838705865248318,
            "auditor_fn_violation": 0.009613720474261076,
            "auditor_fp_violation": 0.003425395954210448,
            "ave_precision_score": 0.8400417200814962,
            "fpr": 0.20087815587266739,
            "logloss": 0.559851350708353,
            "mae": 0.31467833446011667,
            "precision": 0.692436974789916,
            "recall": 0.8898488120950324
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8272099367264308,
            "auditor_fn_violation": 0.007668739057419518,
            "auditor_fp_violation": 0.017207880151685635,
            "ave_precision_score": 0.8278458608220736,
            "fpr": 0.1699561403508772,
            "logloss": 0.8500053667370299,
            "mae": 0.2748525706488883,
            "precision": 0.7246891651865008,
            "recall": 0.8309572301425662
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8283744900498551,
            "auditor_fn_violation": 0.011033848356895443,
            "auditor_fp_violation": 0.02517347498823899,
            "ave_precision_score": 0.8290554679191994,
            "fpr": 0.1778265642151482,
            "logloss": 0.8767467381625678,
            "mae": 0.27220471291850973,
            "precision": 0.7049180327868853,
            "recall": 0.8358531317494601
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8261803741324166,
            "auditor_fn_violation": 0.009305659770607783,
            "auditor_fp_violation": 0.01322300704254699,
            "ave_precision_score": 0.8269276975591546,
            "fpr": 0.16337719298245615,
            "logloss": 0.8479239872865193,
            "mae": 0.27103935467942974,
            "precision": 0.7339285714285714,
            "recall": 0.8370672097759674
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8374493834184552,
            "auditor_fn_violation": 0.014134895553031941,
            "auditor_fp_violation": 0.022169515446134547,
            "ave_precision_score": 0.8377439907587879,
            "fpr": 0.1668496158068057,
            "logloss": 0.827470451093672,
            "mae": 0.2646884909711616,
            "precision": 0.7205882352941176,
            "recall": 0.8466522678185745
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8299174521888273,
            "auditor_fn_violation": 0.012396380462357522,
            "auditor_fp_violation": 0.014879464099679126,
            "ave_precision_score": 0.8304856158527203,
            "fpr": 0.16447368421052633,
            "logloss": 0.7917842548367457,
            "mae": 0.2713979776195387,
            "precision": 0.7359154929577465,
            "recall": 0.8513238289205702
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8407189898939781,
            "auditor_fn_violation": 0.012930513308660884,
            "auditor_fp_violation": 0.022492943390308922,
            "ave_precision_score": 0.8409995850924592,
            "fpr": 0.1712403951701427,
            "logloss": 0.7785038869798373,
            "mae": 0.26427890132898596,
            "precision": 0.7179023508137432,
            "recall": 0.857451403887689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8344040931352801,
            "auditor_fn_violation": 0.013276253260442353,
            "auditor_fp_violation": 0.016770325457348842,
            "ave_precision_score": 0.8348546486583248,
            "fpr": 0.15679824561403508,
            "logloss": 0.7774686644796872,
            "mae": 0.2720364166468825,
            "precision": 0.7380952380952381,
            "recall": 0.8207739307535642
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8365182021919368,
            "auditor_fn_violation": 0.011427406334386771,
            "auditor_fp_violation": 0.02030735455543359,
            "ave_precision_score": 0.8368214750772955,
            "fpr": 0.15916575192096596,
            "logloss": 0.7313831330980501,
            "mae": 0.2656554855768036,
            "precision": 0.7248576850094877,
            "recall": 0.8250539956803455
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.809675159748158,
            "auditor_fn_violation": 0.014638492871690433,
            "auditor_fp_violation": 0.012551048047672628,
            "ave_precision_score": 0.8100792460867866,
            "fpr": 0.14583333333333334,
            "logloss": 0.8476953620044956,
            "mae": 0.2877000511641503,
            "precision": 0.7392156862745098,
            "recall": 0.7678207739307535
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8222265483724185,
            "auditor_fn_violation": 0.01218844314628266,
            "auditor_fp_violation": 0.01788899560922064,
            "ave_precision_score": 0.8228263293142682,
            "fpr": 0.145993413830955,
            "logloss": 0.7979263279490416,
            "mae": 0.27068475241145273,
            "precision": 0.7323943661971831,
            "recall": 0.7861771058315334
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8287907648813984,
            "auditor_fn_violation": 0.012085968485368206,
            "auditor_fp_violation": 0.016291098887360925,
            "ave_precision_score": 0.8294687741689244,
            "fpr": 0.16447368421052633,
            "logloss": 0.8089646619019343,
            "mae": 0.2721168802604404,
            "precision": 0.7359154929577465,
            "recall": 0.8513238289205702
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8399538740060339,
            "auditor_fn_violation": 0.014376720334382032,
            "auditor_fp_violation": 0.022024952955935394,
            "ave_precision_score": 0.8402424818779559,
            "fpr": 0.17233809001097694,
            "logloss": 0.7931475334770843,
            "mae": 0.2650111022780261,
            "precision": 0.7171171171171171,
            "recall": 0.8596112311015118
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.836419231164852,
            "auditor_fn_violation": 0.012188694751134458,
            "auditor_fp_violation": 0.019929574530149614,
            "ave_precision_score": 0.8371464954380774,
            "fpr": 0.17324561403508773,
            "logloss": 0.7466541798997117,
            "mae": 0.2694971783321708,
            "precision": 0.726643598615917,
            "recall": 0.8553971486761711
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8465826820270144,
            "auditor_fn_violation": 0.012235859770076792,
            "auditor_fp_violation": 0.025874235533950133,
            "ave_precision_score": 0.846841484795998,
            "fpr": 0.17014270032930845,
            "logloss": 0.7341571898491221,
            "mae": 0.263911692163311,
            "precision": 0.7207207207207207,
            "recall": 0.8639308855291576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 20300,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.8466186909316286,
            "auditor_fn_violation": 0.0054444920856111766,
            "auditor_fp_violation": 0.002851918989873733,
            "ave_precision_score": 0.8473147329391104,
            "fpr": 0.3059210526315789,
            "logloss": 0.6775790977254583,
            "mae": 0.35340254937029003,
            "precision": 0.6219512195121951,
            "recall": 0.9348268839103869
        },
        "train": {
            "accuracy": 0.6608122941822173,
            "auc_prc": 0.8611462657934443,
            "auditor_fn_violation": 0.006569573226677541,
            "auditor_fp_violation": 0.0004777912811667046,
            "ave_precision_score": 0.8613708945688778,
            "fpr": 0.3172338090010977,
            "logloss": 0.6745453480026393,
            "mae": 0.3551012725128693,
            "precision": 0.605191256830601,
            "recall": 0.9568034557235421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 20300,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8246394318934243,
            "auditor_fn_violation": 0.012836316861399934,
            "auditor_fp_violation": 0.012459890819685798,
            "ave_precision_score": 0.8250669757737037,
            "fpr": 0.17214912280701755,
            "logloss": 0.8230487106685355,
            "mae": 0.27535325035620173,
            "precision": 0.7279029462738301,
            "recall": 0.8553971486761711
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8306428090999614,
            "auditor_fn_violation": 0.015009732262033747,
            "auditor_fp_violation": 0.02234593068841148,
            "ave_precision_score": 0.8309856532857924,
            "fpr": 0.1756311745334797,
            "logloss": 0.8176822399802296,
            "mae": 0.2675817588881894,
            "precision": 0.7153024911032029,
            "recall": 0.8682505399568035
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8237270091215526,
            "auditor_fn_violation": 0.009071175903097868,
            "auditor_fp_violation": 0.013462620327540954,
            "ave_precision_score": 0.8240469196355636,
            "fpr": 0.16557017543859648,
            "logloss": 0.8769672728041865,
            "mae": 0.2735416580706055,
            "precision": 0.7303571428571428,
            "recall": 0.8329938900203666
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8342016335742561,
            "auditor_fn_violation": 0.01362279601605527,
            "auditor_fp_violation": 0.022331229418221733,
            "ave_precision_score": 0.8345085132475826,
            "fpr": 0.1690450054884742,
            "logloss": 0.8576661911464487,
            "mae": 0.26567259827726686,
            "precision": 0.7174311926605504,
            "recall": 0.8444924406047516
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8273076377678005,
            "auditor_fn_violation": 0.011337853289027049,
            "auditor_fp_violation": 0.011407675959494946,
            "ave_precision_score": 0.8277465690881964,
            "fpr": 0.16666666666666666,
            "logloss": 0.8130712438561232,
            "mae": 0.27668750818187055,
            "precision": 0.7309734513274336,
            "recall": 0.8411405295315683
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8345850536199142,
            "auditor_fn_violation": 0.01092004845978952,
            "auditor_fp_violation": 0.02265465736239612,
            "ave_precision_score": 0.8348867829256976,
            "fpr": 0.1734357848518112,
            "logloss": 0.7997782206542522,
            "mae": 0.2675709426413783,
            "precision": 0.7132486388384754,
            "recall": 0.8488120950323974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8311422501586576,
            "auditor_fn_violation": 0.013258387822917782,
            "auditor_fp_violation": 0.014879464099679126,
            "ave_precision_score": 0.8314504477523817,
            "fpr": 0.16447368421052633,
            "logloss": 0.7761996139093398,
            "mae": 0.26939245045600846,
            "precision": 0.7368421052631579,
            "recall": 0.8553971486761711
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8407668853292243,
            "auditor_fn_violation": 0.012930513308660884,
            "auditor_fp_violation": 0.021395248549474676,
            "ave_precision_score": 0.8410443601951588,
            "fpr": 0.1712403951701427,
            "logloss": 0.7701120500536516,
            "mae": 0.2648200060397569,
            "precision": 0.7179023508137432,
            "recall": 0.857451403887689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8257314168959187,
            "auditor_fn_violation": 0.01349063851073713,
            "auditor_fp_violation": 0.016671354752677423,
            "ave_precision_score": 0.8262682028126442,
            "fpr": 0.17434210526315788,
            "logloss": 0.8298096605916516,
            "mae": 0.27390435863006446,
            "precision": 0.7282051282051282,
            "recall": 0.8676171079429735
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8381847206051511,
            "auditor_fn_violation": 0.01407799560447898,
            "auditor_fp_violation": 0.021196781401913126,
            "ave_precision_score": 0.8384756869841811,
            "fpr": 0.18331503841931943,
            "logloss": 0.8152722273441596,
            "mae": 0.2672692263897136,
            "precision": 0.708041958041958,
            "recall": 0.8747300215982722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 20300,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.8075922664668265,
            "auditor_fn_violation": 0.00914710401257727,
            "auditor_fp_violation": 0.014246572488227704,
            "ave_precision_score": 0.8089638898511307,
            "fpr": 0.25877192982456143,
            "logloss": 0.8360049334426226,
            "mae": 0.34611666061911567,
            "precision": 0.650887573964497,
            "recall": 0.8961303462321792
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.8206503362592956,
            "auditor_fn_violation": 0.006787689696130566,
            "auditor_fp_violation": 0.013912302022894786,
            "ave_precision_score": 0.8210786002822104,
            "fpr": 0.2864983534577388,
            "logloss": 0.7093129704383159,
            "mae": 0.34093016802617815,
            "precision": 0.6200873362445415,
            "recall": 0.9200863930885529
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8276070271086629,
            "auditor_fn_violation": 0.007655339979276095,
            "auditor_fp_violation": 0.011105554861024302,
            "ave_precision_score": 0.8278922904495649,
            "fpr": 0.15899122807017543,
            "logloss": 0.7943246003474325,
            "mae": 0.27223588080911537,
            "precision": 0.7396768402154399,
            "recall": 0.8391038696537678
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8369684408723324,
            "auditor_fn_violation": 0.009158520885837367,
            "auditor_fp_violation": 0.0243502038576133,
            "ave_precision_score": 0.8372442869052383,
            "fpr": 0.16794731064763996,
            "logloss": 0.7833992186121521,
            "mae": 0.26862720392892586,
            "precision": 0.71875,
            "recall": 0.8444924406047516
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.824240852656593,
            "auditor_fn_violation": 0.008434719691285241,
            "auditor_fp_violation": 0.013811622286119096,
            "ave_precision_score": 0.8249354407297997,
            "fpr": 0.1611842105263158,
            "logloss": 0.820617788490155,
            "mae": 0.27782148139398244,
            "precision": 0.7346570397111913,
            "recall": 0.8289205702647657
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8325591180389913,
            "auditor_fn_violation": 0.012233488938887089,
            "auditor_fp_violation": 0.02373520072134233,
            "ave_precision_score": 0.8328610254210922,
            "fpr": 0.16575192096597147,
            "logloss": 0.8351724694407284,
            "mae": 0.2716081559857267,
            "precision": 0.7177570093457943,
            "recall": 0.8293736501079914
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8277978148870634,
            "auditor_fn_violation": 0.00793002108121628,
            "auditor_fp_violation": 0.01512428636912948,
            "ave_precision_score": 0.828233208257452,
            "fpr": 0.16666666666666666,
            "logloss": 0.7738905259128761,
            "mae": 0.2786916322383161,
            "precision": 0.7280858676207513,
            "recall": 0.8289205702647657
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8322181562511461,
            "auditor_fn_violation": 0.011460597971042671,
            "auditor_fp_violation": 0.02389691469342952,
            "ave_precision_score": 0.8325401923725826,
            "fpr": 0.16794731064763996,
            "logloss": 0.7871409642557611,
            "mae": 0.2726186648716654,
            "precision": 0.7171903881700554,
            "recall": 0.838012958963283
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.825897432878488,
            "auditor_fn_violation": 0.012275788759066713,
            "auditor_fp_violation": 0.011665520690086265,
            "ave_precision_score": 0.8263108744066004,
            "fpr": 0.1699561403508772,
            "logloss": 0.8201758262751166,
            "mae": 0.2755713699737255,
            "precision": 0.7309027777777778,
            "recall": 0.8574338085539714
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8313661479624022,
            "auditor_fn_violation": 0.014353012022484965,
            "auditor_fp_violation": 0.02234593068841148,
            "ave_precision_score": 0.8317097474397193,
            "fpr": 0.1756311745334797,
            "logloss": 0.8156316819608351,
            "mae": 0.26816492079389,
            "precision": 0.714795008912656,
            "recall": 0.8660907127429806
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8282304856413274,
            "auditor_fn_violation": 0.009685300318004793,
            "auditor_fp_violation": 0.016457786389965423,
            "ave_precision_score": 0.8288243745309947,
            "fpr": 0.13596491228070176,
            "logloss": 0.8075666706938553,
            "mae": 0.27245421118791435,
            "precision": 0.7524950099800399,
            "recall": 0.7678207739307535
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8348038995253717,
            "auditor_fn_violation": 0.010144786660755403,
            "auditor_fp_violation": 0.023659244158695313,
            "ave_precision_score": 0.8351245626868173,
            "fpr": 0.13611416026344675,
            "logloss": 0.7967960097702786,
            "mae": 0.26328829048141267,
            "precision": 0.746938775510204,
            "recall": 0.7904967602591793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8273912557486458,
            "auditor_fn_violation": 0.01330975095580091,
            "auditor_fp_violation": 0.01567383422927866,
            "ave_precision_score": 0.827733535108181,
            "fpr": 0.16666666666666666,
            "logloss": 0.8161016105385954,
            "mae": 0.27700797267311,
            "precision": 0.7300177619893428,
            "recall": 0.8370672097759674
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8343472198014721,
            "auditor_fn_violation": 0.013866991628595069,
            "auditor_fp_violation": 0.02359553865453975,
            "ave_precision_score": 0.8346521020968298,
            "fpr": 0.17014270032930845,
            "logloss": 0.8013189014894033,
            "mae": 0.26682075540821376,
            "precision": 0.7161172161172161,
            "recall": 0.8444924406047516
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 20300,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.8215488006763714,
            "auditor_fn_violation": 0.009707632114910495,
            "auditor_fp_violation": 0.015332645747385094,
            "ave_precision_score": 0.8223174633138839,
            "fpr": 0.26644736842105265,
            "logloss": 0.7053142958542795,
            "mae": 0.3354145300649211,
            "precision": 0.6473149492017417,
            "recall": 0.9083503054989817
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.839620781919943,
            "auditor_fn_violation": 0.00708878525722333,
            "auditor_fp_violation": 0.01271169829073234,
            "ave_precision_score": 0.8409271035728414,
            "fpr": 0.2854006586169045,
            "logloss": 0.6877914161039727,
            "mae": 0.3334913411845454,
            "precision": 0.6258992805755396,
            "recall": 0.9395248380129589
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.804015744370998,
            "auditor_fn_violation": 0.0195135241362061,
            "auditor_fp_violation": 0.006980039171563118,
            "ave_precision_score": 0.804407041235533,
            "fpr": 0.13925438596491227,
            "logloss": 0.9116737684980674,
            "mae": 0.2855438391659768,
            "precision": 0.748015873015873,
            "recall": 0.7678207739307535
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8147011346316179,
            "auditor_fn_violation": 0.009409828991946288,
            "auditor_fp_violation": 0.020848851340755845,
            "ave_precision_score": 0.8150675526040043,
            "fpr": 0.14818880351262348,
            "logloss": 0.897964187953167,
            "mae": 0.27419620336591355,
            "precision": 0.7227926078028748,
            "recall": 0.7602591792656588
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 20300,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8300712287793874,
            "auditor_fn_violation": 0.006400292993175405,
            "auditor_fp_violation": 0.015202421135975336,
            "ave_precision_score": 0.8306510399818605,
            "fpr": 0.15350877192982457,
            "logloss": 0.7822821054726308,
            "mae": 0.2778934501982714,
            "precision": 0.7392923649906891,
            "recall": 0.8085539714867617
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.836635436011011,
            "auditor_fn_violation": 0.013243463025702186,
            "auditor_fp_violation": 0.024024325701740634,
            "ave_precision_score": 0.8369321545550459,
            "fpr": 0.15477497255762898,
            "logloss": 0.7931455993917029,
            "mae": 0.26900922858830273,
            "precision": 0.7277992277992278,
            "recall": 0.8142548596112311
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.852931969063252,
            "auditor_fn_violation": 0.004888430342659092,
            "auditor_fp_violation": 0.011152435721131813,
            "ave_precision_score": 0.854185759314497,
            "fpr": 0.20942982456140352,
            "logloss": 0.545366986092581,
            "mae": 0.3240841394857087,
            "precision": 0.6977848101265823,
            "recall": 0.8981670061099797
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8512696274556679,
            "auditor_fn_violation": 0.004177404556263381,
            "auditor_fp_violation": 0.019660498667084837,
            "ave_precision_score": 0.8515706164216419,
            "fpr": 0.2349066959385291,
            "logloss": 0.562293340223993,
            "mae": 0.3283588738900402,
            "precision": 0.6666666666666666,
            "recall": 0.9244060475161987
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 20300,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8294126139078573,
            "auditor_fn_violation": 0.010739361131954126,
            "auditor_fp_violation": 0.01445753635871151,
            "ave_precision_score": 0.8299345215969597,
            "fpr": 0.16885964912280702,
            "logloss": 0.8166165019289732,
            "mae": 0.27212050204157984,
            "precision": 0.733102253032929,
            "recall": 0.8615071283095723
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8412315111509878,
            "auditor_fn_violation": 0.013819575004800938,
            "auditor_fp_violation": 0.022203818409910613,
            "ave_precision_score": 0.8415141726945052,
            "fpr": 0.18221734357848518,
            "logloss": 0.8021796226367097,
            "mae": 0.2654009751133187,
            "precision": 0.7061946902654868,
            "recall": 0.8617710583153347
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8195341367285964,
            "auditor_fn_violation": 0.008481616464787223,
            "auditor_fp_violation": 0.012735966995874488,
            "ave_precision_score": 0.8202784271058426,
            "fpr": 0.16337719298245615,
            "logloss": 0.8769604457690848,
            "mae": 0.2773771769073399,
            "precision": 0.7295825771324864,
            "recall": 0.8187372708757638
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8268420772073268,
            "auditor_fn_violation": 0.011700051921203056,
            "auditor_fp_violation": 0.023267210286968803,
            "ave_precision_score": 0.8270915023787828,
            "fpr": 0.1668496158068057,
            "logloss": 0.8883819575525218,
            "mae": 0.2687841328931316,
            "precision": 0.7185185185185186,
            "recall": 0.838012958963283
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8337860916429967,
            "auditor_fn_violation": 0.015444670739986423,
            "auditor_fp_violation": 0.01939565362336959,
            "ave_precision_score": 0.8342393635264279,
            "fpr": 0.16337719298245615,
            "logloss": 0.843065361302071,
            "mae": 0.27180321145885106,
            "precision": 0.7281021897810219,
            "recall": 0.8126272912423625
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8322486566820261,
            "auditor_fn_violation": 0.00849705898390917,
            "auditor_fp_violation": 0.026577446291359573,
            "ave_precision_score": 0.8325557427837288,
            "fpr": 0.17453347969264543,
            "logloss": 0.8505642540507173,
            "mae": 0.2672748541174805,
            "precision": 0.7050092764378478,
            "recall": 0.8207343412526998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8087230902281903,
            "auditor_fn_violation": 0.019819469753814267,
            "auditor_fp_violation": 0.014775284410551323,
            "ave_precision_score": 0.8090062364751098,
            "fpr": 0.14802631578947367,
            "logloss": 0.9090891179117565,
            "mae": 0.28917927127381793,
            "precision": 0.735812133072407,
            "recall": 0.7657841140529531
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8084565337929002,
            "auditor_fn_violation": 0.013463950326344917,
            "auditor_fp_violation": 0.02465893053159793,
            "ave_precision_score": 0.8088647511033731,
            "fpr": 0.15477497255762898,
            "logloss": 0.927834129850771,
            "mae": 0.2831460785772956,
            "precision": 0.7134146341463414,
            "recall": 0.7580993520518359
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.828933181153107,
            "auditor_fn_violation": 0.007432022010219031,
            "auditor_fp_violation": 0.01951546026586657,
            "ave_precision_score": 0.8293650386486169,
            "fpr": 0.1962719298245614,
            "logloss": 0.6011770884245718,
            "mae": 0.3305286261530822,
            "precision": 0.7016666666666667,
            "recall": 0.8574338085539714
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.8394237270711586,
            "auditor_fn_violation": 0.0006970243697737999,
            "auditor_fp_violation": 0.017849792222047987,
            "ave_precision_score": 0.8397057309054157,
            "fpr": 0.21844127332601537,
            "logloss": 0.5754253417120156,
            "mae": 0.32813879297192605,
            "precision": 0.6732348111658456,
            "recall": 0.8855291576673866
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8115919824270801,
            "auditor_fn_violation": 0.006339997141529997,
            "auditor_fp_violation": 0.014056444555569453,
            "ave_precision_score": 0.8122347471753154,
            "fpr": 0.16885964912280702,
            "logloss": 0.9221279903165356,
            "mae": 0.2792308145458225,
            "precision": 0.7264653641207816,
            "recall": 0.8329938900203666
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8202199191776639,
            "auditor_fn_violation": 0.010204057440498062,
            "auditor_fp_violation": 0.024526619099890234,
            "ave_precision_score": 0.8206067294449622,
            "fpr": 0.1690450054884742,
            "logloss": 0.8960604764106,
            "mae": 0.2728839614684322,
            "precision": 0.7142857142857143,
            "recall": 0.8315334773218143
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 20300,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8374335002726986,
            "auditor_fn_violation": 0.016532229249294317,
            "auditor_fp_violation": 0.01711151393924241,
            "ave_precision_score": 0.8380359032195428,
            "fpr": 0.16228070175438597,
            "logloss": 0.7965290807554616,
            "mae": 0.2674444159459953,
            "precision": 0.7380530973451327,
            "recall": 0.8492871690427699
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8446818906389261,
            "auditor_fn_violation": 0.015839523178431132,
            "auditor_fp_violation": 0.029718617688568295,
            "ave_precision_score": 0.8448720310749083,
            "fpr": 0.16575192096597147,
            "logloss": 0.7717360824424452,
            "mae": 0.25910342454095914,
            "precision": 0.7229357798165138,
            "recall": 0.8509719222462203
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8234801384631774,
            "auditor_fn_violation": 0.009015346410833605,
            "auditor_fp_violation": 0.014968016835437763,
            "ave_precision_score": 0.8237865802747695,
            "fpr": 0.17214912280701755,
            "logloss": 0.8662898518324432,
            "mae": 0.27372860227158236,
            "precision": 0.7255244755244755,
            "recall": 0.845213849287169
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8292403954112877,
            "auditor_fn_violation": 0.015244444549814724,
            "auditor_fp_violation": 0.022522345930688425,
            "ave_precision_score": 0.8295932801958152,
            "fpr": 0.1800219538968167,
            "logloss": 0.8583124135517407,
            "mae": 0.2682933495360789,
            "precision": 0.7071428571428572,
            "recall": 0.8552915766738661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8281703497963682,
            "auditor_fn_violation": 0.009127005395362136,
            "auditor_fp_violation": 0.012337479684960632,
            "ave_precision_score": 0.8287806368236326,
            "fpr": 0.15789473684210525,
            "logloss": 0.7695466694666048,
            "mae": 0.27291625479645654,
            "precision": 0.7367458866544789,
            "recall": 0.8207739307535642
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8400009720596135,
            "auditor_fn_violation": 0.015159094626985278,
            "auditor_fp_violation": 0.021378097067586647,
            "ave_precision_score": 0.8402856101908158,
            "fpr": 0.16355653128430298,
            "logloss": 0.7473688285700366,
            "mae": 0.2632648155613928,
            "precision": 0.7255985267034991,
            "recall": 0.8509719222462203
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8237074183814679,
            "auditor_fn_violation": 0.012713491978418551,
            "auditor_fp_violation": 0.010196587073384179,
            "ave_precision_score": 0.8243308974203324,
            "fpr": 0.12938596491228072,
            "logloss": 0.8239488295814775,
            "mae": 0.2795161142810965,
            "precision": 0.7581967213114754,
            "recall": 0.7535641547861507
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8229958926596352,
            "auditor_fn_violation": 0.008798154545001935,
            "auditor_fp_violation": 0.017242139720871883,
            "ave_precision_score": 0.8233449403223221,
            "fpr": 0.13721185510428102,
            "logloss": 0.8752778350580707,
            "mae": 0.2706003501722301,
            "precision": 0.7395833333333334,
            "recall": 0.7667386609071274
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8202711433582933,
            "auditor_fn_violation": 0.010578572194233042,
            "auditor_fp_violation": 0.010918031420594247,
            "ave_precision_score": 0.8207526238610803,
            "fpr": 0.16776315789473684,
            "logloss": 0.8559325670539439,
            "mae": 0.2809657768770854,
            "precision": 0.7258064516129032,
            "recall": 0.824847250509165
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8268298882420348,
            "auditor_fn_violation": 0.012491909538565127,
            "auditor_fp_violation": 0.023122647796769643,
            "ave_precision_score": 0.8271653444082309,
            "fpr": 0.17233809001097694,
            "logloss": 0.8661462861450224,
            "mae": 0.2705066379306247,
            "precision": 0.7113970588235294,
            "recall": 0.8358531317494601
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8322313719315345,
            "auditor_fn_violation": 0.0057080072890985135,
            "auditor_fp_violation": 0.016890132099845818,
            "ave_precision_score": 0.8328301487207191,
            "fpr": 0.15679824561403508,
            "logloss": 0.794726530120807,
            "mae": 0.2723970933018325,
            "precision": 0.7371323529411765,
            "recall": 0.8167006109979633
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8428281199866887,
            "auditor_fn_violation": 0.01374845006910973,
            "auditor_fp_violation": 0.02325005880508076,
            "ave_precision_score": 0.8430980427448814,
            "fpr": 0.15916575192096596,
            "logloss": 0.7715185559332551,
            "mae": 0.26278280612266136,
            "precision": 0.7279549718574109,
            "recall": 0.838012958963283
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8304784381663897,
            "auditor_fn_violation": 0.006083181477114374,
            "auditor_fp_violation": 0.016890132099845818,
            "ave_precision_score": 0.8315029464162083,
            "fpr": 0.15679824561403508,
            "logloss": 0.8035657529956087,
            "mae": 0.2732493398085809,
            "precision": 0.7376146788990826,
            "recall": 0.8187372708757638
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8420247075790567,
            "auditor_fn_violation": 0.01167634360930599,
            "auditor_fp_violation": 0.02204210443782343,
            "ave_precision_score": 0.8423001202425051,
            "fpr": 0.1602634467618002,
            "logloss": 0.7827103808081581,
            "mae": 0.2629099864977156,
            "precision": 0.7271028037383177,
            "recall": 0.8401727861771058
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8284713774333683,
            "auditor_fn_violation": 0.012644263408010867,
            "auditor_fp_violation": 0.013327186731674793,
            "ave_precision_score": 0.8284865404795574,
            "fpr": 0.16666666666666666,
            "logloss": 0.7895051699499209,
            "mae": 0.2736547892611337,
            "precision": 0.735191637630662,
            "recall": 0.8594704684317719
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8399503142298104,
            "auditor_fn_violation": 0.015597698397081033,
            "auditor_fp_violation": 0.02171867649364905,
            "ave_precision_score": 0.8402379184346426,
            "fpr": 0.1756311745334797,
            "logloss": 0.7767644325443752,
            "mae": 0.2665823388797008,
            "precision": 0.714795008912656,
            "recall": 0.8660907127429806
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8256936213356254,
            "auditor_fn_violation": 0.0050291206631650435,
            "auditor_fp_violation": 0.014691940659249073,
            "ave_precision_score": 0.8258160681661233,
            "fpr": 0.15570175438596492,
            "logloss": 0.81719812151691,
            "mae": 0.272360498982867,
            "precision": 0.7413479052823315,
            "recall": 0.8289205702647657
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8316586014675282,
            "auditor_fn_violation": 0.008366663268475296,
            "auditor_fp_violation": 0.022385134075584137,
            "ave_precision_score": 0.8319645663477279,
            "fpr": 0.15806805708013172,
            "logloss": 0.8229122428896313,
            "mae": 0.2670678634518609,
            "precision": 0.7277882797731569,
            "recall": 0.8315334773218143
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8296573602761612,
            "auditor_fn_violation": 0.012787186908207385,
            "auditor_fp_violation": 0.016465599866650006,
            "ave_precision_score": 0.830181021943478,
            "fpr": 0.16666666666666666,
            "logloss": 0.8096392336773832,
            "mae": 0.2722707065805981,
            "precision": 0.7361111111111112,
            "recall": 0.8635437881873728
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8403264124770747,
            "auditor_fn_violation": 0.01363939183438322,
            "auditor_fp_violation": 0.020476419162615653,
            "ave_precision_score": 0.8406072133264904,
            "fpr": 0.18111964873765093,
            "logloss": 0.7938802219840289,
            "mae": 0.26572230886919784,
            "precision": 0.7100175746924429,
            "recall": 0.8725701943844493
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8311443782103494,
            "auditor_fn_violation": 0.013258387822917782,
            "auditor_fp_violation": 0.014879464099679126,
            "ave_precision_score": 0.8314101495291917,
            "fpr": 0.16447368421052633,
            "logloss": 0.7761050366157175,
            "mae": 0.2693921928541201,
            "precision": 0.7368421052631579,
            "recall": 0.8553971486761711
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8407619257005314,
            "auditor_fn_violation": 0.012930513308660884,
            "auditor_fp_violation": 0.021395248549474676,
            "ave_precision_score": 0.8410383775673533,
            "fpr": 0.1712403951701427,
            "logloss": 0.7700055799837195,
            "mae": 0.2648318133761111,
            "precision": 0.7179023508137432,
            "recall": 0.857451403887689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8285297346152212,
            "auditor_fn_violation": 0.012644263408010867,
            "auditor_fp_violation": 0.013327186731674793,
            "ave_precision_score": 0.8285447681963081,
            "fpr": 0.16666666666666666,
            "logloss": 0.7891855518430266,
            "mae": 0.2736671019117586,
            "precision": 0.735191637630662,
            "recall": 0.8594704684317719
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8399482806088855,
            "auditor_fn_violation": 0.015597698397081033,
            "auditor_fp_violation": 0.02171867649364905,
            "ave_precision_score": 0.8402370568412004,
            "fpr": 0.1756311745334797,
            "logloss": 0.7764550204903315,
            "mae": 0.26658447628225107,
            "precision": 0.714795008912656,
            "recall": 0.8660907127429806
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 20300,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8309994747293659,
            "auditor_fn_violation": 0.010815289241433525,
            "auditor_fp_violation": 0.015004479726632495,
            "ave_precision_score": 0.831318670616264,
            "fpr": 0.16447368421052633,
            "logloss": 0.8101916771277723,
            "mae": 0.27088420348982195,
            "precision": 0.7307001795332136,
            "recall": 0.8289205702647657
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8291522041028683,
            "auditor_fn_violation": 0.012745588475863753,
            "auditor_fp_violation": 0.027819703622392975,
            "ave_precision_score": 0.8294657521902886,
            "fpr": 0.1690450054884742,
            "logloss": 0.833801108018256,
            "mae": 0.26730700230226195,
            "precision": 0.7142857142857143,
            "recall": 0.8315334773218143
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7496131223209509,
            "auditor_fn_violation": 0.016605924179083145,
            "auditor_fp_violation": 0.041830749677042964,
            "ave_precision_score": 0.7505940903912798,
            "fpr": 0.1787280701754386,
            "logloss": 0.8408890844736917,
            "mae": 0.29963478608359556,
            "precision": 0.7165217391304348,
            "recall": 0.8391038696537678
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7262480220690333,
            "auditor_fn_violation": 0.012634159409947538,
            "auditor_fp_violation": 0.04812215775442999,
            "ave_precision_score": 0.728072096472638,
            "fpr": 0.20856201975850713,
            "logloss": 0.9312011463001548,
            "mae": 0.31454744659605166,
            "precision": 0.6684118673647469,
            "recall": 0.8272138228941684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8285348002216799,
            "auditor_fn_violation": 0.011252992460785368,
            "auditor_fp_violation": 0.01141028045172314,
            "ave_precision_score": 0.8290516508736825,
            "fpr": 0.15899122807017543,
            "logloss": 0.8055317041984946,
            "mae": 0.27038361766689734,
            "precision": 0.7368421052631579,
            "recall": 0.8268839103869654
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8372901303176853,
            "auditor_fn_violation": 0.012131543197729692,
            "auditor_fp_violation": 0.021243335424180648,
            "ave_precision_score": 0.8375843676279406,
            "fpr": 0.15697036223929747,
            "logloss": 0.7934994508408264,
            "mae": 0.26403905821508983,
            "precision": 0.730188679245283,
            "recall": 0.8358531317494601
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8278238351157697,
            "auditor_fn_violation": 0.010929181405652627,
            "auditor_fp_violation": 0.014668500229195327,
            "ave_precision_score": 0.8289422778066011,
            "fpr": 0.16666666666666666,
            "logloss": 0.8145057351236876,
            "mae": 0.2724082868553436,
            "precision": 0.7314487632508834,
            "recall": 0.8431771894093686
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8388926403751688,
            "auditor_fn_violation": 0.01357063772988172,
            "auditor_fp_violation": 0.020314705190528463,
            "ave_precision_score": 0.8391711755303903,
            "fpr": 0.17892425905598244,
            "logloss": 0.7997864849986818,
            "mae": 0.2661775224165889,
            "precision": 0.7073608617594255,
            "recall": 0.8509719222462203
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8257730623933763,
            "auditor_fn_violation": 0.0050291206631650435,
            "auditor_fp_violation": 0.01072790348793599,
            "ave_precision_score": 0.8265741979595471,
            "fpr": 0.15570175438596492,
            "logloss": 0.8165454565407478,
            "mae": 0.2721043501860388,
            "precision": 0.7413479052823315,
            "recall": 0.8289205702647657
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8320216317569817,
            "auditor_fn_violation": 0.008366663268475296,
            "auditor_fp_violation": 0.02269876117296535,
            "ave_precision_score": 0.8323318238078622,
            "fpr": 0.15916575192096596,
            "logloss": 0.82090752157926,
            "mae": 0.2669657241911076,
            "precision": 0.7264150943396226,
            "recall": 0.8315334773218143
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8132771367168166,
            "auditor_fn_violation": 0.011683996141065502,
            "auditor_fp_violation": 0.016861482685335674,
            "ave_precision_score": 0.813678806215854,
            "fpr": 0.16666666666666666,
            "logloss": 0.8560809013757096,
            "mae": 0.2826240144402353,
            "precision": 0.7295373665480427,
            "recall": 0.835030549898167
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8259369242785624,
            "auditor_fn_violation": 0.014559274335989456,
            "auditor_fp_violation": 0.02494805551199624,
            "ave_precision_score": 0.8267030988649873,
            "fpr": 0.17014270032930845,
            "logloss": 0.8232323890427363,
            "mae": 0.2719823373544743,
            "precision": 0.716636197440585,
            "recall": 0.8466522678185745
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 20300,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8270504340492697,
            "auditor_fn_violation": 0.012485707649980348,
            "auditor_fp_violation": 0.012183814643497104,
            "ave_precision_score": 0.8274565408321087,
            "fpr": 0.1699561403508772,
            "logloss": 0.8171971672219477,
            "mae": 0.27487765730075675,
            "precision": 0.7294938917975567,
            "recall": 0.8513238289205702
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8330495729864651,
            "auditor_fn_violation": 0.011489047945319153,
            "auditor_fp_violation": 0.019846714756154935,
            "ave_precision_score": 0.8333623080422617,
            "fpr": 0.1800219538968167,
            "logloss": 0.8109788736448205,
            "mae": 0.26713425115562583,
            "precision": 0.7071428571428572,
            "recall": 0.8552915766738661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8316348351470721,
            "auditor_fn_violation": 0.011424947296959306,
            "auditor_fp_violation": 0.013327186731674793,
            "ave_precision_score": 0.8322406302266767,
            "fpr": 0.16666666666666666,
            "logloss": 0.7840934724080996,
            "mae": 0.2716101943259564,
            "precision": 0.7328646748681898,
            "recall": 0.8492871690427699
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8432414682778391,
            "auditor_fn_violation": 0.013914408252389205,
            "auditor_fp_violation": 0.02342892425905598,
            "ave_precision_score": 0.8435128446250386,
            "fpr": 0.1690450054884742,
            "logloss": 0.7644263855071175,
            "mae": 0.26361413807484313,
            "precision": 0.7210144927536232,
            "recall": 0.8596112311015118
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8273256541668683,
            "auditor_fn_violation": 0.011337853289027049,
            "auditor_fp_violation": 0.011407675959494946,
            "ave_precision_score": 0.8277728444836236,
            "fpr": 0.16666666666666666,
            "logloss": 0.8128562331493474,
            "mae": 0.2766934587158526,
            "precision": 0.7309734513274336,
            "recall": 0.8411405295315683
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8345741989480934,
            "auditor_fn_violation": 0.01092004845978952,
            "auditor_fp_violation": 0.02265465736239612,
            "ave_precision_score": 0.834875978864575,
            "fpr": 0.1734357848518112,
            "logloss": 0.7996163040954023,
            "mae": 0.26756598528726355,
            "precision": 0.7132486388384754,
            "recall": 0.8488120950323974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8258721121994712,
            "auditor_fn_violation": 0.0036691142316075385,
            "auditor_fp_violation": 0.0070607784306371615,
            "ave_precision_score": 0.8264104498797424,
            "fpr": 0.1600877192982456,
            "logloss": 0.793482434614835,
            "mae": 0.2731839153967434,
            "precision": 0.737410071942446,
            "recall": 0.835030549898167
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8376905025238282,
            "auditor_fn_violation": 0.0074610057540073025,
            "auditor_fp_violation": 0.02274286498353458,
            "ave_precision_score": 0.8379052487487959,
            "fpr": 0.1690450054884742,
            "logloss": 0.7769151884701375,
            "mae": 0.2673080100205163,
            "precision": 0.717948717948718,
            "recall": 0.8466522678185745
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8295855830907598,
            "auditor_fn_violation": 0.01224229106370815,
            "auditor_fp_violation": 0.01665051881485186,
            "ave_precision_score": 0.8303568065117043,
            "fpr": 0.16885964912280702,
            "logloss": 0.7788484128038463,
            "mae": 0.27297170531294784,
            "precision": 0.7335640138408305,
            "recall": 0.8635437881873728
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8415603416963532,
            "auditor_fn_violation": 0.013316958792583091,
            "auditor_fp_violation": 0.02171867649364905,
            "ave_precision_score": 0.8418433946818394,
            "fpr": 0.1756311745334797,
            "logloss": 0.7662695154844148,
            "mae": 0.26560562804891147,
            "precision": 0.7163120567375887,
            "recall": 0.8725701943844493
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 20300,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.8263575418755824,
            "auditor_fn_violation": 0.009439650552042022,
            "auditor_fp_violation": 0.013144872275701129,
            "ave_precision_score": 0.826842680536098,
            "fpr": 0.22587719298245615,
            "logloss": 0.5988022579602951,
            "mae": 0.35393087212236296,
            "precision": 0.6755905511811023,
            "recall": 0.8737270875763747
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.8395532589642418,
            "auditor_fn_violation": 0.006960760372979163,
            "auditor_fp_violation": 0.013503116669280231,
            "ave_precision_score": 0.8398130348066735,
            "fpr": 0.24259055982436883,
            "logloss": 0.5739739053604854,
            "mae": 0.3492180469009359,
            "precision": 0.6541471048513302,
            "recall": 0.9028077753779697
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 20300,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8295227014719804,
            "auditor_fn_violation": 0.016342408975595815,
            "auditor_fp_violation": 0.012715131058048935,
            "ave_precision_score": 0.8299743222211392,
            "fpr": 0.12719298245614036,
            "logloss": 0.7775421352754512,
            "mae": 0.27575586750768033,
            "precision": 0.7627811860940695,
            "recall": 0.7596741344195519
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8328594989151015,
            "auditor_fn_violation": 0.012992154919593265,
            "auditor_fp_violation": 0.01993002195389682,
            "ave_precision_score": 0.8331366455429516,
            "fpr": 0.12952799121844127,
            "logloss": 0.7646326603278791,
            "mae": 0.2658955869958856,
            "precision": 0.7531380753138075,
            "recall": 0.7775377969762419
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 20300,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8250273989813044,
            "auditor_fn_violation": 0.011576803515918105,
            "auditor_fp_violation": 0.016733862566154105,
            "ave_precision_score": 0.8253004941727811,
            "fpr": 0.1600877192982456,
            "logloss": 0.8299329547262263,
            "mae": 0.2742224533842818,
            "precision": 0.7364620938628159,
            "recall": 0.8309572301425662
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8364094988309437,
            "auditor_fn_violation": 0.015412773564283904,
            "auditor_fp_violation": 0.02342892425905598,
            "ave_precision_score": 0.8367051662863093,
            "fpr": 0.1690450054884742,
            "logloss": 0.8107710081434234,
            "mae": 0.26453941195348873,
            "precision": 0.7184643510054844,
            "recall": 0.8488120950323974
        }
    }
]
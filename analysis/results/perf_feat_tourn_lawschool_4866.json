[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8447040558313947,
            "auditor_fn_violation": 0.028225310269662136,
            "auditor_fp_violation": 0.021210335159933737,
            "ave_precision_score": 0.8449778884967356,
            "fpr": 0.13596491228070176,
            "logloss": 0.8861747245059372,
            "mae": 0.2673895888500679,
            "precision": 0.7578125,
            "recall": 0.7775551102204409
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8161098040345304,
            "auditor_fn_violation": 0.02354856998106175,
            "auditor_fp_violation": 0.038650413079900636,
            "ave_precision_score": 0.8165825212193863,
            "fpr": 0.16465422612513722,
            "logloss": 0.8612773337172228,
            "mae": 0.2780468838169196,
            "precision": 0.7053045186640472,
            "recall": 0.789010989010989
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8431055184952715,
            "auditor_fn_violation": 0.027508965299019094,
            "auditor_fp_violation": 0.019455418206533286,
            "ave_precision_score": 0.8433635864101368,
            "fpr": 0.13596491228070176,
            "logloss": 0.909908438011602,
            "mae": 0.26664164071338203,
            "precision": 0.7568627450980392,
            "recall": 0.7735470941883767
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8138256371083583,
            "auditor_fn_violation": 0.02512635553250263,
            "auditor_fp_violation": 0.03967589115489052,
            "ave_precision_score": 0.8142619502360922,
            "fpr": 0.16465422612513722,
            "logloss": 0.8849831794539678,
            "mae": 0.2772672159386256,
            "precision": 0.7047244094488189,
            "recall": 0.7868131868131868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 4866,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8524970366020714,
            "auditor_fn_violation": 0.00896529901909082,
            "auditor_fp_violation": 0.014124293785310736,
            "ave_precision_score": 0.8527206370169462,
            "fpr": 0.13048245614035087,
            "logloss": 0.7421929357223345,
            "mae": 0.26085189185668073,
            "precision": 0.7689320388349514,
            "recall": 0.7935871743486974
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8235407784656937,
            "auditor_fn_violation": 0.008371430983944708,
            "auditor_fp_violation": 0.02906724825235427,
            "ave_precision_score": 0.8238836449760085,
            "fpr": 0.15477497255762898,
            "logloss": 0.7381178254907031,
            "mae": 0.2756797668529124,
            "precision": 0.7157258064516129,
            "recall": 0.7802197802197802
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8464742388521849,
            "auditor_fn_violation": 0.024491966388918186,
            "auditor_fp_violation": 0.018980183509621516,
            "ave_precision_score": 0.846738507949185,
            "fpr": 0.13486842105263158,
            "logloss": 0.8936389998881757,
            "mae": 0.2655602296310797,
            "precision": 0.7602339181286549,
            "recall": 0.781563126252505
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8165377999215008,
            "auditor_fn_violation": 0.025683646759387703,
            "auditor_fp_violation": 0.04162092938163191,
            "ave_precision_score": 0.8169489098839076,
            "fpr": 0.1602634467618002,
            "logloss": 0.8707429075744754,
            "mae": 0.27527154424110994,
            "precision": 0.7108910891089109,
            "recall": 0.789010989010989
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8284279753373273,
            "auditor_fn_violation": 0.05022105614738248,
            "auditor_fp_violation": 0.044664096682383934,
            "ave_precision_score": 0.8296633432680045,
            "fpr": 0.15021929824561403,
            "logloss": 0.6450494557273425,
            "mae": 0.32086342007520025,
            "precision": 0.7344961240310077,
            "recall": 0.7595190380761523
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.8258673636925996,
            "auditor_fn_violation": 0.04954101880556327,
            "auditor_fp_violation": 0.049239798178211724,
            "ave_precision_score": 0.8261586342373878,
            "fpr": 0.18551042810098792,
            "logloss": 0.6150686141772657,
            "mae": 0.3222521076980965,
            "precision": 0.6835205992509363,
            "recall": 0.8021978021978022
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.844394721983011,
            "auditor_fn_violation": 0.0062647224273107704,
            "auditor_fp_violation": 0.010370205173951827,
            "ave_precision_score": 0.8446205099463604,
            "fpr": 0.11513157894736842,
            "logloss": 0.8270480191907286,
            "mae": 0.2726785357018531,
            "precision": 0.777542372881356,
            "recall": 0.7354709418837675
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8130366927039521,
            "auditor_fn_violation": 0.011568014861099391,
            "auditor_fp_violation": 0.02294808095980897,
            "ave_precision_score": 0.8134032373242068,
            "fpr": 0.132821075740944,
            "logloss": 0.8037222795496958,
            "mae": 0.2820665155443206,
            "precision": 0.7317073170731707,
            "recall": 0.7252747252747253
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8423182340652111,
            "auditor_fn_violation": 0.026032327813521782,
            "auditor_fp_violation": 0.024061743341404363,
            "ave_precision_score": 0.8425828487756998,
            "fpr": 0.13596491228070176,
            "logloss": 0.9188707165295892,
            "mae": 0.2689784148062988,
            "precision": 0.7578125,
            "recall": 0.7775551102204409
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8096429240182661,
            "auditor_fn_violation": 0.02507086766142749,
            "auditor_fp_violation": 0.03713386099716911,
            "ave_precision_score": 0.8105701737574,
            "fpr": 0.16465422612513722,
            "logloss": 0.9016707409951042,
            "mae": 0.2782422201591613,
            "precision": 0.7076023391812866,
            "recall": 0.7978021978021979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8492079403916322,
            "auditor_fn_violation": 0.02275823576978518,
            "auditor_fp_violation": 0.020543944607280918,
            "ave_precision_score": 0.8494705218570985,
            "fpr": 0.13267543859649122,
            "logloss": 0.8747588792045697,
            "mae": 0.2639569555884886,
            "precision": 0.7650485436893204,
            "recall": 0.7895791583166333
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8113000264507115,
            "auditor_fn_violation": 0.02441466327306064,
            "auditor_fp_violation": 0.03813045236582125,
            "ave_precision_score": 0.8126949169525931,
            "fpr": 0.16355653128430298,
            "logloss": 0.8643621050751044,
            "mae": 0.27661739594983864,
            "precision": 0.7066929133858267,
            "recall": 0.789010989010989
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.844252360582456,
            "auditor_fn_violation": 0.027128819041591958,
            "auditor_fp_violation": 0.01993065290344505,
            "ave_precision_score": 0.8445254444780812,
            "fpr": 0.13706140350877194,
            "logloss": 0.9068961067453927,
            "mae": 0.2664680557678753,
            "precision": 0.7563352826510721,
            "recall": 0.7775551102204409
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8139235715070972,
            "auditor_fn_violation": 0.025683646759387703,
            "auditor_fp_violation": 0.03904760529204461,
            "ave_precision_score": 0.8144239980814996,
            "fpr": 0.16575192096597147,
            "logloss": 0.8806597202542806,
            "mae": 0.27662829241731474,
            "precision": 0.703921568627451,
            "recall": 0.789010989010989
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8471027049541366,
            "auditor_fn_violation": 0.024421650318180225,
            "auditor_fp_violation": 0.020676691729323307,
            "ave_precision_score": 0.847356602527932,
            "fpr": 0.12938596491228072,
            "logloss": 0.902518648617599,
            "mae": 0.26580098789982204,
            "precision": 0.7663366336633664,
            "recall": 0.7755511022044088
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.816206379863762,
            "auditor_fn_violation": 0.02512635553250263,
            "auditor_fp_violation": 0.03676314826583473,
            "ave_precision_score": 0.816627430801783,
            "fpr": 0.15367727771679474,
            "logloss": 0.8726605380396342,
            "mae": 0.2741059328667188,
            "precision": 0.7188755020080321,
            "recall": 0.7868131868131868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8488034925098422,
            "auditor_fn_violation": 0.02606748584889076,
            "auditor_fp_violation": 0.020222696571938325,
            "ave_precision_score": 0.8490519008128254,
            "fpr": 0.13267543859649122,
            "logloss": 0.8917333978142492,
            "mae": 0.26398720711569146,
            "precision": 0.7632093933463796,
            "recall": 0.781563126252505
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8166795697830597,
            "auditor_fn_violation": 0.024800665854452904,
            "auditor_fp_violation": 0.038014905540470274,
            "ave_precision_score": 0.8171156646393676,
            "fpr": 0.1602634467618002,
            "logloss": 0.8754799633628629,
            "mae": 0.2748668742185757,
            "precision": 0.7114624505928854,
            "recall": 0.7912087912087912
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8451084972454833,
            "auditor_fn_violation": 0.027583676124178187,
            "auditor_fp_violation": 0.02325464083938661,
            "ave_precision_score": 0.8452878631898707,
            "fpr": 0.1206140350877193,
            "logloss": 1.149500771926593,
            "mae": 0.2628477822181494,
            "precision": 0.7768762677484787,
            "recall": 0.7675350701402806
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8042054461192114,
            "auditor_fn_violation": 0.01934837939228718,
            "auditor_fp_violation": 0.030562135305332486,
            "ave_precision_score": 0.8045509884076336,
            "fpr": 0.14818880351262348,
            "logloss": 1.121390301900735,
            "mae": 0.28310442189154517,
            "precision": 0.7157894736842105,
            "recall": 0.7472527472527473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 4866,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8319739094302367,
            "auditor_fn_violation": 0.018897444010828688,
            "auditor_fp_violation": 0.009361327046429637,
            "ave_precision_score": 0.8322254564614434,
            "fpr": 0.07785087719298246,
            "logloss": 1.2891519192705496,
            "mae": 0.29455909157727467,
            "precision": 0.8207070707070707,
            "recall": 0.6513026052104208
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7821678971558728,
            "auditor_fn_violation": 0.01405049396267838,
            "auditor_fp_violation": 0.026113582529320008,
            "ave_precision_score": 0.7825871646101729,
            "fpr": 0.10537870472008781,
            "logloss": 1.2244130900912136,
            "mae": 0.3052981159659508,
            "precision": 0.746031746031746,
            "recall": 0.6197802197802198
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8452511307225414,
            "auditor_fn_violation": 0.02565218155609465,
            "auditor_fp_violation": 0.020822713563569954,
            "ave_precision_score": 0.8455015552785842,
            "fpr": 0.1337719298245614,
            "logloss": 0.8871364345702988,
            "mae": 0.2657886311673447,
            "precision": 0.76171875,
            "recall": 0.781563126252505
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8101068659099411,
            "auditor_fn_violation": 0.025452045210552346,
            "auditor_fp_violation": 0.03743717141371541,
            "ave_precision_score": 0.8106743126622862,
            "fpr": 0.15806805708013172,
            "logloss": 0.8717181931610709,
            "mae": 0.278110541197773,
            "precision": 0.712,
            "recall": 0.7824175824175824
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 4866,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8458501236789919,
            "auditor_fn_violation": 0.028225310269662136,
            "auditor_fp_violation": 0.0193067414298458,
            "ave_precision_score": 0.846099182573469,
            "fpr": 0.13486842105263158,
            "logloss": 0.8922941513679041,
            "mae": 0.2658835826671401,
            "precision": 0.7592954990215264,
            "recall": 0.7775551102204409
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8157938754360511,
            "auditor_fn_violation": 0.02512635553250263,
            "auditor_fp_violation": 0.04342394130221273,
            "ave_precision_score": 0.8162149303931234,
            "fpr": 0.16355653128430298,
            "logloss": 0.8698408671813885,
            "mae": 0.2758845441070801,
            "precision": 0.7061143984220908,
            "recall": 0.7868131868131868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8491727166517119,
            "auditor_fn_violation": 0.01064189783074922,
            "auditor_fp_violation": 0.008880782464636168,
            "ave_precision_score": 0.8493890106862483,
            "fpr": 0.09758771929824561,
            "logloss": 0.8360552312398678,
            "mae": 0.27368148838960205,
            "precision": 0.7954022988505747,
            "recall": 0.6933867735470942
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8132182082677845,
            "auditor_fn_violation": 0.008410031242083931,
            "auditor_fp_violation": 0.017789396652993627,
            "ave_precision_score": 0.8136428888862972,
            "fpr": 0.11306256860592755,
            "logloss": 0.8093684844644877,
            "mae": 0.2794239925646124,
            "precision": 0.7553444180522565,
            "recall": 0.6989010989010989
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7817982456140351,
            "auc_prc": 0.8700711473944686,
            "auditor_fn_violation": 0.040255950497486204,
            "auditor_fp_violation": 0.019112930631663905,
            "ave_precision_score": 0.8712117626475688,
            "fpr": 0.08662280701754387,
            "logloss": 0.5315518329873206,
            "mae": 0.2921186920142438,
            "precision": 0.8275109170305677,
            "recall": 0.7595190380761523
        },
        "train": {
            "accuracy": 0.7848518111964874,
            "auc_prc": 0.8526005594943051,
            "auditor_fn_violation": 0.04209840653309369,
            "auditor_fp_violation": 0.024804051842008974,
            "ave_precision_score": 0.8528442558985165,
            "fpr": 0.10976948408342481,
            "logloss": 0.5030506404438372,
            "mae": 0.2961453120793396,
            "precision": 0.7821350762527233,
            "recall": 0.789010989010989
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8331470417792857,
            "auditor_fn_violation": 0.016240814963259857,
            "auditor_fp_violation": 0.008756000169916324,
            "ave_precision_score": 0.8333893443016095,
            "fpr": 0.07894736842105263,
            "logloss": 1.3136861622228475,
            "mae": 0.29303690117713727,
            "precision": 0.8208955223880597,
            "recall": 0.6613226452905812
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7845822557669186,
            "auditor_fn_violation": 0.016465422612513728,
            "auditor_fp_violation": 0.026604656537061646,
            "ave_precision_score": 0.7849905690435564,
            "fpr": 0.10208562019758508,
            "logloss": 1.2426974389470244,
            "mae": 0.30325069314146413,
            "precision": 0.753968253968254,
            "recall": 0.6263736263736264
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.8035152556514655,
            "auditor_fn_violation": 0.024485374257286512,
            "auditor_fp_violation": 0.019925343018563357,
            "ave_precision_score": 0.790560767106714,
            "fpr": 0.15570175438596492,
            "logloss": 2.643169533192621,
            "mae": 0.30197862406194975,
            "precision": 0.72265625,
            "recall": 0.7414829659318637
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.7555846063350025,
            "auditor_fn_violation": 0.02310225449632695,
            "auditor_fp_violation": 0.031717603558842226,
            "ave_precision_score": 0.7404539342958181,
            "fpr": 0.1778265642151482,
            "logloss": 2.700326518706069,
            "mae": 0.32332349196717786,
            "precision": 0.6652892561983471,
            "recall": 0.7076923076923077
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 4866,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8031041804186974,
            "auditor_fn_violation": 0.018165717399711706,
            "auditor_fp_violation": 0.019476657746060073,
            "ave_precision_score": 0.7969094835544288,
            "fpr": 0.14583333333333334,
            "logloss": 1.1730180891124822,
            "mae": 0.2710864203623334,
            "precision": 0.7495291902071564,
            "recall": 0.7975951903807615
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7563541432369523,
            "auditor_fn_violation": 0.019290479005078343,
            "auditor_fp_violation": 0.031262637934022765,
            "ave_precision_score": 0.7503445292841765,
            "fpr": 0.16794731064763996,
            "logloss": 1.2876772065831488,
            "mae": 0.28102210230057956,
            "precision": 0.7029126213592233,
            "recall": 0.7956043956043956
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 4866,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8470246957850166,
            "auditor_fn_violation": 0.025355535632668853,
            "auditor_fp_violation": 0.020881122297268608,
            "ave_precision_score": 0.8472945396149318,
            "fpr": 0.13157894736842105,
            "logloss": 0.8700641780521163,
            "mae": 0.26456560073675844,
            "precision": 0.7651663405088063,
            "recall": 0.7835671342685371
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8172842972769798,
            "auditor_fn_violation": 0.024088973595010922,
            "auditor_fp_violation": 0.035191229995955864,
            "ave_precision_score": 0.8177245783599927,
            "fpr": 0.15477497255762898,
            "logloss": 0.8452167061959001,
            "mae": 0.2738161117848061,
            "precision": 0.7191235059760956,
            "recall": 0.7934065934065934
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.877844544945953,
            "auditor_fn_violation": 0.009527827584994555,
            "auditor_fp_violation": 0.006552397944012576,
            "ave_precision_score": 0.8679803167596387,
            "fpr": 0.044956140350877194,
            "logloss": 3.543842049725042,
            "mae": 0.260579403167722,
            "precision": 0.8815028901734104,
            "recall": 0.6112224448897795
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8408713504084757,
            "auditor_fn_violation": 0.0107067466013679,
            "auditor_fp_violation": 0.011667822134920179,
            "ave_precision_score": 0.8320601526736064,
            "fpr": 0.06476399560922064,
            "logloss": 2.8751740021961267,
            "mae": 0.2547849305527483,
            "precision": 0.8323863636363636,
            "recall": 0.643956043956044
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8447074080859625,
            "auditor_fn_violation": 0.028225310269662136,
            "auditor_fp_violation": 0.019455418206533286,
            "ave_precision_score": 0.8449618025147899,
            "fpr": 0.13596491228070176,
            "logloss": 0.8999411035034821,
            "mae": 0.26611066298738223,
            "precision": 0.7578125,
            "recall": 0.7775551102204409
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8117218812524236,
            "auditor_fn_violation": 0.02512635553250263,
            "auditor_fp_violation": 0.04342394130221273,
            "ave_precision_score": 0.8124408985099156,
            "fpr": 0.16355653128430298,
            "logloss": 0.8764628702996058,
            "mae": 0.27656240225778084,
            "precision": 0.7061143984220908,
            "recall": 0.7868131868131868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8177167500787905,
            "auditor_fn_violation": 0.014245596456070035,
            "auditor_fp_violation": 0.01834034238137717,
            "ave_precision_score": 0.8180030268400047,
            "fpr": 0.16557017543859648,
            "logloss": 0.9601247910020176,
            "mae": 0.2731179182298186,
            "precision": 0.7293906810035843,
            "recall": 0.8156312625250501
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7890464155825149,
            "auditor_fn_violation": 0.01757518003401648,
            "auditor_fp_violation": 0.02110173897972153,
            "ave_precision_score": 0.7896562167277427,
            "fpr": 0.19538968166849616,
            "logloss": 0.9581680173620732,
            "mae": 0.2929265409727421,
            "precision": 0.6751824817518248,
            "recall": 0.8131868131868132
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 4866,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8444913897619061,
            "auditor_fn_violation": 0.026390500298843298,
            "auditor_fp_violation": 0.019455418206533286,
            "ave_precision_score": 0.8447691930259289,
            "fpr": 0.13596491228070176,
            "logloss": 0.9050390401058115,
            "mae": 0.26605146993650397,
            "precision": 0.7582846003898636,
            "recall": 0.779559118236473
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8157427086780488,
            "auditor_fn_violation": 0.025683646759387703,
            "auditor_fp_violation": 0.03967589115489052,
            "ave_precision_score": 0.8161805118283384,
            "fpr": 0.16465422612513722,
            "logloss": 0.8790732599272214,
            "mae": 0.2759360503456455,
            "precision": 0.7053045186640472,
            "recall": 0.789010989010989
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8535621952206623,
            "auditor_fn_violation": 0.022479168864043882,
            "auditor_fp_violation": 0.01377384138311882,
            "ave_precision_score": 0.8537792733844091,
            "fpr": 0.11732456140350878,
            "logloss": 0.8466199540844285,
            "mae": 0.26461136076575864,
            "precision": 0.7780082987551867,
            "recall": 0.751503006012024
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8191256599545871,
            "auditor_fn_violation": 0.019276003908276138,
            "auditor_fp_violation": 0.031139869432087353,
            "ave_precision_score": 0.8195562041006208,
            "fpr": 0.13062568605927552,
            "logloss": 0.8068605256106567,
            "mae": 0.2724885562491781,
            "precision": 0.7407407407407407,
            "recall": 0.7472527472527473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8180781960000962,
            "auditor_fn_violation": 0.02085530710543895,
            "auditor_fp_violation": 0.021542202965039725,
            "ave_precision_score": 0.8188715288343641,
            "fpr": 0.14692982456140352,
            "logloss": 0.9311199427474072,
            "mae": 0.280594575798745,
            "precision": 0.7418111753371869,
            "recall": 0.7715430861723447
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7949764577558395,
            "auditor_fn_violation": 0.02177295810665734,
            "auditor_fp_violation": 0.03289473684210527,
            "ave_precision_score": 0.7956226679992346,
            "fpr": 0.17453347969264543,
            "logloss": 0.9204319645024536,
            "mae": 0.29385204223714867,
            "precision": 0.6906614785992218,
            "recall": 0.7802197802197802
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7207003302906175,
            "auditor_fn_violation": 0.015269574236191687,
            "auditor_fp_violation": 0.019389044645512096,
            "ave_precision_score": 0.6920950257388252,
            "fpr": 0.16776315789473684,
            "logloss": 2.8854579420636144,
            "mae": 0.29650183030637384,
            "precision": 0.7166666666666667,
            "recall": 0.7755511022044088
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7217628696280677,
            "auditor_fn_violation": 0.012303832281878384,
            "auditor_fp_violation": 0.016566526084695832,
            "ave_precision_score": 0.6906375702905511,
            "fpr": 0.18111964873765093,
            "logloss": 2.6101071959033404,
            "mae": 0.2965030837329331,
            "precision": 0.6869070208728653,
            "recall": 0.7956043956043956
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8428800917903128,
            "auditor_fn_violation": 0.021918837675350706,
            "auditor_fp_violation": 0.013994201605709198,
            "ave_precision_score": 0.8431539975081587,
            "fpr": 0.11513157894736842,
            "logloss": 0.9042077950574077,
            "mae": 0.26644160189330185,
            "precision": 0.782608695652174,
            "recall": 0.7575150300601202
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8062575812154187,
            "auditor_fn_violation": 0.01811799616409935,
            "auditor_fp_violation": 0.028944479750418865,
            "ave_precision_score": 0.8067276283884275,
            "fpr": 0.13062568605927552,
            "logloss": 0.868088013672238,
            "mae": 0.27706406625424107,
            "precision": 0.7390350877192983,
            "recall": 0.7406593406593407
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8449954189938158,
            "auditor_fn_violation": 0.027003568540589953,
            "auditor_fp_violation": 0.021027144131515237,
            "ave_precision_score": 0.8452651747861122,
            "fpr": 0.13706140350877194,
            "logloss": 0.8884253448515306,
            "mae": 0.2667598016096576,
            "precision": 0.7568093385214008,
            "recall": 0.779559118236473
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8156057641870346,
            "auditor_fn_violation": 0.02354856998106175,
            "auditor_fp_violation": 0.03908371367496679,
            "ave_precision_score": 0.8160771172073223,
            "fpr": 0.16575192096597147,
            "logloss": 0.8660578150199417,
            "mae": 0.27802280207681246,
            "precision": 0.703921568627451,
            "recall": 0.789010989010989
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7839218772832658,
            "auditor_fn_violation": 0.011094557536124885,
            "auditor_fp_violation": 0.016455333248375177,
            "ave_precision_score": 0.7786429264971307,
            "fpr": 0.15021929824561403,
            "logloss": 1.459416893886608,
            "mae": 0.2739879903369805,
            "precision": 0.7444029850746269,
            "recall": 0.7995991983967936
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.761414423947006,
            "auditor_fn_violation": 0.00811329175763863,
            "auditor_fp_violation": 0.023246576925298985,
            "ave_precision_score": 0.7553979760457424,
            "fpr": 0.16794731064763996,
            "logloss": 1.454798359765097,
            "mae": 0.29505825882945463,
            "precision": 0.6994106090373281,
            "recall": 0.7824175824175824
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8365523122062366,
            "auditor_fn_violation": 0.016928594030165597,
            "auditor_fp_violation": 0.015247334437789393,
            "ave_precision_score": 0.8378553073131816,
            "fpr": 0.12609649122807018,
            "logloss": 0.8843672645150347,
            "mae": 0.25953818936868056,
            "precision": 0.7718253968253969,
            "recall": 0.779559118236473
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.810338228734026,
            "auditor_fn_violation": 0.014489571899012081,
            "auditor_fp_violation": 0.028012883471026637,
            "ave_precision_score": 0.811095401972411,
            "fpr": 0.141602634467618,
            "logloss": 0.8528314470445042,
            "mae": 0.27726834213366564,
            "precision": 0.7266949152542372,
            "recall": 0.7538461538461538
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8415836335070669,
            "auditor_fn_violation": 0.02710684526948634,
            "auditor_fp_violation": 0.021868760885264016,
            "ave_precision_score": 0.8419173050424507,
            "fpr": 0.13596491228070176,
            "logloss": 0.9032791115151905,
            "mae": 0.26554253768779934,
            "precision": 0.7592233009708738,
            "recall": 0.7835671342685371
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8130786946207993,
            "auditor_fn_violation": 0.024028660691668376,
            "auditor_fp_violation": 0.03998642324802127,
            "ave_precision_score": 0.8133715873562266,
            "fpr": 0.16794731064763996,
            "logloss": 0.8872489503524419,
            "mae": 0.2781500885576904,
            "precision": 0.700587084148728,
            "recall": 0.7868131868131868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8430676738732799,
            "auditor_fn_violation": 0.026390500298843298,
            "auditor_fp_violation": 0.021289983433159177,
            "ave_precision_score": 0.8433252858194755,
            "fpr": 0.13486842105263158,
            "logloss": 0.897583093516066,
            "mae": 0.266275137236862,
            "precision": 0.759765625,
            "recall": 0.779559118236473
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8074706274840743,
            "auditor_fn_violation": 0.02512635553250263,
            "auditor_fp_violation": 0.03744680031582799,
            "ave_precision_score": 0.807594505334039,
            "fpr": 0.1602634467618002,
            "logloss": 0.8788721703822111,
            "mae": 0.2790880204314584,
            "precision": 0.7103174603174603,
            "recall": 0.7868131868131868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8451580861352728,
            "auditor_fn_violation": 0.028249481418978303,
            "auditor_fp_violation": 0.01618983900429039,
            "ave_precision_score": 0.8454123520200866,
            "fpr": 0.07236842105263158,
            "logloss": 0.7915735361138899,
            "mae": 0.30296256768718455,
            "precision": 0.835,
            "recall": 0.6693386773547094
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8008396053473285,
            "auditor_fn_violation": 0.029283120830870563,
            "auditor_fp_violation": 0.014414466462533946,
            "ave_precision_score": 0.8012729848797523,
            "fpr": 0.10098792535675083,
            "logloss": 0.7765187589179164,
            "mae": 0.30235727612262275,
            "precision": 0.7788461538461539,
            "recall": 0.7120879120879121
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7048074011572406,
            "auditor_fn_violation": 0.0100156453257392,
            "auditor_fp_violation": 0.022941357631366552,
            "ave_precision_score": 0.695109875774694,
            "fpr": 0.19846491228070176,
            "logloss": 1.944099589828115,
            "mae": 0.2997521352604123,
            "precision": 0.6957983193277311,
            "recall": 0.8296593186372746
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6697393108824328,
            "auditor_fn_violation": 0.009457063244110445,
            "auditor_fp_violation": 0.02159281298746318,
            "ave_precision_score": 0.6620125478186272,
            "fpr": 0.23600439077936333,
            "logloss": 1.9149704550011533,
            "mae": 0.3180668782952556,
            "precision": 0.6416666666666667,
            "recall": 0.8461538461538461
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.813300397971418,
            "auditor_fn_violation": 0.048228035017403234,
            "auditor_fp_violation": 0.04564377044305679,
            "ave_precision_score": 0.8146417773320558,
            "fpr": 0.14802631578947367,
            "logloss": 0.7169336263368123,
            "mae": 0.33503528237232577,
            "precision": 0.7326732673267327,
            "recall": 0.7414829659318637
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.8095761945224424,
            "auditor_fn_violation": 0.044800424602839534,
            "auditor_fp_violation": 0.051396672251429895,
            "ave_precision_score": 0.809953553195211,
            "fpr": 0.1942919868276619,
            "logloss": 0.6493752765949266,
            "mae": 0.3357952009127026,
            "precision": 0.6666666666666666,
            "recall": 0.778021978021978
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8306845921703047,
            "auditor_fn_violation": 0.050579228632704,
            "auditor_fp_violation": 0.045102162185123834,
            "ave_precision_score": 0.8318934933040483,
            "fpr": 0.1513157894736842,
            "logloss": 0.6414354481234043,
            "mae": 0.31999117156121565,
            "precision": 0.7335907335907336,
            "recall": 0.7615230460921844
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.8257976451629947,
            "auditor_fn_violation": 0.048807613900917965,
            "auditor_fp_violation": 0.04898463227222832,
            "ave_precision_score": 0.8261030833966161,
            "fpr": 0.1877058177826564,
            "logloss": 0.6135835144640801,
            "mae": 0.32144408600087576,
            "precision": 0.6815642458100558,
            "recall": 0.8043956043956044
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7896196590554984,
            "auditor_fn_violation": 0.016045248391519887,
            "auditor_fp_violation": 0.009950724268297868,
            "ave_precision_score": 0.7900399004865116,
            "fpr": 0.1600877192982456,
            "logloss": 1.020156907791551,
            "mae": 0.2773452944198912,
            "precision": 0.7330895795246801,
            "recall": 0.8036072144288577
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7645208591371168,
            "auditor_fn_violation": 0.008065041434964596,
            "auditor_fp_violation": 0.027579582875960486,
            "ave_precision_score": 0.7648154297162566,
            "fpr": 0.1877058177826564,
            "logloss": 1.0291294391228605,
            "mae": 0.29332898956979125,
            "precision": 0.6797752808988764,
            "recall": 0.7978021978021979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8113251730232097,
            "auditor_fn_violation": 0.01438403122033541,
            "auditor_fp_violation": 0.019662503716919424,
            "ave_precision_score": 0.8117003241039569,
            "fpr": 0.14802631578947367,
            "logloss": 0.9692941470307072,
            "mae": 0.2732752834931388,
            "precision": 0.7428571428571429,
            "recall": 0.781563126252505
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7835371018173457,
            "auditor_fn_violation": 0.016836950097103773,
            "auditor_fp_violation": 0.03219904866447128,
            "ave_precision_score": 0.7838712543833161,
            "fpr": 0.1668496158068057,
            "logloss": 0.9746159495406814,
            "mae": 0.2854900886092262,
            "precision": 0.699009900990099,
            "recall": 0.7758241758241758
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8518851502932067,
            "auditor_fn_violation": 0.027128819041591958,
            "auditor_fp_violation": 0.021632471008028547,
            "ave_precision_score": 0.8521290271234904,
            "fpr": 0.12280701754385964,
            "logloss": 0.8563995662187115,
            "mae": 0.26073194299816027,
            "precision": 0.776,
            "recall": 0.7775551102204409
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8207406356320287,
            "auditor_fn_violation": 0.02311190456086175,
            "auditor_fp_violation": 0.03353024438153563,
            "ave_precision_score": 0.8211640037969055,
            "fpr": 0.14818880351262348,
            "logloss": 0.8335862779549732,
            "mae": 0.2715797623225718,
            "precision": 0.7244897959183674,
            "recall": 0.7802197802197802
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8436749755118554,
            "auditor_fn_violation": 0.006436117849734566,
            "auditor_fp_violation": 0.012414510853404701,
            "ave_precision_score": 0.8439025725416421,
            "fpr": 0.1162280701754386,
            "logloss": 0.8294575402451861,
            "mae": 0.2728842513158525,
            "precision": 0.7763713080168776,
            "recall": 0.7374749498997996
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8108784358079695,
            "auditor_fn_violation": 0.012523371250045242,
            "auditor_fp_violation": 0.022266836135343852,
            "ave_precision_score": 0.8113210531526014,
            "fpr": 0.13391877058177826,
            "logloss": 0.8079767520976484,
            "mae": 0.28266895616572485,
            "precision": 0.7306843267108167,
            "recall": 0.7274725274725274
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.84159611053789,
            "auditor_fn_violation": 0.02567415532820026,
            "auditor_fp_violation": 0.021451934922050893,
            "ave_precision_score": 0.841832306287678,
            "fpr": 0.13267543859649122,
            "logloss": 0.9109616498947058,
            "mae": 0.26654707348703044,
            "precision": 0.7618110236220472,
            "recall": 0.7755511022044088
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8036793296774165,
            "auditor_fn_violation": 0.025452045210552346,
            "auditor_fp_violation": 0.03748772314980646,
            "ave_precision_score": 0.8051218258376538,
            "fpr": 0.15697036223929747,
            "logloss": 0.8958620928014044,
            "mae": 0.2792464302381556,
            "precision": 0.7134268537074149,
            "recall": 0.7824175824175824
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8325521731273552,
            "auditor_fn_violation": 0.009103733783356185,
            "auditor_fp_violation": 0.01495794571173697,
            "ave_precision_score": 0.8328418992645502,
            "fpr": 0.1118421052631579,
            "logloss": 0.8878816380135238,
            "mae": 0.27739717832192773,
            "precision": 0.7811158798283262,
            "recall": 0.7294589178356713
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8013947911553558,
            "auditor_fn_violation": 0.014494396931279485,
            "auditor_fp_violation": 0.025169950122287062,
            "ave_precision_score": 0.8017660706444578,
            "fpr": 0.13611416026344675,
            "logloss": 0.8725523593028593,
            "mae": 0.2875254257863827,
            "precision": 0.7280701754385965,
            "recall": 0.7296703296703296
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8530098521427446,
            "auditor_fn_violation": 0.023210895475160857,
            "auditor_fp_violation": 0.013418079096045206,
            "ave_precision_score": 0.8532271802360553,
            "fpr": 0.11513157894736842,
            "logloss": 0.8493806139845562,
            "mae": 0.2651139469343484,
            "precision": 0.7807933194154488,
            "recall": 0.749498997995992
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8179054658951781,
            "auditor_fn_violation": 0.019276003908276138,
            "auditor_fp_violation": 0.031139869432087353,
            "ave_precision_score": 0.8183635471037678,
            "fpr": 0.13062568605927552,
            "logloss": 0.8097747623189517,
            "mae": 0.2731013017451801,
            "precision": 0.7407407407407407,
            "recall": 0.7472527472527473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7945849198575328,
            "auditor_fn_violation": 0.041752364377878576,
            "auditor_fp_violation": 0.01662524956458944,
            "ave_precision_score": 0.7901249643562459,
            "fpr": 0.08552631578947369,
            "logloss": 1.0144108924948387,
            "mae": 0.3193645370214117,
            "precision": 0.8156028368794326,
            "recall": 0.6913827655310621
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8115595345943981,
            "auditor_fn_violation": 0.03826250588050808,
            "auditor_fp_violation": 0.02231498064590676,
            "ave_precision_score": 0.8079071894103852,
            "fpr": 0.10757409440175632,
            "logloss": 0.7161399990656322,
            "mae": 0.31049427337984425,
            "precision": 0.776255707762557,
            "recall": 0.7472527472527473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8550691405178908,
            "auditor_fn_violation": 0.024856731005871397,
            "auditor_fp_violation": 0.013375600016991635,
            "ave_precision_score": 0.8552814616283556,
            "fpr": 0.1162280701754386,
            "logloss": 0.8287818362032585,
            "mae": 0.26419604159830307,
            "precision": 0.7791666666666667,
            "recall": 0.749498997995992
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8208394622753066,
            "auditor_fn_violation": 0.019276003908276138,
            "auditor_fp_violation": 0.031139869432087353,
            "ave_precision_score": 0.8212673940448856,
            "fpr": 0.13062568605927552,
            "logloss": 0.788093670532032,
            "mae": 0.27189876042542493,
            "precision": 0.7407407407407407,
            "recall": 0.7472527472527473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8454211768492914,
            "auditor_fn_violation": 0.024491966388918186,
            "auditor_fp_violation": 0.020403232657915982,
            "ave_precision_score": 0.8456991054168376,
            "fpr": 0.13486842105263158,
            "logloss": 0.8846542543657135,
            "mae": 0.265874892278014,
            "precision": 0.7602339181286549,
            "recall": 0.781563126252505
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8172047192670319,
            "auditor_fn_violation": 0.025186668435845164,
            "auditor_fp_violation": 0.03884780557320855,
            "ave_precision_score": 0.8176409103728368,
            "fpr": 0.16355653128430298,
            "logloss": 0.8567118525862093,
            "mae": 0.27561866519480377,
            "precision": 0.707843137254902,
            "recall": 0.7934065934065934
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8170569831764025,
            "auditor_fn_violation": 0.01175816545371445,
            "auditor_fp_violation": 0.01292160485960665,
            "ave_precision_score": 0.8166197911987386,
            "fpr": 0.11951754385964912,
            "logloss": 0.9936363263698921,
            "mae": 0.26357862616116867,
            "precision": 0.7757201646090535,
            "recall": 0.7555110220440882
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.7731660443349325,
            "auditor_fn_violation": 0.010180818084220942,
            "auditor_fp_violation": 0.0254804822154178,
            "ave_precision_score": 0.773486667955356,
            "fpr": 0.1437980241492865,
            "logloss": 0.9951856655356688,
            "mae": 0.2785100531287063,
            "precision": 0.7247899159663865,
            "recall": 0.7582417582417582
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8208647633258699,
            "auditor_fn_violation": 0.02213637801919629,
            "auditor_fp_violation": 0.0173712883904677,
            "ave_precision_score": 0.821216230886284,
            "fpr": 0.12609649122807018,
            "logloss": 1.081468158288675,
            "mae": 0.2776796876797887,
            "precision": 0.7638603696098563,
            "recall": 0.7454909819639278
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7770724997178924,
            "auditor_fn_violation": 0.02038576132977889,
            "auditor_fp_violation": 0.02854006586169045,
            "ave_precision_score": 0.778833464593357,
            "fpr": 0.145993413830955,
            "logloss": 1.0866270559368776,
            "mae": 0.2951442545805438,
            "precision": 0.7158119658119658,
            "recall": 0.7362637362637363
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.7697335974963231,
            "auditor_fn_violation": 0.01204821924550856,
            "auditor_fp_violation": 0.017358013678263458,
            "ave_precision_score": 0.7700749456195686,
            "fpr": 0.15679824561403508,
            "logloss": 1.0559688031179608,
            "mae": 0.27629086191499985,
            "precision": 0.7376146788990826,
            "recall": 0.8056112224448898
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7442047438654498,
            "auditor_fn_violation": 0.00550053678483975,
            "auditor_fp_violation": 0.0249966298842606,
            "ave_precision_score": 0.7446914878235829,
            "fpr": 0.1734357848518112,
            "logloss": 1.0474181460162513,
            "mae": 0.2927253579221027,
            "precision": 0.694980694980695,
            "recall": 0.7912087912087912
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8482757880226706,
            "auditor_fn_violation": 0.022936223323840665,
            "auditor_fp_violation": 0.017395182872435324,
            "ave_precision_score": 0.848516652351329,
            "fpr": 0.13048245614035087,
            "logloss": 0.864436085275553,
            "mae": 0.26586077940538,
            "precision": 0.7643564356435644,
            "recall": 0.7735470941883767
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8186146990142942,
            "auditor_fn_violation": 0.022950265979903742,
            "auditor_fp_violation": 0.03776936853659946,
            "ave_precision_score": 0.8189853374031368,
            "fpr": 0.15477497255762898,
            "logloss": 0.83908780305101,
            "mae": 0.27476494554040826,
            "precision": 0.7145748987854251,
            "recall": 0.7758241758241758
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8213926112944178,
            "auditor_fn_violation": 0.012116337939035972,
            "auditor_fp_violation": 0.014172082749246005,
            "ave_precision_score": 0.8217180348118245,
            "fpr": 0.11951754385964912,
            "logloss": 0.8972119806766751,
            "mae": 0.28559657731267407,
            "precision": 0.7685774946921444,
            "recall": 0.7254509018036072
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7915268857839015,
            "auditor_fn_violation": 0.012277294604407669,
            "auditor_fp_violation": 0.019989600785718417,
            "ave_precision_score": 0.7918961969824704,
            "fpr": 0.13172338090010977,
            "logloss": 0.8933334597695243,
            "mae": 0.2980022714320667,
            "precision": 0.7285067873303167,
            "recall": 0.7076923076923077
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.840121860014384,
            "auditor_fn_violation": 0.026390500298843298,
            "auditor_fp_violation": 0.01993065290344505,
            "ave_precision_score": 0.8404372809605367,
            "fpr": 0.13706140350877194,
            "logloss": 0.9164261528941311,
            "mae": 0.26690586164001173,
            "precision": 0.7568093385214008,
            "recall": 0.779559118236473
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8118390364282244,
            "auditor_fn_violation": 0.026474952051241844,
            "auditor_fp_violation": 0.04061470911086719,
            "ave_precision_score": 0.8121413700413873,
            "fpr": 0.1668496158068057,
            "logloss": 0.8890802143986976,
            "mae": 0.2768492790855244,
            "precision": 0.7037037037037037,
            "recall": 0.7934065934065934
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8465358510521961,
            "auditor_fn_violation": 0.027867137784340613,
            "auditor_fp_violation": 0.018029714115797967,
            "ave_precision_score": 0.84678422452633,
            "fpr": 0.13267543859649122,
            "logloss": 0.8866260535650359,
            "mae": 0.2652475247494653,
            "precision": 0.7618110236220472,
            "recall": 0.7755511022044088
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.816615547185439,
            "auditor_fn_violation": 0.02512635553250263,
            "auditor_fp_violation": 0.04301230573689988,
            "ave_precision_score": 0.8168936104013683,
            "fpr": 0.16245883644346873,
            "logloss": 0.8661520701412802,
            "mae": 0.2758928402378774,
            "precision": 0.7075098814229249,
            "recall": 0.7868131868131868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8393011235679335,
            "auditor_fn_violation": 0.028260468305031115,
            "auditor_fp_violation": 0.02893356272036023,
            "ave_precision_score": 0.8396020581552232,
            "fpr": 0.15021929824561403,
            "logloss": 0.9209392365649216,
            "mae": 0.2692954632332144,
            "precision": 0.7453531598513011,
            "recall": 0.8036072144288577
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8177703482571157,
            "auditor_fn_violation": 0.030385640703972212,
            "auditor_fp_violation": 0.039285920619330984,
            "ave_precision_score": 0.8181506897234806,
            "fpr": 0.18111964873765093,
            "logloss": 0.9086309488622567,
            "mae": 0.2799404285632635,
            "precision": 0.6944444444444444,
            "recall": 0.8241758241758241
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 4866,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8317371750117567,
            "auditor_fn_violation": 0.027128819041591958,
            "auditor_fp_violation": 0.019633299350070094,
            "ave_precision_score": 0.8330037622068545,
            "fpr": 0.13486842105263158,
            "logloss": 0.9449573596029862,
            "mae": 0.2695598352311283,
            "precision": 0.7592954990215264,
            "recall": 0.7775551102204409
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8126369903129966,
            "auditor_fn_violation": 0.02557267101723743,
            "auditor_fp_violation": 0.04148131030099948,
            "ave_precision_score": 0.8130719393661417,
            "fpr": 0.16794731064763996,
            "logloss": 0.9087031874829135,
            "mae": 0.27877914506236917,
            "precision": 0.7029126213592233,
            "recall": 0.7956043956043956
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.831020097975445,
            "auditor_fn_violation": 0.022307773441620084,
            "auditor_fp_violation": 0.017134998513232236,
            "ave_precision_score": 0.8323140731759019,
            "fpr": 0.13048245614035087,
            "logloss": 0.8794605964244229,
            "mae": 0.2611057802215652,
            "precision": 0.7693798449612403,
            "recall": 0.7955911823647295
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7964471595210326,
            "auditor_fn_violation": 0.013970880930266221,
            "auditor_fp_violation": 0.030381593390721592,
            "ave_precision_score": 0.7979629817352903,
            "fpr": 0.16355653128430298,
            "logloss": 0.8820651404374417,
            "mae": 0.28310125270944947,
            "precision": 0.7061143984220908,
            "recall": 0.7868131868131868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8416803394080558,
            "auditor_fn_violation": 0.026032327813521782,
            "auditor_fp_violation": 0.02212363535958541,
            "ave_precision_score": 0.8419566652875003,
            "fpr": 0.13706140350877194,
            "logloss": 0.924042613316546,
            "mae": 0.2690441093110862,
            "precision": 0.7563352826510721,
            "recall": 0.7775551102204409
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8104883671841132,
            "auditor_fn_violation": 0.02507086766142749,
            "auditor_fp_violation": 0.03805342114892061,
            "ave_precision_score": 0.8111688648404646,
            "fpr": 0.1668496158068057,
            "logloss": 0.9052531448417265,
            "mae": 0.2781538433732927,
            "precision": 0.7048543689320388,
            "recall": 0.7978021978021979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8498022423753248,
            "auditor_fn_violation": 0.02558845761698836,
            "auditor_fp_violation": 0.020076674737691688,
            "ave_precision_score": 0.8500652797441766,
            "fpr": 0.13486842105263158,
            "logloss": 0.8903068963933584,
            "mae": 0.26365291614707703,
            "precision": 0.7602339181286549,
            "recall": 0.781563126252505
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8185063665947335,
            "auditor_fn_violation": 0.025186668435845164,
            "auditor_fp_violation": 0.03813045236582125,
            "ave_precision_score": 0.8187837223576085,
            "fpr": 0.16355653128430298,
            "logloss": 0.8754194886491775,
            "mae": 0.27395812411773623,
            "precision": 0.707843137254902,
            "recall": 0.7934065934065934
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.803525097617479,
            "auditor_fn_violation": 0.03568101114509721,
            "auditor_fp_violation": 0.040163969245146765,
            "ave_precision_score": 0.8025374071168332,
            "fpr": 0.15679824561403508,
            "logloss": 0.7920864323318528,
            "mae": 0.3123711662757811,
            "precision": 0.7296786389413988,
            "recall": 0.7735470941883767
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7667773586620403,
            "auditor_fn_violation": 0.030986357221263916,
            "auditor_fp_violation": 0.04526546883124386,
            "ave_precision_score": 0.7670330463284539,
            "fpr": 0.19758507135016465,
            "logloss": 0.8848384256332626,
            "mae": 0.3192387147456665,
            "precision": 0.674502712477396,
            "recall": 0.8197802197802198
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8469321589340415,
            "auditor_fn_violation": 0.027128819041591958,
            "auditor_fp_violation": 0.01896956373985812,
            "ave_precision_score": 0.8471930149435607,
            "fpr": 0.13157894736842105,
            "logloss": 0.917062328520948,
            "mae": 0.2643398068690725,
            "precision": 0.7637795275590551,
            "recall": 0.7775551102204409
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8144964442909313,
            "auditor_fn_violation": 0.025683646759387703,
            "auditor_fp_violation": 0.03772122402603655,
            "ave_precision_score": 0.8150823328349016,
            "fpr": 0.15587266739846323,
            "logloss": 0.8880942033101474,
            "mae": 0.27374305232459784,
            "precision": 0.716566866267465,
            "recall": 0.789010989010989
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8224831481740766,
            "auditor_fn_violation": 0.027904493196920156,
            "auditor_fp_violation": 0.014286245274202459,
            "ave_precision_score": 0.822136064744921,
            "fpr": 0.08771929824561403,
            "logloss": 2.3126216782668534,
            "mae": 0.2961078348750817,
            "precision": 0.8058252427184466,
            "recall": 0.6653306613226453
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7859741520508876,
            "auditor_fn_violation": 0.0239128599172507,
            "auditor_fp_violation": 0.031813892579968035,
            "ave_precision_score": 0.786827595676142,
            "fpr": 0.11306256860592755,
            "logloss": 2.1446120955440136,
            "mae": 0.30720142505348297,
            "precision": 0.7405541561712846,
            "recall": 0.6461538461538462
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8638838657399135,
            "auditor_fn_violation": 0.00937620855746582,
            "auditor_fp_violation": 0.01317913427636889,
            "ave_precision_score": 0.864150742867288,
            "fpr": 0.08333333333333333,
            "logloss": 0.5004720233424588,
            "mae": 0.3201726746335401,
            "precision": 0.8240740740740741,
            "recall": 0.7134268537074149
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8518918863448954,
            "auditor_fn_violation": 0.013297788928963461,
            "auditor_fp_violation": 0.008069019970342985,
            "ave_precision_score": 0.8521736841712704,
            "fpr": 0.09659714599341383,
            "logloss": 0.4748816912239762,
            "mae": 0.317236682652873,
            "precision": 0.7948717948717948,
            "recall": 0.7494505494505495
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8348392122046956,
            "auditor_fn_violation": 0.024821572970502408,
            "auditor_fp_violation": 0.01658011554309503,
            "ave_precision_score": 0.8351224917875234,
            "fpr": 0.12609649122807018,
            "logloss": 0.91584516793265,
            "mae": 0.27494186298965245,
            "precision": 0.766260162601626,
            "recall": 0.7555110220440882
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7992613348214822,
            "auditor_fn_violation": 0.01911919035958553,
            "auditor_fp_violation": 0.030504361892657005,
            "ave_precision_score": 0.7998957489273999,
            "fpr": 0.15587266739846323,
            "logloss": 0.8995396453087237,
            "mae": 0.28720745532191266,
            "precision": 0.7084188911704312,
            "recall": 0.7582417582417582
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.7974208012785118,
            "auditor_fn_violation": 0.010215606651900291,
            "auditor_fp_violation": 0.020732445520581122,
            "ave_precision_score": 0.7913791735503606,
            "fpr": 0.10855263157894737,
            "logloss": 1.6313824422033487,
            "mae": 0.27149072220157744,
            "precision": 0.7884615384615384,
            "recall": 0.7394789579158316
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7553840362125392,
            "auditor_fn_violation": 0.012668122218067337,
            "auditor_fp_violation": 0.03301750534404068,
            "ave_precision_score": 0.7498581372666506,
            "fpr": 0.14489571899012074,
            "logloss": 1.82679527491621,
            "mae": 0.291158582667006,
            "precision": 0.7185501066098081,
            "recall": 0.7406593406593407
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8441719952047737,
            "auditor_fn_violation": 0.026390500298843298,
            "auditor_fp_violation": 0.021289983433159177,
            "ave_precision_score": 0.844427689873011,
            "fpr": 0.13486842105263158,
            "logloss": 0.890665665065494,
            "mae": 0.2659555956311342,
            "precision": 0.759765625,
            "recall": 0.779559118236473
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8101694066481504,
            "auditor_fn_violation": 0.024740352951110364,
            "auditor_fp_violation": 0.03788010091089414,
            "ave_precision_score": 0.8107052095776939,
            "fpr": 0.15916575192096596,
            "logloss": 0.8725370924500767,
            "mae": 0.2782852919398514,
            "precision": 0.7111553784860558,
            "recall": 0.7846153846153846
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8429839813282347,
            "auditor_fn_violation": 0.026390500298843298,
            "auditor_fp_violation": 0.021289983433159177,
            "ave_precision_score": 0.8432418505521773,
            "fpr": 0.13486842105263158,
            "logloss": 0.8970933727686068,
            "mae": 0.26639139370743736,
            "precision": 0.759765625,
            "recall": 0.779559118236473
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8086444824128967,
            "auditor_fn_violation": 0.02512635553250263,
            "auditor_fp_violation": 0.03744680031582799,
            "ave_precision_score": 0.8093137363011876,
            "fpr": 0.1602634467618002,
            "logloss": 0.8779352943196679,
            "mae": 0.27904659197144027,
            "precision": 0.7103174603174603,
            "recall": 0.7868131868131868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8300816693591091,
            "auditor_fn_violation": 0.012010863832929026,
            "auditor_fp_violation": 0.012608321651586596,
            "ave_precision_score": 0.8303605903910842,
            "fpr": 0.12609649122807018,
            "logloss": 0.9104048644987812,
            "mae": 0.2767999110896084,
            "precision": 0.764344262295082,
            "recall": 0.7474949899799599
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.7995445989409045,
            "auditor_fn_violation": 0.010820134859651878,
            "auditor_fp_violation": 0.026460223005372922,
            "ave_precision_score": 0.7998957599646703,
            "fpr": 0.14928649835345773,
            "logloss": 0.9029587216554371,
            "mae": 0.28865504363217387,
            "precision": 0.7142857142857143,
            "recall": 0.7472527472527473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8464036316900876,
            "auditor_fn_violation": 0.024421650318180225,
            "auditor_fp_violation": 0.018807612250966402,
            "ave_precision_score": 0.8466590621378585,
            "fpr": 0.13048245614035087,
            "logloss": 0.9072268460317132,
            "mae": 0.26593460554199605,
            "precision": 0.7648221343873518,
            "recall": 0.7755511022044088
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.815858444759182,
            "auditor_fn_violation": 0.02512635553250263,
            "auditor_fp_violation": 0.036320218768656,
            "ave_precision_score": 0.8162780811734622,
            "fpr": 0.1525795828759605,
            "logloss": 0.876166705881078,
            "mae": 0.2744791432068462,
            "precision": 0.7203219315895373,
            "recall": 0.7868131868131868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7194657409345151,
            "auditor_fn_violation": 0.015524469992616814,
            "auditor_fp_violation": 0.02869461790068392,
            "ave_precision_score": 0.702533135586515,
            "fpr": 0.15350877192982457,
            "logloss": 2.222848334264509,
            "mae": 0.2914381791233881,
            "precision": 0.7348484848484849,
            "recall": 0.7775551102204409
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.6690346472451658,
            "auditor_fn_violation": 0.015420803126620917,
            "auditor_fp_violation": 0.033060835403547296,
            "ave_precision_score": 0.65144414492557,
            "fpr": 0.1778265642151482,
            "logloss": 2.4557478972338074,
            "mae": 0.30642856803287816,
            "precision": 0.6817288801571709,
            "recall": 0.7626373626373626
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8512301324149059,
            "auditor_fn_violation": 0.02332076433568892,
            "auditor_fp_violation": 0.016277452104838372,
            "ave_precision_score": 0.8514589986705118,
            "fpr": 0.13596491228070176,
            "logloss": 0.835680136806077,
            "mae": 0.2639219525990373,
            "precision": 0.7587548638132295,
            "recall": 0.781563126252505
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8178595292086038,
            "auditor_fn_violation": 0.020366461200709285,
            "auditor_fp_violation": 0.03324859899474262,
            "ave_precision_score": 0.8185012131075122,
            "fpr": 0.16245883644346873,
            "logloss": 0.8209153521889538,
            "mae": 0.27611843798358665,
            "precision": 0.7080867850098619,
            "recall": 0.789010989010989
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7974167195108829,
            "auditor_fn_violation": 0.014689466652603466,
            "auditor_fp_violation": 0.017506690454950936,
            "ave_precision_score": 0.7715481566093493,
            "fpr": 0.15460526315789475,
            "logloss": 3.5168799071401806,
            "mae": 0.2914654951049183,
            "precision": 0.7262135922330097,
            "recall": 0.749498997995992
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7476615461230693,
            "auditor_fn_violation": 0.015114413577640798,
            "auditor_fp_violation": 0.023966337358214417,
            "ave_precision_score": 0.7135961763175864,
            "fpr": 0.18660812294182216,
            "logloss": 3.75962370705372,
            "mae": 0.315134087812426,
            "precision": 0.6666666666666666,
            "recall": 0.7472527472527473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8524218876830172,
            "auditor_fn_violation": 0.0188732728615125,
            "auditor_fp_violation": 0.016917293233082706,
            "ave_precision_score": 0.8526536407814255,
            "fpr": 0.12938596491228072,
            "logloss": 0.8109123217336481,
            "mae": 0.26248925805446754,
            "precision": 0.7649402390438247,
            "recall": 0.7695390781563126
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8151499000524236,
            "auditor_fn_violation": 0.00903487292071266,
            "auditor_fp_violation": 0.028222312091975274,
            "ave_precision_score": 0.8157042476244082,
            "fpr": 0.14489571899012074,
            "logloss": 0.8134191661477091,
            "mae": 0.27599992794986683,
            "precision": 0.7261410788381742,
            "recall": 0.7692307692307693
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.6736793962101398,
            "auditor_fn_violation": 0.013707239039482472,
            "auditor_fp_violation": 0.013704812879656771,
            "ave_precision_score": 0.654178738862655,
            "fpr": 0.16557017543859648,
            "logloss": 2.999331616027767,
            "mae": 0.30842243184486356,
            "precision": 0.7129277566539924,
            "recall": 0.751503006012024
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.6396450601019529,
            "auditor_fn_violation": 0.012238694346268442,
            "auditor_fp_violation": 0.019705548173397276,
            "ave_precision_score": 0.6208538347102074,
            "fpr": 0.1734357848518112,
            "logloss": 2.9248081260721337,
            "mae": 0.31670587064723116,
            "precision": 0.6833667334669339,
            "recall": 0.7494505494505495
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 4866,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.7831022608092914,
            "auditor_fn_violation": 0.04126674401434448,
            "auditor_fp_violation": 0.024104222420457927,
            "ave_precision_score": 0.7844014744163988,
            "fpr": 0.09210526315789473,
            "logloss": 2.386746580476474,
            "mae": 0.3687566854196167,
            "precision": 0.7795275590551181,
            "recall": 0.5951903807615231
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7524423900203352,
            "auditor_fn_violation": 0.038026079299405324,
            "auditor_fp_violation": 0.028243977121728582,
            "ave_precision_score": 0.7528526100258033,
            "fpr": 0.10208562019758508,
            "logloss": 2.6626089294597457,
            "mae": 0.37824858742414086,
            "precision": 0.7342857142857143,
            "recall": 0.5648351648351648
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8446975422192244,
            "auditor_fn_violation": 0.022830749217733717,
            "auditor_fp_violation": 0.014323414468374328,
            "ave_precision_score": 0.8449150203728308,
            "fpr": 0.0756578947368421,
            "logloss": 1.206585419229859,
            "mae": 0.286124383054124,
            "precision": 0.8207792207792208,
            "recall": 0.6332665330661322
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7992873794535916,
            "auditor_fn_violation": 0.028274689086983277,
            "auditor_fp_violation": 0.02341508271226915,
            "ave_precision_score": 0.7997216344494267,
            "fpr": 0.09330406147091108,
            "logloss": 1.1124490243232144,
            "mae": 0.29626484822293425,
            "precision": 0.7658402203856749,
            "recall": 0.610989010989011
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8502906314149324,
            "auditor_fn_violation": 0.026010354041416165,
            "auditor_fp_violation": 0.019272227178114783,
            "ave_precision_score": 0.8505354524521077,
            "fpr": 0.13048245614035087,
            "logloss": 0.8673420199912979,
            "mae": 0.2636234977958166,
            "precision": 0.7648221343873518,
            "recall": 0.7755511022044088
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8188305976049411,
            "auditor_fn_violation": 0.02512635553250263,
            "auditor_fp_violation": 0.03676314826583473,
            "ave_precision_score": 0.819230579692466,
            "fpr": 0.15367727771679474,
            "logloss": 0.8429571607413955,
            "mae": 0.27346644477764404,
            "precision": 0.7188755020080321,
            "recall": 0.7868131868131868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.8088222009609249,
            "auditor_fn_violation": 0.023432830573427557,
            "auditor_fp_violation": 0.023944925874007057,
            "ave_precision_score": 0.7990146751144356,
            "fpr": 0.14692982456140352,
            "logloss": 2.3955769084033687,
            "mae": 0.2960227238570879,
            "precision": 0.7351778656126482,
            "recall": 0.7454909819639278
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.7625825279354267,
            "auditor_fn_violation": 0.023608882884404294,
            "auditor_fp_violation": 0.030995435900398643,
            "ave_precision_score": 0.7501618742917383,
            "fpr": 0.17233809001097694,
            "logloss": 2.4833343948670312,
            "mae": 0.3184079574268265,
            "precision": 0.6722338204592901,
            "recall": 0.7076923076923077
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8490523257135709,
            "auditor_fn_violation": 0.026412474070948915,
            "auditor_fp_violation": 0.016420818996644153,
            "ave_precision_score": 0.8492935147502324,
            "fpr": 0.12828947368421054,
            "logloss": 0.8487841959270217,
            "mae": 0.2651587811973113,
            "precision": 0.7673956262425448,
            "recall": 0.7735470941883767
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8183156886888346,
            "auditor_fn_violation": 0.019922558232108183,
            "auditor_fp_violation": 0.033970766653186206,
            "ave_precision_score": 0.8186988505129458,
            "fpr": 0.14818880351262348,
            "logloss": 0.8252382168905322,
            "mae": 0.2736356933170161,
            "precision": 0.7193347193347194,
            "recall": 0.7604395604395604
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8270801266366451,
            "auditor_fn_violation": 0.029255880181415464,
            "auditor_fp_violation": 0.022229833057219317,
            "ave_precision_score": 0.8274079206006384,
            "fpr": 0.14912280701754385,
            "logloss": 0.9163008759434453,
            "mae": 0.27937844797208805,
            "precision": 0.7448405253283302,
            "recall": 0.7955911823647295
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.795330337316028,
            "auditor_fn_violation": 0.02261251372118552,
            "auditor_fp_violation": 0.042309395882681455,
            "ave_precision_score": 0.7957838584974622,
            "fpr": 0.18660812294182216,
            "logloss": 0.9324152426083995,
            "mae": 0.2990469337086752,
            "precision": 0.6816479400749064,
            "recall": 0.8
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8446741790308077,
            "auditor_fn_violation": 0.026390500298843298,
            "auditor_fp_violation": 0.01993065290344505,
            "ave_precision_score": 0.8449553446841361,
            "fpr": 0.13706140350877194,
            "logloss": 0.9032911454208133,
            "mae": 0.2661017523897485,
            "precision": 0.7568093385214008,
            "recall": 0.779559118236473
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8145960025708378,
            "auditor_fn_violation": 0.024981604564480526,
            "auditor_fp_violation": 0.03929314229591542,
            "ave_precision_score": 0.815085866890822,
            "fpr": 0.16355653128430298,
            "logloss": 0.8773588982278715,
            "mae": 0.2761263225655592,
            "precision": 0.7072691552062869,
            "recall": 0.7912087912087912
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8451035962589262,
            "auditor_fn_violation": 0.026390500298843298,
            "auditor_fp_violation": 0.0193067414298458,
            "ave_precision_score": 0.8453772785067264,
            "fpr": 0.13486842105263158,
            "logloss": 0.9034843078357799,
            "mae": 0.26615053840353486,
            "precision": 0.759765625,
            "recall": 0.779559118236473
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8159515229238671,
            "auditor_fn_violation": 0.025683646759387703,
            "auditor_fp_violation": 0.04342394130221273,
            "ave_precision_score": 0.8163930594323232,
            "fpr": 0.16355653128430298,
            "logloss": 0.8771030990094456,
            "mae": 0.2758151099829385,
            "precision": 0.7066929133858267,
            "recall": 0.789010989010989
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8345978190423661,
            "auditor_fn_violation": 0.025050100200400802,
            "auditor_fp_violation": 0.020655452189796534,
            "ave_precision_score": 0.8358845464093682,
            "fpr": 0.14802631578947367,
            "logloss": 0.9027371801695645,
            "mae": 0.26587358814762824,
            "precision": 0.7481343283582089,
            "recall": 0.8036072144288577
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8175689789748899,
            "auditor_fn_violation": 0.019348379392287188,
            "auditor_fp_violation": 0.03528029734049724,
            "ave_precision_score": 0.817935307209859,
            "fpr": 0.1800219538968167,
            "logloss": 0.8746940325428628,
            "mae": 0.28247173360763705,
            "precision": 0.6957328385899815,
            "recall": 0.8241758241758241
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8485583161577863,
            "auditor_fn_violation": 0.02312300038673839,
            "auditor_fp_violation": 0.019736842105263157,
            "ave_precision_score": 0.8488034318277506,
            "fpr": 0.13048245614035087,
            "logloss": 0.8684531741585714,
            "mae": 0.2657348310786111,
            "precision": 0.7643564356435644,
            "recall": 0.7735470941883767
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8163640309791899,
            "auditor_fn_violation": 0.022291649075403194,
            "auditor_fp_violation": 0.03872262984574499,
            "ave_precision_score": 0.8167799400371174,
            "fpr": 0.15148188803512624,
            "logloss": 0.8480565814572707,
            "mae": 0.27480541056470087,
            "precision": 0.7172131147540983,
            "recall": 0.7692307692307693
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8411406312911658,
            "auditor_fn_violation": 0.022492353127307248,
            "auditor_fp_violation": 0.01742969712416635,
            "ave_precision_score": 0.841436392461798,
            "fpr": 0.11293859649122807,
            "logloss": 0.8933849139564434,
            "mae": 0.2691498095937761,
            "precision": 0.7845188284518828,
            "recall": 0.751503006012024
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8050282835105522,
            "auditor_fn_violation": 0.019358029456821996,
            "auditor_fp_violation": 0.031756119167292554,
            "ave_precision_score": 0.8059149750959351,
            "fpr": 0.12952799121844127,
            "logloss": 0.8338631083718298,
            "mae": 0.2791921974236699,
            "precision": 0.738359201773836,
            "recall": 0.7318681318681318
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8534872270804115,
            "auditor_fn_violation": 0.022852722989839338,
            "auditor_fp_violation": 0.013123380485111089,
            "ave_precision_score": 0.8537041728436261,
            "fpr": 0.1162280701754386,
            "logloss": 0.8510482923510319,
            "mae": 0.2648132794002596,
            "precision": 0.778705636743215,
            "recall": 0.7474949899799599
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8202152653310937,
            "auditor_fn_violation": 0.019276003908276138,
            "auditor_fp_violation": 0.031139869432087353,
            "ave_precision_score": 0.8205632877939097,
            "fpr": 0.13062568605927552,
            "logloss": 0.8114497868831564,
            "mae": 0.2724795882968345,
            "precision": 0.7407407407407407,
            "recall": 0.7472527472527473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.846219415706581,
            "auditor_fn_violation": 0.03189493021129979,
            "auditor_fp_violation": 0.02146520963425513,
            "ave_precision_score": 0.8464583979022529,
            "fpr": 0.13048245614035087,
            "logloss": 0.9661563379128467,
            "mae": 0.26458166509411246,
            "precision": 0.7643564356435644,
            "recall": 0.7735470941883767
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8135789489344146,
            "auditor_fn_violation": 0.030407353349175523,
            "auditor_fp_violation": 0.03748772314980646,
            "ave_precision_score": 0.8143778924412436,
            "fpr": 0.15697036223929747,
            "logloss": 0.9242521432981801,
            "mae": 0.2775786934326357,
            "precision": 0.7122736418511066,
            "recall": 0.778021978021978
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7915940812325417,
            "auditor_fn_violation": 0.03815745526139999,
            "auditor_fp_violation": 0.0422003100972771,
            "ave_precision_score": 0.7915318759235281,
            "fpr": 0.16228070175438597,
            "logloss": 2.116929550115219,
            "mae": 0.2855085625900116,
            "precision": 0.7264325323475046,
            "recall": 0.7875751503006012
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7527707951921524,
            "auditor_fn_violation": 0.036527906780376596,
            "auditor_fp_violation": 0.04876076029811081,
            "ave_precision_score": 0.7526402879794722,
            "fpr": 0.20087815587266739,
            "logloss": 1.96338266644678,
            "mae": 0.30608603301299053,
            "precision": 0.6648351648351648,
            "recall": 0.7978021978021979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8507007616133869,
            "auditor_fn_violation": 0.025212706113982356,
            "auditor_fp_violation": 0.0164951573849879,
            "ave_precision_score": 0.8509283566662192,
            "fpr": 0.13157894736842105,
            "logloss": 0.8342568604610378,
            "mae": 0.26586543979235033,
            "precision": 0.7628458498023716,
            "recall": 0.7735470941883767
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8198344733749583,
            "auditor_fn_violation": 0.019276003908276138,
            "auditor_fp_violation": 0.03776455408554317,
            "ave_precision_score": 0.820205436325022,
            "fpr": 0.1525795828759605,
            "logloss": 0.8148413353501385,
            "mae": 0.27590569874269516,
            "precision": 0.7151639344262295,
            "recall": 0.7670329670329671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 4866,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8511042878113023,
            "auditor_fn_violation": 0.018005308863340716,
            "auditor_fp_violation": 0.014480056072384359,
            "ave_precision_score": 0.8513338303102032,
            "fpr": 0.13157894736842105,
            "logloss": 0.818095963020651,
            "mae": 0.26327571983309,
            "precision": 0.7623762376237624,
            "recall": 0.7715430861723447
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8132957520389039,
            "auditor_fn_violation": 0.008718833307197742,
            "auditor_fp_violation": 0.028222312091975274,
            "ave_precision_score": 0.8138681074228777,
            "fpr": 0.14489571899012074,
            "logloss": 0.8192272854964353,
            "mae": 0.27671334050793467,
            "precision": 0.7267080745341615,
            "recall": 0.7714285714285715
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.843420498921831,
            "auditor_fn_violation": 0.02519952185071898,
            "auditor_fp_violation": 0.021289983433159177,
            "ave_precision_score": 0.8436752094537496,
            "fpr": 0.13486842105263158,
            "logloss": 0.8877000720572181,
            "mae": 0.266386028807092,
            "precision": 0.759765625,
            "recall": 0.779559118236473
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8090939813452589,
            "auditor_fn_violation": 0.0243543503697181,
            "auditor_fp_violation": 0.0379571321277948,
            "ave_precision_score": 0.8096982058507556,
            "fpr": 0.15806805708013172,
            "logloss": 0.873622479719537,
            "mae": 0.27914170951150175,
            "precision": 0.712,
            "recall": 0.7824175824175824
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8239586353903992,
            "auditor_fn_violation": 0.04725239953591393,
            "auditor_fp_violation": 0.047677456352746284,
            "ave_precision_score": 0.8252111018921284,
            "fpr": 0.15570175438596492,
            "logloss": 0.6660038387161865,
            "mae": 0.3232505602406327,
            "precision": 0.7335834896810507,
            "recall": 0.7835671342685371
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.8188247068512345,
            "auditor_fn_violation": 0.045656867830303614,
            "auditor_fp_violation": 0.05578022993818245,
            "ave_precision_score": 0.8191563277071467,
            "fpr": 0.2052689352360044,
            "logloss": 0.648981437430233,
            "mae": 0.3289707920210637,
            "precision": 0.6636690647482014,
            "recall": 0.810989010989011
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 4866,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.840815130688136,
            "auditor_fn_violation": 0.023597633864219668,
            "auditor_fp_violation": 0.020684656556645857,
            "ave_precision_score": 0.8410861909429548,
            "fpr": 0.13048245614035087,
            "logloss": 0.8657580954806378,
            "mae": 0.2680681298820043,
            "precision": 0.7662082514734774,
            "recall": 0.781563126252505
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8056908789370348,
            "auditor_fn_violation": 0.022479825333831922,
            "auditor_fp_violation": 0.03901390413465057,
            "ave_precision_score": 0.8063222912837692,
            "fpr": 0.15697036223929747,
            "logloss": 0.8546342534106877,
            "mae": 0.2832189668403971,
            "precision": 0.7122736418511066,
            "recall": 0.778021978021978
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8475595570823868,
            "auditor_fn_violation": 0.027128819041591958,
            "auditor_fp_violation": 0.020676691729323307,
            "ave_precision_score": 0.8478199746656341,
            "fpr": 0.12938596491228072,
            "logloss": 0.885280894151675,
            "mae": 0.2655924362970901,
            "precision": 0.766798418972332,
            "recall": 0.7775551102204409
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8171942824074252,
            "auditor_fn_violation": 0.025287994113460634,
            "auditor_fp_violation": 0.03642613669189439,
            "ave_precision_score": 0.8176303982199276,
            "fpr": 0.15367727771679474,
            "logloss": 0.8548688375817817,
            "mae": 0.2741671967428846,
            "precision": 0.7188755020080321,
            "recall": 0.7868131868131868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8487403140791727,
            "auditor_fn_violation": 0.007376595295854877,
            "auditor_fp_violation": 0.012284418673803154,
            "ave_precision_score": 0.8489537593657676,
            "fpr": 0.12280701754385964,
            "logloss": 0.8090270885564549,
            "mae": 0.26625403442308876,
            "precision": 0.7723577235772358,
            "recall": 0.7615230460921844
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8168450902174773,
            "auditor_fn_violation": 0.007358174207790021,
            "auditor_fp_violation": 0.025189207926512216,
            "ave_precision_score": 0.8171967236925011,
            "fpr": 0.14050493962678376,
            "logloss": 0.7968670823285403,
            "mae": 0.2774222487353111,
            "precision": 0.729957805907173,
            "recall": 0.7604395604395604
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 4866,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.7945563533525362,
            "auditor_fn_violation": 0.014548834511127534,
            "auditor_fp_violation": 0.010922433201648192,
            "ave_precision_score": 0.7952241981804797,
            "fpr": 0.11842105263157894,
            "logloss": 0.5514967020132883,
            "mae": 0.368761936856205,
            "precision": 0.7754677754677755,
            "recall": 0.7474949899799599
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.7878694385021014,
            "auditor_fn_violation": 0.007642851111566814,
            "auditor_fp_violation": 0.016571340535752113,
            "ave_precision_score": 0.7894044634009595,
            "fpr": 0.1437980241492865,
            "logloss": 0.5388413979195334,
            "mae": 0.36505945664794726,
            "precision": 0.733739837398374,
            "recall": 0.7934065934065934
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 4866,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8470077972541965,
            "auditor_fn_violation": 0.025355535632668853,
            "auditor_fp_violation": 0.020881122297268608,
            "ave_precision_score": 0.8472797405879164,
            "fpr": 0.13157894736842105,
            "logloss": 0.8701221116490006,
            "mae": 0.2646031799802969,
            "precision": 0.7651663405088063,
            "recall": 0.7835671342685371
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8172109108311114,
            "auditor_fn_violation": 0.024088973595010922,
            "auditor_fp_violation": 0.035191229995955864,
            "ave_precision_score": 0.8176316955254717,
            "fpr": 0.15477497255762898,
            "logloss": 0.8452263493268931,
            "mae": 0.2738496180734772,
            "precision": 0.7191235059760956,
            "recall": 0.7934065934065934
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 4866,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8453866691079351,
            "auditor_fn_violation": 0.022808775445628097,
            "auditor_fp_violation": 0.014400407799158913,
            "ave_precision_score": 0.8456418099103096,
            "fpr": 0.125,
            "logloss": 0.8695947810724166,
            "mae": 0.26697534390853966,
            "precision": 0.768762677484787,
            "recall": 0.7595190380761523
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8182917167028144,
            "auditor_fn_violation": 0.01935320442455459,
            "auditor_fp_violation": 0.03476515107747415,
            "ave_precision_score": 0.8186805864875764,
            "fpr": 0.141602634467618,
            "logloss": 0.8319507578400924,
            "mae": 0.2743478262244358,
            "precision": 0.7266949152542372,
            "recall": 0.7538461538461538
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 4866,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8526702278338174,
            "auditor_fn_violation": 0.023964595858383436,
            "auditor_fp_violation": 0.013338430822819766,
            "ave_precision_score": 0.8528951034047,
            "fpr": 0.12390350877192982,
            "logloss": 0.8187137179359282,
            "mae": 0.2604252419984823,
            "precision": 0.7757936507936508,
            "recall": 0.7835671342685371
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.814741443304756,
            "auditor_fn_violation": 0.018479873584154597,
            "auditor_fp_violation": 0.02586804552544919,
            "ave_precision_score": 0.8151497622829146,
            "fpr": 0.15477497255762898,
            "logloss": 0.8377063012471142,
            "mae": 0.28042057338904297,
            "precision": 0.7157258064516129,
            "recall": 0.7802197802197802
        }
    }
]
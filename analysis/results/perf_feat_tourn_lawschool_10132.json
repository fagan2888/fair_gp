[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 10132,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.7490514230856857,
            "auditor_fn_violation": 0.01965249662618084,
            "auditor_fp_violation": 0.0302768958358774,
            "ave_precision_score": 0.7495755273499771,
            "fpr": 0.1337719298245614,
            "logloss": 2.113963463606145,
            "mae": 0.33439593903508513,
            "precision": 0.7081339712918661,
            "recall": 0.6153846153846154
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7569090514764878,
            "auditor_fn_violation": 0.040856062733376185,
            "auditor_fp_violation": 0.03161260895498448,
            "ave_precision_score": 0.7573925488515211,
            "fpr": 0.12952799121844127,
            "logloss": 2.1996508306125935,
            "mae": 0.3107006639806218,
            "precision": 0.728110599078341,
            "recall": 0.6680761099365751
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8466478080153153,
            "auditor_fn_violation": 0.00299768391873655,
            "auditor_fp_violation": 0.00028747913868197106,
            "ave_precision_score": 0.7899787027243708,
            "fpr": 0.09320175438596491,
            "logloss": 6.7579758381444535,
            "mae": 0.24622058733014207,
            "precision": 0.8014018691588785,
            "recall": 0.7130977130977131
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8555184924720991,
            "auditor_fn_violation": 0.0024251397646338087,
            "auditor_fp_violation": 0.015944142870747687,
            "ave_precision_score": 0.7991730428565634,
            "fpr": 0.09989023051591657,
            "logloss": 6.4603610738301445,
            "mae": 0.23139354853911964,
            "precision": 0.7945823927765236,
            "recall": 0.7441860465116279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8472173730486081,
            "auditor_fn_violation": 0.0012059123901229237,
            "auditor_fp_violation": 0.0016714495054341212,
            "ave_precision_score": 0.7889412973345412,
            "fpr": 0.10416666666666667,
            "logloss": 6.724462789657506,
            "mae": 0.24877979630658348,
            "precision": 0.786036036036036,
            "recall": 0.7255717255717256
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8501734922435761,
            "auditor_fn_violation": 0.006328570467135299,
            "auditor_fp_violation": 0.014184823742287321,
            "ave_precision_score": 0.7870066269243822,
            "fpr": 0.10647639956092206,
            "logloss": 6.589659260272145,
            "mae": 0.23538558865554107,
            "precision": 0.7858719646799117,
            "recall": 0.7526427061310782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8244458665467463,
            "auditor_fn_violation": 0.012980085348506401,
            "auditor_fp_violation": 0.022939817641551678,
            "ave_precision_score": 0.8246739037870637,
            "fpr": 0.20175438596491227,
            "logloss": 1.017878070820426,
            "mae": 0.2831648580107217,
            "precision": 0.6948590381426202,
            "recall": 0.8711018711018711
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8373696208118961,
            "auditor_fn_violation": 0.0072916642492625965,
            "auditor_fp_violation": 0.02429714950202748,
            "ave_precision_score": 0.8377845392938876,
            "fpr": 0.23161361141602635,
            "logloss": 1.0076320573864732,
            "mae": 0.2867537526448851,
            "precision": 0.6713395638629284,
            "recall": 0.9112050739957717
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8486433357947675,
            "auditor_fn_violation": 0.005669383958857651,
            "auditor_fp_violation": 0.00023659787519843996,
            "ave_precision_score": 0.7922573320492117,
            "fpr": 0.09868421052631579,
            "logloss": 6.2629423575028795,
            "mae": 0.23814712578724007,
            "precision": 0.7963800904977375,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.859776699193144,
            "auditor_fn_violation": 0.0026711348029602963,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.8055944035994997,
            "fpr": 0.10098792535675083,
            "logloss": 5.934166121620644,
            "mae": 0.22857694556976368,
            "precision": 0.7955555555555556,
            "recall": 0.7568710359408034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.850574385417391,
            "auditor_fn_violation": 0.0003806944596418335,
            "auditor_fp_violation": 0.001483188830545039,
            "ave_precision_score": 0.7972363267715141,
            "fpr": 0.09100877192982457,
            "logloss": 6.196228545372205,
            "mae": 0.23558626815849415,
            "precision": 0.8069767441860465,
            "recall": 0.7214137214137214
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8626335788435241,
            "auditor_fn_violation": 0.006801994880518355,
            "auditor_fp_violation": 0.013513174844242618,
            "ave_precision_score": 0.8116218980815085,
            "fpr": 0.09001097694840834,
            "logloss": 5.768912967271295,
            "mae": 0.21773235489768963,
            "precision": 0.8127853881278538,
            "recall": 0.7526427061310782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 10132,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7574967211927168,
            "auditor_fn_violation": 0.021827242221979067,
            "auditor_fp_violation": 0.028274718117800305,
            "ave_precision_score": 0.7582222598358331,
            "fpr": 0.14692982456140352,
            "logloss": 1.0669060816088514,
            "mae": 0.3290201666111347,
            "precision": 0.7074235807860262,
            "recall": 0.6735966735966736
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.763704375545494,
            "auditor_fn_violation": 0.023889367212574526,
            "auditor_fp_violation": 0.0287455703752713,
            "ave_precision_score": 0.7642734972491492,
            "fpr": 0.14709110867178923,
            "logloss": 0.9909277266653662,
            "mae": 0.3083598105104122,
            "precision": 0.7190775681341719,
            "recall": 0.7251585623678647
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8498233227487269,
            "auditor_fn_violation": 0.005669383958857651,
            "auditor_fp_violation": 0.00558421866731795,
            "ave_precision_score": 0.7967402076293136,
            "fpr": 0.09100877192982457,
            "logloss": 6.042565842714398,
            "mae": 0.23575311474086372,
            "precision": 0.8091954022988506,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8633818469367498,
            "auditor_fn_violation": 0.006096499676261243,
            "auditor_fp_violation": 0.0188688229603677,
            "ave_precision_score": 0.813396865844882,
            "fpr": 0.09769484083424808,
            "logloss": 5.67298648853802,
            "mae": 0.22498403563251979,
            "precision": 0.8004484304932735,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8488145984077785,
            "auditor_fn_violation": 0.00256227887806835,
            "auditor_fp_violation": 0.005042333211218301,
            "ave_precision_score": 0.7897074831082334,
            "fpr": 0.09210526315789473,
            "logloss": 6.659210354444906,
            "mae": 0.23895297068977966,
            "precision": 0.8060046189376443,
            "recall": 0.7255717255717256
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8552883045040535,
            "auditor_fn_violation": 0.00046414158174809685,
            "auditor_fp_violation": 0.017663363557533743,
            "ave_precision_score": 0.7948349275699946,
            "fpr": 0.10098792535675083,
            "logloss": 6.4075747482587255,
            "mae": 0.22865361181858979,
            "precision": 0.7946428571428571,
            "recall": 0.7526427061310782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.849726277940749,
            "auditor_fn_violation": 0.005669383958857651,
            "auditor_fp_violation": 0.0014501160092807478,
            "ave_precision_score": 0.7957544142645379,
            "fpr": 0.09649122807017543,
            "logloss": 6.082109765670762,
            "mae": 0.23754918696926813,
            "precision": 0.8,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8620514083888287,
            "auditor_fn_violation": 0.0026711348029602963,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.8104829977773607,
            "fpr": 0.10098792535675083,
            "logloss": 5.754414744255765,
            "mae": 0.22767582036927564,
            "precision": 0.7955555555555556,
            "recall": 0.7568710359408034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8493307690336896,
            "auditor_fn_violation": 0.002662581609950036,
            "auditor_fp_violation": 0.0025466072373509173,
            "ave_precision_score": 0.7983095693878237,
            "fpr": 0.08223684210526316,
            "logloss": 6.264391634657318,
            "mae": 0.23621709247339184,
            "precision": 0.8201438848920863,
            "recall": 0.7110187110187111
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8615609994372139,
            "auditor_fn_violation": 0.0021118441969538424,
            "auditor_fp_violation": 0.008781558726673985,
            "ave_precision_score": 0.8155512203360945,
            "fpr": 0.0801317233809001,
            "logloss": 5.767212079624524,
            "mae": 0.21805362029957345,
            "precision": 0.8266033254156769,
            "recall": 0.7357293868921776
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 10132,
        "test": {
            "accuracy": 0.5372807017543859,
            "auc_prc": 0.7642386512458099,
            "auditor_fn_violation": 0.001969580916949338,
            "auditor_fp_violation": 0.002208246835185431,
            "ave_precision_score": 0.5323682718428011,
            "fpr": 0.4594298245614035,
            "logloss": 15.974962622056745,
            "mae": 0.4628771112188896,
            "precision": 0.5328874024526199,
            "recall": 0.9937629937629938
        },
        "train": {
            "accuracy": 0.5301866081229418,
            "auc_prc": 0.7627777777777778,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.002816915527620318,
            "ave_precision_score": 0.5255555555555556,
            "fpr": 0.4698133918770582,
            "logloss": 16.20084363010444,
            "mae": 0.4698135547173505,
            "precision": 0.5249722530521642,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8493749496780378,
            "auditor_fn_violation": 0.00256227887806835,
            "auditor_fp_violation": 0.0009285830585745116,
            "ave_precision_score": 0.7932333596760375,
            "fpr": 0.08991228070175439,
            "logloss": 6.4399447109681205,
            "mae": 0.23651791596680918,
            "precision": 0.8097447795823666,
            "recall": 0.7255717255717256
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.860427583121486,
            "auditor_fn_violation": 0.006096499676261243,
            "auditor_fp_violation": 0.01900916750622779,
            "ave_precision_score": 0.8066938541261193,
            "fpr": 0.09989023051591657,
            "logloss": 6.073338403818582,
            "mae": 0.2264928992703341,
            "precision": 0.796875,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7979775380869885,
            "auditor_fn_violation": 0.015350877192982462,
            "auditor_fp_violation": 0.02073665893271462,
            "ave_precision_score": 0.7983260424190253,
            "fpr": 0.14692982456140352,
            "logloss": 0.9883145301767169,
            "mae": 0.29415982322889955,
            "precision": 0.7237113402061855,
            "recall": 0.7297297297297297
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8041937875030932,
            "auditor_fn_violation": 0.01676943534855873,
            "auditor_fp_violation": 0.013092141206662355,
            "ave_precision_score": 0.8047077787943746,
            "fpr": 0.14928649835345773,
            "logloss": 0.9113308911035602,
            "mae": 0.27604856642453734,
            "precision": 0.7328094302554028,
            "recall": 0.7885835095137421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8497277674379801,
            "auditor_fn_violation": 0.005669383958857651,
            "auditor_fp_violation": 0.0014501160092807478,
            "ave_precision_score": 0.7957573031554993,
            "fpr": 0.09649122807017543,
            "logloss": 6.07718648416062,
            "mae": 0.23751543492288787,
            "precision": 0.8,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8620482850737353,
            "auditor_fn_violation": 0.0026711348029602963,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.8104825721437569,
            "fpr": 0.10098792535675083,
            "logloss": 5.74975346514872,
            "mae": 0.22764113317225465,
            "precision": 0.7955555555555556,
            "recall": 0.7568710359408034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8496922672661642,
            "auditor_fn_violation": 0.0012743006164058887,
            "auditor_fp_violation": 0.0007276020678145501,
            "ave_precision_score": 0.7956955326619444,
            "fpr": 0.09100877192982457,
            "logloss": 6.140839653082167,
            "mae": 0.2368621739981542,
            "precision": 0.8087557603686636,
            "recall": 0.7297297297297297
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8619708568888207,
            "auditor_fn_violation": 0.006096499676261243,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.81030236156584,
            "fpr": 0.10098792535675083,
            "logloss": 5.797439397911052,
            "mae": 0.22682948857905968,
            "precision": 0.7951002227171492,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8482006154944007,
            "auditor_fn_violation": 0.0029338549075391155,
            "auditor_fp_violation": 0.005500264582570116,
            "ave_precision_score": 0.7959903266230771,
            "fpr": 0.08442982456140351,
            "logloss": 6.206335273288266,
            "mae": 0.23601587806388846,
            "precision": 0.8166666666666667,
            "recall": 0.7130977130977131
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8617818558228825,
            "auditor_fn_violation": 0.00028080565695759593,
            "auditor_fp_violation": 0.01810444641595116,
            "ave_precision_score": 0.8135159542961797,
            "fpr": 0.0889132821075741,
            "logloss": 5.7473942072357,
            "mae": 0.22174234670352683,
            "precision": 0.812933025404157,
            "recall": 0.7441860465116279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7989857809312093,
            "auditor_fn_violation": 0.011792409818725615,
            "auditor_fp_violation": 0.020215125982008388,
            "ave_precision_score": 0.8003707037363273,
            "fpr": 0.1425438596491228,
            "logloss": 0.9437033783188405,
            "mae": 0.2954330609817516,
            "precision": 0.7274633123689728,
            "recall": 0.7214137214137214
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8073245965397895,
            "auditor_fn_violation": 0.018229160623156486,
            "auditor_fp_violation": 0.014676029652797625,
            "ave_precision_score": 0.8078369251067924,
            "fpr": 0.14489571899012074,
            "logloss": 0.8734855770403278,
            "mae": 0.2762599843093391,
            "precision": 0.736,
            "recall": 0.7780126849894292
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8490752416654759,
            "auditor_fn_violation": 0.00256227887806835,
            "auditor_fp_violation": 0.003162270525501692,
            "ave_precision_score": 0.7899819885831283,
            "fpr": 0.09539473684210527,
            "logloss": 6.642874847022592,
            "mae": 0.23996484254415387,
            "precision": 0.8004587155963303,
            "recall": 0.7255717255717256
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8555944721338886,
            "auditor_fn_violation": 0.0005035936161966851,
            "auditor_fp_violation": 0.017663363557533743,
            "ave_precision_score": 0.7951278663009318,
            "fpr": 0.10098792535675083,
            "logloss": 6.3970133360200805,
            "mae": 0.22943366516904454,
            "precision": 0.7951002227171492,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8539264515925803,
            "auditor_fn_violation": 0.008352482036692565,
            "auditor_fp_violation": 0.0018164611063621933,
            "ave_precision_score": 0.8122324371697011,
            "fpr": 0.0800438596491228,
            "logloss": 5.374270929785667,
            "mae": 0.23890575359781427,
            "precision": 0.821078431372549,
            "recall": 0.6964656964656964
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8669581774510666,
            "auditor_fn_violation": 0.002777887366762358,
            "auditor_fp_violation": 0.013698630136986308,
            "ave_precision_score": 0.8298306089716019,
            "fpr": 0.08342480790340286,
            "logloss": 4.924227318727251,
            "mae": 0.21590384998334902,
            "precision": 0.8215962441314554,
            "recall": 0.7399577167019028
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8495228081124612,
            "auditor_fn_violation": 0.006223328591749653,
            "auditor_fp_violation": 0.00558421866731795,
            "ave_precision_score": 0.7976211356934886,
            "fpr": 0.09100877192982457,
            "logloss": 5.8220860899396145,
            "mae": 0.23429450622502462,
            "precision": 0.8087557603686636,
            "recall": 0.7297297297297297
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.863079361161889,
            "auditor_fn_violation": 0.0007472679466144388,
            "auditor_fp_violation": 0.016969159286047246,
            "ave_precision_score": 0.8157978724215191,
            "fpr": 0.09549945115257959,
            "logloss": 5.428899912359344,
            "mae": 0.22211252817974847,
            "precision": 0.8036117381489842,
            "recall": 0.7526427061310782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8497450699504338,
            "auditor_fn_violation": 0.005669383958857651,
            "auditor_fp_violation": 0.004006899499328368,
            "ave_precision_score": 0.7957732779454574,
            "fpr": 0.09320175438596491,
            "logloss": 6.0853614471655835,
            "mae": 0.23642641435151054,
            "precision": 0.8054919908466819,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8620881185420678,
            "auditor_fn_violation": 0.006096499676261243,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.8105177436677282,
            "fpr": 0.10098792535675083,
            "logloss": 5.75165465662231,
            "mae": 0.2266953367812367,
            "precision": 0.7951002227171492,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 10132,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.8329333128116403,
            "auditor_fn_violation": 0.007825892694313748,
            "auditor_fp_violation": 0.02103177026091913,
            "ave_precision_score": 0.8333511267921143,
            "fpr": 0.30372807017543857,
            "logloss": 1.208101029471054,
            "mae": 0.3339898489733709,
            "precision": 0.6241519674355496,
            "recall": 0.9563409563409564
        },
        "train": {
            "accuracy": 0.6564215148188803,
            "auc_prc": 0.8523408437796475,
            "auditor_fn_violation": 0.006024557731090291,
            "auditor_fp_violation": 0.030068818950523546,
            "ave_precision_score": 0.8527235041766139,
            "fpr": 0.32821075740944017,
            "logloss": 1.244425853162613,
            "mae": 0.34346588246560306,
            "precision": 0.6055408970976254,
            "recall": 0.9704016913319239
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8095097028221511,
            "auditor_fn_violation": 0.02016996753838859,
            "auditor_fp_violation": 0.03315931941221965,
            "ave_precision_score": 0.810156742998559,
            "fpr": 0.13157894736842105,
            "logloss": 1.8596351856009257,
            "mae": 0.28062925974908853,
            "precision": 0.7468354430379747,
            "recall": 0.735966735966736
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8338003582511081,
            "auditor_fn_violation": 0.030078695205185392,
            "auditor_fp_violation": 0.02849495511480686,
            "ave_precision_score": 0.8338652456520015,
            "fpr": 0.13611416026344675,
            "logloss": 1.9650061379363408,
            "mae": 0.26859644966862956,
            "precision": 0.7432712215320911,
            "recall": 0.758985200845666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8478428475973188,
            "auditor_fn_violation": 0.0029338549075391155,
            "auditor_fp_violation": 0.0018953270647616746,
            "ave_precision_score": 0.7956214991534885,
            "fpr": 0.08442982456140351,
            "logloss": 6.243337280327037,
            "mae": 0.23682388183820363,
            "precision": 0.8166666666666667,
            "recall": 0.7130977130977131
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8613190988472185,
            "auditor_fn_violation": 0.00028080565695759593,
            "auditor_fp_violation": 0.01748793287520864,
            "ave_precision_score": 0.8129489848441166,
            "fpr": 0.08781558726673985,
            "logloss": 5.78320900158687,
            "mae": 0.221640872446295,
            "precision": 0.8148148148148148,
            "recall": 0.7441860465116279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.8325721404028271,
            "auditor_fn_violation": 0.00945353247984827,
            "auditor_fp_violation": 0.023138254569137467,
            "ave_precision_score": 0.8329862991705377,
            "fpr": 0.24561403508771928,
            "logloss": 1.145632716019481,
            "mae": 0.29918272939828855,
            "precision": 0.6636636636636637,
            "recall": 0.918918918918919
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.8503618598762905,
            "auditor_fn_violation": 0.009187682610703568,
            "auditor_fp_violation": 0.021397530938453917,
            "ave_precision_score": 0.8505011739100687,
            "fpr": 0.270032930845225,
            "logloss": 1.16096494175968,
            "mae": 0.30815600566030893,
            "precision": 0.6434782608695652,
            "recall": 0.9386892177589852
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8016007669926943,
            "auditor_fn_violation": 0.013992231097494263,
            "auditor_fp_violation": 0.021097915903447725,
            "ave_precision_score": 0.8029076700587962,
            "fpr": 0.1524122807017544,
            "logloss": 0.9120607338949985,
            "mae": 0.29269742790688114,
            "precision": 0.7203219315895373,
            "recall": 0.7442827442827443
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8110828132341759,
            "auditor_fn_violation": 0.016736945437836364,
            "auditor_fp_violation": 0.01692404853916365,
            "ave_precision_score": 0.8115772812245069,
            "fpr": 0.15477497255762898,
            "logloss": 0.8441124643156966,
            "mae": 0.27490481745385803,
            "precision": 0.7267441860465116,
            "recall": 0.7928118393234672
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8606868235511285,
            "auditor_fn_violation": 0.008473301236459134,
            "auditor_fp_violation": 0.012939105303862912,
            "ave_precision_score": 0.8588945240272307,
            "fpr": 0.19188596491228072,
            "logloss": 1.1392521748782982,
            "mae": 0.25836428601247047,
            "precision": 0.7116968698517299,
            "recall": 0.8981288981288982
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8915108218130632,
            "auditor_fn_violation": 0.008134081220135392,
            "auditor_fp_violation": 0.01983619786576045,
            "ave_precision_score": 0.8906417097868832,
            "fpr": 0.20965971459934138,
            "logloss": 1.023546633838448,
            "mae": 0.2693981099712578,
            "precision": 0.6909385113268608,
            "recall": 0.9027484143763214
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8489360433696985,
            "auditor_fn_violation": 0.0022499726447094853,
            "auditor_fp_violation": 0.0004579313713518115,
            "ave_precision_score": 0.7935189309724459,
            "fpr": 0.09978070175438597,
            "logloss": 6.072668457855696,
            "mae": 0.23923068574518153,
            "precision": 0.795045045045045,
            "recall": 0.7338877338877339
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8613077535362157,
            "auditor_fn_violation": 0.0009654144900360414,
            "auditor_fp_violation": 0.017578154368975838,
            "ave_precision_score": 0.8089618676385444,
            "fpr": 0.10537870472008781,
            "logloss": 5.749794451801423,
            "mae": 0.22938588714771718,
            "precision": 0.789010989010989,
            "recall": 0.758985200845666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8491204387922837,
            "auditor_fn_violation": 0.00256227887806835,
            "auditor_fp_violation": 0.005240770138804085,
            "ave_precision_score": 0.7900088621545975,
            "fpr": 0.09320175438596491,
            "logloss": 6.646819469971451,
            "mae": 0.23968723207713719,
            "precision": 0.804147465437788,
            "recall": 0.7255717255717256
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8555903852403318,
            "auditor_fn_violation": 0.0005035936161966851,
            "auditor_fp_violation": 0.017663363557533743,
            "ave_precision_score": 0.7951245519206326,
            "fpr": 0.10098792535675083,
            "logloss": 6.400160618831929,
            "mae": 0.22928677449681295,
            "precision": 0.7951002227171492,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8682981628962241,
            "auditor_fn_violation": 0.008204307546412814,
            "auditor_fp_violation": 0.00416971954247568,
            "ave_precision_score": 0.8586583581420955,
            "fpr": 0.08223684210526316,
            "logloss": 2.105728953544359,
            "mae": 0.2304899069356903,
            "precision": 0.8218527315914489,
            "recall": 0.7193347193347194
        },
        "train": {
            "accuracy": 0.7936333699231614,
            "auc_prc": 0.8912263791243076,
            "auditor_fn_violation": 0.0032373875326929752,
            "auditor_fp_violation": 0.013989343839125055,
            "ave_precision_score": 0.8881482120122376,
            "fpr": 0.07793633369923161,
            "logloss": 1.689092620273769,
            "mae": 0.2060356316949289,
            "precision": 0.8337236533957846,
            "recall": 0.7526427061310782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.849273789832481,
            "auditor_fn_violation": 0.0032712368238684056,
            "auditor_fp_violation": 0.0027704847966784733,
            "ave_precision_score": 0.7961318354803816,
            "fpr": 0.08662280701754387,
            "logloss": 6.225070500813412,
            "mae": 0.23538020450835856,
            "precision": 0.8136792452830188,
            "recall": 0.7172557172557172
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8606246157979076,
            "auditor_fn_violation": 0.0050846710280503995,
            "auditor_fp_violation": 0.01810444641595116,
            "ave_precision_score": 0.8106280446885152,
            "fpr": 0.0889132821075741,
            "logloss": 5.805990563487976,
            "mae": 0.22217458837988552,
            "precision": 0.8137931034482758,
            "recall": 0.7484143763213531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8484905815966524,
            "auditor_fn_violation": 0.00220893970893971,
            "auditor_fp_violation": 0.0040883095209020255,
            "ave_precision_score": 0.7896996725853892,
            "fpr": 0.09429824561403509,
            "logloss": 6.564093267628823,
            "mae": 0.23836677802329814,
            "precision": 0.8027522935779816,
            "recall": 0.7276507276507277
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8558366432650397,
            "auditor_fn_violation": 0.0005035936161966851,
            "auditor_fp_violation": 0.017663363557533743,
            "ave_precision_score": 0.7964309804288456,
            "fpr": 0.10098792535675083,
            "logloss": 6.3004983670039,
            "mae": 0.22892461920684284,
            "precision": 0.7951002227171492,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8486015816129091,
            "auditor_fn_violation": 0.0029338549075391155,
            "auditor_fp_violation": 0.0030681401880571523,
            "ave_precision_score": 0.7970901119289677,
            "fpr": 0.08333333333333333,
            "logloss": 6.218373489555048,
            "mae": 0.23651507978303032,
            "precision": 0.8186157517899761,
            "recall": 0.7130977130977131
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8615232872170091,
            "auditor_fn_violation": 0.00028080565695759593,
            "auditor_fp_violation": 0.01810444641595116,
            "ave_precision_score": 0.8132146747799688,
            "fpr": 0.0889132821075741,
            "logloss": 5.775303549426989,
            "mae": 0.2219392764714549,
            "precision": 0.812933025404157,
            "recall": 0.7441860465116279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8486173800288191,
            "auditor_fn_violation": 0.0009665535981325513,
            "auditor_fp_violation": 0.00018826067488908046,
            "ave_precision_score": 0.7929103548274326,
            "fpr": 0.08881578947368421,
            "logloss": 6.578682634358031,
            "mae": 0.23595444102158325,
            "precision": 0.8107476635514018,
            "recall": 0.7214137214137214
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8612332359134355,
            "auditor_fn_violation": 0.007101366200745878,
            "auditor_fp_violation": 0.018971575217158122,
            "ave_precision_score": 0.808819985453506,
            "fpr": 0.09440175631174534,
            "logloss": 6.157705831668412,
            "mae": 0.22420002153084126,
            "precision": 0.8045454545454546,
            "recall": 0.7484143763213531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8478773961344092,
            "auditor_fn_violation": 0.0018419228945544797,
            "auditor_fp_violation": 0.0030681401880571523,
            "ave_precision_score": 0.7956345395288797,
            "fpr": 0.08333333333333333,
            "logloss": 6.331994724086239,
            "mae": 0.2373043374323776,
            "precision": 0.8173076923076923,
            "recall": 0.7068607068607069
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8614053923940859,
            "auditor_fn_violation": 0.00028080565695759593,
            "auditor_fp_violation": 0.01810444641595116,
            "ave_precision_score": 0.8129293349361749,
            "fpr": 0.0889132821075741,
            "logloss": 5.851789102404152,
            "mae": 0.2219471471491946,
            "precision": 0.812933025404157,
            "recall": 0.7441860465116279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8478827019666627,
            "auditor_fn_violation": 0.0029338549075391155,
            "auditor_fp_violation": 0.0018953270647616746,
            "ave_precision_score": 0.7956675089381584,
            "fpr": 0.08442982456140351,
            "logloss": 6.231106438068974,
            "mae": 0.23676893195026194,
            "precision": 0.8166666666666667,
            "recall": 0.7130977130977131
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8615050041429854,
            "auditor_fn_violation": 0.00028080565695759593,
            "auditor_fp_violation": 0.01748793287520864,
            "ave_precision_score": 0.8132273556396918,
            "fpr": 0.08781558726673985,
            "logloss": 5.771042368677291,
            "mae": 0.22168143214361669,
            "precision": 0.8148148148148148,
            "recall": 0.7441860465116279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8493682837788169,
            "auditor_fn_violation": 0.0009665535981325513,
            "auditor_fp_violation": 0.0012974722188301366,
            "ave_precision_score": 0.7960508342038085,
            "fpr": 0.08771929824561403,
            "logloss": 6.312342667307504,
            "mae": 0.2355196525380841,
            "precision": 0.8126463700234192,
            "recall": 0.7214137214137214
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8599494763618354,
            "auditor_fn_violation": 0.003144559216343352,
            "auditor_fp_violation": 0.020111874652271326,
            "ave_precision_score": 0.8090639566893778,
            "fpr": 0.09330406147091108,
            "logloss": 5.914716214744923,
            "mae": 0.22375410300585133,
            "precision": 0.8068181818181818,
            "recall": 0.7505285412262156
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 10132,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.8506294091707828,
            "auditor_fn_violation": 0.0022499726447094853,
            "auditor_fp_violation": 0.00558421866731795,
            "ave_precision_score": 0.7985426938951002,
            "fpr": 0.09100877192982457,
            "logloss": 5.83072034082409,
            "mae": 0.23564865069057886,
            "precision": 0.8096330275229358,
            "recall": 0.7338877338877339
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8620748727732734,
            "auditor_fn_violation": 0.0007472679466144388,
            "auditor_fp_violation": 0.0188688229603677,
            "ave_precision_score": 0.8123520628173093,
            "fpr": 0.09769484083424808,
            "logloss": 5.505631871773205,
            "mae": 0.22400821262775975,
            "precision": 0.8,
            "recall": 0.7526427061310782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8492545718283934,
            "auditor_fn_violation": 0.0022499726447094853,
            "auditor_fp_violation": 0.0019665608336386226,
            "ave_precision_score": 0.7937787559960034,
            "fpr": 0.09649122807017543,
            "logloss": 6.067651525008692,
            "mae": 0.2375815403538226,
            "precision": 0.800453514739229,
            "recall": 0.7338877338877339
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8605708201170033,
            "auditor_fn_violation": 0.0009654144900360414,
            "auditor_fp_violation": 0.018986612132785997,
            "ave_precision_score": 0.8074639853347839,
            "fpr": 0.10318331503841932,
            "logloss": 5.762351102331413,
            "mae": 0.22806572483745083,
            "precision": 0.7924944812362031,
            "recall": 0.758985200845666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.8012339513622625,
            "auditor_fn_violation": 0.015603913630229428,
            "auditor_fp_violation": 0.019968351854113248,
            "ave_precision_score": 0.8026219803144353,
            "fpr": 0.14802631578947367,
            "logloss": 0.9210121860153806,
            "mae": 0.2934270263514879,
            "precision": 0.7210743801652892,
            "recall": 0.7255717255717256
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8109058155697919,
            "auditor_fn_violation": 0.01817578434125546,
            "auditor_fp_violation": 0.013836468530241747,
            "ave_precision_score": 0.8114001567823296,
            "fpr": 0.145993413830955,
            "logloss": 0.8461632161007088,
            "mae": 0.27446210683010275,
            "precision": 0.7361111111111112,
            "recall": 0.7843551797040169
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8495811994710344,
            "auditor_fn_violation": 0.0029338549075391155,
            "auditor_fp_violation": 0.0027704847966784733,
            "ave_precision_score": 0.7960207966018217,
            "fpr": 0.08662280701754387,
            "logloss": 6.402578046480279,
            "mae": 0.23572348876603674,
            "precision": 0.8127962085308057,
            "recall": 0.7130977130977131
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8610991679801118,
            "auditor_fn_violation": 0.003153842047978313,
            "auditor_fp_violation": 0.01810444641595116,
            "ave_precision_score": 0.8099506839866499,
            "fpr": 0.0889132821075741,
            "logloss": 5.976115351565549,
            "mae": 0.22189196578991485,
            "precision": 0.8133640552995391,
            "recall": 0.7463002114164905
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8272766642645935,
            "auditor_fn_violation": 0.012403344640186744,
            "auditor_fp_violation": 0.0046429152928725535,
            "ave_precision_score": 0.7778677935743792,
            "fpr": 0.08223684210526316,
            "logloss": 7.261539626915676,
            "mae": 0.26221726941826445,
            "precision": 0.8081841432225064,
            "recall": 0.656964656964657
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.839433358576594,
            "auditor_fn_violation": 0.014956962471832412,
            "auditor_fp_violation": 0.011382945130294881,
            "ave_precision_score": 0.7961160098077205,
            "fpr": 0.07025246981339188,
            "logloss": 6.9179354343585135,
            "mae": 0.24763412952491376,
            "precision": 0.8293333333333334,
            "recall": 0.6575052854122622
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8518833226121977,
            "auditor_fn_violation": 0.002626107889265792,
            "auditor_fp_violation": 0.0032614889892945806,
            "ave_precision_score": 0.8045307470879147,
            "fpr": 0.08333333333333333,
            "logloss": 5.82699390637844,
            "mae": 0.235770864004781,
            "precision": 0.819047619047619,
            "recall": 0.7151767151767152
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8623524881821096,
            "auditor_fn_violation": 0.0006753260014434784,
            "auditor_fp_violation": 0.020159491551759577,
            "ave_precision_score": 0.8159789269971844,
            "fpr": 0.09001097694840834,
            "logloss": 5.464854130713095,
            "mae": 0.22115564752813213,
            "precision": 0.8123569794050344,
            "recall": 0.7505285412262156
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8496395022222986,
            "auditor_fn_violation": 0.00220893970893971,
            "auditor_fp_violation": 0.0007912036471689717,
            "ave_precision_score": 0.7946658379360728,
            "fpr": 0.09100877192982457,
            "logloss": 6.281992631007139,
            "mae": 0.23715187632792342,
            "precision": 0.8083140877598153,
            "recall": 0.7276507276507277
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8607961800501496,
            "auditor_fn_violation": 0.006096499676261243,
            "auditor_fp_violation": 0.01839014781288062,
            "ave_precision_score": 0.8081411032213108,
            "fpr": 0.09879253567508232,
            "logloss": 5.933492548254832,
            "mae": 0.22707378966745273,
            "precision": 0.7986577181208053,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8498239367874993,
            "auditor_fn_violation": 0.005669383958857651,
            "auditor_fp_violation": 0.00558421866731795,
            "ave_precision_score": 0.7967388706102272,
            "fpr": 0.09100877192982457,
            "logloss": 6.042311915856352,
            "mae": 0.23576680367072453,
            "precision": 0.8091954022988506,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8633843802326325,
            "auditor_fn_violation": 0.006096499676261243,
            "auditor_fp_violation": 0.0188688229603677,
            "ave_precision_score": 0.8134000377510764,
            "fpr": 0.09769484083424808,
            "logloss": 5.673397961698439,
            "mae": 0.22500843820396696,
            "precision": 0.8004484304932735,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8497288311558598,
            "auditor_fn_violation": 0.005669383958857651,
            "auditor_fp_violation": 0.0014501160092807478,
            "ave_precision_score": 0.7957569661383794,
            "fpr": 0.09649122807017543,
            "logloss": 6.082076074878195,
            "mae": 0.2375536994212971,
            "precision": 0.8,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8620514083888287,
            "auditor_fn_violation": 0.0026711348029602963,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.8104829977773607,
            "fpr": 0.10098792535675083,
            "logloss": 5.754414989391443,
            "mae": 0.22768081050100697,
            "precision": 0.7955555555555556,
            "recall": 0.7568710359408034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8500155328887729,
            "auditor_fn_violation": 0.0038411387095597708,
            "auditor_fp_violation": 0.0030681401880571523,
            "ave_precision_score": 0.8009024429879532,
            "fpr": 0.08333333333333333,
            "logloss": 6.106739894936876,
            "mae": 0.2370067806595986,
            "precision": 0.8177458033573142,
            "recall": 0.7089397089397089
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8630033424025116,
            "auditor_fn_violation": 0.00028080565695759593,
            "auditor_fp_violation": 0.01810444641595116,
            "ave_precision_score": 0.8164051183616186,
            "fpr": 0.0889132821075741,
            "logloss": 5.674485971133719,
            "mae": 0.22199887074829655,
            "precision": 0.812933025404157,
            "recall": 0.7441860465116279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8516700906492088,
            "auditor_fn_violation": 0.0031048254732465295,
            "auditor_fp_violation": 0.0031164773883665077,
            "ave_precision_score": 0.8033895816265818,
            "fpr": 0.08552631578947369,
            "logloss": 5.907170776535712,
            "mae": 0.23555444627246142,
            "precision": 0.8156028368794326,
            "recall": 0.7172557172557172
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8633390036292291,
            "auditor_fn_violation": 0.0050846710280503995,
            "auditor_fp_violation": 0.01810444641595116,
            "ave_precision_score": 0.8168488064614838,
            "fpr": 0.0889132821075741,
            "logloss": 5.518339213787664,
            "mae": 0.22095149786198898,
            "precision": 0.8137931034482758,
            "recall": 0.7484143763213531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.848962327199453,
            "auditor_fn_violation": 0.0029338549075391155,
            "auditor_fp_violation": 0.0031164773883665077,
            "ave_precision_score": 0.7966409959694418,
            "fpr": 0.08552631578947369,
            "logloss": 6.234878486725588,
            "mae": 0.235993507450822,
            "precision": 0.8147268408551069,
            "recall": 0.7130977130977131
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8615313532780476,
            "auditor_fn_violation": 0.003153842047978313,
            "auditor_fp_violation": 0.01810444641595116,
            "ave_precision_score": 0.8131292403915652,
            "fpr": 0.0889132821075741,
            "logloss": 5.77719967017299,
            "mae": 0.22159813001139608,
            "precision": 0.8133640552995391,
            "recall": 0.7463002114164905
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8481114780320409,
            "auditor_fn_violation": 0.0033966152387205025,
            "auditor_fp_violation": 0.0002849350755077945,
            "ave_precision_score": 0.7939002619028317,
            "fpr": 0.08771929824561403,
            "logloss": 6.416574022998392,
            "mae": 0.2392715496158118,
            "precision": 0.8113207547169812,
            "recall": 0.7151767151767152
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8586828562762051,
            "auditor_fn_violation": 0.0038245266336043144,
            "auditor_fp_violation": 0.01839014781288062,
            "ave_precision_score": 0.8060921858312212,
            "fpr": 0.09220636663007684,
            "logloss": 6.060924902730964,
            "mae": 0.22461262105327476,
            "precision": 0.8077803203661327,
            "recall": 0.7463002114164905
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8496337818860304,
            "auditor_fn_violation": 0.00220893970893971,
            "auditor_fp_violation": 0.0007912036471689717,
            "ave_precision_score": 0.7946601210976569,
            "fpr": 0.09100877192982457,
            "logloss": 6.281893342433161,
            "mae": 0.23716041708388738,
            "precision": 0.8083140877598153,
            "recall": 0.7276507276507277
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8607903836694172,
            "auditor_fn_violation": 0.006096499676261243,
            "auditor_fp_violation": 0.01839014781288062,
            "ave_precision_score": 0.8081360301877195,
            "fpr": 0.09879253567508232,
            "logloss": 5.93346353190421,
            "mae": 0.22708093095229434,
            "precision": 0.7986577181208053,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8488878449036028,
            "auditor_fn_violation": 0.004659517817412553,
            "auditor_fp_violation": 0.0003612569707330987,
            "ave_precision_score": 0.7914536898293245,
            "fpr": 0.08552631578947369,
            "logloss": 6.695474268093141,
            "mae": 0.23648068717878457,
            "precision": 0.8151658767772512,
            "recall": 0.7151767151767152
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8608156807024482,
            "auditor_fn_violation": 0.003153842047978313,
            "auditor_fp_violation": 0.016924048539163652,
            "ave_precision_score": 0.8073246931091218,
            "fpr": 0.0889132821075741,
            "logloss": 6.235591666311775,
            "mae": 0.22214110491315656,
            "precision": 0.8133640552995391,
            "recall": 0.7463002114164905
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8499510702237478,
            "auditor_fn_violation": 0.0022499726447094853,
            "auditor_fp_violation": 0.0016994342003500645,
            "ave_precision_score": 0.7952446501050208,
            "fpr": 0.09758771929824561,
            "logloss": 6.0505088750356695,
            "mae": 0.2380817614425625,
            "precision": 0.7986425339366516,
            "recall": 0.7338877338877339
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.862400390509948,
            "auditor_fn_violation": 0.0009654144900360414,
            "auditor_fp_violation": 0.018284889403485563,
            "ave_precision_score": 0.8107915797310649,
            "fpr": 0.10428100987925357,
            "logloss": 5.69908604770934,
            "mae": 0.22693675557827964,
            "precision": 0.7907488986784141,
            "recall": 0.758985200845666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 10132,
        "test": {
            "accuracy": 0.6206140350877193,
            "auc_prc": 0.7533982215815126,
            "auditor_fn_violation": 0.01735009300798774,
            "auditor_fp_violation": 0.010916575080392397,
            "ave_precision_score": 0.7540034468958862,
            "fpr": 0.047149122807017545,
            "logloss": 2.0304300735689402,
            "mae": 0.3856225983602189,
            "precision": 0.8054298642533937,
            "recall": 0.3700623700623701
        },
        "train": {
            "accuracy": 0.6344676180021954,
            "auc_prc": 0.7455088779489146,
            "auditor_fn_violation": 0.020580037734710604,
            "auditor_fp_violation": 0.006656341317935537,
            "ave_precision_score": 0.7467277814673347,
            "fpr": 0.0570801317233809,
            "logloss": 1.6558527300785135,
            "mae": 0.3698469571053359,
            "precision": 0.7868852459016393,
            "recall": 0.4059196617336152
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8492066642995233,
            "auditor_fn_violation": 0.0009665535981325513,
            "auditor_fp_violation": 0.00018826067488908046,
            "ave_precision_score": 0.7926090813011534,
            "fpr": 0.08881578947368421,
            "logloss": 6.640768861011815,
            "mae": 0.2359202484322921,
            "precision": 0.8107476635514018,
            "recall": 0.7214137214137214
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.859754874058465,
            "auditor_fn_violation": 0.007101366200745878,
            "auditor_fp_violation": 0.018971575217158122,
            "ave_precision_score": 0.8056650873762827,
            "fpr": 0.09440175631174534,
            "logloss": 6.241088755024347,
            "mae": 0.22419642168237727,
            "precision": 0.8045454545454546,
            "recall": 0.7484143763213531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8495830604386432,
            "auditor_fn_violation": 0.0029338549075391155,
            "auditor_fp_violation": 0.005500264582570116,
            "ave_precision_score": 0.7987997268613954,
            "fpr": 0.08442982456140351,
            "logloss": 6.157941176020149,
            "mae": 0.2363259355007236,
            "precision": 0.8166666666666667,
            "recall": 0.7130977130977131
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8631341868336174,
            "auditor_fn_violation": 0.00028080565695759593,
            "auditor_fp_violation": 0.01810444641595116,
            "ave_precision_score": 0.8165233432005585,
            "fpr": 0.0889132821075741,
            "logloss": 5.694089956962838,
            "mae": 0.22120672779851536,
            "precision": 0.812933025404157,
            "recall": 0.7441860465116279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8486900873502137,
            "auditor_fn_violation": 0.005669383958857651,
            "auditor_fp_violation": 0.0010176252696707014,
            "ave_precision_score": 0.7923569846518517,
            "fpr": 0.09758771929824561,
            "logloss": 6.255484640041289,
            "mae": 0.2381679172346501,
            "precision": 0.7981859410430839,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8597746224301355,
            "auditor_fn_violation": 0.0026711348029602963,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.805590898318594,
            "fpr": 0.10098792535675083,
            "logloss": 5.927692355458946,
            "mae": 0.2286035078454406,
            "precision": 0.7955555555555556,
            "recall": 0.7568710359408034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8487967214532111,
            "auditor_fn_violation": 0.005669383958857651,
            "auditor_fp_violation": 0.0014348516302356824,
            "ave_precision_score": 0.7950988613809138,
            "fpr": 0.09429824561403509,
            "logloss": 6.004631488657678,
            "mae": 0.23869323409775534,
            "precision": 0.8036529680365296,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8620105733143972,
            "auditor_fn_violation": 0.006096499676261243,
            "auditor_fp_violation": 0.018051817211253627,
            "ave_precision_score": 0.8113241422797695,
            "fpr": 0.10208562019758508,
            "logloss": 5.666380867314265,
            "mae": 0.22750149786061757,
            "precision": 0.7933333333333333,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8487875389487296,
            "auditor_fn_violation": 0.005129116971222238,
            "auditor_fp_violation": 0.0012643993975658414,
            "ave_precision_score": 0.7928688138384496,
            "fpr": 0.08333333333333333,
            "logloss": 6.905184102340946,
            "mae": 0.24282099453945982,
            "precision": 0.8146341463414634,
            "recall": 0.6943866943866944
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.859418152842877,
            "auditor_fn_violation": 0.002578306486610678,
            "auditor_fp_violation": 0.017157120731395574,
            "ave_precision_score": 0.8065507379660325,
            "fpr": 0.08562019758507135,
            "logloss": 6.375445828877474,
            "mae": 0.2217933029300475,
            "precision": 0.8177570093457944,
            "recall": 0.7399577167019028
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 10132,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.7556193402316997,
            "auditor_fn_violation": 0.01183800196958092,
            "auditor_fp_violation": 0.02873773761550047,
            "ave_precision_score": 0.7562443610689812,
            "fpr": 0.2050438596491228,
            "logloss": 1.1685467970344634,
            "mae": 0.3251527549369315,
            "precision": 0.670774647887324,
            "recall": 0.7920997920997921
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7556336018035488,
            "auditor_fn_violation": 0.016040733065214217,
            "auditor_fp_violation": 0.02601637018881355,
            "ave_precision_score": 0.7559993111993145,
            "fpr": 0.21624588364434688,
            "logloss": 1.1390900177747212,
            "mae": 0.3102082112312802,
            "precision": 0.6683501683501684,
            "recall": 0.8393234672304439
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.849696983996142,
            "auditor_fn_violation": 0.0012743006164058887,
            "auditor_fp_violation": 0.0010023608906256371,
            "ave_precision_score": 0.7956910881309512,
            "fpr": 0.09429824561403509,
            "logloss": 6.140985503787476,
            "mae": 0.23717834173660923,
            "precision": 0.8032036613272311,
            "recall": 0.7297297297297297
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8619620451115657,
            "auditor_fn_violation": 0.006096499676261243,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.8102951738230457,
            "fpr": 0.10098792535675083,
            "logloss": 5.802246471467496,
            "mae": 0.2273293186092384,
            "precision": 0.7951002227171492,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8486392525100092,
            "auditor_fn_violation": 0.00220893970893971,
            "auditor_fp_violation": 0.004401229291325761,
            "ave_precision_score": 0.7898976319482754,
            "fpr": 0.09649122807017543,
            "logloss": 6.521889709605597,
            "mae": 0.2382833810290383,
            "precision": 0.7990867579908676,
            "recall": 0.7276507276507277
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8564037308093608,
            "auditor_fn_violation": 0.0026711348029602963,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.7971159277602005,
            "fpr": 0.10098792535675083,
            "logloss": 6.255263630467582,
            "mae": 0.2284188472377919,
            "precision": 0.7955555555555556,
            "recall": 0.7568710359408034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7927890009890977,
            "auditor_fn_violation": 0.0074110041215304494,
            "auditor_fp_violation": 0.019627447388773568,
            "ave_precision_score": 0.7936478881431795,
            "fpr": 0.13048245614035087,
            "logloss": 0.9429837992139677,
            "mae": 0.29996955427245814,
            "precision": 0.7424242424242424,
            "recall": 0.7130977130977131
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7913772984507778,
            "auditor_fn_violation": 0.015541780864835014,
            "auditor_fp_violation": 0.0129367597451744,
            "ave_precision_score": 0.7919237429300587,
            "fpr": 0.14709110867178923,
            "logloss": 0.8761641476655584,
            "mae": 0.2866662006038714,
            "precision": 0.728744939271255,
            "recall": 0.7610993657505285
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8496961147303036,
            "auditor_fn_violation": 0.002311522048364155,
            "auditor_fp_violation": 0.0027704847966784733,
            "ave_precision_score": 0.7963156959031629,
            "fpr": 0.08662280701754387,
            "logloss": 6.3384914874638385,
            "mae": 0.2376810087002127,
            "precision": 0.8123515439429929,
            "recall": 0.7110187110187111
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8611152854057587,
            "auditor_fn_violation": 0.0050846710280503995,
            "auditor_fp_violation": 0.01915703050990181,
            "ave_precision_score": 0.8109505796345189,
            "fpr": 0.0867178924259056,
            "logloss": 5.886302680446128,
            "mae": 0.21890210016412934,
            "precision": 0.8175519630484989,
            "recall": 0.7484143763213531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8478166776301237,
            "auditor_fn_violation": 0.0033966152387205025,
            "auditor_fp_violation": 0.0002849350755077945,
            "ave_precision_score": 0.7937148549815876,
            "fpr": 0.08771929824561403,
            "logloss": 6.399458961504575,
            "mae": 0.23911294594083868,
            "precision": 0.8113207547169812,
            "recall": 0.7151767151767152
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8591963407900134,
            "auditor_fn_violation": 0.0038245266336043144,
            "auditor_fp_violation": 0.01839014781288062,
            "ave_precision_score": 0.8074066105446438,
            "fpr": 0.09220636663007684,
            "logloss": 6.0232381903015515,
            "mae": 0.22469181268199814,
            "precision": 0.8077803203661327,
            "recall": 0.7463002114164905
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8496344980988291,
            "auditor_fn_violation": 0.005669383958857651,
            "auditor_fp_violation": 0.002770484796678472,
            "ave_precision_score": 0.7956770804764085,
            "fpr": 0.09539473684210527,
            "logloss": 6.080106774164286,
            "mae": 0.23747265919008562,
            "precision": 0.8018223234624146,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8620184906867429,
            "auditor_fn_violation": 0.006096499676261243,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.8104453152345434,
            "fpr": 0.10098792535675083,
            "logloss": 5.751851028588699,
            "mae": 0.22753627666454054,
            "precision": 0.7951002227171492,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 10132,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.8286734650612549,
            "auditor_fn_violation": 0.009904894773315829,
            "auditor_fp_violation": 0.024674868726340227,
            "ave_precision_score": 0.8288553515624462,
            "fpr": 0.25,
            "logloss": 1.2113263667692677,
            "mae": 0.3025102615761859,
            "precision": 0.6591928251121076,
            "recall": 0.9168399168399168
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.8482143987214701,
            "auditor_fn_violation": 0.007980914498158518,
            "auditor_fp_violation": 0.027292001864577534,
            "ave_precision_score": 0.848255661690991,
            "fpr": 0.27661909989023054,
            "logloss": 1.2242686671641458,
            "mae": 0.31013452991863866,
            "precision": 0.6389684813753582,
            "recall": 0.9429175475687104
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8258662663061883,
            "auditor_fn_violation": 0.011847120399751986,
            "auditor_fp_violation": 0.018062848536654866,
            "ave_precision_score": 0.8266063358771072,
            "fpr": 0.11732456140350878,
            "logloss": 0.8247337024972355,
            "mae": 0.28380075561216955,
            "precision": 0.7648351648351648,
            "recall": 0.7234927234927235
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8425239513998918,
            "auditor_fn_violation": 0.009786425251158614,
            "auditor_fp_violation": 0.012270123152338993,
            "ave_precision_score": 0.8427840103189468,
            "fpr": 0.11855104281009879,
            "logloss": 0.7309609233153209,
            "mae": 0.26447684785254083,
            "precision": 0.7711864406779662,
            "recall": 0.7695560253699789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8496344349098748,
            "auditor_fn_violation": 0.0012743006164058887,
            "auditor_fp_violation": 0.0014501160092807478,
            "ave_precision_score": 0.79564985681602,
            "fpr": 0.09649122807017543,
            "logloss": 6.117111286155715,
            "mae": 0.23760182771828897,
            "precision": 0.7995444191343963,
            "recall": 0.7297297297297297
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8619112464877039,
            "auditor_fn_violation": 0.006096499676261243,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.8102677423849515,
            "fpr": 0.10098792535675083,
            "logloss": 5.787348215451138,
            "mae": 0.227695900769431,
            "precision": 0.7951002227171492,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8484911487960904,
            "auditor_fn_violation": 0.00220893970893971,
            "auditor_fp_violation": 0.0040883095209020255,
            "ave_precision_score": 0.7896992996702916,
            "fpr": 0.09429824561403509,
            "logloss": 6.562563361053515,
            "mae": 0.23833059794490863,
            "precision": 0.8027522935779816,
            "recall": 0.7276507276507277
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8558218855824142,
            "auditor_fn_violation": 0.0005035936161966851,
            "auditor_fp_violation": 0.017663363557533743,
            "ave_precision_score": 0.7964318377442104,
            "fpr": 0.10098792535675083,
            "logloss": 6.299120298079528,
            "mae": 0.22888713082802106,
            "precision": 0.7951002227171492,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8493654790728935,
            "auditor_fn_violation": 0.0012743006164058887,
            "auditor_fp_violation": 0.004401229291325761,
            "ave_precision_score": 0.7936904010759346,
            "fpr": 0.09649122807017543,
            "logloss": 6.25862651077794,
            "mae": 0.2374442392976031,
            "precision": 0.7995444191343963,
            "recall": 0.7297297297297297
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8609983098699437,
            "auditor_fn_violation": 0.0026711348029602963,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.808362432493776,
            "fpr": 0.10098792535675083,
            "logloss": 5.901649360067083,
            "mae": 0.22789317618949637,
            "precision": 0.7955555555555556,
            "recall": 0.7568710359408034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 10132,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8451434445908621,
            "auditor_fn_violation": 0.00497182405077142,
            "auditor_fp_violation": 0.004808279399194041,
            "ave_precision_score": 0.8037773024601924,
            "fpr": 0.09320175438596491,
            "logloss": 5.012829531059406,
            "mae": 0.24525013526639527,
            "precision": 0.8018648018648019,
            "recall": 0.7151767151767152
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8550419459013539,
            "auditor_fn_violation": 0.002035260835965404,
            "auditor_fp_violation": 0.01449558666526322,
            "ave_precision_score": 0.8166110440098895,
            "fpr": 0.09769484083424808,
            "logloss": 4.77669691228184,
            "mae": 0.2294816422235997,
            "precision": 0.7995495495495496,
            "recall": 0.7505285412262156
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8499364758275003,
            "auditor_fn_violation": 0.0012743006164058887,
            "auditor_fp_violation": 0.0025109903529124465,
            "ave_precision_score": 0.7950646022825727,
            "fpr": 0.09539473684210527,
            "logloss": 6.219149395536744,
            "mae": 0.2382647840236142,
            "precision": 0.8013698630136986,
            "recall": 0.7297297297297297
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8599691399399568,
            "auditor_fn_violation": 0.0026711348029602963,
            "auditor_fp_violation": 0.018051817211253627,
            "ave_precision_score": 0.8065839006923511,
            "fpr": 0.10208562019758508,
            "logloss": 5.910627278656746,
            "mae": 0.22880634033700675,
            "precision": 0.7937915742793792,
            "recall": 0.7568710359408034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8524807045990757,
            "auditor_fn_violation": 0.014274902432797177,
            "auditor_fp_violation": 0.012000345992591694,
            "ave_precision_score": 0.8520374195375812,
            "fpr": 0.10197368421052631,
            "logloss": 1.0140531305983764,
            "mae": 0.24847150186089495,
            "precision": 0.7933333333333333,
            "recall": 0.7422037422037422
        },
        "train": {
            "accuracy": 0.7903402854006586,
            "auc_prc": 0.8806650359197808,
            "auditor_fn_violation": 0.010603314435035262,
            "auditor_fp_violation": 0.00701973344560897,
            "ave_precision_score": 0.8808401809458306,
            "fpr": 0.09989023051591657,
            "logloss": 0.7837611897804015,
            "mae": 0.2256294758000686,
            "precision": 0.8038793103448276,
            "recall": 0.7885835095137421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8492039679153429,
            "auditor_fn_violation": 0.004659517817412553,
            "auditor_fp_violation": 0.0031164773883665077,
            "ave_precision_score": 0.7968327165079472,
            "fpr": 0.08552631578947369,
            "logloss": 6.247807300966788,
            "mae": 0.23616448654570762,
            "precision": 0.8151658767772512,
            "recall": 0.7151767151767152
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8617910849142777,
            "auditor_fn_violation": 0.0050846710280503995,
            "auditor_fp_violation": 0.01810444641595116,
            "ave_precision_score": 0.8133419276955878,
            "fpr": 0.0889132821075741,
            "logloss": 5.793165571505448,
            "mae": 0.22167319787854808,
            "precision": 0.8137931034482758,
            "recall": 0.7484143763213531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8449767578601637,
            "auditor_fn_violation": 0.0012059123901229237,
            "auditor_fp_violation": 6.868970570277214e-05,
            "ave_precision_score": 0.7854193828505732,
            "fpr": 0.10635964912280702,
            "logloss": 6.742467241736436,
            "mae": 0.2500274699324196,
            "precision": 0.7825112107623319,
            "recall": 0.7255717255717256
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8497791469582542,
            "auditor_fn_violation": 0.0021396926918587256,
            "auditor_fp_violation": 0.014330180593356688,
            "ave_precision_score": 0.785949610252294,
            "fpr": 0.10867178924259056,
            "logloss": 6.598035384861923,
            "mae": 0.2367866825828991,
            "precision": 0.7828947368421053,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7933415406719511,
            "auditor_fn_violation": 0.025973848342269398,
            "auditor_fp_violation": 0.03487401799161477,
            "ave_precision_score": 0.7942339388407553,
            "fpr": 0.15789473684210525,
            "logloss": 0.6458003179143353,
            "mae": 0.33861818681694184,
            "precision": 0.7225433526011561,
            "recall": 0.7796257796257796
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8272319414226544,
            "auditor_fn_violation": 0.030380387233321652,
            "auditor_fp_violation": 0.029517465377501773,
            "ave_precision_score": 0.8275419957799216,
            "fpr": 0.16465422612513722,
            "logloss": 0.5532140626122259,
            "mae": 0.3339230482479928,
            "precision": 0.7180451127819549,
            "recall": 0.8076109936575053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8497191917860039,
            "auditor_fn_violation": 0.005669383958857651,
            "auditor_fp_violation": 0.0014501160092807478,
            "ave_precision_score": 0.7957455905514641,
            "fpr": 0.09649122807017543,
            "logloss": 6.076069301505322,
            "mae": 0.2377979376969745,
            "precision": 0.8,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8620423944532071,
            "auditor_fn_violation": 0.0026711348029602963,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.8104720864299854,
            "fpr": 0.10098792535675083,
            "logloss": 5.751359573016175,
            "mae": 0.22786630160287752,
            "precision": 0.7955555555555556,
            "recall": 0.7568710359408034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.8083861702852275,
            "auditor_fn_violation": 0.011981617244775153,
            "auditor_fp_violation": 0.02380479912077177,
            "ave_precision_score": 0.8087365395252208,
            "fpr": 0.13267543859649122,
            "logloss": 0.9578046940376751,
            "mae": 0.29870192003520724,
            "precision": 0.7369565217391304,
            "recall": 0.7047817047817048
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8172818647435973,
            "auditor_fn_violation": 0.01045478912887588,
            "auditor_fp_violation": 0.01784881885027744,
            "ave_precision_score": 0.8177599858822204,
            "fpr": 0.13830954994511527,
            "logloss": 0.8273166882811486,
            "mae": 0.27391822609313554,
            "precision": 0.744421906693712,
            "recall": 0.7758985200845666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8506661209109032,
            "auditor_fn_violation": 0.0003806944596418335,
            "auditor_fp_violation": 0.0015188057149835153,
            "ave_precision_score": 0.7962874272223891,
            "fpr": 0.08881578947368421,
            "logloss": 6.327438549845911,
            "mae": 0.2352289407983112,
            "precision": 0.8107476635514018,
            "recall": 0.7214137214137214
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8619656809226512,
            "auditor_fn_violation": 0.002657210555507863,
            "auditor_fp_violation": 0.015432887739400228,
            "ave_precision_score": 0.810014108662265,
            "fpr": 0.09001097694840834,
            "logloss": 5.888379490078685,
            "mae": 0.21881319438308383,
            "precision": 0.8123569794050344,
            "recall": 0.7505285412262156
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 10132,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.8353388815658287,
            "auditor_fn_violation": 0.006410256410256411,
            "auditor_fp_violation": 0.02117169373549885,
            "ave_precision_score": 0.8354091866351063,
            "fpr": 0.27521929824561403,
            "logloss": 1.2527071111257322,
            "mae": 0.31254693211513274,
            "precision": 0.6429587482219061,
            "recall": 0.9397089397089398
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.8543547000823373,
            "auditor_fn_violation": 0.005671810128961738,
            "auditor_fp_violation": 0.02719175576039177,
            "ave_precision_score": 0.8546693170725624,
            "fpr": 0.300768386388584,
            "logloss": 1.2875706534083384,
            "mae": 0.32492486226779005,
            "precision": 0.6236263736263736,
            "recall": 0.959830866807611
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8487953519874414,
            "auditor_fn_violation": 0.0022613706824233144,
            "auditor_fp_violation": 0.002589856311311925,
            "ave_precision_score": 0.7948785690337649,
            "fpr": 0.09539473684210527,
            "logloss": 6.1482611780003875,
            "mae": 0.23907952735335353,
            "precision": 0.8004587155963303,
            "recall": 0.7255717255717256
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8613712992536908,
            "auditor_fn_violation": 0.006096499676261243,
            "auditor_fp_violation": 0.01948784265371487,
            "ave_precision_score": 0.8097105489383023,
            "fpr": 0.09879253567508232,
            "logloss": 5.809358456643118,
            "mae": 0.2268143532644316,
            "precision": 0.7986577181208053,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8537706625778244,
            "auditor_fn_violation": 0.0006200532516321986,
            "auditor_fp_violation": 0.007311637562583954,
            "ave_precision_score": 0.8064054144055451,
            "fpr": 0.09320175438596491,
            "logloss": 5.203817403247012,
            "mae": 0.2364994504132908,
            "precision": 0.8054919908466819,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7837541163556532,
            "auc_prc": 0.8634963356414452,
            "auditor_fn_violation": 0.0004200481314820294,
            "auditor_fp_violation": 0.01918710434115755,
            "ave_precision_score": 0.8171730390224039,
            "fpr": 0.09440175631174534,
            "logloss": 4.972595918103782,
            "mae": 0.21938539350827851,
            "precision": 0.8080357142857143,
            "recall": 0.7653276955602537
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8484939672819282,
            "auditor_fn_violation": 0.00220893970893971,
            "auditor_fp_violation": 0.0040883095209020255,
            "ave_precision_score": 0.7897030555580034,
            "fpr": 0.09429824561403509,
            "logloss": 6.5565367834785855,
            "mae": 0.23834467815443494,
            "precision": 0.8027522935779816,
            "recall": 0.7276507276507277
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8558113762618358,
            "auditor_fn_violation": 0.0005035936161966851,
            "auditor_fp_violation": 0.017663363557533743,
            "ave_precision_score": 0.796444260690904,
            "fpr": 0.10098792535675083,
            "logloss": 6.293403108972776,
            "mae": 0.2289012163416952,
            "precision": 0.7951002227171492,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8583974651021604,
            "auditor_fn_violation": 0.0007271948061421822,
            "auditor_fp_violation": 0.004823543778239104,
            "ave_precision_score": 0.8222961949245869,
            "fpr": 0.08881578947368421,
            "logloss": 4.428451021200474,
            "mae": 0.23408687973297984,
            "precision": 0.8116279069767441,
            "recall": 0.7255717255717256
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8696438390758222,
            "auditor_fn_violation": 0.005098595275502843,
            "auditor_fp_violation": 0.01770847430441735,
            "ave_precision_score": 0.8384672653731117,
            "fpr": 0.09110867178924259,
            "logloss": 4.117707838344667,
            "mae": 0.2177533890086638,
            "precision": 0.8117913832199547,
            "recall": 0.7568710359408034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.849520845303141,
            "auditor_fn_violation": 0.006223328591749653,
            "auditor_fp_violation": 0.00558421866731795,
            "ave_precision_score": 0.7976201202108448,
            "fpr": 0.09100877192982457,
            "logloss": 5.817471029498532,
            "mae": 0.2343162378180679,
            "precision": 0.8087557603686636,
            "recall": 0.7297297297297297
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8633511994500149,
            "auditor_fn_violation": 0.0007472679466144388,
            "auditor_fp_violation": 0.016969159286047246,
            "ave_precision_score": 0.8160433753821491,
            "fpr": 0.09549945115257959,
            "logloss": 5.427124398438523,
            "mae": 0.22221386007278665,
            "precision": 0.8036117381489842,
            "recall": 0.7526427061310782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8481959502511154,
            "auditor_fn_violation": 0.0029338549075391155,
            "auditor_fp_violation": 0.005500264582570116,
            "ave_precision_score": 0.7959880214306727,
            "fpr": 0.08442982456140351,
            "logloss": 6.201133357763852,
            "mae": 0.2357617799239424,
            "precision": 0.8166666666666667,
            "recall": 0.7130977130977131
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8618932247145231,
            "auditor_fn_violation": 0.003153842047978313,
            "auditor_fp_violation": 0.01810444641595116,
            "ave_precision_score": 0.8136563183642,
            "fpr": 0.0889132821075741,
            "logloss": 5.736198676356246,
            "mae": 0.22122222340952633,
            "precision": 0.8133640552995391,
            "recall": 0.7463002114164905
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 10132,
        "test": {
            "accuracy": 0.5625,
            "auc_prc": 0.7954431760439099,
            "auditor_fn_violation": 0.027077178392967867,
            "auditor_fp_violation": 0.004126470468514674,
            "ave_precision_score": 0.7958093928047459,
            "fpr": 0.013157894736842105,
            "logloss": 2.765900280531253,
            "mae": 0.4326504498740164,
            "precision": 0.8867924528301887,
            "recall": 0.19542619542619544
        },
        "train": {
            "accuracy": 0.5729967069154775,
            "auc_prc": 0.7918636866370582,
            "auditor_fn_violation": 0.036504735404487794,
            "auditor_fp_violation": 0.005511029577613041,
            "ave_precision_score": 0.7922437790464658,
            "fpr": 0.01646542261251372,
            "logloss": 2.5105448296452697,
            "mae": 0.42040642490630137,
            "precision": 0.868421052631579,
            "recall": 0.20930232558139536
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8495087155686051,
            "auditor_fn_violation": 0.0018829558303242607,
            "auditor_fp_violation": 0.0014348516302356824,
            "ave_precision_score": 0.7965203061634536,
            "fpr": 0.09429824561403509,
            "logloss": 6.003396131631003,
            "mae": 0.23811311698679616,
            "precision": 0.8027522935779816,
            "recall": 0.7276507276507277
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8617041414233668,
            "auditor_fn_violation": 0.006096499676261243,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.8110389445391983,
            "fpr": 0.10098792535675083,
            "logloss": 5.676153569281881,
            "mae": 0.22682830210306687,
            "precision": 0.7951002227171492,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8599690924699703,
            "auditor_fn_violation": 0.002466535361272212,
            "auditor_fp_violation": 0.003805918508568408,
            "ave_precision_score": 0.8368277068527576,
            "fpr": 0.08552631578947369,
            "logloss": 3.207731766383965,
            "mae": 0.23576875960253196,
            "precision": 0.8151658767772512,
            "recall": 0.7151767151767152
        },
        "train": {
            "accuracy": 0.7903402854006586,
            "auc_prc": 0.877766523987592,
            "auditor_fn_violation": 0.0055116812832586475,
            "auditor_fp_violation": 0.016174708910374973,
            "ave_precision_score": 0.8611427103971475,
            "fpr": 0.08342480790340286,
            "logloss": 2.7990865596787367,
            "mae": 0.21173509444682592,
            "precision": 0.8248847926267281,
            "recall": 0.7568710359408034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8455296662014213,
            "auditor_fn_violation": 0.0027583251267461768,
            "auditor_fp_violation": 0.0022896568567590692,
            "ave_precision_score": 0.7877290741871444,
            "fpr": 0.11074561403508772,
            "logloss": 6.32965757653846,
            "mae": 0.2520099568129797,
            "precision": 0.7770419426048565,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8500961526462655,
            "auditor_fn_violation": 0.0021396926918587256,
            "auditor_fp_violation": 0.016600754853164517,
            "ave_precision_score": 0.7877807764991355,
            "fpr": 0.1141602634467618,
            "logloss": 6.2541805672046795,
            "mae": 0.23978074690718285,
            "precision": 0.7744034707158352,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.788137673566738,
            "auditor_fn_violation": 0.009827388116861808,
            "auditor_fp_violation": 0.020266007245491924,
            "ave_precision_score": 0.7886612384985634,
            "fpr": 0.14583333333333334,
            "logloss": 0.9771009873354652,
            "mae": 0.2966463251247362,
            "precision": 0.7263374485596708,
            "recall": 0.7338877338877339
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.7884034088734759,
            "auditor_fn_violation": 0.016256558900727077,
            "auditor_fp_violation": 0.01463091890591403,
            "ave_precision_score": 0.788172704122832,
            "fpr": 0.15367727771679474,
            "logloss": 0.9759936382634632,
            "mae": 0.2798276631272336,
            "precision": 0.7265625,
            "recall": 0.7864693446088795
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8490274593480591,
            "auditor_fn_violation": 0.003316828974723715,
            "auditor_fp_violation": 0.00075049863638214,
            "ave_precision_score": 0.7905181122368502,
            "fpr": 0.09100877192982457,
            "logloss": 6.702575912193962,
            "mae": 0.23814079345115044,
            "precision": 0.8074245939675174,
            "recall": 0.7234927234927235
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8563295190243245,
            "auditor_fn_violation": 0.005110198815046549,
            "auditor_fp_violation": 0.01913196898385537,
            "ave_precision_score": 0.797229543302544,
            "fpr": 0.09659714599341383,
            "logloss": 6.418794872097714,
            "mae": 0.22760450003001986,
            "precision": 0.801354401805869,
            "recall": 0.7505285412262156
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 10132,
        "test": {
            "accuracy": 0.581140350877193,
            "auc_prc": 0.8486888457402475,
            "auditor_fn_violation": 0.003164095269358427,
            "auditor_fp_violation": 0.016017421744616765,
            "ave_precision_score": 0.7888424661943181,
            "fpr": 0.41118421052631576,
            "logloss": 4.916724263534922,
            "mae": 0.41503316809448537,
            "precision": 0.558303886925795,
            "recall": 0.9854469854469855
        },
        "train": {
            "accuracy": 0.5675082327113062,
            "auc_prc": 0.8563785471253662,
            "auditor_fn_violation": 0.0006033840562725254,
            "auditor_fp_violation": 0.012345307730478337,
            "ave_precision_score": 0.7890707853215904,
            "fpr": 0.4313940724478595,
            "logloss": 5.290807526888728,
            "mae": 0.43152307295724507,
            "precision": 0.545664739884393,
            "recall": 0.9978858350951374
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.849729314745339,
            "auditor_fn_violation": 0.0015318962687383836,
            "auditor_fp_violation": 0.0030681401880571523,
            "ave_precision_score": 0.7997251339727993,
            "fpr": 0.08333333333333333,
            "logloss": 6.178451647818123,
            "mae": 0.23669919206835482,
            "precision": 0.8181818181818182,
            "recall": 0.7110187110187111
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8631879181878871,
            "auditor_fn_violation": 0.00028080565695759593,
            "auditor_fp_violation": 0.01616969660516569,
            "ave_precision_score": 0.8165333425296534,
            "fpr": 0.0867178924259056,
            "logloss": 5.706888340508463,
            "mae": 0.2208290072345689,
            "precision": 0.8167053364269141,
            "recall": 0.7441860465116279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8497191917860039,
            "auditor_fn_violation": 0.005669383958857651,
            "auditor_fp_violation": 0.0014501160092807478,
            "ave_precision_score": 0.7957455905514641,
            "fpr": 0.09649122807017543,
            "logloss": 6.076968784708224,
            "mae": 0.23779487017083406,
            "precision": 0.8,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8620401465657597,
            "auditor_fn_violation": 0.0026711348029602963,
            "auditor_fp_violation": 0.018530492358740706,
            "ave_precision_score": 0.8104720864299853,
            "fpr": 0.10098792535675083,
            "logloss": 5.752376978195151,
            "mae": 0.22786608798311667,
            "precision": 0.7955555555555556,
            "recall": 0.7568710359408034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.850620502818572,
            "auditor_fn_violation": 0.0022499726447094853,
            "auditor_fp_violation": 0.007311637562583954,
            "ave_precision_score": 0.7985285791928827,
            "fpr": 0.09320175438596491,
            "logloss": 5.81377368086686,
            "mae": 0.2360542547722054,
            "precision": 0.8059360730593608,
            "recall": 0.7338877338877339
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.863198347436902,
            "auditor_fn_violation": 0.0018913769456234933,
            "auditor_fp_violation": 0.0188688229603677,
            "ave_precision_score": 0.8133687710112065,
            "fpr": 0.09769484083424808,
            "logloss": 5.497576603625301,
            "mae": 0.2244537587471178,
            "precision": 0.8008948545861297,
            "recall": 0.7568710359408034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8556026777963224,
            "auditor_fn_violation": 2.5075682970428153e-05,
            "auditor_fp_violation": 0.008677799487116866,
            "ave_precision_score": 0.8213993607103087,
            "fpr": 0.08333333333333333,
            "logloss": 4.422911698819365,
            "mae": 0.24070890739306214,
            "precision": 0.8159806295399515,
            "recall": 0.7006237006237006
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.8730325565042569,
            "auditor_fn_violation": 0.0008424169708727959,
            "auditor_fp_violation": 0.016736087093815313,
            "ave_precision_score": 0.8454351636927236,
            "fpr": 0.0845225027442371,
            "logloss": 3.899852507032198,
            "mae": 0.21340196627839372,
            "precision": 0.8225806451612904,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 10132,
        "test": {
            "accuracy": 0.7708333333333334,
            "auc_prc": 0.8701870044252761,
            "auditor_fn_violation": 0.004593409198672363,
            "auditor_fp_violation": 0.002653457890666345,
            "ave_precision_score": 0.8683357894466214,
            "fpr": 0.09210526315789473,
            "logloss": 1.3011611229889317,
            "mae": 0.22582855127523901,
            "precision": 0.8090909090909091,
            "recall": 0.7401247401247402
        },
        "train": {
            "accuracy": 0.8111964873765093,
            "auc_prc": 0.894382559742262,
            "auditor_fn_violation": 0.011468938484995469,
            "auditor_fp_violation": 0.008881804830859761,
            "ave_precision_score": 0.8932190521709802,
            "fpr": 0.08342480790340286,
            "logloss": 1.0362650264764102,
            "mae": 0.2004549077638352,
            "precision": 0.8322295805739515,
            "recall": 0.7970401691331924
        }
    }
]
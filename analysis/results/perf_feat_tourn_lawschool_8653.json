[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7720676153019459,
            "auditor_fn_violation": 0.008982529392374474,
            "auditor_fp_violation": 0.004246687735505055,
            "ave_precision_score": 0.7673258402691268,
            "fpr": 0.15021929824561403,
            "logloss": 1.84307888969622,
            "mae": 0.32460429711515504,
            "precision": 0.702819956616052,
            "recall": 0.6764091858037579
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.8128771270466237,
            "auditor_fn_violation": 0.005162632156681503,
            "auditor_fp_violation": 0.009738265239327693,
            "ave_precision_score": 0.8099436039747256,
            "fpr": 0.16245883644346873,
            "logloss": 1.6180706586058693,
            "mae": 0.31058903083467454,
            "precision": 0.6991869918699187,
            "recall": 0.7242105263157895
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 8653,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7712318039759694,
            "auditor_fn_violation": 0.005468721385928291,
            "auditor_fp_violation": 0.0071056683278635435,
            "ave_precision_score": 0.7655600944539627,
            "fpr": 0.17434210526315788,
            "logloss": 1.9011993646928764,
            "mae": 0.32381023844669715,
            "precision": 0.6857707509881423,
            "recall": 0.7244258872651357
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.811285063985165,
            "auditor_fn_violation": 0.0068889017274250414,
            "auditor_fp_violation": 0.006012145137413268,
            "ave_precision_score": 0.8082493881351436,
            "fpr": 0.1800219538968167,
            "logloss": 1.6935702181069974,
            "mae": 0.3124016916850034,
            "precision": 0.688212927756654,
            "recall": 0.7621052631578947
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8194872884529057,
            "auditor_fn_violation": 0.017170732153975757,
            "auditor_fp_violation": 0.006667578299096476,
            "ave_precision_score": 0.819200815304366,
            "fpr": 0.06578947368421052,
            "logloss": 1.435786821803198,
            "mae": 0.2917783357167968,
            "precision": 0.8342541436464088,
            "recall": 0.6304801670146137
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8560655598332749,
            "auditor_fn_violation": 0.023467560228782717,
            "auditor_fp_violation": 0.009234735495825739,
            "ave_precision_score": 0.8562306530492667,
            "fpr": 0.048298572996706916,
            "logloss": 1.5195842770001817,
            "mae": 0.26837528096227037,
            "precision": 0.8739255014326648,
            "recall": 0.6421052631578947
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.6590846663087853,
            "auditor_fn_violation": 0.014485587664359232,
            "auditor_fp_violation": 0.016092844698350966,
            "ave_precision_score": 0.6348236252878163,
            "fpr": 0.13048245614035087,
            "logloss": 4.065709836387237,
            "mae": 0.3466031612473053,
            "precision": 0.711864406779661,
            "recall": 0.6137787056367432
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.6655678549866441,
            "auditor_fn_violation": 0.010849846900456418,
            "auditor_fp_violation": 0.01833351796090595,
            "ave_precision_score": 0.636751642056512,
            "fpr": 0.13721185510428102,
            "logloss": 3.9362409479922627,
            "mae": 0.3211560006062605,
            "precision": 0.7191011235955056,
            "recall": 0.6736842105263158
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7959818573417684,
            "auditor_fn_violation": 0.013469215837087512,
            "auditor_fp_violation": 0.0074171427413800076,
            "ave_precision_score": 0.791329459102302,
            "fpr": 0.125,
            "logloss": 1.658455298465782,
            "mae": 0.30232844026818706,
            "precision": 0.7455357142857143,
            "recall": 0.697286012526096
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.8343439332598749,
            "auditor_fn_violation": 0.011032410884510954,
            "auditor_fp_violation": 0.007588193234574366,
            "ave_precision_score": 0.832523347352509,
            "fpr": 0.13721185510428102,
            "logloss": 1.4508645046483055,
            "mae": 0.288364619373431,
            "precision": 0.7334754797441365,
            "recall": 0.7242105263157895
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7813830155195086,
            "auditor_fn_violation": 0.012487180895872254,
            "auditor_fp_violation": 0.00716391151087882,
            "ave_precision_score": 0.7777365638143345,
            "fpr": 0.14364035087719298,
            "logloss": 1.6712376090649619,
            "mae": 0.312062723586571,
            "precision": 0.7158351409978309,
            "recall": 0.6889352818371608
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.8244492051698026,
            "auditor_fn_violation": 0.008765382171124847,
            "auditor_fp_violation": 0.005986968650238175,
            "ave_precision_score": 0.8235917309780618,
            "fpr": 0.15148188803512624,
            "logloss": 1.4418396114681127,
            "mae": 0.29703401931463963,
            "precision": 0.7154639175257732,
            "recall": 0.7305263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.7732899517554523,
            "auditor_fn_violation": 0.009220598469032708,
            "auditor_fp_violation": 0.007176573072403884,
            "ave_precision_score": 0.7686051613259739,
            "fpr": 0.15021929824561403,
            "logloss": 1.8226822918984573,
            "mae": 0.3212151502361263,
            "precision": 0.7041036717062635,
            "recall": 0.6805845511482255
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.8152059749907655,
            "auditor_fn_violation": 0.010279045583222603,
            "auditor_fp_violation": 0.004237202791568896,
            "ave_precision_score": 0.8132297112620656,
            "fpr": 0.15916575192096596,
            "logloss": 1.5863098739187678,
            "mae": 0.30732626614117736,
            "precision": 0.7028688524590164,
            "recall": 0.7221052631578947
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7703605783010451,
            "auditor_fn_violation": 0.010527689264915954,
            "auditor_fp_violation": 0.004768344070337507,
            "ave_precision_score": 0.7655723573423399,
            "fpr": 0.1611842105263158,
            "logloss": 1.848381673513556,
            "mae": 0.32290400655346596,
            "precision": 0.6987704918032787,
            "recall": 0.7118997912317327
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.811837200297348,
            "auditor_fn_violation": 0.007570627996995791,
            "auditor_fp_violation": 0.004420991147947109,
            "ave_precision_score": 0.8098674714883276,
            "fpr": 0.1734357848518112,
            "logloss": 1.626620332210533,
            "mae": 0.3106208474041308,
            "precision": 0.6926070038910506,
            "recall": 0.7494736842105263
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7743912636991191,
            "auditor_fn_violation": 0.01593689338167967,
            "auditor_fp_violation": 0.020519326607511858,
            "ave_precision_score": 0.7744722075247027,
            "fpr": 0.14473684210526316,
            "logloss": 1.0209044446444095,
            "mae": 0.2899070877919229,
            "precision": 0.7333333333333333,
            "recall": 0.7578288100208769
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8243302855807952,
            "auditor_fn_violation": 0.01974001964296031,
            "auditor_fp_violation": 0.018111964873765103,
            "ave_precision_score": 0.8246249388764386,
            "fpr": 0.1394072447859495,
            "logloss": 0.941991988317022,
            "mae": 0.2721768090004143,
            "precision": 0.742914979757085,
            "recall": 0.7726315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.7767193349943778,
            "auditor_fn_violation": 0.009135900816760075,
            "auditor_fp_violation": 0.003383169239495974,
            "ave_precision_score": 0.772009322862974,
            "fpr": 0.14802631578947367,
            "logloss": 1.7819840662110362,
            "mae": 0.31820053661498743,
            "precision": 0.7077922077922078,
            "recall": 0.6826722338204593
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.8168483513797664,
            "auditor_fn_violation": 0.009809925472297653,
            "auditor_fp_violation": 0.005986968650238175,
            "ave_precision_score": 0.8139792693838561,
            "fpr": 0.15148188803512624,
            "logloss": 1.5528578891901381,
            "mae": 0.303723064440352,
            "precision": 0.7118997912317327,
            "recall": 0.7178947368421053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.7786929377623665,
            "auditor_fn_violation": 0.011795864923268505,
            "auditor_fp_violation": 0.009455654146914641,
            "ave_precision_score": 0.7740593497261659,
            "fpr": 0.1524122807017544,
            "logloss": 1.723256788157361,
            "mae": 0.316093012596622,
            "precision": 0.7067510548523207,
            "recall": 0.6993736951983298
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.8211389566288961,
            "auditor_fn_violation": 0.008977988329770642,
            "auditor_fp_violation": 0.0030513902456218178,
            "ave_precision_score": 0.8192017109530143,
            "fpr": 0.15806805708013172,
            "logloss": 1.4945099673040385,
            "mae": 0.30117953861512536,
            "precision": 0.7073170731707317,
            "recall": 0.7326315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 8653,
        "test": {
            "accuracy": 0.5493421052631579,
            "auc_prc": 0.5200373636050024,
            "auditor_fn_violation": 0.00019457568765337218,
            "auditor_fp_violation": 0.004588549896681671,
            "ave_precision_score": 0.44628474709502675,
            "fpr": 0.4243421052631579,
            "logloss": 9.558877447083727,
            "mae": 0.4533715603744097,
            "precision": 0.5403800475059383,
            "recall": 0.9498956158663883
        },
        "train": {
            "accuracy": 0.5894621295279913,
            "auc_prc": 0.5260970220482936,
            "auditor_fn_violation": 0.0015737477612802597,
            "auditor_fp_violation": 0.007278522442320678,
            "ave_precision_score": 0.45423249014541806,
            "fpr": 0.39846322722283206,
            "logloss": 9.47211536053079,
            "mae": 0.42062402615357286,
            "precision": 0.5610640870616687,
            "recall": 0.9768421052631578
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7735433091888291,
            "auditor_fn_violation": 0.015517983371790653,
            "auditor_fp_violation": 0.020995401320854097,
            "ave_precision_score": 0.7751016255103693,
            "fpr": 0.1425438596491228,
            "logloss": 1.0004504309832511,
            "mae": 0.2905859760081005,
            "precision": 0.7341513292433538,
            "recall": 0.7494780793319415
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8258763788880761,
            "auditor_fn_violation": 0.01926165578600728,
            "auditor_fp_violation": 0.016253940120242904,
            "ave_precision_score": 0.8261499054649313,
            "fpr": 0.1350164654226125,
            "logloss": 0.9336522419321895,
            "mae": 0.2728231928995041,
            "precision": 0.7453416149068323,
            "recall": 0.7578947368421053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7753424894193394,
            "auditor_fn_violation": 0.015680511299124646,
            "auditor_fp_violation": 0.020519326607511858,
            "ave_precision_score": 0.7753211545449724,
            "fpr": 0.14473684210526316,
            "logloss": 1.0271974758975702,
            "mae": 0.29002473917441135,
            "precision": 0.7327935222672065,
            "recall": 0.755741127348643
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8240978078463862,
            "auditor_fn_violation": 0.018475937373620663,
            "auditor_fp_violation": 0.019436248099175223,
            "ave_precision_score": 0.8243697244131626,
            "fpr": 0.14050493962678376,
            "logloss": 0.9475787798216044,
            "mae": 0.27257468892231074,
            "precision": 0.7424547283702213,
            "recall": 0.7768421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8052684908090816,
            "auditor_fn_violation": 0.014197157821484823,
            "auditor_fp_violation": 0.013036343746201532,
            "ave_precision_score": 0.8026149267512211,
            "fpr": 0.10197368421052631,
            "logloss": 1.5063247088984495,
            "mae": 0.29428545221707586,
            "precision": 0.7720588235294118,
            "recall": 0.6576200417536534
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8479302846878698,
            "auditor_fn_violation": 0.010685770408458034,
            "auditor_fp_violation": 0.006193415845073973,
            "ave_precision_score": 0.8480817226600583,
            "fpr": 0.09001097694840834,
            "logloss": 1.3517096841944387,
            "mae": 0.26420394961860255,
            "precision": 0.7990196078431373,
            "recall": 0.6863157894736842
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7345369432133362,
            "auditor_fn_violation": 0.06772836318353295,
            "auditor_fp_violation": 0.06016774036708399,
            "ave_precision_score": 0.7334490896813117,
            "fpr": 0.17324561403508773,
            "logloss": 1.6567491470052778,
            "mae": 0.3276079818629069,
            "precision": 0.6814516129032258,
            "recall": 0.7056367432150313
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.778706265172699,
            "auditor_fn_violation": 0.05931480732566873,
            "auditor_fp_violation": 0.06509128994249691,
            "ave_precision_score": 0.7786787837541893,
            "fpr": 0.17014270032930845,
            "logloss": 1.5594068715526175,
            "mae": 0.30885853821195036,
            "precision": 0.6948818897637795,
            "recall": 0.7431578947368421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7774852588314795,
            "auditor_fn_violation": 0.009946251327692935,
            "auditor_fp_violation": 0.0023145334467809305,
            "ave_precision_score": 0.7729207520761119,
            "fpr": 0.14583333333333334,
            "logloss": 1.7352025135246505,
            "mae": 0.31869192037759053,
            "precision": 0.7114967462039046,
            "recall": 0.6847599164926931
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.8207580149904141,
            "auditor_fn_violation": 0.011314345138367331,
            "auditor_fp_violation": 0.006878216296236627,
            "ave_precision_score": 0.8187735128365206,
            "fpr": 0.14928649835345773,
            "logloss": 1.491947560969946,
            "mae": 0.3024142792098988,
            "precision": 0.7160751565762005,
            "recall": 0.7221052631578947
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.7751224126538483,
            "auditor_fn_violation": 0.017195912537083836,
            "auditor_fp_violation": 0.020985272071634052,
            "ave_precision_score": 0.7751884872094835,
            "fpr": 0.14364035087719298,
            "logloss": 1.0225459991430867,
            "mae": 0.2889726083884356,
            "precision": 0.7348178137651822,
            "recall": 0.7578288100208769
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8242848006019652,
            "auditor_fn_violation": 0.020255358484025654,
            "auditor_fp_violation": 0.01955709523761569,
            "ave_precision_score": 0.8245603562770246,
            "fpr": 0.14050493962678376,
            "logloss": 0.9431008466726145,
            "mae": 0.27178772001952656,
            "precision": 0.7419354838709677,
            "recall": 0.7747368421052632
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 8653,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7592564101251948,
            "auditor_fn_violation": 0.006423286818298356,
            "auditor_fp_violation": 0.008627588023175722,
            "ave_precision_score": 0.7597185324952196,
            "fpr": 0.125,
            "logloss": 1.2414960820872467,
            "mae": 0.32924063131974957,
            "precision": 0.7259615384615384,
            "recall": 0.6304801670146137
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7942481844996175,
            "auditor_fn_violation": 0.012504477439482348,
            "auditor_fp_violation": 0.010755395321201629,
            "ave_precision_score": 0.794548433364074,
            "fpr": 0.13830954994511527,
            "logloss": 1.1358216293631491,
            "mae": 0.31406053389494637,
            "precision": 0.71875,
            "recall": 0.6778947368421052
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7780583236335421,
            "auditor_fn_violation": 0.011312859392740723,
            "auditor_fp_violation": 0.007085409829423446,
            "ave_precision_score": 0.7744818989901716,
            "fpr": 0.13048245614035087,
            "logloss": 1.7560967960324112,
            "mae": 0.3131355495130325,
            "precision": 0.7289293849658315,
            "recall": 0.6680584551148225
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.8186471994621278,
            "auditor_fn_violation": 0.008409497949043852,
            "auditor_fp_violation": 0.005901368593842842,
            "ave_precision_score": 0.8178793841413222,
            "fpr": 0.13830954994511527,
            "logloss": 1.5183811344657514,
            "mae": 0.29794285891173033,
            "precision": 0.7248908296943232,
            "recall": 0.6989473684210527
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.7877703015475899,
            "auditor_fn_violation": 0.018754807164047906,
            "auditor_fp_violation": 0.023715104736436943,
            "ave_precision_score": 0.788508525755335,
            "fpr": 0.14473684210526316,
            "logloss": 0.8270945879545524,
            "mae": 0.2834461655924477,
            "precision": 0.7421875,
            "recall": 0.7933194154488518
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8388257899197155,
            "auditor_fn_violation": 0.017780345485007797,
            "auditor_fp_violation": 0.021389943503962783,
            "ave_precision_score": 0.8390646579619939,
            "fpr": 0.14489571899012074,
            "logloss": 0.7605570578281522,
            "mae": 0.26874104104850677,
            "precision": 0.7441860465116279,
            "recall": 0.8084210526315789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7756529184609761,
            "auditor_fn_violation": 0.01593689338167967,
            "auditor_fp_violation": 0.019422835379441686,
            "ave_precision_score": 0.7757765556443705,
            "fpr": 0.14473684210526316,
            "logloss": 1.033084047504375,
            "mae": 0.2898939943024713,
            "precision": 0.7333333333333333,
            "recall": 0.7578288100208769
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8238220196607823,
            "auditor_fn_violation": 0.020779941071119072,
            "auditor_fp_violation": 0.01904097725052619,
            "ave_precision_score": 0.8241485211833217,
            "fpr": 0.141602634467618,
            "logloss": 0.9478561724196334,
            "mae": 0.2726270467767366,
            "precision": 0.7404426559356136,
            "recall": 0.7747368421052632
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.711834260311486,
            "auditor_fn_violation": 0.00835530894040948,
            "auditor_fp_violation": 0.005305194279000043,
            "ave_precision_score": 0.6566877815405066,
            "fpr": 0.15350877192982457,
            "logloss": 6.248292365628917,
            "mae": 0.3571214324252356,
            "precision": 0.680365296803653,
            "recall": 0.6221294363256785
        },
        "train": {
            "accuracy": 0.6608122941822173,
            "auc_prc": 0.7174181300305357,
            "auditor_fn_violation": 0.007076087584493621,
            "auditor_fp_violation": 0.006903392783411722,
            "ave_precision_score": 0.6559241338107389,
            "fpr": 0.1602634467618002,
            "logloss": 6.338631910880472,
            "mae": 0.3424143341447849,
            "precision": 0.6812227074235808,
            "recall": 0.6568421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7701917446207688,
            "auditor_fn_violation": 0.015950628136102263,
            "auditor_fp_violation": 0.021694319517037403,
            "ave_precision_score": 0.7705785680671543,
            "fpr": 0.14802631578947367,
            "logloss": 1.1070321194086568,
            "mae": 0.2901423867494569,
            "precision": 0.7305389221556886,
            "recall": 0.7640918580375783
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8196367219507037,
            "auditor_fn_violation": 0.019157663643191408,
            "auditor_fp_violation": 0.01866081229418222,
            "ave_precision_score": 0.8199196214561856,
            "fpr": 0.14489571899012074,
            "logloss": 1.0070035681632714,
            "mae": 0.2743062720581389,
            "precision": 0.736,
            "recall": 0.7747368421052632
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7742300693253679,
            "auditor_fn_violation": 0.016339779511408997,
            "auditor_fp_violation": 0.020985272071634052,
            "ave_precision_score": 0.7747150918598462,
            "fpr": 0.14364035087719298,
            "logloss": 1.021976587824067,
            "mae": 0.2901479357951226,
            "precision": 0.7326530612244898,
            "recall": 0.7494780793319415
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8244389416472429,
            "auditor_fn_violation": 0.0183257265006644,
            "auditor_fp_violation": 0.01834358855577599,
            "ave_precision_score": 0.8247352408584976,
            "fpr": 0.13830954994511527,
            "logloss": 0.9430095393375874,
            "mae": 0.2721409365140141,
            "precision": 0.7439024390243902,
            "recall": 0.7705263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7602784530788615,
            "auditor_fn_violation": 0.008634582280335498,
            "auditor_fp_violation": 0.012808435638750457,
            "ave_precision_score": 0.7567078571079684,
            "fpr": 0.11732456140350878,
            "logloss": 1.7646714140728603,
            "mae": 0.31004308991877005,
            "precision": 0.7476415094339622,
            "recall": 0.6617954070981211
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.799037106841284,
            "auditor_fn_violation": 0.008883239932982852,
            "auditor_fp_violation": 0.009879253567508232,
            "ave_precision_score": 0.798228043511591,
            "fpr": 0.11964873765093303,
            "logloss": 1.6018192743758637,
            "mae": 0.2876534344603282,
            "precision": 0.7522727272727273,
            "recall": 0.6968421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.7770733311410347,
            "auditor_fn_violation": 0.012445976632604475,
            "auditor_fp_violation": 0.021739901138527626,
            "ave_precision_score": 0.7774015936336073,
            "fpr": 0.17653508771929824,
            "logloss": 1.0874863635126994,
            "mae": 0.286260188556603,
            "precision": 0.7140319715808171,
            "recall": 0.8392484342379958
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8269165338153696,
            "auditor_fn_violation": 0.014836212375064994,
            "auditor_fp_violation": 0.022160344011520765,
            "ave_precision_score": 0.8271492895566689,
            "fpr": 0.1734357848518112,
            "logloss": 1.0236185025810967,
            "mae": 0.27575620308925936,
            "precision": 0.7132486388384754,
            "recall": 0.8273684210526315
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7665663963773417,
            "auditor_fn_violation": 0.011109127202138962,
            "auditor_fp_violation": 0.006616932052996238,
            "ave_precision_score": 0.7617220447895983,
            "fpr": 0.15460526315789475,
            "logloss": 1.864176249741403,
            "mae": 0.32712493773694556,
            "precision": 0.6980728051391863,
            "recall": 0.6805845511482255
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.8081749657973336,
            "auditor_fn_violation": 0.013188514645560109,
            "auditor_fp_violation": 0.01099708959808256,
            "ave_precision_score": 0.8060947435616628,
            "fpr": 0.17233809001097694,
            "logloss": 1.6272511004650219,
            "mae": 0.3143185174893779,
            "precision": 0.6866267465069861,
            "recall": 0.7242105263157895
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.7626294667050235,
            "auditor_fn_violation": 0.007265685089550601,
            "auditor_fp_violation": 0.013079393055386749,
            "ave_precision_score": 0.7279559129160856,
            "fpr": 0.17653508771929824,
            "logloss": 3.030390399586997,
            "mae": 0.3246898968593085,
            "precision": 0.6891891891891891,
            "recall": 0.7453027139874739
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7953624597397599,
            "auditor_fn_violation": 0.011247327979663765,
            "auditor_fp_violation": 0.010770501213506685,
            "ave_precision_score": 0.76519118410665,
            "fpr": 0.19099890230515917,
            "logloss": 2.764095509154644,
            "mae": 0.31290311028015994,
            "precision": 0.6813186813186813,
            "recall": 0.783157894736842
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7849311509050467,
            "auditor_fn_violation": 0.011940079844705721,
            "auditor_fp_violation": 0.007151249949353758,
            "ave_precision_score": 0.780412226392374,
            "fpr": 0.1513157894736842,
            "logloss": 1.5878769492921085,
            "mae": 0.3097625244891148,
            "precision": 0.7118997912317327,
            "recall": 0.7118997912317327
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.8281551970080228,
            "auditor_fn_violation": 0.00899416488531978,
            "auditor_fp_violation": 0.00695122810904441,
            "ave_precision_score": 0.827148793725782,
            "fpr": 0.15697036223929747,
            "logloss": 1.3476402571297124,
            "mae": 0.2950866565388266,
            "precision": 0.7122736418511066,
            "recall": 0.7452631578947368
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7816119011700876,
            "auditor_fn_violation": 0.011198403105885811,
            "auditor_fp_violation": 0.0036009480977269998,
            "ave_precision_score": 0.7778652112749631,
            "fpr": 0.12280701754385964,
            "logloss": 1.732861959845688,
            "mae": 0.31576808561230135,
            "precision": 0.7389277389277389,
            "recall": 0.6617954070981211
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.8226002250372794,
            "auditor_fn_violation": 0.007602981108094061,
            "auditor_fp_violation": 0.0068253456731689165,
            "ave_precision_score": 0.8217783908858844,
            "fpr": 0.1350164654226125,
            "logloss": 1.461865947476213,
            "mae": 0.30032049098172575,
            "precision": 0.7284768211920529,
            "recall": 0.6947368421052632
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7727391934456534,
            "auditor_fn_violation": 0.01238417023770282,
            "auditor_fp_violation": 0.017857866374944298,
            "ave_precision_score": 0.7364190777587325,
            "fpr": 0.12828947368421054,
            "logloss": 2.7075805233712904,
            "mae": 0.3051600801372249,
            "precision": 0.7388392857142857,
            "recall": 0.6910229645093946
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.803095857633354,
            "auditor_fn_violation": 0.012132416661852217,
            "auditor_fp_violation": 0.015191492361453795,
            "ave_precision_score": 0.7683352032051503,
            "fpr": 0.11086717892425905,
            "logloss": 2.475532355954816,
            "mae": 0.2777232085157163,
            "precision": 0.7709750566893424,
            "recall": 0.7157894736842105
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.7840677123032402,
            "auditor_fn_violation": 0.015666776544702048,
            "auditor_fp_violation": 0.023808800291722384,
            "ave_precision_score": 0.7855075193422176,
            "fpr": 0.13925438596491227,
            "logloss": 0.9392298745007703,
            "mae": 0.28446899233879835,
            "precision": 0.7392197125256673,
            "recall": 0.7515657620041754
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.835307377618643,
            "auditor_fn_violation": 0.01749378935813739,
            "auditor_fp_violation": 0.018962930140283392,
            "ave_precision_score": 0.8355476945972229,
            "fpr": 0.132821075740944,
            "logloss": 0.8835334446213978,
            "mae": 0.267730920026161,
            "precision": 0.7505154639175258,
            "recall": 0.7663157894736842
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7893638709081352,
            "auditor_fn_violation": 0.03218052961213054,
            "auditor_fp_violation": 0.03555366476236782,
            "ave_precision_score": 0.7909633157597589,
            "fpr": 0.18640350877192982,
            "logloss": 0.6003520378050196,
            "mae": 0.34707574108056416,
            "precision": 0.6936936936936937,
            "recall": 0.8037578288100209
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.82611257686343,
            "auditor_fn_violation": 0.027571783465249296,
            "auditor_fp_violation": 0.03870129608555978,
            "ave_precision_score": 0.8264854754858604,
            "fpr": 0.1756311745334797,
            "logloss": 0.5595812710716843,
            "mae": 0.33471420480754444,
            "precision": 0.7085610200364298,
            "recall": 0.8189473684210526
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7744406267274593,
            "auditor_fn_violation": 0.01593689338167967,
            "auditor_fp_violation": 0.020519326607511858,
            "ave_precision_score": 0.7745496470432853,
            "fpr": 0.14473684210526316,
            "logloss": 1.0211577863844328,
            "mae": 0.28986953366557183,
            "precision": 0.7333333333333333,
            "recall": 0.7578288100208769
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8242813665819951,
            "auditor_fn_violation": 0.01974001964296031,
            "auditor_fp_violation": 0.018111964873765103,
            "ave_precision_score": 0.8245764207369758,
            "fpr": 0.1394072447859495,
            "logloss": 0.9421767227809319,
            "mae": 0.27215395104278367,
            "precision": 0.742914979757085,
            "recall": 0.7726315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.7594143881557025,
            "auditor_fn_violation": 0.012773321613009567,
            "auditor_fp_violation": 0.004256816984725104,
            "ave_precision_score": 0.755647699822227,
            "fpr": 0.1524122807017544,
            "logloss": 1.7303381801762048,
            "mae": 0.34222213865860807,
            "precision": 0.6971677559912854,
            "recall": 0.6680584551148225
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7998409372877648,
            "auditor_fn_violation": 0.008462649488705301,
            "auditor_fp_violation": 0.007563016747399273,
            "ave_precision_score": 0.7990334376924777,
            "fpr": 0.16575192096597147,
            "logloss": 1.488528150041082,
            "mae": 0.327340221531347,
            "precision": 0.688659793814433,
            "recall": 0.7031578947368421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7735739209362396,
            "auditor_fn_violation": 0.012185016298575253,
            "auditor_fp_violation": 0.005543231635671173,
            "ave_precision_score": 0.7679738752182648,
            "fpr": 0.1524122807017544,
            "logloss": 1.8053699164072332,
            "mae": 0.3272412371207658,
            "precision": 0.707983193277311,
            "recall": 0.7035490605427975
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.8166517378329371,
            "auditor_fn_violation": 0.011117915535270669,
            "auditor_fp_violation": 0.004043343840320658,
            "ave_precision_score": 0.8157380896801995,
            "fpr": 0.15697036223929747,
            "logloss": 1.5185194233310126,
            "mae": 0.31000736933011513,
            "precision": 0.7122736418511066,
            "recall": 0.7452631578947368
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8120632199065034,
            "auditor_fn_violation": 0.004791140167747135,
            "auditor_fp_violation": 0.019045520845994906,
            "ave_precision_score": 0.8115218302366664,
            "fpr": 0.15021929824561403,
            "logloss": 1.0770193853186705,
            "mae": 0.28621543502358593,
            "precision": 0.7254509018036072,
            "recall": 0.755741127348643
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8582870477288616,
            "auditor_fn_violation": 0.015252180946328504,
            "auditor_fp_violation": 0.019829001299106738,
            "ave_precision_score": 0.8584528800891134,
            "fpr": 0.14709110867178923,
            "logloss": 0.9467758245146329,
            "mae": 0.2704334534925827,
            "precision": 0.7314629258517034,
            "recall": 0.7684210526315789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7735591536214517,
            "auditor_fn_violation": 0.01264741969746914,
            "auditor_fp_violation": 0.005961063165998138,
            "ave_precision_score": 0.7690218566451469,
            "fpr": 0.14144736842105263,
            "logloss": 1.7906257889070114,
            "mae": 0.3183389842512903,
            "precision": 0.7164835164835165,
            "recall": 0.6805845511482255
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.8159144310886914,
            "auditor_fn_violation": 0.0048575885377549295,
            "auditor_fp_violation": 0.0031118138148420425,
            "ave_precision_score": 0.8150437523037487,
            "fpr": 0.15367727771679474,
            "logloss": 1.535737581552115,
            "mae": 0.3051018651438384,
            "precision": 0.7058823529411765,
            "recall": 0.7073684210526315
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7718534137086117,
            "auditor_fn_violation": 0.016740376515401244,
            "auditor_fp_violation": 0.02068899153194766,
            "ave_precision_score": 0.771924596404039,
            "fpr": 0.15350877192982457,
            "logloss": 1.0614729157390972,
            "mae": 0.28933355006166933,
            "precision": 0.7254901960784313,
            "recall": 0.7724425887265136
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8197083550682714,
            "auditor_fn_violation": 0.018281818707031026,
            "auditor_fp_violation": 0.01821770611990051,
            "ave_precision_score": 0.8199396414533267,
            "fpr": 0.14818880351262348,
            "logloss": 0.9770686669315726,
            "mae": 0.2732219455576595,
            "precision": 0.733201581027668,
            "recall": 0.7810526315789473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.7876035308603343,
            "auditor_fn_violation": 0.011933212467494415,
            "auditor_fp_violation": 0.02218305579190471,
            "ave_precision_score": 0.7874273170025613,
            "fpr": 0.14364035087719298,
            "logloss": 1.0550497297990353,
            "mae": 0.2741220119744403,
            "precision": 0.7431372549019608,
            "recall": 0.791231732776618
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.833739802241289,
            "auditor_fn_violation": 0.021145069039228147,
            "auditor_fp_violation": 0.0195344363991581,
            "ave_precision_score": 0.8338973616853047,
            "fpr": 0.141602634467618,
            "logloss": 0.9529566617217189,
            "mae": 0.26305249173303447,
            "precision": 0.7455621301775148,
            "recall": 0.7957894736842105
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7831857662553172,
            "auditor_fn_violation": 0.009202285463135921,
            "auditor_fp_violation": 0.01545470199748795,
            "ave_precision_score": 0.7832044635944044,
            "fpr": 0.13048245614035087,
            "logloss": 1.0367260901053303,
            "mae": 0.2938431052224346,
            "precision": 0.7367256637168141,
            "recall": 0.6951983298538622
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8296351103443458,
            "auditor_fn_violation": 0.01805765786585014,
            "auditor_fp_violation": 0.01500770400507558,
            "ave_precision_score": 0.8299146215812787,
            "fpr": 0.11306256860592755,
            "logloss": 0.9426795015795534,
            "mae": 0.26767265329934303,
            "precision": 0.7669683257918553,
            "recall": 0.7136842105263158
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7582320252707453,
            "auditor_fn_violation": 0.013762223931436111,
            "auditor_fp_violation": 0.014145496535796763,
            "ave_precision_score": 0.7392695406798837,
            "fpr": 0.20942982456140352,
            "logloss": 2.104356290545508,
            "mae": 0.3243468576531455,
            "precision": 0.6773648648648649,
            "recall": 0.837160751565762
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7994921668004117,
            "auditor_fn_violation": 0.009601941186665897,
            "auditor_fp_violation": 0.0035322611506661745,
            "ave_precision_score": 0.7822616629576125,
            "fpr": 0.21405049396267836,
            "logloss": 1.8703773204359446,
            "mae": 0.3166243864027673,
            "precision": 0.675,
            "recall": 0.8526315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.5979910501020191,
            "auditor_fn_violation": 0.010111068380764022,
            "auditor_fp_violation": 0.014294902961792479,
            "ave_precision_score": 0.5428237927973317,
            "fpr": 0.30701754385964913,
            "logloss": 5.604256310621654,
            "mae": 0.42203253320834266,
            "precision": 0.5888399412628488,
            "recall": 0.837160751565762
        },
        "train": {
            "accuracy": 0.610318331503842,
            "auc_prc": 0.6160753826037673,
            "auditor_fn_violation": 0.00270148477670576,
            "auditor_fp_violation": 0.009574618072689552,
            "ave_precision_score": 0.5539052146712606,
            "fpr": 0.3172338090010977,
            "logloss": 5.775708401309987,
            "mae": 0.41268868824129323,
            "precision": 0.5859598853868195,
            "recall": 0.8610526315789474
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.7569928625614124,
            "auditor_fn_violation": 0.011928634216020223,
            "auditor_fp_violation": 0.010443255945869295,
            "ave_precision_score": 0.7558999062994051,
            "fpr": 0.18201754385964913,
            "logloss": 1.4355880954792464,
            "mae": 0.3218564413682053,
            "precision": 0.6838095238095238,
            "recall": 0.7494780793319415
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7991062208817312,
            "auditor_fn_violation": 0.005939106823040037,
            "auditor_fp_violation": 0.01309680862848568,
            "ave_precision_score": 0.7993162791271307,
            "fpr": 0.18660812294182216,
            "logloss": 1.325177590942157,
            "mae": 0.30605620803830735,
            "precision": 0.6875,
            "recall": 0.7873684210526316
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7791677789626017,
            "auditor_fn_violation": 0.010866479874006523,
            "auditor_fp_violation": 0.004023844252663995,
            "ave_precision_score": 0.7755986365051454,
            "fpr": 0.12719298245614036,
            "logloss": 1.7651853901063277,
            "mae": 0.31423205197449766,
            "precision": 0.7308584686774942,
            "recall": 0.6576200417536534
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.8185443987143078,
            "auditor_fn_violation": 0.010789762551273913,
            "auditor_fp_violation": 0.007734216860189937,
            "ave_precision_score": 0.8177888339470247,
            "fpr": 0.13172338090010977,
            "logloss": 1.5253716817292242,
            "mae": 0.29807490731045194,
            "precision": 0.7315436241610739,
            "recall": 0.6884210526315789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.7511435247023242,
            "auditor_fn_violation": 0.008105794235065744,
            "auditor_fp_violation": 0.00278301122320814,
            "ave_precision_score": 0.7464912754372575,
            "fpr": 0.16228070175438597,
            "logloss": 1.8294092916471403,
            "mae": 0.35000929459995,
            "precision": 0.6810344827586207,
            "recall": 0.6597077244258872
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.7934064677003225,
            "auditor_fn_violation": 0.011612455947772842,
            "auditor_fp_violation": 0.007238240062840513,
            "ave_precision_score": 0.7925720317051828,
            "fpr": 0.17892425905598244,
            "logloss": 1.55723791541751,
            "mae": 0.3345160155909684,
            "precision": 0.6765873015873016,
            "recall": 0.7178947368421053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.774438615701329,
            "auditor_fn_violation": 0.01593689338167967,
            "auditor_fp_violation": 0.020519326607511858,
            "ave_precision_score": 0.7745298824037508,
            "fpr": 0.14473684210526316,
            "logloss": 1.0211191937716764,
            "mae": 0.2898819092923841,
            "precision": 0.7333333333333333,
            "recall": 0.7578288100208769
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8242890069589909,
            "auditor_fn_violation": 0.01974001964296031,
            "auditor_fp_violation": 0.018111964873765103,
            "ave_precision_score": 0.8245839575764213,
            "fpr": 0.1394072447859495,
            "logloss": 0.9421538348701268,
            "mae": 0.272166268074433,
            "precision": 0.742914979757085,
            "recall": 0.7726315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7763157894736842,
            "auc_prc": 0.8668947955633887,
            "auditor_fn_violation": 0.01858541185950262,
            "auditor_fp_violation": 0.009235342976378593,
            "ave_precision_score": 0.8671288596894623,
            "fpr": 0.07346491228070176,
            "logloss": 0.4793223433178017,
            "mae": 0.3105339214365047,
            "precision": 0.8361858190709046,
            "recall": 0.7139874739039666
        },
        "train": {
            "accuracy": 0.7837541163556532,
            "auc_prc": 0.8789217507693048,
            "auditor_fn_violation": 0.02353457738748629,
            "auditor_fp_violation": 0.012185419792747159,
            "ave_precision_score": 0.8791345632657682,
            "fpr": 0.07025246981339188,
            "logloss": 0.4669181099489484,
            "mae": 0.3061609329511621,
            "precision": 0.8423645320197044,
            "recall": 0.72
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7990865415992741,
            "auditor_fn_violation": 0.011795864923268508,
            "auditor_fp_violation": 0.013411125967343307,
            "ave_precision_score": 0.7995214401998083,
            "fpr": 0.12938596491228072,
            "logloss": 0.7069836259622654,
            "mae": 0.30342266766915044,
            "precision": 0.7440347071583514,
            "recall": 0.7160751565762005
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8523005203681763,
            "auditor_fn_violation": 0.01200531515396615,
            "auditor_fp_violation": 0.015926645786966637,
            "ave_precision_score": 0.8525304192570118,
            "fpr": 0.1119648737650933,
            "logloss": 0.6550455649849182,
            "mae": 0.2805686658423695,
            "precision": 0.7697516930022573,
            "recall": 0.7178947368421053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.7482229175826881,
            "auditor_fn_violation": 0.008419404461048236,
            "auditor_fp_violation": 0.004651857704306958,
            "ave_precision_score": 0.7485531964406389,
            "fpr": 0.13267543859649122,
            "logloss": 1.6078245327553824,
            "mae": 0.33265274073983303,
            "precision": 0.7211981566820277,
            "recall": 0.6534446764091858
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7763518326735208,
            "auditor_fn_violation": 0.009232191345542777,
            "auditor_fp_violation": 0.009708053454717569,
            "ave_precision_score": 0.776645881941985,
            "fpr": 0.13391877058177826,
            "logloss": 1.4803468871917926,
            "mae": 0.3163997007616177,
            "precision": 0.7239819004524887,
            "recall": 0.6736842105263158
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7702438945253287,
            "auditor_fn_violation": 0.017431692488004982,
            "auditor_fp_violation": 0.02354543981200114,
            "ave_precision_score": 0.7704453760200474,
            "fpr": 0.14802631578947367,
            "logloss": 1.0826697010609594,
            "mae": 0.2908849473105198,
            "precision": 0.7267206477732794,
            "recall": 0.7494780793319415
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.819488021696245,
            "auditor_fn_violation": 0.018829510659194644,
            "auditor_fp_violation": 0.01710994068419622,
            "ave_precision_score": 0.8197562642995999,
            "fpr": 0.14050493962678376,
            "logloss": 1.0088770054952876,
            "mae": 0.2734239755265961,
            "precision": 0.7408906882591093,
            "recall": 0.7705263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.783020549500955,
            "auditor_fn_violation": 0.013194520748635677,
            "auditor_fp_violation": 0.02261354888375674,
            "ave_precision_score": 0.783748343547619,
            "fpr": 0.13706140350877194,
            "logloss": 0.9563352406678701,
            "mae": 0.290384374418408,
            "precision": 0.7379454926624738,
            "recall": 0.7348643006263048
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8344706072517949,
            "auditor_fn_violation": 0.017711017389797214,
            "auditor_fp_violation": 0.01591909284081412,
            "ave_precision_score": 0.8347105615918099,
            "fpr": 0.13062568605927552,
            "logloss": 0.9147891270894267,
            "mae": 0.27187599790059924,
            "precision": 0.7510460251046025,
            "recall": 0.7557894736842106
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.7317829181780002,
            "auditor_fn_violation": 0.009889023184265468,
            "auditor_fp_violation": 0.015411652688302746,
            "ave_precision_score": 0.7300170179472079,
            "fpr": 0.17982456140350878,
            "logloss": 1.8871323096750217,
            "mae": 0.3391244733668237,
            "precision": 0.6733067729083665,
            "recall": 0.7056367432150313
        },
        "train": {
            "accuracy": 0.6695938529088913,
            "auc_prc": 0.7614600007451702,
            "auditor_fn_violation": 0.018882662198856093,
            "auditor_fp_violation": 0.008373699634437414,
            "ave_precision_score": 0.7604687407577211,
            "fpr": 0.19538968166849616,
            "logloss": 1.7420602874321058,
            "mae": 0.33290848898262676,
            "precision": 0.6641509433962264,
            "recall": 0.7410526315789474
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7700139357312653,
            "auditor_fn_violation": 0.01445811815551405,
            "auditor_fp_violation": 0.022980734167983474,
            "ave_precision_score": 0.7705450814723762,
            "fpr": 0.14912280701754385,
            "logloss": 1.116464224161219,
            "mae": 0.28925162438739455,
            "precision": 0.7296222664015904,
            "recall": 0.7661795407098121
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8192569960850699,
            "auditor_fn_violation": 0.01911144491305102,
            "auditor_fp_violation": 0.01899314192489351,
            "ave_precision_score": 0.8195126175526186,
            "fpr": 0.14489571899012074,
            "logloss": 1.0152226566249447,
            "mae": 0.2739503954455956,
            "precision": 0.7370517928286853,
            "recall": 0.7789473684210526
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7802183329648776,
            "auditor_fn_violation": 0.009254935355089195,
            "auditor_fp_violation": 0.005604007130991451,
            "ave_precision_score": 0.7757068217487502,
            "fpr": 0.14583333333333334,
            "logloss": 1.691651962643095,
            "mae": 0.3141428358033167,
            "precision": 0.7114967462039046,
            "recall": 0.6847599164926931
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.8244957067801792,
            "auditor_fn_violation": 0.007961176266682076,
            "auditor_fp_violation": 0.0031546138430397098,
            "ave_precision_score": 0.8225937444679567,
            "fpr": 0.150384193194292,
            "logloss": 1.4497203633627371,
            "mae": 0.29818981472678685,
            "precision": 0.7157676348547718,
            "recall": 0.7263157894736842
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 8653,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.7884836414655836,
            "auditor_fn_violation": 0.018155056220928105,
            "auditor_fp_violation": 0.01909869940440015,
            "ave_precision_score": 0.7885763224639517,
            "fpr": 0.13267543859649122,
            "logloss": 1.1260167385777706,
            "mae": 0.27659036034106316,
            "precision": 0.7515400410677618,
            "recall": 0.7640918580375783
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8328033162308073,
            "auditor_fn_violation": 0.020821537928245423,
            "auditor_fp_violation": 0.0182428826070756,
            "ave_precision_score": 0.8329657902850429,
            "fpr": 0.13830954994511527,
            "logloss": 1.0540549610999892,
            "mae": 0.2662055742537709,
            "precision": 0.7459677419354839,
            "recall": 0.7789473684210526
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7743077875887006,
            "auditor_fn_violation": 0.015680511299124646,
            "auditor_fp_violation": 0.020519326607511858,
            "ave_precision_score": 0.7744134206788932,
            "fpr": 0.14473684210526316,
            "logloss": 1.0231849014166607,
            "mae": 0.2899773985296796,
            "precision": 0.7327935222672065,
            "recall": 0.755741127348643
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8242289514099083,
            "auditor_fn_violation": 0.01974001964296031,
            "auditor_fp_violation": 0.018111964873765103,
            "ave_precision_score": 0.8245153597766514,
            "fpr": 0.1394072447859495,
            "logloss": 0.944558610549983,
            "mae": 0.27221311668099357,
            "precision": 0.742914979757085,
            "recall": 0.7726315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.7360338129027375,
            "auditor_fn_violation": 0.009275537486723068,
            "auditor_fp_violation": 0.007503241359750422,
            "ave_precision_score": 0.7343911762858054,
            "fpr": 0.16776315789473684,
            "logloss": 1.781764461309127,
            "mae": 0.34895505158268864,
            "precision": 0.6730769230769231,
            "recall": 0.6576200417536534
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.7731102502497011,
            "auditor_fn_violation": 0.009329250678837601,
            "auditor_fp_violation": 0.009974924218773606,
            "ave_precision_score": 0.7728214432405907,
            "fpr": 0.1734357848518112,
            "logloss": 1.584331329070501,
            "mae": 0.3331939327276644,
            "precision": 0.6782077393075356,
            "recall": 0.7010526315789474
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.736499827558911,
            "auditor_fn_violation": 0.010005768596857489,
            "auditor_fp_violation": 0.012079129694907015,
            "ave_precision_score": 0.7355433570388631,
            "fpr": 0.1611842105263158,
            "logloss": 2.288728811619217,
            "mae": 0.34752264558574114,
            "precision": 0.6885593220338984,
            "recall": 0.6784968684759917
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.755763618641437,
            "auditor_fn_violation": 0.006902767346467161,
            "auditor_fp_violation": 0.004697932506873187,
            "ave_precision_score": 0.7551287229947006,
            "fpr": 0.15148188803512624,
            "logloss": 2.3769191474164466,
            "mae": 0.33811616254360366,
            "precision": 0.7044967880085653,
            "recall": 0.6926315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.8003334619773659,
            "auditor_fn_violation": 0.008636871406072595,
            "auditor_fp_violation": 0.01999007333576435,
            "ave_precision_score": 0.8008211516455285,
            "fpr": 0.24342105263157895,
            "logloss": 1.3232844253671483,
            "mae": 0.3146845255853474,
            "precision": 0.6547433903576982,
            "recall": 0.8789144050104384
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.8526194139382466,
            "auditor_fn_violation": 0.009678202091397541,
            "auditor_fp_violation": 0.025488675616068643,
            "ave_precision_score": 0.8527294283126323,
            "fpr": 0.2239297475301866,
            "logloss": 1.2041502690754466,
            "mae": 0.2971265289506877,
            "precision": 0.6714975845410628,
            "recall": 0.8778947368421053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 8653,
        "test": {
            "accuracy": 0.5460526315789473,
            "auc_prc": 0.5331676646863247,
            "auditor_fn_violation": 0.004914752957550458,
            "auditor_fp_violation": 0.0023322596329160084,
            "ave_precision_score": 0.5303423878501441,
            "fpr": 0.20614035087719298,
            "logloss": 3.6324818086320634,
            "mae": 0.46203066151408106,
            "precision": 0.5736961451247166,
            "recall": 0.5281837160751566
        },
        "train": {
            "accuracy": 0.566410537870472,
            "auc_prc": 0.54310611142135,
            "auditor_fn_violation": 0.0008319371425270156,
            "auditor_fp_violation": 0.0027341665072155856,
            "ave_precision_score": 0.5400139384047593,
            "fpr": 0.2074643249176729,
            "logloss": 3.659536994663685,
            "mae": 0.44326582961809025,
            "precision": 0.5873362445414847,
            "recall": 0.5663157894736842
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 8653,
        "test": {
            "accuracy": 0.5789473684210527,
            "auc_prc": 0.6210555078286336,
            "auditor_fn_violation": 0.005464143134454089,
            "auditor_fp_violation": 0.01394544386370084,
            "ave_precision_score": 0.532879789806343,
            "fpr": 0.3081140350877193,
            "logloss": 7.479516995622029,
            "mae": 0.43622061063726425,
            "precision": 0.5722983257229832,
            "recall": 0.7849686847599165
        },
        "train": {
            "accuracy": 0.5949506037321625,
            "auc_prc": 0.6559849923123655,
            "auditor_fn_violation": 0.004053382633312151,
            "auditor_fp_violation": 0.0148037744589573,
            "ave_precision_score": 0.5556401824453663,
            "fpr": 0.31613611416026344,
            "logloss": 7.671923675257383,
            "mae": 0.41221726798404684,
            "precision": 0.5777126099706745,
            "recall": 0.8294736842105264
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7752344258579379,
            "auditor_fn_violation": 0.016802182910302905,
            "auditor_fp_violation": 0.020519326607511858,
            "ave_precision_score": 0.7752170705902584,
            "fpr": 0.14473684210526316,
            "logloss": 1.0282024394562608,
            "mae": 0.289955753902579,
            "precision": 0.7327935222672065,
            "recall": 0.755741127348643
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8240877034599355,
            "auditor_fn_violation": 0.019157663643191408,
            "auditor_fp_violation": 0.019436248099175223,
            "ave_precision_score": 0.8243810782438121,
            "fpr": 0.14050493962678376,
            "logloss": 0.9484379603342336,
            "mae": 0.27254397939785896,
            "precision": 0.7419354838709677,
            "recall": 0.7747368421052632
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.791947740885666,
            "auditor_fn_violation": 0.014535948430575402,
            "auditor_fp_violation": 0.023846784976297553,
            "ave_precision_score": 0.7922991094770945,
            "fpr": 0.1425438596491228,
            "logloss": 0.960820318259411,
            "mae": 0.2757914586293566,
            "precision": 0.7440944881889764,
            "recall": 0.7891440501043842
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8372847541118604,
            "auditor_fn_violation": 0.018949679357559656,
            "auditor_fp_violation": 0.01600217524849193,
            "ave_precision_score": 0.8375450050218545,
            "fpr": 0.13830954994511527,
            "logloss": 0.8780521076674291,
            "mae": 0.26035493931845083,
            "precision": 0.75,
            "recall": 0.7957894736842105
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7734369812247273,
            "auditor_fn_violation": 0.015517983371790653,
            "auditor_fp_violation": 0.020995401320854097,
            "ave_precision_score": 0.7750569143820423,
            "fpr": 0.1425438596491228,
            "logloss": 1.000799894027216,
            "mae": 0.2906444136678184,
            "precision": 0.7341513292433538,
            "recall": 0.7494780793319415
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8258110285005502,
            "auditor_fn_violation": 0.01926165578600728,
            "auditor_fp_violation": 0.016253940120242904,
            "ave_precision_score": 0.8260915065319745,
            "fpr": 0.1350164654226125,
            "logloss": 0.9337896812977536,
            "mae": 0.2728375799832663,
            "precision": 0.7453416149068323,
            "recall": 0.7578947368421053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.7337292518068406,
            "auditor_fn_violation": 0.006965809617990694,
            "auditor_fp_violation": 0.011646104290749974,
            "ave_precision_score": 0.7326378088458826,
            "fpr": 0.17653508771929824,
            "logloss": 1.8255721531493536,
            "mae": 0.3422265661741234,
            "precision": 0.6754032258064516,
            "recall": 0.6993736951983298
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.7628050700921013,
            "auditor_fn_violation": 0.007113062568605926,
            "auditor_fp_violation": 0.011223677982658442,
            "ave_precision_score": 0.7626284769018863,
            "fpr": 0.1877058177826564,
            "logloss": 1.7138760787328267,
            "mae": 0.3339241908665369,
            "precision": 0.6673151750972762,
            "recall": 0.7221052631578947
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.774766034474272,
            "auditor_fn_violation": 0.014414624766509176,
            "auditor_fp_violation": 0.021917162999878455,
            "ave_precision_score": 0.7747393782489581,
            "fpr": 0.14692982456140352,
            "logloss": 1.029609427796168,
            "mae": 0.2903691162926408,
            "precision": 0.7309236947791165,
            "recall": 0.7599164926931107
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8226762903338267,
            "auditor_fn_violation": 0.01594777283494136,
            "auditor_fp_violation": 0.01527709241784912,
            "ave_precision_score": 0.8228773639543641,
            "fpr": 0.13391877058177826,
            "logloss": 0.957439377296562,
            "mae": 0.27248275253332493,
            "precision": 0.7505112474437627,
            "recall": 0.7726315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8385340455080204,
            "auditor_fn_violation": 0.03170210233307695,
            "auditor_fp_violation": 0.01913921640128034,
            "ave_precision_score": 0.8389468125077708,
            "fpr": 0.12280701754385964,
            "logloss": 0.5938836418264553,
            "mae": 0.30584183908934454,
            "precision": 0.7690721649484537,
            "recall": 0.778705636743215
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8796659482728728,
            "auditor_fn_violation": 0.034019296319833615,
            "auditor_fp_violation": 0.0251689342289449,
            "ave_precision_score": 0.8797975222568881,
            "fpr": 0.11745334796926454,
            "logloss": 0.5452936536171238,
            "mae": 0.2908205163557187,
            "precision": 0.7766179540709812,
            "recall": 0.783157894736842
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7798157224353661,
            "auditor_fn_violation": 0.007515199794894339,
            "auditor_fp_violation": 0.010747133422470731,
            "ave_precision_score": 0.7801473937237847,
            "fpr": 0.13486842105263158,
            "logloss": 1.2910525813851779,
            "mae": 0.31135207216512245,
            "precision": 0.7284768211920529,
            "recall": 0.6889352818371608
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8180586700480923,
            "auditor_fn_violation": 0.003947079553989252,
            "auditor_fp_violation": 0.01253789061319852,
            "ave_precision_score": 0.8182273314272283,
            "fpr": 0.13830954994511527,
            "logloss": 1.1765338224423507,
            "mae": 0.29391759992221245,
            "precision": 0.7341772151898734,
            "recall": 0.7326315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7743652982753924,
            "auditor_fn_violation": 0.015680511299124646,
            "auditor_fp_violation": 0.020519326607511858,
            "ave_precision_score": 0.7743000916735752,
            "fpr": 0.14473684210526316,
            "logloss": 1.0207838200636719,
            "mae": 0.290496103049391,
            "precision": 0.7327935222672065,
            "recall": 0.755741127348643
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8237869520000614,
            "auditor_fn_violation": 0.01974001964296031,
            "auditor_fp_violation": 0.019436248099175223,
            "ave_precision_score": 0.824086199598151,
            "fpr": 0.14050493962678376,
            "logloss": 0.9418167644188181,
            "mae": 0.2734509817427556,
            "precision": 0.7414141414141414,
            "recall": 0.7726315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 8653,
        "test": {
            "accuracy": 0.5241228070175439,
            "auc_prc": 0.6651468073247142,
            "auditor_fn_violation": 0.0027034574955133164,
            "auditor_fp_violation": 0.006140857339653992,
            "ave_precision_score": 0.6639775327097616,
            "fpr": 0.4298245614035088,
            "logloss": 3.3028975903310736,
            "mae": 0.45941593099159833,
            "precision": 0.5271411338962606,
            "recall": 0.9123173277661796
        },
        "train": {
            "accuracy": 0.5389681668496158,
            "auc_prc": 0.6826839716551911,
            "auditor_fn_violation": 0.0019180773008261633,
            "auditor_fp_violation": 0.006314262983514445,
            "ave_precision_score": 0.6825052846483164,
            "fpr": 0.4226125137211855,
            "logloss": 3.28032336879302,
            "mae": 0.44930311781971055,
            "precision": 0.5333333333333333,
            "recall": 0.9263157894736842
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.7681354500990255,
            "auditor_fn_violation": 0.010976357909387247,
            "auditor_fp_violation": 0.005520440824926063,
            "ave_precision_score": 0.7623683426646799,
            "fpr": 0.15460526315789475,
            "logloss": 1.9045613203935623,
            "mae": 0.32587071832268644,
            "precision": 0.7006369426751592,
            "recall": 0.6889352818371608
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.8098035757979568,
            "auditor_fn_violation": 0.009276099139176161,
            "auditor_fp_violation": 0.011691960644115257,
            "ave_precision_score": 0.8067693819795687,
            "fpr": 0.1734357848518112,
            "logloss": 1.66673085232562,
            "mae": 0.312965240131775,
            "precision": 0.6865079365079365,
            "recall": 0.728421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8367928504767761,
            "auditor_fn_violation": 0.010232392044830244,
            "auditor_fp_violation": 0.020066042704914713,
            "ave_precision_score": 0.8380563936307956,
            "fpr": 0.10855263157894737,
            "logloss": 0.5260820809762017,
            "mae": 0.3248258376056982,
            "precision": 0.7843137254901961,
            "recall": 0.7515657620041754
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8693738287097335,
            "auditor_fn_violation": 0.014538101565659482,
            "auditor_fp_violation": 0.013081702736180627,
            "ave_precision_score": 0.8695395575645739,
            "fpr": 0.10976948408342481,
            "logloss": 0.47657762995305536,
            "mae": 0.3080736243781459,
            "precision": 0.7867803837953091,
            "recall": 0.7768421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7960019192823078,
            "auditor_fn_violation": 0.008069168223272166,
            "auditor_fp_violation": 0.009463251083829671,
            "ave_precision_score": 0.7955581650214597,
            "fpr": 0.12390350877192982,
            "logloss": 1.2683774071731295,
            "mae": 0.2974577543260698,
            "precision": 0.7477678571428571,
            "recall": 0.6993736951983298
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.840273543724621,
            "auditor_fn_violation": 0.0050586400138656245,
            "auditor_fp_violation": 0.01483398624356741,
            "ave_precision_score": 0.8404189603676717,
            "fpr": 0.14050493962678376,
            "logloss": 1.0989211122806903,
            "mae": 0.2785633470931831,
            "precision": 0.7360824742268042,
            "recall": 0.751578947368421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7656979576793007,
            "auditor_fn_violation": 0.01313729260520822,
            "auditor_fp_violation": 0.020802945585673194,
            "ave_precision_score": 0.7667605920267757,
            "fpr": 0.1513157894736842,
            "logloss": 1.0202799820989796,
            "mae": 0.2940566486713,
            "precision": 0.7206477732793523,
            "recall": 0.7432150313152401
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8280857358288047,
            "auditor_fn_violation": 0.018616904500548846,
            "auditor_fp_violation": 0.018106929576330078,
            "ave_precision_score": 0.8283132332599479,
            "fpr": 0.1437980241492865,
            "logloss": 0.9817978392713306,
            "mae": 0.27892064962518465,
            "precision": 0.733739837398374,
            "recall": 0.76
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.6493233590885988,
            "auditor_fn_violation": 0.01484726953082079,
            "auditor_fp_violation": 0.00590281998298286,
            "ave_precision_score": 0.6001918963446695,
            "fpr": 0.24342105263157895,
            "logloss": 5.0794572794756085,
            "mae": 0.3619491636624939,
            "precision": 0.6448,
            "recall": 0.8413361169102297
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.6843280477383151,
            "auditor_fn_violation": 0.016872147437749146,
            "auditor_fp_violation": 0.005249297576007829,
            "ave_precision_score": 0.6345088858190379,
            "fpr": 0.2513721185510428,
            "logloss": 4.60021948365117,
            "mae": 0.3459140463108511,
            "precision": 0.6433021806853583,
            "recall": 0.8694736842105263
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.7769040864672534,
            "auditor_fn_violation": 0.02115152181079002,
            "auditor_fp_violation": 0.02350998743973097,
            "ave_precision_score": 0.7772792292364441,
            "fpr": 0.14364035087719298,
            "logloss": 1.0250777324610885,
            "mae": 0.28396605721677526,
            "precision": 0.7421259842519685,
            "recall": 0.7870563674321504
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8265615100901719,
            "auditor_fn_violation": 0.021905367150037554,
            "auditor_fp_violation": 0.020317425150303638,
            "ave_precision_score": 0.8267680950077775,
            "fpr": 0.13830954994511527,
            "logloss": 0.9347602882702493,
            "mae": 0.2699952439013828,
            "precision": 0.7454545454545455,
            "recall": 0.7768421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7750603637029244,
            "auditor_fn_violation": 0.01553171812621324,
            "auditor_fp_violation": 0.020519326607511858,
            "ave_precision_score": 0.7748753048981227,
            "fpr": 0.14473684210526316,
            "logloss": 1.0304652484877503,
            "mae": 0.2897742217941737,
            "precision": 0.7322515212981744,
            "recall": 0.7536534446764092
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8241040637741681,
            "auditor_fn_violation": 0.01974001964296031,
            "auditor_fp_violation": 0.018111964873765103,
            "ave_precision_score": 0.8244110049666494,
            "fpr": 0.1394072447859495,
            "logloss": 0.9514001507560046,
            "mae": 0.27196113159113083,
            "precision": 0.742914979757085,
            "recall": 0.7726315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.7452873581032383,
            "auditor_fn_violation": 0.005654140570633262,
            "auditor_fp_violation": 0.009169502856448287,
            "ave_precision_score": 0.7434443738039532,
            "fpr": 0.16776315789473684,
            "logloss": 1.6290631665429745,
            "mae": 0.3341186264094223,
            "precision": 0.6845360824742268,
            "recall": 0.6931106471816284
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.7864841103826595,
            "auditor_fn_violation": 0.01099543590039864,
            "auditor_fp_violation": 0.005307203496510543,
            "ave_precision_score": 0.7863200085525399,
            "fpr": 0.1800219538968167,
            "logloss": 1.4558960287100018,
            "mae": 0.32410896857452204,
            "precision": 0.6752475247524753,
            "recall": 0.7178947368421053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 8653,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7690653191267913,
            "auditor_fn_violation": 0.010461304618540092,
            "auditor_fp_violation": 0.005520440824926063,
            "ave_precision_score": 0.7624270607375624,
            "fpr": 0.15460526315789475,
            "logloss": 1.9436198328421883,
            "mae": 0.3262831835284716,
            "precision": 0.7,
            "recall": 0.6868475991649269
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.8102065050891646,
            "auditor_fn_violation": 0.011822751169911607,
            "auditor_fp_violation": 0.009939677136728471,
            "ave_precision_score": 0.80717679842827,
            "fpr": 0.1712403951701427,
            "logloss": 1.6913031268915746,
            "mae": 0.31402793501175186,
            "precision": 0.6892430278884463,
            "recall": 0.728421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7752819463953837,
            "auditor_fn_violation": 0.014922810680145041,
            "auditor_fp_violation": 0.021463879097281306,
            "ave_precision_score": 0.7753733609741699,
            "fpr": 0.15350877192982457,
            "logloss": 1.0385274854018975,
            "mae": 0.2876496653476529,
            "precision": 0.7281553398058253,
            "recall": 0.7828810020876826
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8242841300326718,
            "auditor_fn_violation": 0.017727193945346355,
            "auditor_fp_violation": 0.021329519934742545,
            "ave_precision_score": 0.8245818944021885,
            "fpr": 0.15148188803512624,
            "logloss": 0.955947693485883,
            "mae": 0.271745302931908,
            "precision": 0.7341040462427746,
            "recall": 0.8021052631578948
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.7906913111036636,
            "auditor_fn_violation": 0.017097480130388608,
            "auditor_fp_violation": 0.014920384101130434,
            "ave_precision_score": 0.783311496452459,
            "fpr": 0.22587719298245615,
            "logloss": 1.7844387320866473,
            "mae": 0.3274444677294744,
            "precision": 0.6526138279932546,
            "recall": 0.8079331941544885
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.8366314901468395,
            "auditor_fn_violation": 0.007217054711421805,
            "auditor_fp_violation": 0.01839897682756121,
            "ave_precision_score": 0.8335966179900143,
            "fpr": 0.18660812294182216,
            "logloss": 1.5044574418501273,
            "mae": 0.2903134833445052,
            "precision": 0.6947935368043088,
            "recall": 0.8147368421052632
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7747743497761855,
            "auditor_fn_violation": 0.014414624766509176,
            "auditor_fp_violation": 0.021917162999878455,
            "ave_precision_score": 0.7747476816247862,
            "fpr": 0.14692982456140352,
            "logloss": 1.0296311846988189,
            "mae": 0.2903650639472664,
            "precision": 0.7309236947791165,
            "recall": 0.7599164926931107
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8226931392396335,
            "auditor_fn_violation": 0.01594777283494136,
            "auditor_fp_violation": 0.01527709241784912,
            "ave_precision_score": 0.8228951655623505,
            "fpr": 0.13391877058177826,
            "logloss": 0.9574511390148596,
            "mae": 0.2724780894101272,
            "precision": 0.7505112474437627,
            "recall": 0.7726315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.774782929551058,
            "auditor_fn_violation": 0.014414624766509176,
            "auditor_fp_violation": 0.021917162999878455,
            "ave_precision_score": 0.7747819824731126,
            "fpr": 0.14692982456140352,
            "logloss": 1.0296409561964657,
            "mae": 0.2903718140607557,
            "precision": 0.7309236947791165,
            "recall": 0.7599164926931107
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8226720815381283,
            "auditor_fn_violation": 0.01594777283494136,
            "auditor_fp_violation": 0.01527709241784912,
            "ave_precision_score": 0.822873156635016,
            "fpr": 0.13391877058177826,
            "logloss": 0.9574421379087829,
            "mae": 0.27248715722380257,
            "precision": 0.7505112474437627,
            "recall": 0.7726315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 8653,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.7747640517266037,
            "auditor_fn_violation": 0.01683423067062228,
            "auditor_fp_violation": 0.021086564563834533,
            "ave_precision_score": 0.7749630470213175,
            "fpr": 0.14364035087719298,
            "logloss": 1.0008741312241278,
            "mae": 0.2903953347358548,
            "precision": 0.7358870967741935,
            "recall": 0.7620041753653445
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8240712379475372,
            "auditor_fn_violation": 0.016061008723785318,
            "auditor_fp_violation": 0.01781488232509895,
            "ave_precision_score": 0.8243618766213626,
            "fpr": 0.14050493962678376,
            "logloss": 0.9245030167899777,
            "mae": 0.27148792839524,
            "precision": 0.7429718875502008,
            "recall": 0.7789473684210526
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7789518333358676,
            "auditor_fn_violation": 0.013618009009998901,
            "auditor_fp_violation": 0.021162533932984888,
            "ave_precision_score": 0.7804010207686958,
            "fpr": 0.13925438596491227,
            "logloss": 0.9526385924761972,
            "mae": 0.29076546847528834,
            "precision": 0.7348643006263048,
            "recall": 0.7348643006263048
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8339177762794614,
            "auditor_fn_violation": 0.015675082327113066,
            "auditor_fp_violation": 0.015277092417849131,
            "ave_precision_score": 0.8341616171318242,
            "fpr": 0.12623490669593854,
            "logloss": 0.913024800456322,
            "mae": 0.2710488845132476,
            "precision": 0.7584033613445378,
            "recall": 0.76
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7770225344778334,
            "auditor_fn_violation": 0.01209574039482841,
            "auditor_fp_violation": 0.024573558607835996,
            "ave_precision_score": 0.7772657253778867,
            "fpr": 0.22039473684210525,
            "logloss": 1.223658448379739,
            "mae": 0.293597120940635,
            "precision": 0.6789137380191693,
            "recall": 0.8872651356993737
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8240077647556802,
            "auditor_fn_violation": 0.013888728407187013,
            "auditor_fp_violation": 0.024411121964974483,
            "ave_precision_score": 0.8242609341569853,
            "fpr": 0.21405049396267836,
            "logloss": 1.130739824074943,
            "mae": 0.2804193333892125,
            "precision": 0.6829268292682927,
            "recall": 0.8842105263157894
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.7371521900113716,
            "auditor_fn_violation": 0.005189448046002278,
            "auditor_fp_violation": 0.005452068392690739,
            "ave_precision_score": 0.7356987077430077,
            "fpr": 0.1600877192982456,
            "logloss": 1.7983247227505605,
            "mae": 0.34653906120365197,
            "precision": 0.683982683982684,
            "recall": 0.6597077244258872
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.7731899165139544,
            "auditor_fn_violation": 0.005673349124732803,
            "auditor_fp_violation": 0.0074421696089587985,
            "ave_precision_score": 0.7728944881063281,
            "fpr": 0.16794731064763996,
            "logloss": 1.595474188911509,
            "mae": 0.33104894342133523,
            "precision": 0.6832298136645962,
            "recall": 0.6947368421052632
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7713451524246346,
            "auditor_fn_violation": 0.01216899241841556,
            "auditor_fp_violation": 0.023464405818240757,
            "ave_precision_score": 0.7681984282477012,
            "fpr": 0.13815789473684212,
            "logloss": 1.182758736087439,
            "mae": 0.29172867257872853,
            "precision": 0.7358490566037735,
            "recall": 0.732776617954071
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8108552197560521,
            "auditor_fn_violation": 0.018808712230631464,
            "auditor_fp_violation": 0.01643521082790362,
            "ave_precision_score": 0.8090664034565553,
            "fpr": 0.12623490669593854,
            "logloss": 1.0877312202081566,
            "mae": 0.27076920271886085,
            "precision": 0.7573839662447257,
            "recall": 0.7557894736842106
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7750060669487346,
            "auditor_fn_violation": 0.015680511299124646,
            "auditor_fp_violation": 0.020519326607511858,
            "ave_precision_score": 0.7750462200149253,
            "fpr": 0.14473684210526316,
            "logloss": 1.0327085275428904,
            "mae": 0.2900970678340846,
            "precision": 0.7327935222672065,
            "recall": 0.755741127348643
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8237665536467547,
            "auditor_fn_violation": 0.018059968802357156,
            "auditor_fp_violation": 0.019436248099175223,
            "ave_precision_score": 0.8240436964082181,
            "fpr": 0.14050493962678376,
            "logloss": 0.9531102396688713,
            "mae": 0.2726899435182454,
            "precision": 0.7419354838709677,
            "recall": 0.7747368421052632
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7725033201220077,
            "auditor_fn_violation": 0.010584917408343409,
            "auditor_fp_violation": 0.004907621247113162,
            "ave_precision_score": 0.76702769431,
            "fpr": 0.15570175438596492,
            "logloss": 1.8414314486399743,
            "mae": 0.31892614247704426,
            "precision": 0.7047817047817048,
            "recall": 0.7077244258872651
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.8152377116691091,
            "auditor_fn_violation": 0.009218325726500665,
            "auditor_fp_violation": 0.0079834640832234,
            "ave_precision_score": 0.8133136284999811,
            "fpr": 0.16355653128430298,
            "logloss": 1.5983768632159572,
            "mae": 0.30670685766444633,
            "precision": 0.7002012072434608,
            "recall": 0.7326315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7755144617437852,
            "auditor_fn_violation": 0.013521865729040768,
            "auditor_fp_violation": 0.020947287387058876,
            "ave_precision_score": 0.7760662429899542,
            "fpr": 0.14473684210526316,
            "logloss": 0.9983725274641368,
            "mae": 0.2966816890458419,
            "precision": 0.728952772073922,
            "recall": 0.7411273486430062
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8270314263603211,
            "auditor_fn_violation": 0.017902825119879836,
            "auditor_fp_violation": 0.02061702534768729,
            "ave_precision_score": 0.827301800998923,
            "fpr": 0.1394072447859495,
            "logloss": 0.9277168521384394,
            "mae": 0.27146811140414906,
            "precision": 0.737603305785124,
            "recall": 0.751578947368421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 8653,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.6735512418573947,
            "auditor_fn_violation": 0.0034336886056477315,
            "auditor_fp_violation": 0.007531096795105558,
            "ave_precision_score": 0.5665053958518435,
            "fpr": 0.3684210526315789,
            "logloss": 8.28305386366403,
            "mae": 0.43032814835099337,
            "precision": 0.5602094240837696,
            "recall": 0.8935281837160751
        },
        "train": {
            "accuracy": 0.5894621295279913,
            "auc_prc": 0.695108930794557,
            "auditor_fn_violation": 0.0025350973482003583,
            "auditor_fp_violation": 0.004846473781206258,
            "ave_precision_score": 0.5769671889431494,
            "fpr": 0.3677277716794731,
            "logloss": 8.534168214654832,
            "mae": 0.41651373622753884,
            "precision": 0.5654993514915694,
            "recall": 0.9178947368421052
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7744196323997348,
            "auditor_fn_violation": 0.01593689338167967,
            "auditor_fp_violation": 0.020519326607511858,
            "ave_precision_score": 0.7745037518330844,
            "fpr": 0.14473684210526316,
            "logloss": 1.0210778970713086,
            "mae": 0.28990441789743043,
            "precision": 0.7333333333333333,
            "recall": 0.7578288100208769
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8242796969449404,
            "auditor_fn_violation": 0.01974001964296031,
            "auditor_fp_violation": 0.018111964873765103,
            "ave_precision_score": 0.8245748496495254,
            "fpr": 0.1394072447859495,
            "logloss": 0.9421256500047359,
            "mae": 0.27218732969591364,
            "precision": 0.742914979757085,
            "recall": 0.7726315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.7738526503047708,
            "auditor_fn_violation": 0.006327143537340219,
            "auditor_fp_violation": 0.007604533851950896,
            "ave_precision_score": 0.7681115282548023,
            "fpr": 0.1611842105263158,
            "logloss": 1.888145686471373,
            "mae": 0.3208456091647418,
            "precision": 0.7,
            "recall": 0.7160751565762005
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.8150206925196803,
            "auditor_fn_violation": 0.008927147726616218,
            "auditor_fp_violation": 0.010649654075066212,
            "ave_precision_score": 0.8120321537281482,
            "fpr": 0.1690450054884742,
            "logloss": 1.6655989777717763,
            "mae": 0.30730697273811436,
            "precision": 0.6980392156862745,
            "recall": 0.7494736842105263
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6389531869582333,
            "auditor_fn_violation": 0.011585265355455445,
            "auditor_fp_violation": 0.0030362424537093407,
            "ave_precision_score": 0.6053456708541584,
            "fpr": 0.25548245614035087,
            "logloss": 4.803165803013947,
            "mae": 0.3835290790679443,
            "precision": 0.6223662884927067,
            "recall": 0.8016701461377871
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.6569221992787964,
            "auditor_fn_violation": 0.004009474839678781,
            "auditor_fp_violation": 0.0031797903302148286,
            "ave_precision_score": 0.6202299946241171,
            "fpr": 0.2557628979143798,
            "logloss": 4.606158515073482,
            "mae": 0.37556011897986213,
            "precision": 0.6229773462783171,
            "recall": 0.8105263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7746025342071721,
            "auditor_fn_violation": 0.012482602644398058,
            "auditor_fp_violation": 0.01021534783841822,
            "ave_precision_score": 0.7750825085028867,
            "fpr": 0.17434210526315788,
            "logloss": 1.1587340432162356,
            "mae": 0.2998602384944201,
            "precision": 0.7033582089552238,
            "recall": 0.7870563674321504
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8220488106453396,
            "auditor_fn_violation": 0.01099543590039864,
            "auditor_fp_violation": 0.018915094814650712,
            "ave_precision_score": 0.8223000046616382,
            "fpr": 0.17014270032930845,
            "logloss": 1.0596783668953584,
            "mae": 0.28290074937453547,
            "precision": 0.7113594040968343,
            "recall": 0.8042105263157895
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 8653,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7704599262972559,
            "auditor_fn_violation": 0.012388748489177018,
            "auditor_fp_violation": 0.004743020947287392,
            "ave_precision_score": 0.7649225682827634,
            "fpr": 0.15679824561403508,
            "logloss": 1.86634452566236,
            "mae": 0.32095136469426655,
            "precision": 0.7020833333333333,
            "recall": 0.7035490605427975
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.8126181203668359,
            "auditor_fn_violation": 0.007970420012710156,
            "auditor_fp_violation": 0.008529793854923012,
            "ave_precision_score": 0.8107316963241133,
            "fpr": 0.1668496158068057,
            "logloss": 1.6237544423792916,
            "mae": 0.3090982843851838,
            "precision": 0.6953907815631263,
            "recall": 0.7305263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 8653,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7741225990531815,
            "auditor_fn_violation": 0.015158590631066185,
            "auditor_fp_violation": 0.019615291114622586,
            "ave_precision_score": 0.7746052397360074,
            "fpr": 0.14583333333333334,
            "logloss": 1.007768148761025,
            "mae": 0.28816300974917064,
            "precision": 0.7323943661971831,
            "recall": 0.7599164926931107
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8242238057808976,
            "auditor_fn_violation": 0.019465018198625,
            "auditor_fp_violation": 0.020065660278552656,
            "ave_precision_score": 0.8245256247881074,
            "fpr": 0.14270032930845225,
            "logloss": 0.9314298225556535,
            "mae": 0.2722407621967796,
            "precision": 0.7389558232931727,
            "recall": 0.7747368421052632
        }
    }
]
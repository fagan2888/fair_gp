[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7901883009909989,
            "auditor_fn_violation": 0.01253132832080201,
            "auditor_fp_violation": 0.008059990022449493,
            "ave_precision_score": 0.7748137835383382,
            "fpr": 0.14692982456140352,
            "logloss": 1.672296791730202,
            "mae": 0.28486008305621724,
            "precision": 0.7418111753371869,
            "recall": 0.7857142857142857
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8078977705041954,
            "auditor_fn_violation": 0.01578409478027178,
            "auditor_fp_violation": 0.024365387496101593,
            "ave_precision_score": 0.794322437672487,
            "fpr": 0.12623490669593854,
            "logloss": 1.4476401788651951,
            "mae": 0.2555943235039905,
            "precision": 0.7638603696098563,
            "recall": 0.8017241379310345
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8158798143113428,
            "auditor_fn_violation": 0.007328589330469038,
            "auditor_fp_violation": 0.0038870873867132296,
            "ave_precision_score": 0.8162197829986825,
            "fpr": 0.06907894736842106,
            "logloss": 0.6754649055437684,
            "mae": 0.34238689466531314,
            "precision": 0.8342105263157895,
            "recall": 0.6469387755102041
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8184847769926058,
            "auditor_fn_violation": 0.019403648889057126,
            "auditor_fp_violation": 0.011023606578310828,
            "ave_precision_score": 0.8187561282704413,
            "fpr": 0.07683863885839737,
            "logloss": 0.6136280255801475,
            "mae": 0.3335180512650866,
            "precision": 0.8028169014084507,
            "recall": 0.6142241379310345
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8236768914077244,
            "auditor_fn_violation": 0.01752148227712138,
            "auditor_fp_violation": 0.00834580527147252,
            "ave_precision_score": 0.8239343354604829,
            "fpr": 0.1425438596491228,
            "logloss": 0.9142737134079376,
            "mae": 0.2909398905149938,
            "precision": 0.7357723577235772,
            "recall": 0.7387755102040816
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8470680054343267,
            "auditor_fn_violation": 0.01756311745334797,
            "auditor_fp_violation": 0.027024903184297316,
            "ave_precision_score": 0.8474204043859477,
            "fpr": 0.11745334796926454,
            "logloss": 0.7402626520630866,
            "mae": 0.24690011527309394,
            "precision": 0.7708779443254818,
            "recall": 0.7758620689655172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7877878707297679,
            "auditor_fn_violation": 0.0012978875760830653,
            "auditor_fp_violation": 0.007163569468695445,
            "ave_precision_score": 0.7701708837618951,
            "fpr": 0.14364035087719298,
            "logloss": 1.7355499746822436,
            "mae": 0.290383404246728,
            "precision": 0.7421259842519685,
            "recall": 0.7693877551020408
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.7998758786124204,
            "auditor_fn_violation": 0.0128884514932435,
            "auditor_fp_violation": 0.025276449657062453,
            "ave_precision_score": 0.7853999896107391,
            "fpr": 0.13172338090010977,
            "logloss": 1.5190410998809778,
            "mae": 0.2727257226490028,
            "precision": 0.7489539748953975,
            "recall": 0.771551724137931
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7576254767608441,
            "auditor_fn_violation": 0.008261725742928749,
            "auditor_fp_violation": 0.007109004739336495,
            "ave_precision_score": 0.7591777756165419,
            "fpr": 0.18859649122807018,
            "logloss": 0.6650109096802638,
            "mae": 0.3767199699309059,
            "precision": 0.6785046728971963,
            "recall": 0.7408163265306122
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.76538255620681,
            "auditor_fn_violation": 0.0008895113365380992,
            "auditor_fp_violation": 0.003924197663653542,
            "ave_precision_score": 0.7660221471603106,
            "fpr": 0.18880351262349068,
            "logloss": 0.6397485593200528,
            "mae": 0.36520987105556857,
            "precision": 0.6766917293233082,
            "recall": 0.7758620689655172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7909455036399131,
            "auditor_fn_violation": 0.01999194414607949,
            "auditor_fp_violation": 0.015605512596657526,
            "ave_precision_score": 0.7755034785740984,
            "fpr": 0.1600877192982456,
            "logloss": 1.680735357208617,
            "mae": 0.2890103836893525,
            "precision": 0.7345454545454545,
            "recall": 0.8244897959183674
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8057406881932886,
            "auditor_fn_violation": 0.01782807827699762,
            "auditor_fp_violation": 0.03068634168023438,
            "ave_precision_score": 0.7904758961060477,
            "fpr": 0.14709110867178923,
            "logloss": 1.5279095132811085,
            "mae": 0.258412545790337,
            "precision": 0.7413127413127413,
            "recall": 0.8275862068965517
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.8240209613484191,
            "auditor_fn_violation": 0.01418725384890799,
            "auditor_fp_violation": 0.017964787561320368,
            "ave_precision_score": 0.8242800639133672,
            "fpr": 0.15570175438596492,
            "logloss": 0.666773108168177,
            "mae": 0.33721210469450347,
            "precision": 0.7215686274509804,
            "recall": 0.7510204081632653
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8523929223577269,
            "auditor_fn_violation": 0.019079545024414245,
            "auditor_fp_violation": 0.018044433311968804,
            "ave_precision_score": 0.8525843880325225,
            "fpr": 0.13062568605927552,
            "logloss": 0.5400677913332806,
            "mae": 0.3084926807586151,
            "precision": 0.7541322314049587,
            "recall": 0.7866379310344828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7910651698873203,
            "auditor_fn_violation": 0.022075277479412823,
            "auditor_fp_violation": 0.006430843103018216,
            "ave_precision_score": 0.7835484840534122,
            "fpr": 0.15021929824561403,
            "logloss": 1.2395633337017853,
            "mae": 0.29134774953448694,
            "precision": 0.7424812030075187,
            "recall": 0.8061224489795918
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8141204389881158,
            "auditor_fn_violation": 0.018329611264620158,
            "auditor_fp_violation": 0.02259483273046067,
            "ave_precision_score": 0.8094615712679338,
            "fpr": 0.13391877058177826,
            "logloss": 1.005688192422221,
            "mae": 0.26543830286554937,
            "precision": 0.7540322580645161,
            "recall": 0.8060344827586207
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.7925256662600877,
            "auditor_fn_violation": 0.01512934121016828,
            "auditor_fp_violation": 0.00974370167123972,
            "ave_precision_score": 0.778706424184973,
            "fpr": 0.15570175438596492,
            "logloss": 1.6029721752395438,
            "mae": 0.2839772681725126,
            "precision": 0.7325800376647834,
            "recall": 0.7938775510204081
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8119957390777801,
            "auditor_fn_violation": 0.017700329308452254,
            "auditor_fp_violation": 0.026985612093797667,
            "ave_precision_score": 0.8006989423079414,
            "fpr": 0.13611416026344675,
            "logloss": 1.3642365491257062,
            "mae": 0.2523505181433306,
            "precision": 0.7524950099800399,
            "recall": 0.8125
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.7872409403326068,
            "auditor_fn_violation": 0.017340225563909777,
            "auditor_fp_violation": 0.017439926831296252,
            "ave_precision_score": 0.7729312194408762,
            "fpr": 0.15789473684210525,
            "logloss": 1.6227609722059446,
            "mae": 0.29084404403575387,
            "precision": 0.7318435754189944,
            "recall": 0.8020408163265306
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8110455476142493,
            "auditor_fn_violation": 0.01931611718838715,
            "auditor_fp_violation": 0.026381511577365395,
            "ave_precision_score": 0.799902543657373,
            "fpr": 0.132821075740944,
            "logloss": 1.3521823351642475,
            "mae": 0.2528831304946898,
            "precision": 0.7560483870967742,
            "recall": 0.8081896551724138
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.784588685948517,
            "auditor_fn_violation": 0.008129699248120301,
            "auditor_fp_violation": 0.007031055125966575,
            "ave_precision_score": 0.7707276604572418,
            "fpr": 0.14144736842105263,
            "logloss": 1.6183131702205718,
            "mae": 0.29564144502290096,
            "precision": 0.742,
            "recall": 0.7571428571428571
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8001206264703006,
            "auditor_fn_violation": 0.01655532003482342,
            "auditor_fp_violation": 0.023653236480795252,
            "ave_precision_score": 0.7883329506102693,
            "fpr": 0.12403951701427003,
            "logloss": 1.4126007865867622,
            "mae": 0.26788713284684057,
            "precision": 0.7640918580375783,
            "recall": 0.7887931034482759
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.7636264411407364,
            "auditor_fn_violation": 0.014822771213748663,
            "auditor_fp_violation": 0.012794129874449162,
            "ave_precision_score": 0.7273857179375174,
            "fpr": 0.1611842105263158,
            "logloss": 4.271874893886559,
            "mae": 0.33032909130309296,
            "precision": 0.6975308641975309,
            "recall": 0.6918367346938775
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7779977338084691,
            "auditor_fn_violation": 0.022703830576479057,
            "auditor_fp_violation": 0.03212783356294065,
            "ave_precision_score": 0.740623698271991,
            "fpr": 0.14818880351262348,
            "logloss": 3.6971069329236865,
            "mae": 0.29515124776723406,
            "precision": 0.709051724137931,
            "recall": 0.709051724137931
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.7882396477349476,
            "auditor_fn_violation": 0.01412012173290369,
            "auditor_fp_violation": 0.01007368836783903,
            "ave_precision_score": 0.7752352958728657,
            "fpr": 0.15460526315789475,
            "logloss": 1.5312620138341764,
            "mae": 0.2874675782945128,
            "precision": 0.7344632768361582,
            "recall": 0.7959183673469388
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.807036216028648,
            "auditor_fn_violation": 0.016271433438055947,
            "auditor_fp_violation": 0.02422786867935278,
            "ave_precision_score": 0.7966273711568548,
            "fpr": 0.132821075740944,
            "logloss": 1.2978417630604604,
            "mae": 0.2590595243561534,
            "precision": 0.7535641547861507,
            "recall": 0.7974137931034483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7873033830445134,
            "auditor_fn_violation": 0.01798245614035088,
            "auditor_fp_violation": 0.011785981541531557,
            "ave_precision_score": 0.7704139155239185,
            "fpr": 0.1513157894736842,
            "logloss": 1.7625094715741079,
            "mae": 0.28608698711637054,
            "precision": 0.7396226415094339,
            "recall": 0.8
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8072956796812754,
            "auditor_fn_violation": 0.017847004050115453,
            "auditor_fp_violation": 0.026472372224145852,
            "ave_precision_score": 0.7910829400114252,
            "fpr": 0.12952799121844127,
            "logloss": 1.5690332402147293,
            "mae": 0.24980719121066267,
            "precision": 0.7611336032388664,
            "recall": 0.8103448275862069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8179327948132061,
            "auditor_fn_violation": 0.016290726817042616,
            "auditor_fp_violation": 0.006480211191485821,
            "ave_precision_score": 0.8183866575384938,
            "fpr": 0.09320175438596491,
            "logloss": 0.7018109041864102,
            "mae": 0.32631676139041,
            "precision": 0.7956730769230769,
            "recall": 0.6755102040816326
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8292425131123796,
            "auditor_fn_violation": 0.019564517960558688,
            "auditor_fp_violation": 0.011566314765837383,
            "ave_precision_score": 0.8295227653881133,
            "fpr": 0.09220636663007684,
            "logloss": 0.6044292018847697,
            "mae": 0.3172908373842132,
            "precision": 0.7771883289124668,
            "recall": 0.6314655172413793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8203213763120207,
            "auditor_fn_violation": 0.012676781238811323,
            "auditor_fp_violation": 0.007698823480502211,
            "ave_precision_score": 0.8207943077072748,
            "fpr": 0.14583333333333334,
            "logloss": 0.6814540819998652,
            "mae": 0.3321251123495244,
            "precision": 0.734,
            "recall": 0.7489795918367347
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8423912354720055,
            "auditor_fn_violation": 0.018935236004390784,
            "auditor_fp_violation": 0.01810336994771829,
            "ave_precision_score": 0.8426123118313913,
            "fpr": 0.12952799121844127,
            "logloss": 0.572423054927678,
            "mae": 0.31481004703707516,
            "precision": 0.7467811158798283,
            "recall": 0.75
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8408339221159622,
            "auditor_fn_violation": 0.014607948442534912,
            "auditor_fp_violation": 0.008688783570300156,
            "ave_precision_score": 0.8410476294604161,
            "fpr": 0.1513157894736842,
            "logloss": 0.6158013662736015,
            "mae": 0.3079551086844187,
            "precision": 0.7401129943502824,
            "recall": 0.8020408163265306
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8534273467526678,
            "auditor_fn_violation": 0.016692531889927708,
            "auditor_fp_violation": 0.02468953899272379,
            "ave_precision_score": 0.8536718374374885,
            "fpr": 0.132821075740944,
            "logloss": 0.5331143321096902,
            "mae": 0.28052005260920876,
            "precision": 0.7555555555555555,
            "recall": 0.8060344827586207
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7912176657398873,
            "auditor_fn_violation": 0.011099176512710345,
            "auditor_fp_violation": 0.005648748648873372,
            "ave_precision_score": 0.7765976729368271,
            "fpr": 0.1513157894736842,
            "logloss": 1.6444699987053222,
            "mae": 0.2849194030541,
            "precision": 0.7366412213740458,
            "recall": 0.7877551020408163
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.807751862552821,
            "auditor_fn_violation": 0.01578409478027178,
            "auditor_fp_violation": 0.024365387496101593,
            "ave_precision_score": 0.794239487131071,
            "fpr": 0.12623490669593854,
            "logloss": 1.4389857910835364,
            "mae": 0.2555287732896841,
            "precision": 0.7638603696098563,
            "recall": 0.8017241379310345
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7924741049523822,
            "auditor_fn_violation": 0.009644647332617258,
            "auditor_fp_violation": 0.0105439843685042,
            "ave_precision_score": 0.7786057187095814,
            "fpr": 0.15021929824561403,
            "logloss": 1.6120100787306382,
            "mae": 0.28494343150564305,
            "precision": 0.7400379506641366,
            "recall": 0.7959183673469388
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8067854074391697,
            "auditor_fn_violation": 0.015493111018585111,
            "auditor_fp_violation": 0.0258780944803385,
            "ave_precision_score": 0.7933104243698789,
            "fpr": 0.13062568605927552,
            "logloss": 1.43543302900016,
            "mae": 0.25698846834423206,
            "precision": 0.7571428571428571,
            "recall": 0.7995689655172413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.792860082593194,
            "auditor_fn_violation": 0.013265306122448986,
            "auditor_fp_violation": 0.012999397189656611,
            "ave_precision_score": 0.7790819501015542,
            "fpr": 0.15679824561403508,
            "logloss": 1.6003161197983018,
            "mae": 0.2833629655070869,
            "precision": 0.7376146788990826,
            "recall": 0.8204081632653061
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8119707433377323,
            "auditor_fn_violation": 0.014989212309322836,
            "auditor_fp_violation": 0.02704209303639092,
            "ave_precision_score": 0.800659187436646,
            "fpr": 0.13611416026344675,
            "logloss": 1.3610871101086774,
            "mae": 0.252584162509441,
            "precision": 0.7549407114624506,
            "recall": 0.8232758620689655
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.7920068665844113,
            "auditor_fn_violation": 0.026006981740064446,
            "auditor_fp_violation": 0.020747588758626423,
            "ave_precision_score": 0.7821768511861146,
            "fpr": 0.17214912280701755,
            "logloss": 1.3309930455885806,
            "mae": 0.2936948691290683,
            "precision": 0.7250437828371279,
            "recall": 0.8448979591836735
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8093915918705197,
            "auditor_fn_violation": 0.020600704038759986,
            "auditor_fp_violation": 0.029866140166053975,
            "ave_precision_score": 0.803825164266606,
            "fpr": 0.1668496158068057,
            "logloss": 1.1071796429700551,
            "mae": 0.27626197191575885,
            "precision": 0.7174721189591078,
            "recall": 0.8318965517241379
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.8186708490717897,
            "auditor_fn_violation": 0.013650196920873613,
            "auditor_fp_violation": 0.01720088135029517,
            "ave_precision_score": 0.8190118423410118,
            "fpr": 0.16447368421052633,
            "logloss": 0.6707742288784715,
            "mae": 0.33765280675044784,
            "precision": 0.7115384615384616,
            "recall": 0.7551020408163265
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8530772136136797,
            "auditor_fn_violation": 0.019642586774669745,
            "auditor_fp_violation": 0.01791673726784491,
            "ave_precision_score": 0.8532774833355596,
            "fpr": 0.13830954994511527,
            "logloss": 0.5371042300321075,
            "mae": 0.3077670788988978,
            "precision": 0.7454545454545455,
            "recall": 0.7952586206896551
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7845144079028871,
            "auditor_fn_violation": 0.008129699248120301,
            "auditor_fp_violation": 0.007031055125966575,
            "ave_precision_score": 0.7706567681928245,
            "fpr": 0.14144736842105263,
            "logloss": 1.6199744398482718,
            "mae": 0.295632701708852,
            "precision": 0.742,
            "recall": 0.7571428571428571
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8000303996565469,
            "auditor_fn_violation": 0.01655532003482342,
            "auditor_fp_violation": 0.023653236480795252,
            "ave_precision_score": 0.7882547962027944,
            "fpr": 0.12403951701427003,
            "logloss": 1.4140688060428548,
            "mae": 0.2679525820811525,
            "precision": 0.7640918580375783,
            "recall": 0.7887931034482759
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7850341387760396,
            "auditor_fn_violation": 0.011566863587540279,
            "auditor_fp_violation": 0.008454934730190404,
            "ave_precision_score": 0.7712007737410714,
            "fpr": 0.1337719298245614,
            "logloss": 1.6113242386760231,
            "mae": 0.29783655968903067,
            "precision": 0.7409766454352441,
            "recall": 0.7122448979591837
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8008170055680944,
            "auditor_fn_violation": 0.013964854839320192,
            "auditor_fp_violation": 0.02421067882725918,
            "ave_precision_score": 0.7889262750342666,
            "fpr": 0.1163556531284303,
            "logloss": 1.394649516831697,
            "mae": 0.26928934681224576,
            "precision": 0.7670329670329671,
            "recall": 0.7521551724137931
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8238439358085701,
            "auditor_fn_violation": 0.01137665592552811,
            "auditor_fp_violation": 0.0037571713644300375,
            "ave_precision_score": 0.8241530043786183,
            "fpr": 0.11951754385964912,
            "logloss": 0.680823693361699,
            "mae": 0.3302698276577266,
            "precision": 0.7655913978494624,
            "recall": 0.726530612244898
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8393313661085096,
            "auditor_fn_violation": 0.0178186153904387,
            "auditor_fp_violation": 0.010851708057374811,
            "ave_precision_score": 0.8395441332875588,
            "fpr": 0.10537870472008781,
            "logloss": 0.5768253394324935,
            "mae": 0.31614596244011123,
            "precision": 0.7757009345794392,
            "recall": 0.7155172413793104
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.784619773049237,
            "auditor_fn_violation": 0.008129699248120301,
            "auditor_fp_violation": 0.007031055125966575,
            "ave_precision_score": 0.7707468086641951,
            "fpr": 0.14144736842105263,
            "logloss": 1.6249527650403766,
            "mae": 0.29563040416416964,
            "precision": 0.742,
            "recall": 0.7571428571428571
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.7993906931938861,
            "auditor_fn_violation": 0.01655532003482342,
            "auditor_fp_violation": 0.023653236480795252,
            "ave_precision_score": 0.786740262545826,
            "fpr": 0.12403951701427003,
            "logloss": 1.4372319695677405,
            "mae": 0.2679068550138721,
            "precision": 0.7640918580375783,
            "recall": 0.7887931034482759
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8564409379327178,
            "auditor_fn_violation": 0.011076799140708913,
            "auditor_fp_violation": 0.015621102519331506,
            "ave_precision_score": 0.8566305185412584,
            "fpr": 0.0800438596491228,
            "logloss": 0.5485379697011905,
            "mae": 0.30754016092204867,
            "precision": 0.8193069306930693,
            "recall": 0.6755102040816326
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8569312473780761,
            "auditor_fn_violation": 0.002526590711230558,
            "auditor_fp_violation": 0.010890999147874478,
            "ave_precision_score": 0.8572079195621509,
            "fpr": 0.07025246981339188,
            "logloss": 0.5097821126634059,
            "mae": 0.2938963002655614,
            "precision": 0.8320209973753281,
            "recall": 0.6831896551724138
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7914838089955798,
            "auditor_fn_violation": 0.004981203007518804,
            "auditor_fp_violation": 0.005040741664588018,
            "ave_precision_score": 0.7775186004929082,
            "fpr": 0.1425438596491228,
            "logloss": 1.6111019352342288,
            "mae": 0.2852868428180316,
            "precision": 0.7470817120622568,
            "recall": 0.7836734693877551
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8065624580175753,
            "auditor_fn_violation": 0.01124664067527159,
            "auditor_fp_violation": 0.024581488493849717,
            "ave_precision_score": 0.793606067016609,
            "fpr": 0.132821075740944,
            "logloss": 1.442245266326348,
            "mae": 0.2577472119911921,
            "precision": 0.7550607287449392,
            "recall": 0.8038793103448276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7905163897099845,
            "auditor_fn_violation": 0.016131847475832444,
            "auditor_fp_violation": 0.008706971813419812,
            "ave_precision_score": 0.7773278874807916,
            "fpr": 0.15021929824561403,
            "logloss": 1.5348164426652178,
            "mae": 0.28475032946504003,
            "precision": 0.740530303030303,
            "recall": 0.7979591836734694
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8079892616342628,
            "auditor_fn_violation": 0.017468488587758818,
            "auditor_fp_violation": 0.027140320762640068,
            "ave_precision_score": 0.7975309541697839,
            "fpr": 0.12843029637760703,
            "logloss": 1.3068763048897594,
            "mae": 0.2575391922932391,
            "precision": 0.7602459016393442,
            "recall": 0.7995689655172413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.8264984678672254,
            "auditor_fn_violation": 0.007308449695667743,
            "auditor_fp_violation": 0.007711815082730529,
            "ave_precision_score": 0.8270596695478396,
            "fpr": 0.16337719298245615,
            "logloss": 0.595373733823475,
            "mae": 0.34379089633761106,
            "precision": 0.7178030303030303,
            "recall": 0.773469387755102
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8352369242482136,
            "auditor_fn_violation": 0.011047920057534353,
            "auditor_fp_violation": 0.019648000942986178,
            "ave_precision_score": 0.8355123101120198,
            "fpr": 0.15587266739846323,
            "logloss": 0.5523726404648953,
            "mae": 0.3284833226253894,
            "precision": 0.72265625,
            "recall": 0.7974137931034483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.8134901383067517,
            "auditor_fn_violation": 0.016416040100250627,
            "auditor_fp_violation": 0.01787124802527646,
            "ave_precision_score": 0.8138661321486096,
            "fpr": 0.16557017543859648,
            "logloss": 0.6976886676572086,
            "mae": 0.33839315609945897,
            "precision": 0.7067961165048544,
            "recall": 0.7428571428571429
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8494139590262132,
            "auditor_fn_violation": 0.01970882698058216,
            "auditor_fp_violation": 0.01740840878450557,
            "ave_precision_score": 0.849614141031408,
            "fpr": 0.141602634467618,
            "logloss": 0.5569829247415917,
            "mae": 0.3100784799352006,
            "precision": 0.7345679012345679,
            "recall": 0.7693965517241379
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.7917148855606763,
            "auditor_fn_violation": 0.012804332259219479,
            "auditor_fp_violation": 0.006282738837615371,
            "ave_precision_score": 0.7779135741763494,
            "fpr": 0.14692982456140352,
            "logloss": 1.6034727071877042,
            "mae": 0.2840069774228617,
            "precision": 0.7413127413127413,
            "recall": 0.7836734693877551
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8094446300719337,
            "auditor_fn_violation": 0.015173738597221697,
            "auditor_fp_violation": 0.023906172875886814,
            "ave_precision_score": 0.7982031174014106,
            "fpr": 0.12843029637760703,
            "logloss": 1.3629345926232912,
            "mae": 0.2551287596271862,
            "precision": 0.7597535934291582,
            "recall": 0.7974137931034483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7924428386708644,
            "auditor_fn_violation": 0.009644647332617258,
            "auditor_fp_violation": 0.0105439843685042,
            "ave_precision_score": 0.7785633114794359,
            "fpr": 0.15021929824561403,
            "logloss": 1.6135929218701945,
            "mae": 0.2849354445579743,
            "precision": 0.7400379506641366,
            "recall": 0.7959183673469388
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8070388297985949,
            "auditor_fn_violation": 0.015493111018585111,
            "auditor_fp_violation": 0.0258780944803385,
            "ave_precision_score": 0.7935232178810435,
            "fpr": 0.13062568605927552,
            "logloss": 1.4371063900659478,
            "mae": 0.25700505253468814,
            "precision": 0.7571428571428571,
            "recall": 0.7995689655172413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8251272826479094,
            "auditor_fn_violation": 0.008964375223773724,
            "auditor_fp_violation": 0.008514696100440679,
            "ave_precision_score": 0.8255262186267844,
            "fpr": 0.08223684210526316,
            "logloss": 0.6853176205247928,
            "mae": 0.3202710414423377,
            "precision": 0.8138957816377171,
            "recall": 0.6693877551020408
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8425168946665677,
            "auditor_fn_violation": 0.019564517960558688,
            "auditor_fp_violation": 0.010949935783623969,
            "ave_precision_score": 0.8427583866167223,
            "fpr": 0.07354555433589462,
            "logloss": 0.5790428613360015,
            "mae": 0.30675324243493435,
            "precision": 0.8138888888888889,
            "recall": 0.6314655172413793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8317470892300844,
            "auditor_fn_violation": 0.012150912996777659,
            "auditor_fp_violation": 0.017374968820154657,
            "ave_precision_score": 0.832003453760876,
            "fpr": 0.12828947368421054,
            "logloss": 0.6542194530046482,
            "mae": 0.3261911893445556,
            "precision": 0.7572614107883817,
            "recall": 0.7448979591836735
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8538604238692817,
            "auditor_fn_violation": 0.019682804042545146,
            "auditor_fp_violation": 0.02325050280317374,
            "ave_precision_score": 0.854057503712556,
            "fpr": 0.11855104281009879,
            "logloss": 0.5464819189441964,
            "mae": 0.3034099815148916,
            "precision": 0.7652173913043478,
            "recall": 0.7586206896551724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 9540,
        "test": {
            "accuracy": 0.5285087719298246,
            "auc_prc": 0.6034680853133086,
            "auditor_fn_violation": 0.01885517364840674,
            "auditor_fp_violation": 0.014467448241456722,
            "ave_precision_score": 0.6045959086713097,
            "fpr": 0.10197368421052631,
            "logloss": 3.3461006644646196,
            "mae": 0.48413623991717203,
            "precision": 0.6219512195121951,
            "recall": 0.3122448979591837
        },
        "train": {
            "accuracy": 0.5850713501646543,
            "auc_prc": 0.6455950572267636,
            "auditor_fn_violation": 0.01113545175820433,
            "auditor_fp_violation": 0.00856791342208209,
            "ave_precision_score": 0.6463493898978045,
            "fpr": 0.0889132821075741,
            "logloss": 3.0506922264307827,
            "mae": 0.4301415374544192,
            "precision": 0.6733870967741935,
            "recall": 0.3599137931034483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 9540,
        "test": {
            "accuracy": 0.5526315789473685,
            "auc_prc": 0.601658268012943,
            "auditor_fn_violation": 0.011278195488721802,
            "auditor_fp_violation": 0.008748544940550428,
            "ave_precision_score": 0.6082970964193172,
            "fpr": 0.0581140350877193,
            "logloss": 10.638458261951897,
            "mae": 0.4613309014711088,
            "precision": 0.7180851063829787,
            "recall": 0.2755102040816326
        },
        "train": {
            "accuracy": 0.6037321624588364,
            "auc_prc": 0.6197355106939928,
            "auditor_fn_violation": 0.015008138082440678,
            "auditor_fp_violation": 0.010682265229595032,
            "ave_precision_score": 0.6218412172478519,
            "fpr": 0.05378704720087816,
            "logloss": 9.575369190204245,
            "mae": 0.4119657034715367,
            "precision": 0.7562189054726368,
            "recall": 0.3275862068965517
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8168881781312464,
            "auditor_fn_violation": 0.02098102398854279,
            "auditor_fp_violation": 0.016494138189074584,
            "ave_precision_score": 0.8172342664472094,
            "fpr": 0.09868421052631579,
            "logloss": 0.6707618004851036,
            "mae": 0.32815314890734737,
            "precision": 0.7926267281105991,
            "recall": 0.7020408163265306
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8460186024032409,
            "auditor_fn_violation": 0.03299945115257959,
            "auditor_fp_violation": 0.022982832249144804,
            "ave_precision_score": 0.8462410416447734,
            "fpr": 0.09001097694840834,
            "logloss": 0.5542719044177035,
            "mae": 0.2997895454186896,
            "precision": 0.7955112219451371,
            "recall": 0.6875
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.787302115162463,
            "auditor_fn_violation": 0.014086555674901542,
            "auditor_fp_violation": 0.013628190737507274,
            "ave_precision_score": 0.7759170267366637,
            "fpr": 0.17214912280701755,
            "logloss": 1.4787462331524266,
            "mae": 0.29382592082764536,
            "precision": 0.7255244755244755,
            "recall": 0.8469387755102041
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.813693436412203,
            "auditor_fn_violation": 0.013110829327378025,
            "auditor_fp_violation": 0.033004516019714304,
            "ave_precision_score": 0.806255730999679,
            "fpr": 0.14818880351262348,
            "logloss": 1.1931407721609493,
            "mae": 0.26540935345406563,
            "precision": 0.7462406015037594,
            "recall": 0.8556034482758621
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7929241229973834,
            "auditor_fn_violation": 0.0052810597923379846,
            "auditor_fp_violation": 0.004263843851334502,
            "ave_precision_score": 0.7792884003997926,
            "fpr": 0.14144736842105263,
            "logloss": 1.5240539700025306,
            "mae": 0.2881333567466323,
            "precision": 0.7465618860510805,
            "recall": 0.7755102040816326
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8022931643980533,
            "auditor_fn_violation": 0.009581172640902379,
            "auditor_fp_violation": 0.025679183334683973,
            "ave_precision_score": 0.7912926080858111,
            "fpr": 0.132821075740944,
            "logloss": 1.3478034376938859,
            "mae": 0.270441613906103,
            "precision": 0.7468619246861925,
            "recall": 0.7693965517241379
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7918836701207761,
            "auditor_fn_violation": 0.016447368421052634,
            "auditor_fp_violation": 0.010206202710567892,
            "ave_precision_score": 0.778281816232584,
            "fpr": 0.1513157894736842,
            "logloss": 1.5619502184748248,
            "mae": 0.28472159070877723,
            "precision": 0.7396226415094339,
            "recall": 0.8
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8101413805205709,
            "auditor_fn_violation": 0.01733127673265453,
            "auditor_fp_violation": 0.02487126028628471,
            "ave_precision_score": 0.7991516509938759,
            "fpr": 0.12843029637760703,
            "logloss": 1.3226882680361949,
            "mae": 0.25530834944548647,
            "precision": 0.7617107942973523,
            "recall": 0.8060344827586207
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7915046999505543,
            "auditor_fn_violation": 0.00765529896168994,
            "auditor_fp_violation": 0.007820944541448414,
            "ave_precision_score": 0.7776964909545496,
            "fpr": 0.14473684210526316,
            "logloss": 1.5969442395944073,
            "mae": 0.2849879684545528,
            "precision": 0.7436893203883496,
            "recall": 0.7816326530612245
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8090026291977751,
            "auditor_fn_violation": 0.015216321586736819,
            "auditor_fp_violation": 0.02276673125139668,
            "ave_precision_score": 0.7978365051311702,
            "fpr": 0.12623490669593854,
            "logloss": 1.3628404208219649,
            "mae": 0.2563273916325207,
            "precision": 0.7619047619047619,
            "recall": 0.7931034482758621
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.7889093019769795,
            "auditor_fn_violation": 0.016138560687432867,
            "auditor_fp_violation": 0.008522491061777668,
            "ave_precision_score": 0.775669009546449,
            "fpr": 0.14583333333333334,
            "logloss": 1.5686697105888576,
            "mae": 0.28378936695740314,
            "precision": 0.747148288973384,
            "recall": 0.8020408163265306
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8091556979401276,
            "auditor_fn_violation": 0.017828078276997616,
            "auditor_fp_violation": 0.024291716701414726,
            "ave_precision_score": 0.7992996385717129,
            "fpr": 0.12733260153677278,
            "logloss": 1.305898865130742,
            "mae": 0.2519408893819573,
            "precision": 0.7603305785123967,
            "recall": 0.7931034482758621
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7888557295422174,
            "auditor_fn_violation": 0.01565297171500179,
            "auditor_fp_violation": 0.014384301987195484,
            "ave_precision_score": 0.775666195946295,
            "fpr": 0.15789473684210525,
            "logloss": 1.5467980142420021,
            "mae": 0.28678730809874764,
            "precision": 0.7328385899814471,
            "recall": 0.8061224489795918
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8073987698702908,
            "auditor_fn_violation": 0.016739846322722282,
            "auditor_fp_violation": 0.029097508208154384,
            "ave_precision_score": 0.796703223202005,
            "fpr": 0.132821075740944,
            "logloss": 1.3186095975907346,
            "mae": 0.25850806031315254,
            "precision": 0.7555555555555555,
            "recall": 0.8060344827586207
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.7909899685967945,
            "auditor_fn_violation": 0.013079573934837097,
            "auditor_fp_violation": 0.008436746487070766,
            "ave_precision_score": 0.7763571182976152,
            "fpr": 0.14583333333333334,
            "logloss": 1.6427051246289646,
            "mae": 0.2849838863635244,
            "precision": 0.7432432432432432,
            "recall": 0.7857142857142857
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8078502749160422,
            "auditor_fn_violation": 0.01570129452288126,
            "auditor_fp_violation": 0.024365387496101593,
            "ave_precision_score": 0.7943210864735295,
            "fpr": 0.12623490669593854,
            "logloss": 1.438863931044686,
            "mae": 0.25556175809687626,
            "precision": 0.7633744855967078,
            "recall": 0.7995689655172413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7805540310759193,
            "auditor_fn_violation": 0.011716791979949875,
            "auditor_fp_violation": 0.008213290928743655,
            "ave_precision_score": 0.7414166831662169,
            "fpr": 0.13925438596491227,
            "logloss": 4.465983901919745,
            "mae": 0.30506233909977476,
            "precision": 0.7348643006263048,
            "recall": 0.7183673469387755
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.7984676922487179,
            "auditor_fn_violation": 0.013664408191074603,
            "auditor_fp_violation": 0.02381776792226258,
            "ave_precision_score": 0.7584242272976253,
            "fpr": 0.11964873765093303,
            "logloss": 3.751574157164921,
            "mae": 0.26270966136059226,
            "precision": 0.7655913978494624,
            "recall": 0.7672413793103449
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.791868369384535,
            "auditor_fn_violation": 0.013010204081632651,
            "auditor_fp_violation": 0.0057318949031346204,
            "ave_precision_score": 0.7777862934442256,
            "fpr": 0.15021929824561403,
            "logloss": 1.6265392940553132,
            "mae": 0.28563707735116983,
            "precision": 0.7390476190476191,
            "recall": 0.7918367346938775
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8061458844807097,
            "auditor_fn_violation": 0.015216321586736819,
            "auditor_fp_violation": 0.02427698254247735,
            "ave_precision_score": 0.7918957415992961,
            "fpr": 0.12952799121844127,
            "logloss": 1.4701286538960103,
            "mae": 0.25773523952310423,
            "precision": 0.757201646090535,
            "recall": 0.7931034482758621
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7914070299441601,
            "auditor_fn_violation": 0.016210168277837448,
            "auditor_fp_violation": 0.010580360854743497,
            "ave_precision_score": 0.7779278657165767,
            "fpr": 0.15679824561403508,
            "logloss": 1.5426131717007745,
            "mae": 0.2859517767641313,
            "precision": 0.7337057728119181,
            "recall": 0.8040816326530612
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8088511372591947,
            "auditor_fn_violation": 0.016086907150157086,
            "auditor_fp_violation": 0.02635941033895933,
            "ave_precision_score": 0.7981470361398235,
            "fpr": 0.13172338090010977,
            "logloss": 1.3146202698933365,
            "mae": 0.25855751122964515,
            "precision": 0.7540983606557377,
            "recall": 0.7931034482758621
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 9540,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8298967347599866,
            "auditor_fn_violation": 0.007480755460078772,
            "auditor_fp_violation": 0.007537727612871045,
            "ave_precision_score": 0.8302066392136744,
            "fpr": 0.07785087719298246,
            "logloss": 0.6719588833777991,
            "mae": 0.3205934973595005,
            "precision": 0.821608040201005,
            "recall": 0.6673469387755102
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8428552830166061,
            "auditor_fn_violation": 0.019881524660282375,
            "auditor_fp_violation": 0.011156214008747178,
            "ave_precision_score": 0.8430637138503039,
            "fpr": 0.07464324917672886,
            "logloss": 0.5779391589252548,
            "mae": 0.3091968503148735,
            "precision": 0.8121546961325967,
            "recall": 0.6336206896551724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.8208748059531374,
            "auditor_fn_violation": 0.014218582169709993,
            "auditor_fp_violation": 0.01510403675064439,
            "ave_precision_score": 0.8212333544228221,
            "fpr": 0.17434210526315788,
            "logloss": 0.6708630649849361,
            "mae": 0.33837102199527364,
            "precision": 0.701688555347092,
            "recall": 0.763265306122449
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8494988383188204,
            "auditor_fn_violation": 0.018743612551572728,
            "auditor_fp_violation": 0.018717293236775488,
            "ave_precision_score": 0.8497033750913141,
            "fpr": 0.14709110867178923,
            "logloss": 0.5478592242695576,
            "mae": 0.31287825656733265,
            "precision": 0.7314629258517034,
            "recall": 0.7866379310344828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8388095311437083,
            "auditor_fn_violation": 0.01651450053705694,
            "auditor_fp_violation": 0.004910825642304812,
            "ave_precision_score": 0.839057584043936,
            "fpr": 0.09429824561403509,
            "logloss": 0.7826549071514038,
            "mae": 0.29928694725792115,
            "precision": 0.7932692307692307,
            "recall": 0.673469387755102
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8545992623860347,
            "auditor_fn_violation": 0.023524735985465013,
            "auditor_fp_violation": 0.010608594434908173,
            "ave_precision_score": 0.8549229145919783,
            "fpr": 0.07903402854006586,
            "logloss": 0.6440664592247756,
            "mae": 0.27064847561506267,
            "precision": 0.8144329896907216,
            "recall": 0.6810344827586207
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.7918087978668727,
            "auditor_fn_violation": 0.01629072681704261,
            "auditor_fp_violation": 0.009863224411740253,
            "ave_precision_score": 0.7783659157946714,
            "fpr": 0.1600877192982456,
            "logloss": 1.5469389550304422,
            "mae": 0.2879957246084177,
            "precision": 0.7355072463768116,
            "recall": 0.8285714285714286
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8096937507810069,
            "auditor_fn_violation": 0.01642520534463833,
            "auditor_fp_violation": 0.020937239850006267,
            "ave_precision_score": 0.7989477962989571,
            "fpr": 0.14270032930845225,
            "logloss": 1.3164697082154997,
            "mae": 0.2625294078928433,
            "precision": 0.7485493230174082,
            "recall": 0.834051724137931
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.7905345407431079,
            "auditor_fn_violation": 0.009353741496598641,
            "auditor_fp_violation": 0.012305645630664348,
            "ave_precision_score": 0.771275811057994,
            "fpr": 0.28618421052631576,
            "logloss": 2.0897644791476018,
            "mae": 0.3340947565379958,
            "precision": 0.6359832635983264,
            "recall": 0.9306122448979591
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.8129122402604945,
            "auditor_fn_violation": 0.007636549453045156,
            "auditor_fp_violation": 0.019147039539115524,
            "ave_precision_score": 0.797158626474328,
            "fpr": 0.278814489571899,
            "logloss": 1.8126693988893792,
            "mae": 0.31977406030785754,
            "precision": 0.6318840579710145,
            "recall": 0.9396551724137931
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.7859455504594595,
            "auditor_fn_violation": 0.016756176154672398,
            "auditor_fp_violation": 0.015751018541614703,
            "ave_precision_score": 0.7715603807960705,
            "fpr": 0.1611842105263158,
            "logloss": 1.6542094117347432,
            "mae": 0.2932769070561784,
            "precision": 0.7297794117647058,
            "recall": 0.810204081632653
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.811268732526166,
            "auditor_fn_violation": 0.018641886521064386,
            "auditor_fp_violation": 0.026381511577365395,
            "ave_precision_score": 0.8000990713650574,
            "fpr": 0.132821075740944,
            "logloss": 1.3748177476305121,
            "mae": 0.2541023266049428,
            "precision": 0.7555555555555555,
            "recall": 0.8060344827586207
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8266804653493106,
            "auditor_fn_violation": 0.006959362692445402,
            "auditor_fp_violation": 0.005116092957512264,
            "ave_precision_score": 0.8270868873515502,
            "fpr": 0.09539473684210527,
            "logloss": 0.6479952340800174,
            "mae": 0.3210619423844721,
            "precision": 0.7986111111111112,
            "recall": 0.7040816326530612
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8489419064196209,
            "auditor_fn_violation": 0.016709091941405806,
            "auditor_fp_violation": 0.012590338811984769,
            "ave_precision_score": 0.8491390665204612,
            "fpr": 0.08562019758507135,
            "logloss": 0.5484517504481459,
            "mae": 0.3067748124985436,
            "precision": 0.8025316455696202,
            "recall": 0.6831896551724138
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7803140373186611,
            "auditor_fn_violation": 0.006757966344432516,
            "auditor_fp_violation": 0.007280493888750319,
            "ave_precision_score": 0.7733979478830177,
            "fpr": 0.13596491228070176,
            "logloss": 1.307008733040863,
            "mae": 0.2967801280871295,
            "precision": 0.743801652892562,
            "recall": 0.7346938775510204
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8004980105720163,
            "auditor_fn_violation": 0.010939096862106817,
            "auditor_fp_violation": 0.02322103448529899,
            "ave_precision_score": 0.7957538768477257,
            "fpr": 0.11525795828759605,
            "logloss": 1.0867305769614801,
            "mae": 0.2663744061861831,
            "precision": 0.7702407002188184,
            "recall": 0.7586206896551724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7681007683001921,
            "auditor_fn_violation": 0.01935642678123881,
            "auditor_fp_violation": 0.0006521784318616435,
            "ave_precision_score": 0.769593417699552,
            "fpr": 0.19188596491228072,
            "logloss": 1.0119972625156244,
            "mae": 0.3146696266605521,
            "precision": 0.6956521739130435,
            "recall": 0.8163265306122449
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8291106737443522,
            "auditor_fn_violation": 0.014090238086225825,
            "auditor_fp_violation": 0.0226660478319913,
            "ave_precision_score": 0.8294874831356908,
            "fpr": 0.1668496158068057,
            "logloss": 0.7789117007695294,
            "mae": 0.276248069804285,
            "precision": 0.7153558052434457,
            "recall": 0.8232758620689655
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7900503833374475,
            "auditor_fn_violation": 0.014621374865735774,
            "auditor_fp_violation": 0.00934356032260747,
            "ave_precision_score": 0.7738137942707024,
            "fpr": 0.14912280701754385,
            "logloss": 1.7365475500600582,
            "mae": 0.2835002808635039,
            "precision": 0.739961759082218,
            "recall": 0.789795918367347
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.809360731652069,
            "auditor_fn_violation": 0.01651983421022749,
            "auditor_fp_violation": 0.02704946011585961,
            "ave_precision_score": 0.7941453225192263,
            "fpr": 0.12733260153677278,
            "logloss": 1.5311774499368802,
            "mae": 0.24908246039234871,
            "precision": 0.7627811860940695,
            "recall": 0.8038793103448276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8255351360375592,
            "auditor_fn_violation": 0.008552631578947372,
            "auditor_fp_violation": 0.00858225243202794,
            "ave_precision_score": 0.8259400275029185,
            "fpr": 0.09100877192982457,
            "logloss": 0.6720980829841482,
            "mae": 0.31908229637487345,
            "precision": 0.801909307875895,
            "recall": 0.6857142857142857
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8427382100950565,
            "auditor_fn_violation": 0.018187667966236422,
            "auditor_fp_violation": 0.008253584698084803,
            "ave_precision_score": 0.8429790727993796,
            "fpr": 0.0845225027442371,
            "logloss": 0.5678954660153689,
            "mae": 0.30713781001517537,
            "precision": 0.7957559681697612,
            "recall": 0.646551724137931
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 9540,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.7906329141271788,
            "auditor_fn_violation": 0.012866988900823493,
            "auditor_fp_violation": 0.00689074582190073,
            "ave_precision_score": 0.7799569111142558,
            "fpr": 0.16228070175438597,
            "logloss": 1.3944695051939864,
            "mae": 0.2891537165656031,
            "precision": 0.7347670250896058,
            "recall": 0.8367346938775511
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8121996697591507,
            "auditor_fn_violation": 0.014587039630568911,
            "auditor_fp_violation": 0.021364530459190068,
            "ave_precision_score": 0.8050043751217184,
            "fpr": 0.14818880351262348,
            "logloss": 1.1378848744677588,
            "mae": 0.2676172069432488,
            "precision": 0.7403846153846154,
            "recall": 0.8297413793103449
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.7627228939631296,
            "auditor_fn_violation": 0.011860007160759042,
            "auditor_fp_violation": 0.0028945289764696226,
            "ave_precision_score": 0.7532605136629554,
            "fpr": 0.17324561403508773,
            "logloss": 1.3704198338423168,
            "mae": 0.33021123594453294,
            "precision": 0.7095588235294118,
            "recall": 0.7877551020408163
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7745819442103948,
            "auditor_fn_violation": 0.01249101025776903,
            "auditor_fp_violation": 0.019969696746452142,
            "ave_precision_score": 0.767556078418379,
            "fpr": 0.1668496158068057,
            "logloss": 1.1919901132897794,
            "mae": 0.3254773376272372,
            "precision": 0.7099236641221374,
            "recall": 0.8017241379310345
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7850710473724912,
            "auditor_fn_violation": 0.01648317221625492,
            "auditor_fp_violation": 0.017143718300490565,
            "ave_precision_score": 0.7758883605338236,
            "fpr": 0.15350877192982457,
            "logloss": 1.3640416740185013,
            "mae": 0.2941636060997275,
            "precision": 0.7378277153558053,
            "recall": 0.8040816326530612
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8135181929473736,
            "auditor_fn_violation": 0.017210624929028355,
            "auditor_fp_violation": 0.02624153706746035,
            "ave_precision_score": 0.8063173218828057,
            "fpr": 0.13062568605927552,
            "logloss": 1.1142520794919577,
            "mae": 0.25896861030481183,
            "precision": 0.7591093117408907,
            "recall": 0.8081896551724138
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7918326436840015,
            "auditor_fn_violation": 0.014035087719298244,
            "auditor_fp_violation": 0.00930978215681384,
            "ave_precision_score": 0.7779968770918938,
            "fpr": 0.15460526315789475,
            "logloss": 1.6022914923499827,
            "mae": 0.2851335628267151,
            "precision": 0.7354596622889306,
            "recall": 0.8
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8093106514552909,
            "auditor_fn_violation": 0.01699534425981302,
            "auditor_fp_violation": 0.024986677864627466,
            "ave_precision_score": 0.7981246289126706,
            "fpr": 0.13062568605927552,
            "logloss": 1.3647226324330894,
            "mae": 0.2567715375432694,
            "precision": 0.7566462167689162,
            "recall": 0.7974137931034483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.7916719405010164,
            "auditor_fn_violation": 0.016704708199069104,
            "auditor_fp_violation": 0.01174960505529226,
            "ave_precision_score": 0.7780556485585096,
            "fpr": 0.16885964912280702,
            "logloss": 1.5836758942563123,
            "mae": 0.2849402435144187,
            "precision": 0.7264653641207816,
            "recall": 0.8346938775510204
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8124357202477275,
            "auditor_fn_violation": 0.01536063060676029,
            "auditor_fp_violation": 0.029166267616528782,
            "ave_precision_score": 0.8012643393950473,
            "fpr": 0.14050493962678376,
            "logloss": 1.3446434086887653,
            "mae": 0.2548819741000235,
            "precision": 0.7514563106796116,
            "recall": 0.834051724137931
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7879179540604802,
            "auditor_fn_violation": 0.02430630146795561,
            "auditor_fp_violation": 0.021740147168870037,
            "ave_precision_score": 0.7746120180075016,
            "fpr": 0.20065789473684212,
            "logloss": 1.6200571017877887,
            "mae": 0.30452927856200535,
            "precision": 0.6955074875207987,
            "recall": 0.8530612244897959
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8112898174495268,
            "auditor_fn_violation": 0.018826412808963244,
            "auditor_fp_violation": 0.031624416465913754,
            "ave_precision_score": 0.8004790179170113,
            "fpr": 0.1756311745334797,
            "logloss": 1.3818656318783407,
            "mae": 0.27119775048269706,
            "precision": 0.7153024911032029,
            "recall": 0.8663793103448276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7901522579988661,
            "auditor_fn_violation": 0.013972431077694237,
            "auditor_fp_violation": 0.012009437099858663,
            "ave_precision_score": 0.7739560559721397,
            "fpr": 0.15789473684210525,
            "logloss": 1.7246544476239478,
            "mae": 0.28295671003507644,
            "precision": 0.7362637362637363,
            "recall": 0.8204081632653061
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8096349295997279,
            "auditor_fn_violation": 0.01544579658579053,
            "auditor_fp_violation": 0.0258780944803385,
            "ave_precision_score": 0.7947047799181418,
            "fpr": 0.13062568605927552,
            "logloss": 1.5175989115162525,
            "mae": 0.24788128092695022,
            "precision": 0.7629482071713147,
            "recall": 0.8254310344827587
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.7970915791153514,
            "auditor_fn_violation": 0.008069280343716439,
            "auditor_fp_violation": 0.00474193481333667,
            "ave_precision_score": 0.791096472509647,
            "fpr": 0.12609649122807018,
            "logloss": 1.1167872869341768,
            "mae": 0.27996122270537616,
            "precision": 0.7704590818363274,
            "recall": 0.7877551020408163
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8157207572155782,
            "auditor_fn_violation": 0.014781028805026684,
            "auditor_fp_violation": 0.022265769847526016,
            "ave_precision_score": 0.811196358232102,
            "fpr": 0.12403951701427003,
            "logloss": 0.9537269328398807,
            "mae": 0.26192325588245313,
            "precision": 0.7610993657505285,
            "recall": 0.7758620689655172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.7916675911107474,
            "auditor_fn_violation": 0.010047440028643039,
            "auditor_fp_violation": 0.009533237715140941,
            "ave_precision_score": 0.7778299381336249,
            "fpr": 0.14583333333333334,
            "logloss": 1.6094986177596982,
            "mae": 0.2846669843502074,
            "precision": 0.744721689059501,
            "recall": 0.7918367346938775
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8073102249878465,
            "auditor_fn_violation": 0.013725916953707557,
            "auditor_fp_violation": 0.0244857164607568,
            "ave_precision_score": 0.794703922590642,
            "fpr": 0.12733260153677278,
            "logloss": 1.4093262516066711,
            "mae": 0.2563228828101339,
            "precision": 0.7618069815195072,
            "recall": 0.7995689655172413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.7917348785978362,
            "auditor_fn_violation": 0.013345864661654138,
            "auditor_fp_violation": 0.01589912280701754,
            "ave_precision_score": 0.7771778026719831,
            "fpr": 0.17434210526315788,
            "logloss": 1.6250267593130343,
            "mae": 0.28828583418277687,
            "precision": 0.7225130890052356,
            "recall": 0.8448979591836735
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.810964391231179,
            "auditor_fn_violation": 0.0181119648737651,
            "auditor_fp_violation": 0.02857198987272143,
            "ave_precision_score": 0.7989730492225288,
            "fpr": 0.145993413830955,
            "logloss": 1.399523342857055,
            "mae": 0.25856845387837485,
            "precision": 0.7485822306238186,
            "recall": 0.853448275862069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7896188747754025,
            "auditor_fn_violation": 0.014621374865735774,
            "auditor_fp_violation": 0.00934356032260747,
            "ave_precision_score": 0.7735276703677276,
            "fpr": 0.14912280701754385,
            "logloss": 1.723443000183276,
            "mae": 0.2835392580089164,
            "precision": 0.739961759082218,
            "recall": 0.789795918367347
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8083147814152389,
            "auditor_fn_violation": 0.01651983421022749,
            "auditor_fp_violation": 0.02704946011585961,
            "ave_precision_score": 0.7933005041661997,
            "fpr": 0.12733260153677278,
            "logloss": 1.518044333078367,
            "mae": 0.2490215550629324,
            "precision": 0.7627811860940695,
            "recall": 0.8038793103448276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.7919713359158871,
            "auditor_fn_violation": 0.010047440028643039,
            "auditor_fp_violation": 0.009533237715140941,
            "ave_precision_score": 0.7781236054470791,
            "fpr": 0.14583333333333334,
            "logloss": 1.6102054909947527,
            "mae": 0.2846570647121462,
            "precision": 0.744721689059501,
            "recall": 0.7918367346938775
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8075517310126554,
            "auditor_fn_violation": 0.013725916953707557,
            "auditor_fp_violation": 0.0244857164607568,
            "ave_precision_score": 0.7948757684886169,
            "fpr": 0.12733260153677278,
            "logloss": 1.4099724122889983,
            "mae": 0.2563367970214748,
            "precision": 0.7618069815195072,
            "recall": 0.7995689655172413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.782361854333982,
            "auditor_fn_violation": 0.01663310060866452,
            "auditor_fp_violation": 0.01768936559407999,
            "ave_precision_score": 0.7677436997000526,
            "fpr": 0.14912280701754385,
            "logloss": 1.6833346083666716,
            "mae": 0.29744574551411956,
            "precision": 0.7328094302554028,
            "recall": 0.7612244897959184
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8020418566848841,
            "auditor_fn_violation": 0.01690308111586359,
            "auditor_fp_violation": 0.02592475265030684,
            "ave_precision_score": 0.7892460537872225,
            "fpr": 0.12294182217343579,
            "logloss": 1.470412231270241,
            "mae": 0.2591257078245005,
            "precision": 0.7661795407098121,
            "recall": 0.790948275862069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.7885633415354656,
            "auditor_fn_violation": 0.012793143573218762,
            "auditor_fp_violation": 0.010824602976635905,
            "ave_precision_score": 0.7724639640349316,
            "fpr": 0.15679824561403508,
            "logloss": 1.7221390677369914,
            "mae": 0.2872450000987777,
            "precision": 0.7332089552238806,
            "recall": 0.8020408163265306
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8092050936538546,
            "auditor_fn_violation": 0.017345471062492904,
            "auditor_fp_violation": 0.02528381673653114,
            "ave_precision_score": 0.794748123563323,
            "fpr": 0.13172338090010977,
            "logloss": 1.4797945542556263,
            "mae": 0.25576379065017435,
            "precision": 0.7575757575757576,
            "recall": 0.8081896551724138
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 9540,
        "test": {
            "accuracy": 0.5932017543859649,
            "auc_prc": 0.7688461654648415,
            "auditor_fn_violation": 0.020179914070891515,
            "auditor_fp_violation": 0.03380414899808764,
            "ave_precision_score": 0.7275283660354541,
            "fpr": 0.37719298245614036,
            "logloss": 4.314006315497995,
            "mae": 0.39361344904190315,
            "precision": 0.573729863692689,
            "recall": 0.9448979591836735
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.784616929571006,
            "auditor_fn_violation": 0.016205193232143533,
            "auditor_fp_violation": 0.04184501138213778,
            "ave_precision_score": 0.747758176284128,
            "fpr": 0.38199780461031835,
            "logloss": 3.8954504676252624,
            "mae": 0.39573466797335116,
            "precision": 0.55893536121673,
            "recall": 0.9504310344827587
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.8116203344995772,
            "auditor_fn_violation": 0.016827783745076978,
            "auditor_fp_violation": 0.01693585266483745,
            "ave_precision_score": 0.8119796788532652,
            "fpr": 0.16885964912280702,
            "logloss": 0.7027811750632065,
            "mae": 0.3422720637937972,
            "precision": 0.7021276595744681,
            "recall": 0.7408163265306122
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8442380844074333,
            "auditor_fn_violation": 0.0191812710549226,
            "auditor_fp_violation": 0.021858124783592045,
            "ave_precision_score": 0.8444568065869553,
            "fpr": 0.14818880351262348,
            "logloss": 0.568735094507136,
            "mae": 0.3118360334378619,
            "precision": 0.725050916496945,
            "recall": 0.7672413793103449
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.818870465990178,
            "auditor_fn_violation": 0.013404045828857859,
            "auditor_fp_violation": 0.017736135362101946,
            "ave_precision_score": 0.8192179981449069,
            "fpr": 0.16447368421052633,
            "logloss": 0.6743541301417867,
            "mae": 0.33759965426891136,
            "precision": 0.7115384615384616,
            "recall": 0.7551020408163265
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8522049216215826,
            "auditor_fn_violation": 0.01945569476513116,
            "auditor_fp_violation": 0.01804197761881257,
            "ave_precision_score": 0.8524062754067941,
            "fpr": 0.1350164654226125,
            "logloss": 0.5412134848695527,
            "mae": 0.308083128730965,
            "precision": 0.7494908350305499,
            "recall": 0.7931034482758621
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.7913509254973408,
            "auditor_fn_violation": 0.013113139992839244,
            "auditor_fp_violation": 0.00993597738421885,
            "ave_precision_score": 0.7759199596670128,
            "fpr": 0.14802631578947367,
            "logloss": 1.6650582984665239,
            "mae": 0.2825009748273732,
            "precision": 0.7452830188679245,
            "recall": 0.8061224489795918
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8062503642638258,
            "auditor_fn_violation": 0.014781028805026686,
            "auditor_fp_violation": 0.028571989872721432,
            "ave_precision_score": 0.7914513405949617,
            "fpr": 0.13611416026344675,
            "logloss": 1.502267423283803,
            "mae": 0.25265669109025846,
            "precision": 0.752,
            "recall": 0.8103448275862069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.8132230989887957,
            "auditor_fn_violation": 0.01582080200501254,
            "auditor_fp_violation": 0.017034588841772686,
            "ave_precision_score": 0.8135465026254483,
            "fpr": 0.1600877192982456,
            "logloss": 0.694202324798901,
            "mae": 0.34061558839327427,
            "precision": 0.7137254901960784,
            "recall": 0.7428571428571429
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.849808854403259,
            "auditor_fn_violation": 0.021163745789015483,
            "auditor_fp_violation": 0.016823953813323123,
            "ave_precision_score": 0.850009382666236,
            "fpr": 0.13721185510428102,
            "logloss": 0.5507040841386294,
            "mae": 0.3107751716622719,
            "precision": 0.7412008281573499,
            "recall": 0.771551724137931
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7932621296418665,
            "auditor_fn_violation": 0.007630683852488363,
            "auditor_fp_violation": 0.010086679970067352,
            "ave_precision_score": 0.7806207541590229,
            "fpr": 0.14912280701754385,
            "logloss": 1.4588917249994569,
            "mae": 0.2871143873317602,
            "precision": 0.7414448669201521,
            "recall": 0.7959183673469388
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8049322821393063,
            "auditor_fn_violation": 0.013534293500889517,
            "auditor_fp_violation": 0.02559814546052842,
            "ave_precision_score": 0.7940113748282833,
            "fpr": 0.13611416026344675,
            "logloss": 1.301824030997683,
            "mae": 0.26853530001024073,
            "precision": 0.7544554455445545,
            "recall": 0.8211206896551724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 9540,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.8133389695606593,
            "auditor_fn_violation": 0.01766022198353026,
            "auditor_fp_violation": 0.017512679803774838,
            "ave_precision_score": 0.8136521471163152,
            "fpr": 0.15350877192982457,
            "logloss": 0.6941846957281745,
            "mae": 0.3402950957136825,
            "precision": 0.7211155378486056,
            "recall": 0.7387755102040816
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8497711323081627,
            "auditor_fn_violation": 0.0184715545630039,
            "auditor_fp_violation": 0.014623652745342167,
            "ave_precision_score": 0.8499691172029982,
            "fpr": 0.1251372118551043,
            "logloss": 0.5514153950299265,
            "mae": 0.3103733558429587,
            "precision": 0.7553648068669528,
            "recall": 0.7586206896551724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7905639324662483,
            "auditor_fn_violation": 0.013524883637665595,
            "auditor_fp_violation": 0.006947908871705329,
            "ave_precision_score": 0.7752764283941629,
            "fpr": 0.14583333333333334,
            "logloss": 1.6704856984084082,
            "mae": 0.2841935205827653,
            "precision": 0.7442307692307693,
            "recall": 0.789795918367347
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8084761970571364,
            "auditor_fn_violation": 0.01570129452288126,
            "auditor_fp_violation": 0.023955286739011396,
            "ave_precision_score": 0.7949435119576602,
            "fpr": 0.12403951701427003,
            "logloss": 1.445986311289863,
            "mae": 0.2539458346547345,
            "precision": 0.7665289256198347,
            "recall": 0.7995689655172413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.807570259641273,
            "auditor_fn_violation": 0.012482098102398854,
            "auditor_fp_violation": 0.016140766608464294,
            "ave_precision_score": 0.8078929664671132,
            "fpr": 0.20942982456140352,
            "logloss": 0.7013101226418191,
            "mae": 0.35212916811979195,
            "precision": 0.6672473867595818,
            "recall": 0.7816326530612245
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8444531353788236,
            "auditor_fn_violation": 0.01641337673643969,
            "auditor_fp_violation": 0.017327370910350016,
            "ave_precision_score": 0.8446550069200227,
            "fpr": 0.18441273326015367,
            "logloss": 0.5694363071989004,
            "mae": 0.3252675518070641,
            "precision": 0.6945454545454546,
            "recall": 0.8232758620689655
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8216436328674295,
            "auditor_fn_violation": 0.032330827067669175,
            "auditor_fp_violation": 0.009873617693522909,
            "ave_precision_score": 0.8218515690554049,
            "fpr": 0.07894736842105263,
            "logloss": 1.457419765615655,
            "mae": 0.33043057318925206,
            "precision": 0.8153846153846154,
            "recall": 0.6489795918367347
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8417614169103773,
            "auditor_fn_violation": 0.031047730799803178,
            "auditor_fp_violation": 0.021067391587286387,
            "ave_precision_score": 0.8419292787896948,
            "fpr": 0.0801317233809001,
            "logloss": 1.1363489688773845,
            "mae": 0.3048092699229954,
            "precision": 0.8068783068783069,
            "recall": 0.6573275862068966
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.7909868913078226,
            "auditor_fn_violation": 0.013079573934837097,
            "auditor_fp_violation": 0.008436746487070766,
            "ave_precision_score": 0.7763593026194704,
            "fpr": 0.14583333333333334,
            "logloss": 1.643897315530671,
            "mae": 0.284982566297437,
            "precision": 0.7432432432432432,
            "recall": 0.7857142857142857
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8076515368405464,
            "auditor_fn_violation": 0.01570129452288126,
            "auditor_fp_violation": 0.024365387496101593,
            "ave_precision_score": 0.79407890913508,
            "fpr": 0.12623490669593854,
            "logloss": 1.4410074320475021,
            "mae": 0.255565518715257,
            "precision": 0.7633744855967078,
            "recall": 0.7995689655172413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8213950704068922,
            "auditor_fn_violation": 0.016823308270676702,
            "auditor_fp_violation": 0.004011806768105099,
            "ave_precision_score": 0.8217985458182426,
            "fpr": 0.1118421052631579,
            "logloss": 0.6846402102116257,
            "mae": 0.32379330585789556,
            "precision": 0.7723214285714286,
            "recall": 0.7061224489795919
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8303849840075426,
            "auditor_fn_violation": 0.016030129830803593,
            "auditor_fp_violation": 0.009437228799387064,
            "ave_precision_score": 0.8306599671706214,
            "fpr": 0.10867178924259056,
            "logloss": 0.5911647312863143,
            "mae": 0.314977021311018,
            "precision": 0.7579462102689487,
            "recall": 0.6681034482758621
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7911485896470322,
            "auditor_fn_violation": 0.01253132832080201,
            "auditor_fp_violation": 0.008059990022449493,
            "ave_precision_score": 0.7764828319581385,
            "fpr": 0.14692982456140352,
            "logloss": 1.6488230627536875,
            "mae": 0.2848728155957963,
            "precision": 0.7418111753371869,
            "recall": 0.7857142857142857
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8076836590361623,
            "auditor_fn_violation": 0.01578409478027178,
            "auditor_fp_violation": 0.024365387496101593,
            "ave_precision_score": 0.7941537008123822,
            "fpr": 0.12623490669593854,
            "logloss": 1.444592290804771,
            "mae": 0.2555667912549892,
            "precision": 0.7638603696098563,
            "recall": 0.8017241379310345
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7912338998541711,
            "auditor_fn_violation": 0.01253132832080201,
            "auditor_fp_violation": 0.008059990022449493,
            "ave_precision_score": 0.776612027632039,
            "fpr": 0.14692982456140352,
            "logloss": 1.6462675426628062,
            "mae": 0.28488663582402346,
            "precision": 0.7418111753371869,
            "recall": 0.7857142857142857
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8077692589358895,
            "auditor_fn_violation": 0.01578409478027178,
            "auditor_fp_violation": 0.024365387496101593,
            "ave_precision_score": 0.7942295164416713,
            "fpr": 0.12623490669593854,
            "logloss": 1.442255661287901,
            "mae": 0.25558113876395216,
            "precision": 0.7638603696098563,
            "recall": 0.8017241379310345
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7911270675428737,
            "auditor_fn_violation": 0.013113139992839242,
            "auditor_fp_violation": 0.006859565976552762,
            "ave_precision_score": 0.7772898288833386,
            "fpr": 0.14473684210526316,
            "logloss": 1.6137011102797691,
            "mae": 0.2852373910857432,
            "precision": 0.7436893203883496,
            "recall": 0.7816326530612245
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8085575676933693,
            "auditor_fn_violation": 0.015743877512396382,
            "auditor_fp_violation": 0.023918451341667965,
            "ave_precision_score": 0.7966453474247057,
            "fpr": 0.12403951701427003,
            "logloss": 1.390335857638645,
            "mae": 0.2556263326298324,
            "precision": 0.7655601659751037,
            "recall": 0.7952586206896551
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.7887720420974884,
            "auditor_fn_violation": 0.012793143573218762,
            "auditor_fp_violation": 0.010824602976635905,
            "ave_precision_score": 0.7727074351233505,
            "fpr": 0.15679824561403508,
            "logloss": 1.719938790532221,
            "mae": 0.28715943092323765,
            "precision": 0.7332089552238806,
            "recall": 0.8020408163265306
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.809339170342519,
            "auditor_fn_violation": 0.017345471062492904,
            "auditor_fp_violation": 0.02528381673653114,
            "ave_precision_score": 0.7948786673509537,
            "fpr": 0.13172338090010977,
            "logloss": 1.4776808417934704,
            "mae": 0.2556663132378159,
            "precision": 0.7575757575757576,
            "recall": 0.8081896551724138
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.790665129708372,
            "auditor_fn_violation": 0.01505325814536341,
            "auditor_fp_violation": 0.00689074582190073,
            "ave_precision_score": 0.7799850409446529,
            "fpr": 0.16228070175438597,
            "logloss": 1.3936657740086467,
            "mae": 0.28898677085972885,
            "precision": 0.7361853832442068,
            "recall": 0.8428571428571429
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8127613916305721,
            "auditor_fn_violation": 0.014587039630568911,
            "auditor_fp_violation": 0.021072302973598852,
            "ave_precision_score": 0.8055567684030276,
            "fpr": 0.14928649835345773,
            "logloss": 1.1371664365259595,
            "mae": 0.2674335891705846,
            "precision": 0.7389635316698656,
            "recall": 0.8297413793103449
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.793145886879494,
            "auditor_fn_violation": 0.015077873254564986,
            "auditor_fp_violation": 0.01139103683379064,
            "ave_precision_score": 0.7793830677481528,
            "fpr": 0.15789473684210525,
            "logloss": 1.585135826498936,
            "mae": 0.2835419088001365,
            "precision": 0.7372262773722628,
            "recall": 0.8244897959183674
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8127690622909889,
            "auditor_fn_violation": 0.015845603542904735,
            "auditor_fp_violation": 0.028405002738097873,
            "ave_precision_score": 0.8015750804852424,
            "fpr": 0.13721185510428102,
            "logloss": 1.3483381216873198,
            "mae": 0.25305611618464335,
            "precision": 0.7572815533980582,
            "recall": 0.8405172413793104
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 9540,
        "test": {
            "accuracy": 0.5493421052631579,
            "auc_prc": 0.6008968440990967,
            "auditor_fn_violation": 0.011246867167919793,
            "auditor_fp_violation": 0.008070383304232147,
            "ave_precision_score": 0.6073898699267507,
            "fpr": 0.0581140350877193,
            "logloss": 10.902158378234944,
            "mae": 0.4614064752812933,
            "precision": 0.7135135135135136,
            "recall": 0.2693877551020408
        },
        "train": {
            "accuracy": 0.6026344676180022,
            "auc_prc": 0.6179712363682788,
            "auditor_fn_violation": 0.015192664370339538,
            "auditor_fp_violation": 0.01127899866655862,
            "ave_precision_score": 0.6196420194168624,
            "fpr": 0.05598243688254665,
            "logloss": 9.783472607826702,
            "mae": 0.41280611069130385,
            "precision": 0.75,
            "recall": 0.3297413793103448
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8251401838556367,
            "auditor_fn_violation": 0.016823308270676695,
            "auditor_fp_violation": 0.01451421800947868,
            "ave_precision_score": 0.825493990605356,
            "fpr": 0.16776315789473684,
            "logloss": 0.6670136477822246,
            "mae": 0.3087591773739134,
            "precision": 0.7228260869565217,
            "recall": 0.8142857142857143
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8641298761192027,
            "auditor_fn_violation": 0.018026798894734856,
            "auditor_fp_violation": 0.019957418280670998,
            "ave_precision_score": 0.8643308064515123,
            "fpr": 0.141602634467618,
            "logloss": 0.5290659491591082,
            "mae": 0.27146697397423875,
            "precision": 0.7514450867052023,
            "recall": 0.8405172413793104
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.79205842116737,
            "auditor_fn_violation": 0.014035087719298248,
            "auditor_fp_violation": 0.013126714891494144,
            "ave_precision_score": 0.7785948066815584,
            "fpr": 0.15350877192982457,
            "logloss": 1.5389012123546053,
            "mae": 0.2847258193766435,
            "precision": 0.7368421052631579,
            "recall": 0.8
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8095188661624177,
            "auditor_fn_violation": 0.016086907150157086,
            "auditor_fp_violation": 0.026968422241704062,
            "ave_precision_score": 0.7987574475844954,
            "fpr": 0.13062568605927552,
            "logloss": 1.312542080655902,
            "mae": 0.25730311648332366,
            "precision": 0.75564681724846,
            "recall": 0.7931034482758621
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7908750925967593,
            "auditor_fn_violation": 0.015008503401360548,
            "auditor_fp_violation": 0.00949426290845598,
            "ave_precision_score": 0.777295743414798,
            "fpr": 0.15899122807017543,
            "logloss": 1.578930967637693,
            "mae": 0.2846107505925175,
            "precision": 0.7354014598540146,
            "recall": 0.8224489795918367
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8114838451164985,
            "auditor_fn_violation": 0.014225084219690378,
            "auditor_fp_violation": 0.029419204011620344,
            "ave_precision_score": 0.8004310766649793,
            "fpr": 0.13721185510428102,
            "logloss": 1.3407019677880874,
            "mae": 0.2529049035869816,
            "precision": 0.7539370078740157,
            "recall": 0.8254310344827587
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7861536286347991,
            "auditor_fn_violation": 0.010483798782670962,
            "auditor_fp_violation": 0.006612725534214687,
            "ave_precision_score": 0.7667575195735092,
            "fpr": 0.1524122807017544,
            "logloss": 1.9225006397536843,
            "mae": 0.29061552021122544,
            "precision": 0.7306201550387597,
            "recall": 0.7693877551020408
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8052689571921323,
            "auditor_fn_violation": 0.01745429425792044,
            "auditor_fp_violation": 0.023552553061389876,
            "ave_precision_score": 0.7869825229607512,
            "fpr": 0.1251372118551043,
            "logloss": 1.7049344279589185,
            "mae": 0.2581273730001722,
            "precision": 0.7610062893081762,
            "recall": 0.7823275862068966
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.7884154323961137,
            "auditor_fn_violation": 0.014833959899749376,
            "auditor_fp_violation": 0.012290055707990362,
            "ave_precision_score": 0.7750380930035423,
            "fpr": 0.1611842105263158,
            "logloss": 1.576331743775916,
            "mae": 0.28887803489191166,
            "precision": 0.7307692307692307,
            "recall": 0.8142857142857143
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8097190658950572,
            "auditor_fn_violation": 0.015916575192096598,
            "auditor_fp_violation": 0.028061205696225854,
            "ave_precision_score": 0.7990286688544737,
            "fpr": 0.1350164654226125,
            "logloss": 1.3320693252959817,
            "mae": 0.2581225146643346,
            "precision": 0.754,
            "recall": 0.8125
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.7892785093666219,
            "auditor_fn_violation": 0.009814715359828145,
            "auditor_fp_violation": 0.011048058534963006,
            "ave_precision_score": 0.7569194149754237,
            "fpr": 0.14692982456140352,
            "logloss": 2.3190089163761978,
            "mae": 0.2909369023751392,
            "precision": 0.7413127413127413,
            "recall": 0.7836734693877551
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8019690928417507,
            "auditor_fn_violation": 0.014561016692531896,
            "auditor_fp_violation": 0.03008469685695833,
            "ave_precision_score": 0.769001264801108,
            "fpr": 0.1437980241492865,
            "logloss": 2.176313863889869,
            "mae": 0.26217955760725514,
            "precision": 0.7400793650793651,
            "recall": 0.8038793103448276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7916867062671911,
            "auditor_fn_violation": 0.016447368421052634,
            "auditor_fp_violation": 0.010206202710567892,
            "ave_precision_score": 0.7780317445393802,
            "fpr": 0.1513157894736842,
            "logloss": 1.5717058223668536,
            "mae": 0.28488966934821275,
            "precision": 0.7396226415094339,
            "recall": 0.8
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8099935830527184,
            "auditor_fn_violation": 0.016699629054846895,
            "auditor_fp_violation": 0.02487126028628471,
            "ave_precision_score": 0.7988536838896686,
            "fpr": 0.12843029637760703,
            "logloss": 1.3340171460646755,
            "mae": 0.2554762593771522,
            "precision": 0.7621951219512195,
            "recall": 0.8081896551724138
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7911581600964532,
            "auditor_fn_violation": 0.009718492660221985,
            "auditor_fp_violation": 0.009260414068346224,
            "ave_precision_score": 0.7764168289333997,
            "fpr": 0.14912280701754385,
            "logloss": 1.650083360594115,
            "mae": 0.28456569103512763,
            "precision": 0.739961759082218,
            "recall": 0.789795918367347
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8077166148126778,
            "auditor_fn_violation": 0.015263636019531398,
            "auditor_fp_violation": 0.024365387496101593,
            "ave_precision_score": 0.7941800507672877,
            "fpr": 0.12623490669593854,
            "logloss": 1.4428919922876802,
            "mae": 0.2560759111406264,
            "precision": 0.7633744855967078,
            "recall": 0.7995689655172413
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.6349082262312798,
            "auditor_fn_violation": 0.0026397011046133865,
            "auditor_fp_violation": 0.009598159130219925,
            "ave_precision_score": 0.5475583466891432,
            "fpr": 0.33223684210526316,
            "logloss": 7.384308951596861,
            "mae": 0.4069756937096827,
            "precision": 0.5910931174089069,
            "recall": 0.9012345679012346
        },
        "train": {
            "accuracy": 0.6158068057080132,
            "auc_prc": 0.6428184591241404,
            "auditor_fn_violation": 0.005788698434143,
            "auditor_fp_violation": 0.015072861663193535,
            "ave_precision_score": 0.5574868413540159,
            "fpr": 0.3260153677277717,
            "logloss": 6.83206380182347,
            "mae": 0.3969656187367782,
            "precision": 0.5828651685393258,
            "recall": 0.8867521367521367
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5910087719298246,
            "auc_prc": 0.6225708620543202,
            "auditor_fn_violation": 0.010585878275936758,
            "auditor_fp_violation": 0.014377934272300474,
            "ave_precision_score": 0.5370423888526183,
            "fpr": 0.3333333333333333,
            "logloss": 7.444863886525071,
            "mae": 0.418543039377407,
            "precision": 0.5783633841886269,
            "recall": 0.8580246913580247
        },
        "train": {
            "accuracy": 0.6048298572996706,
            "auc_prc": 0.6290633014049934,
            "auditor_fn_violation": 0.008664283636841269,
            "auditor_fp_violation": 0.00352352610308422,
            "ave_precision_score": 0.5451342163503222,
            "fpr": 0.32821075740944017,
            "logloss": 6.892830076907835,
            "mae": 0.4127374284543847,
            "precision": 0.5764872521246459,
            "recall": 0.8696581196581197
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 12498,
        "test": {
            "accuracy": 0.7752192982456141,
            "auc_prc": 0.8757004648304842,
            "auditor_fn_violation": 0.02048137318605155,
            "auditor_fp_violation": 0.009811794745078662,
            "ave_precision_score": 0.8757803780344916,
            "fpr": 0.07017543859649122,
            "logloss": 1.0102480791094652,
            "mae": 0.23297764692739367,
            "precision": 0.843520782396088,
            "recall": 0.7098765432098766
        },
        "train": {
            "accuracy": 0.7892425905598244,
            "auc_prc": 0.8757793295776145,
            "auditor_fn_violation": 0.011483576796419828,
            "auditor_fp_violation": 0.000805306598806167,
            "ave_precision_score": 0.8759025819849691,
            "fpr": 0.07244785949506037,
            "logloss": 0.892042911158788,
            "mae": 0.2225855571325358,
            "precision": 0.8382352941176471,
            "recall": 0.7307692307692307
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5800438596491229,
            "auc_prc": 0.622648117560356,
            "auditor_fn_violation": 0.01049111977474551,
            "auditor_fp_violation": 0.017891339263652092,
            "ave_precision_score": 0.5386095615264652,
            "fpr": 0.3059210526315789,
            "logloss": 7.34544060864362,
            "mae": 0.42200848023563403,
            "precision": 0.5779122541603631,
            "recall": 0.7860082304526749
        },
        "train": {
            "accuracy": 0.6125137211855104,
            "auc_prc": 0.6354194580404591,
            "auditor_fn_violation": 0.01578991809507726,
            "auditor_fp_violation": 0.014431094250606461,
            "ave_precision_score": 0.5529070738302437,
            "fpr": 0.29857299670691545,
            "logloss": 6.7040242169557605,
            "mae": 0.4026693856019939,
            "precision": 0.5872534142640364,
            "recall": 0.8269230769230769
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.6527977702298094,
            "auditor_fn_violation": 0.009462313190383376,
            "auditor_fp_violation": 0.006267502676879998,
            "ave_precision_score": 0.6363322969305887,
            "fpr": 0.19188596491228072,
            "logloss": 2.4921690835460444,
            "mae": 0.3522867193509898,
            "precision": 0.6728971962616822,
            "recall": 0.7407407407407407
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.673742549894925,
            "auditor_fn_violation": 0.007712009907399594,
            "auditor_fp_violation": 0.0037217554197134075,
            "ave_precision_score": 0.6571121113842966,
            "fpr": 0.19758507135016465,
            "logloss": 2.0509864588215683,
            "mae": 0.3331320639260744,
            "precision": 0.6685082872928176,
            "recall": 0.7756410256410257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6107456140350878,
            "auc_prc": 0.6378005354356628,
            "auditor_fn_violation": 0.003970832430871419,
            "auditor_fp_violation": 0.006764269829503353,
            "ave_precision_score": 0.5478663300172113,
            "fpr": 0.34539473684210525,
            "logloss": 7.531603147755583,
            "mae": 0.4077009536017533,
            "precision": 0.5860709592641261,
            "recall": 0.9176954732510288
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6470634592117926,
            "auditor_fn_violation": 0.0024533948793004773,
            "auditor_fp_violation": 0.00912598216431724,
            "ave_precision_score": 0.5583265927360781,
            "fpr": 0.3424807903402854,
            "logloss": 7.001015292045213,
            "mae": 0.39678391703975957,
            "precision": 0.5800807537012113,
            "recall": 0.9209401709401709
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5997807017543859,
            "auc_prc": 0.6436944934585447,
            "auditor_fn_violation": 0.004688289654176597,
            "auditor_fp_violation": 0.006965035829009136,
            "ave_precision_score": 0.5439727473645835,
            "fpr": 0.3519736842105263,
            "logloss": 8.076558770710182,
            "mae": 0.4133945544640403,
            "precision": 0.5792922673656619,
            "recall": 0.9094650205761317
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.6488759494360118,
            "auditor_fn_violation": 0.008739339694334205,
            "auditor_fp_violation": 0.014349324657496922,
            "ave_precision_score": 0.5530883860375054,
            "fpr": 0.34357848518111966,
            "logloss": 7.447374651386329,
            "mae": 0.4018696354120422,
            "precision": 0.5747282608695652,
            "recall": 0.9038461538461539
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.6350641189264725,
            "auditor_fn_violation": 0.004552920366760522,
            "auditor_fp_violation": 0.00611564121571537,
            "ave_precision_score": 0.5469179667123354,
            "fpr": 0.3355263157894737,
            "logloss": 7.441858766780793,
            "mae": 0.40718133060931505,
            "precision": 0.5898123324396782,
            "recall": 0.9053497942386831
        },
        "train": {
            "accuracy": 0.6158068057080132,
            "auc_prc": 0.6432047706406319,
            "auditor_fn_violation": 0.0069051572893504875,
            "auditor_fp_violation": 0.007089175935952126,
            "ave_precision_score": 0.557359008612111,
            "fpr": 0.32930845225027444,
            "logloss": 6.871906731777736,
            "mae": 0.3980960472403595,
            "precision": 0.5821727019498607,
            "recall": 0.8931623931623932
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7653620306752705,
            "auditor_fn_violation": 0.01700238249945854,
            "auditor_fp_violation": 0.012347108969607118,
            "ave_precision_score": 0.7661276035013704,
            "fpr": 0.09758771929824561,
            "logloss": 0.9572343631897682,
            "mae": 0.32443681716515066,
            "precision": 0.770618556701031,
            "recall": 0.6152263374485597
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.7942306150298207,
            "auditor_fn_violation": 0.005446255171831468,
            "auditor_fp_violation": 0.005421571809808881,
            "ave_precision_score": 0.7948879597339888,
            "fpr": 0.09330406147091108,
            "logloss": 0.7979490329919181,
            "mae": 0.3011390685515105,
            "precision": 0.7875,
            "recall": 0.6730769230769231
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5942982456140351,
            "auc_prc": 0.62677798809167,
            "auditor_fn_violation": 0.00987067720742185,
            "auditor_fp_violation": 0.021412466024215475,
            "ave_precision_score": 0.5430164873179828,
            "fpr": 0.3059210526315789,
            "logloss": 7.2825027757374725,
            "mae": 0.4118068105571773,
            "precision": 0.586053412462908,
            "recall": 0.8127572016460906
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6430199311869491,
            "auditor_fn_violation": 0.012271665400095699,
            "auditor_fp_violation": 0.008471825419440865,
            "ave_precision_score": 0.5612848481537291,
            "fpr": 0.29198682766191,
            "logloss": 6.569054308521159,
            "mae": 0.3936808313123992,
            "precision": 0.5913978494623656,
            "recall": 0.8226495726495726
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5921052631578947,
            "auc_prc": 0.6277807835683435,
            "auditor_fn_violation": 0.0038715616200996348,
            "auditor_fp_violation": 0.011078164895807607,
            "ave_precision_score": 0.5429642712707343,
            "fpr": 0.3201754385964912,
            "logloss": 7.32104887948702,
            "mae": 0.4128361092944192,
            "precision": 0.5816618911174785,
            "recall": 0.8353909465020576
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.6415289126526532,
            "auditor_fn_violation": 0.013868952123617327,
            "auditor_fp_violation": 0.01091747961335371,
            "ave_precision_score": 0.5575596700523742,
            "fpr": 0.305159165751921,
            "logloss": 6.713215019246497,
            "mae": 0.3976719833334906,
            "precision": 0.5893648449039882,
            "recall": 0.8525641025641025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 12498,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.628942039583561,
            "auditor_fn_violation": 0.009836834885567835,
            "auditor_fp_violation": 0.01441911704142987,
            "ave_precision_score": 0.54643473056647,
            "fpr": 0.30043859649122806,
            "logloss": 7.192384085790903,
            "mae": 0.4072505484925356,
            "precision": 0.5940740740740741,
            "recall": 0.8251028806584362
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.6408001364011787,
            "auditor_fn_violation": 0.008800322741047221,
            "auditor_fp_violation": 0.009839607704182402,
            "ave_precision_score": 0.5602826362525587,
            "fpr": 0.29308452250274425,
            "logloss": 6.514442994899326,
            "mae": 0.390397880511943,
            "precision": 0.5923664122137404,
            "recall": 0.8290598290598291
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5986842105263158,
            "auc_prc": 0.6237309627753425,
            "auditor_fn_violation": 0.0019560862031622267,
            "auditor_fp_violation": 0.010648319743019516,
            "ave_precision_score": 0.5371599077059868,
            "fpr": 0.36951754385964913,
            "logloss": 7.531162726832912,
            "mae": 0.41808665544343393,
            "precision": 0.575566750629723,
            "recall": 0.9403292181069959
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.6274093235344629,
            "auditor_fn_violation": 0.0060467036317749835,
            "auditor_fp_violation": 0.009227574689089727,
            "ave_precision_score": 0.5410747364884992,
            "fpr": 0.3754116355653128,
            "logloss": 7.1779906012713965,
            "mae": 0.4241268781619647,
            "precision": 0.5620998719590269,
            "recall": 0.938034188034188
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5899122807017544,
            "auc_prc": 0.6343971770804826,
            "auditor_fn_violation": 0.010166233484946935,
            "auditor_fp_violation": 0.009297010130961202,
            "ave_precision_score": 0.5424253093335415,
            "fpr": 0.35964912280701755,
            "logloss": 7.663040805748589,
            "mae": 0.41927555454354626,
            "precision": 0.5729166666666666,
            "recall": 0.9053497942386831
        },
        "train": {
            "accuracy": 0.5982436882546652,
            "auc_prc": 0.6369589257571844,
            "auditor_fn_violation": 0.011929222137784159,
            "auditor_fp_violation": 0.006184754678831341,
            "ave_precision_score": 0.5471253006760401,
            "fpr": 0.3545554335894621,
            "logloss": 7.190595968656246,
            "mae": 0.4126984253289325,
            "precision": 0.5681818181818182,
            "recall": 0.9081196581196581
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5942982456140351,
            "auc_prc": 0.6264049789124905,
            "auditor_fn_violation": 0.007127192982456138,
            "auditor_fp_violation": 0.020146095873486536,
            "ave_precision_score": 0.544643760213915,
            "fpr": 0.3081140350877193,
            "logloss": 7.111608798303216,
            "mae": 0.4121869826610731,
            "precision": 0.5855457227138643,
            "recall": 0.8168724279835391
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.6396346017572833,
            "auditor_fn_violation": 0.006468893955172776,
            "auditor_fp_violation": 0.01403215775089019,
            "ave_precision_score": 0.5600040861583311,
            "fpr": 0.2996706915477497,
            "logloss": 6.46583348616179,
            "mae": 0.39301474185626745,
            "precision": 0.5925373134328358,
            "recall": 0.8482905982905983
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5866228070175439,
            "auc_prc": 0.6230900642193011,
            "auditor_fn_violation": 0.0073686015450148,
            "auditor_fp_violation": 0.017914504571287376,
            "ave_precision_score": 0.5410535162304466,
            "fpr": 0.2993421052631579,
            "logloss": 7.16293309034458,
            "mae": 0.4212502982173056,
            "precision": 0.583206106870229,
            "recall": 0.7860082304526749
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.6380208597529287,
            "auditor_fn_violation": 0.010385881955585579,
            "auditor_fp_violation": 0.00905907976995488,
            "ave_precision_score": 0.5589036269865644,
            "fpr": 0.2864983534577388,
            "logloss": 6.428689299950252,
            "mae": 0.3979899256301555,
            "precision": 0.5972222222222222,
            "recall": 0.8269230769230769
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5932017543859649,
            "auc_prc": 0.6335798684337284,
            "auditor_fn_violation": 0.01198018193632229,
            "auditor_fp_violation": 0.012215838893007175,
            "ave_precision_score": 0.5456698464495443,
            "fpr": 0.3190789473684211,
            "logloss": 7.424621273439403,
            "mae": 0.4132028666050214,
            "precision": 0.5824964131994261,
            "recall": 0.8353909465020576
        },
        "train": {
            "accuracy": 0.6180021953896817,
            "auc_prc": 0.6424584778866107,
            "auditor_fn_violation": 0.01023107883700639,
            "auditor_fp_violation": 0.01562294801683959,
            "ave_precision_score": 0.5560144604314519,
            "fpr": 0.3062568605927552,
            "logloss": 6.868299956600831,
            "mae": 0.39662468111301996,
            "precision": 0.588495575221239,
            "recall": 0.8525641025641025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.6367996626791199,
            "auditor_fn_violation": 0.007925871778210962,
            "auditor_fp_violation": 0.008020344287949932,
            "ave_precision_score": 0.5473732201402982,
            "fpr": 0.34649122807017546,
            "logloss": 7.520231041576264,
            "mae": 0.4069072626951853,
            "precision": 0.5880052151238592,
            "recall": 0.9279835390946503
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.6434840605964717,
            "auditor_fn_violation": 0.003668364809967446,
            "auditor_fp_violation": 0.0023316723368510695,
            "ave_precision_score": 0.5568386677472545,
            "fpr": 0.3446761800219539,
            "logloss": 6.957390009306615,
            "mae": 0.40036755024085413,
            "precision": 0.5785234899328859,
            "recall": 0.9209401709401709
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 12498,
        "test": {
            "accuracy": 0.581140350877193,
            "auc_prc": 0.6344285652915373,
            "auditor_fn_violation": 0.005798317810988377,
            "auditor_fp_violation": 0.005050037064492243,
            "ave_precision_score": 0.5325476971928572,
            "fpr": 0.40350877192982454,
            "logloss": 8.5213642449601,
            "mae": 0.4268058141481066,
            "precision": 0.5619047619047619,
            "recall": 0.9711934156378601
        },
        "train": {
            "accuracy": 0.579582875960483,
            "auc_prc": 0.6457279491370538,
            "auditor_fn_violation": 0.00048786437370411086,
            "auditor_fp_violation": 0.015209144318376116,
            "ave_precision_score": 0.5491592532704059,
            "fpr": 0.4061470911086718,
            "logloss": 7.711111882073935,
            "mae": 0.4205655769846769,
            "precision": 0.5515151515151515,
            "recall": 0.9722222222222222
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6041666666666666,
            "auc_prc": 0.6392087046382979,
            "auditor_fn_violation": 0.006939932134863909,
            "auditor_fp_violation": 0.007837595749938225,
            "ave_precision_score": 0.5422963298305348,
            "fpr": 0.3541666666666667,
            "logloss": 7.979639927349834,
            "mae": 0.41024002970401563,
            "precision": 0.5810635538261998,
            "recall": 0.9218106995884774
        },
        "train": {
            "accuracy": 0.6037321624588364,
            "auc_prc": 0.6451503695949881,
            "auditor_fn_violation": 0.006257798793473875,
            "auditor_fp_violation": 0.007646695888971766,
            "ave_precision_score": 0.5510952642032123,
            "fpr": 0.3545554335894621,
            "logloss": 7.394524348729662,
            "mae": 0.40167973278413405,
            "precision": 0.5710491367861886,
            "recall": 0.9188034188034188
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5833333333333334,
            "auc_prc": 0.6199221760992756,
            "auditor_fn_violation": 0.015418561836690492,
            "auditor_fp_violation": 0.018534820031298912,
            "ave_precision_score": 0.5383635626531094,
            "fpr": 0.30153508771929827,
            "logloss": 7.157798861443604,
            "mae": 0.42630899249802734,
            "precision": 0.5807926829268293,
            "recall": 0.7839506172839507
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.6339992478431945,
            "auditor_fn_violation": 0.008500098511075463,
            "auditor_fp_violation": 0.012738711459884583,
            "ave_precision_score": 0.5551183827028191,
            "fpr": 0.28869374313940727,
            "logloss": 6.444971191481084,
            "mae": 0.4035385714991593,
            "precision": 0.5909797822706065,
            "recall": 0.811965811965812
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.6346039398957198,
            "auditor_fn_violation": 0.009139683055375065,
            "auditor_fp_violation": 0.00809241413392637,
            "ave_precision_score": 0.5469710581041969,
            "fpr": 0.3333333333333333,
            "logloss": 7.413898556247018,
            "mae": 0.40727121744860423,
            "precision": 0.5908479138627187,
            "recall": 0.9032921810699589
        },
        "train": {
            "accuracy": 0.6158068057080132,
            "auc_prc": 0.6431922985942713,
            "auditor_fn_violation": 0.0074164766810211405,
            "auditor_fp_violation": 0.007530236165452102,
            "ave_precision_score": 0.55731595610754,
            "fpr": 0.32821075740944017,
            "logloss": 6.863162513276205,
            "mae": 0.39783392773284837,
            "precision": 0.5824022346368715,
            "recall": 0.8910256410256411
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 12498,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8355323339103826,
            "auditor_fn_violation": 0.012657028373402648,
            "auditor_fp_violation": 0.01487212750185323,
            "ave_precision_score": 0.8359076917562736,
            "fpr": 0.07236842105263158,
            "logloss": 0.8800881003113389,
            "mae": 0.3003362540807677,
            "precision": 0.8206521739130435,
            "recall": 0.6213991769547325
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8520050041734233,
            "auditor_fn_violation": 0.006501730980325925,
            "auditor_fp_violation": 0.008865806186241401,
            "ave_precision_score": 0.8521548237761183,
            "fpr": 0.07683863885839737,
            "logloss": 0.7492935263462442,
            "mae": 0.2862283148587557,
            "precision": 0.8177083333333334,
            "recall": 0.6709401709401709
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5921052631578947,
            "auc_prc": 0.6266395853549133,
            "auditor_fn_violation": 0.007364089235434268,
            "auditor_fp_violation": 0.021435631331850766,
            "ave_precision_score": 0.5456476955811462,
            "fpr": 0.3026315789473684,
            "logloss": 7.033843232516742,
            "mae": 0.4139188390953789,
            "precision": 0.5855855855855856,
            "recall": 0.8024691358024691
        },
        "train": {
            "accuracy": 0.6158068057080132,
            "auc_prc": 0.6381798350420035,
            "auditor_fn_violation": 0.00891759783087994,
            "auditor_fp_violation": 0.017124535090305853,
            "ave_precision_score": 0.5598572690759048,
            "fpr": 0.29418221734357847,
            "logloss": 6.374141239735053,
            "mae": 0.3943067380061279,
            "precision": 0.5902140672782875,
            "recall": 0.8247863247863247
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5789473684210527,
            "auc_prc": 0.6229618078186134,
            "auditor_fn_violation": 0.010010558804418462,
            "auditor_fp_violation": 0.017914504571287376,
            "ave_precision_score": 0.5402073092543168,
            "fpr": 0.2993421052631579,
            "logloss": 7.26437194623454,
            "mae": 0.42258573819734074,
            "precision": 0.5787037037037037,
            "recall": 0.7716049382716049
        },
        "train": {
            "accuracy": 0.6114160263446762,
            "auc_prc": 0.6358928250155432,
            "auditor_fn_violation": 0.015768808578907372,
            "auditor_fp_violation": 0.014455872915185106,
            "ave_precision_score": 0.5544624277286248,
            "fpr": 0.29308452250274425,
            "logloss": 6.628416841104544,
            "mae": 0.40065767779123623,
            "precision": 0.5879629629629629,
            "recall": 0.8141025641025641
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.6499935485394176,
            "auditor_fn_violation": 0.0034022814237239193,
            "auditor_fp_violation": 0.008795095132196697,
            "ave_precision_score": 0.5458013927964755,
            "fpr": 0.41118421052631576,
            "logloss": 8.5054362824509,
            "mae": 0.42496691936251235,
            "precision": 0.5572609208972845,
            "recall": 0.9711934156378601
        },
        "train": {
            "accuracy": 0.5762897914379802,
            "auc_prc": 0.6528024628848555,
            "auditor_fn_violation": 0.0028873127116815374,
            "auditor_fp_violation": 0.007431121507137517,
            "ave_precision_score": 0.5531116596568623,
            "fpr": 0.411635565312843,
            "logloss": 7.922005428271217,
            "mae": 0.4241966975793553,
            "precision": 0.5492788461538461,
            "recall": 0.9764957264957265
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.6348023963180879,
            "auditor_fn_violation": 0.007621290881524804,
            "auditor_fp_violation": 0.011628984432913268,
            "ave_precision_score": 0.5474212261295648,
            "fpr": 0.3223684210526316,
            "logloss": 7.375446699290564,
            "mae": 0.4092620880115295,
            "precision": 0.5893854748603352,
            "recall": 0.8683127572016461
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6426352013708702,
            "auditor_fn_violation": 0.010076275718427206,
            "auditor_fp_violation": 0.011048806535620582,
            "ave_precision_score": 0.5573110449137155,
            "fpr": 0.31284302963776073,
            "logloss": 6.814928958931664,
            "mae": 0.3966503570665954,
            "precision": 0.5863570391872278,
            "recall": 0.8632478632478633
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.6128841881575906,
            "auditor_fn_violation": 0.0034270991264168655,
            "auditor_fp_violation": 0.009603306976361097,
            "ave_precision_score": 0.5226448426942607,
            "fpr": 0.40899122807017546,
            "logloss": 7.9703031318999855,
            "mae": 0.4394996112882437,
            "precision": 0.555952380952381,
            "recall": 0.9609053497942387
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.6130960705637584,
            "auditor_fn_violation": 0.003926370007599426,
            "auditor_fp_violation": 0.011073585200199226,
            "ave_precision_score": 0.5247007502570725,
            "fpr": 0.42041712403951703,
            "logloss": 7.534926786552404,
            "mae": 0.4488919740103177,
            "precision": 0.5379975874547648,
            "recall": 0.9529914529914529
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6052631578947368,
            "auc_prc": 0.6340297312983674,
            "auditor_fn_violation": 0.004467186484730343,
            "auditor_fp_violation": 0.009255827361831812,
            "ave_precision_score": 0.5448467991741084,
            "fpr": 0.34649122807017546,
            "logloss": 7.530493997487943,
            "mae": 0.4107487094568154,
            "precision": 0.58311345646438,
            "recall": 0.9094650205761317
        },
        "train": {
            "accuracy": 0.6048298572996706,
            "auc_prc": 0.6387903760285472,
            "auditor_fn_violation": 0.006473584958766081,
            "auditor_fp_violation": 0.011569158491772268,
            "ave_precision_score": 0.5520030871044893,
            "fpr": 0.34357848518111966,
            "logloss": 7.001088105180803,
            "mae": 0.40269660222053766,
            "precision": 0.5735694822888283,
            "recall": 0.8995726495726496
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.6228892366812472,
            "auditor_fn_violation": 0.007869467908454266,
            "auditor_fp_violation": 0.013924923811877129,
            "ave_precision_score": 0.5339318362159984,
            "fpr": 0.32127192982456143,
            "logloss": 7.60774427905609,
            "mae": 0.4325074414077231,
            "precision": 0.5716374269005848,
            "recall": 0.8045267489711934
        },
        "train": {
            "accuracy": 0.5960482985729967,
            "auc_prc": 0.6275538280643629,
            "auditor_fn_violation": 0.01594472121365645,
            "auditor_fp_violation": 0.014535164641836795,
            "ave_precision_score": 0.5395408180788528,
            "fpr": 0.3194291986827662,
            "logloss": 7.130397302378263,
            "mae": 0.418613262973551,
            "precision": 0.5733137829912024,
            "recall": 0.8354700854700855
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 12498,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8256956304266483,
            "auditor_fn_violation": 0.017015919428200132,
            "auditor_fp_violation": 0.01698789226587596,
            "ave_precision_score": 0.8259887649336284,
            "fpr": 0.08223684210526316,
            "logloss": 0.8479652350244283,
            "mae": 0.29541727093180126,
            "precision": 0.8134328358208955,
            "recall": 0.6728395061728395
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8285369122086432,
            "auditor_fn_violation": 0.011300627656280786,
            "auditor_fp_violation": 0.0046360881426656415,
            "ave_precision_score": 0.8297542026939317,
            "fpr": 0.09110867178924259,
            "logloss": 0.7670491992440895,
            "mae": 0.28329756281836493,
            "precision": 0.8009592326139089,
            "recall": 0.7136752136752137
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6107456140350878,
            "auc_prc": 0.6375414186875418,
            "auditor_fn_violation": 0.0009881957981373193,
            "auditor_fp_violation": 0.005822213985668396,
            "ave_precision_score": 0.5470944715597432,
            "fpr": 0.35526315789473684,
            "logloss": 7.598630452791035,
            "mae": 0.4107660987667674,
            "precision": 0.5840821566110398,
            "recall": 0.9362139917695473
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.6488096917084845,
            "auditor_fn_violation": 0.0030116243069042204,
            "auditor_fp_violation": 0.000631855946755629,
            "ave_precision_score": 0.5593971152107352,
            "fpr": 0.3578485181119649,
            "logloss": 7.055378997671638,
            "mae": 0.4016322208206557,
            "precision": 0.5721784776902887,
            "recall": 0.9316239316239316
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 12498,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.6422652558022699,
            "auditor_fn_violation": 0.01206140350877193,
            "auditor_fp_violation": 0.0053692035252450665,
            "ave_precision_score": 0.5492737340195466,
            "fpr": 0.3574561403508772,
            "logloss": 7.710838072157264,
            "mae": 0.4074339685511516,
            "precision": 0.5815147625160462,
            "recall": 0.9320987654320988
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.6503193250904398,
            "auditor_fn_violation": 0.008668974640434575,
            "auditor_fp_violation": 0.005161395831733056,
            "ave_precision_score": 0.5598416399895564,
            "fpr": 0.3589462129527991,
            "logloss": 7.135394748130585,
            "mae": 0.3985167270660212,
            "precision": 0.5719895287958116,
            "recall": 0.9337606837606838
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5942982456140351,
            "auc_prc": 0.6256831500234501,
            "auditor_fn_violation": 0.008099595697061583,
            "auditor_fp_violation": 0.017044518573428882,
            "ave_precision_score": 0.5441914701778097,
            "fpr": 0.3081140350877193,
            "logloss": 7.1167977278590335,
            "mae": 0.4116670125743105,
            "precision": 0.5855457227138643,
            "recall": 0.8168724279835391
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6376678725856908,
            "auditor_fn_violation": 0.00787150402957209,
            "auditor_fp_violation": 0.017550728121058654,
            "ave_precision_score": 0.5581809426484964,
            "fpr": 0.29857299670691545,
            "logloss": 6.486501905459294,
            "mae": 0.39315082640673366,
            "precision": 0.5897435897435898,
            "recall": 0.8354700854700855
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.6383682109800164,
            "auditor_fn_violation": 0.004530358818857844,
            "auditor_fp_violation": 0.006594390906844569,
            "ave_precision_score": 0.5479186474882747,
            "fpr": 0.35526315789473684,
            "logloss": 7.592454234432441,
            "mae": 0.4084053464395223,
            "precision": 0.5856777493606138,
            "recall": 0.9423868312757202
        },
        "train": {
            "accuracy": 0.610318331503842,
            "auc_prc": 0.6493032960904195,
            "auditor_fn_violation": 0.001960839502003059,
            "auditor_fp_violation": 0.0022350355449943316,
            "ave_precision_score": 0.5598839622328741,
            "fpr": 0.3589462129527991,
            "logloss": 7.051118516232823,
            "mae": 0.40037237023106653,
            "precision": 0.5736636245110821,
            "recall": 0.9401709401709402
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.635018672311527,
            "auditor_fn_violation": 0.00567197314273338,
            "auditor_fp_violation": 0.00843989374845566,
            "ave_precision_score": 0.547638769593292,
            "fpr": 0.3300438596491228,
            "logloss": 7.38226062622224,
            "mae": 0.4074256896927724,
            "precision": 0.5910326086956522,
            "recall": 0.8950617283950617
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.6429917263838225,
            "auditor_fn_violation": 0.007029468884573164,
            "auditor_fp_violation": 0.014768084088876119,
            "ave_precision_score": 0.5576738754844608,
            "fpr": 0.3238199780461032,
            "logloss": 6.827280863051187,
            "mae": 0.397000139982733,
            "precision": 0.5821529745042493,
            "recall": 0.8782051282051282
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.6246702800436986,
            "auditor_fn_violation": 0.009696953288571227,
            "auditor_fp_violation": 0.017914504571287376,
            "ave_precision_score": 0.540403636359043,
            "fpr": 0.2993421052631579,
            "logloss": 7.3505616551037924,
            "mae": 0.4224567327909059,
            "precision": 0.5760869565217391,
            "recall": 0.7633744855967078
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.6393409022851342,
            "auditor_fn_violation": 0.015217615656693594,
            "auditor_fp_violation": 0.016524891407502504,
            "ave_precision_score": 0.5566881138732944,
            "fpr": 0.2897914379802415,
            "logloss": 6.663695814039388,
            "mae": 0.4004837886322214,
            "precision": 0.5900621118012422,
            "recall": 0.811965811965812
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5910087719298246,
            "auc_prc": 0.6259423255344407,
            "auditor_fn_violation": 0.009198343079922032,
            "auditor_fp_violation": 0.008977843670208388,
            "ave_precision_score": 0.5346649603433228,
            "fpr": 0.34649122807017546,
            "logloss": 7.725044555836656,
            "mae": 0.42458035028794894,
            "precision": 0.5758389261744966,
            "recall": 0.8827160493827161
        },
        "train": {
            "accuracy": 0.5949506037321625,
            "auc_prc": 0.6282075469935481,
            "auditor_fn_violation": 0.008364059406869505,
            "auditor_fp_violation": 0.006041038424275167,
            "ave_precision_score": 0.5385931735051049,
            "fpr": 0.3446761800219539,
            "logloss": 7.3040365434540915,
            "mae": 0.4201229763527541,
            "precision": 0.5680880330123796,
            "recall": 0.8824786324786325
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5986842105263158,
            "auc_prc": 0.6244377359348882,
            "auditor_fn_violation": 0.002605858782759368,
            "auditor_fp_violation": 0.01813586195535788,
            "ave_precision_score": 0.5385601430178548,
            "fpr": 0.38048245614035087,
            "logloss": 7.551979264267602,
            "mae": 0.4138775021693028,
            "precision": 0.5737100737100738,
            "recall": 0.9609053497942387
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.6341377315222048,
            "auditor_fn_violation": 0.005774625423363075,
            "auditor_fp_violation": 0.012260483233516618,
            "ave_precision_score": 0.5501672443477348,
            "fpr": 0.38419319429198684,
            "logloss": 6.96259539597025,
            "mae": 0.41423868578904677,
            "precision": 0.5603015075376885,
            "recall": 0.9529914529914529
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6008771929824561,
            "auc_prc": 0.6333070639487569,
            "auditor_fn_violation": 0.008507959714100068,
            "auditor_fp_violation": 0.013513096120583176,
            "ave_precision_score": 0.545104612474175,
            "fpr": 0.32127192982456143,
            "logloss": 7.4690666188276165,
            "mae": 0.41064831243548133,
            "precision": 0.5861581920903954,
            "recall": 0.8539094650205762
        },
        "train": {
            "accuracy": 0.6180021953896817,
            "auc_prc": 0.6423831089938459,
            "auditor_fn_violation": 0.009457063244110449,
            "auditor_fp_violation": 0.010540843911758222,
            "ave_precision_score": 0.5559066683605449,
            "fpr": 0.3062568605927552,
            "logloss": 6.893611058590115,
            "mae": 0.39450652963411814,
            "precision": 0.588495575221239,
            "recall": 0.8525641025641025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5877192982456141,
            "auc_prc": 0.6594475891909687,
            "auditor_fn_violation": 0.00794617717132337,
            "auditor_fp_violation": 0.010964912280701757,
            "ave_precision_score": 0.5248322648547765,
            "fpr": 0.375,
            "logloss": 10.280498665347826,
            "mae": 0.43838652514555965,
            "precision": 0.5692695214105793,
            "recall": 0.9300411522633745
        },
        "train": {
            "accuracy": 0.5686059275521405,
            "auc_prc": 0.6551762414661075,
            "auditor_fn_violation": 0.004116355653128432,
            "auditor_fp_violation": 0.003756445550123519,
            "ave_precision_score": 0.5215274276388335,
            "fpr": 0.3885839736553238,
            "logloss": 9.882638153989417,
            "mae": 0.44577229176918254,
            "precision": 0.5478927203065134,
            "recall": 0.9166666666666666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6107456140350878,
            "auc_prc": 0.6381942776395609,
            "auditor_fn_violation": 0.009656342502346402,
            "auditor_fp_violation": 0.006741104521868054,
            "ave_precision_score": 0.548267135531999,
            "fpr": 0.3541666666666667,
            "logloss": 7.548923918838575,
            "mae": 0.407473802149884,
            "precision": 0.5842985842985843,
            "recall": 0.934156378600823
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.6480601953859484,
            "auditor_fn_violation": 0.0017919633726439447,
            "auditor_fp_violation": 0.0029090152215336756,
            "ave_precision_score": 0.5596400679509899,
            "fpr": 0.3534577387486279,
            "logloss": 7.0032318945645935,
            "mae": 0.39966435764806857,
            "precision": 0.5768725361366623,
            "recall": 0.938034188034188
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.6367673372202245,
            "auditor_fn_violation": 0.00467249657064472,
            "auditor_fp_violation": 0.010578823820113669,
            "ave_precision_score": 0.5476044074736968,
            "fpr": 0.3442982456140351,
            "logloss": 7.489589792373943,
            "mae": 0.40726502694336597,
            "precision": 0.5884665792922673,
            "recall": 0.9238683127572016
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6436031618361294,
            "auditor_fn_violation": 0.005366508110745213,
            "auditor_fp_violation": 0.006439974923991448,
            "ave_precision_score": 0.5572063259886173,
            "fpr": 0.3413830954994512,
            "logloss": 6.918513946782301,
            "mae": 0.39999496055083017,
            "precision": 0.5802968960863698,
            "recall": 0.9188034188034188
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.6347899199224085,
            "auditor_fn_violation": 0.007621290881524804,
            "auditor_fp_violation": 0.011628984432913268,
            "ave_precision_score": 0.5474138472394571,
            "fpr": 0.3223684210526316,
            "logloss": 7.3770651881597304,
            "mae": 0.4092617764943402,
            "precision": 0.5893854748603352,
            "recall": 0.8683127572016461
        },
        "train": {
            "accuracy": 0.6180021953896817,
            "auc_prc": 0.6426324343301615,
            "auditor_fn_violation": 0.010076275718427206,
            "auditor_fp_violation": 0.01324667408374694,
            "ave_precision_score": 0.5573126987607572,
            "fpr": 0.31174533479692645,
            "logloss": 6.816095627708677,
            "mae": 0.3966308690070325,
            "precision": 0.5872093023255814,
            "recall": 0.8632478632478633
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7515860429982779,
            "auditor_fn_violation": 0.015296729478016025,
            "auditor_fp_violation": 0.014439708425994565,
            "ave_precision_score": 0.7525605079895287,
            "fpr": 0.08223684210526316,
            "logloss": 1.0955047546578895,
            "mae": 0.34825355349899484,
            "precision": 0.7869318181818182,
            "recall": 0.5699588477366255
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7736643184756229,
            "auditor_fn_violation": 0.008063835176897748,
            "auditor_fp_violation": 0.00765660735480322,
            "ave_precision_score": 0.7751248174649692,
            "fpr": 0.09220636663007684,
            "logloss": 0.9191056827369919,
            "mae": 0.3269118032021472,
            "precision": 0.7795275590551181,
            "recall": 0.6346153846153846
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.6365264125103154,
            "auditor_fn_violation": 0.00467249657064472,
            "auditor_fp_violation": 0.009057635285396584,
            "ave_precision_score": 0.5478769893707982,
            "fpr": 0.34539473684210525,
            "logloss": 7.464028948985646,
            "mae": 0.4067330901519183,
            "precision": 0.587696335078534,
            "recall": 0.9238683127572016
        },
        "train": {
            "accuracy": 0.6180021953896817,
            "auc_prc": 0.6436988288480345,
            "auditor_fn_violation": 0.004756677643615078,
            "auditor_fp_violation": 0.007483156702752679,
            "ave_precision_score": 0.5573049755447158,
            "fpr": 0.3391877058177827,
            "logloss": 6.916921143604224,
            "mae": 0.39939792820637887,
            "precision": 0.5813008130081301,
            "recall": 0.9166666666666666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6052631578947368,
            "auc_prc": 0.6461475261044447,
            "auditor_fn_violation": 0.005969785575048734,
            "auditor_fp_violation": 0.011652149740548567,
            "ave_precision_score": 0.5519291857985633,
            "fpr": 0.3651315789473684,
            "logloss": 7.767483089478155,
            "mae": 0.4045307432603103,
            "precision": 0.5795454545454546,
            "recall": 0.9444444444444444
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.6533906809119328,
            "auditor_fn_violation": 0.005875482000619213,
            "auditor_fp_violation": 0.0003469013041011244,
            "ave_precision_score": 0.5624472251500949,
            "fpr": 0.3677277716794731,
            "logloss": 7.183965748797899,
            "mae": 0.39810104226362797,
            "precision": 0.5705128205128205,
            "recall": 0.9508547008547008
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.6447766154026793,
            "auditor_fn_violation": 0.005590751570283736,
            "auditor_fp_violation": 0.006280372292232936,
            "ave_precision_score": 0.5531908311486059,
            "fpr": 0.3432017543859649,
            "logloss": 7.528603373282107,
            "mae": 0.40411767294861434,
            "precision": 0.5886990801576872,
            "recall": 0.9218106995884774
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6524785666892439,
            "auditor_fn_violation": 0.002889658213478192,
            "auditor_fp_violation": 0.004318921236058908,
            "ave_precision_score": 0.5640063531235716,
            "fpr": 0.3391877058177827,
            "logloss": 6.945232097445776,
            "mae": 0.395404639216154,
            "precision": 0.5807327001356852,
            "recall": 0.9145299145299145
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.6373368090586009,
            "auditor_fn_violation": 0.00467249657064472,
            "auditor_fp_violation": 0.009057635285396584,
            "ave_precision_score": 0.5484432571403537,
            "fpr": 0.34539473684210525,
            "logloss": 7.463971298200812,
            "mae": 0.4064926132263632,
            "precision": 0.587696335078534,
            "recall": 0.9238683127572016
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6437248249347455,
            "auditor_fn_violation": 0.004756677643615078,
            "auditor_fp_violation": 0.011016594271668347,
            "ave_precision_score": 0.557328192743372,
            "fpr": 0.3402854006586169,
            "logloss": 6.917853521719476,
            "mae": 0.399461602067583,
            "precision": 0.5805142083897158,
            "recall": 0.9166666666666666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6030701754385965,
            "auc_prc": 0.6202933511405524,
            "auditor_fn_violation": 0.002490794888455709,
            "auditor_fp_violation": 0.016622395189852566,
            "ave_precision_score": 0.5348513937294589,
            "fpr": 0.35635964912280704,
            "logloss": 7.526813364214834,
            "mae": 0.4186529785419177,
            "precision": 0.5801033591731266,
            "recall": 0.9238683127572016
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.631086095315976,
            "auditor_fn_violation": 0.005319598074812126,
            "auditor_fp_violation": 0.011492344631578421,
            "ave_precision_score": 0.547915651409109,
            "fpr": 0.35236004390779363,
            "logloss": 6.8547826935666585,
            "mae": 0.4108738353870539,
            "precision": 0.5770750988142292,
            "recall": 0.9358974358974359
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6162280701754386,
            "auc_prc": 0.6262678320923041,
            "auditor_fn_violation": 0.010455021298101221,
            "auditor_fp_violation": 0.00889805205502019,
            "ave_precision_score": 0.5452790845053899,
            "fpr": 0.3432017543859649,
            "logloss": 7.072472887919083,
            "mae": 0.4071737453779877,
            "precision": 0.589238845144357,
            "recall": 0.9238683127572016
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.6392085393508922,
            "auditor_fn_violation": 0.004240667248351115,
            "auditor_fp_violation": 0.01763993131354179,
            "ave_precision_score": 0.5611653175269018,
            "fpr": 0.3391877058177827,
            "logloss": 6.407959040709187,
            "mae": 0.39567060348333466,
            "precision": 0.5818673883626523,
            "recall": 0.9188034188034188
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.7377079966897119,
            "auditor_fn_violation": 0.014768789257093351,
            "auditor_fp_violation": 0.018887447491969366,
            "ave_precision_score": 0.738468344693646,
            "fpr": 0.14144736842105263,
            "logloss": 0.8646756742212578,
            "mae": 0.3420281744219481,
            "precision": 0.7158590308370044,
            "recall": 0.668724279835391
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7429190785860293,
            "auditor_fn_violation": 0.005089738898740004,
            "auditor_fp_violation": 0.011157832659766638,
            "ave_precision_score": 0.743655783661202,
            "fpr": 0.1602634467618002,
            "logloss": 0.7990435961673705,
            "mae": 0.3292320494603419,
            "precision": 0.6964656964656964,
            "recall": 0.7158119658119658
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6162280701754386,
            "auc_prc": 0.634553018592105,
            "auditor_fn_violation": 0.00573063316728034,
            "auditor_fp_violation": 0.016416481344205595,
            "ave_precision_score": 0.5490267964279887,
            "fpr": 0.3223684210526316,
            "logloss": 7.261282413201546,
            "mae": 0.40400380494324245,
            "precision": 0.5939226519337016,
            "recall": 0.8847736625514403
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.6439819303876476,
            "auditor_fn_violation": 0.0075572067888204,
            "auditor_fp_violation": 0.014654102231814342,
            "ave_precision_score": 0.560067829397634,
            "fpr": 0.31394072447859495,
            "logloss": 6.709747377293942,
            "mae": 0.39124568812083965,
            "precision": 0.5890804597701149,
            "recall": 0.8760683760683761
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 12498,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8453984458552586,
            "auditor_fn_violation": 0.01555167496931631,
            "auditor_fp_violation": 0.012825858660736348,
            "ave_precision_score": 0.845775377674786,
            "fpr": 0.0625,
            "logloss": 0.7490324949992159,
            "mae": 0.30863772883496593,
            "precision": 0.85,
            "recall": 0.6646090534979424
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8630145938886841,
            "auditor_fn_violation": 0.003283702515316131,
            "auditor_fp_violation": 0.007495546035041989,
            "ave_precision_score": 0.8632017100845973,
            "fpr": 0.06915477497255763,
            "logloss": 0.6380049855992959,
            "mae": 0.29705909696986754,
            "precision": 0.8363636363636363,
            "recall": 0.688034188034188
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 12498,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8238639613295885,
            "auditor_fn_violation": 0.013509854884123897,
            "auditor_fp_violation": 0.016308376575240924,
            "ave_precision_score": 0.8243315363102264,
            "fpr": 0.11293859649122807,
            "logloss": 0.6228389624222599,
            "mae": 0.29269648727538305,
            "precision": 0.774617067833698,
            "recall": 0.7283950617283951
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8347170592200729,
            "auditor_fn_violation": 0.007303892594781728,
            "auditor_fp_violation": 0.012349686425999762,
            "ave_precision_score": 0.8349497501011464,
            "fpr": 0.12733260153677278,
            "logloss": 0.6021340662938529,
            "mae": 0.3008038370272515,
            "precision": 0.7483731019522777,
            "recall": 0.7371794871794872
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6107456140350878,
            "auc_prc": 0.6335894664113357,
            "auditor_fn_violation": 0.0028495235001082956,
            "auditor_fp_violation": 0.011834898278560258,
            "ave_precision_score": 0.5469721186777319,
            "fpr": 0.32456140350877194,
            "logloss": 7.347683651995591,
            "mae": 0.40864954727064623,
            "precision": 0.5905947441217151,
            "recall": 0.8786008230452675
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6427946091187686,
            "auditor_fn_violation": 0.008328876879919692,
            "auditor_fp_violation": 0.016336573556704745,
            "ave_precision_score": 0.5574651345101619,
            "fpr": 0.31613611416026344,
            "logloss": 6.807584570156342,
            "mae": 0.3963905473469228,
            "precision": 0.5856115107913669,
            "recall": 0.8696581196581197
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5855263157894737,
            "auc_prc": 0.6253481790364098,
            "auditor_fn_violation": 0.003645946141072848,
            "auditor_fp_violation": 0.011711349971172066,
            "ave_precision_score": 0.5247686158102248,
            "fpr": 0.38377192982456143,
            "logloss": 8.439837319029333,
            "mae": 0.43074163440248187,
            "precision": 0.5668316831683168,
            "recall": 0.9423868312757202
        },
        "train": {
            "accuracy": 0.5773874862788145,
            "auc_prc": 0.6276115130111592,
            "auditor_fn_violation": 0.006093613667708071,
            "auditor_fp_violation": 0.00649944371898021,
            "ave_precision_score": 0.5296487392128215,
            "fpr": 0.3896816684961581,
            "logloss": 7.976944563657006,
            "mae": 0.4402198518791451,
            "precision": 0.5523329129886507,
            "recall": 0.9358974358974359
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5910087719298246,
            "auc_prc": 0.6236152646036477,
            "auditor_fn_violation": 0.010328676629846222,
            "auditor_fp_violation": 0.018534820031298912,
            "ave_precision_score": 0.5403229420013753,
            "fpr": 0.30153508771929827,
            "logloss": 7.275875561970316,
            "mae": 0.4179389593373982,
            "precision": 0.5852187028657617,
            "recall": 0.7983539094650206
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.6367618761801893,
            "auditor_fn_violation": 0.015972867235216303,
            "auditor_fp_violation": 0.010545799644673963,
            "ave_precision_score": 0.5553723974192597,
            "fpr": 0.3018660812294182,
            "logloss": 6.620236827432371,
            "mae": 0.3990462210069489,
            "precision": 0.5870870870870871,
            "recall": 0.8354700854700855
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.6254819939520627,
            "auditor_fn_violation": 0.010283553534040866,
            "auditor_fp_violation": 0.00748754221233836,
            "ave_precision_score": 0.5437402702938303,
            "fpr": 0.3432017543859649,
            "logloss": 7.155470227162317,
            "mae": 0.40619835065541665,
            "precision": 0.5886990801576872,
            "recall": 0.9218106995884774
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.6408931412367741,
            "auditor_fn_violation": 0.005160103952639629,
            "auditor_fp_violation": 0.01763993131354179,
            "ave_precision_score": 0.5615584891840362,
            "fpr": 0.3391877058177827,
            "logloss": 6.475157129675546,
            "mae": 0.3947060421419981,
            "precision": 0.5818673883626523,
            "recall": 0.9188034188034188
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5921052631578947,
            "auc_prc": 0.6248028676336004,
            "auditor_fn_violation": 0.006572178904050253,
            "auditor_fp_violation": 0.019713676797627876,
            "ave_precision_score": 0.5420275716432159,
            "fpr": 0.3125,
            "logloss": 7.226605514318745,
            "mae": 0.41414207151070803,
            "precision": 0.5833333333333334,
            "recall": 0.8209876543209876
        },
        "train": {
            "accuracy": 0.6158068057080132,
            "auc_prc": 0.6369100698642918,
            "auditor_fn_violation": 0.014551493146443755,
            "auditor_fp_violation": 0.015984716519687903,
            "ave_precision_score": 0.5560584843600337,
            "fpr": 0.3029637760702525,
            "logloss": 6.590250943015663,
            "mae": 0.3971553967818904,
            "precision": 0.5880597014925373,
            "recall": 0.8418803418803419
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6118421052631579,
            "auc_prc": 0.6376744929361188,
            "auditor_fn_violation": 0.0063082087935889125,
            "auditor_fp_violation": 0.00464335721933945,
            "ave_precision_score": 0.548007810564026,
            "fpr": 0.3530701754385965,
            "logloss": 7.547609597180385,
            "mae": 0.40699339638619375,
            "precision": 0.5850515463917526,
            "recall": 0.934156378600823
        },
        "train": {
            "accuracy": 0.605927552140505,
            "auc_prc": 0.6448213575732387,
            "auditor_fn_violation": 0.0032602474973495834,
            "auditor_fp_violation": 0.002633972044710619,
            "ave_precision_score": 0.557364370302159,
            "fpr": 0.3600439077936334,
            "logloss": 7.007910527950854,
            "mae": 0.4024761475456018,
            "precision": 0.5712418300653594,
            "recall": 0.9337606837606838
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.6337837888704877,
            "auditor_fn_violation": 0.0034767345318027585,
            "auditor_fp_violation": 0.008818260439831978,
            "ave_precision_score": 0.5471948895566316,
            "fpr": 0.33114035087719296,
            "logloss": 7.342958477015342,
            "mae": 0.4072892561518146,
            "precision": 0.591339648173207,
            "recall": 0.8991769547325102
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.6425713296301018,
            "auditor_fn_violation": 0.006830101231857549,
            "auditor_fp_violation": 0.01549657682748846,
            "ave_precision_score": 0.5578043154654109,
            "fpr": 0.3238199780461032,
            "logloss": 6.7899039325951245,
            "mae": 0.3963865161005461,
            "precision": 0.5850914205344585,
            "recall": 0.8888888888888888
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 12498,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7625335557770104,
            "auditor_fn_violation": 0.011896704209082383,
            "auditor_fp_violation": 0.01924779672185158,
            "ave_precision_score": 0.7632633709784509,
            "fpr": 0.12938596491228072,
            "logloss": 0.921011705861393,
            "mae": 0.3222940726697919,
            "precision": 0.739514348785872,
            "recall": 0.6893004115226338
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7868799902877064,
            "auditor_fn_violation": 0.01004812969686735,
            "auditor_fp_violation": 0.00842722382319928,
            "ave_precision_score": 0.7877211481805115,
            "fpr": 0.13172338090010977,
            "logloss": 0.7833041638540478,
            "mae": 0.3096384160035585,
            "precision": 0.7350993377483444,
            "recall": 0.7115384615384616
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.6307550378704199,
            "auditor_fn_violation": 0.009191574615551226,
            "auditor_fp_violation": 0.029188287620459608,
            "ave_precision_score": 0.6014078304186501,
            "fpr": 0.21929824561403508,
            "logloss": 3.5652716347326963,
            "mae": 0.3502006797475348,
            "precision": 0.6621621621621622,
            "recall": 0.8065843621399177
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.6461844250980697,
            "auditor_fn_violation": 0.011136442530514983,
            "auditor_fp_violation": 0.0333099587930808,
            "ave_precision_score": 0.6261133102215559,
            "fpr": 0.2283205268935236,
            "logloss": 2.616666173720722,
            "mae": 0.34231677430177887,
            "precision": 0.6498316498316499,
            "recall": 0.8247863247863247
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5394736842105263,
            "auc_prc": 0.558030043688359,
            "auditor_fn_violation": 0.030200888022525456,
            "auditor_fp_violation": 0.03536055514372787,
            "ave_precision_score": 0.511838448582205,
            "fpr": 0.20285087719298245,
            "logloss": 8.258621260720746,
            "mae": 0.48498103764845735,
            "precision": 0.5756880733944955,
            "recall": 0.5164609053497943
        },
        "train": {
            "accuracy": 0.5126234906695939,
            "auc_prc": 0.5260043831995725,
            "auditor_fn_violation": 0.04152945481156239,
            "auditor_fp_violation": 0.010897656681690805,
            "ave_precision_score": 0.4797087449660675,
            "fpr": 0.21624588364434688,
            "logloss": 8.4421717971535,
            "mae": 0.5112287667671656,
            "precision": 0.5287081339712919,
            "recall": 0.4722222222222222
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.6347948237383908,
            "auditor_fn_violation": 0.007621290881524804,
            "auditor_fp_violation": 0.011628984432913268,
            "ave_precision_score": 0.5474121390339283,
            "fpr": 0.3223684210526316,
            "logloss": 7.375926119874752,
            "mae": 0.40926340581408066,
            "precision": 0.5893854748603352,
            "recall": 0.8683127572016461
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6426316594065857,
            "auditor_fn_violation": 0.010076275718427206,
            "auditor_fp_violation": 0.011048806535620582,
            "ave_precision_score": 0.5573091666807951,
            "fpr": 0.31284302963776073,
            "logloss": 6.815112561375325,
            "mae": 0.3966502075770545,
            "precision": 0.5863570391872278,
            "recall": 0.8632478632478633
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.6344229894023456,
            "auditor_fn_violation": 0.001051368132264819,
            "auditor_fp_violation": 0.009667655053125772,
            "ave_precision_score": 0.5480511292816371,
            "fpr": 0.3300438596491228,
            "logloss": 7.343220054835171,
            "mae": 0.4065359127059311,
            "precision": 0.5882352941176471,
            "recall": 0.8847736625514403
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6414456690359283,
            "auditor_fn_violation": 0.007113906949252723,
            "auditor_fp_violation": 0.006727407433103797,
            "ave_precision_score": 0.5582998656957183,
            "fpr": 0.3238199780461032,
            "logloss": 6.726745322269985,
            "mae": 0.39541533290385933,
            "precision": 0.5891364902506964,
            "recall": 0.9038461538461539
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.6262123545462945,
            "auditor_fn_violation": 0.0070256660168940875,
            "auditor_fp_violation": 0.007917387365126444,
            "ave_precision_score": 0.5436737342246487,
            "fpr": 0.3442982456140351,
            "logloss": 7.218133258954902,
            "mae": 0.40742917516716287,
            "precision": 0.5879265091863517,
            "recall": 0.9218106995884774
        },
        "train": {
            "accuracy": 0.6158068057080132,
            "auc_prc": 0.6399225863465434,
            "auditor_fn_violation": 0.004756677643615078,
            "auditor_fp_violation": 0.013259063416036264,
            "ave_precision_score": 0.5591568829610707,
            "fpr": 0.3413830954994512,
            "logloss": 6.598210807994307,
            "mae": 0.39590962961291576,
            "precision": 0.5797297297297297,
            "recall": 0.9166666666666666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 12498,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7565085923713054,
            "auditor_fn_violation": 0.010143671937044256,
            "auditor_fp_violation": 0.020354583642204105,
            "ave_precision_score": 0.757459191389354,
            "fpr": 0.16885964912280702,
            "logloss": 0.919335971826508,
            "mae": 0.3047283366590579,
            "precision": 0.7210144927536232,
            "recall": 0.8189300411522634
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.773154538429569,
            "auditor_fn_violation": 0.008040380158931205,
            "auditor_fp_violation": 0.01749621505898562,
            "ave_precision_score": 0.7739650100273674,
            "fpr": 0.18221734357848518,
            "logloss": 0.8218921770369002,
            "mae": 0.29708303107554707,
            "precision": 0.7067137809187279,
            "recall": 0.8547008547008547
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.6594891791881778,
            "auditor_fn_violation": 0.027994368637643492,
            "auditor_fp_violation": 0.01818219257062845,
            "ave_precision_score": 0.644799865815942,
            "fpr": 0.18859649122807018,
            "logloss": 2.246460341512282,
            "mae": 0.3443680600095508,
            "precision": 0.6802973977695167,
            "recall": 0.7530864197530864
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.6793329731794351,
            "auditor_fn_violation": 0.014162139848199126,
            "auditor_fp_violation": 0.02214964826685632,
            "ave_precision_score": 0.6644414537474963,
            "fpr": 0.1986827661909989,
            "logloss": 1.9376504574603401,
            "mae": 0.3311671444562517,
            "precision": 0.6697080291970803,
            "recall": 0.7841880341880342
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.6398021628506947,
            "auditor_fn_violation": 0.004155837123673383,
            "auditor_fp_violation": 0.007706325673338282,
            "ave_precision_score": 0.5424237664133461,
            "fpr": 0.34868421052631576,
            "logloss": 8.00704050141422,
            "mae": 0.4103643525266211,
            "precision": 0.5843137254901961,
            "recall": 0.9197530864197531
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.6482721656310917,
            "auditor_fn_violation": 0.007280437576815185,
            "auditor_fp_violation": 0.009202796024511054,
            "ave_precision_score": 0.5532533900087176,
            "fpr": 0.3424807903402854,
            "logloss": 7.407426073730637,
            "mae": 0.40161648707267034,
            "precision": 0.5783783783783784,
            "recall": 0.9145299145299145
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 12498,
        "test": {
            "accuracy": 0.555921052631579,
            "auc_prc": 0.650376140124096,
            "auditor_fn_violation": 0.014238592881380406,
            "auditor_fp_violation": 0.019288979490980976,
            "ave_precision_score": 0.5358344424210787,
            "fpr": 0.3267543859649123,
            "logloss": 9.173847066826962,
            "mae": 0.4541950637735442,
            "precision": 0.5598227474150664,
            "recall": 0.779835390946502
        },
        "train": {
            "accuracy": 0.5850713501646543,
            "auc_prc": 0.6524899847075167,
            "auditor_fn_violation": 0.009133383996172148,
            "auditor_fp_violation": 0.010971992675426746,
            "ave_precision_score": 0.5399955521262291,
            "fpr": 0.31284302963776073,
            "logloss": 8.60515943187734,
            "mae": 0.4238530567768251,
            "precision": 0.5681818181818182,
            "recall": 0.8012820512820513
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.6335551242877069,
            "auditor_fn_violation": 0.0068632228719948014,
            "auditor_fp_violation": 0.01228018696977185,
            "ave_precision_score": 0.5469553017576406,
            "fpr": 0.32346491228070173,
            "logloss": 7.3431833504796975,
            "mae": 0.40915180148716135,
            "precision": 0.5891364902506964,
            "recall": 0.8703703703703703
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.6427695233156724,
            "auditor_fn_violation": 0.01013256776154691,
            "auditor_fp_violation": 0.01566754961308117,
            "ave_precision_score": 0.5574310399694777,
            "fpr": 0.31174533479692645,
            "logloss": 6.801761696750822,
            "mae": 0.3965244951556231,
            "precision": 0.5878084179970973,
            "recall": 0.8653846153846154
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 12498,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.828465041903672,
            "auditor_fn_violation": 0.0037948523572305243,
            "auditor_fp_violation": 0.009397393130714114,
            "ave_precision_score": 0.8289915449365803,
            "fpr": 0.22697368421052633,
            "logloss": 0.795213271458193,
            "mae": 0.32619814649877843,
            "precision": 0.6830015313935681,
            "recall": 0.9176954732510288
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8225660580704662,
            "auditor_fn_violation": 0.005108502913113232,
            "auditor_fp_violation": 0.023415838026825384,
            "ave_precision_score": 0.8230554964386391,
            "fpr": 0.23600439077936333,
            "logloss": 0.7209741576960791,
            "mae": 0.32213878615643793,
            "precision": 0.6692307692307692,
            "recall": 0.9294871794871795
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.6368076861012985,
            "auditor_fn_violation": 0.007925871778210962,
            "auditor_fp_violation": 0.008020344287949932,
            "ave_precision_score": 0.5473734941307142,
            "fpr": 0.34649122807017546,
            "logloss": 7.519854861195893,
            "mae": 0.4069166591691196,
            "precision": 0.5880052151238592,
            "recall": 0.9279835390946503
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.643478938596519,
            "auditor_fn_violation": 0.003668364809967446,
            "auditor_fp_violation": 0.0023316723368510695,
            "ave_precision_score": 0.5568361579261478,
            "fpr": 0.3446761800219539,
            "logloss": 6.95725460559555,
            "mae": 0.4004289903798121,
            "precision": 0.5785234899328859,
            "recall": 0.9209401709401709
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 12498,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.6272761686387358,
            "auditor_fn_violation": 0.011567305609703272,
            "auditor_fp_violation": 0.014228646734206417,
            "ave_precision_score": 0.5457488931390924,
            "fpr": 0.30701754385964913,
            "logloss": 7.068995254732875,
            "mae": 0.41025483668776713,
            "precision": 0.5924308588064047,
            "recall": 0.8374485596707819
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.6405992107805019,
            "auditor_fn_violation": 0.017140927129950186,
            "auditor_fp_violation": 0.008385100093415561,
            "ave_precision_score": 0.5601149899634736,
            "fpr": 0.29857299670691545,
            "logloss": 6.476944282592205,
            "mae": 0.3939894089016769,
            "precision": 0.592814371257485,
            "recall": 0.8461538461538461
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 12498,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8356754676533527,
            "auditor_fn_violation": 0.012390802108151043,
            "auditor_fp_violation": 0.01393007165801829,
            "ave_precision_score": 0.8360501046616564,
            "fpr": 0.0712719298245614,
            "logloss": 0.8802003529633056,
            "mae": 0.3007988594300083,
            "precision": 0.8228882833787466,
            "recall": 0.6213991769547325
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.851893566441368,
            "auditor_fn_violation": 0.004526818467542945,
            "auditor_fp_violation": 0.008865806186241401,
            "ave_precision_score": 0.8520443998537162,
            "fpr": 0.07683863885839737,
            "logloss": 0.7494250800308918,
            "mae": 0.28655461509125235,
            "precision": 0.8172323759791122,
            "recall": 0.6688034188034188
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6008771929824561,
            "auc_prc": 0.626758415237061,
            "auditor_fn_violation": 0.008483142011407119,
            "auditor_fp_violation": 0.016231158883123313,
            "ave_precision_score": 0.5457763251720256,
            "fpr": 0.3092105263157895,
            "logloss": 7.0422519226885925,
            "mae": 0.4090069446910843,
            "precision": 0.5889212827988338,
            "recall": 0.831275720164609
        },
        "train": {
            "accuracy": 0.6212952799121844,
            "auc_prc": 0.6390718867438361,
            "auditor_fn_violation": 0.010693142690947306,
            "auditor_fp_violation": 0.014108971611084006,
            "ave_precision_score": 0.5602000360323928,
            "fpr": 0.300768386388584,
            "logloss": 6.410515144487894,
            "mae": 0.39159943793238877,
            "precision": 0.5916542473919523,
            "recall": 0.8482905982905983
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.7515805916611062,
            "auditor_fn_violation": 0.010825030683705165,
            "auditor_fp_violation": 0.014403673503006345,
            "ave_precision_score": 0.7519493774146748,
            "fpr": 0.06359649122807018,
            "logloss": 3.4595501889865656,
            "mae": 0.37806174023034467,
            "precision": 0.7986111111111112,
            "recall": 0.4732510288065844
        },
        "train": {
            "accuracy": 0.6355653128430296,
            "auc_prc": 0.7271921410438561,
            "auditor_fn_violation": 0.004700385600495382,
            "auditor_fp_violation": 0.016462944746055856,
            "ave_precision_score": 0.7276259148093989,
            "fpr": 0.0889132821075741,
            "logloss": 3.059023276291513,
            "mae": 0.37549564661957907,
            "precision": 0.7281879194630873,
            "recall": 0.4636752136752137
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5789473684210527,
            "auc_prc": 0.6274558407641581,
            "auditor_fn_violation": 0.00947585011912498,
            "auditor_fp_violation": 0.018655794415616517,
            "ave_precision_score": 0.5420807732233768,
            "fpr": 0.30043859649122806,
            "logloss": 7.361219474782212,
            "mae": 0.42135565189260216,
            "precision": 0.5784615384615385,
            "recall": 0.7736625514403292
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6393234347606652,
            "auditor_fn_violation": 0.015175396624353816,
            "auditor_fp_violation": 0.014849853681985664,
            "ave_precision_score": 0.5558208946256415,
            "fpr": 0.29198682766191,
            "logloss": 6.71411162963009,
            "mae": 0.39980283449122805,
            "precision": 0.5913978494623656,
            "recall": 0.8226495726495726
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5942982456140351,
            "auc_prc": 0.6162043021986015,
            "auditor_fn_violation": 0.004304743339831059,
            "auditor_fp_violation": 0.01917572687587513,
            "ave_precision_score": 0.5385096390119608,
            "fpr": 0.3267543859649123,
            "logloss": 6.913459825125698,
            "mae": 0.42231208771766415,
            "precision": 0.5814606741573034,
            "recall": 0.8518518518518519
        },
        "train": {
            "accuracy": 0.6289791437980241,
            "auc_prc": 0.629113815034507,
            "auditor_fn_violation": 0.00967284940940265,
            "auditor_fp_violation": 0.012230748836022246,
            "ave_precision_score": 0.5538758115795666,
            "fpr": 0.300768386388584,
            "logloss": 6.209785505278348,
            "mae": 0.4036273650710525,
            "precision": 0.5958702064896755,
            "recall": 0.8632478632478633
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 12498,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.6289147884409216,
            "auditor_fn_violation": 0.007869467908454265,
            "auditor_fp_violation": 0.012637962276583474,
            "ave_precision_score": 0.5489518096225217,
            "fpr": 0.3442982456140351,
            "logloss": 7.0025497094547,
            "mae": 0.39979149167336064,
            "precision": 0.5900783289817232,
            "recall": 0.9300411522633745
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.6429428266799362,
            "auditor_fn_violation": 0.0031945734470432627,
            "auditor_fp_violation": 0.008231472373027947,
            "ave_precision_score": 0.5652693147899687,
            "fpr": 0.34796926454445665,
            "logloss": 6.376976578754245,
            "mae": 0.39226156012814506,
            "precision": 0.5801324503311258,
            "recall": 0.9358974358974359
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 12498,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8429173535379904,
            "auditor_fn_violation": 0.004837195870334274,
            "auditor_fp_violation": 0.007361419981879581,
            "ave_precision_score": 0.8433188577435116,
            "fpr": 0.10964912280701754,
            "logloss": 0.7306471457402943,
            "mae": 0.31139512882484205,
            "precision": 0.7863247863247863,
            "recall": 0.757201646090535
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8421769425071042,
            "auditor_fn_violation": 0.0054532916772214255,
            "auditor_fp_violation": 0.015518877625609247,
            "ave_precision_score": 0.8424615218147202,
            "fpr": 0.11745334796926454,
            "logloss": 0.643416931657129,
            "mae": 0.31189253004717626,
            "precision": 0.7723404255319148,
            "recall": 0.7756410256410257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.6357840572887825,
            "auditor_fn_violation": 0.004065590932062669,
            "auditor_fp_violation": 0.019180874722016315,
            "ave_precision_score": 0.6220421831223053,
            "fpr": 0.2631578947368421,
            "logloss": 2.5579995536892226,
            "mae": 0.3583890585096234,
            "precision": 0.6401799100449775,
            "recall": 0.8786008230452675
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.685441904410877,
            "auditor_fn_violation": 0.005807462448516237,
            "auditor_fp_violation": 0.021108944354552968,
            "ave_precision_score": 0.6740304736236856,
            "fpr": 0.25905598243688255,
            "logloss": 1.956080915444995,
            "mae": 0.34708116273178474,
            "precision": 0.6358024691358025,
            "recall": 0.8803418803418803
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6118421052631579,
            "auc_prc": 0.6358843811173842,
            "auditor_fn_violation": 0.005157569850552307,
            "auditor_fp_violation": 0.010578823820113669,
            "ave_precision_score": 0.5464365121381027,
            "fpr": 0.3442982456140351,
            "logloss": 7.522708016341131,
            "mae": 0.40861529659729023,
            "precision": 0.5868421052631579,
            "recall": 0.9176954732510288
        },
        "train": {
            "accuracy": 0.6158068057080132,
            "auc_prc": 0.64362158348923,
            "auditor_fn_violation": 0.004934935780160809,
            "auditor_fp_violation": 0.011844201668595272,
            "ave_precision_score": 0.5561577805274083,
            "fpr": 0.3358946212952799,
            "logloss": 6.972901168874024,
            "mae": 0.4001653933505048,
            "precision": 0.5808219178082191,
            "recall": 0.905982905982906
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6129385964912281,
            "auc_prc": 0.6366426572943535,
            "auditor_fn_violation": 0.003576005342574545,
            "auditor_fp_violation": 0.007737212750185345,
            "ave_precision_score": 0.5479968555019797,
            "fpr": 0.34649122807017546,
            "logloss": 7.47312714741107,
            "mae": 0.40678380999260727,
            "precision": 0.5869281045751634,
            "recall": 0.9238683127572016
        },
        "train": {
            "accuracy": 0.6180021953896817,
            "auc_prc": 0.6431624332534516,
            "auditor_fn_violation": 0.0039732800435325144,
            "auditor_fp_violation": 0.01650259060938171,
            "ave_precision_score": 0.5570246068220288,
            "fpr": 0.3402854006586169,
            "logloss": 6.926591679223022,
            "mae": 0.39995843750137416,
            "precision": 0.581081081081081,
            "recall": 0.9188034188034188
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5877192982456141,
            "auc_prc": 0.6419148954175187,
            "auditor_fn_violation": 0.012426900584795324,
            "auditor_fp_violation": 0.01405361996540648,
            "ave_precision_score": 0.541696575473711,
            "fpr": 0.32346491228070173,
            "logloss": 8.140207666045555,
            "mae": 0.42268418074685576,
            "precision": 0.5785714285714286,
            "recall": 0.8333333333333334
        },
        "train": {
            "accuracy": 0.6026344676180022,
            "auc_prc": 0.6487250807796906,
            "auditor_fn_violation": 0.014607785189563459,
            "auditor_fp_violation": 0.012857649049862101,
            "ave_precision_score": 0.5519273298469505,
            "fpr": 0.31833150384193193,
            "logloss": 7.491375525852935,
            "mae": 0.4035684390832608,
            "precision": 0.577259475218659,
            "recall": 0.8461538461538461
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5975877192982456,
            "auc_prc": 0.6324894801902349,
            "auditor_fn_violation": 0.008968215291314708,
            "auditor_fp_violation": 0.012040812124207233,
            "ave_precision_score": 0.5461230262117422,
            "fpr": 0.3267543859649123,
            "logloss": 7.345751806681099,
            "mae": 0.4111593854531907,
            "precision": 0.5832167832167832,
            "recall": 0.8580246913580247
        },
        "train": {
            "accuracy": 0.610318331503842,
            "auc_prc": 0.6416329552241284,
            "auditor_fn_violation": 0.007041196393556438,
            "auditor_fp_violation": 0.006791831961008301,
            "ave_precision_score": 0.5568362107772956,
            "fpr": 0.3227222832052689,
            "logloss": 6.789113152996587,
            "mae": 0.3973066849377452,
            "precision": 0.5805991440798859,
            "recall": 0.8696581196581197
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5921052631578947,
            "auc_prc": 0.6237803601851166,
            "auditor_fn_violation": 0.006572178904050253,
            "auditor_fp_violation": 0.020454966641957006,
            "ave_precision_score": 0.5412675968610536,
            "fpr": 0.3125,
            "logloss": 7.223968497895312,
            "mae": 0.4143578219316847,
            "precision": 0.5833333333333334,
            "recall": 0.8209876543209876
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.6359723341597989,
            "auditor_fn_violation": 0.015205888147710324,
            "auditor_fp_violation": 0.015984716519687903,
            "ave_precision_score": 0.555355021982475,
            "fpr": 0.3029637760702525,
            "logloss": 6.587313895392008,
            "mae": 0.39716790100272326,
            "precision": 0.5874439461883408,
            "recall": 0.8397435897435898
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5975877192982456,
            "auc_prc": 0.6463136589623295,
            "auditor_fn_violation": 0.00546891921160927,
            "auditor_fp_violation": 0.008501667902149733,
            "ave_precision_score": 0.5441067805826176,
            "fpr": 0.3519736842105263,
            "logloss": 8.226460391988235,
            "mae": 0.41677042880307946,
            "precision": 0.5781865965834428,
            "recall": 0.9053497942386831
        },
        "train": {
            "accuracy": 0.6048298572996706,
            "auc_prc": 0.6500778937811029,
            "auditor_fn_violation": 0.008002852130184734,
            "auditor_fp_violation": 0.005414138210435293,
            "ave_precision_score": 0.5522064930011659,
            "fpr": 0.3413830954994512,
            "logloss": 7.603348054951459,
            "mae": 0.4055943627398089,
            "precision": 0.5739726027397261,
            "recall": 0.8952991452991453
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6107456140350878,
            "auc_prc": 0.6342964820947317,
            "auditor_fn_violation": 0.0028495235001082956,
            "auditor_fp_violation": 0.011834898278560258,
            "ave_precision_score": 0.546663535140966,
            "fpr": 0.32456140350877194,
            "logloss": 7.403384669568189,
            "mae": 0.4088774973923839,
            "precision": 0.5905947441217151,
            "recall": 0.8786008230452675
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.6423203809326072,
            "auditor_fn_violation": 0.009588411344723092,
            "auditor_fp_violation": 0.014188263337735688,
            "ave_precision_score": 0.5567048762464102,
            "fpr": 0.3172338090010977,
            "logloss": 6.84503640330031,
            "mae": 0.3965839134270078,
            "precision": 0.5841726618705037,
            "recall": 0.8675213675213675
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 12498,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7633501433731814,
            "auditor_fn_violation": 0.014883853151397014,
            "auditor_fp_violation": 0.02349734371139116,
            "ave_precision_score": 0.7618171543565734,
            "fpr": 0.20723684210526316,
            "logloss": 1.1105437574632693,
            "mae": 0.30131812802529456,
            "precision": 0.6911764705882353,
            "recall": 0.8703703703703703
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.770146501634361,
            "auditor_fn_violation": 0.008795631737453912,
            "auditor_fp_violation": 0.017845594229544598,
            "ave_precision_score": 0.7697950827125588,
            "fpr": 0.22722283205268934,
            "logloss": 0.9582288814279166,
            "mae": 0.3030485385472942,
            "precision": 0.6724683544303798,
            "recall": 0.9081196581196581
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6019736842105263,
            "auc_prc": 0.6216017301409783,
            "auditor_fn_violation": 0.001498086780737855,
            "auditor_fp_violation": 0.016475681574829096,
            "ave_precision_score": 0.5442229980918682,
            "fpr": 0.3541666666666667,
            "logloss": 6.787252065375199,
            "mae": 0.41320202453547,
            "precision": 0.5799739921976593,
            "recall": 0.9176954732510288
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.6287681924809092,
            "auditor_fn_violation": 0.0031898824434499523,
            "auditor_fp_violation": 0.0054488283408454085,
            "ave_precision_score": 0.5521488104480047,
            "fpr": 0.36663007683863885,
            "logloss": 6.354679345657703,
            "mae": 0.413799004915245,
            "precision": 0.5599472990777339,
            "recall": 0.9081196581196581
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 12498,
        "test": {
            "accuracy": 0.6096491228070176,
            "auc_prc": 0.633179060185916,
            "auditor_fn_violation": 0.0052365352682116825,
            "auditor_fp_violation": 0.009502923976608187,
            "ave_precision_score": 0.5468658105500501,
            "fpr": 0.3366228070175439,
            "logloss": 7.360235988865065,
            "mae": 0.4080707590553191,
            "precision": 0.5873655913978495,
            "recall": 0.8991769547325102
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.6423665140719358,
            "auditor_fn_violation": 0.007346111627121512,
            "auditor_fp_violation": 0.010630047104241362,
            "ave_precision_score": 0.5573122708510845,
            "fpr": 0.32821075740944017,
            "logloss": 6.833797353663928,
            "mae": 0.3965277902681777,
            "precision": 0.5841446453407511,
            "recall": 0.8974358974358975
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5844298245614035,
            "auc_prc": 0.6260921791678502,
            "auditor_fn_violation": 0.010949119197169881,
            "auditor_fp_violation": 0.02167758010048596,
            "ave_precision_score": 0.5451043338388603,
            "fpr": 0.2982456140350877,
            "logloss": 7.064370738823436,
            "mae": 0.4183852699125214,
            "precision": 0.5821812596006144,
            "recall": 0.779835390946502
        },
        "train": {
            "accuracy": 0.6180021953896817,
            "auc_prc": 0.6374149503609444,
            "auditor_fn_violation": 0.009738523459708973,
            "auditor_fp_violation": 0.010748984694218899,
            "ave_precision_score": 0.5587739533899097,
            "fpr": 0.2854006586169045,
            "logloss": 6.417844255005143,
            "mae": 0.3968258229578599,
            "precision": 0.59375,
            "recall": 0.811965811965812
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 12498,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7519614543456665,
            "auditor_fn_violation": 0.0180988737275287,
            "auditor_fp_violation": 0.018202783955193147,
            "ave_precision_score": 0.7499440387539352,
            "fpr": 0.17982456140350878,
            "logloss": 0.9392802168443377,
            "mae": 0.3001376674126636,
            "precision": 0.7132867132867133,
            "recall": 0.8395061728395061
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7609945139829615,
            "auditor_fn_violation": 0.006473584958766083,
            "auditor_fp_violation": 0.012376942957036277,
            "ave_precision_score": 0.7601458321456378,
            "fpr": 0.19978046103183314,
            "logloss": 0.8392855625406209,
            "mae": 0.3052707774922176,
            "precision": 0.6872852233676976,
            "recall": 0.8547008547008547
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5942982456140351,
            "auc_prc": 0.6271212404534597,
            "auditor_fn_violation": 0.008207891126994441,
            "auditor_fp_violation": 0.021798554484803574,
            "ave_precision_score": 0.547653283789977,
            "fpr": 0.2993421052631579,
            "logloss": 6.889624301573684,
            "mae": 0.4104345839773407,
            "precision": 0.5876132930513596,
            "recall": 0.8004115226337448
        },
        "train": {
            "accuracy": 0.6201975850713501,
            "auc_prc": 0.6402079686147681,
            "auditor_fn_violation": 0.010109112743580362,
            "auditor_fp_violation": 0.012191102972696391,
            "ave_precision_score": 0.5635833226126579,
            "fpr": 0.28869374313940727,
            "logloss": 6.20599434370557,
            "mae": 0.39146459027567654,
            "precision": 0.5941358024691358,
            "recall": 0.8226495726495726
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5789473684210527,
            "auc_prc": 0.6469011925300857,
            "auditor_fn_violation": 0.0012453974442278538,
            "auditor_fp_violation": 0.003104151223128254,
            "ave_precision_score": 0.5553272181738174,
            "fpr": 0.4144736842105263,
            "logloss": 7.787686721228454,
            "mae": 0.4185697561862119,
            "precision": 0.5594405594405595,
            "recall": 0.9876543209876543
        },
        "train": {
            "accuracy": 0.5817782656421515,
            "auc_prc": 0.661827975409429,
            "auditor_fn_violation": 0.0012665709701933639,
            "auditor_fp_violation": 0.01008243861705317,
            "ave_precision_score": 0.5727441058032899,
            "fpr": 0.4094401756311745,
            "logloss": 7.183897628885234,
            "mae": 0.41411626324569756,
            "precision": 0.5522208883553421,
            "recall": 0.9829059829059829
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 12498,
        "test": {
            "accuracy": 0.5899122807017544,
            "auc_prc": 0.6326022904519302,
            "auditor_fn_violation": 0.00956158400115515,
            "auditor_fp_violation": 0.02050129725722759,
            "ave_precision_score": 0.546855736758728,
            "fpr": 0.2993421052631579,
            "logloss": 7.201551966761058,
            "mae": 0.41507842986773774,
            "precision": 0.5851063829787234,
            "recall": 0.7921810699588477
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.6370569463716281,
            "auditor_fn_violation": 0.01459371217878353,
            "auditor_fp_violation": 0.016534802873333955,
            "ave_precision_score": 0.551964120514968,
            "fpr": 0.29747530186608123,
            "logloss": 6.797539818051642,
            "mae": 0.39992374187757906,
            "precision": 0.5843558282208589,
            "recall": 0.8141025641025641
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 12498,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8337393921828773,
            "auditor_fn_violation": 0.0038783300844704403,
            "auditor_fp_violation": 0.02342269994234412,
            "ave_precision_score": 0.8341542461551497,
            "fpr": 0.20833333333333334,
            "logloss": 0.9174535870823194,
            "mae": 0.300624983945837,
            "precision": 0.6910569105691057,
            "recall": 0.8744855967078189
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.8467887146816562,
            "auditor_fn_violation": 0.011342846688620564,
            "auditor_fp_violation": 0.019111783989513666,
            "ave_precision_score": 0.846972957672982,
            "fpr": 0.2349066959385291,
            "logloss": 0.8591773147961344,
            "mae": 0.3074186681115511,
            "precision": 0.6624605678233438,
            "recall": 0.8974358974358975
        }
    }
]
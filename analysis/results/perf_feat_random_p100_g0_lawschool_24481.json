[
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(0)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8417343541826627,
            "auditor_fn_violation": 0.014350129481708437,
            "auditor_fp_violation": 0.025244738877355805,
            "ave_precision_score": 0.8422031770033636,
            "fpr": 0.12171052631578948,
            "logloss": 0.7751123367163728,
            "mae": 0.26249626334471154,
            "precision": 0.76875,
            "recall": 0.7671517671517671
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8200206472752407,
            "auditor_fn_violation": 0.019055332638668102,
            "auditor_fp_violation": 0.020841165060222846,
            "ave_precision_score": 0.8204063767336784,
            "fpr": 0.14709110867178923,
            "logloss": 0.8288097599381291,
            "mae": 0.2783502435671526,
            "precision": 0.7309236947791165,
            "recall": 0.7695560253699789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(1)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4725877192982456,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 1.6458545595389176,
            "mae": 0.52278868050903,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4807903402854007,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 1.57327785202868,
            "mae": 0.5146446705314349,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(2)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7554131490520665,
            "auditor_fn_violation": 0.008297771455666195,
            "auditor_fp_violation": 0.011377050514918403,
            "ave_precision_score": 0.7632983315032879,
            "fpr": 0.26206140350877194,
            "logloss": 0.6024020008923127,
            "mae": 0.41658144504681494,
            "precision": 0.6351145038167939,
            "recall": 0.8648648648648649
        },
        "train": {
            "accuracy": 0.6553238199780461,
            "auc_prc": 0.7165619514369246,
            "auditor_fn_violation": 0.018783809813345466,
            "auditor_fp_violation": 0.025322165917327055,
            "ave_precision_score": 0.7314435976569359,
            "fpr": 0.27332601536772777,
            "logloss": 0.6147674352209536,
            "mae": 0.4206454212788562,
            "precision": 0.6210045662100456,
            "recall": 0.8625792811839323
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(3)",
        "seed": 24481,
        "test": {
            "accuracy": 0.518640350877193,
            "auc_prc": 0.5755136616049736,
            "auditor_fn_violation": 0.010545464492832913,
            "auditor_fp_violation": 0.005797919973948797,
            "ave_precision_score": 0.5767642916837716,
            "fpr": 0.10635964912280702,
            "logloss": 0.7023107855786999,
            "mae": 0.49429284434830933,
            "precision": 0.5889830508474576,
            "recall": 0.288981288981289
        },
        "train": {
            "accuracy": 0.544456641053787,
            "auc_prc": 0.6012497833958401,
            "auditor_fn_violation": 0.006342494714587734,
            "auditor_fp_violation": 0.010713802384854827,
            "ave_precision_score": 0.6020462338804099,
            "fpr": 0.09989023051591657,
            "logloss": 0.6871833887719455,
            "mae": 0.48933948128270527,
            "precision": 0.6208333333333333,
            "recall": 0.3150105708245243
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(4)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7861842105263158,
            "auc_prc": 0.8807268702558357,
            "auditor_fn_violation": 0.012690575190575192,
            "auditor_fp_violation": 0.007108112508649817,
            "ave_precision_score": 0.8787420323857915,
            "fpr": 0.08442982456140351,
            "logloss": 0.4658400403565785,
            "mae": 0.3069172326328331,
            "precision": 0.825,
            "recall": 0.7546777546777547
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8343032457756833,
            "auditor_fn_violation": 0.0014109904085142157,
            "auditor_fp_violation": 0.013668556305730576,
            "ave_precision_score": 0.8305784215845959,
            "fpr": 0.10428100987925357,
            "logloss": 0.5171076318063461,
            "mae": 0.33053616063851293,
            "precision": 0.782608695652174,
            "recall": 0.7230443974630021
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(5)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7850877192982456,
            "auc_prc": 0.8529338079775396,
            "auditor_fn_violation": 0.010727833096254155,
            "auditor_fp_violation": 0.005716509952375137,
            "ave_precision_score": 0.8532843521208202,
            "fpr": 0.11513157894736842,
            "logloss": 0.5062754171011818,
            "mae": 0.3363078314024406,
            "precision": 0.7878787878787878,
            "recall": 0.8108108108108109
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8226672224708975,
            "auditor_fn_violation": 0.0003898789286683988,
            "auditor_fp_violation": 0.01224004932108326,
            "ave_precision_score": 0.8229602049229572,
            "fpr": 0.14489571899012074,
            "logloss": 0.5268931047622117,
            "mae": 0.35387092308319174,
            "precision": 0.7338709677419355,
            "recall": 0.7695560253699789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(6)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5482456140350878,
            "auc_prc": 0.5720962674916321,
            "auditor_fn_violation": 0.001545573913994967,
            "auditor_fp_violation": 0.003594761265111739,
            "ave_precision_score": 0.5737460920283453,
            "fpr": 0.44298245614035087,
            "logloss": 0.7054251544815975,
            "mae": 0.48041745781767786,
            "precision": 0.5393386545039909,
            "recall": 0.9833679833679834
        },
        "train": {
            "accuracy": 0.5433589462129528,
            "auc_prc": 0.5844368168522766,
            "auditor_fn_violation": 0.001896018361440974,
            "auditor_fp_violation": 0.004330631700825542,
            "ave_precision_score": 0.5853978354659375,
            "fpr": 0.45115257958287597,
            "logloss": 0.7043054236836586,
            "mae": 0.4805629174710629,
            "precision": 0.5324232081911263,
            "recall": 0.9894291754756871
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(7)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8366063260914055,
            "auditor_fn_violation": 0.020833333333333336,
            "auditor_fp_violation": 0.040799141124272395,
            "ave_precision_score": 0.8389816762218919,
            "fpr": 0.19517543859649122,
            "logloss": 0.5168031187705,
            "mae": 0.35577244233143956,
            "precision": 0.7003367003367004,
            "recall": 0.8648648648648649
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.760561761482447,
            "auditor_fn_violation": 0.028570235064504082,
            "auditor_fp_violation": 0.03690059095078418,
            "ave_precision_score": 0.7835830065631835,
            "fpr": 0.20197585071350166,
            "logloss": 0.5438978054073484,
            "mae": 0.3678578496895297,
            "precision": 0.6907563025210084,
            "recall": 0.86892177589852
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(8)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5427631578947368,
            "auc_prc": 0.5447739249375237,
            "auditor_fn_violation": 0.0012651821862348178,
            "auditor_fp_violation": 0.007125920950869052,
            "ave_precision_score": 0.5464008700071792,
            "fpr": 0.4473684210526316,
            "logloss": 0.7018317431884293,
            "mae": 0.49029064387605903,
            "precision": 0.5363636363636364,
            "recall": 0.9812889812889813
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.5116797903882783,
            "auditor_fn_violation": 0.0014666873983239853,
            "auditor_fp_violation": 0.007698900801467613,
            "ave_precision_score": 0.51373565547048,
            "fpr": 0.45334796926454446,
            "logloss": 0.7058074250935772,
            "mae": 0.49212686742045614,
            "precision": 0.5306818181818181,
            "recall": 0.9873150105708245
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(9)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4725877192982456,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.216174831087763,
            "mae": 0.5274122807017544,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4807903402854007,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.932866338960213,
            "mae": 0.5192096597145993,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(10)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7716110355133605,
            "auditor_fn_violation": 0.015072765072765087,
            "auditor_fp_violation": 0.023664875646192053,
            "ave_precision_score": 0.761102228837669,
            "fpr": 0.14144736842105263,
            "logloss": 0.6164931937120055,
            "mae": 0.3533663276262304,
            "precision": 0.7183406113537117,
            "recall": 0.683991683991684
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7662085133856048,
            "auditor_fn_violation": 0.011415562203094436,
            "auditor_fp_violation": 0.012688650637314615,
            "ave_precision_score": 0.7548503140232435,
            "fpr": 0.1525795828759605,
            "logloss": 0.6239157447084235,
            "mae": 0.3566920445929358,
            "precision": 0.7067510548523207,
            "recall": 0.7082452431289641
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(11)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.7630696030084327,
            "auditor_fn_violation": 0.008564485538169755,
            "auditor_fp_violation": 0.023987971669312505,
            "ave_precision_score": 0.7100241938435067,
            "fpr": 0.25548245614035087,
            "logloss": 4.846793959543849,
            "mae": 0.3537755299864771,
            "precision": 0.6260032102728732,
            "recall": 0.8108108108108109
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.7440308480273492,
            "auditor_fn_violation": 0.00546990854090132,
            "auditor_fp_violation": 0.009844167431043223,
            "ave_precision_score": 0.6893587006620887,
            "fpr": 0.2557628979143798,
            "logloss": 5.144926261533033,
            "mae": 0.33987562210440525,
            "precision": 0.6307448494453248,
            "recall": 0.8414376321353065
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(12)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.8108055783097963,
            "auditor_fn_violation": 0.04361345150818836,
            "auditor_fp_violation": 0.03203229942605935,
            "ave_precision_score": 0.8111574102004016,
            "fpr": 0.11074561403508772,
            "logloss": 0.577562843593449,
            "mae": 0.377415889504431,
            "precision": 0.7595238095238095,
            "recall": 0.6632016632016632
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.7609140386981242,
            "auditor_fn_violation": 0.0467669057769382,
            "auditor_fp_violation": 0.04077510287756443,
            "ave_precision_score": 0.7614546721612474,
            "fpr": 0.14270032930845225,
            "logloss": 0.608119862849735,
            "mae": 0.39720765312542705,
            "precision": 0.6997690531177829,
            "recall": 0.6405919661733616
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(13)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4934210526315789,
            "auc_prc": 0.6397560273344106,
            "auditor_fn_violation": 0.0010053069263595735,
            "auditor_fp_violation": 0.00308340456710221,
            "ave_precision_score": 0.6182973778147945,
            "fpr": 0.006578947368421052,
            "logloss": 0.8892441035323638,
            "mae": 0.49742878520893946,
            "precision": 0.8064516129032258,
            "recall": 0.05197505197505198
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0.6276738059448886,
            "auditor_fn_violation": 0.010359640104617528,
            "auditor_fp_violation": 0.0012480639971129122,
            "ave_precision_score": 0.606212756886622,
            "fpr": 0.006586169045005488,
            "logloss": 0.8818284093898714,
            "mae": 0.49368585179980057,
            "precision": 0.7272727272727273,
            "recall": 0.03382663847780127
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(14)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.5555820709938705,
            "auditor_fn_violation": 0.03479364992522889,
            "auditor_fp_violation": 0.039773883665079175,
            "ave_precision_score": 0.5628382895395583,
            "fpr": 0.14692982456140352,
            "logloss": 0.7736412474937753,
            "mae": 0.49296972386359256,
            "precision": 0.5772870662460567,
            "recall": 0.3804573804573805
        },
        "train": {
            "accuracy": 0.5236004390779363,
            "auc_prc": 0.5279784669562811,
            "auditor_fn_violation": 0.0392338879051666,
            "auditor_fp_violation": 0.040584635279611446,
            "ave_precision_score": 0.5431540891301894,
            "fpr": 0.15806805708013172,
            "logloss": 0.7759969893479637,
            "mae": 0.4940001866877341,
            "precision": 0.5596330275229358,
            "recall": 0.386892177589852
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(15)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8264833815400013,
            "auditor_fn_violation": 0.020931356457672256,
            "auditor_fp_violation": 0.013239304758415768,
            "ave_precision_score": 0.8272258916124146,
            "fpr": 0.12280701754385964,
            "logloss": 0.5469887849956587,
            "mae": 0.37082189046407366,
            "precision": 0.7632135306553911,
            "recall": 0.7505197505197505
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.8077495613650099,
            "auditor_fn_violation": 0.018746678486805616,
            "auditor_fp_violation": 0.015472986181074542,
            "ave_precision_score": 0.808255278945219,
            "fpr": 0.15477497255762898,
            "logloss": 0.5479050695986875,
            "mae": 0.3784442176713286,
            "precision": 0.7157258064516129,
            "recall": 0.7505285412262156
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(16)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.7054553434642656,
            "auditor_fn_violation": 0.009159463106831545,
            "auditor_fp_violation": 0.02424746611307852,
            "ave_precision_score": 0.6122463159988487,
            "fpr": 0.13706140350877194,
            "logloss": 0.9934426786400512,
            "mae": 0.48384368019833346,
            "precision": 0.6710526315789473,
            "recall": 0.5301455301455301
        },
        "train": {
            "accuracy": 0.6158068057080132,
            "auc_prc": 0.7079237304555742,
            "auditor_fn_violation": 0.015769210239891574,
            "auditor_fp_violation": 0.021367457107198173,
            "ave_precision_score": 0.607757883013476,
            "fpr": 0.16794731064763996,
            "logloss": 0.8305374028706654,
            "mae": 0.4739320042156106,
            "precision": 0.6433566433566433,
            "recall": 0.5835095137420718
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(17)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.7368947400342178,
            "auditor_fn_violation": 0.013638891928365612,
            "auditor_fp_violation": 0.011397403020311805,
            "ave_precision_score": 0.6202261823017783,
            "fpr": 0.15021929824561403,
            "logloss": 0.6843458818172119,
            "mae": 0.4533936764591521,
            "precision": 0.6768867924528302,
            "recall": 0.5966735966735967
        },
        "train": {
            "accuracy": 0.6125137211855104,
            "auc_prc": 0.7128768443961099,
            "auditor_fn_violation": 0.018208274251977834,
            "auditor_fp_violation": 0.009142444701742783,
            "ave_precision_score": 0.5918113519075149,
            "fpr": 0.1668496158068057,
            "logloss": 0.691350355129963,
            "mae": 0.4600470642335345,
            "precision": 0.6415094339622641,
            "recall": 0.5750528541226215
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(18)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.6817779710924333,
            "auditor_fn_violation": 0.023810500784185,
            "auditor_fp_violation": 0.02705356779419547,
            "ave_precision_score": 0.6831170278269791,
            "fpr": 0.15021929824561403,
            "logloss": 0.6236082589856305,
            "mae": 0.39351967746685995,
            "precision": 0.7078891257995735,
            "recall": 0.6902286902286903
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.6694768670278877,
            "auditor_fn_violation": 0.018579587517376296,
            "auditor_fp_violation": 0.020753449719060298,
            "ave_precision_score": 0.6710966619295721,
            "fpr": 0.15697036223929747,
            "logloss": 0.6321539158994343,
            "mae": 0.3981905569752799,
            "precision": 0.6911447084233261,
            "recall": 0.6765327695560254
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(19)",
        "seed": 24481,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.790511136405609,
            "auditor_fn_violation": 0.02329986869460554,
            "auditor_fp_violation": 0.03307790939064599,
            "ave_precision_score": 0.7702404741519164,
            "fpr": 0.20833333333333334,
            "logloss": 0.570768387209878,
            "mae": 0.3872566556133199,
            "precision": 0.6735395189003437,
            "recall": 0.814968814968815
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7734661062696895,
            "auditor_fn_violation": 0.03028059679324582,
            "auditor_fp_violation": 0.038253913357292155,
            "ave_precision_score": 0.7513067020465642,
            "fpr": 0.21953896816684962,
            "logloss": 0.583415345657048,
            "mae": 0.39163368730780257,
            "precision": 0.6604414261460102,
            "recall": 0.8224101479915433
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(20)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.7992321277984078,
            "auditor_fn_violation": 0.014924590582485322,
            "auditor_fp_violation": 0.02737157569096757,
            "ave_precision_score": 0.732289957548556,
            "fpr": 0.3223684210526316,
            "logloss": 0.5846928259677397,
            "mae": 0.405436433622973,
            "precision": 0.5939226519337016,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.6300768386388584,
            "auc_prc": 0.8039603997997044,
            "auditor_fn_violation": 0.022629222818128448,
            "auditor_fp_violation": 0.030903367767870133,
            "ave_precision_score": 0.7306359680024394,
            "fpr": 0.3238199780461032,
            "logloss": 0.5747650838809797,
            "mae": 0.40087124913865463,
            "precision": 0.59366391184573,
            "recall": 0.9112050739957717
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(21)",
        "seed": 24481,
        "test": {
            "accuracy": 0.48355263157894735,
            "auc_prc": 0.6262241492504651,
            "auditor_fn_violation": 0.005573640442061511,
            "auditor_fp_violation": 0.0031419180201082753,
            "ave_precision_score": 0.5338065798592114,
            "fpr": 0.007675438596491228,
            "logloss": 0.6923075350411094,
            "mae": 0.4964413739610137,
            "precision": 0.7083333333333334,
            "recall": 0.035343035343035345
        },
        "train": {
            "accuracy": 0.49396267837541163,
            "auc_prc": 0.6677365903695263,
            "auditor_fn_violation": 0.0021234477364975475,
            "auditor_fp_violation": 0.0023758326692028932,
            "ave_precision_score": 0.5287078530434923,
            "fpr": 0.0043907793633369925,
            "logloss": 0.689139815421237,
            "mae": 0.4956578364683689,
            "precision": 0.8,
            "recall": 0.03382663847780127
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(22)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.7406926151165023,
            "auditor_fn_violation": 0.051382354013932964,
            "auditor_fp_violation": 0.06646365042536738,
            "ave_precision_score": 0.546893028683551,
            "fpr": 0.3684210526315789,
            "logloss": 0.6942336394390515,
            "mae": 0.49399799466329186,
            "precision": 0.5502008032128514,
            "recall": 0.8544698544698545
        },
        "train": {
            "accuracy": 0.544456641053787,
            "auc_prc": 0.7389004455153109,
            "auditor_fn_violation": 0.0450472612165615,
            "auditor_fp_violation": 0.0572655870161246,
            "ave_precision_score": 0.5355074185204415,
            "fpr": 0.3907793633369923,
            "logloss": 0.6925202147814352,
            "mae": 0.4941208342133186,
            "precision": 0.5376623376623376,
            "recall": 0.8752642706131079
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(23)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5208333333333334,
            "auc_prc": 0.7462384920047459,
            "auditor_fn_violation": 0.002076722471459315,
            "auditor_fp_violation": 0.003495542801318843,
            "ave_precision_score": 0.5259904975230052,
            "fpr": 0.4418859649122807,
            "logloss": 0.7137757232511267,
            "mae": 0.494782142275781,
            "precision": 0.5258823529411765,
            "recall": 0.9293139293139293
        },
        "train": {
            "accuracy": 0.5159165751920965,
            "auc_prc": 0.7412047264959462,
            "auditor_fn_violation": 0.005876032424930903,
            "auditor_fp_violation": 0.00023307219223193238,
            "ave_precision_score": 0.5190181517007952,
            "fpr": 0.4445664105378705,
            "logloss": 0.7162896831859813,
            "mae": 0.49602972849402027,
            "precision": 0.5190023752969121,
            "recall": 0.9238900634249472
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(24)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.7123704147938987,
            "auditor_fn_violation": 0.021314330524856852,
            "auditor_fp_violation": 0.037611429967028945,
            "ave_precision_score": 0.6874366217213018,
            "fpr": 0.16447368421052633,
            "logloss": 0.6829974009878201,
            "mae": 0.408015073783565,
            "precision": 0.6543778801843319,
            "recall": 0.5904365904365905
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.6880787332704116,
            "auditor_fn_violation": 0.02174967452071581,
            "auditor_fp_violation": 0.027713035502157802,
            "ave_precision_score": 0.6668321705036546,
            "fpr": 0.19099890230515917,
            "logloss": 0.7126634898954276,
            "mae": 0.4195555742302527,
            "precision": 0.6124721603563474,
            "recall": 0.5813953488372093
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(25)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5778508771929824,
            "auc_prc": 0.7038343860182777,
            "auditor_fn_violation": 0.01104469854469855,
            "auditor_fp_violation": 0.020639984532095895,
            "ave_precision_score": 0.6580039581327324,
            "fpr": 0.2719298245614035,
            "logloss": 8.016320670215823,
            "mae": 0.41702405555828276,
            "precision": 0.581081081081081,
            "recall": 0.7151767151767152
        },
        "train": {
            "accuracy": 0.601536772777168,
            "auc_prc": 0.7070418869145273,
            "auditor_fn_violation": 0.008704975365685553,
            "auditor_fp_violation": 0.026615340661323542,
            "ave_precision_score": 0.6594201583731436,
            "fpr": 0.2502744237102086,
            "logloss": 7.818734416129684,
            "mae": 0.3953017410421301,
            "precision": 0.5971731448763251,
            "recall": 0.7145877378435518
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(26)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4725877192982456,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 1.0213770544785172,
            "mae": 0.46472956708173707,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4807903402854007,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 1.0019776998653058,
            "mae": 0.4636470421372837,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(27)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5241228070175439,
            "auc_prc": 0.5338195678230042,
            "auditor_fn_violation": 0.005402669876354087,
            "auditor_fp_violation": 0.0030783164407538616,
            "ave_precision_score": 0.5329115243478629,
            "fpr": 0.43969298245614036,
            "logloss": 2.4734024981926916,
            "mae": 0.5199841961035865,
            "precision": 0.5276796230859835,
            "recall": 0.9313929313929314
        },
        "train": {
            "accuracy": 0.5115257958287596,
            "auc_prc": 0.5117751017416796,
            "auditor_fn_violation": 0.01147822131663043,
            "auditor_fp_violation": 0.0035938228350600823,
            "ave_precision_score": 0.5109085348500719,
            "fpr": 0.4434687156970362,
            "logloss": 2.490521939575323,
            "mae": 0.5268189491035516,
            "precision": 0.5167464114832536,
            "recall": 0.9133192389006343
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(28)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5274122807017544,
            "auc_prc": 0.7553209396525509,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.544620170952105,
            "fpr": 0.4725877192982456,
            "logloss": 0.694661434567249,
            "mae": 0.49317891830415056,
            "precision": 0.5274122807017544,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5192096597145993,
            "auc_prc": 0.7593390117590048,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5390997956277036,
            "fpr": 0.4807903402854007,
            "logloss": 0.6970551697823014,
            "mae": 0.4941692441276609,
            "precision": 0.5192096597145993,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(29)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6293859649122807,
            "auc_prc": 0.6424116381892042,
            "auditor_fn_violation": 0.030663001057737903,
            "auditor_fp_violation": 0.06394757194610658,
            "ave_precision_score": 0.644124293781962,
            "fpr": 0.2598684210526316,
            "logloss": 0.6572039884953083,
            "mae": 0.468445205244056,
            "precision": 0.6158833063209076,
            "recall": 0.7900207900207901
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.6657958843389857,
            "auditor_fn_violation": 0.028549348693325417,
            "auditor_fp_violation": 0.05912515224877073,
            "ave_precision_score": 0.6666292198998987,
            "fpr": 0.27661909989023054,
            "logloss": 0.6624047664351865,
            "mae": 0.47098317222197417,
            "precision": 0.592891760904685,
            "recall": 0.7758985200845666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(30)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5274122807017544,
            "auc_prc": 0.4474893438709229,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5260503188134767,
            "fpr": 0.4725877192982456,
            "logloss": 0.6935689747707534,
            "mae": 0.4999395435708657,
            "precision": 0.5274122807017544,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5192096597145993,
            "auc_prc": 0.581344293263217,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5209434394958747,
            "fpr": 0.4807903402854007,
            "logloss": 0.6922539223358148,
            "mae": 0.49929988508559475,
            "precision": 0.5192096597145993,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(31)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.776970319066257,
            "auditor_fn_violation": 0.017986103512419305,
            "auditor_fp_violation": 0.0268093377294745,
            "ave_precision_score": 0.6587515272812675,
            "fpr": 0.14692982456140352,
            "logloss": 0.632065159797013,
            "mae": 0.41803009942043245,
            "precision": 0.7048458149779736,
            "recall": 0.6652806652806653
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.7721033889167005,
            "auditor_fn_violation": 0.01747725126072458,
            "auditor_fp_violation": 0.020781017397711384,
            "ave_precision_score": 0.6472993092985375,
            "fpr": 0.15806805708013172,
            "logloss": 0.6306291697168634,
            "mae": 0.41945281621522623,
            "precision": 0.6909871244635193,
            "recall": 0.6807610993657506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(32)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5668859649122807,
            "auc_prc": 0.5575101724550187,
            "auditor_fn_violation": 0.030063464273990592,
            "auditor_fp_violation": 0.05058106402898198,
            "ave_precision_score": 0.5556798310379631,
            "fpr": 0.38706140350877194,
            "logloss": 1.1960249974325976,
            "mae": 0.4694722731155548,
            "precision": 0.5542929292929293,
            "recall": 0.9126819126819127
        },
        "train": {
            "accuracy": 0.5521405049396267,
            "auc_prc": 0.5488193662803666,
            "auditor_fn_violation": 0.024283887557060405,
            "auditor_fp_violation": 0.04056959836398359,
            "ave_precision_score": 0.5450223576613291,
            "fpr": 0.4094401756311745,
            "logloss": 1.2784214250206598,
            "mae": 0.47617196912430515,
            "precision": 0.5400739827373613,
            "recall": 0.9260042283298098
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(33)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.8488080561686787,
            "auditor_fn_violation": 0.016317430791115025,
            "auditor_fp_violation": 0.004233321121830099,
            "ave_precision_score": 0.8094385566901079,
            "fpr": 0.02631578947368421,
            "logloss": 0.589149698336323,
            "mae": 0.4112405475686517,
            "precision": 0.9,
            "recall": 0.4490644490644491
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.8202384028927444,
            "auditor_fn_violation": 0.01210017103617289,
            "auditor_fp_violation": 0.005413289626031909,
            "ave_precision_score": 0.7828716806769838,
            "fpr": 0.026344676180021953,
            "logloss": 0.5933988496212181,
            "mae": 0.41580232009537005,
            "precision": 0.8928571428571429,
            "recall": 0.42283298097251587
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(34)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.6368137075719321,
            "auditor_fn_violation": 0.009918572418572419,
            "auditor_fp_violation": 0.027038303415150403,
            "ave_precision_score": 0.6458185922825875,
            "fpr": 0.16228070175438597,
            "logloss": 0.6251804217459187,
            "mae": 0.4354090803188452,
            "precision": 0.6985743380855397,
            "recall": 0.7130977130977131
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.6213704182765256,
            "auditor_fn_violation": 0.011190453535946616,
            "auditor_fp_violation": 0.015500553859725635,
            "ave_precision_score": 0.6322866538718547,
            "fpr": 0.17233809001097694,
            "logloss": 0.6305121157752095,
            "mae": 0.4380125062853,
            "precision": 0.6821862348178138,
            "recall": 0.7124735729386892
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(35)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7872807017543859,
            "auc_prc": 0.854555769837673,
            "auditor_fn_violation": 0.010752908779224572,
            "auditor_fp_violation": 0.008184251231326578,
            "ave_precision_score": 0.826993716683627,
            "fpr": 0.07346491228070176,
            "logloss": 0.4992833700347409,
            "mae": 0.33668638318532,
            "precision": 0.8408551068883611,
            "recall": 0.735966735966736
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8244701644616157,
            "auditor_fn_violation": 0.0015432707593124233,
            "auditor_fp_violation": 0.009884265872717525,
            "ave_precision_score": 0.7912569612766097,
            "fpr": 0.09659714599341383,
            "logloss": 0.5362789817811349,
            "mae": 0.35482585656688714,
            "precision": 0.7889688249400479,
            "recall": 0.6955602536997886
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(36)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8463689497548799,
            "auditor_fn_violation": 0.012031768610715981,
            "auditor_fp_violation": 0.023191679895795174,
            "ave_precision_score": 0.8468819101367895,
            "fpr": 0.14364035087719298,
            "logloss": 0.6805664190359174,
            "mae": 0.2747710887114201,
            "precision": 0.7421259842519685,
            "recall": 0.7837837837837838
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8281107419052014,
            "auditor_fn_violation": 0.011494466271991617,
            "auditor_fp_violation": 0.020936398859199335,
            "ave_precision_score": 0.8283861155263922,
            "fpr": 0.15367727771679474,
            "logloss": 0.7237819632703486,
            "mae": 0.28314714747916553,
            "precision": 0.7238658777120316,
            "recall": 0.7758985200845666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(37)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7807017543859649,
            "auc_prc": 0.8391340833115998,
            "auditor_fn_violation": 0.004148885727833099,
            "auditor_fp_violation": 0.011417755525705217,
            "ave_precision_score": 0.8394755682717603,
            "fpr": 0.11842105263157894,
            "logloss": 0.5178267480671668,
            "mae": 0.3359247334631786,
            "precision": 0.7826961770623743,
            "recall": 0.8087318087318087
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8360691341183811,
            "auditor_fn_violation": 0.012578236865373417,
            "auditor_fp_violation": 0.021515320110872194,
            "ave_precision_score": 0.8363273479014371,
            "fpr": 0.1394072447859495,
            "logloss": 0.5312335426474597,
            "mae": 0.3522943340701157,
            "precision": 0.7449799196787149,
            "recall": 0.7843551797040169
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(38)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8686513403740395,
            "auditor_fn_violation": 0.008074369916475184,
            "auditor_fp_violation": 0.016030142060487657,
            "ave_precision_score": 0.8691595941425633,
            "fpr": 0.13925438596491227,
            "logloss": 0.49304778765281265,
            "mae": 0.27787636493214224,
            "precision": 0.7571701720841301,
            "recall": 0.8232848232848233
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.847170145049262,
            "auditor_fn_violation": 0.002200031097485975,
            "auditor_fp_violation": 0.02414928649835346,
            "ave_precision_score": 0.8474048792538402,
            "fpr": 0.1602634467618002,
            "logloss": 0.5365500460840246,
            "mae": 0.29937548277967096,
            "precision": 0.7291280148423006,
            "recall": 0.8308668076109936
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(39)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5635964912280702,
            "auc_prc": 0.586570894629242,
            "auditor_fn_violation": 0.017806014516540843,
            "auditor_fp_violation": 0.0051415516750111965,
            "ave_precision_score": 0.5880623310884538,
            "fpr": 0.23464912280701755,
            "logloss": 0.6863180030097703,
            "mae": 0.48840195492008015,
            "precision": 0.5812133072407045,
            "recall": 0.6174636174636174
        },
        "train": {
            "accuracy": 0.566410537870472,
            "auc_prc": 0.5974930843339022,
            "auditor_fn_violation": 0.006126668879074883,
            "auditor_fp_violation": 0.005618794139612755,
            "ave_precision_score": 0.5984447249436731,
            "fpr": 0.2283205268935236,
            "logloss": 0.6790938354706346,
            "mae": 0.48786885979760164,
            "precision": 0.5789473684210527,
            "recall": 0.6046511627906976
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(40)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.6724662449183354,
            "auditor_fn_violation": 0.006458128168654486,
            "auditor_fp_violation": 0.0183554158016852,
            "ave_precision_score": 0.673623473868171,
            "fpr": 0.16557017543859648,
            "logloss": 0.6581571665952056,
            "mae": 0.44912792095228243,
            "precision": 0.6759656652360515,
            "recall": 0.6548856548856549
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6621222803564029,
            "auditor_fn_violation": 0.010102041526747318,
            "auditor_fp_violation": 0.007353051742026681,
            "ave_precision_score": 0.6629013954764282,
            "fpr": 0.18660812294182216,
            "logloss": 0.6419325624241443,
            "mae": 0.45524647239796023,
            "precision": 0.6336206896551724,
            "recall": 0.6215644820295984
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(41)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.6883128190208794,
            "auditor_fn_violation": 0.016116825327351654,
            "auditor_fp_violation": 0.014625819188342085,
            "ave_precision_score": 0.6576503750378048,
            "fpr": 0.11513157894736842,
            "logloss": 0.6411193385522702,
            "mae": 0.4388063668248881,
            "precision": 0.6974063400576369,
            "recall": 0.5031185031185031
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6839411940673463,
            "auditor_fn_violation": 0.02224398530527753,
            "auditor_fp_violation": 0.017939040344044633,
            "ave_precision_score": 0.6544938301548785,
            "fpr": 0.12184412733260154,
            "logloss": 0.6335454107278135,
            "mae": 0.43812500400025023,
            "precision": 0.6791907514450867,
            "recall": 0.49682875264270615
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(42)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7962096402805721,
            "auditor_fn_violation": 0.00910931174089069,
            "auditor_fp_violation": 0.018383400496601133,
            "ave_precision_score": 0.8088809778620494,
            "fpr": 0.13267543859649122,
            "logloss": 0.5303976878589178,
            "mae": 0.32122632436323584,
            "precision": 0.7535641547861507,
            "recall": 0.7692307692307693
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7659269474599838,
            "auditor_fn_violation": 0.004954711385160934,
            "auditor_fp_violation": 0.01783378193464957,
            "ave_precision_score": 0.7802806932500198,
            "fpr": 0.15148188803512624,
            "logloss": 0.5808327864244748,
            "mae": 0.34583881032028524,
            "precision": 0.7148760330578512,
            "recall": 0.7315010570824524
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(43)",
        "seed": 24481,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7733835453034439,
            "auditor_fn_violation": 0.00813136010504432,
            "auditor_fp_violation": 0.027877844262628734,
            "ave_precision_score": 0.7501803385633355,
            "fpr": 0.16447368421052633,
            "logloss": 1.903428067998683,
            "mae": 0.3583325929939747,
            "precision": 0.701195219123506,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.7523825994302136,
            "auditor_fn_violation": 0.010777367528190801,
            "auditor_fp_violation": 0.014139712995403718,
            "ave_precision_score": 0.7236580982351039,
            "fpr": 0.1756311745334797,
            "logloss": 2.1774367471441756,
            "mae": 0.36893975263631174,
            "precision": 0.6825396825396826,
            "recall": 0.7272727272727273
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(44)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.8074254701978048,
            "auditor_fn_violation": 0.020482273771747463,
            "auditor_fp_violation": 0.023049212358041273,
            "ave_precision_score": 0.8079249298553087,
            "fpr": 0.1074561403508772,
            "logloss": 0.8219131184153485,
            "mae": 0.326318762528971,
            "precision": 0.7586206896551724,
            "recall": 0.6403326403326404
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.8059586705087416,
            "auditor_fn_violation": 0.019802600585282534,
            "auditor_fp_violation": 0.01687141933446612,
            "ave_precision_score": 0.8063078678331312,
            "fpr": 0.10867178924259056,
            "logloss": 0.7761986768261501,
            "mae": 0.32791433589457003,
            "precision": 0.7561576354679803,
            "recall": 0.6490486257928119
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(45)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7730263157894737,
            "auc_prc": 0.855456601849897,
            "auditor_fn_violation": 0.012904858299595149,
            "auditor_fp_violation": 0.008280925631945291,
            "ave_precision_score": 0.8521819030647694,
            "fpr": 0.10526315789473684,
            "logloss": 0.5090815425089396,
            "mae": 0.3199757024969522,
            "precision": 0.7939914163090128,
            "recall": 0.7692307692307693
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8171996866213167,
            "auditor_fn_violation": 0.013840701967728233,
            "auditor_fp_violation": 0.017793683492975252,
            "ave_precision_score": 0.8127638639361845,
            "fpr": 0.13611416026344675,
            "logloss": 0.5357916792128684,
            "mae": 0.3398223797137451,
            "precision": 0.7389473684210527,
            "recall": 0.7420718816067653
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(46)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.850172834787124,
            "auditor_fn_violation": 0.010983149141043883,
            "auditor_fp_violation": 0.013328346969511946,
            "ave_precision_score": 0.8504096061539059,
            "fpr": 0.14692982456140352,
            "logloss": 0.556835752358949,
            "mae": 0.3054598492443594,
            "precision": 0.7462121212121212,
            "recall": 0.8191268191268192
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8279649030044941,
            "auditor_fn_violation": 0.012076963957085473,
            "auditor_fp_violation": 0.01908184593176248,
            "ave_precision_score": 0.8282230271971157,
            "fpr": 0.1756311745334797,
            "logloss": 0.6035200801765557,
            "mae": 0.32759284688329526,
            "precision": 0.7026022304832714,
            "recall": 0.7991543340380549
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(47)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.8227721202098299,
            "auditor_fn_violation": 0.012556078345552035,
            "auditor_fp_violation": 0.02368014002523711,
            "ave_precision_score": 0.8230056934281178,
            "fpr": 0.15460526315789475,
            "logloss": 0.6692818935020679,
            "mae": 0.3424923136215416,
            "precision": 0.7098765432098766,
            "recall": 0.7172557172557172
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.8087909979827016,
            "auditor_fn_violation": 0.009150551284163727,
            "auditor_fp_violation": 0.015573232285260322,
            "ave_precision_score": 0.8090543277020893,
            "fpr": 0.1668496158068057,
            "logloss": 0.6654224541134309,
            "mae": 0.3464154187955714,
            "precision": 0.692929292929293,
            "recall": 0.7251585623678647
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(48)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7559379717775204,
            "auditor_fn_violation": 0.013390414706204189,
            "auditor_fp_violation": 0.027038303415150403,
            "ave_precision_score": 0.7532725204580004,
            "fpr": 0.16228070175438597,
            "logloss": 1.1791593217449483,
            "mae": 0.3239649960043378,
            "precision": 0.6991869918699187,
            "recall": 0.7151767151767152
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.7309493900103784,
            "auditor_fn_violation": 0.01226726200560219,
            "auditor_fp_violation": 0.01485647264033202,
            "ave_precision_score": 0.7284936171603462,
            "fpr": 0.1712403951701427,
            "logloss": 1.2357186704625118,
            "mae": 0.32626844616169554,
            "precision": 0.6829268292682927,
            "recall": 0.7103594080338267
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(49)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5274122807017544,
            "auc_prc": 0.7637061403508771,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5274122807017544,
            "fpr": 0.4725877192982456,
            "logloss": 0.6917039628314521,
            "mae": 0.49879808410217885,
            "precision": 0.5274122807017544,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5192096597145993,
            "auc_prc": 0.7596048298572997,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5192096597145993,
            "fpr": 0.4807903402854007,
            "logloss": 0.6924237267748764,
            "mae": 0.4991577353357091,
            "precision": 0.5192096597145993,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(50)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5592105263157895,
            "auc_prc": 0.656863037098704,
            "auditor_fn_violation": 0.0006747638326585696,
            "auditor_fp_violation": 0.009105202100378566,
            "ave_precision_score": 0.6577954738030476,
            "fpr": 0.4375,
            "logloss": 0.7301019234225558,
            "mae": 0.4617549881580073,
            "precision": 0.5450399087799316,
            "recall": 0.9937629937629938
        },
        "train": {
            "accuracy": 0.5433589462129528,
            "auc_prc": 0.6561374066729077,
            "auditor_fn_violation": 0.0011603539543702413,
            "auditor_fp_violation": 0.003999819557012466,
            "ave_precision_score": 0.6580037577843056,
            "fpr": 0.4544456641053787,
            "logloss": 0.7338711623362842,
            "mae": 0.4665747052390828,
            "precision": 0.5322033898305085,
            "recall": 0.9957716701902748
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(51)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7763157894736842,
            "auc_prc": 0.8640476891285015,
            "auditor_fn_violation": 0.00670204617573039,
            "auditor_fp_violation": 0.00797054992469573,
            "ave_precision_score": 0.8564433110897518,
            "fpr": 0.09868421052631579,
            "logloss": 1.7336369970446426,
            "mae": 0.22287198235661013,
            "precision": 0.8030634573304157,
            "recall": 0.762993762993763
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8271095829748861,
            "auditor_fn_violation": 0.003775791767520769,
            "auditor_fp_violation": 0.015924093649910532,
            "ave_precision_score": 0.817825604572028,
            "fpr": 0.13391877058177826,
            "logloss": 2.018412528974481,
            "mae": 0.269037956639621,
            "precision": 0.7393162393162394,
            "recall": 0.7315010570824524
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(52)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4934210526315789,
            "auc_prc": 0.588931788931789,
            "auditor_fn_violation": 0.0037339971550497905,
            "auditor_fp_violation": 0.011590751821549235,
            "ave_precision_score": 0.5362760443841526,
            "fpr": 0.05043859649122807,
            "logloss": 0.6979147994636877,
            "mae": 0.4965178033269599,
            "precision": 0.5855855855855856,
            "recall": 0.13513513513513514
        },
        "train": {
            "accuracy": 0.4994511525795829,
            "auc_prc": 0.5809905824012894,
            "auditor_fn_violation": 0.004462721308507957,
            "auditor_fp_violation": 0.0053080312166368425,
            "ave_precision_score": 0.5283865231815944,
            "fpr": 0.052689352360043906,
            "logloss": 0.6971125576280431,
            "mae": 0.49585258885942046,
            "precision": 0.5752212389380531,
            "recall": 0.13742071881606766
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(53)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.7941134159035431,
            "auditor_fn_violation": 0.012061403508771936,
            "auditor_fp_violation": 0.02475373468473969,
            "ave_precision_score": 0.7935090855427882,
            "fpr": 0.18859649122807018,
            "logloss": 0.6647092550618544,
            "mae": 0.36290127377050896,
            "precision": 0.7029360967184801,
            "recall": 0.8461538461538461
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7767351863159592,
            "auditor_fn_violation": 0.022399472735163132,
            "auditor_fp_violation": 0.04099564430677313,
            "ave_precision_score": 0.7771298468230203,
            "fpr": 0.21185510428100987,
            "logloss": 0.598810516826969,
            "mae": 0.38214162205594865,
            "precision": 0.6761744966442953,
            "recall": 0.8520084566596194
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(54)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7839912280701754,
            "auc_prc": 0.8602915929961572,
            "auditor_fn_violation": 0.01105381697486961,
            "auditor_fp_violation": 0.00494057068425123,
            "ave_precision_score": 0.8602658442168588,
            "fpr": 0.08771929824561403,
            "logloss": 0.48420990338529346,
            "mae": 0.3256870686064327,
            "precision": 0.8198198198198198,
            "recall": 0.7567567567567568
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8344202797728687,
            "auditor_fn_violation": 0.0039312791974063805,
            "auditor_fp_violation": 0.016821296282373232,
            "ave_precision_score": 0.8341536089440569,
            "fpr": 0.1163556531284303,
            "logloss": 0.5174848082684566,
            "mae": 0.3469319307295407,
            "precision": 0.7628635346756152,
            "recall": 0.7209302325581395
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(55)",
        "seed": 24481,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8159805825061908,
            "auditor_fn_violation": 0.004071379071379071,
            "auditor_fp_violation": 0.00456913746082143,
            "ave_precision_score": 0.8160522363482223,
            "fpr": 0.16337719298245615,
            "logloss": 0.5676012148503151,
            "mae": 0.40233231434624767,
            "precision": 0.726605504587156,
            "recall": 0.8232848232848233
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7785045925354583,
            "auditor_fn_violation": 0.008212985289032573,
            "auditor_fp_violation": 0.009202592364254244,
            "ave_precision_score": 0.7787880773405236,
            "fpr": 0.19319429198682767,
            "logloss": 0.5740451450419521,
            "mae": 0.409086634051686,
            "precision": 0.6805807622504537,
            "recall": 0.7928118393234672
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(56)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5866228070175439,
            "auc_prc": 0.6962096973015286,
            "auditor_fn_violation": 0.0017233833023306713,
            "auditor_fp_violation": 0.012743212439451316,
            "ave_precision_score": 0.5433788053556416,
            "fpr": 0.3892543859649123,
            "logloss": 0.7249833376026797,
            "mae": 0.4750743162517689,
            "precision": 0.5638820638820639,
            "recall": 0.9542619542619543
        },
        "train": {
            "accuracy": 0.5762897914379802,
            "auc_prc": 0.6753300772344157,
            "auditor_fn_violation": 0.0065884897529142296,
            "auditor_fp_violation": 0.017074417695442306,
            "ave_precision_score": 0.5199469271394022,
            "fpr": 0.40285400658616904,
            "logloss": 0.6987972584685164,
            "mae": 0.4779020368289598,
            "precision": 0.5529841656516443,
            "recall": 0.959830866807611
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(57)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8296825311450557,
            "auditor_fn_violation": 0.012986924171134706,
            "auditor_fp_violation": 0.020416106972768352,
            "ave_precision_score": 0.8299563774720371,
            "fpr": 0.18311403508771928,
            "logloss": 0.87500022782291,
            "mae": 0.2892172417526198,
            "precision": 0.7012522361359571,
            "recall": 0.814968814968815
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.8172632446371118,
            "auditor_fn_violation": 0.00806678069078192,
            "auditor_fp_violation": 0.021277235613430978,
            "ave_precision_score": 0.8175773771418022,
            "fpr": 0.1877058177826564,
            "logloss": 0.8949463642196734,
            "mae": 0.29378957942634937,
            "precision": 0.6924460431654677,
            "recall": 0.813953488372093
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(58)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5274122807017544,
            "auc_prc": 0.7637061403508771,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5274122807017544,
            "fpr": 0.4725877192982456,
            "logloss": 0.6922114314831689,
            "mae": 0.49757610718932066,
            "precision": 0.5274122807017544,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5192096597145993,
            "auc_prc": 0.7596048298572997,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5192096597145993,
            "fpr": 0.4807903402854007,
            "logloss": 0.6936658406794762,
            "mae": 0.49830141254628135,
            "precision": 0.5192096597145993,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(59)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7315969629214956,
            "auditor_fn_violation": 0.019841704052230374,
            "auditor_fp_violation": 0.01982842837953352,
            "ave_precision_score": 0.7329301624558926,
            "fpr": 0.14473684210526316,
            "logloss": 2.077096662760477,
            "mae": 0.3565196098392439,
            "precision": 0.7375745526838966,
            "recall": 0.7713097713097713
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7261154329336608,
            "auditor_fn_violation": 0.02299589466770944,
            "auditor_fp_violation": 0.020555463663293392,
            "ave_precision_score": 0.7263907585607615,
            "fpr": 0.1778265642151482,
            "logloss": 1.7015991405064208,
            "mae": 0.36280111568165857,
            "precision": 0.6954887218045113,
            "recall": 0.7822410147991543
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(60)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5383771929824561,
            "auc_prc": 0.6104640051078266,
            "auditor_fn_violation": 0.0026215486741802537,
            "auditor_fp_violation": 0.005645276183498197,
            "ave_precision_score": 0.6115519215899675,
            "fpr": 0.43859649122807015,
            "logloss": 0.7047734451096813,
            "mae": 0.4806462618473329,
            "precision": 0.5348837209302325,
            "recall": 0.9563409563409564
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.6092747596267059,
            "auditor_fn_violation": 0.0022580487952044894,
            "auditor_fp_violation": 0.0001027522567904453,
            "ave_precision_score": 0.6100178641921141,
            "fpr": 0.44017563117453345,
            "logloss": 0.707165351364504,
            "mae": 0.4823727867475064,
            "precision": 0.5315420560747663,
            "recall": 0.9619450317124736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(61)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.7105742381404886,
            "auditor_fn_violation": 0.0026375059269796147,
            "auditor_fp_violation": 0.002383787194203619,
            "ave_precision_score": 0.6705654892995274,
            "fpr": 0.2598684210526316,
            "logloss": 0.6998504286230053,
            "mae": 0.3826982674205251,
            "precision": 0.6441441441441441,
            "recall": 0.8918918918918919
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.6264123653329341,
            "auditor_fn_violation": 0.0032954052304114867,
            "auditor_fp_violation": 0.0021903773764592144,
            "ave_precision_score": 0.6628830223533745,
            "fpr": 0.29747530186608123,
            "logloss": 0.7241779989480438,
            "mae": 0.3978131670488452,
            "precision": 0.6072463768115942,
            "recall": 0.8858350951374208
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(62)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7233482637631268,
            "auditor_fn_violation": 0.017213316555421822,
            "auditor_fp_violation": 0.009186612121952216,
            "ave_precision_score": 0.7238955795426332,
            "fpr": 0.12828947368421054,
            "logloss": 0.6262137846854118,
            "mae": 0.42163545927910295,
            "precision": 0.7153284671532847,
            "recall": 0.6112266112266113
        },
        "train": {
            "accuracy": 0.6586169045005489,
            "auc_prc": 0.711602739151078,
            "auditor_fn_violation": 0.007147780358920696,
            "auditor_fp_violation": 0.010921813051040303,
            "ave_precision_score": 0.7131347266697904,
            "fpr": 0.13391877058177826,
            "logloss": 0.6196854718932178,
            "mae": 0.4215994081272727,
            "precision": 0.6995073891625616,
            "recall": 0.6004228329809725
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(63)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6041666666666666,
            "auc_prc": 0.592582122028587,
            "auditor_fn_violation": 0.03609530583214794,
            "auditor_fp_violation": 0.06451998616029635,
            "ave_precision_score": 0.5746327584187164,
            "fpr": 0.33881578947368424,
            "logloss": 0.6640090075698785,
            "mae": 0.468962988080947,
            "precision": 0.5813008130081301,
            "recall": 0.8918918918918919
        },
        "train": {
            "accuracy": 0.5960482985729967,
            "auc_prc": 0.5992796341859896,
            "auditor_fn_violation": 0.03135740526290139,
            "auditor_fp_violation": 0.057967309745425015,
            "ave_precision_score": 0.5727116663038949,
            "fpr": 0.34906695938529086,
            "logloss": 0.6646020924746866,
            "mae": 0.4691393590817467,
            "precision": 0.5708502024291497,
            "recall": 0.8942917547568711
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(64)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7861842105263158,
            "auc_prc": 0.8403607836123641,
            "auditor_fn_violation": 0.010059908086223878,
            "auditor_fp_violation": 0.007896772092644605,
            "ave_precision_score": 0.8270014963374421,
            "fpr": 0.07236842105263158,
            "logloss": 0.4864843142291464,
            "mae": 0.32634733224352985,
            "precision": 0.8421052631578947,
            "recall": 0.7318087318087318
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8260508316692041,
            "auditor_fn_violation": 0.001865849158627352,
            "auditor_fp_violation": 0.01502187871223855,
            "ave_precision_score": 0.803607943937714,
            "fpr": 0.0889132821075741,
            "logloss": 0.5187864996077709,
            "mae": 0.3431359400439079,
            "precision": 0.8048192771084337,
            "recall": 0.7061310782241015
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(65)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7839254832371991,
            "auditor_fn_violation": 0.005074406390195863,
            "auditor_fp_violation": 0.014923474579720764,
            "ave_precision_score": 0.7771875703768769,
            "fpr": 0.13157894736842105,
            "logloss": 0.5966478360572419,
            "mae": 0.39827213285229446,
            "precision": 0.7254004576659039,
            "recall": 0.659043659043659
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.7703482075541976,
            "auditor_fn_violation": 0.014908227605748864,
            "auditor_fp_violation": 0.004320607090406949,
            "ave_precision_score": 0.7609145767928782,
            "fpr": 0.14270032930845225,
            "logloss": 0.5984685759042534,
            "mae": 0.40356244514854756,
            "precision": 0.7065462753950339,
            "recall": 0.6617336152219874
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(66)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5767543859649122,
            "auc_prc": 0.7242879396420556,
            "auditor_fn_violation": 0.0028130357077725543,
            "auditor_fp_violation": 0.005724142141897668,
            "ave_precision_score": 0.7260777673857648,
            "fpr": 0.03728070175438596,
            "logloss": 0.7139050798320205,
            "mae": 0.439160543380392,
            "precision": 0.7914110429447853,
            "recall": 0.2681912681912682
        },
        "train": {
            "accuracy": 0.5784851811196488,
            "auc_prc": 0.7157896842671578,
            "auditor_fn_violation": 0.008127119096409166,
            "auditor_fp_violation": 0.0041025718138028855,
            "ave_precision_score": 0.7175288469967096,
            "fpr": 0.038419319429198684,
            "logloss": 0.7072832244489972,
            "mae": 0.4430377657431828,
            "precision": 0.779874213836478,
            "recall": 0.26215644820295986
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(67)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5427631578947368,
            "auc_prc": 0.5810057639452,
            "auditor_fn_violation": 0.0014179158916001022,
            "auditor_fp_violation": 0.008306366263687073,
            "ave_precision_score": 0.5405508496436189,
            "fpr": 0.44846491228070173,
            "logloss": 0.6868909574072019,
            "mae": 0.49501770146583257,
            "precision": 0.536281179138322,
            "recall": 0.9833679833679834
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.5287572621635337,
            "auditor_fn_violation": 0.0014666873983239853,
            "auditor_fp_violation": 0.007698900801467613,
            "ave_precision_score": 0.5278315233074248,
            "fpr": 0.45334796926454446,
            "logloss": 0.687167841252482,
            "mae": 0.49524907743237284,
            "precision": 0.5306818181818181,
            "recall": 0.9873150105708245
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(68)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.850975886982878,
            "auditor_fn_violation": 0.006164058795637757,
            "auditor_fp_violation": 0.002325273741197541,
            "ave_precision_score": 0.8453501355104979,
            "fpr": 0.044956140350877194,
            "logloss": 0.5135752954907793,
            "mae": 0.3533929742284511,
            "precision": 0.8779761904761905,
            "recall": 0.6133056133056133
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.7910383103882959,
            "auditor_fn_violation": 0.003727056901437223,
            "auditor_fp_violation": 0.009127407786114915,
            "ave_precision_score": 0.7897137387583532,
            "fpr": 0.05817782656421515,
            "logloss": 0.560124998272258,
            "mae": 0.37661734859650275,
            "precision": 0.8384146341463414,
            "recall": 0.5813953488372093
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(69)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5427631578947368,
            "auc_prc": 0.5431841829108641,
            "auditor_fn_violation": 0.0012651821862348178,
            "auditor_fp_violation": 0.007125920950869052,
            "ave_precision_score": 0.5442703905416184,
            "fpr": 0.4473684210526316,
            "logloss": 0.8622369429704274,
            "mae": 0.47205541460987244,
            "precision": 0.5363636363636364,
            "recall": 0.9812889812889813
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.5285041407515366,
            "auditor_fn_violation": 0.0014666873983239853,
            "auditor_fp_violation": 0.007698900801467613,
            "ave_precision_score": 0.5300959672214389,
            "fpr": 0.45334796926454446,
            "logloss": 0.861197695659366,
            "mae": 0.47271302155536954,
            "precision": 0.5306818181818181,
            "recall": 0.9873150105708245
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(70)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.7438867458597314,
            "auditor_fn_violation": 0.012047725863515338,
            "auditor_fp_violation": 0.02819076403305248,
            "ave_precision_score": 0.7013598293635666,
            "fpr": 0.29605263157894735,
            "logloss": 0.6231883203323969,
            "mae": 0.4195967381411608,
            "precision": 0.6131805157593123,
            "recall": 0.8898128898128899
        },
        "train": {
            "accuracy": 0.6421514818880352,
            "auc_prc": 0.727888504196644,
            "auditor_fn_violation": 0.013005247120581665,
            "auditor_fp_violation": 0.0326626868963305,
            "ave_precision_score": 0.6854089973898705,
            "fpr": 0.3018660812294182,
            "logloss": 0.6261295150484977,
            "mae": 0.4224764875470611,
            "precision": 0.6054519368723099,
            "recall": 0.8921775898520085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(71)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.37619032157174065,
            "auditor_fn_violation": 0.08892749024327973,
            "auditor_fp_violation": 0.0924716082549762,
            "ave_precision_score": 0.564571609942578,
            "fpr": 0.25548245614035087,
            "logloss": 0.7112706248462786,
            "mae": 0.4890980208619383,
            "precision": 0.5801801801801801,
            "recall": 0.6694386694386695
        },
        "train": {
            "accuracy": 0.5565312843029637,
            "auc_prc": 0.4449634484716119,
            "auditor_fn_violation": 0.08073510743717262,
            "auditor_fp_violation": 0.0878857595396699,
            "ave_precision_score": 0.5468632922528548,
            "fpr": 0.2843029637760702,
            "logloss": 0.7200280736493937,
            "mae": 0.49228464158581065,
            "precision": 0.5587734241908007,
            "recall": 0.693446088794926
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(72)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4605263157894737,
            "auc_prc": 0.4828014821331724,
            "auditor_fn_violation": 0.0012651821862348202,
            "auditor_fp_violation": 0.006100663491675826,
            "ave_precision_score": 0.5309664055026262,
            "fpr": 0.021929824561403508,
            "logloss": 0.8731639123754307,
            "mae": 0.513264340649026,
            "precision": 0.3103448275862069,
            "recall": 0.018711018711018712
        },
        "train": {
            "accuracy": 0.45993413830954993,
            "auc_prc": 0.43654254135802073,
            "auditor_fn_violation": 0.0008447376787815419,
            "auditor_fp_violation": 0.008104897523419995,
            "ave_precision_score": 0.5114938799859445,
            "fpr": 0.026344676180021953,
            "logloss": 0.8693403191406848,
            "mae": 0.5134375972774122,
            "precision": 0.1724137931034483,
            "recall": 0.010570824524312896
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(73)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7817982456140351,
            "auc_prc": 0.8544737587483255,
            "auditor_fn_violation": 0.013091786118101913,
            "auditor_fp_violation": 0.006377966377661092,
            "ave_precision_score": 0.8450761397673691,
            "fpr": 0.06359649122807018,
            "logloss": 0.4975616072492859,
            "mae": 0.3257825429298049,
            "precision": 0.8542713567839196,
            "recall": 0.7068607068607069
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8262877291277444,
            "auditor_fn_violation": 0.0021327305681325057,
            "auditor_fp_violation": 0.0098040689893689,
            "ave_precision_score": 0.8140423594538765,
            "fpr": 0.08562019758507135,
            "logloss": 0.5198386719476885,
            "mae": 0.3400065872989979,
            "precision": 0.8064516129032258,
            "recall": 0.6871035940803383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(74)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7440220028002426,
            "auditor_fn_violation": 0.05393779406937302,
            "auditor_fp_violation": 0.047446778198396225,
            "ave_precision_score": 0.7415356862749851,
            "fpr": 0.19517543859649122,
            "logloss": 0.6214034097766312,
            "mae": 0.45117094503356175,
            "precision": 0.6691449814126395,
            "recall": 0.7484407484407485
        },
        "train": {
            "accuracy": 0.6421514818880352,
            "auc_prc": 0.683751604985851,
            "auditor_fn_violation": 0.053951817462398736,
            "auditor_fp_violation": 0.05955621049676957,
            "ave_precision_score": 0.6769157148119702,
            "fpr": 0.22283205268935236,
            "logloss": 0.6375597341375189,
            "mae": 0.45870007883704195,
            "precision": 0.6329113924050633,
            "recall": 0.7399577167019028
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(75)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8555363100060207,
            "auditor_fn_violation": 0.008589561221140175,
            "auditor_fp_violation": 0.015989437049700815,
            "ave_precision_score": 0.8567260101991728,
            "fpr": 0.15350877192982457,
            "logloss": 0.5262638872500407,
            "mae": 0.30634977289459164,
            "precision": 0.7388059701492538,
            "recall": 0.8232848232848233
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8464127979511336,
            "auditor_fn_violation": 0.006913388860137899,
            "auditor_fp_violation": 0.008666275706860339,
            "ave_precision_score": 0.8466494041258421,
            "fpr": 0.1525795828759605,
            "logloss": 0.5365895411978265,
            "mae": 0.3133467464711881,
            "precision": 0.734225621414914,
            "recall": 0.8118393234672304
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(76)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.7641287933281234,
            "auditor_fn_violation": 0.012163985848196382,
            "auditor_fp_violation": 0.029086274270362693,
            "ave_precision_score": 0.7625474954539777,
            "fpr": 0.1962719298245614,
            "logloss": 1.0195718018384612,
            "mae": 0.32851199134050685,
            "precision": 0.6757246376811594,
            "recall": 0.7754677754677755
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7341134874732443,
            "auditor_fn_violation": 0.007526055748045384,
            "auditor_fp_violation": 0.023743289776401083,
            "ave_precision_score": 0.7311977998708586,
            "fpr": 0.20417124039517015,
            "logloss": 1.1392128744514647,
            "mae": 0.32755760024423897,
            "precision": 0.6713780918727915,
            "recall": 0.8033826638477801
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(77)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.8486022472119383,
            "auditor_fn_violation": 0.010860050333734545,
            "auditor_fp_violation": 0.005787743721252088,
            "ave_precision_score": 0.8116354203192566,
            "fpr": 0.0668859649122807,
            "logloss": 0.5139097835211138,
            "mae": 0.3347910071494417,
            "precision": 0.8478802992518704,
            "recall": 0.7068607068607069
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8228614171896362,
            "auditor_fn_violation": 0.003093503642351066,
            "auditor_fp_violation": 0.009849179736252505,
            "ave_precision_score": 0.7798249410347411,
            "fpr": 0.0845225027442371,
            "logloss": 0.5400793902207173,
            "mae": 0.35061069523545574,
            "precision": 0.8084577114427861,
            "recall": 0.6871035940803383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(78)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.780555640499017,
            "auditor_fn_violation": 0.008623755334281656,
            "auditor_fp_violation": 0.010819900679773686,
            "ave_precision_score": 0.7408745061518105,
            "fpr": 0.1118421052631579,
            "logloss": 0.5641071729034415,
            "mae": 0.38878441490886506,
            "precision": 0.7811158798283262,
            "recall": 0.7567567567567568
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7576894767280046,
            "auditor_fn_violation": 0.0011046569645604802,
            "auditor_fp_violation": 0.012661082958663515,
            "ave_precision_score": 0.7150000898580379,
            "fpr": 0.12733260153677278,
            "logloss": 0.5867409562423082,
            "mae": 0.39961864484390236,
            "precision": 0.7467248908296943,
            "recall": 0.7230443974630021
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(79)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8135220577418144,
            "auditor_fn_violation": 0.012647262647262647,
            "auditor_fp_violation": 0.013763381772296175,
            "ave_precision_score": 0.82293193607689,
            "fpr": 0.13267543859649122,
            "logloss": 0.5255597605683772,
            "mae": 0.35108142061845254,
            "precision": 0.7618110236220472,
            "recall": 0.8045738045738046
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7873315984247374,
            "auditor_fn_violation": 0.0010280736035720329,
            "auditor_fp_violation": 0.018335012455578444,
            "ave_precision_score": 0.7817037852755307,
            "fpr": 0.16245883644346873,
            "logloss": 0.5723526565220269,
            "mae": 0.37033916486408786,
            "precision": 0.7175572519083969,
            "recall": 0.7949260042283298
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(80)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.699602050489441,
            "auditor_fn_violation": 0.07276279315753,
            "auditor_fp_violation": 0.030162412993039445,
            "ave_precision_score": 0.6019618497741395,
            "fpr": 0.0625,
            "logloss": 0.6679876433007033,
            "mae": 0.4711408263170405,
            "precision": 0.7246376811594203,
            "recall": 0.31185031185031187
        },
        "train": {
            "accuracy": 0.5740944017563118,
            "auc_prc": 0.6867100861257767,
            "auditor_fn_violation": 0.07675741408159144,
            "auditor_fp_violation": 0.03837922098752437,
            "ave_precision_score": 0.5894305911201964,
            "fpr": 0.06805708013172337,
            "logloss": 0.6680401660100989,
            "mae": 0.471752240198831,
            "precision": 0.7033492822966507,
            "recall": 0.3107822410147992
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(81)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4725877192982456,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 4.8153656628291275,
            "mae": 0.5273669209058874,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4807903402854007,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 4.664399668726645,
            "mae": 0.519165548234577,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(82)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.6195248009716924,
            "auditor_fn_violation": 0.013390414706204189,
            "auditor_fp_violation": 0.027038303415150403,
            "ave_precision_score": 0.6475690911113577,
            "fpr": 0.16228070175438597,
            "logloss": 0.6320507671831306,
            "mae": 0.4245409002821696,
            "precision": 0.6991869918699187,
            "recall": 0.7151767151767152
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.6727178188791456,
            "auditor_fn_violation": 0.011190453535946616,
            "auditor_fp_violation": 0.015046940238284992,
            "ave_precision_score": 0.6374200737484574,
            "fpr": 0.1734357848518112,
            "logloss": 0.6387337075954497,
            "mae": 0.4279336733038156,
            "precision": 0.6808080808080809,
            "recall": 0.7124735729386892
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(83)",
        "seed": 24481,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8079573619058353,
            "auditor_fn_violation": 0.033225279935806255,
            "auditor_fp_violation": 0.005683437131110842,
            "ave_precision_score": 0.7702607972796127,
            "fpr": 0.06359649122807018,
            "logloss": 0.5481665425812653,
            "mae": 0.36942179200419206,
            "precision": 0.8352272727272727,
            "recall": 0.6112266112266113
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7248038185544021,
            "auditor_fn_violation": 0.030675117137731696,
            "auditor_fp_violation": 0.005363166573939022,
            "ave_precision_score": 0.7197569787577904,
            "fpr": 0.08781558726673985,
            "logloss": 0.5929038568492696,
            "mae": 0.39065331878175324,
            "precision": 0.778393351800554,
            "recall": 0.5940803382663847
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(84)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5537280701754386,
            "auc_prc": 0.7108526831422799,
            "auditor_fn_violation": 0.008439107123317656,
            "auditor_fp_violation": 0.0023558024992876626,
            "ave_precision_score": 0.7118318563164482,
            "fpr": 0.02412280701754386,
            "logloss": 1.2916125920111599,
            "mae": 0.45016869678997945,
            "precision": 0.8135593220338984,
            "recall": 0.1995841995841996
        },
        "train": {
            "accuracy": 0.5609220636663008,
            "auc_prc": 0.6880541813310441,
            "auditor_fn_violation": 0.005908522335653286,
            "auditor_fp_violation": 0.007227744111794455,
            "ave_precision_score": 0.689376450076261,
            "fpr": 0.027442371020856202,
            "logloss": 1.326800485669065,
            "mae": 0.4459992667784583,
            "precision": 0.7967479674796748,
            "recall": 0.20718816067653276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(85)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8597341107001527,
            "auditor_fn_violation": 0.026603020024072662,
            "auditor_fp_violation": 0.010166076444010259,
            "ave_precision_score": 0.8599987168621444,
            "fpr": 0.049342105263157895,
            "logloss": 0.6176569137614883,
            "mae": 0.3932639201915943,
            "precision": 0.8660714285714286,
            "recall": 0.604989604989605
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8439852552609352,
            "auditor_fn_violation": 0.010204152674731903,
            "auditor_fp_violation": 0.013142264258755247,
            "ave_precision_score": 0.8441837850334059,
            "fpr": 0.06586169045005488,
            "logloss": 0.6085612501653679,
            "mae": 0.3983551880357042,
            "precision": 0.8309859154929577,
            "recall": 0.6236786469344608
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(86)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7861842105263158,
            "auc_prc": 0.8561404020085771,
            "auditor_fn_violation": 0.009633621475726743,
            "auditor_fp_violation": 0.008069768388488625,
            "ave_precision_score": 0.7550460710814867,
            "fpr": 0.07785087719298246,
            "logloss": 0.519834525203476,
            "mae": 0.3580934495518082,
            "precision": 0.8341121495327103,
            "recall": 0.7422037422037422
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8168263282298454,
            "auditor_fn_violation": 0.0003875582207596616,
            "auditor_fp_violation": 0.013698630136986299,
            "ave_precision_score": 0.6997902096203918,
            "fpr": 0.10647639956092206,
            "logloss": 0.569771318290864,
            "mae": 0.3806813181297185,
            "precision": 0.7749419953596288,
            "recall": 0.7061310782241015
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(87)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8181057929598519,
            "auditor_fn_violation": 0.006973319473319478,
            "auditor_fp_violation": 0.020133715960434735,
            "ave_precision_score": 0.8182822031113001,
            "fpr": 0.15570175438596492,
            "logloss": 0.7935315982647727,
            "mae": 0.3162494019122571,
            "precision": 0.7360594795539034,
            "recall": 0.8232848232848233
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7903115216363615,
            "auditor_fn_violation": 0.011719574939139435,
            "auditor_fp_violation": 0.017848818850277434,
            "ave_precision_score": 0.7906196742495524,
            "fpr": 0.16794731064763996,
            "logloss": 0.8565888011579922,
            "mae": 0.333241019990441,
            "precision": 0.7140186915887851,
            "recall": 0.8076109936575053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(88)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7770736484873139,
            "auditor_fn_violation": 0.024375843454790828,
            "auditor_fp_violation": 0.013519151707575207,
            "ave_precision_score": 0.7644636248522516,
            "fpr": 0.19846491228070176,
            "logloss": 0.5953006520941164,
            "mae": 0.4020655968373543,
            "precision": 0.6715063520871143,
            "recall": 0.7692307692307693
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7811501934959943,
            "auditor_fn_violation": 0.022612977862767263,
            "auditor_fp_violation": 0.023938769679563338,
            "ave_precision_score": 0.7692556758514228,
            "fpr": 0.19758507135016465,
            "logloss": 0.5759318400183641,
            "mae": 0.39625164011797975,
            "precision": 0.6727272727272727,
            "recall": 0.7822410147991543
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(89)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4725877192982456,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.216174831087763,
            "mae": 0.5274122807017544,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4807903402854007,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.932866338960213,
            "mae": 0.5192096597145993,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(90)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5427631578947368,
            "auc_prc": 0.760615663769912,
            "auditor_fn_violation": 0.0023593938067622286,
            "auditor_fp_violation": 0.0011702690601212984,
            "ave_precision_score": 0.5366341882082736,
            "fpr": 0.43969298245614036,
            "logloss": 0.6896005564483183,
            "mae": 0.49761174272811204,
            "precision": 0.5369515011547344,
            "recall": 0.9667359667359667
        },
        "train": {
            "accuracy": 0.5192096597145993,
            "auc_prc": 0.7513371994386554,
            "auditor_fn_violation": 0.006247345690329379,
            "auditor_fp_violation": 0.0016891468555303214,
            "ave_precision_score": 0.5199276237622612,
            "fpr": 0.4621295279912184,
            "logloss": 0.693166961316529,
            "mae": 0.49949659820969095,
            "precision": 0.5199543899657925,
            "recall": 0.9640591966173362
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(91)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5877192982456141,
            "auc_prc": 0.680009911823588,
            "auditor_fn_violation": 0.013898767188240875,
            "auditor_fp_violation": 0.0067214149061749495,
            "ave_precision_score": 0.6804663839985903,
            "fpr": 0.03179824561403509,
            "logloss": 2.3239421777153275,
            "mae": 0.4482007071152408,
            "precision": 0.8220858895705522,
            "recall": 0.2785862785862786
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.7031882737959971,
            "auditor_fn_violation": 0.013035416323395298,
            "auditor_fp_violation": 0.008405635835977325,
            "ave_precision_score": 0.703633961667083,
            "fpr": 0.038419319429198684,
            "logloss": 2.268769078994318,
            "mae": 0.4300099019712205,
            "precision": 0.8118279569892473,
            "recall": 0.3192389006342495
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(92)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8274979723639552,
            "auditor_fn_violation": 0.026555148265674587,
            "auditor_fp_violation": 0.03510043961411652,
            "ave_precision_score": 0.8273702281011126,
            "fpr": 0.19298245614035087,
            "logloss": 1.0971709841169426,
            "mae": 0.3521824796805606,
            "precision": 0.697594501718213,
            "recall": 0.8440748440748441
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.8397498655161818,
            "auditor_fn_violation": 0.024346546670596398,
            "auditor_fp_violation": 0.025277055170443437,
            "ave_precision_score": 0.8394824361107717,
            "fpr": 0.22063666300768386,
            "logloss": 0.9518865261126466,
            "mae": 0.3515180203928735,
            "precision": 0.6773675762439807,
            "recall": 0.8921775898520085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(93)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5614035087719298,
            "auc_prc": 0.564036954038664,
            "auditor_fn_violation": 0.04574488456067405,
            "auditor_fp_violation": 0.04111206089469615,
            "ave_precision_score": 0.5652828189169835,
            "fpr": 0.26535087719298245,
            "logloss": 1.1472637920323858,
            "mae": 0.4857729851368109,
            "precision": 0.5716814159292035,
            "recall": 0.6715176715176715
        },
        "train": {
            "accuracy": 0.5345773874862788,
            "auc_prc": 0.5517219068695298,
            "auditor_fn_violation": 0.04369660921367455,
            "auditor_fp_violation": 0.03775017668375864,
            "ave_precision_score": 0.5529290883045863,
            "fpr": 0.2864983534577388,
            "logloss": 1.1120055344815105,
            "mae": 0.48275878418106916,
            "precision": 0.542907180385289,
            "recall": 0.6553911205073996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(94)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5339912280701754,
            "auc_prc": 0.8452577535076381,
            "auditor_fn_violation": 0.0006086552139183719,
            "auditor_fp_violation": 0.004630194977001681,
            "ave_precision_score": 0.8453712864778868,
            "fpr": 0.4649122807017544,
            "logloss": 3.7089873396411854,
            "mae": 0.46581920386814163,
            "precision": 0.5309734513274337,
            "recall": 0.997920997920998
        },
        "train": {
            "accuracy": 0.5246981339187706,
            "auc_prc": 0.8001299429577264,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0016465422612513788,
            "ave_precision_score": 0.800972128403722,
            "fpr": 0.47530186608122943,
            "logloss": 3.788194435082692,
            "mae": 0.47510530128222134,
            "precision": 0.522075055187638,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(95)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.8040903247157134,
            "auditor_fn_violation": 0.027186599555020605,
            "auditor_fp_violation": 0.0634107746163553,
            "ave_precision_score": 0.8038713959605872,
            "fpr": 0.28728070175438597,
            "logloss": 0.6693674241770153,
            "mae": 0.379873008027636,
            "precision": 0.624103299856528,
            "recall": 0.9043659043659044
        },
        "train": {
            "accuracy": 0.6399560922063666,
            "auc_prc": 0.7955619021758176,
            "auditor_fn_violation": 0.02300981891516188,
            "auditor_fp_violation": 0.0643379496664311,
            "ave_precision_score": 0.7958710404617989,
            "fpr": 0.3172338090010977,
            "logloss": 0.6706662359906841,
            "mae": 0.39431729555563466,
            "precision": 0.6002766251728907,
            "recall": 0.9175475687103594
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(96)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.8479238159784159,
            "auditor_fn_violation": 0.035951690556953714,
            "auditor_fp_violation": 0.07606240078153623,
            "ave_precision_score": 0.7948195692062887,
            "fpr": 0.2730263157894737,
            "logloss": 0.5469480395885352,
            "mae": 0.3725065061834788,
            "precision": 0.6300148588410104,
            "recall": 0.8814968814968815
        },
        "train": {
            "accuracy": 0.6509330406147091,
            "auc_prc": 0.813902087036826,
            "auditor_fn_violation": 0.03574818462623839,
            "auditor_fp_violation": 0.07489386443719331,
            "ave_precision_score": 0.7479537583230281,
            "fpr": 0.29418221734357847,
            "logloss": 0.5766507961061415,
            "mae": 0.38621431652714144,
            "precision": 0.6121562952243126,
            "recall": 0.8942917547568711
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(97)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5317982456140351,
            "auc_prc": 0.7370655974051903,
            "auditor_fn_violation": 0.00048327679906627277,
            "auditor_fp_violation": 0.0020454267920380943,
            "ave_precision_score": 0.7380393206310829,
            "fpr": 0.46710526315789475,
            "logloss": 2.426284155523582,
            "mae": 0.46385582548355797,
            "precision": 0.5298013245033113,
            "recall": 0.997920997920998
        },
        "train": {
            "accuracy": 0.5290889132821076,
            "auc_prc": 0.7143461164926335,
            "auditor_fn_violation": 0.001475970229958947,
            "auditor_fp_violation": 0.002751755559899549,
            "ave_precision_score": 0.715607784677363,
            "fpr": 0.4676180021953897,
            "logloss": 2.5074467521595625,
            "mae": 0.4672545042267462,
            "precision": 0.5245535714285714,
            "recall": 0.9936575052854123
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(98)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5570175438596491,
            "auc_prc": 0.6697641686202986,
            "auditor_fn_violation": 0.011140442061494695,
            "auditor_fp_violation": 0.014078845605894102,
            "ave_precision_score": 0.5586908421759622,
            "fpr": 0.4067982456140351,
            "logloss": 0.9166447897003328,
            "mae": 0.4888393953726344,
            "precision": 0.5470085470085471,
            "recall": 0.9313929313929314
        },
        "train": {
            "accuracy": 0.5455543358946213,
            "auc_prc": 0.6489998216483738,
            "auditor_fn_violation": 0.01304469915503025,
            "auditor_fp_violation": 0.008545980381837414,
            "ave_precision_score": 0.5408140361372065,
            "fpr": 0.42371020856201974,
            "logloss": 0.8958005514192865,
            "mae": 0.4926729605709684,
            "precision": 0.5354993983152828,
            "recall": 0.9408033826638478
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(99)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8036857423390862,
            "auditor_fn_violation": 0.06156080169238064,
            "auditor_fp_violation": 0.030976513208776,
            "ave_precision_score": 0.8040131581624188,
            "fpr": 0.10197368421052631,
            "logloss": 3.9719013804188745,
            "mae": 0.36049514150287715,
            "precision": 0.7686567164179104,
            "recall": 0.6424116424116424
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7646099190274067,
            "auditor_fn_violation": 0.058138374529766564,
            "auditor_fp_violation": 0.029141542486805106,
            "ave_precision_score": 0.7651202231549563,
            "fpr": 0.1251372118551043,
            "logloss": 3.7079847681124694,
            "mae": 0.3791485482786796,
            "precision": 0.7259615384615384,
            "recall": 0.638477801268499
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.5432758086984574,
            "auditor_fn_violation": 0.005443047364711995,
            "auditor_fp_violation": 0.024609858929784612,
            "ave_precision_score": 0.4981702111825013,
            "fpr": 0.2532894736842105,
            "logloss": 6.855875386513084,
            "mae": 0.3847724503113265,
            "precision": 0.6091370558375635,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.5626361459515721,
            "auditor_fn_violation": 0.014762968659785351,
            "auditor_fp_violation": 0.003931279197406381,
            "ave_precision_score": 0.5187900082417525,
            "fpr": 0.22283205268935236,
            "logloss": 6.519552660686229,
            "mae": 0.36097096350351837,
            "precision": 0.6394316163410302,
            "recall": 0.7484407484407485
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.787280140672858,
            "auditor_fn_violation": 0.017615722710581948,
            "auditor_fp_violation": 0.015290932342245135,
            "ave_precision_score": 0.788100524292921,
            "fpr": 0.15460526315789475,
            "logloss": 0.715995947652584,
            "mae": 0.31329476281245117,
            "precision": 0.7191235059760956,
            "recall": 0.7632135306553911
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8553672735812639,
            "auditor_fn_violation": 0.017273289501610032,
            "auditor_fp_violation": 0.022326602506828684,
            "ave_precision_score": 0.8548903545104958,
            "fpr": 0.12184412733260154,
            "logloss": 0.572634436844297,
            "mae": 0.2794125596175464,
            "precision": 0.7730061349693251,
            "recall": 0.7858627858627859
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6118421052631579,
            "auc_prc": 0.536466137495617,
            "auditor_fn_violation": 0.012035903712770296,
            "auditor_fp_violation": 0.02422771050633418,
            "ave_precision_score": 0.49426155274126576,
            "fpr": 0.25548245614035087,
            "logloss": 6.8806678850054235,
            "mae": 0.39045312127578063,
            "precision": 0.6017094017094017,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.5555401145204449,
            "auditor_fn_violation": 0.01599302587227944,
            "auditor_fp_violation": 0.00556250478646006,
            "ave_precision_score": 0.5144497417277876,
            "fpr": 0.22063666300768386,
            "logloss": 6.562661352888902,
            "mae": 0.36536258514054615,
            "precision": 0.6410714285714286,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8107605373516245,
            "auditor_fn_violation": 0.014017933311078975,
            "auditor_fp_violation": 0.005944531031451065,
            "ave_precision_score": 0.8059796720659573,
            "fpr": 0.08114035087719298,
            "logloss": 1.856756268012844,
            "mae": 0.2551204738473374,
            "precision": 0.8159203980099502,
            "recall": 0.693446088794926
        },
        "train": {
            "accuracy": 0.7914379802414928,
            "auc_prc": 0.8381389407145348,
            "auditor_fn_violation": 0.00047696095994669577,
            "auditor_fp_violation": 0.0037015291144410745,
            "ave_precision_score": 0.833880168199118,
            "fpr": 0.06695938529088913,
            "logloss": 1.4926647796016024,
            "mae": 0.22375009855062045,
            "precision": 0.8523002421307506,
            "recall": 0.7318087318087318
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 9296,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.5410540641409638,
            "auditor_fn_violation": 0.007707892882311487,
            "auditor_fp_violation": 0.025224293649842158,
            "ave_precision_score": 0.4970651123475869,
            "fpr": 0.2543859649122807,
            "logloss": 6.883108164535525,
            "mae": 0.3865798525405612,
            "precision": 0.6061120543293718,
            "recall": 0.7547568710359408
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.5585826456438826,
            "auditor_fn_violation": 0.011250801591087,
            "auditor_fp_violation": 0.0025527786996145324,
            "ave_precision_score": 0.5169451716920819,
            "fpr": 0.21953896816684962,
            "logloss": 6.552301602322408,
            "mae": 0.362006089975743,
            "precision": 0.6415770609318996,
            "recall": 0.7442827442827443
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6008771929824561,
            "auc_prc": 0.5430277242108715,
            "auditor_fn_violation": 0.003736879195875525,
            "auditor_fp_violation": 0.020536106781760776,
            "ave_precision_score": 0.4918961430077243,
            "fpr": 0.2741228070175439,
            "logloss": 7.1101765866206215,
            "mae": 0.40967919576316253,
            "precision": 0.5894909688013136,
            "recall": 0.758985200845666
        },
        "train": {
            "accuracy": 0.6300768386388584,
            "auc_prc": 0.5656233520336953,
            "auditor_fn_violation": 0.007713531313970393,
            "auditor_fp_violation": 0.016046766905776947,
            "ave_precision_score": 0.5150753643742411,
            "fpr": 0.2557628979143798,
            "logloss": 6.679060417034964,
            "mae": 0.3816551872686045,
            "precision": 0.6180327868852459,
            "recall": 0.7837837837837838
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.5423327067283459,
            "auditor_fn_violation": 0.009444197173695342,
            "auditor_fp_violation": 0.026762878152100072,
            "ave_precision_score": 0.4966514862377145,
            "fpr": 0.2598684210526316,
            "logloss": 6.835756207102158,
            "mae": 0.38703828091632186,
            "precision": 0.6069651741293532,
            "recall": 0.773784355179704
        },
        "train": {
            "accuracy": 0.6509330406147091,
            "auc_prc": 0.5585678367449943,
            "auditor_fn_violation": 0.01511897779735321,
            "auditor_fp_violation": 0.00525361856380671,
            "ave_precision_score": 0.515776263199622,
            "fpr": 0.2327113062568606,
            "logloss": 6.502815466069976,
            "mae": 0.36157196803943975,
            "precision": 0.6388415672913118,
            "recall": 0.7796257796257796
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8094843700412281,
            "auditor_fn_violation": 0.014365657802010313,
            "auditor_fp_violation": 0.007043519961635297,
            "ave_precision_score": 0.8071187351341287,
            "fpr": 0.1074561403508772,
            "logloss": 1.0984512905637442,
            "mae": 0.27071342471899057,
            "precision": 0.772093023255814,
            "recall": 0.7019027484143763
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8065538806447705,
            "auditor_fn_violation": 0.013573989424702932,
            "auditor_fp_violation": 0.007581752737855158,
            "ave_precision_score": 0.7983715217813718,
            "fpr": 0.0889132821075741,
            "logloss": 1.2785023262518374,
            "mae": 0.249765630801404,
            "precision": 0.8089622641509434,
            "recall": 0.7130977130977131
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6041666666666666,
            "auc_prc": 0.5257104186735411,
            "auditor_fn_violation": 0.010144282482103776,
            "auditor_fp_violation": 0.027986752187987046,
            "ave_precision_score": 0.49363362096440067,
            "fpr": 0.2817982456140351,
            "logloss": 6.108546029127484,
            "mae": 0.39357027497420893,
            "precision": 0.5894568690095847,
            "recall": 0.7801268498942917
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.5317996666855338,
            "auditor_fn_violation": 0.008925331647614856,
            "auditor_fp_violation": 0.010609348275597985,
            "ave_precision_score": 0.506623884427768,
            "fpr": 0.2502744237102086,
            "logloss": 6.166374684353912,
            "mae": 0.36523835338359756,
            "precision": 0.6274509803921569,
            "recall": 0.7983367983367984
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 9296,
        "test": {
            "accuracy": 0.39364035087719296,
            "auc_prc": 0.47775630659231216,
            "auditor_fn_violation": 0.015330013723526572,
            "auditor_fp_violation": 0.033291871478240025,
            "ave_precision_score": 0.4410523291134191,
            "fpr": 0.2050438596491228,
            "logloss": 9.534912514007374,
            "mae": 0.5996158605520565,
            "precision": 0.36394557823129253,
            "recall": 0.226215644820296
        },
        "train": {
            "accuracy": 0.38638858397365533,
            "auc_prc": 0.48022384939332247,
            "auditor_fn_violation": 0.010527372766670243,
            "auditor_fp_violation": 0.008031041788987309,
            "ave_precision_score": 0.4504275194258878,
            "fpr": 0.18221734357848518,
            "logloss": 9.634125141838169,
            "mae": 0.6150005979865518,
            "precision": 0.3464566929133858,
            "recall": 0.18295218295218296
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.533383661150526,
            "auditor_fn_violation": 0.014752791068580543,
            "auditor_fp_violation": 0.026670463173880034,
            "ave_precision_score": 0.4917630734084786,
            "fpr": 0.25109649122807015,
            "logloss": 6.954325672688572,
            "mae": 0.3966893311348751,
            "precision": 0.6003490401396161,
            "recall": 0.7272727272727273
        },
        "train": {
            "accuracy": 0.6366630076838639,
            "auc_prc": 0.5539669329863698,
            "auditor_fn_violation": 0.015103003028359785,
            "auditor_fp_violation": 0.006846552472366167,
            "ave_precision_score": 0.5122561304919218,
            "fpr": 0.21734357848518113,
            "logloss": 6.6529022325216065,
            "mae": 0.37408107900955756,
            "precision": 0.6373626373626373,
            "recall": 0.7234927234927235
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8181479355776042,
            "auditor_fn_violation": 0.008141389414339228,
            "auditor_fp_violation": 0.0059894896695040604,
            "ave_precision_score": 0.818910004218764,
            "fpr": 0.10307017543859649,
            "logloss": 0.7701393701900725,
            "mae": 0.27367495364971506,
            "precision": 0.7873303167420814,
            "recall": 0.7357293868921776
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.87223808495955,
            "auditor_fn_violation": 0.010689402566460749,
            "auditor_fp_violation": 0.009394225614581472,
            "ave_precision_score": 0.8724102497753792,
            "fpr": 0.0845225027442371,
            "logloss": 0.6142170240258147,
            "mae": 0.25016361028530065,
            "precision": 0.8237986270022883,
            "recall": 0.7484407484407485
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.7932201188953557,
            "auditor_fn_violation": 0.018617169244464226,
            "auditor_fp_violation": 0.008482196379331017,
            "ave_precision_score": 0.7939463117871592,
            "fpr": 0.10416666666666667,
            "logloss": 0.6865019809299566,
            "mae": 0.3145669130485578,
            "precision": 0.7721822541966427,
            "recall": 0.6807610993657506
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.85696388147261,
            "auditor_fn_violation": 0.028250237909952517,
            "auditor_fp_violation": 0.007671099992341668,
            "ave_precision_score": 0.8566826233382251,
            "fpr": 0.07135016465422613,
            "logloss": 0.5735035357673536,
            "mae": 0.27918867991280505,
            "precision": 0.8395061728395061,
            "recall": 0.7068607068607069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7480553960636814,
            "auditor_fn_violation": 0.022801453952004745,
            "auditor_fp_violation": 0.012565939335811058,
            "ave_precision_score": 0.7476305798816019,
            "fpr": 0.08771929824561403,
            "logloss": 1.0996240641198722,
            "mae": 0.31574035098681535,
            "precision": 0.7777777777777778,
            "recall": 0.5919661733615222
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.8003923049483979,
            "auditor_fn_violation": 0.029637760702524697,
            "auditor_fp_violation": 0.009169581089015394,
            "ave_precision_score": 0.7998669969981371,
            "fpr": 0.07793633369923161,
            "logloss": 0.9793927632007364,
            "mae": 0.30291516297114385,
            "precision": 0.8065395095367848,
            "recall": 0.6153846153846154
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6206140350877193,
            "auc_prc": 0.5396751329005782,
            "auditor_fn_violation": 0.010072419420644636,
            "auditor_fp_violation": 0.02422771050633418,
            "ave_precision_score": 0.4957314752393841,
            "fpr": 0.25548245614035087,
            "logloss": 6.889249087557075,
            "mae": 0.38709875555984874,
            "precision": 0.6070826306913997,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.6421514818880352,
            "auc_prc": 0.5589567866546169,
            "auditor_fn_violation": 0.01460093885999485,
            "auditor_fp_violation": 0.004053812574987877,
            "ave_precision_score": 0.5161646973130393,
            "fpr": 0.2239297475301866,
            "logloss": 6.56027150420758,
            "mae": 0.36209748967532734,
            "precision": 0.6376554174067496,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.528054076673131,
            "auditor_fn_violation": 0.009664422684618526,
            "auditor_fp_violation": 0.02446748990928347,
            "ave_precision_score": 0.4921099568014684,
            "fpr": 0.25219298245614036,
            "logloss": 6.830300530462658,
            "mae": 0.385365362780141,
            "precision": 0.608843537414966,
            "recall": 0.7568710359408034
        },
        "train": {
            "accuracy": 0.6520307354555434,
            "auc_prc": 0.53921819449661,
            "auditor_fn_violation": 0.014240365502714573,
            "auditor_fp_violation": 0.006065402190284126,
            "ave_precision_score": 0.5087460733539528,
            "fpr": 0.21734357848518113,
            "logloss": 6.532192425961721,
            "mae": 0.35658900906922597,
            "precision": 0.6464285714285715,
            "recall": 0.7525987525987526
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5866228070175439,
            "auc_prc": 0.5065404730888693,
            "auditor_fn_violation": 0.023165405585846226,
            "auditor_fp_violation": 0.025321704032290303,
            "ave_precision_score": 0.47051802783514424,
            "fpr": 0.2543859649122807,
            "logloss": 6.814247257480271,
            "mae": 0.44599159436684366,
            "precision": 0.5857142857142857,
            "recall": 0.693446088794926
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.5255559633093132,
            "auditor_fn_violation": 0.031981487524846475,
            "auditor_fp_violation": 0.006223674469660223,
            "ave_precision_score": 0.4924242077076042,
            "fpr": 0.2217343578485181,
            "logloss": 6.500962543558766,
            "mae": 0.4268737529017518,
            "precision": 0.6166982922201139,
            "recall": 0.6756756756756757
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 9296,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.5389107651500061,
            "auditor_fn_violation": 0.009427970030785209,
            "auditor_fp_violation": 0.02422771050633418,
            "ave_precision_score": 0.4955666704517894,
            "fpr": 0.25548245614035087,
            "logloss": 6.894479817515865,
            "mae": 0.3867109728158886,
            "precision": 0.6057529610829103,
            "recall": 0.7568710359408034
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.5580688713853182,
            "auditor_fn_violation": 0.01460093885999485,
            "auditor_fp_violation": 0.006601485717203186,
            "ave_precision_score": 0.5158500318277273,
            "fpr": 0.2217343578485181,
            "logloss": 6.566983659753263,
            "mae": 0.36218756148037845,
            "precision": 0.6399286987522281,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8254469143653997,
            "auditor_fn_violation": 0.007863209821594165,
            "auditor_fp_violation": 0.0037515485753107165,
            "ave_precision_score": 0.8246773703796563,
            "fpr": 0.09978070175438597,
            "logloss": 1.009024141304936,
            "mae": 0.26674382796416246,
            "precision": 0.7828162291169452,
            "recall": 0.693446088794926
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8659860008865754,
            "auditor_fn_violation": 0.017305239039596895,
            "auditor_fp_violation": 0.0061828300104663945,
            "ave_precision_score": 0.8652085486260743,
            "fpr": 0.07244785949506037,
            "logloss": 0.8478695782435711,
            "mae": 0.23820561377741178,
            "precision": 0.837037037037037,
            "recall": 0.7047817047817048
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 9296,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.5390628542506346,
            "auditor_fn_violation": 0.009427970030785209,
            "auditor_fp_violation": 0.02422771050633418,
            "ave_precision_score": 0.49570015108294024,
            "fpr": 0.25548245614035087,
            "logloss": 6.889886941931059,
            "mae": 0.38674055711636046,
            "precision": 0.6057529610829103,
            "recall": 0.7568710359408034
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.558379479694688,
            "auditor_fn_violation": 0.01460093885999485,
            "auditor_fp_violation": 0.004783907283077628,
            "ave_precision_score": 0.5161508798190824,
            "fpr": 0.22063666300768386,
            "logloss": 6.561474255220378,
            "mae": 0.36218651838813104,
            "precision": 0.6410714285714286,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 9296,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.5516723889986347,
            "auditor_fn_violation": 0.007598939208486337,
            "auditor_fp_violation": 0.031763277784438326,
            "ave_precision_score": 0.5027990419742004,
            "fpr": 0.2576754385964912,
            "logloss": 6.954528547924348,
            "mae": 0.3828510066278932,
            "precision": 0.6050420168067226,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.6553238199780461,
            "auc_prc": 0.5683911164263049,
            "auditor_fn_violation": 0.01229829001508475,
            "auditor_fp_violation": 0.006969085849947671,
            "ave_precision_score": 0.5214576360795798,
            "fpr": 0.2239297475301866,
            "logloss": 6.6249218274575705,
            "mae": 0.3517453497562898,
            "precision": 0.6452173913043479,
            "recall": 0.7713097713097713
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8254469143653997,
            "auditor_fn_violation": 0.007863209821594165,
            "auditor_fp_violation": 0.0037515485753107165,
            "ave_precision_score": 0.8246773703796563,
            "fpr": 0.09978070175438597,
            "logloss": 1.0090242656551849,
            "mae": 0.26674383798858886,
            "precision": 0.7828162291169452,
            "recall": 0.693446088794926
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8659860008865754,
            "auditor_fn_violation": 0.017305239039596895,
            "auditor_fp_violation": 0.0061828300104663945,
            "ave_precision_score": 0.8652085486260743,
            "fpr": 0.07244785949506037,
            "logloss": 0.8478696868522122,
            "mae": 0.2382056297005485,
            "precision": 0.837037037037037,
            "recall": 0.7047817047817048
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 9296,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.5389135399821652,
            "auditor_fn_violation": 0.009427970030785209,
            "auditor_fp_violation": 0.02422771050633418,
            "ave_precision_score": 0.49556944448043666,
            "fpr": 0.25548245614035087,
            "logloss": 6.894479412698673,
            "mae": 0.3867109496883618,
            "precision": 0.6057529610829103,
            "recall": 0.7568710359408034
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.5580688713853182,
            "auditor_fn_violation": 0.01460093885999485,
            "auditor_fp_violation": 0.006601485717203186,
            "ave_precision_score": 0.5158500318277273,
            "fpr": 0.2217343578485181,
            "logloss": 6.566983762968425,
            "mae": 0.3621875180789613,
            "precision": 0.6399286987522281,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.6341868574169834,
            "auditor_fn_violation": 0.0035305626645895922,
            "auditor_fp_violation": 0.02375064940254967,
            "ave_precision_score": 0.5423962778207778,
            "fpr": 0.3081140350877193,
            "logloss": 7.6610848402597576,
            "mae": 0.3804750592880321,
            "precision": 0.5951008645533141,
            "recall": 0.8731501057082452
        },
        "train": {
            "accuracy": 0.6608122941822173,
            "auc_prc": 0.6506871884954084,
            "auditor_fn_violation": 0.009712659548005322,
            "auditor_fp_violation": 0.009501442319965294,
            "ave_precision_score": 0.5614370649731346,
            "fpr": 0.27661909989023054,
            "logloss": 7.267977867961398,
            "mae": 0.3471570561269426,
            "precision": 0.6272189349112426,
            "recall": 0.8814968814968815
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5888157894736842,
            "auc_prc": 0.509420223018278,
            "auditor_fn_violation": 0.02447980416156671,
            "auditor_fp_violation": 0.030849118810694166,
            "ave_precision_score": 0.472105324182699,
            "fpr": 0.2543859649122807,
            "logloss": 6.831049187573949,
            "mae": 0.44174384460130933,
            "precision": 0.5871886120996441,
            "recall": 0.6976744186046512
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.5244558145696729,
            "auditor_fn_violation": 0.0306030931716991,
            "auditor_fp_violation": 0.006228780027059459,
            "ave_precision_score": 0.4926699547546316,
            "fpr": 0.22502744237102085,
            "logloss": 6.511665433491687,
            "mae": 0.4248678677202418,
            "precision": 0.6153846153846154,
            "recall": 0.681912681912682
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 9296,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.5283949124626915,
            "auditor_fn_violation": 0.006773673083342613,
            "auditor_fp_violation": 0.027919314230907575,
            "ave_precision_score": 0.49310364739717233,
            "fpr": 0.2598684210526316,
            "logloss": 6.6958659935155245,
            "mae": 0.3845285116289584,
            "precision": 0.6082644628099173,
            "recall": 0.7780126849894292
        },
        "train": {
            "accuracy": 0.6663007683863886,
            "auc_prc": 0.5364540303296548,
            "auditor_fn_violation": 0.005182671483439873,
            "auditor_fp_violation": 0.003793429147627205,
            "ave_precision_score": 0.5094842215310991,
            "fpr": 0.22941822173435786,
            "logloss": 6.411453071049651,
            "mae": 0.35193710261711464,
            "precision": 0.6487394957983194,
            "recall": 0.8024948024948025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5953947368421053,
            "auc_prc": 0.6121560519111585,
            "auditor_fn_violation": 0.009154426764585888,
            "auditor_fp_violation": 0.02476971186508412,
            "ave_precision_score": 0.5275241974503172,
            "fpr": 0.3201754385964912,
            "logloss": 7.592423793188419,
            "mae": 0.3977352843921167,
            "precision": 0.5755813953488372,
            "recall": 0.8372093023255814
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.6326791611971175,
            "auditor_fn_violation": 0.00693989607271715,
            "auditor_fp_violation": 0.009215531105608453,
            "ave_precision_score": 0.5477446783771193,
            "fpr": 0.2843029637760702,
            "logloss": 7.252559964093432,
            "mae": 0.3570394007497166,
            "precision": 0.6179941002949852,
            "recall": 0.8711018711018711
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5603070175438597,
            "auc_prc": 0.5865487777650753,
            "auditor_fn_violation": 0.0666101034828085,
            "auditor_fp_violation": 0.01718918594892699,
            "ave_precision_score": 0.6063290289264753,
            "fpr": 0.04276315789473684,
            "logloss": 11.174426683094344,
            "mae": 0.4507868300349683,
            "precision": 0.74,
            "recall": 0.2346723044397463
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.6282610104924959,
            "auditor_fn_violation": 0.06035039514732161,
            "auditor_fp_violation": 0.008845378194164349,
            "ave_precision_score": 0.6472277061328634,
            "fpr": 0.038419319429198684,
            "logloss": 10.729145066422046,
            "mae": 0.43796746504133977,
            "precision": 0.779874213836478,
            "recall": 0.2577962577962578
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8213737969247356,
            "auditor_fn_violation": 0.009319016356960056,
            "auditor_fp_violation": 0.012003956360148674,
            "ave_precision_score": 0.8198563079942525,
            "fpr": 0.11293859649122807,
            "logloss": 0.9973031413276511,
            "mae": 0.26905942271521033,
            "precision": 0.7664399092970522,
            "recall": 0.7145877378435518
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8679303141001309,
            "auditor_fn_violation": 0.019402497997448608,
            "auditor_fp_violation": 0.0037985347050264215,
            "ave_precision_score": 0.8671576419454587,
            "fpr": 0.08122941822173436,
            "logloss": 0.7951853880303399,
            "mae": 0.24027540079146617,
            "precision": 0.8246445497630331,
            "recall": 0.7234927234927235
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 9296,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.5340863648391304,
            "auditor_fn_violation": 0.012506490857164055,
            "auditor_fp_violation": 0.02290642608799905,
            "ave_precision_score": 0.4936768086965032,
            "fpr": 0.25109649122807015,
            "logloss": 6.814187503065682,
            "mae": 0.3885551564221156,
            "precision": 0.6072041166380789,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.5505976516444218,
            "auditor_fn_violation": 0.016768943223388896,
            "auditor_fp_violation": 0.004023179230592502,
            "ave_precision_score": 0.5131296334527056,
            "fpr": 0.21844127332601537,
            "logloss": 6.505647191376952,
            "mae": 0.36586595560564933,
            "precision": 0.6420863309352518,
            "recall": 0.7422037422037422
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0.5779541630536358,
            "auditor_fn_violation": 0.011314954934905972,
            "auditor_fp_violation": 0.01806338168884627,
            "ave_precision_score": 0.4892766549045672,
            "fpr": 0.28618421052631576,
            "logloss": 10.490977869640645,
            "mae": 0.4977906587620821,
            "precision": 0.5157699443413729,
            "recall": 0.587737843551797
        },
        "train": {
            "accuracy": 0.5203073545554336,
            "auc_prc": 0.5748145595929103,
            "auditor_fn_violation": 0.004605297689820197,
            "auditor_fp_violation": 0.006323232838945199,
            "ave_precision_score": 0.4951444434623277,
            "fpr": 0.27332601536772777,
            "logloss": 10.282120664851758,
            "mae": 0.4823501380985238,
            "precision": 0.540590405904059,
            "recall": 0.6091476091476091
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6162280701754386,
            "auc_prc": 0.540267539311998,
            "auditor_fn_violation": 0.011653406772745824,
            "auditor_fp_violation": 0.023980437997042733,
            "ave_precision_score": 0.49636681276505706,
            "fpr": 0.25877192982456143,
            "logloss": 6.872923417296315,
            "mae": 0.3854401190610787,
            "precision": 0.6033613445378151,
            "recall": 0.758985200845666
        },
        "train": {
            "accuracy": 0.6421514818880352,
            "auc_prc": 0.5592996190126617,
            "auditor_fn_violation": 0.013263622484259153,
            "auditor_fp_violation": 0.005350624154392061,
            "ave_precision_score": 0.5165833773795039,
            "fpr": 0.2261251372118551,
            "logloss": 6.54659874059905,
            "mae": 0.3615333877245002,
            "precision": 0.63668430335097,
            "recall": 0.7505197505197505
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.5103335857878124,
            "auditor_fn_violation": 0.021403601498460746,
            "auditor_fp_violation": 0.025806258242416984,
            "ave_precision_score": 0.47303189505935417,
            "fpr": 0.2565789473684211,
            "logloss": 6.8056529550705545,
            "mae": 0.43475220905077694,
            "precision": 0.5986277873070326,
            "recall": 0.7378435517970402
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.5277393160691914,
            "auditor_fn_violation": 0.02546378177552712,
            "auditor_fp_violation": 0.013578229903249698,
            "ave_precision_score": 0.4939310461272846,
            "fpr": 0.23819978046103182,
            "logloss": 6.477373991117454,
            "mae": 0.4167178178490543,
            "precision": 0.6152482269503546,
            "recall": 0.7214137214137214
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.5380340770438568,
            "auditor_fn_violation": 0.009829012276992692,
            "auditor_fp_violation": 0.026380729728649653,
            "ave_precision_score": 0.49523016457707403,
            "fpr": 0.2565789473684211,
            "logloss": 6.836929397715801,
            "mae": 0.388281247249029,
            "precision": 0.6060606060606061,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.5560873617666655,
            "auditor_fn_violation": 0.014817739296334254,
            "auditor_fp_violation": 0.004252929313557806,
            "ave_precision_score": 0.5156010302622276,
            "fpr": 0.2239297475301866,
            "logloss": 6.50613781854489,
            "mae": 0.36416353492163894,
            "precision": 0.6395759717314488,
            "recall": 0.7525987525987526
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6118421052631579,
            "auc_prc": 0.5262933884195223,
            "auditor_fn_violation": 0.006502447980416164,
            "auditor_fp_violation": 0.022217060304519847,
            "ave_precision_score": 0.49105647856849416,
            "fpr": 0.24451754385964913,
            "logloss": 6.9564456030317325,
            "mae": 0.39661755243904107,
            "precision": 0.6053097345132743,
            "recall": 0.7230443974630021
        },
        "train": {
            "accuracy": 0.6410537870472008,
            "auc_prc": 0.534987479207833,
            "auditor_fn_violation": 0.005844483341739108,
            "auditor_fp_violation": 0.017813289765910203,
            "ave_precision_score": 0.5052641833987264,
            "fpr": 0.22063666300768386,
            "logloss": 6.689549305675075,
            "mae": 0.37406699935249466,
            "precision": 0.6384892086330936,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.5386374561956716,
            "auditor_fn_violation": 0.005660954712362304,
            "auditor_fp_violation": 0.02695520121488231,
            "ave_precision_score": 0.4952314589130224,
            "fpr": 0.26206140350877194,
            "logloss": 6.760489235627225,
            "mae": 0.388095768948947,
            "precision": 0.6062602965403624,
            "recall": 0.7780126849894292
        },
        "train": {
            "accuracy": 0.6498353457738749,
            "auc_prc": 0.5509977705194622,
            "auditor_fn_violation": 0.009660171021312624,
            "auditor_fp_violation": 0.013034488040231806,
            "ave_precision_score": 0.5129477208151215,
            "fpr": 0.23819978046103182,
            "logloss": 6.4543800668848785,
            "mae": 0.3633610090870011,
            "precision": 0.6359060402684564,
            "recall": 0.7879417879417879
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.5361569620943869,
            "auditor_fn_violation": 0.005074459404324766,
            "auditor_fp_violation": 0.02372567238140911,
            "ave_precision_score": 0.495180325554908,
            "fpr": 0.26864035087719296,
            "logloss": 6.816934504697608,
            "mae": 0.38279582917526933,
            "precision": 0.6009771986970684,
            "recall": 0.7801268498942917
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.551675359978919,
            "auditor_fn_violation": 0.011294161678354878,
            "auditor_fp_violation": 0.004278457100553951,
            "ave_precision_score": 0.5142506324418245,
            "fpr": 0.23161361141602635,
            "logloss": 6.478047474213115,
            "mae": 0.3523528745858635,
            "precision": 0.6453781512605042,
            "recall": 0.7983367983367984
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5032894736842105,
            "auc_prc": 0.4748345380745222,
            "auditor_fn_violation": 0.021531100478468894,
            "auditor_fp_violation": 0.030014886304599774,
            "ave_precision_score": 0.43936862739407306,
            "fpr": 0.25877192982456143,
            "logloss": 8.878222093919156,
            "mae": 0.5038614757290798,
            "precision": 0.5203252032520326,
            "recall": 0.5412262156448203
        },
        "train": {
            "accuracy": 0.5345773874862788,
            "auc_prc": 0.4816542912326577,
            "auditor_fn_violation": 0.005440549897190965,
            "auditor_fp_violation": 0.014129630102366427,
            "ave_precision_score": 0.4524573628339298,
            "fpr": 0.22502744237102085,
            "logloss": 8.782201999157603,
            "mae": 0.48566565608109896,
            "precision": 0.5610278372591007,
            "recall": 0.5446985446985447
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5932017543859649,
            "auc_prc": 0.5087692994067871,
            "auditor_fn_violation": 0.022759727013092992,
            "auditor_fp_violation": 0.02802172001758383,
            "ave_precision_score": 0.47208153505712513,
            "fpr": 0.2576754385964912,
            "logloss": 6.8060414968567535,
            "mae": 0.4414020499537225,
            "precision": 0.5891608391608392,
            "recall": 0.7124735729386892
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.5243807372593033,
            "auditor_fn_violation": 0.02854463008140286,
            "auditor_fp_violation": 0.009036836596635435,
            "ave_precision_score": 0.49327223601518605,
            "fpr": 0.2283205268935236,
            "logloss": 6.4820671748746435,
            "mae": 0.42252151623888873,
            "precision": 0.618348623853211,
            "recall": 0.7006237006237006
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.5365436500908948,
            "auditor_fn_violation": 0.009733967582804799,
            "auditor_fp_violation": 0.02594862726291812,
            "ave_precision_score": 0.4937206142600825,
            "fpr": 0.25548245614035087,
            "logloss": 6.807474042045009,
            "mae": 0.3911220602355905,
            "precision": 0.6030664395229983,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6465422612513722,
            "auc_prc": 0.5519786274682543,
            "auditor_fn_violation": 0.019877176847539094,
            "auditor_fp_violation": 0.0067393357669823595,
            "ave_precision_score": 0.5126529380298435,
            "fpr": 0.2217343578485181,
            "logloss": 6.496243789073025,
            "mae": 0.3678267627556933,
            "precision": 0.6412078152753108,
            "recall": 0.7505197505197505
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 9296,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.5435852842635656,
            "auditor_fn_violation": 0.008092707985608846,
            "auditor_fp_violation": 0.023855552891339976,
            "ave_precision_score": 0.4979784941477345,
            "fpr": 0.2576754385964912,
            "logloss": 6.914951384409056,
            "mae": 0.3862692538202098,
            "precision": 0.6050420168067226,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.6487376509330406,
            "auc_prc": 0.5635096203543931,
            "auditor_fn_violation": 0.016412934085821024,
            "auditor_fp_violation": 0.00328797896510352,
            "ave_precision_score": 0.5185212521995903,
            "fpr": 0.21844127332601537,
            "logloss": 6.578785906936729,
            "mae": 0.36016989557041157,
            "precision": 0.6440071556350626,
            "recall": 0.7484407484407485
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.5365566487010147,
            "auditor_fn_violation": 0.011491135343644522,
            "auditor_fp_violation": 0.02293889621548176,
            "ave_precision_score": 0.4949470898940718,
            "fpr": 0.2532894736842105,
            "logloss": 6.814000480327259,
            "mae": 0.3886166536656977,
            "precision": 0.6071428571428571,
            "recall": 0.7547568710359408
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.5554123811319637,
            "auditor_fn_violation": 0.012747865656756986,
            "auditor_fp_violation": 0.004977918464248341,
            "ave_precision_score": 0.5154736195054108,
            "fpr": 0.22063666300768386,
            "logloss": 6.497052542801277,
            "mae": 0.36542626004893713,
            "precision": 0.6410714285714286,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5888157894736842,
            "auc_prc": 0.5084016249723163,
            "auditor_fn_violation": 0.02447980416156671,
            "auditor_fp_violation": 0.030849118810694166,
            "ave_precision_score": 0.471752082532886,
            "fpr": 0.2543859649122807,
            "logloss": 6.830907508048066,
            "mae": 0.4417323368568823,
            "precision": 0.5871886120996441,
            "recall": 0.6976744186046512
        },
        "train": {
            "accuracy": 0.605927552140505,
            "auc_prc": 0.5244827420548228,
            "auditor_fn_violation": 0.03162776049713481,
            "auditor_fp_violation": 0.006228780027059459,
            "ave_precision_score": 0.4926746122645882,
            "fpr": 0.22502744237102085,
            "logloss": 6.511354903438121,
            "mae": 0.42491388179422646,
            "precision": 0.6146616541353384,
            "recall": 0.6798336798336798
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5975877192982456,
            "auc_prc": 0.5148945459102238,
            "auditor_fn_violation": 0.0037832424613330386,
            "auditor_fp_violation": 0.016584742037325666,
            "ave_precision_score": 0.46952524545794583,
            "fpr": 0.3432017543859649,
            "logloss": 7.5619509153796525,
            "mae": 0.4137085354954603,
            "precision": 0.5724043715846995,
            "recall": 0.8858350951374208
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.5253841254273329,
            "auditor_fn_violation": 0.006942178182573354,
            "auditor_fp_violation": 0.010841651137262914,
            "ave_precision_score": 0.4850434795067935,
            "fpr": 0.32821075740944017,
            "logloss": 7.185794626486692,
            "mae": 0.3954012418759317,
            "precision": 0.5892857142857143,
            "recall": 0.8918918918918919
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8138025772073607,
            "auditor_fn_violation": 0.010781777382144583,
            "auditor_fp_violation": 0.0062892139231906655,
            "ave_precision_score": 0.801798805711756,
            "fpr": 0.0756578947368421,
            "logloss": 3.6365363414387613,
            "mae": 0.25578241465422125,
            "precision": 0.8203125,
            "recall": 0.6659619450317125
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8422222256795271,
            "auditor_fn_violation": 0.016260032725455346,
            "auditor_fp_violation": 0.009225742220406915,
            "ave_precision_score": 0.8297667850718813,
            "fpr": 0.05817782656421515,
            "logloss": 3.4983931635618477,
            "mae": 0.23825653383265719,
            "precision": 0.8605263157894737,
            "recall": 0.6798336798336798
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 9296,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.5334941896903008,
            "auditor_fn_violation": 0.014752791068580543,
            "auditor_fp_violation": 0.026188406665867414,
            "ave_precision_score": 0.4918717336999488,
            "fpr": 0.25219298245614036,
            "logloss": 6.956463677135144,
            "mae": 0.39540272792790054,
            "precision": 0.5993031358885017,
            "recall": 0.7272727272727273
        },
        "train": {
            "accuracy": 0.6377607025246982,
            "auc_prc": 0.5532817767104657,
            "auditor_fn_violation": 0.015103003028359785,
            "auditor_fp_violation": 0.005485921425471629,
            "ave_precision_score": 0.512163867652623,
            "fpr": 0.21624588364434688,
            "logloss": 6.650581867248927,
            "mae": 0.37295916899189613,
            "precision": 0.6385321100917432,
            "recall": 0.7234927234927235
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.5259561374800246,
            "auditor_fn_violation": 0.01884434924520605,
            "auditor_fp_violation": 0.023573312552451747,
            "ave_precision_score": 0.48803985470834566,
            "fpr": 0.2532894736842105,
            "logloss": 6.74679835251894,
            "mae": 0.3979644876751097,
            "precision": 0.6044520547945206,
            "recall": 0.7463002114164905
        },
        "train": {
            "accuracy": 0.6366630076838639,
            "auc_prc": 0.5448660241326015,
            "auditor_fn_violation": 0.019302085163775618,
            "auditor_fp_violation": 0.004666479462895364,
            "ave_precision_score": 0.5091850882149713,
            "fpr": 0.2261251372118551,
            "logloss": 6.438194292469811,
            "mae": 0.3771065355572433,
            "precision": 0.6334519572953736,
            "recall": 0.7401247401247402
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8304166318184076,
            "auditor_fn_violation": 0.011627906976744191,
            "auditor_fp_violation": 0.0031521000679374995,
            "ave_precision_score": 0.8198744465630186,
            "fpr": 0.06140350877192982,
            "logloss": 3.6672587156251515,
            "mae": 0.26390560359836795,
            "precision": 0.8390804597701149,
            "recall": 0.6173361522198731
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8662572113019897,
            "auditor_fn_violation": 0.013594528413408778,
            "auditor_fp_violation": 0.010348964848237309,
            "ave_precision_score": 0.8552277187017647,
            "fpr": 0.036223929747530186,
            "logloss": 3.4084441872412263,
            "mae": 0.24211584260803257,
            "precision": 0.8996960486322189,
            "recall": 0.6153846153846154
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 9296,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.5144609314587294,
            "auditor_fn_violation": 0.018719168428470757,
            "auditor_fp_violation": 0.0272898932981657,
            "ave_precision_score": 0.4785156116435546,
            "fpr": 0.24561403508771928,
            "logloss": 6.813727646761048,
            "mae": 0.4275507511863483,
            "precision": 0.6014234875444839,
            "recall": 0.7145877378435518
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.5291310301600272,
            "auditor_fn_violation": 0.022382933469651364,
            "auditor_fp_violation": 0.010726776095780256,
            "ave_precision_score": 0.49797400216213017,
            "fpr": 0.2217343578485181,
            "logloss": 6.490922145989833,
            "mae": 0.4082563742202436,
            "precision": 0.6293577981651376,
            "recall": 0.7130977130977131
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7484483089797942,
            "auditor_fn_violation": 0.03387300174325878,
            "auditor_fp_violation": 0.012036426487631379,
            "ave_precision_score": 0.7493482981746065,
            "fpr": 0.13267543859649122,
            "logloss": 0.7303390487879533,
            "mae": 0.3356609499951287,
            "precision": 0.7256235827664399,
            "recall": 0.6765327695560254
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8222020635805602,
            "auditor_fn_violation": 0.033227519506334,
            "auditor_fp_violation": 0.013846271666709216,
            "ave_precision_score": 0.8219213375071199,
            "fpr": 0.1119648737650933,
            "logloss": 0.6098481532163689,
            "mae": 0.3064628256205592,
            "precision": 0.7748344370860927,
            "recall": 0.7297297297297297
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.5365626714193978,
            "auditor_fn_violation": 0.013651663513964614,
            "auditor_fp_violation": 0.022414378771530196,
            "ave_precision_score": 0.505867679757378,
            "fpr": 0.23464912280701755,
            "logloss": 5.951153369378703,
            "mae": 0.3783447328041603,
            "precision": 0.624561403508772,
            "recall": 0.7526427061310782
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.5448608358348017,
            "auditor_fn_violation": 0.010575297073650536,
            "auditor_fp_violation": 0.012238021085952067,
            "ave_precision_score": 0.5191480728552254,
            "fpr": 0.21295279912184412,
            "logloss": 5.960070982513441,
            "mae": 0.34967497148820786,
            "precision": 0.6590509666080844,
            "recall": 0.7796257796257796
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8371603036765709,
            "auditor_fn_violation": 0.009154426764585888,
            "auditor_fp_violation": 0.007705411021859891,
            "ave_precision_score": 0.8365705409936726,
            "fpr": 0.09868421052631579,
            "logloss": 1.001003856328604,
            "mae": 0.2601735405767007,
            "precision": 0.7862232779097387,
            "recall": 0.6997885835095138
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8722155766592963,
            "auditor_fn_violation": 0.019610169994363193,
            "auditor_fp_violation": 0.0023945064202384315,
            "ave_precision_score": 0.8714480904075872,
            "fpr": 0.06915477497255763,
            "logloss": 0.8573733935659756,
            "mae": 0.22938538079809767,
            "precision": 0.8455882352941176,
            "recall": 0.7172557172557172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.540080911983911,
            "auditor_fn_violation": 0.00990087533845184,
            "auditor_fp_violation": 0.02587869160372458,
            "ave_precision_score": 0.4961213980225846,
            "fpr": 0.2532894736842105,
            "logloss": 6.897315239839957,
            "mae": 0.3864655122775718,
            "precision": 0.6071428571428571,
            "recall": 0.7547568710359408
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.5591560686057169,
            "auditor_fn_violation": 0.01460093885999485,
            "auditor_fp_violation": 0.004783907283077628,
            "ave_precision_score": 0.516369383757826,
            "fpr": 0.22063666300768386,
            "logloss": 6.572393778122887,
            "mae": 0.3617845370742317,
            "precision": 0.6410714285714286,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.5364024728541634,
            "auditor_fn_violation": 0.012958532695374801,
            "auditor_fp_violation": 0.023855552891339976,
            "ave_precision_score": 0.49417437041861456,
            "fpr": 0.2576754385964912,
            "logloss": 6.833802007746666,
            "mae": 0.38936742112853884,
            "precision": 0.6023688663282571,
            "recall": 0.7526427061310782
        },
        "train": {
            "accuracy": 0.6399560922063666,
            "auc_prc": 0.5543543656737303,
            "auditor_fn_violation": 0.010369907186592153,
            "auditor_fp_violation": 0.006785285783575436,
            "ave_precision_score": 0.5138179106045702,
            "fpr": 0.22941822173435786,
            "logloss": 6.516967283816702,
            "mae": 0.366260613506854,
            "precision": 0.6339754816112084,
            "recall": 0.7525987525987526
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7534563576434804,
            "auditor_fn_violation": 0.006731946144430842,
            "auditor_fp_violation": 0.007895236382528077,
            "ave_precision_score": 0.7471486352428998,
            "fpr": 0.09210526315789473,
            "logloss": 1.4434357062583978,
            "mae": 0.2911162346480401,
            "precision": 0.7857142857142857,
            "recall": 0.6511627906976745
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8243436701341367,
            "auditor_fn_violation": 0.008398164270831681,
            "auditor_fp_violation": 0.005953079927501086,
            "ave_precision_score": 0.8198174117824195,
            "fpr": 0.06915477497255763,
            "logloss": 1.0743831020515064,
            "mae": 0.2566410883853486,
            "precision": 0.8342105263157895,
            "recall": 0.659043659043659
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8535373001091606,
            "auditor_fn_violation": 0.010005192685731246,
            "auditor_fp_violation": 0.011486932022539264,
            "ave_precision_score": 0.8518732761657779,
            "fpr": 0.10855263157894737,
            "logloss": 1.1477852414820868,
            "mae": 0.2442950417445078,
            "precision": 0.7819383259911894,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.7991218441273326,
            "auc_prc": 0.8817791606267884,
            "auditor_fn_violation": 0.010531936986382658,
            "auditor_fp_violation": 0.007780869476425092,
            "ave_precision_score": 0.8809841771325095,
            "fpr": 0.08562019758507135,
            "logloss": 0.975639127039242,
            "mae": 0.21121830145459727,
            "precision": 0.8281938325991189,
            "recall": 0.7817047817047817
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6118421052631579,
            "auc_prc": 0.5187563445400877,
            "auditor_fn_violation": 0.016586458217425173,
            "auditor_fp_violation": 0.015380849618351133,
            "ave_precision_score": 0.48281220852847007,
            "fpr": 0.2982456140350877,
            "logloss": 6.648193239235032,
            "mae": 0.4213769795585489,
            "precision": 0.5897435897435898,
            "recall": 0.8266384778012685
        },
        "train": {
            "accuracy": 0.6333699231613611,
            "auc_prc": 0.5313892650980694,
            "auditor_fn_violation": 0.015639298844567783,
            "auditor_fp_violation": 0.018155362111658555,
            "ave_precision_score": 0.5022830343296779,
            "fpr": 0.2722283205268935,
            "logloss": 6.356992944600779,
            "mae": 0.3995894848928264,
            "precision": 0.614307931570762,
            "recall": 0.8212058212058212
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.82726340185394,
            "auditor_fn_violation": 0.009272653091502544,
            "auditor_fp_violation": 0.005020381249250695,
            "ave_precision_score": 0.8195041190672918,
            "fpr": 0.09978070175438597,
            "logloss": 3.519239790205635,
            "mae": 0.26177238135298203,
            "precision": 0.7833333333333333,
            "recall": 0.6955602536997886
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8588943428089055,
            "auditor_fn_violation": 0.005340137063517969,
            "auditor_fp_violation": 0.0095218645495622,
            "ave_precision_score": 0.8515234867564676,
            "fpr": 0.07683863885839737,
            "logloss": 3.3075730739331806,
            "mae": 0.2348947071599734,
            "precision": 0.833729216152019,
            "recall": 0.7297297297297297
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.5407307039330205,
            "auditor_fn_violation": 0.010633414932680546,
            "auditor_fp_violation": 0.023778124125804267,
            "ave_precision_score": 0.4961888110409853,
            "fpr": 0.2576754385964912,
            "logloss": 6.88831308146857,
            "mae": 0.38687745341342417,
            "precision": 0.6057046979865772,
            "recall": 0.7632135306553911
        },
        "train": {
            "accuracy": 0.6421514818880352,
            "auc_prc": 0.5599783145929014,
            "auditor_fn_violation": 0.01460093885999485,
            "auditor_fp_violation": 0.004053812574987877,
            "ave_precision_score": 0.5166208416459915,
            "fpr": 0.2239297475301866,
            "logloss": 6.55752594656129,
            "mae": 0.3615733491069587,
            "precision": 0.6376554174067496,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.6140541220879038,
            "auditor_fn_violation": 0.009970420236638104,
            "auditor_fp_violation": 0.021053131119370183,
            "ave_precision_score": 0.5775749670516036,
            "fpr": 0.21600877192982457,
            "logloss": 4.734293774958229,
            "mae": 0.33447005214666153,
            "precision": 0.6469534050179212,
            "recall": 0.7632135306553911
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.6209920368591377,
            "auditor_fn_violation": 0.011526936883687708,
            "auditor_fp_violation": 0.006478952339621682,
            "ave_precision_score": 0.5833716259650322,
            "fpr": 0.18990120746432493,
            "logloss": 4.814186540250068,
            "mae": 0.3111837782915907,
            "precision": 0.6843065693430657,
            "recall": 0.7796257796257796
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 9296,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.5146021004343637,
            "auditor_fn_violation": 0.014476929639108346,
            "auditor_fp_violation": 0.024260180633816898,
            "ave_precision_score": 0.4780041193419031,
            "fpr": 0.26973684210526316,
            "logloss": 6.684820224807167,
            "mae": 0.4302391554040042,
            "precision": 0.594059405940594,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.6410537870472008,
            "auc_prc": 0.531348201898359,
            "auditor_fn_violation": 0.018747532468717985,
            "auditor_fp_violation": 0.010869731702958671,
            "ave_precision_score": 0.5015548328468501,
            "fpr": 0.24259055982436883,
            "logloss": 6.381624972590873,
            "mae": 0.4005263692403031,
            "precision": 0.6291946308724832,
            "recall": 0.7796257796257796
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5515350877192983,
            "auc_prc": 0.5102970653288625,
            "auditor_fn_violation": 0.01903675679685472,
            "auditor_fp_violation": 0.029532829796587137,
            "ave_precision_score": 0.475006522227265,
            "fpr": 0.2324561403508772,
            "logloss": 6.70328085130502,
            "mae": 0.4702742620929871,
            "precision": 0.5655737704918032,
            "recall": 0.5835095137420718
        },
        "train": {
            "accuracy": 0.562019758507135,
            "auc_prc": 0.5229489588086049,
            "auditor_fn_violation": 0.025078105209828592,
            "auditor_fp_violation": 0.005544635335562772,
            "ave_precision_score": 0.49456403770811475,
            "fpr": 0.20636663007683864,
            "logloss": 6.469054725771135,
            "mae": 0.45684317498721905,
            "precision": 0.5895196506550219,
            "recall": 0.5613305613305614
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.5516946304776981,
            "auditor_fn_violation": 0.008417250843811433,
            "auditor_fp_violation": 0.03225782280302124,
            "ave_precision_score": 0.5026464681157039,
            "fpr": 0.24671052631578946,
            "logloss": 6.67335981436372,
            "mae": 0.3908551801397635,
            "precision": 0.609375,
            "recall": 0.7420718816067653
        },
        "train": {
            "accuracy": 0.6542261251372119,
            "auc_prc": 0.5542662132909378,
            "auditor_fn_violation": 0.015516064912332758,
            "auditor_fp_violation": 0.00774002501723126,
            "ave_precision_score": 0.5143796012487128,
            "fpr": 0.21734357848518113,
            "logloss": 6.441273103663225,
            "mae": 0.3703899694462486,
            "precision": 0.6476868327402135,
            "recall": 0.7567567567567568
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.7910511891515217,
            "auditor_fn_violation": 0.008034753903786954,
            "auditor_fp_violation": 0.010293030412020948,
            "ave_precision_score": 0.7446641277188596,
            "fpr": 0.1206140350877193,
            "logloss": 5.47120380018753,
            "mae": 0.267352743358604,
            "precision": 0.7555555555555555,
            "recall": 0.718816067653277
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8151628656741059,
            "auditor_fn_violation": 0.014804046637197029,
            "auditor_fp_violation": 0.012661782350088075,
            "ave_precision_score": 0.7711184931847483,
            "fpr": 0.10318331503841932,
            "logloss": 5.358547609706757,
            "mae": 0.250052162812297,
            "precision": 0.7878103837471784,
            "recall": 0.7255717255717256
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 9296,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.5354108304365441,
            "auditor_fn_violation": 0.008533159007455214,
            "auditor_fp_violation": 0.026508112536466455,
            "ave_precision_score": 0.4949948729002283,
            "fpr": 0.2543859649122807,
            "logloss": 6.813277543590282,
            "mae": 0.3850580535075918,
            "precision": 0.6061120543293718,
            "recall": 0.7547568710359408
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.5534583671910904,
            "auditor_fn_violation": 0.013631042171108039,
            "auditor_fp_violation": 0.006851658029765403,
            "ave_precision_score": 0.5146907238744067,
            "fpr": 0.2217343578485181,
            "logloss": 6.501435708460237,
            "mae": 0.36103654675082314,
            "precision": 0.6399286987522281,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.5246624691687157,
            "auditor_fn_violation": 0.01588405474574386,
            "auditor_fp_violation": 0.026043539943252224,
            "ave_precision_score": 0.488718725874371,
            "fpr": 0.2412280701754386,
            "logloss": 6.742797063461319,
            "mae": 0.40024057712881794,
            "precision": 0.608540925266904,
            "recall": 0.7230443974630021
        },
        "train": {
            "accuracy": 0.6421514818880352,
            "auc_prc": 0.5367879807088234,
            "auditor_fn_violation": 0.019973025461499667,
            "auditor_fp_violation": 0.007045669210936106,
            "ave_precision_score": 0.5069959922594948,
            "fpr": 0.20856201975850713,
            "logloss": 6.472976775407997,
            "mae": 0.3809041969644527,
            "precision": 0.6448598130841121,
            "recall": 0.7172557172557172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8213178013403448,
            "auditor_fn_violation": 0.009968102073365237,
            "auditor_fp_violation": 0.008209946848899015,
            "ave_precision_score": 0.8090356523920194,
            "fpr": 0.11842105263157894,
            "logloss": 3.715570632152749,
            "mae": 0.26809997606355246,
            "precision": 0.7610619469026548,
            "recall": 0.7272727272727273
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8506137829101059,
            "auditor_fn_violation": 0.008389035831406856,
            "auditor_fp_violation": 0.015102238786919563,
            "ave_precision_score": 0.8394955265131617,
            "fpr": 0.09220636663007684,
            "logloss": 3.4941743914661085,
            "mae": 0.244165293203507,
            "precision": 0.8082191780821918,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 9296,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.5389121529678415,
            "auditor_fn_violation": 0.009427970030785209,
            "auditor_fp_violation": 0.02422771050633418,
            "ave_precision_score": 0.4955666704517894,
            "fpr": 0.25548245614035087,
            "logloss": 6.894479541477844,
            "mae": 0.38671095684867995,
            "precision": 0.6057529610829103,
            "recall": 0.7568710359408034
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.5580688713853182,
            "auditor_fn_violation": 0.01460093885999485,
            "auditor_fp_violation": 0.006601485717203186,
            "ave_precision_score": 0.5158500318277273,
            "fpr": 0.2217343578485181,
            "logloss": 6.566983881837463,
            "mae": 0.3621875309635916,
            "precision": 0.6399286987522281,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.6723221934785055,
            "auditor_fn_violation": 0.007814528392863767,
            "auditor_fp_violation": 0.02078587699316629,
            "ave_precision_score": 0.5729935692853054,
            "fpr": 0.26096491228070173,
            "logloss": 7.841057965667109,
            "mae": 0.3640284930750429,
            "precision": 0.6148867313915858,
            "recall": 0.8033826638477801
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.6963497869148623,
            "auditor_fn_violation": 0.01349867979944819,
            "auditor_fp_violation": 0.01047915656191765,
            "ave_precision_score": 0.5964392091680962,
            "fpr": 0.23380900109769484,
            "logloss": 7.656972927490239,
            "mae": 0.3350000673727225,
            "precision": 0.6461794019933554,
            "recall": 0.8087318087318087
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.840941837661737,
            "auditor_fn_violation": 0.007285987166648123,
            "auditor_fp_violation": 0.005922051712424572,
            "ave_precision_score": 0.8401435825201891,
            "fpr": 0.125,
            "logloss": 0.9236566484179612,
            "mae": 0.2602083047261807,
            "precision": 0.7548387096774194,
            "recall": 0.7420718816067653
        },
        "train": {
            "accuracy": 0.7892425905598244,
            "auc_prc": 0.8782693648120582,
            "auditor_fn_violation": 0.01294184499453435,
            "auditor_fp_violation": 0.001756311745334802,
            "ave_precision_score": 0.8774971703187076,
            "fpr": 0.09440175631174534,
            "logloss": 0.7548290221278869,
            "mae": 0.22410852182883276,
            "precision": 0.8134490238611713,
            "recall": 0.7796257796257796
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6501074939180063,
            "auditor_fn_violation": 0.030736526835058047,
            "auditor_fp_violation": 0.020740918355113293,
            "ave_precision_score": 0.6504435811949805,
            "fpr": 0.1337719298245614,
            "logloss": 1.040952621503432,
            "mae": 0.3806442910465289,
            "precision": 0.7031630170316302,
            "recall": 0.6109936575052854
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.6917656715279629,
            "auditor_fn_violation": 0.033505936908790925,
            "auditor_fp_violation": 0.011487504148265389,
            "ave_precision_score": 0.6918020688362957,
            "fpr": 0.1207464324917673,
            "logloss": 0.9461139791493542,
            "mae": 0.36920139825423415,
            "precision": 0.7243107769423559,
            "recall": 0.6008316008316008
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5888157894736842,
            "auc_prc": 0.5084598656940005,
            "auditor_fn_violation": 0.02447980416156671,
            "auditor_fp_violation": 0.030849118810694166,
            "ave_precision_score": 0.47181031236689086,
            "fpr": 0.2543859649122807,
            "logloss": 6.830236803608633,
            "mae": 0.4416641507066966,
            "precision": 0.5871886120996441,
            "recall": 0.6976744186046512
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.5244993233288577,
            "auditor_fn_violation": 0.0306030931716991,
            "auditor_fp_violation": 0.006228780027059459,
            "ave_precision_score": 0.4926887763340318,
            "fpr": 0.22502744237102085,
            "logloss": 6.510696699625506,
            "mae": 0.42484970626822016,
            "precision": 0.6153846153846154,
            "recall": 0.681912681912682
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.5288025504594716,
            "auditor_fn_violation": 0.010665869218500806,
            "auditor_fp_violation": 0.023965451784358396,
            "ave_precision_score": 0.491520231815589,
            "fpr": 0.24671052631578946,
            "logloss": 6.757360538290445,
            "mae": 0.3892041834480009,
            "precision": 0.6066433566433567,
            "recall": 0.733615221987315
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.5394930549399071,
            "auditor_fn_violation": 0.020114516272584333,
            "auditor_fp_violation": 0.005577821458657759,
            "ave_precision_score": 0.5070522194222339,
            "fpr": 0.21624588364434688,
            "logloss": 6.475347865570915,
            "mae": 0.3716944553330739,
            "precision": 0.6431159420289855,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6162280701754386,
            "auc_prc": 0.5374294426140993,
            "auditor_fn_violation": 0.00846361410926895,
            "auditor_fp_violation": 0.023645745913759342,
            "ave_precision_score": 0.495215974557489,
            "fpr": 0.25548245614035087,
            "logloss": 6.880954456673073,
            "mae": 0.3861323800329126,
            "precision": 0.6044142614601019,
            "recall": 0.7526427061310782
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.5565502550218395,
            "auditor_fn_violation": 0.011250801591087,
            "auditor_fp_violation": 0.004855385086666835,
            "ave_precision_score": 0.5154379819400692,
            "fpr": 0.2217343578485181,
            "logloss": 6.552484796204274,
            "mae": 0.3607760448910233,
            "precision": 0.6392857142857142,
            "recall": 0.7442827442827443
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 9296,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.5393579669815403,
            "auditor_fn_violation": 0.00846361410926895,
            "auditor_fp_violation": 0.024552411781161344,
            "ave_precision_score": 0.49594998866493056,
            "fpr": 0.2532894736842105,
            "logloss": 6.86826143618278,
            "mae": 0.38773828142071626,
            "precision": 0.606473594548552,
            "recall": 0.7526427061310782
        },
        "train": {
            "accuracy": 0.6465422612513722,
            "auc_prc": 0.556736010653781,
            "auditor_fn_violation": 0.0182203650919348,
            "auditor_fp_violation": 0.007837030607816608,
            "ave_precision_score": 0.5150435021766631,
            "fpr": 0.2217343578485181,
            "logloss": 6.555539069847246,
            "mae": 0.3628925343807301,
            "precision": 0.6412078152753108,
            "recall": 0.7505197505197505
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 9296,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8361840075459146,
            "auditor_fn_violation": 0.012360446570972886,
            "auditor_fp_violation": 0.005145266354953447,
            "ave_precision_score": 0.8266208312246757,
            "fpr": 0.06907894736842106,
            "logloss": 3.4830263867153475,
            "mae": 0.261811153460506,
            "precision": 0.8273972602739726,
            "recall": 0.638477801268499
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8668774472951792,
            "auditor_fn_violation": 0.008199620713341911,
            "auditor_fp_violation": 0.0043907793633369925,
            "ave_precision_score": 0.8571501526835436,
            "fpr": 0.04610318331503842,
            "logloss": 3.2957140494059103,
            "mae": 0.22993566545593622,
            "precision": 0.8842975206611571,
            "recall": 0.6673596673596673
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8047695775678824,
            "auditor_fn_violation": 0.004946960424316612,
            "auditor_fp_violation": 0.00344433121528194,
            "ave_precision_score": 0.798289081184453,
            "fpr": 0.11951754385964912,
            "logloss": 1.338401173244299,
            "mae": 0.2703663499883815,
            "precision": 0.7588495575221239,
            "recall": 0.7251585623678647
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8493782653621373,
            "auditor_fn_violation": 0.01541565207865977,
            "auditor_fp_violation": 0.004041048681489803,
            "ave_precision_score": 0.8457104773556513,
            "fpr": 0.0867178924259056,
            "logloss": 1.0357480858693662,
            "mae": 0.23767857491143746,
            "precision": 0.8167053364269141,
            "recall": 0.7318087318087318
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.5307823295900564,
            "auditor_fn_violation": 0.013461574125588823,
            "auditor_fp_violation": 0.03137613395675978,
            "ave_precision_score": 0.4941787312080625,
            "fpr": 0.26206140350877194,
            "logloss": 6.820499653173851,
            "mae": 0.3872015548578626,
            "precision": 0.6016666666666667,
            "recall": 0.7632135306553911
        },
        "train": {
            "accuracy": 0.6520307354555434,
            "auc_prc": 0.5403027801124357,
            "auditor_fn_violation": 0.012734172997619765,
            "auditor_fp_violation": 0.010716564980981804,
            "ave_precision_score": 0.5099571365600083,
            "fpr": 0.22941822173435786,
            "logloss": 6.5148311255289855,
            "mae": 0.35496828055851104,
            "precision": 0.6408934707903781,
            "recall": 0.7754677754677755
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.5108945799775764,
            "auditor_fn_violation": 0.03445254256147769,
            "auditor_fp_violation": 0.029203133117531872,
            "ave_precision_score": 0.4762258338343765,
            "fpr": 0.35526315789473684,
            "logloss": 6.807959554416025,
            "mae": 0.44074027666846194,
            "precision": 0.55858310626703,
            "recall": 0.8668076109936576
        },
        "train": {
            "accuracy": 0.601536772777168,
            "auc_prc": 0.5215290998410421,
            "auditor_fn_violation": 0.03016949229902029,
            "auditor_fp_violation": 0.014857172031756573,
            "ave_precision_score": 0.49109342170437387,
            "fpr": 0.32930845225027444,
            "logloss": 6.477322382040654,
            "mae": 0.42649633493466294,
            "precision": 0.5821727019498607,
            "recall": 0.8690228690228691
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5921052631578947,
            "auc_prc": 0.5176588938558853,
            "auditor_fn_violation": 0.012476354734616665,
            "auditor_fp_violation": 0.024749730248171706,
            "ave_precision_score": 0.47845169740709226,
            "fpr": 0.3026315789473684,
            "logloss": 6.74147738471337,
            "mae": 0.4304761000083792,
            "precision": 0.5773353751914242,
            "recall": 0.7970401691331924
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.5327655883184699,
            "auditor_fn_violation": 0.010536501206095067,
            "auditor_fp_violation": 0.015301355525489505,
            "ave_precision_score": 0.5002302404408205,
            "fpr": 0.2645444566410538,
            "logloss": 6.4297411276507015,
            "mae": 0.3999217423352512,
            "precision": 0.622848200312989,
            "recall": 0.8274428274428275
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.5498551794939381,
            "auditor_fn_violation": 0.008106616965246106,
            "auditor_fp_violation": 0.02564890300923152,
            "ave_precision_score": 0.5020927394978913,
            "fpr": 0.2532894736842105,
            "logloss": 6.9771213758773705,
            "mae": 0.38310083275626733,
            "precision": 0.60580204778157,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6509330406147091,
            "auc_prc": 0.5682825647120324,
            "auditor_fn_violation": 0.013053668377488356,
            "auditor_fp_violation": 0.003114390013529733,
            "ave_precision_score": 0.5219186264865588,
            "fpr": 0.22063666300768386,
            "logloss": 6.640572342946353,
            "mae": 0.3530209944092383,
            "precision": 0.6442477876106195,
            "recall": 0.7567567567567568
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5285087719298246,
            "auc_prc": 0.4924414420870902,
            "auditor_fn_violation": 0.015819146174103346,
            "auditor_fp_violation": 0.025813751348759146,
            "ave_precision_score": 0.45718764254162547,
            "fpr": 0.23793859649122806,
            "logloss": 8.502449190655053,
            "mae": 0.47292688015215256,
            "precision": 0.5450733752620545,
            "recall": 0.5496828752642706
        },
        "train": {
            "accuracy": 0.544456641053787,
            "auc_prc": 0.4998114765857602,
            "auditor_fn_violation": 0.013697223356937965,
            "auditor_fp_violation": 0.010338753733438845,
            "ave_precision_score": 0.4707129081284279,
            "fpr": 0.20856201975850713,
            "logloss": 8.208399554716635,
            "mae": 0.4621103146550797,
            "precision": 0.5739910313901345,
            "recall": 0.5322245322245323
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5997807017543859,
            "auc_prc": 0.5957876532690145,
            "auditor_fn_violation": 0.006298449612403101,
            "auditor_fp_violation": 0.02828647644167367,
            "ave_precision_score": 0.5201128033142167,
            "fpr": 0.3092105263157895,
            "logloss": 7.292871886460463,
            "mae": 0.3968123081148298,
            "precision": 0.5803571428571429,
            "recall": 0.8245243128964059
        },
        "train": {
            "accuracy": 0.6630076838638859,
            "auc_prc": 0.6057825304219439,
            "auditor_fn_violation": 0.008658324794438958,
            "auditor_fp_violation": 0.00975161463252751,
            "ave_precision_score": 0.5340989735939122,
            "fpr": 0.26344676180021953,
            "logloss": 6.990114563720838,
            "mae": 0.3492674657398902,
            "precision": 0.6330275229357798,
            "recall": 0.8607068607068608
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8311209659877605,
            "auditor_fn_violation": 0.009077927376580986,
            "auditor_fp_violation": 0.00510780082324262,
            "ave_precision_score": 0.8205962372120658,
            "fpr": 0.0537280701754386,
            "logloss": 3.679561519545932,
            "mae": 0.26379816295198416,
            "precision": 0.8537313432835821,
            "recall": 0.6046511627906976
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8641627626500503,
            "auditor_fn_violation": 0.014687659034530618,
            "auditor_fp_violation": 0.01017027033926429,
            "ave_precision_score": 0.8531435244594938,
            "fpr": 0.03512623490669594,
            "logloss": 3.5106002659068336,
            "mae": 0.24589970733640507,
            "precision": 0.9012345679012346,
            "recall": 0.6070686070686071
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.5381811778958284,
            "auditor_fn_violation": 0.007582712065576207,
            "auditor_fp_violation": 0.025941134156575948,
            "ave_precision_score": 0.49535214130634075,
            "fpr": 0.2565789473684211,
            "logloss": 6.808890816757548,
            "mae": 0.38697733475390506,
            "precision": 0.6086956521739131,
            "recall": 0.7695560253699789
        },
        "train": {
            "accuracy": 0.6498353457738749,
            "auc_prc": 0.5570068490944854,
            "auditor_fn_violation": 0.01817015867509831,
            "auditor_fp_violation": 0.0040869986980828655,
            "ave_precision_score": 0.5158754335986633,
            "fpr": 0.22722283205268934,
            "logloss": 6.479070735422415,
            "mae": 0.3629536501247708,
            "precision": 0.640625,
            "recall": 0.7671517671517671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.5259661097944693,
            "auditor_fn_violation": 0.08149966618448871,
            "auditor_fp_violation": 0.05481457059505257,
            "ave_precision_score": 0.47466357286700045,
            "fpr": 0.32894736842105265,
            "logloss": 7.7720184988323275,
            "mae": 0.47855364643737286,
            "precision": 0.5215311004784688,
            "recall": 0.6913319238900634
        },
        "train": {
            "accuracy": 0.5422612513721186,
            "auc_prc": 0.5522143817256643,
            "auditor_fn_violation": 0.0777606112403039,
            "auditor_fp_violation": 0.054731575319735555,
            "ave_precision_score": 0.49854450672562667,
            "fpr": 0.31284302963776073,
            "logloss": 7.25870832716765,
            "mae": 0.4524242436591828,
            "precision": 0.5504731861198738,
            "recall": 0.7255717255717256
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.5167263645774337,
            "auditor_fn_violation": 0.002457253069248173,
            "auditor_fp_violation": 0.004720656995564088,
            "ave_precision_score": 0.4717307281468388,
            "fpr": 0.4375,
            "logloss": 7.660490508223635,
            "mae": 0.4432180758607583,
            "precision": 0.5387283236994219,
            "recall": 0.985200845665962
        },
        "train": {
            "accuracy": 0.5740944017563118,
            "auc_prc": 0.5324127498280896,
            "auditor_fn_violation": 0.001821123665250998,
            "auditor_fp_violation": 0.0027978454547775283,
            "ave_precision_score": 0.49065942159448933,
            "fpr": 0.41602634467618005,
            "logloss": 7.202833896316504,
            "mae": 0.4284879973881013,
            "precision": 0.554641598119859,
            "recall": 0.9812889812889813
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5888157894736842,
            "auc_prc": 0.5085025823198145,
            "auditor_fn_violation": 0.02447980416156671,
            "auditor_fp_violation": 0.030849118810694166,
            "ave_precision_score": 0.47185302264617013,
            "fpr": 0.2543859649122807,
            "logloss": 6.830027338969873,
            "mae": 0.4416583698598276,
            "precision": 0.5871886120996441,
            "recall": 0.6976744186046512
        },
        "train": {
            "accuracy": 0.605927552140505,
            "auc_prc": 0.5245242405111092,
            "auditor_fn_violation": 0.03162776049713481,
            "auditor_fp_violation": 0.006228780027059459,
            "ave_precision_score": 0.49271369358066874,
            "fpr": 0.22502744237102085,
            "logloss": 6.510583578876203,
            "mae": 0.42486390384894507,
            "precision": 0.6146616541353384,
            "recall": 0.6798336798336798
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.827618339222541,
            "auditor_fn_violation": 0.010334371870479595,
            "auditor_fp_violation": 0.010445390240978302,
            "ave_precision_score": 0.8183403597316026,
            "fpr": 0.07236842105263158,
            "logloss": 3.5703017948320244,
            "mae": 0.260695890733808,
            "precision": 0.8186813186813187,
            "recall": 0.6300211416490487
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8589167436307856,
            "auditor_fn_violation": 0.015267314938006496,
            "auditor_fp_violation": 0.007367319327087535,
            "ave_precision_score": 0.8482487164909145,
            "fpr": 0.04610318331503842,
            "logloss": 3.401519758114357,
            "mae": 0.23568488018406442,
            "precision": 0.8820224719101124,
            "recall": 0.6528066528066528
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.5497141085626893,
            "auditor_fn_violation": 0.022986907013834802,
            "auditor_fp_violation": 0.018785217599808182,
            "ave_precision_score": 0.5020100437812463,
            "fpr": 0.2598684210526316,
            "logloss": 7.127447255429498,
            "mae": 0.3789121120414622,
            "precision": 0.6095551894563427,
            "recall": 0.7822410147991543
        },
        "train": {
            "accuracy": 0.6553238199780461,
            "auc_prc": 0.5630559639890899,
            "auditor_fn_violation": 0.0065998617041427175,
            "auditor_fp_violation": 0.01641436703852143,
            "ave_precision_score": 0.5183418733400982,
            "fpr": 0.22502744237102085,
            "logloss": 6.816635381100112,
            "mae": 0.35663073883478086,
            "precision": 0.6447140381282496,
            "recall": 0.7733887733887734
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.5385351680412297,
            "auditor_fn_violation": 0.006986944104447165,
            "auditor_fp_violation": 0.02281151340766495,
            "ave_precision_score": 0.4964023852769613,
            "fpr": 0.27631578947368424,
            "logloss": 6.826377554173487,
            "mae": 0.3831277218848156,
            "precision": 0.597444089456869,
            "recall": 0.7906976744186046
        },
        "train": {
            "accuracy": 0.6575192096597146,
            "auc_prc": 0.5570802926211453,
            "auditor_fn_violation": 0.01501171863411161,
            "auditor_fp_violation": 0.009828197993515954,
            "ave_precision_score": 0.5166099662012881,
            "fpr": 0.23710208562019758,
            "logloss": 6.476289579854082,
            "mae": 0.35086791496183584,
            "precision": 0.6405990016638935,
            "recall": 0.8004158004158004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8135473652276737,
            "auditor_fn_violation": 0.004330328993731686,
            "auditor_fp_violation": 0.004291052231946611,
            "ave_precision_score": 0.7969938748575632,
            "fpr": 0.06359649122807018,
            "logloss": 3.9898302520235682,
            "mae": 0.2643001398918893,
            "precision": 0.8333333333333334,
            "recall": 0.6131078224101479
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8546213591454056,
            "auditor_fn_violation": 0.013432498613618265,
            "auditor_fp_violation": 0.008526280856712533,
            "ave_precision_score": 0.8402322484209988,
            "fpr": 0.03951701427003293,
            "logloss": 3.5924664520470415,
            "mae": 0.23949661953381868,
            "precision": 0.8947368421052632,
            "recall": 0.6361746361746362
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 9296,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.5307775692851273,
            "auditor_fn_violation": 0.0067899002262527385,
            "auditor_fp_violation": 0.023455920553091168,
            "ave_precision_score": 0.49499939869771453,
            "fpr": 0.2532894736842105,
            "logloss": 6.933064938936314,
            "mae": 0.38361885639539123,
            "precision": 0.606473594548552,
            "recall": 0.7526427061310782
        },
        "train": {
            "accuracy": 0.6531284302963776,
            "auc_prc": 0.5453288377499886,
            "auditor_fn_violation": 0.012083771688601554,
            "auditor_fp_violation": 0.006279835601051751,
            "ave_precision_score": 0.5122636810713177,
            "fpr": 0.21953896816684962,
            "logloss": 6.611387438262133,
            "mae": 0.35157373014987303,
            "precision": 0.6460176991150443,
            "recall": 0.7588357588357588
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8338201677289432,
            "auditor_fn_violation": 0.007387986350654649,
            "auditor_fp_violation": 0.013527554649722256,
            "ave_precision_score": 0.8265671621253664,
            "fpr": 0.07236842105263158,
            "logloss": 2.9866512443607127,
            "mae": 0.2563307541760936,
            "precision": 0.8225806451612904,
            "recall": 0.6469344608879493
        },
        "train": {
            "accuracy": 0.7848518111964874,
            "auc_prc": 0.8706925578661513,
            "auditor_fn_violation": 0.013243083495553308,
            "auditor_fp_violation": 0.006366630076838642,
            "ave_precision_score": 0.864492760732325,
            "fpr": 0.04720087815587267,
            "logloss": 2.5895489189139353,
            "mae": 0.2265306641580666,
            "precision": 0.8840970350404312,
            "recall": 0.681912681912682
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5921052631578947,
            "auc_prc": 0.5240679927119,
            "auditor_fn_violation": 0.009638922888616891,
            "auditor_fp_violation": 0.028678615673580307,
            "ave_precision_score": 0.470155786134655,
            "fpr": 0.2905701754385965,
            "logloss": 8.043861505988428,
            "mae": 0.4121169351510058,
            "precision": 0.5800316957210776,
            "recall": 0.773784355179704
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.5376931983078526,
            "auditor_fn_violation": 0.016816867530369182,
            "auditor_fp_violation": 0.014458938554616704,
            "ave_precision_score": 0.4857491052124957,
            "fpr": 0.25905598243688255,
            "logloss": 7.714121934747093,
            "mae": 0.3924406340223255,
            "precision": 0.6073211314475874,
            "recall": 0.7588357588357588
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6513157894736842,
            "auc_prc": 0.7351860530907249,
            "auditor_fn_violation": 0.01133581840436186,
            "auditor_fp_violation": 0.0014336810134676094,
            "ave_precision_score": 0.7106989324693297,
            "fpr": 0.03837719298245614,
            "logloss": 8.40084003059125,
            "mae": 0.3543759031294662,
            "precision": 0.8444444444444444,
            "recall": 0.40169133192389006
        },
        "train": {
            "accuracy": 0.6641053787047201,
            "auc_prc": 0.7710110951072635,
            "auditor_fn_violation": 0.014646581057118926,
            "auditor_fp_violation": 0.005059607382636,
            "ave_precision_score": 0.7475698749007564,
            "fpr": 0.030735455543358946,
            "logloss": 8.072391116454856,
            "mae": 0.34136402246999786,
            "precision": 0.8787878787878788,
            "recall": 0.42203742203742206
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.5400250901606514,
            "auditor_fn_violation": 0.00990087533845184,
            "auditor_fp_violation": 0.02587869160372458,
            "ave_precision_score": 0.4960609643645632,
            "fpr": 0.2532894736842105,
            "logloss": 6.898143115285353,
            "mae": 0.38657091866512894,
            "precision": 0.6071428571428571,
            "recall": 0.7547568710359408
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.559098577734311,
            "auditor_fn_violation": 0.01460093885999485,
            "auditor_fp_violation": 0.004783907283077628,
            "ave_precision_score": 0.5163031844220531,
            "fpr": 0.22063666300768386,
            "logloss": 6.573525617577155,
            "mae": 0.36198965187313903,
            "precision": 0.6410714285714286,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.7735739120920027,
            "auditor_fn_violation": 0.023156132932754727,
            "auditor_fp_violation": 0.007053510770091519,
            "ave_precision_score": 0.77415968168761,
            "fpr": 0.06907894736842106,
            "logloss": 2.909397282519512,
            "mae": 0.3101500529772861,
            "precision": 0.8130563798219584,
            "recall": 0.5792811839323467
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8292479025269232,
            "auditor_fn_violation": 0.01928382828492598,
            "auditor_fp_violation": 0.009879253567508232,
            "ave_precision_score": 0.8288863604294594,
            "fpr": 0.050493962678375415,
            "logloss": 2.648200823030536,
            "mae": 0.2929590273803295,
            "precision": 0.8630952380952381,
            "recall": 0.6029106029106029
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6206140350877193,
            "auc_prc": 0.537014680924991,
            "auditor_fn_violation": 0.00944883350024109,
            "auditor_fp_violation": 0.025571474243695803,
            "ave_precision_score": 0.494834120989571,
            "fpr": 0.25,
            "logloss": 6.847037147859825,
            "mae": 0.38652357962484185,
            "precision": 0.6089193825042881,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6410537870472008,
            "auc_prc": 0.5538772138976151,
            "auditor_fn_violation": 0.015671248382554643,
            "auditor_fp_violation": 0.008906644882955114,
            "ave_precision_score": 0.5139096673328188,
            "fpr": 0.22283205268935236,
            "logloss": 6.52973853999319,
            "mae": 0.3650956424866177,
            "precision": 0.6375,
            "recall": 0.7422037422037422
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6129385964912281,
            "auc_prc": 0.5824767293549509,
            "auditor_fn_violation": 0.01065891472868217,
            "auditor_fp_violation": 0.02365074131798746,
            "ave_precision_score": 0.5083500478804202,
            "fpr": 0.26864035087719296,
            "logloss": 7.5294445796737595,
            "mae": 0.3929900257849532,
            "precision": 0.5983606557377049,
            "recall": 0.7716701902748414
        },
        "train": {
            "accuracy": 0.6388583973655324,
            "auc_prc": 0.5955820954783076,
            "auditor_fn_violation": 0.011401420841596479,
            "auditor_fp_violation": 0.00732647486789371,
            "ave_precision_score": 0.5300488030258731,
            "fpr": 0.2414928649835346,
            "logloss": 6.8202460886520555,
            "mae": 0.3676970233686611,
            "precision": 0.6283783783783784,
            "recall": 0.7733887733887734
        }
    }
]
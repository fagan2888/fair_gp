[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.6696934550723985,
            "auditor_fn_violation": 0.029280884502923977,
            "auditor_fp_violation": 0.044204840805718,
            "ave_precision_score": 0.6691583519381537,
            "fpr": 0.23135964912280702,
            "logloss": 2.4294688882831585,
            "mae": 0.33526585289775146,
            "precision": 0.6512396694214876,
            "recall": 0.8208333333333333
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7092768061324817,
            "auditor_fn_violation": 0.036914041693877464,
            "auditor_fp_violation": 0.03975313169575014,
            "ave_precision_score": 0.7088751431512648,
            "fpr": 0.20636663007683864,
            "logloss": 2.3497703504333147,
            "mae": 0.3137450690457902,
            "precision": 0.6701754385964912,
            "recall": 0.8059071729957806
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.6492391640302908,
            "auditor_fn_violation": 0.028810307017543856,
            "auditor_fp_violation": 0.047748131903833674,
            "ave_precision_score": 0.6465277359243693,
            "fpr": 0.23793859649122806,
            "logloss": 2.5513800490743406,
            "mae": 0.33237447526579456,
            "precision": 0.6494345718901454,
            "recall": 0.8375
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6915338006200837,
            "auditor_fn_violation": 0.03257652600425183,
            "auditor_fp_violation": 0.04180534378948373,
            "ave_precision_score": 0.689826752425356,
            "fpr": 0.21844127332601537,
            "logloss": 2.422380634531562,
            "mae": 0.31727970444142317,
            "precision": 0.656896551724138,
            "recall": 0.8037974683544303
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7263645456224284,
            "auditor_fn_violation": 0.030270010964912285,
            "auditor_fp_violation": 0.03913356887589344,
            "ave_precision_score": 0.7266012810082026,
            "fpr": 0.18969298245614036,
            "logloss": 2.304347809344912,
            "mae": 0.3141247355920602,
            "precision": 0.6808118081180812,
            "recall": 0.76875
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7584406229970455,
            "auditor_fn_violation": 0.03686772545586757,
            "auditor_fp_violation": 0.034885093705963474,
            "ave_precision_score": 0.7579248558606076,
            "fpr": 0.14709110867178923,
            "logloss": 2.290481989674142,
            "mae": 0.2873655970463093,
            "precision": 0.7276422764227642,
            "recall": 0.7552742616033755
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 28699,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.724515321222636,
            "auditor_fn_violation": 0.012758132309941522,
            "auditor_fp_violation": 0.029422514619883048,
            "ave_precision_score": 0.7230705161589585,
            "fpr": 0.20614035087719298,
            "logloss": 1.6445631502105251,
            "mae": 0.3030249698608614,
            "precision": 0.6850921273031826,
            "recall": 0.8520833333333333
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.7739361659914361,
            "auditor_fn_violation": 0.010587892009059458,
            "auditor_fp_violation": 0.031642246933613324,
            "ave_precision_score": 0.7734072629579805,
            "fpr": 0.1668496158068057,
            "logloss": 1.208499887230318,
            "mae": 0.27450855319730255,
            "precision": 0.7226277372262774,
            "recall": 0.8354430379746836
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7230471514787271,
            "auditor_fn_violation": 0.03365999634502924,
            "auditor_fp_violation": 0.03837719298245615,
            "ave_precision_score": 0.7232031687255459,
            "fpr": 0.17543859649122806,
            "logloss": 2.1525779584408054,
            "mae": 0.3070061928717606,
            "precision": 0.6952380952380952,
            "recall": 0.7604166666666666
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.7563934007380104,
            "auditor_fn_violation": 0.0339451708374439,
            "auditor_fp_violation": 0.026889755769177636,
            "ave_precision_score": 0.7552515388799985,
            "fpr": 0.14270032930845225,
            "logloss": 2.1316668530624217,
            "mae": 0.2893305132204873,
            "precision": 0.7308488612836439,
            "recall": 0.7447257383966245
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 28699,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.8084412916502993,
            "auditor_fn_violation": 0.03706140350877193,
            "auditor_fp_violation": 0.037059880604288505,
            "ave_precision_score": 0.809224890336665,
            "fpr": 0.1611842105263158,
            "logloss": 1.6515867901304866,
            "mae": 0.2937983804703461,
            "precision": 0.7167630057803468,
            "recall": 0.775
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8040520429597704,
            "auditor_fn_violation": 0.03991996554071892,
            "auditor_fp_violation": 0.03735930290097889,
            "ave_precision_score": 0.8044460817485064,
            "fpr": 0.13611416026344675,
            "logloss": 1.751278860478512,
            "mae": 0.28778600090811113,
            "precision": 0.7405857740585774,
            "recall": 0.7468354430379747
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7360377847481521,
            "auditor_fn_violation": 0.03688322368421053,
            "auditor_fp_violation": 0.044600795971410014,
            "ave_precision_score": 0.7377307836874192,
            "fpr": 0.22149122807017543,
            "logloss": 1.9350732295244035,
            "mae": 0.3287716841414277,
            "precision": 0.6582064297800339,
            "recall": 0.8104166666666667
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7567219557318424,
            "auditor_fn_violation": 0.0345055973173635,
            "auditor_fp_violation": 0.04224994787833422,
            "ave_precision_score": 0.7575041516602969,
            "fpr": 0.18990120746432493,
            "logloss": 1.946924103665403,
            "mae": 0.3080751235014566,
            "precision": 0.6854545454545454,
            "recall": 0.7953586497890295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.6743934744746463,
            "auditor_fn_violation": 0.050027412280701754,
            "auditor_fp_violation": 0.046479044834307995,
            "ave_precision_score": 0.6746422886129578,
            "fpr": 0.17324561403508773,
            "logloss": 2.3544342616781635,
            "mae": 0.34472014120825134,
            "precision": 0.6827309236947792,
            "recall": 0.7083333333333334
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7142119830441527,
            "auditor_fn_violation": 0.04421811242803615,
            "auditor_fp_violation": 0.044834680123685335,
            "ave_precision_score": 0.7138551466674695,
            "fpr": 0.14928649835345773,
            "logloss": 2.3477734667975447,
            "mae": 0.3183433198380369,
            "precision": 0.7142857142857143,
            "recall": 0.7172995780590717
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.7223926737936485,
            "auditor_fn_violation": 0.0331140350877193,
            "auditor_fp_violation": 0.048268457602339186,
            "ave_precision_score": 0.7232209612694451,
            "fpr": 0.23793859649122806,
            "logloss": 2.0241527659272913,
            "mae": 0.3304885907912918,
            "precision": 0.6477272727272727,
            "recall": 0.83125
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.741212293190199,
            "auditor_fn_violation": 0.036029401547888674,
            "auditor_fp_violation": 0.04605545745239346,
            "ave_precision_score": 0.7422972777650142,
            "fpr": 0.20856201975850713,
            "logloss": 2.0597930154861923,
            "mae": 0.3173491169911459,
            "precision": 0.6678321678321678,
            "recall": 0.8059071729957806
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.6800982072642674,
            "auditor_fn_violation": 0.034882127192982455,
            "auditor_fp_violation": 0.04834714100064977,
            "ave_precision_score": 0.679524895694384,
            "fpr": 0.22478070175438597,
            "logloss": 2.3251408556153623,
            "mae": 0.32809555745675373,
            "precision": 0.6554621848739496,
            "recall": 0.8125
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7121314037598809,
            "auditor_fn_violation": 0.038085842515527525,
            "auditor_fp_violation": 0.04337276159424476,
            "ave_precision_score": 0.711863725640428,
            "fpr": 0.1942919868276619,
            "logloss": 2.268790351681176,
            "mae": 0.30890653031229975,
            "precision": 0.6810810810810811,
            "recall": 0.7974683544303798
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6129385964912281,
            "auc_prc": 0.6267825477879093,
            "auditor_fn_violation": 0.010471491228070177,
            "auditor_fp_violation": 0.0285696881091618,
            "ave_precision_score": 0.6017536441131714,
            "fpr": 0.3574561403508772,
            "logloss": 4.524477673381693,
            "mae": 0.38313561209427627,
            "precision": 0.5815147625160462,
            "recall": 0.94375
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6588640927874099,
            "auditor_fn_violation": 0.010921368922730622,
            "auditor_fp_violation": 0.025794572815851024,
            "ave_precision_score": 0.6346792890644023,
            "fpr": 0.3424807903402854,
            "logloss": 3.777414741894515,
            "mae": 0.36603848057328503,
            "precision": 0.5883905013192612,
            "recall": 0.9409282700421941
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.6672208293426589,
            "auditor_fn_violation": 0.02242324561403509,
            "auditor_fp_violation": 0.04545869883040937,
            "ave_precision_score": 0.6607380066379576,
            "fpr": 0.2565789473684211,
            "logloss": 2.698875998460105,
            "mae": 0.3251123853534586,
            "precision": 0.6432926829268293,
            "recall": 0.8791666666666667
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.6907861494359904,
            "auditor_fn_violation": 0.03225925977388413,
            "auditor_fp_violation": 0.044673919323197045,
            "ave_precision_score": 0.6848910785518976,
            "fpr": 0.2239297475301866,
            "logloss": 2.570679517208092,
            "mae": 0.31755418472450636,
            "precision": 0.6588628762541806,
            "recall": 0.8312236286919831
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.6591585025114284,
            "auditor_fn_violation": 0.036060855263157894,
            "auditor_fp_violation": 0.05056804337231969,
            "ave_precision_score": 0.6585225701255808,
            "fpr": 0.24013157894736842,
            "logloss": 2.5373852223448496,
            "mae": 0.3477424668728949,
            "precision": 0.6386138613861386,
            "recall": 0.80625
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.6729056509695277,
            "auditor_fn_violation": 0.04128166293820951,
            "auditor_fp_violation": 0.05135302820598482,
            "ave_precision_score": 0.6727051196326984,
            "fpr": 0.20856201975850713,
            "logloss": 2.60398660398333,
            "mae": 0.3235027284701722,
            "precision": 0.6654929577464789,
            "recall": 0.7974683544303798
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.6545598571219928,
            "auditor_fn_violation": 0.028645833333333336,
            "auditor_fp_violation": 0.04805525097465887,
            "ave_precision_score": 0.6539782129647219,
            "fpr": 0.23135964912280702,
            "logloss": 2.4752112699839426,
            "mae": 0.34002668418760895,
            "precision": 0.64891846921797,
            "recall": 0.8125
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6951940629903758,
            "auditor_fn_violation": 0.0380951057631295,
            "auditor_fp_violation": 0.04319944135621831,
            "ave_precision_score": 0.6947658622699346,
            "fpr": 0.20636663007683864,
            "logloss": 2.3923243039698447,
            "mae": 0.31840394667781974,
            "precision": 0.6672566371681415,
            "recall": 0.7953586497890295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7551148158479007,
            "auditor_fn_violation": 0.020719115497076022,
            "auditor_fp_violation": 0.035559819688109166,
            "ave_precision_score": 0.7287364563875829,
            "fpr": 0.23464912280701755,
            "logloss": 2.660869414188758,
            "mae": 0.3197379162838676,
            "precision": 0.6565008025682183,
            "recall": 0.8520833333333333
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.7659905401249293,
            "auditor_fn_violation": 0.02053661993358252,
            "auditor_fp_violation": 0.03126044003245359,
            "ave_precision_score": 0.7410378385576395,
            "fpr": 0.20856201975850713,
            "logloss": 2.3409360773136023,
            "mae": 0.2967378180393464,
            "precision": 0.6822742474916388,
            "recall": 0.8607594936708861
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.6786258550679283,
            "auditor_fn_violation": 0.02945677997076024,
            "auditor_fp_violation": 0.04377842755035737,
            "ave_precision_score": 0.678131341572822,
            "fpr": 0.23464912280701755,
            "logloss": 2.3948265578703003,
            "mae": 0.3324690915274099,
            "precision": 0.6486042692939245,
            "recall": 0.8229166666666666
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7139837016672979,
            "auditor_fn_violation": 0.03698814767469328,
            "auditor_fp_violation": 0.04286033654268829,
            "ave_precision_score": 0.7136841208516893,
            "fpr": 0.19978046103183314,
            "logloss": 2.321562168622632,
            "mae": 0.31099189961145024,
            "precision": 0.6784452296819788,
            "recall": 0.810126582278481
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 28699,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7520918131341958,
            "auditor_fn_violation": 0.024067982456140356,
            "auditor_fp_violation": 0.03987979207277454,
            "ave_precision_score": 0.7505093780084744,
            "fpr": 0.19298245614035087,
            "logloss": 1.559083899468092,
            "mae": 0.29436878207890654,
            "precision": 0.697594501718213,
            "recall": 0.8458333333333333
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.7868761872050722,
            "auditor_fn_violation": 0.023260014728563688,
            "auditor_fp_violation": 0.03296852353764189,
            "ave_precision_score": 0.7855973896477347,
            "fpr": 0.16575192096597147,
            "logloss": 1.190753824695451,
            "mae": 0.2742059635701075,
            "precision": 0.7224264705882353,
            "recall": 0.8291139240506329
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.6664457130951393,
            "auditor_fn_violation": 0.031010142543859653,
            "auditor_fp_violation": 0.04505766731643925,
            "ave_precision_score": 0.6652821589024145,
            "fpr": 0.22478070175438597,
            "logloss": 2.3975158971448547,
            "mae": 0.32802074456144054,
            "precision": 0.6560402684563759,
            "recall": 0.8145833333333333
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7039810949285118,
            "auditor_fn_violation": 0.03771068098764746,
            "auditor_fp_violation": 0.03917288568148764,
            "ave_precision_score": 0.7030163197516558,
            "fpr": 0.18880351262349068,
            "logloss": 2.2373327108890004,
            "mae": 0.31043179711316204,
            "precision": 0.6884057971014492,
            "recall": 0.8016877637130801
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.6518577632083593,
            "auditor_fn_violation": 0.039348044590643276,
            "auditor_fp_violation": 0.015513320337881754,
            "ave_precision_score": 0.6504045810072064,
            "fpr": 0.19298245614035087,
            "logloss": 1.72697115927135,
            "mae": 0.35163143314336565,
            "precision": 0.6634799235181644,
            "recall": 0.7229166666666667
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.6934104911107446,
            "auditor_fn_violation": 0.020073457553483685,
            "auditor_fp_violation": 0.021953896816684963,
            "ave_precision_score": 0.6930921102667398,
            "fpr": 0.1668496158068057,
            "logloss": 1.2693332215261401,
            "mae": 0.3221195753519868,
            "precision": 0.7001972386587771,
            "recall": 0.7489451476793249
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.6786067194387355,
            "auditor_fn_violation": 0.02945677997076024,
            "auditor_fp_violation": 0.04377842755035737,
            "ave_precision_score": 0.678152213152926,
            "fpr": 0.23464912280701755,
            "logloss": 2.388155754492066,
            "mae": 0.3323918687574541,
            "precision": 0.6486042692939245,
            "recall": 0.8229166666666666
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7139877269657552,
            "auditor_fn_violation": 0.03698814767469328,
            "auditor_fp_violation": 0.0423654947036852,
            "ave_precision_score": 0.7136737643637413,
            "fpr": 0.1986827661909989,
            "logloss": 2.3108038864712537,
            "mae": 0.31102429525874875,
            "precision": 0.679646017699115,
            "recall": 0.810126582278481
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.6923707016096405,
            "auditor_fn_violation": 0.03411001461988304,
            "auditor_fp_violation": 0.042392584470435354,
            "ave_precision_score": 0.690577734304926,
            "fpr": 0.22916666666666666,
            "logloss": 2.319019929817998,
            "mae": 0.3360966922843547,
            "precision": 0.6499162479061976,
            "recall": 0.8083333333333333
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7234066555337839,
            "auditor_fn_violation": 0.038579110450332785,
            "auditor_fp_violation": 0.036326917135343015,
            "ave_precision_score": 0.722563315027871,
            "fpr": 0.18990120746432493,
            "logloss": 2.1457486641910126,
            "mae": 0.3125512622725653,
            "precision": 0.6808118081180812,
            "recall": 0.7784810126582279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.6704105051887799,
            "auditor_fn_violation": 0.018640350877192985,
            "auditor_fp_violation": 0.03884421702404159,
            "ave_precision_score": 0.6677032681456168,
            "fpr": 0.2642543859649123,
            "logloss": 2.1683718029513233,
            "mae": 0.3416160518768584,
            "precision": 0.6354009077155824,
            "recall": 0.875
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7127548802710095,
            "auditor_fn_violation": 0.015576150842723954,
            "auditor_fp_violation": 0.04297337148053162,
            "ave_precision_score": 0.7126087435157689,
            "fpr": 0.2261251372118551,
            "logloss": 1.6357534664058844,
            "mae": 0.30407417005109977,
            "precision": 0.670926517571885,
            "recall": 0.8860759493670886
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.6764858334564582,
            "auditor_fn_violation": 0.029824561403508774,
            "auditor_fp_violation": 0.044793697205977916,
            "ave_precision_score": 0.6760168563111288,
            "fpr": 0.2324561403508772,
            "logloss": 2.4228155967473404,
            "mae": 0.33397247510503586,
            "precision": 0.6495867768595042,
            "recall": 0.81875
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7161303314247631,
            "auditor_fn_violation": 0.03664772332532062,
            "auditor_fp_violation": 0.03838164111658424,
            "ave_precision_score": 0.7158488540094845,
            "fpr": 0.20087815587266739,
            "logloss": 2.3096226624531364,
            "mae": 0.3124934891390318,
            "precision": 0.675531914893617,
            "recall": 0.8037974683544303
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 28699,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7236155456699802,
            "auditor_fn_violation": 0.01779742324561404,
            "auditor_fp_violation": 0.03749898473034439,
            "ave_precision_score": 0.7213731759242709,
            "fpr": 0.20833333333333334,
            "logloss": 1.4671889673451517,
            "mae": 0.32769580341271387,
            "precision": 0.6838602329450915,
            "recall": 0.85625
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.7564752992468196,
            "auditor_fn_violation": 0.015187094443440928,
            "auditor_fp_violation": 0.03936630101957514,
            "ave_precision_score": 0.7554176698352625,
            "fpr": 0.18331503841931943,
            "logloss": 1.1043754633489682,
            "mae": 0.3055754329437707,
            "precision": 0.7105719237435009,
            "recall": 0.8649789029535865
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6052631578947368,
            "auc_prc": 0.6117970670795133,
            "auditor_fn_violation": 0.07593658625730994,
            "auditor_fp_violation": 0.053235664392462634,
            "ave_precision_score": 0.6001898786449795,
            "fpr": 0.23464912280701755,
            "logloss": 3.8839951408472095,
            "mae": 0.3937714076157515,
            "precision": 0.6094890510948905,
            "recall": 0.6958333333333333
        },
        "train": {
            "accuracy": 0.6377607025246982,
            "auc_prc": 0.6384802743450344,
            "auditor_fn_violation": 0.05870583167752783,
            "auditor_fp_violation": 0.04915010286179344,
            "ave_precision_score": 0.6273939512101279,
            "fpr": 0.21514818880351264,
            "logloss": 3.545613074212657,
            "mae": 0.3643348156588735,
            "precision": 0.6343283582089553,
            "recall": 0.7172995780590717
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.6716718701670729,
            "auditor_fn_violation": 0.032894736842105275,
            "auditor_fp_violation": 0.045098278102664084,
            "ave_precision_score": 0.6722280921784404,
            "fpr": 0.2412280701754386,
            "logloss": 2.3725423215506436,
            "mae": 0.3344278942489614,
            "precision": 0.6451612903225806,
            "recall": 0.8333333333333334
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7119096163686303,
            "auditor_fn_violation": 0.03684919896066362,
            "auditor_fp_violation": 0.043875139095770746,
            "ave_precision_score": 0.712410724910107,
            "fpr": 0.20965971459934138,
            "logloss": 2.265796415363754,
            "mae": 0.31473088508662395,
            "precision": 0.6689774696707106,
            "recall": 0.8143459915611815
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.6825566397367548,
            "auditor_fn_violation": 0.025644188596491233,
            "auditor_fp_violation": 0.04468201754385965,
            "ave_precision_score": 0.6820331891287322,
            "fpr": 0.25,
            "logloss": 2.251919721286046,
            "mae": 0.3299481438014539,
            "precision": 0.6420722135007849,
            "recall": 0.8520833333333333
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.719290403385973,
            "auditor_fn_violation": 0.02489497793031259,
            "auditor_fp_violation": 0.046326741303217484,
            "ave_precision_score": 0.7189730911442329,
            "fpr": 0.22502744237102085,
            "logloss": 2.274416077485306,
            "mae": 0.3121773821671806,
            "precision": 0.6600331674958541,
            "recall": 0.8396624472573839
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.6546177529207972,
            "auditor_fn_violation": 0.028645833333333336,
            "auditor_fp_violation": 0.04805525097465887,
            "ave_precision_score": 0.654116756473207,
            "fpr": 0.23135964912280702,
            "logloss": 2.4898766995761115,
            "mae": 0.34017442016918853,
            "precision": 0.64891846921797,
            "recall": 0.8125
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6946979399772399,
            "auditor_fn_violation": 0.0380951057631295,
            "auditor_fp_violation": 0.04319944135621831,
            "ave_precision_score": 0.6943986666394534,
            "fpr": 0.20636663007683864,
            "logloss": 2.4167758973314526,
            "mae": 0.3183472759205228,
            "precision": 0.6672566371681415,
            "recall": 0.7953586497890295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.6731334695504892,
            "auditor_fn_violation": 0.02763157894736842,
            "auditor_fp_violation": 0.04274285250162443,
            "ave_precision_score": 0.6726109652483689,
            "fpr": 0.23464912280701755,
            "logloss": 2.453959239898638,
            "mae": 0.335122989466717,
            "precision": 0.6491803278688525,
            "recall": 0.825
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7123263356121158,
            "auditor_fn_violation": 0.03828268652706953,
            "auditor_fp_violation": 0.04155164315121311,
            "ave_precision_score": 0.7119419765407592,
            "fpr": 0.2052689352360044,
            "logloss": 2.344701104328413,
            "mae": 0.31341847800185596,
            "precision": 0.6719298245614035,
            "recall": 0.8080168776371308
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.6593128247057141,
            "auditor_fn_violation": 0.021018366228070174,
            "auditor_fp_violation": 0.03798885233918129,
            "ave_precision_score": 0.6553509988406395,
            "fpr": 0.25548245614035087,
            "logloss": 2.061200043788397,
            "mae": 0.32781700040486894,
            "precision": 0.6426380368098159,
            "recall": 0.8729166666666667
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.6929778148641779,
            "auditor_fn_violation": 0.01957092637107644,
            "auditor_fp_violation": 0.03750248048891378,
            "ave_precision_score": 0.6900152612487521,
            "fpr": 0.21734357848518113,
            "logloss": 1.693403617860894,
            "mae": 0.30207861352130594,
            "precision": 0.6748768472906403,
            "recall": 0.8670886075949367
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 28699,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7929071431865504,
            "auditor_fn_violation": 0.02578810307017544,
            "auditor_fp_violation": 0.02697571474983756,
            "ave_precision_score": 0.7762529380730625,
            "fpr": 0.15570175438596492,
            "logloss": 2.6403319928364626,
            "mae": 0.2788274673309667,
            "precision": 0.7242718446601941,
            "recall": 0.7770833333333333
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.815385306945829,
            "auditor_fn_violation": 0.024100654448443084,
            "auditor_fp_violation": 0.024541140949543717,
            "ave_precision_score": 0.8068310996784244,
            "fpr": 0.13721185510428102,
            "logloss": 2.3496061231555254,
            "mae": 0.2682397071653401,
            "precision": 0.7417355371900827,
            "recall": 0.7573839662447257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.7484440590151882,
            "auditor_fn_violation": 0.039884868421052634,
            "auditor_fp_violation": 0.03667153996101365,
            "ave_precision_score": 0.7487810416123222,
            "fpr": 0.21820175438596492,
            "logloss": 1.427034770646797,
            "mae": 0.34535075841304846,
            "precision": 0.6514886164623468,
            "recall": 0.775
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7427739844769055,
            "auditor_fn_violation": 0.03057334871032436,
            "auditor_fp_violation": 0.03347090103916786,
            "ave_precision_score": 0.7420365667546183,
            "fpr": 0.17453347969264543,
            "logloss": 1.353737127309639,
            "mae": 0.32321631016995617,
            "precision": 0.6924564796905223,
            "recall": 0.7552742616033755
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.6986264340469761,
            "auditor_fn_violation": 0.01512244152046784,
            "auditor_fp_violation": 0.04215399610136452,
            "ave_precision_score": 0.6988240512746959,
            "fpr": 0.23026315789473684,
            "logloss": 2.1945569298296537,
            "mae": 0.31878813726052774,
            "precision": 0.6579804560260586,
            "recall": 0.8416666666666667
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7193674582783836,
            "auditor_fn_violation": 0.022924222002992035,
            "auditor_fp_violation": 0.03830628449135534,
            "ave_precision_score": 0.7189585751120143,
            "fpr": 0.1986827661909989,
            "logloss": 2.245310783556538,
            "mae": 0.30664366991882835,
            "precision": 0.6790780141843972,
            "recall": 0.8080168776371308
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 28699,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7961076496110975,
            "auditor_fn_violation": 0.03052357456140351,
            "auditor_fp_violation": 0.017833211500974663,
            "ave_precision_score": 0.796535386021952,
            "fpr": 0.17653508771929824,
            "logloss": 0.9809791493325951,
            "mae": 0.29758058720446673,
            "precision": 0.6985018726591761,
            "recall": 0.7770833333333333
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8002721463804991,
            "auditor_fn_violation": 0.005168892161903041,
            "auditor_fp_violation": 0.015623940297457724,
            "ave_precision_score": 0.8007503563806753,
            "fpr": 0.14709110867178923,
            "logloss": 0.8469101689242975,
            "mae": 0.2770124749317531,
            "precision": 0.7314629258517034,
            "recall": 0.770042194092827
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.6545201825279078,
            "auditor_fn_violation": 0.028645833333333336,
            "auditor_fp_violation": 0.04805525097465887,
            "ave_precision_score": 0.6540567232531503,
            "fpr": 0.23135964912280702,
            "logloss": 2.476026146472215,
            "mae": 0.3400089109130408,
            "precision": 0.64891846921797,
            "recall": 0.8125
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6951373571188206,
            "auditor_fn_violation": 0.0380951057631295,
            "auditor_fp_violation": 0.04319944135621831,
            "ave_precision_score": 0.6947149094313918,
            "fpr": 0.20636663007683864,
            "logloss": 2.3936095443741916,
            "mae": 0.31840799879763154,
            "precision": 0.6672566371681415,
            "recall": 0.7953586497890295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7162016794005588,
            "auditor_fn_violation": 0.02642543859649123,
            "auditor_fp_violation": 0.03173732943469786,
            "ave_precision_score": 0.714748267590956,
            "fpr": 0.22039473684210525,
            "logloss": 1.8598019093625449,
            "mae": 0.32536641341452316,
            "precision": 0.6581632653061225,
            "recall": 0.80625
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7714418798400513,
            "auditor_fn_violation": 0.01646542261251372,
            "auditor_fp_violation": 0.038220880316095926,
            "ave_precision_score": 0.7709503674846059,
            "fpr": 0.1800219538968167,
            "logloss": 1.3034198458756499,
            "mae": 0.2877938934423527,
            "precision": 0.7066189624329159,
            "recall": 0.8333333333333334
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.6501661607155662,
            "auditor_fn_violation": 0.029605263157894742,
            "auditor_fp_violation": 0.04765168128654971,
            "ave_precision_score": 0.6474449868218173,
            "fpr": 0.23574561403508773,
            "logloss": 2.5361782922375387,
            "mae": 0.33229125679530974,
            "precision": 0.6504065040650406,
            "recall": 0.8333333333333334
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.693537200168022,
            "auditor_fn_violation": 0.03369737896409102,
            "auditor_fp_violation": 0.04237051847870045,
            "ave_precision_score": 0.6919948251841246,
            "fpr": 0.21405049396267836,
            "logloss": 2.418190880656875,
            "mae": 0.317042251896013,
            "precision": 0.6602787456445993,
            "recall": 0.79957805907173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7462156788914328,
            "auditor_fn_violation": 0.04713770102339182,
            "auditor_fp_violation": 0.03498111598440546,
            "ave_precision_score": 0.7473333964682881,
            "fpr": 0.1425438596491228,
            "logloss": 1.731166326342946,
            "mae": 0.33210081145058484,
            "precision": 0.7142857142857143,
            "recall": 0.6770833333333334
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7681390798187389,
            "auditor_fn_violation": 0.043741055176534344,
            "auditor_fp_violation": 0.03142371272044953,
            "ave_precision_score": 0.7687356040739279,
            "fpr": 0.12184412733260154,
            "logloss": 1.819249418503654,
            "mae": 0.31593047605059255,
            "precision": 0.7448275862068966,
            "recall": 0.6835443037974683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.6352061899836433,
            "auditor_fn_violation": 0.04236339546783626,
            "auditor_fp_violation": 0.03584155701754386,
            "ave_precision_score": 0.6307187632997062,
            "fpr": 0.22039473684210525,
            "logloss": 3.1023253913814983,
            "mae": 0.346138825057378,
            "precision": 0.6486013986013986,
            "recall": 0.7729166666666667
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.6461191231635788,
            "auditor_fn_violation": 0.03514939302570088,
            "auditor_fp_violation": 0.03075052686840473,
            "ave_precision_score": 0.6409247463495644,
            "fpr": 0.18551042810098792,
            "logloss": 3.134150131839665,
            "mae": 0.3253729952877,
            "precision": 0.6799242424242424,
            "recall": 0.7573839662447257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.6794242803179573,
            "auditor_fn_violation": 0.028172971491228074,
            "auditor_fp_violation": 0.04272000893437298,
            "ave_precision_score": 0.6789752997758306,
            "fpr": 0.23574561403508773,
            "logloss": 2.505907081474937,
            "mae": 0.3312604073738746,
            "precision": 0.6498371335504886,
            "recall": 0.83125
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7150795766991088,
            "auditor_fn_violation": 0.035473606691770077,
            "auditor_fp_violation": 0.04020527144712351,
            "ave_precision_score": 0.7146693771131851,
            "fpr": 0.19978046103183314,
            "logloss": 2.4075262590360533,
            "mae": 0.30989887525786597,
            "precision": 0.6784452296819788,
            "recall": 0.810126582278481
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.6761883793957985,
            "auditor_fn_violation": 0.027658991228070178,
            "auditor_fp_violation": 0.04318957115009747,
            "ave_precision_score": 0.675605502378497,
            "fpr": 0.23355263157894737,
            "logloss": 2.4864782262284795,
            "mae": 0.3327725749811842,
            "precision": 0.6513911620294599,
            "recall": 0.8291666666666667
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.712437727750606,
            "auditor_fn_violation": 0.03650877461129098,
            "auditor_fp_violation": 0.04147628652598423,
            "ave_precision_score": 0.7120669198480227,
            "fpr": 0.2030735455543359,
            "logloss": 2.3846883250743245,
            "mae": 0.3118190745117433,
            "precision": 0.6742957746478874,
            "recall": 0.8080168776371308
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.6957098762973561,
            "auditor_fn_violation": 0.025520833333333336,
            "auditor_fp_violation": 0.03463084795321638,
            "ave_precision_score": 0.6941807243029487,
            "fpr": 0.19956140350877194,
            "logloss": 1.828335637186557,
            "mae": 0.3158421972582003,
            "precision": 0.675,
            "recall": 0.7875
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.738421080544544,
            "auditor_fn_violation": 0.017049007211438264,
            "auditor_fp_violation": 0.023561504821568074,
            "ave_precision_score": 0.7373818496604585,
            "fpr": 0.16355653128430298,
            "logloss": 1.3543796569634972,
            "mae": 0.2855049777220522,
            "precision": 0.7151051625239006,
            "recall": 0.7890295358649789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.6646268562563025,
            "auditor_fn_violation": 0.03367141812865497,
            "auditor_fp_violation": 0.04704759584145549,
            "ave_precision_score": 0.6579464490506163,
            "fpr": 0.21162280701754385,
            "logloss": 3.1446836121639876,
            "mae": 0.3409604066816569,
            "precision": 0.6571936056838366,
            "recall": 0.7708333333333334
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.7154065422484338,
            "auditor_fn_violation": 0.03851658352901944,
            "auditor_fp_violation": 0.04064233987345111,
            "ave_precision_score": 0.7106693747563537,
            "fpr": 0.18551042810098792,
            "logloss": 2.7071531637280857,
            "mae": 0.3086028750038647,
            "precision": 0.6817325800376648,
            "recall": 0.7637130801687764
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 28699,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.6720678864772084,
            "auditor_fn_violation": 0.030553271198830414,
            "auditor_fp_violation": 0.042930677387914236,
            "ave_precision_score": 0.6707447168333895,
            "fpr": 0.22587719298245615,
            "logloss": 2.3789637617499326,
            "mae": 0.32934023351559266,
            "precision": 0.6572379367720466,
            "recall": 0.8229166666666666
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7062270884318782,
            "auditor_fn_violation": 0.03916964248495881,
            "auditor_fp_violation": 0.04065992308600453,
            "ave_precision_score": 0.7057995559422993,
            "fpr": 0.19099890230515917,
            "logloss": 2.3240055513599636,
            "mae": 0.31076935597295463,
            "precision": 0.6876122082585279,
            "recall": 0.8080168776371308
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.6562064762647962,
            "auditor_fn_violation": 0.032389894005847956,
            "auditor_fp_violation": 0.05004264132553607,
            "ave_precision_score": 0.6556364706498882,
            "fpr": 0.2412280701754386,
            "logloss": 2.466504974441527,
            "mae": 0.3490783478285791,
            "precision": 0.6399345335515548,
            "recall": 0.8145833333333333
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.6812240725256496,
            "auditor_fn_violation": 0.04094355440073736,
            "auditor_fp_violation": 0.05232261678392996,
            "ave_precision_score": 0.6809614276610458,
            "fpr": 0.21295279912184412,
            "logloss": 2.5302924919206364,
            "mae": 0.32526627289433646,
            "precision": 0.6614310645724258,
            "recall": 0.79957805907173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.6652632256870574,
            "auditor_fn_violation": 0.014108187134502924,
            "auditor_fp_violation": 0.03514863547758284,
            "ave_precision_score": 0.6642036735267005,
            "fpr": 0.2532894736842105,
            "logloss": 1.9980781310464202,
            "mae": 0.3225088456901793,
            "precision": 0.6473282442748092,
            "recall": 0.8833333333333333
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.712045647759614,
            "auditor_fn_violation": 0.0140407675526963,
            "auditor_fp_violation": 0.035729087908527114,
            "ave_precision_score": 0.7107339451646961,
            "fpr": 0.21953896816684962,
            "logloss": 1.5622953346068724,
            "mae": 0.30018138113822374,
            "precision": 0.6715927750410509,
            "recall": 0.8628691983122363
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.6827219116550213,
            "auditor_fn_violation": 0.028358004385964912,
            "auditor_fp_violation": 0.04440789473684212,
            "ave_precision_score": 0.682310081220609,
            "fpr": 0.23684210526315788,
            "logloss": 2.4486135259966146,
            "mae": 0.3351409506405319,
            "precision": 0.6487804878048781,
            "recall": 0.83125
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7224818709551677,
            "auditor_fn_violation": 0.035066023797283095,
            "auditor_fp_violation": 0.04428960053452966,
            "ave_precision_score": 0.722371841521338,
            "fpr": 0.20856201975850713,
            "logloss": 2.317433854664385,
            "mae": 0.3135678294945956,
            "precision": 0.6695652173913044,
            "recall": 0.8122362869198312
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.677697359438834,
            "auditor_fn_violation": 0.04513432017543861,
            "auditor_fp_violation": 0.04377842755035737,
            "ave_precision_score": 0.6770309077355192,
            "fpr": 0.17543859649122806,
            "logloss": 2.3237423985759302,
            "mae": 0.33253937316123877,
            "precision": 0.6844181459566075,
            "recall": 0.7229166666666667
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7112682207930333,
            "auditor_fn_violation": 0.04215240821279532,
            "auditor_fp_violation": 0.035412590082565754,
            "ave_precision_score": 0.7102417299759893,
            "fpr": 0.145993413830955,
            "logloss": 2.2827923131143595,
            "mae": 0.3176897209249439,
            "precision": 0.7139784946236559,
            "recall": 0.70042194092827
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.6429808431936287,
            "auditor_fn_violation": 0.03172057748538012,
            "auditor_fp_violation": 0.05116451429499675,
            "ave_precision_score": 0.6403825156809916,
            "fpr": 0.24232456140350878,
            "logloss": 2.5564360526259,
            "mae": 0.3399338040885536,
            "precision": 0.6429725363489499,
            "recall": 0.8291666666666667
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6848284301948472,
            "auditor_fn_violation": 0.03655509084930086,
            "auditor_fp_violation": 0.05189308402012524,
            "ave_precision_score": 0.6848401014502361,
            "fpr": 0.21624588364434688,
            "logloss": 2.460078487858468,
            "mae": 0.32405975230421186,
            "precision": 0.6579861111111112,
            "recall": 0.79957805907173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 28699,
        "test": {
            "accuracy": 0.48464912280701755,
            "auc_prc": 0.45549670354846133,
            "auditor_fn_violation": 0.022532894736842102,
            "auditor_fp_violation": 0.042844379467186486,
            "ave_precision_score": 0.44991045115294354,
            "fpr": 0.2916666666666667,
            "logloss": 7.744574122880486,
            "mae": 0.5200200242966209,
            "precision": 0.5092250922509225,
            "recall": 0.575
        },
        "train": {
            "accuracy": 0.5005488474204172,
            "auc_prc": 0.476668828524705,
            "auditor_fn_violation": 0.04337515689625627,
            "auditor_fp_violation": 0.03912767170635031,
            "ave_precision_score": 0.46628310151085384,
            "fpr": 0.27332601536772777,
            "logloss": 7.60655062582808,
            "mae": 0.49942011068135533,
            "precision": 0.5183752417794971,
            "recall": 0.5654008438818565
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.6783484969335196,
            "auditor_fn_violation": 0.03352521929824562,
            "auditor_fp_violation": 0.04824561403508772,
            "ave_precision_score": 0.6798440752389635,
            "fpr": 0.23684210526315788,
            "logloss": 2.323569047277111,
            "mae": 0.33529538335617814,
            "precision": 0.6476345840130505,
            "recall": 0.8270833333333333
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.7140312722159123,
            "auditor_fn_violation": 0.03698814767469328,
            "auditor_fp_violation": 0.04441519490991116,
            "ave_precision_score": 0.7145572355212758,
            "fpr": 0.21075740944017562,
            "logloss": 2.2397164906186897,
            "mae": 0.3149609368978869,
            "precision": 0.6666666666666666,
            "recall": 0.810126582278481
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.7591942408807466,
            "auditor_fn_violation": 0.06500137061403508,
            "auditor_fp_violation": 0.04824561403508773,
            "ave_precision_score": 0.7575583771351775,
            "fpr": 0.15789473684210525,
            "logloss": 1.9934879863383166,
            "mae": 0.34551128822856586,
            "precision": 0.6862745098039216,
            "recall": 0.65625
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.7480462997745727,
            "auditor_fn_violation": 0.06062332393113702,
            "auditor_fp_violation": 0.034719309130459904,
            "ave_precision_score": 0.7465744514514756,
            "fpr": 0.12403951701427003,
            "logloss": 1.9797760030516365,
            "mae": 0.3288825472375524,
            "precision": 0.7250608272506083,
            "recall": 0.6286919831223629
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.6760120985754158,
            "auditor_fn_violation": 0.031144919590643277,
            "auditor_fp_violation": 0.044468810916179345,
            "ave_precision_score": 0.6746186683412654,
            "fpr": 0.2236842105263158,
            "logloss": 2.4033758056472427,
            "mae": 0.3285312276678479,
            "precision": 0.6588628762541806,
            "recall": 0.8208333333333333
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7111084984295749,
            "auditor_fn_violation": 0.039510066834331455,
            "auditor_fp_violation": 0.0401198672718641,
            "ave_precision_score": 0.7100679350286803,
            "fpr": 0.19319429198682767,
            "logloss": 2.3062438385280353,
            "mae": 0.3097526797138146,
            "precision": 0.6840215439856373,
            "recall": 0.8037974683544303
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 28699,
        "test": {
            "accuracy": 0.5997807017543859,
            "auc_prc": 0.5353377103363771,
            "auditor_fn_violation": 0.02109375,
            "auditor_fp_violation": 0.017251969623131913,
            "ave_precision_score": 0.5278725476062036,
            "fpr": 0.3366228070175439,
            "logloss": 3.1047038110501037,
            "mae": 0.4164627631218043,
            "precision": 0.578875171467764,
            "recall": 0.8791666666666667
        },
        "train": {
            "accuracy": 0.6421514818880352,
            "auc_prc": 0.5549210107042607,
            "auditor_fn_violation": 0.013575289360696966,
            "auditor_fp_violation": 0.025156553388913032,
            "ave_precision_score": 0.548979887773039,
            "fpr": 0.3150384193194292,
            "logloss": 2.7563057466827514,
            "mae": 0.38141163955887103,
            "precision": 0.6024930747922438,
            "recall": 0.9177215189873418
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 28699,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.693045196490787,
            "auditor_fn_violation": 0.03271655701754386,
            "auditor_fp_violation": 0.04769736842105264,
            "ave_precision_score": 0.6924099926108529,
            "fpr": 0.24013157894736842,
            "logloss": 2.40084180604566,
            "mae": 0.3312214328055489,
            "precision": 0.6444805194805194,
            "recall": 0.8270833333333333
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.7187993386876941,
            "auditor_fn_violation": 0.03994080784782336,
            "auditor_fp_violation": 0.05417890165206842,
            "ave_precision_score": 0.7183839625398654,
            "fpr": 0.21624588364434688,
            "logloss": 2.4165863239217806,
            "mae": 0.31902765000734745,
            "precision": 0.6591695501730104,
            "recall": 0.8037974683544303
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.6535951463567284,
            "auditor_fn_violation": 0.029057017543859646,
            "auditor_fp_violation": 0.04355760640025991,
            "ave_precision_score": 0.6508030273648785,
            "fpr": 0.22916666666666666,
            "logloss": 2.5273857250425618,
            "mae": 0.3263293780644865,
            "precision": 0.6568144499178982,
            "recall": 0.8333333333333334
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.6909341915285911,
            "auditor_fn_violation": 0.029501127800395547,
            "auditor_fp_violation": 0.036784080661731645,
            "ave_precision_score": 0.6892958255073522,
            "fpr": 0.20417124039517015,
            "logloss": 2.4086837077965235,
            "mae": 0.31201374918445174,
            "precision": 0.6742556917688266,
            "recall": 0.8122362869198312
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7078037481694293,
            "auditor_fn_violation": 0.022535179093567254,
            "auditor_fp_violation": 0.035613121345029246,
            "ave_precision_score": 0.7049556911619846,
            "fpr": 0.22039473684210525,
            "logloss": 1.9901545113363988,
            "mae": 0.31125144581459335,
            "precision": 0.6661129568106312,
            "recall": 0.8354166666666667
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.742957467043469,
            "auditor_fn_violation": 0.017590907196153905,
            "auditor_fp_violation": 0.026696340431090138,
            "ave_precision_score": 0.7412120222208691,
            "fpr": 0.18111964873765093,
            "logloss": 1.5588358399522597,
            "mae": 0.28983652890608447,
            "precision": 0.7010869565217391,
            "recall": 0.8164556962025317
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 28699,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.741923678560581,
            "auditor_fn_violation": 0.02837171052631579,
            "auditor_fp_violation": 0.02302631578947369,
            "ave_precision_score": 0.7403614616102636,
            "fpr": 0.10526315789473684,
            "logloss": 1.6981442227617345,
            "mae": 0.3033007409727657,
            "precision": 0.7746478873239436,
            "recall": 0.6875
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.7937493546266348,
            "auditor_fn_violation": 0.017577012324750937,
            "auditor_fp_violation": 0.021966456254223114,
            "ave_precision_score": 0.7932411586921563,
            "fpr": 0.08122941822173436,
            "logloss": 1.406785543568532,
            "mae": 0.2779592920592449,
            "precision": 0.8136020151133502,
            "recall": 0.6814345991561181
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.6865366841170026,
            "auditor_fn_violation": 0.03742004751461988,
            "auditor_fp_violation": 0.04833698830409356,
            "ave_precision_score": 0.6848981228131557,
            "fpr": 0.2236842105263158,
            "logloss": 2.688343069794339,
            "mae": 0.34020131477992693,
            "precision": 0.6500857632933105,
            "recall": 0.7895833333333333
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7266773018123969,
            "auditor_fn_violation": 0.03771068098764746,
            "auditor_fp_violation": 0.04622124202789702,
            "ave_precision_score": 0.7236098760775722,
            "fpr": 0.1942919868276619,
            "logloss": 2.515723625462083,
            "mae": 0.30800803664294796,
            "precision": 0.6775956284153005,
            "recall": 0.7848101265822784
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.6916068726539137,
            "auditor_fn_violation": 0.021662554824561402,
            "auditor_fp_violation": 0.042029625568551006,
            "ave_precision_score": 0.6910352064447997,
            "fpr": 0.25109649122807015,
            "logloss": 2.247803603540395,
            "mae": 0.3256437771647484,
            "precision": 0.6433021806853583,
            "recall": 0.8604166666666667
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.7252363376010704,
            "auditor_fn_violation": 0.024334551450393,
            "auditor_fp_violation": 0.04213188916547562,
            "ave_precision_score": 0.7248858037783368,
            "fpr": 0.2217343578485181,
            "logloss": 2.225825350185441,
            "mae": 0.31058082358854505,
            "precision": 0.6610738255033557,
            "recall": 0.8312236286919831
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 28699,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7075668499111589,
            "auditor_fn_violation": 0.034128289473684216,
            "auditor_fp_violation": 0.03711318226120857,
            "ave_precision_score": 0.6790515262722264,
            "fpr": 0.18201754385964913,
            "logloss": 3.1005581841638943,
            "mae": 0.32365444021695433,
            "precision": 0.6813819577735125,
            "recall": 0.7395833333333334
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7330809947486159,
            "auditor_fn_violation": 0.020643147281005254,
            "auditor_fp_violation": 0.04419666069674735,
            "ave_precision_score": 0.7090156010195672,
            "fpr": 0.15148188803512624,
            "logloss": 2.5968620678278147,
            "mae": 0.291613839054672,
            "precision": 0.7200811359026369,
            "recall": 0.7489451476793249
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 28699,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.6735406818318646,
            "auditor_fn_violation": 0.031736567982456144,
            "auditor_fp_violation": 0.044468810916179345,
            "ave_precision_score": 0.6721146461317756,
            "fpr": 0.2236842105263158,
            "logloss": 2.4095925453577633,
            "mae": 0.3290990475813148,
            "precision": 0.6582914572864321,
            "recall": 0.81875
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7081714759944875,
            "auditor_fn_violation": 0.03889637668070049,
            "auditor_fp_violation": 0.03798225100287111,
            "ave_precision_score": 0.70778285303984,
            "fpr": 0.1942919868276619,
            "logloss": 2.3109172907953,
            "mae": 0.310365829188087,
            "precision": 0.6833631484794276,
            "recall": 0.8059071729957806
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.677263195494082,
            "auditor_fn_violation": 0.031140350877192986,
            "auditor_fp_violation": 0.044793697205977916,
            "ave_precision_score": 0.6767849403331722,
            "fpr": 0.2324561403508772,
            "logloss": 2.453814900442775,
            "mae": 0.3343655348165027,
            "precision": 0.648424543946932,
            "recall": 0.8145833333333333
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7158415041257262,
            "auditor_fn_violation": 0.035506028058376986,
            "auditor_fp_violation": 0.03838164111658424,
            "ave_precision_score": 0.715531938213089,
            "fpr": 0.20087815587266739,
            "logloss": 2.3598903915818834,
            "mae": 0.31244535024081355,
            "precision": 0.6749555950266429,
            "recall": 0.8016877637130801
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.6962154369147586,
            "auditor_fn_violation": 0.03815789473684212,
            "auditor_fp_violation": 0.05197673001949319,
            "ave_precision_score": 0.6975276350865857,
            "fpr": 0.22916666666666666,
            "logloss": 2.043669553063584,
            "mae": 0.3448701911531849,
            "precision": 0.6475548060708263,
            "recall": 0.8
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7074738394349314,
            "auditor_fn_violation": 0.042346936412436836,
            "auditor_fp_violation": 0.05386993948862995,
            "ave_precision_score": 0.7092009240784904,
            "fpr": 0.20636663007683864,
            "logloss": 2.314525740647929,
            "mae": 0.32472005678572774,
            "precision": 0.6654804270462633,
            "recall": 0.7890295358649789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.7027320835243995,
            "auditor_fn_violation": 0.02291666666666667,
            "auditor_fp_violation": 0.04232659194282003,
            "ave_precision_score": 0.6997952597817279,
            "fpr": 0.26535087719298245,
            "logloss": 2.4163043295298094,
            "mae": 0.3247861958421066,
            "precision": 0.6377245508982036,
            "recall": 0.8875
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7053174525191618,
            "auditor_fn_violation": 0.032764106768191865,
            "auditor_fp_violation": 0.046387026603400605,
            "ave_precision_score": 0.7018842704316822,
            "fpr": 0.2414928649835346,
            "logloss": 2.450124512841561,
            "mae": 0.3233154019681064,
            "precision": 0.6463022508038585,
            "recall": 0.8481012658227848
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7182896682068853,
            "auditor_fn_violation": 0.03008040935672515,
            "auditor_fp_violation": 0.041001665042235215,
            "ave_precision_score": 0.7184243681212814,
            "fpr": 0.18201754385964913,
            "logloss": 2.1975061500098843,
            "mae": 0.3104558943588759,
            "precision": 0.6891385767790262,
            "recall": 0.7666666666666667
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7509816116957142,
            "auditor_fn_violation": 0.038197001486751245,
            "auditor_fp_violation": 0.033980814203216726,
            "ave_precision_score": 0.7504425754080306,
            "fpr": 0.15367727771679474,
            "logloss": 2.131460185765467,
            "mae": 0.2939140709019318,
            "precision": 0.7171717171717171,
            "recall": 0.7489451476793249
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.6666026499465626,
            "auditor_fn_violation": 0.02612390350877193,
            "auditor_fp_violation": 0.044372360298895394,
            "ave_precision_score": 0.6638848267621805,
            "fpr": 0.23574561403508773,
            "logloss": 2.5306378245007277,
            "mae": 0.3286197051860606,
            "precision": 0.6515397082658023,
            "recall": 0.8375
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.698068379111364,
            "auditor_fn_violation": 0.03712709638872293,
            "auditor_fp_violation": 0.04289550296779511,
            "ave_precision_score": 0.6970663834469428,
            "fpr": 0.20087815587266739,
            "logloss": 2.3932944609766125,
            "mae": 0.31167813651874865,
            "precision": 0.6778169014084507,
            "recall": 0.8122362869198312
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.6637734225516033,
            "auditor_fn_violation": 0.029084429824561407,
            "auditor_fp_violation": 0.045275950292397664,
            "ave_precision_score": 0.6634153691630301,
            "fpr": 0.23026315789473684,
            "logloss": 2.4357720833241787,
            "mae": 0.3334732934635748,
            "precision": 0.652317880794702,
            "recall": 0.8208333333333333
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7040149664351004,
            "auditor_fn_violation": 0.036390668204365775,
            "auditor_fp_violation": 0.04021029522213877,
            "ave_precision_score": 0.7036729469794791,
            "fpr": 0.20197585071350166,
            "logloss": 2.283959846544397,
            "mae": 0.3135245896106627,
            "precision": 0.6743362831858407,
            "recall": 0.8037974683544303
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.6950927176906061,
            "auditor_fn_violation": 0.03404605263157895,
            "auditor_fp_violation": 0.03924524853801171,
            "ave_precision_score": 0.694659285477941,
            "fpr": 0.22149122807017543,
            "logloss": 2.320363884503589,
            "mae": 0.33143701486536953,
            "precision": 0.6570458404074703,
            "recall": 0.80625
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7289623036079889,
            "auditor_fn_violation": 0.037960788672900835,
            "auditor_fp_violation": 0.03533974534484447,
            "ave_precision_score": 0.7280597540162823,
            "fpr": 0.18331503841931943,
            "logloss": 2.2240966451856012,
            "mae": 0.30587923067558337,
            "precision": 0.6884328358208955,
            "recall": 0.7784810126582279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 28699,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7151318530636268,
            "auditor_fn_violation": 0.030381944444444448,
            "auditor_fp_violation": 0.03692028102664068,
            "ave_precision_score": 0.7135656535819321,
            "fpr": 0.1962719298245614,
            "logloss": 1.8632123889741203,
            "mae": 0.3141154415395367,
            "precision": 0.6797853309481217,
            "recall": 0.7916666666666666
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.7621290894378854,
            "auditor_fn_violation": 0.01917492253609193,
            "auditor_fp_violation": 0.038989517893430664,
            "ave_precision_score": 0.7615998883281452,
            "fpr": 0.15367727771679474,
            "logloss": 1.476051978796226,
            "mae": 0.2783246644299246,
            "precision": 0.7297297297297297,
            "recall": 0.7974683544303798
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6282894736842105,
            "auc_prc": 0.6999118953281871,
            "auditor_fn_violation": 0.04478709795321639,
            "auditor_fp_violation": 0.016196089181286552,
            "ave_precision_score": 0.70123626996677,
            "fpr": 0.0668859649122807,
            "logloss": 2.5185416231011684,
            "mae": 0.37971054278991606,
            "precision": 0.7680608365019012,
            "recall": 0.42083333333333334
        },
        "train": {
            "accuracy": 0.6092206366630076,
            "auc_prc": 0.7036620903887911,
            "auditor_fn_violation": 0.033222637524489715,
            "auditor_fp_violation": 0.014425769956318278,
            "ave_precision_score": 0.704129691637067,
            "fpr": 0.06147091108671789,
            "logloss": 2.6381261709931327,
            "mae": 0.39219152914422706,
            "precision": 0.7565217391304347,
            "recall": 0.3670886075949367
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.676781252417233,
            "auditor_fn_violation": 0.029824561403508774,
            "auditor_fp_violation": 0.044793697205977916,
            "ave_precision_score": 0.6763917658971658,
            "fpr": 0.2324561403508772,
            "logloss": 2.427294946936968,
            "mae": 0.3340428191384945,
            "precision": 0.6495867768595042,
            "recall": 0.81875
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7160807313925287,
            "auditor_fn_violation": 0.03664772332532062,
            "auditor_fp_violation": 0.03838164111658424,
            "ave_precision_score": 0.715716946853728,
            "fpr": 0.20087815587266739,
            "logloss": 2.31624871134136,
            "mae": 0.3124918214987795,
            "precision": 0.675531914893617,
            "recall": 0.8037974683544303
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.6789813502498994,
            "auditor_fn_violation": 0.027357456140350876,
            "auditor_fp_violation": 0.051291423001949325,
            "ave_precision_score": 0.6760198408235423,
            "fpr": 0.24561403508771928,
            "logloss": 2.585450014618183,
            "mae": 0.3276619575853996,
            "precision": 0.647244094488189,
            "recall": 0.85625
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7114382547946683,
            "auditor_fn_violation": 0.027240895385513216,
            "auditor_fp_violation": 0.047748469632535984,
            "ave_precision_score": 0.7099872258651942,
            "fpr": 0.2239297475301866,
            "logloss": 2.424510367892351,
            "mae": 0.3168618380712273,
            "precision": 0.6582914572864321,
            "recall": 0.8291139240506329
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.6332497223387925,
            "auditor_fn_violation": 0.029970760233918127,
            "auditor_fp_violation": 0.047925804093567254,
            "ave_precision_score": 0.6306686395387028,
            "fpr": 0.24671052631578946,
            "logloss": 2.5435778806411293,
            "mae": 0.344321343224689,
            "precision": 0.64,
            "recall": 0.8333333333333334
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.6732769708813755,
            "auditor_fn_violation": 0.03542729045376018,
            "auditor_fp_violation": 0.049815753051315345,
            "ave_precision_score": 0.6731629784719824,
            "fpr": 0.2217343578485181,
            "logloss": 2.420320056904057,
            "mae": 0.3273423601313192,
            "precision": 0.6576271186440678,
            "recall": 0.8185654008438819
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7553094301659995,
            "auditor_fn_violation": 0.02591374269005848,
            "auditor_fp_violation": 0.05102745289148798,
            "ave_precision_score": 0.721866206490771,
            "fpr": 0.23793859649122806,
            "logloss": 3.519599332208727,
            "mae": 0.3280480225234608,
            "precision": 0.6505636070853462,
            "recall": 0.8416666666666667
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7590143342416605,
            "auditor_fn_violation": 0.03656666990880333,
            "auditor_fp_violation": 0.04549530653819199,
            "ave_precision_score": 0.7302197843604027,
            "fpr": 0.21295279912184412,
            "logloss": 3.3471883331879035,
            "mae": 0.31506846015343964,
            "precision": 0.6649395509499136,
            "recall": 0.8122362869198312
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.6915790427200471,
            "auditor_fn_violation": 0.010546875000000002,
            "auditor_fp_violation": 0.040143762183235863,
            "ave_precision_score": 0.6872820178434329,
            "fpr": 0.24013157894736842,
            "logloss": 2.0789000801314916,
            "mae": 0.31652100872475536,
            "precision": 0.6556603773584906,
            "recall": 0.86875
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.7251466867527213,
            "auditor_fn_violation": 0.015478886742903196,
            "auditor_fp_violation": 0.02651799641804841,
            "ave_precision_score": 0.7228236903247052,
            "fpr": 0.20197585071350166,
            "logloss": 1.6708294463865194,
            "mae": 0.2884324780634048,
            "precision": 0.6860068259385665,
            "recall": 0.8481012658227848
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.6696829268605208,
            "auditor_fn_violation": 0.02605994152046784,
            "auditor_fp_violation": 0.04375812215724497,
            "ave_precision_score": 0.6670881769284189,
            "fpr": 0.23903508771929824,
            "logloss": 2.0129559201676237,
            "mae": 0.33603715810068635,
            "precision": 0.6489533011272142,
            "recall": 0.8395833333333333
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7143010365518672,
            "auditor_fn_violation": 0.021439786574775247,
            "auditor_fp_violation": 0.03965767997046021,
            "ave_precision_score": 0.7139384722577713,
            "fpr": 0.19978046103183314,
            "logloss": 1.5319482909512347,
            "mae": 0.30370469150132534,
            "precision": 0.6851211072664359,
            "recall": 0.8354430379746836
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 28699,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.752602760633507,
            "auditor_fn_violation": 0.030619517543859654,
            "auditor_fp_violation": 0.03821474983755685,
            "ave_precision_score": 0.7536333178905286,
            "fpr": 0.20175438596491227,
            "logloss": 1.90279478619685,
            "mae": 0.30772520789743407,
            "precision": 0.6827586206896552,
            "recall": 0.825
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7608715380813216,
            "auditor_fn_violation": 0.03485991653813911,
            "auditor_fp_violation": 0.03243097961100912,
            "ave_precision_score": 0.7614712047083027,
            "fpr": 0.18331503841931943,
            "logloss": 1.9681820599956326,
            "mae": 0.3035769803732747,
            "precision": 0.6895910780669146,
            "recall": 0.7827004219409283
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6055841433474953,
            "auditor_fn_violation": 0.03212719298245614,
            "auditor_fp_violation": 0.04387995451591944,
            "ave_precision_score": 0.6021280010548209,
            "fpr": 0.2719298245614035,
            "logloss": 3.230468283373824,
            "mae": 0.3616077006939046,
            "precision": 0.6184615384615385,
            "recall": 0.8375
        },
        "train": {
            "accuracy": 0.6695938529088913,
            "auc_prc": 0.6281864203605008,
            "auditor_fn_violation": 0.033139268296071916,
            "auditor_fp_violation": 0.04199373535255599,
            "ave_precision_score": 0.6247039371224694,
            "fpr": 0.24039517014270034,
            "logloss": 3.182835594151023,
            "mae": 0.34095238490766544,
            "precision": 0.6415711947626841,
            "recall": 0.8270042194092827
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.6502637494254249,
            "auditor_fn_violation": 0.029255756578947373,
            "auditor_fp_violation": 0.04714912280701755,
            "ave_precision_score": 0.647556253506813,
            "fpr": 0.23684210526315788,
            "logloss": 2.5349018224096374,
            "mae": 0.3322163808616862,
            "precision": 0.6487804878048781,
            "recall": 0.83125
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.6935825443953434,
            "auditor_fn_violation": 0.03369737896409102,
            "auditor_fp_violation": 0.04237051847870045,
            "ave_precision_score": 0.6920470792070132,
            "fpr": 0.21405049396267836,
            "logloss": 2.416747367982972,
            "mae": 0.316978679345165,
            "precision": 0.6602787456445993,
            "recall": 0.79957805907173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.6641648006160678,
            "auditor_fn_violation": 0.0314624451754386,
            "auditor_fp_violation": 0.04904260071474984,
            "ave_precision_score": 0.6657625262901874,
            "fpr": 0.25219298245614036,
            "logloss": 2.2524500195818984,
            "mae": 0.34485064746833405,
            "precision": 0.6343402225755167,
            "recall": 0.83125
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.6942878418276104,
            "auditor_fn_violation": 0.03567508232711307,
            "auditor_fp_violation": 0.056165804670603635,
            "ave_precision_score": 0.695039988059073,
            "fpr": 0.2239297475301866,
            "logloss": 2.2767499182023165,
            "mae": 0.323590330430743,
            "precision": 0.6559865092748736,
            "recall": 0.820675105485232
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.6793690286482437,
            "auditor_fn_violation": 0.028172971491228074,
            "auditor_fp_violation": 0.04272000893437298,
            "ave_precision_score": 0.6788677931427429,
            "fpr": 0.23574561403508773,
            "logloss": 2.4975220344847693,
            "mae": 0.33123592101354216,
            "precision": 0.6498371335504886,
            "recall": 0.83125
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7153315337792824,
            "auditor_fn_violation": 0.03650877461129098,
            "auditor_fp_violation": 0.04038361546016523,
            "ave_precision_score": 0.7149696430986128,
            "fpr": 0.20197585071350166,
            "logloss": 2.3910252882307317,
            "mae": 0.310012496235146,
            "precision": 0.6754850088183422,
            "recall": 0.8080168776371308
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.712530789097322,
            "auditor_fn_violation": 0.04401041666666667,
            "auditor_fp_violation": 0.037544671864847314,
            "ave_precision_score": 0.7138862087536796,
            "fpr": 0.1699561403508772,
            "logloss": 2.0770884434183454,
            "mae": 0.32424047653927957,
            "precision": 0.6924603174603174,
            "recall": 0.7270833333333333
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7503599078427892,
            "auditor_fn_violation": 0.04296294237796829,
            "auditor_fp_violation": 0.029607618052433147,
            "ave_precision_score": 0.750701108034347,
            "fpr": 0.13830954994511527,
            "logloss": 2.0497715476460496,
            "mae": 0.3037971402905196,
            "precision": 0.728448275862069,
            "recall": 0.7130801687763713
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 28699,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.6337428795312041,
            "auditor_fn_violation": 0.011915204678362575,
            "auditor_fp_violation": 0.03444809941520469,
            "ave_precision_score": 0.6258498675172706,
            "fpr": 0.2741228070175439,
            "logloss": 2.634070562989116,
            "mae": 0.34279410181308667,
            "precision": 0.6312684365781711,
            "recall": 0.8916666666666667
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.664653642580264,
            "auditor_fn_violation": 0.018239334528292277,
            "auditor_fp_violation": 0.028849028025128932,
            "ave_precision_score": 0.6592365930111957,
            "fpr": 0.24478594950603733,
            "logloss": 2.1094026201320113,
            "mae": 0.3192496496940903,
            "precision": 0.6504702194357367,
            "recall": 0.8755274261603375
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6724949541729344,
            "auditor_fn_violation": 0.021198830409356727,
            "auditor_fp_violation": 0.03978334145549059,
            "ave_precision_score": 0.6696967999150978,
            "fpr": 0.26535087719298245,
            "logloss": 2.166513364066582,
            "mae": 0.3424752637601464,
            "precision": 0.6322188449848024,
            "recall": 0.8666666666666667
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7140519170399349,
            "auditor_fn_violation": 0.01691005849740861,
            "auditor_fp_violation": 0.029798521503013004,
            "ave_precision_score": 0.7138154203906426,
            "fpr": 0.21844127332601537,
            "logloss": 1.6182695725647098,
            "mae": 0.3033810232310519,
            "precision": 0.6764227642276422,
            "recall": 0.8776371308016878
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.6696617635712891,
            "auditor_fn_violation": 0.027035361842105265,
            "auditor_fp_violation": 0.04440789473684211,
            "ave_precision_score": 0.6684474150140601,
            "fpr": 0.23684210526315788,
            "logloss": 2.4371540238823695,
            "mae": 0.33574080102757414,
            "precision": 0.646481178396072,
            "recall": 0.8229166666666666
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7077869101059966,
            "auditor_fn_violation": 0.03719657074573775,
            "auditor_fp_violation": 0.04192340250234234,
            "ave_precision_score": 0.7073587859949564,
            "fpr": 0.20636663007683864,
            "logloss": 2.3540524344955314,
            "mae": 0.3138309597814946,
            "precision": 0.6713286713286714,
            "recall": 0.810126582278481
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.6801995910050813,
            "auditor_fn_violation": 0.02837171052631579,
            "auditor_fp_violation": 0.04440789473684212,
            "ave_precision_score": 0.6796820431789803,
            "fpr": 0.23684210526315788,
            "logloss": 2.405763441445913,
            "mae": 0.3310461739485208,
            "precision": 0.6487804878048781,
            "recall": 0.83125
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7149319617386036,
            "auditor_fn_violation": 0.03698814767469328,
            "auditor_fp_violation": 0.03967526318301362,
            "ave_precision_score": 0.7146056291170503,
            "fpr": 0.19758507135016465,
            "logloss": 2.3367581046799923,
            "mae": 0.3099243243011037,
            "precision": 0.6808510638297872,
            "recall": 0.810126582278481
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 28699,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.77324245379069,
            "auditor_fn_violation": 0.020796783625731,
            "auditor_fp_violation": 0.02817373294346979,
            "ave_precision_score": 0.7318567271843307,
            "fpr": 0.17105263157894737,
            "logloss": 3.9053756617393307,
            "mae": 0.2986449750848732,
            "precision": 0.7022900763358778,
            "recall": 0.7666666666666667
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7703909280514593,
            "auditor_fn_violation": 0.02385749419889119,
            "auditor_fp_violation": 0.020062445523439682,
            "ave_precision_score": 0.7334930759035792,
            "fpr": 0.1394072447859495,
            "logloss": 3.904978038159217,
            "mae": 0.2874495415968537,
            "precision": 0.7320675105485233,
            "recall": 0.7320675105485233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7703980119193773,
            "auditor_fn_violation": 0.0324561403508772,
            "auditor_fp_violation": 0.03966150909681612,
            "ave_precision_score": 0.7363118414653208,
            "fpr": 0.19517543859649122,
            "logloss": 3.289262182672002,
            "mae": 0.30801772261569693,
            "precision": 0.6832740213523132,
            "recall": 0.8
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7663058481708601,
            "auditor_fn_violation": 0.034028540065861694,
            "auditor_fp_violation": 0.034983057318761036,
            "ave_precision_score": 0.7312760457681811,
            "fpr": 0.1668496158068057,
            "logloss": 3.460234053281296,
            "mae": 0.29555634644955414,
            "precision": 0.7065637065637066,
            "recall": 0.7721518987341772
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.719910687293675,
            "auditor_fn_violation": 0.03730811403508772,
            "auditor_fp_violation": 0.0451591942820013,
            "ave_precision_score": 0.720135669355437,
            "fpr": 0.1962719298245614,
            "logloss": 2.2557291721170545,
            "mae": 0.32058253466310915,
            "precision": 0.6751361161524501,
            "recall": 0.775
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7476930423367059,
            "auditor_fn_violation": 0.04324778724172908,
            "auditor_fp_violation": 0.045362176500287614,
            "ave_precision_score": 0.7472799254081713,
            "fpr": 0.1712403951701427,
            "logloss": 2.1737099037900234,
            "mae": 0.2995170443251086,
            "precision": 0.6994219653179191,
            "recall": 0.7658227848101266
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 28699,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.6828361359022537,
            "auditor_fn_violation": 0.03693804824561404,
            "auditor_fp_violation": 0.03915895061728396,
            "ave_precision_score": 0.6838444629882593,
            "fpr": 0.2324561403508772,
            "logloss": 2.3468097297440456,
            "mae": 0.34120010409465207,
            "precision": 0.6478405315614618,
            "recall": 0.8125
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7126823086020673,
            "auditor_fn_violation": 0.03563802933670516,
            "auditor_fp_violation": 0.039552180695139744,
            "ave_precision_score": 0.7118013160557434,
            "fpr": 0.1942919868276619,
            "logloss": 2.234758842904494,
            "mae": 0.31904015897891663,
            "precision": 0.6781818181818182,
            "recall": 0.7869198312236287
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 28699,
        "test": {
            "accuracy": 0.5855263157894737,
            "auc_prc": 0.6652415213279905,
            "auditor_fn_violation": 0.0156889619883041,
            "auditor_fp_violation": 0.017378878330084477,
            "ave_precision_score": 0.5670467295937203,
            "fpr": 0.3081140350877193,
            "logloss": 8.872878161337061,
            "mae": 0.41857278430843475,
            "precision": 0.5768072289156626,
            "recall": 0.7979166666666667
        },
        "train": {
            "accuracy": 0.6048298572996706,
            "auc_prc": 0.6680998694582886,
            "auditor_fn_violation": 0.014047714988397788,
            "auditor_fp_violation": 0.02625927200476255,
            "ave_precision_score": 0.5766935084280567,
            "fpr": 0.2864983534577388,
            "logloss": 8.107557437246227,
            "mae": 0.39845168286185456,
            "precision": 0.589622641509434,
            "recall": 0.7911392405063291
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 28699,
        "test": {
            "accuracy": 0.5942982456140351,
            "auc_prc": 0.5941919884114708,
            "auditor_fn_violation": 0.06704358552631579,
            "auditor_fp_violation": 0.051872664879792064,
            "ave_precision_score": 0.5920298195001932,
            "fpr": 0.21162280701754385,
            "logloss": 2.6615880863139076,
            "mae": 0.4135379857818785,
            "precision": 0.6108870967741935,
            "recall": 0.63125
        },
        "train": {
            "accuracy": 0.6377607025246982,
            "auc_prc": 0.6254052642257515,
            "auditor_fn_violation": 0.055510011254845844,
            "auditor_fp_violation": 0.05006191802706309,
            "ave_precision_score": 0.6249991722005914,
            "fpr": 0.19099890230515917,
            "logloss": 2.3648410111417175,
            "mae": 0.3811462053797461,
            "precision": 0.6463414634146342,
            "recall": 0.6708860759493671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.6701784215615744,
            "auditor_fn_violation": 0.027035361842105265,
            "auditor_fp_violation": 0.04440789473684211,
            "ave_precision_score": 0.6689871433565403,
            "fpr": 0.23684210526315788,
            "logloss": 2.43649346296362,
            "mae": 0.33559234618156364,
            "precision": 0.646481178396072,
            "recall": 0.8229166666666666
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.707985765821612,
            "auditor_fn_violation": 0.03719657074573775,
            "auditor_fp_violation": 0.04192340250234234,
            "ave_precision_score": 0.7075692911767961,
            "fpr": 0.20636663007683864,
            "logloss": 2.3529473205167504,
            "mae": 0.3136993871000142,
            "precision": 0.6713286713286714,
            "recall": 0.810126582278481
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.6763208713352484,
            "auditor_fn_violation": 0.029166666666666667,
            "auditor_fp_violation": 0.044793697205977916,
            "ave_precision_score": 0.6758965727171511,
            "fpr": 0.2324561403508772,
            "logloss": 2.4187537138382327,
            "mae": 0.33395889634783316,
            "precision": 0.6501650165016502,
            "recall": 0.8208333333333333
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7161152185380273,
            "auditor_fn_violation": 0.03664772332532062,
            "auditor_fp_violation": 0.04147628652598423,
            "ave_precision_score": 0.7158093378569345,
            "fpr": 0.2030735455543359,
            "logloss": 2.300777120400918,
            "mae": 0.31260826096323585,
            "precision": 0.6731448763250883,
            "recall": 0.8037974683544303
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 28699,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8323084301360882,
            "auditor_fn_violation": 0.02258771929824562,
            "auditor_fp_violation": 0.02375730994152047,
            "ave_precision_score": 0.814254480825408,
            "fpr": 0.12280701754385964,
            "logloss": 2.2591510184991743,
            "mae": 0.2478935325454895,
            "precision": 0.768595041322314,
            "recall": 0.775
        },
        "train": {
            "accuracy": 0.7936333699231614,
            "auc_prc": 0.8602187776915946,
            "auditor_fn_violation": 0.013302023556438655,
            "auditor_fp_violation": 0.018512610931232062,
            "ave_precision_score": 0.8487888786965776,
            "fpr": 0.09659714599341383,
            "logloss": 1.9164525419830951,
            "mae": 0.21965151155226093,
            "precision": 0.8095238095238095,
            "recall": 0.7890295358649789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7234833966671954,
            "auditor_fn_violation": 0.0350077668128655,
            "auditor_fp_violation": 0.04332155620532815,
            "ave_precision_score": 0.7235608570278398,
            "fpr": 0.17982456140350878,
            "logloss": 2.4259298446580617,
            "mae": 0.32448844740324984,
            "precision": 0.6864244741873805,
            "recall": 0.7479166666666667
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7519519025865076,
            "auditor_fn_violation": 0.03369506315219053,
            "auditor_fp_violation": 0.03719854210049057,
            "ave_precision_score": 0.7513624363333267,
            "fpr": 0.1525795828759605,
            "logloss": 2.422106088043511,
            "mae": 0.3031140743184077,
            "precision": 0.718052738336714,
            "recall": 0.7468354430379747
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7056946191758745,
            "auditor_fn_violation": 0.011225328947368421,
            "auditor_fp_violation": 0.01964546783625733,
            "ave_precision_score": 0.7040396285826773,
            "fpr": 0.24342105263157895,
            "logloss": 1.7945148361180983,
            "mae": 0.3195447261719318,
            "precision": 0.6547433903576982,
            "recall": 0.8770833333333333
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7169459851254698,
            "auditor_fn_violation": 0.005530158818380138,
            "auditor_fp_violation": 0.027138432632432985,
            "ave_precision_score": 0.7159336856316738,
            "fpr": 0.22502744237102085,
            "logloss": 1.634752512641283,
            "mae": 0.30513947592619156,
            "precision": 0.6655791190864601,
            "recall": 0.8607594936708861
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.760498297015833,
            "auditor_fn_violation": 0.01138980263157895,
            "auditor_fp_violation": 0.038925438596491224,
            "ave_precision_score": 0.7015447076036128,
            "fpr": 0.2236842105263158,
            "logloss": 5.039557941747921,
            "mae": 0.3070618960461223,
            "precision": 0.66721044045677,
            "recall": 0.8520833333333333
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7563987495946274,
            "auditor_fn_violation": 0.022356848087370953,
            "auditor_fp_violation": 0.02959254672738736,
            "ave_precision_score": 0.7033671593690459,
            "fpr": 0.1964873765093304,
            "logloss": 5.025775122729125,
            "mae": 0.29404666193904955,
            "precision": 0.6820603907637656,
            "recall": 0.810126582278481
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 28699,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.6644195799466982,
            "auditor_fn_violation": 0.02992050438596491,
            "auditor_fp_violation": 0.04879385964912281,
            "ave_precision_score": 0.6631680394854438,
            "fpr": 0.23684210526315788,
            "logloss": 2.585703510819522,
            "mae": 0.33838685813942876,
            "precision": 0.645320197044335,
            "recall": 0.81875
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.7025112195357001,
            "auditor_fn_violation": 0.03753004765940891,
            "auditor_fp_violation": 0.040451436422871245,
            "ave_precision_score": 0.7022065906598622,
            "fpr": 0.2074643249176729,
            "logloss": 2.5338289973216046,
            "mae": 0.31647985711952975,
            "precision": 0.6684210526315789,
            "recall": 0.8037974683544303
        }
    }
]
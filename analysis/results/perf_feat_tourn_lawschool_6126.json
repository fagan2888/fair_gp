[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8022717177076043,
            "auditor_fn_violation": 0.013944194367497694,
            "auditor_fp_violation": 0.03052862419205909,
            "ave_precision_score": 0.7525404708160485,
            "fpr": 0.21052631578947367,
            "logloss": 2.9045264825071313,
            "mae": 0.2887835083386852,
            "precision": 0.6847290640394089,
            "recall": 0.9144736842105263
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.78340471682274,
            "auditor_fn_violation": 0.024389545007692682,
            "auditor_fp_violation": 0.029576629997102945,
            "ave_precision_score": 0.7326301784467636,
            "fpr": 0.22502744237102085,
            "logloss": 3.4408696984335494,
            "mae": 0.3201227875570666,
            "precision": 0.683641975308642,
            "recall": 0.8895582329317269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8048844424807723,
            "auditor_fn_violation": 0.019392986303478,
            "auditor_fp_violation": 0.03798284087411511,
            "ave_precision_score": 0.7543698129584373,
            "fpr": 0.18859649122807018,
            "logloss": 2.83869531563953,
            "mae": 0.29499571362898785,
            "precision": 0.6977152899824253,
            "recall": 0.8706140350877193
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7834054371440172,
            "auditor_fn_violation": 0.02678110906854642,
            "auditor_fp_violation": 0.03974027423766024,
            "ave_precision_score": 0.730286563643664,
            "fpr": 0.20197585071350166,
            "logloss": 3.374098099096195,
            "mae": 0.3208133425895422,
            "precision": 0.6968698517298187,
            "recall": 0.8493975903614458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 6126,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8028940509960543,
            "auditor_fn_violation": 0.01389369806094183,
            "auditor_fp_violation": 0.02394967682363805,
            "ave_precision_score": 0.7531633065965703,
            "fpr": 0.19407894736842105,
            "logloss": 2.871371985975917,
            "mae": 0.27658904765337,
            "precision": 0.6958762886597938,
            "recall": 0.8881578947368421
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7839501657528687,
            "auditor_fn_violation": 0.0287075855562756,
            "auditor_fp_violation": 0.027620447423606555,
            "ave_precision_score": 0.7331778460106602,
            "fpr": 0.2052689352360044,
            "logloss": 3.4238137570815943,
            "mae": 0.31236785891926366,
            "precision": 0.6969205834683955,
            "recall": 0.8634538152610441
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8016415799618919,
            "auditor_fn_violation": 0.017308210218528783,
            "auditor_fp_violation": 0.022292917051400435,
            "ave_precision_score": 0.751912734156686,
            "fpr": 0.1962719298245614,
            "logloss": 2.8814276968862647,
            "mae": 0.2795258541475333,
            "precision": 0.694017094017094,
            "recall": 0.8903508771929824
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7835304278706652,
            "auditor_fn_violation": 0.022469681139486598,
            "auditor_fp_violation": 0.022296760338398326,
            "ave_precision_score": 0.7327594858540288,
            "fpr": 0.21075740944017562,
            "logloss": 3.4283788966014095,
            "mae": 0.31364595136653933,
            "precision": 0.6923076923076923,
            "recall": 0.8674698795180723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8037846910174639,
            "auditor_fn_violation": 0.018378251000307787,
            "auditor_fp_violation": 0.03752596952908587,
            "ave_precision_score": 0.753964175479724,
            "fpr": 0.19956140350877194,
            "logloss": 2.9332120121030445,
            "mae": 0.2863453525620082,
            "precision": 0.6941176470588235,
            "recall": 0.9057017543859649
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7846004012849029,
            "auditor_fn_violation": 0.027270442913255658,
            "auditor_fp_violation": 0.04349051012244746,
            "ave_precision_score": 0.7330724931986674,
            "fpr": 0.21953896816684962,
            "logloss": 3.471894367484335,
            "mae": 0.31671352012886056,
            "precision": 0.6865203761755486,
            "recall": 0.8795180722891566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8233746476690953,
            "auditor_fn_violation": 0.024346433518005545,
            "auditor_fp_violation": 0.033370844875346264,
            "ave_precision_score": 0.8238241762656213,
            "fpr": 0.19517543859649122,
            "logloss": 0.7922217772073759,
            "mae": 0.30632124496291163,
            "precision": 0.6904347826086956,
            "recall": 0.8706140350877193
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.8241188005161144,
            "auditor_fn_violation": 0.03945088807480195,
            "auditor_fp_violation": 0.028861666529344077,
            "ave_precision_score": 0.8244198761296767,
            "fpr": 0.21295279912184412,
            "logloss": 0.9588953692167901,
            "mae": 0.3289789381141211,
            "precision": 0.6830065359477124,
            "recall": 0.8393574297188755
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8033616243753039,
            "auditor_fn_violation": 0.018414319790704833,
            "auditor_fp_violation": 0.03272160664819946,
            "ave_precision_score": 0.7535083282203272,
            "fpr": 0.23135964912280702,
            "logloss": 2.973084101987643,
            "mae": 0.29844467534687474,
            "precision": 0.6661392405063291,
            "recall": 0.9232456140350878
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7827673032315834,
            "auditor_fn_violation": 0.025546753424234812,
            "auditor_fp_violation": 0.03642858471785522,
            "ave_precision_score": 0.7312918433590228,
            "fpr": 0.24588364434687157,
            "logloss": 3.540333117049824,
            "mae": 0.3262456761857479,
            "precision": 0.6671619613670133,
            "recall": 0.9016064257028112
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8054593213625663,
            "auditor_fn_violation": 0.027551746691289628,
            "auditor_fp_violation": 0.032406605878731926,
            "ave_precision_score": 0.8064865822020634,
            "fpr": 0.17653508771929824,
            "logloss": 0.8214831137083802,
            "mae": 0.29755773388875456,
            "precision": 0.7056672760511883,
            "recall": 0.8464912280701754
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.8157296033885211,
            "auditor_fn_violation": 0.04167272823456284,
            "auditor_fp_violation": 0.03297337093314693,
            "ave_precision_score": 0.8161205579348383,
            "fpr": 0.19099890230515917,
            "logloss": 0.993727395622657,
            "mae": 0.3244831223659266,
            "precision": 0.702054794520548,
            "recall": 0.8232931726907631
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 6126,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7993423251356048,
            "auditor_fn_violation": 0.01951081101877501,
            "auditor_fp_violation": 0.03303179824561405,
            "ave_precision_score": 0.7462174974098397,
            "fpr": 0.2708333333333333,
            "logloss": 3.0752451682519766,
            "mae": 0.3346763912938394,
            "precision": 0.6302395209580839,
            "recall": 0.9232456140350878
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.7787508749375698,
            "auditor_fn_violation": 0.022053086109531432,
            "auditor_fp_violation": 0.03518204989860279,
            "ave_precision_score": 0.7234253368660185,
            "fpr": 0.27771679473106475,
            "logloss": 3.553379569386114,
            "mae": 0.34515414246689,
            "precision": 0.6416430594900849,
            "recall": 0.9096385542168675
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 6126,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.7882931479952211,
            "auditor_fn_violation": 0.02770804478301016,
            "auditor_fp_violation": 0.051316270390889526,
            "ave_precision_score": 0.7888952757120471,
            "fpr": 0.26864035087719296,
            "logloss": 0.8235819492277623,
            "mae": 0.346720097075687,
            "precision": 0.6207430340557275,
            "recall": 0.8793859649122807
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.8118286415367815,
            "auditor_fn_violation": 0.04089905175036039,
            "auditor_fp_violation": 0.05222422742748703,
            "ave_precision_score": 0.8123639838043542,
            "fpr": 0.2414928649835346,
            "logloss": 0.8890670571456599,
            "mae": 0.3465513860786499,
            "precision": 0.6578538102643857,
            "recall": 0.8493975903614458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8121384269330484,
            "auditor_fn_violation": 0.009926131117266852,
            "auditor_fp_violation": 0.02173745767928594,
            "ave_precision_score": 0.7624080458821748,
            "fpr": 0.19517543859649122,
            "logloss": 2.7979840159682663,
            "mae": 0.2767236114642193,
            "precision": 0.6998313659359191,
            "recall": 0.9100877192982456
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.7900610482277919,
            "auditor_fn_violation": 0.012268613421854268,
            "auditor_fp_violation": 0.03343849586570382,
            "ave_precision_score": 0.739283569973725,
            "fpr": 0.2052689352360044,
            "logloss": 3.348997437746593,
            "mae": 0.30565820989944276,
            "precision": 0.7045813586097947,
            "recall": 0.8955823293172691
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8057410245389606,
            "auditor_fn_violation": 0.014985380116959067,
            "auditor_fp_violation": 0.03182710064635272,
            "ave_precision_score": 0.7560062963122561,
            "fpr": 0.23026315789473684,
            "logloss": 2.899082407330671,
            "mae": 0.29341391012672896,
            "precision": 0.6677215189873418,
            "recall": 0.9254385964912281
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7850189750228411,
            "auditor_fn_violation": 0.025584224934865698,
            "auditor_fp_violation": 0.03510762990939368,
            "ave_precision_score": 0.7342460120012426,
            "fpr": 0.23929747530186607,
            "logloss": 3.45176132034353,
            "mae": 0.32136223523536694,
            "precision": 0.6721804511278195,
            "recall": 0.8975903614457831
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8020896119492535,
            "auditor_fn_violation": 0.015581717451523546,
            "auditor_fp_violation": 0.02866266543551863,
            "ave_precision_score": 0.752358451598889,
            "fpr": 0.20175438596491227,
            "logloss": 2.899237029408895,
            "mae": 0.282043502771222,
            "precision": 0.6907563025210084,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7833716584187853,
            "auditor_fn_violation": 0.024810548450663248,
            "auditor_fp_violation": 0.02815467663185761,
            "ave_precision_score": 0.7326015276404859,
            "fpr": 0.21844127332601537,
            "logloss": 3.4434839925491096,
            "mae": 0.3164786043777848,
            "precision": 0.6875981161695447,
            "recall": 0.8795180722891566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8083405677300421,
            "auditor_fn_violation": 0.013732590797168362,
            "auditor_fp_violation": 0.032009849184364426,
            "ave_precision_score": 0.7585560964822542,
            "fpr": 0.24561403508771928,
            "logloss": 2.926913763665388,
            "mae": 0.3011000695482605,
            "precision": 0.6559139784946236,
            "recall": 0.9364035087719298
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7857501048721983,
            "auditor_fn_violation": 0.02254903257376377,
            "auditor_fp_violation": 0.038570817264374356,
            "ave_precision_score": 0.7349150096743438,
            "fpr": 0.2557628979143798,
            "logloss": 3.489519272869798,
            "mae": 0.3255834211873916,
            "precision": 0.6603498542274052,
            "recall": 0.9096385542168675
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 6126,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.8025213361062251,
            "auditor_fn_violation": 0.027720067713142508,
            "auditor_fp_violation": 0.04202735457063712,
            "ave_precision_score": 0.8030820920424587,
            "fpr": 0.21710526315789475,
            "logloss": 0.7742088958757654,
            "mae": 0.3279610722825716,
            "precision": 0.660958904109589,
            "recall": 0.8464912280701754
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.8100622886892679,
            "auditor_fn_violation": 0.03857140967822994,
            "auditor_fp_violation": 0.04864143651842028,
            "ave_precision_score": 0.810503458070366,
            "fpr": 0.21844127332601537,
            "logloss": 0.8473634518223983,
            "mae": 0.3416378284367014,
            "precision": 0.676948051948052,
            "recall": 0.8373493975903614
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7444477401224093,
            "auditor_fn_violation": 0.022915704832256086,
            "auditor_fp_violation": 0.03671321945213912,
            "ave_precision_score": 0.7403433058279798,
            "fpr": 0.15679824561403508,
            "logloss": 1.7821098481288218,
            "mae": 0.28196715331407135,
            "precision": 0.7099391480730223,
            "recall": 0.7675438596491229
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7764130111105333,
            "auditor_fn_violation": 0.029598084985386117,
            "auditor_fp_violation": 0.03071685054605667,
            "ave_precision_score": 0.7751659909101678,
            "fpr": 0.15916575192096596,
            "logloss": 1.587802322073098,
            "mae": 0.3120652212025123,
            "precision": 0.7178988326848249,
            "recall": 0.7409638554216867
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8045358696080617,
            "auditor_fn_violation": 0.014494844567559253,
            "auditor_fp_violation": 0.02627731609726072,
            "ave_precision_score": 0.7547017030968988,
            "fpr": 0.22807017543859648,
            "logloss": 2.9373849831958028,
            "mae": 0.2942588909784624,
            "precision": 0.6698412698412698,
            "recall": 0.9254385964912281
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7836909727909305,
            "auditor_fn_violation": 0.02251596947614828,
            "auditor_fp_violation": 0.033369391590009664,
            "ave_precision_score": 0.7328496980997745,
            "fpr": 0.23929747530186607,
            "logloss": 3.4859881672478443,
            "mae": 0.3230110562485967,
            "precision": 0.6721804511278195,
            "recall": 0.8975903614457831
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7986427692588054,
            "auditor_fn_violation": 0.024464258233302554,
            "auditor_fp_violation": 0.03558546860572485,
            "ave_precision_score": 0.7489129034609443,
            "fpr": 0.2225877192982456,
            "logloss": 2.928171626739681,
            "mae": 0.29423883098863673,
            "precision": 0.6672131147540984,
            "recall": 0.8925438596491229
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.783052207431491,
            "auditor_fn_violation": 0.027045613849470334,
            "auditor_fp_violation": 0.027328083180285092,
            "ave_precision_score": 0.732277336444052,
            "fpr": 0.22941822173435786,
            "logloss": 3.459416023484693,
            "mae": 0.320854526575793,
            "precision": 0.6744548286604362,
            "recall": 0.8694779116465864
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 6126,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.7865962968825446,
            "auditor_fn_violation": 0.02567376500461681,
            "auditor_fp_violation": 0.03503000923361034,
            "ave_precision_score": 0.7276667481904914,
            "fpr": 0.2412280701754386,
            "logloss": 3.2818601660922084,
            "mae": 0.3321949538335344,
            "precision": 0.6411092985318108,
            "recall": 0.8618421052631579
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7674525468258739,
            "auditor_fn_violation": 0.03203814158940923,
            "auditor_fp_violation": 0.02850551372384337,
            "ave_precision_score": 0.7054138859691864,
            "fpr": 0.2414928649835346,
            "logloss": 3.8995313168344183,
            "mae": 0.3522407425387878,
            "precision": 0.6599690880989181,
            "recall": 0.857429718875502
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8018791212387931,
            "auditor_fn_violation": 0.015581717451523546,
            "auditor_fp_violation": 0.02695060018467221,
            "ave_precision_score": 0.7521481077919653,
            "fpr": 0.20394736842105263,
            "logloss": 2.9020717248345647,
            "mae": 0.2831607283389517,
            "precision": 0.6884422110552764,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7832189068296135,
            "auditor_fn_violation": 0.024810548450663248,
            "auditor_fp_violation": 0.02815467663185761,
            "ave_precision_score": 0.7324488830322372,
            "fpr": 0.21844127332601537,
            "logloss": 3.446553265792232,
            "mae": 0.31728387240631567,
            "precision": 0.6875981161695447,
            "recall": 0.8795180722891566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8032000647934536,
            "auditor_fn_violation": 0.014393851954447524,
            "auditor_fp_violation": 0.02892236072637735,
            "ave_precision_score": 0.7471701157891542,
            "fpr": 0.2225877192982456,
            "logloss": 3.026003095171275,
            "mae": 0.30973215283931443,
            "precision": 0.6683006535947712,
            "recall": 0.8969298245614035
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7822238983521959,
            "auditor_fn_violation": 0.026476928570483914,
            "auditor_fp_violation": 0.032112225343727326,
            "ave_precision_score": 0.7243922481870131,
            "fpr": 0.23161361141602635,
            "logloss": 3.526215433739419,
            "mae": 0.3285614426367871,
            "precision": 0.674884437596302,
            "recall": 0.8795180722891566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8025770519221378,
            "auditor_fn_violation": 0.01851050323176362,
            "auditor_fp_violation": 0.027566174207448443,
            "ave_precision_score": 0.7528455316645177,
            "fpr": 0.20175438596491227,
            "logloss": 2.8677710272747996,
            "mae": 0.28094392812372393,
            "precision": 0.688663282571912,
            "recall": 0.8925438596491229
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7837313328875378,
            "auditor_fn_violation": 0.023219111352104353,
            "auditor_fp_violation": 0.026746012550399604,
            "ave_precision_score": 0.7329565285271207,
            "fpr": 0.21624588364434688,
            "logloss": 3.41147324686932,
            "mae": 0.3161527908540695,
            "precision": 0.687797147385103,
            "recall": 0.8714859437751004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8021245905091244,
            "auditor_fn_violation": 0.01579091643582641,
            "auditor_fp_violation": 0.030249692212988618,
            "ave_precision_score": 0.7523950862182086,
            "fpr": 0.21600877192982457,
            "logloss": 2.924411490490108,
            "mae": 0.2889561879203416,
            "precision": 0.6791530944625407,
            "recall": 0.9144736842105263
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7836056691458413,
            "auditor_fn_violation": 0.025099299503171857,
            "auditor_fp_violation": 0.03219461890320884,
            "ave_precision_score": 0.7328353934209098,
            "fpr": 0.23380900109769484,
            "logloss": 3.4660192794818796,
            "mae": 0.32022290641574824,
            "precision": 0.6762917933130699,
            "recall": 0.893574297188755
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8019385601116822,
            "auditor_fn_violation": 0.015581717451523546,
            "auditor_fp_violation": 0.02695060018467221,
            "ave_precision_score": 0.7522074993288865,
            "fpr": 0.20394736842105263,
            "logloss": 2.901152315705747,
            "mae": 0.2829363971241251,
            "precision": 0.6884422110552764,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7832443297256667,
            "auditor_fn_violation": 0.024810548450663248,
            "auditor_fp_violation": 0.02815467663185761,
            "ave_precision_score": 0.7324725999642451,
            "fpr": 0.21844127332601537,
            "logloss": 3.4454650924538774,
            "mae": 0.3170660355477326,
            "precision": 0.6875981161695447,
            "recall": 0.8795180722891566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8026876517134979,
            "auditor_fn_violation": 0.015420610187750079,
            "auditor_fp_violation": 0.029388850415512476,
            "ave_precision_score": 0.7529553901523691,
            "fpr": 0.20394736842105263,
            "logloss": 2.876204992991814,
            "mae": 0.28331707572432063,
            "precision": 0.6863406408094435,
            "recall": 0.8925438596491229
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7839887947959094,
            "auditor_fn_violation": 0.02717786623993229,
            "auditor_fp_violation": 0.027849023104748797,
            "ave_precision_score": 0.733216330020314,
            "fpr": 0.20965971459934138,
            "logloss": 3.4204058404793938,
            "mae": 0.3160461125371643,
            "precision": 0.694888178913738,
            "recall": 0.8734939759036144
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8118097045280123,
            "auditor_fn_violation": 0.015971260387811635,
            "auditor_fp_violation": 0.03857917820867959,
            "ave_precision_score": 0.7599168353600586,
            "fpr": 0.23903508771929824,
            "logloss": 3.027808124545008,
            "mae": 0.30035801433690756,
            "precision": 0.6588419405320813,
            "recall": 0.9232456140350878
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7886674251999809,
            "auditor_fn_violation": 0.021953896816684963,
            "auditor_fp_violation": 0.034015250782074354,
            "ave_precision_score": 0.7357465146671778,
            "fpr": 0.24698133918770582,
            "logloss": 3.5830412913987115,
            "mae": 0.32049844009764583,
            "precision": 0.668141592920354,
            "recall": 0.9096385542168675
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.7666362606838593,
            "auditor_fn_violation": 0.030949426746691294,
            "auditor_fp_violation": 0.033774815327793174,
            "ave_precision_score": 0.7681798706822576,
            "fpr": 0.17324561403508773,
            "logloss": 0.8210036677451705,
            "mae": 0.3266912284736013,
            "precision": 0.7013232514177694,
            "recall": 0.8135964912280702
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7824324069645611,
            "auditor_fn_violation": 0.0458651290122069,
            "auditor_fp_violation": 0.03745185956948036,
            "ave_precision_score": 0.7820293677217991,
            "fpr": 0.18441273326015367,
            "logloss": 0.8879420248920381,
            "mae": 0.34480773147601873,
            "precision": 0.7037037037037037,
            "recall": 0.8012048192771084
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.7999251705098929,
            "auditor_fn_violation": 0.016206909818405665,
            "auditor_fp_violation": 0.027498845798707296,
            "ave_precision_score": 0.7620311006958248,
            "fpr": 0.19078947368421054,
            "logloss": 2.459783300034141,
            "mae": 0.27282034555955714,
            "precision": 0.698961937716263,
            "recall": 0.8859649122807017
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7957189353282639,
            "auditor_fn_violation": 0.027036797023439533,
            "auditor_fp_violation": 0.0259353662393719,
            "ave_precision_score": 0.7609111051999125,
            "fpr": 0.1986827661909989,
            "logloss": 2.7302789996158308,
            "mae": 0.3022103192422156,
            "precision": 0.7008264462809918,
            "recall": 0.8514056224899599
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8010288843508702,
            "auditor_fn_violation": 0.018938519544475223,
            "auditor_fp_violation": 0.028073541859033553,
            "ave_precision_score": 0.7512984804044123,
            "fpr": 0.20285087719298245,
            "logloss": 2.9026130348454595,
            "mae": 0.2827039601681089,
            "precision": 0.6890756302521008,
            "recall": 0.8991228070175439
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7827819068503002,
            "auditor_fn_violation": 0.02360705169745943,
            "auditor_fp_violation": 0.02642441188274601,
            "ave_precision_score": 0.7320105462622671,
            "fpr": 0.21624588364434688,
            "logloss": 3.4441313133945766,
            "mae": 0.3167510572650232,
            "precision": 0.6882911392405063,
            "recall": 0.8734939759036144
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 6126,
        "test": {
            "accuracy": 0.6513157894736842,
            "auc_prc": 0.753184648836295,
            "auditor_fn_violation": 0.012648122499230531,
            "auditor_fp_violation": 0.03929093567251461,
            "ave_precision_score": 0.7525699389174056,
            "fpr": 0.3201754385964912,
            "logloss": 1.1202464222494883,
            "mae": 0.3576719565761251,
            "precision": 0.5955678670360111,
            "recall": 0.9429824561403509
        },
        "train": {
            "accuracy": 0.6608122941822173,
            "auc_prc": 0.784617663186949,
            "auditor_fn_violation": 0.020887060866958504,
            "auditor_fp_violation": 0.03696547178286373,
            "ave_precision_score": 0.7825379102967313,
            "fpr": 0.29747530186608123,
            "logloss": 1.0223214526360385,
            "mae": 0.34392483114100314,
            "precision": 0.6292749658002736,
            "recall": 0.9236947791164659
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 6126,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8047275478393849,
            "auditor_fn_violation": 0.01572599261311173,
            "auditor_fp_violation": 0.02634704909202832,
            "ave_precision_score": 0.7548649482964527,
            "fpr": 0.20942982456140352,
            "logloss": 2.9267522889047637,
            "mae": 0.2876139920098022,
            "precision": 0.6868852459016394,
            "recall": 0.918859649122807
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7833705639590648,
            "auditor_fn_violation": 0.023884781717429544,
            "auditor_fp_violation": 0.030267672754044603,
            "ave_precision_score": 0.731902985235191,
            "fpr": 0.2283205268935236,
            "logloss": 3.501396353588582,
            "mae": 0.3198181970438593,
            "precision": 0.6809815950920245,
            "recall": 0.891566265060241
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7742849462476311,
            "auditor_fn_violation": 0.02511830563250231,
            "auditor_fp_violation": 0.03447695444752232,
            "ave_precision_score": 0.7295971854447635,
            "fpr": 0.18640350877192982,
            "logloss": 2.869004580775649,
            "mae": 0.3060006476830707,
            "precision": 0.6755725190839694,
            "recall": 0.7763157894736842
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7699691727441033,
            "auditor_fn_violation": 0.03187282610133178,
            "auditor_fp_violation": 0.027405161026251654,
            "ave_precision_score": 0.7255652934558977,
            "fpr": 0.18880351262349068,
            "logloss": 3.1939218057007004,
            "mae": 0.33152206103779897,
            "precision": 0.6872727272727273,
            "recall": 0.7590361445783133
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8023845999370866,
            "auditor_fn_violation": 0.03152893197907049,
            "auditor_fp_violation": 0.032856263465681754,
            "ave_precision_score": 0.8029281025335718,
            "fpr": 0.12938596491228072,
            "logloss": 0.6771033950697529,
            "mae": 0.31576303004368844,
            "precision": 0.7462365591397849,
            "recall": 0.7609649122807017
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8139950283214602,
            "auditor_fn_violation": 0.04264698751096593,
            "auditor_fp_violation": 0.03933628001052512,
            "ave_precision_score": 0.8144377219631758,
            "fpr": 0.13721185510428102,
            "logloss": 0.7479125760665805,
            "mae": 0.3339457924093457,
            "precision": 0.748995983935743,
            "recall": 0.748995983935743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8029284793239427,
            "auditor_fn_violation": 0.014874769159741461,
            "auditor_fp_violation": 0.028936788242536166,
            "ave_precision_score": 0.7531971160790533,
            "fpr": 0.23464912280701755,
            "logloss": 2.923779694624087,
            "mae": 0.2986533168115628,
            "precision": 0.6640502354788069,
            "recall": 0.9276315789473685
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7840941289079308,
            "auditor_fn_violation": 0.02535939587108037,
            "auditor_fp_violation": 0.02873143154822814,
            "ave_precision_score": 0.7333232496915263,
            "fpr": 0.24698133918770582,
            "logloss": 3.469762347458208,
            "mae": 0.3255379317972464,
            "precision": 0.6651785714285714,
            "recall": 0.8975903614457831
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.846336897986645,
            "auditor_fn_violation": 0.02119883040935673,
            "auditor_fp_violation": 0.030550265466297324,
            "ave_precision_score": 0.8467019300438834,
            "fpr": 0.14144736842105263,
            "logloss": 0.7797790563603332,
            "mae": 0.25481405845804966,
            "precision": 0.7430278884462151,
            "recall": 0.8179824561403509
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8366215355149219,
            "auditor_fn_violation": 0.029457015768893355,
            "auditor_fp_violation": 0.02725100533431852,
            "ave_precision_score": 0.8368648874694191,
            "fpr": 0.15587266739846323,
            "logloss": 0.9754980400632336,
            "mae": 0.2905351574280007,
            "precision": 0.7384898710865562,
            "recall": 0.8052208835341366
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.7949152967039967,
            "auditor_fn_violation": 0.0274435403200985,
            "auditor_fp_violation": 0.03595096568174823,
            "ave_precision_score": 0.7451873331485968,
            "fpr": 0.20942982456140352,
            "logloss": 2.9726578375042307,
            "mae": 0.29186511501360257,
            "precision": 0.6773648648648649,
            "recall": 0.8793859649122807
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7824570782715988,
            "auditor_fn_violation": 0.03001468001534128,
            "auditor_fp_violation": 0.03129360546242721,
            "ave_precision_score": 0.7322631165309815,
            "fpr": 0.21514818880351264,
            "logloss": 3.470428083837077,
            "mae": 0.31645674589041783,
            "precision": 0.6823338735818476,
            "recall": 0.8453815261044176
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8047124540860842,
            "auditor_fn_violation": 0.015990497076023392,
            "auditor_fp_violation": 0.027816251154201298,
            "ave_precision_score": 0.7549333198590986,
            "fpr": 0.2236842105263158,
            "logloss": 2.9037670598909355,
            "mae": 0.2945091777241673,
            "precision": 0.6720257234726688,
            "recall": 0.9166666666666666
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7843589517905359,
            "auditor_fn_violation": 0.022253668901732068,
            "auditor_fp_violation": 0.029829126389062398,
            "ave_precision_score": 0.7335837144736524,
            "fpr": 0.24698133918770582,
            "logloss": 3.442956248046582,
            "mae": 0.32298048077520414,
            "precision": 0.6661721068249258,
            "recall": 0.9016064257028112
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 6126,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8032984175275671,
            "auditor_fn_violation": 0.01644736842105263,
            "auditor_fp_violation": 0.02816251154201293,
            "ave_precision_score": 0.7535662950798258,
            "fpr": 0.19736842105263158,
            "logloss": 2.8606292041118513,
            "mae": 0.27777215369338065,
            "precision": 0.6938775510204082,
            "recall": 0.8947368421052632
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.7846264567677149,
            "auditor_fn_violation": 0.02324556183019675,
            "auditor_fp_violation": 0.026150652636726814,
            "ave_precision_score": 0.733853772478949,
            "fpr": 0.21405049396267836,
            "logloss": 3.3941685713999314,
            "mae": 0.3130170423383919,
            "precision": 0.6914556962025317,
            "recall": 0.8775100401606426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8043597942960539,
            "auditor_fn_violation": 0.01639206294244383,
            "auditor_fp_violation": 0.03146160357032934,
            "ave_precision_score": 0.7546281448192353,
            "fpr": 0.2149122807017544,
            "logloss": 2.889764510888787,
            "mae": 0.286207448574906,
            "precision": 0.6781609195402298,
            "recall": 0.9057017543859649
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7850240903118384,
            "auditor_fn_violation": 0.02292374768007265,
            "auditor_fp_violation": 0.025361269179758828,
            "ave_precision_score": 0.7342528780816233,
            "fpr": 0.2239297475301866,
            "logloss": 3.4421818632238215,
            "mae": 0.3176487423956239,
            "precision": 0.6842105263157895,
            "recall": 0.8875502008032129
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.801906449993319,
            "auditor_fn_violation": 0.015581717451523546,
            "auditor_fp_violation": 0.02917003308710372,
            "ave_precision_score": 0.7521770532889935,
            "fpr": 0.20285087719298245,
            "logloss": 2.9013191735147026,
            "mae": 0.2826541693704258,
            "precision": 0.6895973154362416,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7832090364498849,
            "auditor_fn_violation": 0.02555336604375791,
            "auditor_fp_violation": 0.02815467663185761,
            "ave_precision_score": 0.7324373260775863,
            "fpr": 0.21844127332601537,
            "logloss": 3.446061898503673,
            "mae": 0.3168984105626658,
            "precision": 0.6871069182389937,
            "recall": 0.8775100401606426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8076903819232384,
            "auditor_fn_violation": 0.019845048476454293,
            "auditor_fp_violation": 0.031187480763311796,
            "ave_precision_score": 0.7546356362780651,
            "fpr": 0.20285087719298245,
            "logloss": 2.9385836858656886,
            "mae": 0.2912502117136115,
            "precision": 0.6895973154362416,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7858988995038645,
            "auditor_fn_violation": 0.024925167189063607,
            "auditor_fp_violation": 0.03329231374404309,
            "ave_precision_score": 0.7305715877538042,
            "fpr": 0.22063666300768386,
            "logloss": 3.452521043181421,
            "mae": 0.3206001892300507,
            "precision": 0.6844583987441131,
            "recall": 0.8755020080321285
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.7956877365763294,
            "auditor_fn_violation": 0.029537934749153588,
            "auditor_fp_violation": 0.03616978301015698,
            "ave_precision_score": 0.7965442116657476,
            "fpr": 0.1513157894736842,
            "logloss": 1.3925061093607847,
            "mae": 0.2697754830140099,
            "precision": 0.7250996015936255,
            "recall": 0.7982456140350878
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.8140873159619451,
            "auditor_fn_violation": 0.04380199172100036,
            "auditor_fp_violation": 0.030201226335107897,
            "ave_precision_score": 0.814831500165232,
            "fpr": 0.1525795828759605,
            "logloss": 1.6252582826594157,
            "mae": 0.2981848283609158,
            "precision": 0.7321772639691715,
            "recall": 0.7630522088353414
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8023053001511842,
            "auditor_fn_violation": 0.013944194367497694,
            "auditor_fp_violation": 0.029432132963988934,
            "ave_precision_score": 0.7525725950963835,
            "fpr": 0.21052631578947367,
            "logloss": 2.9060375680424486,
            "mae": 0.28902131037211115,
            "precision": 0.6847290640394089,
            "recall": 0.9144736842105263
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.78336946730559,
            "auditor_fn_violation": 0.024389545007692682,
            "auditor_fp_violation": 0.029576629997102945,
            "ave_precision_score": 0.7325993588135367,
            "fpr": 0.22502744237102085,
            "logloss": 3.4445771906931806,
            "mae": 0.3201341264280694,
            "precision": 0.683641975308642,
            "recall": 0.8895582329317269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8019676673695246,
            "auditor_fn_violation": 0.015581717451523546,
            "auditor_fp_violation": 0.02917003308710372,
            "ave_precision_score": 0.7522365920207317,
            "fpr": 0.20285087719298245,
            "logloss": 2.9000828291041825,
            "mae": 0.28255980707494255,
            "precision": 0.6895973154362416,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7832578545786295,
            "auditor_fn_violation": 0.02555336604375791,
            "auditor_fp_violation": 0.02815467663185761,
            "ave_precision_score": 0.7324861156937063,
            "fpr": 0.21844127332601537,
            "logloss": 3.444985100936506,
            "mae": 0.31686268823147706,
            "precision": 0.6871069182389937,
            "recall": 0.8775100401606426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8018755248049965,
            "auditor_fn_violation": 0.015581717451523546,
            "auditor_fp_violation": 0.02695060018467221,
            "ave_precision_score": 0.7521462893991742,
            "fpr": 0.20394736842105263,
            "logloss": 2.902129456908003,
            "mae": 0.2831792610899283,
            "precision": 0.6884422110552764,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7831979239205544,
            "auditor_fn_violation": 0.024810548450663248,
            "auditor_fp_violation": 0.02815467663185761,
            "ave_precision_score": 0.7324279148425128,
            "fpr": 0.21844127332601537,
            "logloss": 3.4465696477498473,
            "mae": 0.31730482020546213,
            "precision": 0.6875981161695447,
            "recall": 0.8795180722891566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8043904398395839,
            "auditor_fn_violation": 0.015334045090797171,
            "auditor_fp_violation": 0.02866747460757156,
            "ave_precision_score": 0.7544986038597722,
            "fpr": 0.21710526315789475,
            "logloss": 2.9404666950615304,
            "mae": 0.28945863336139954,
            "precision": 0.6769983686786297,
            "recall": 0.9100877192982456
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7826680207709611,
            "auditor_fn_violation": 0.022595320910425456,
            "auditor_fp_violation": 0.031407893302998334,
            "ave_precision_score": 0.7312064885230735,
            "fpr": 0.23161361141602635,
            "logloss": 3.4969370124857657,
            "mae": 0.3212645277422968,
            "precision": 0.6803030303030303,
            "recall": 0.9016064257028112
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7999826931961974,
            "auditor_fn_violation": 0.021131502000615573,
            "auditor_fp_violation": 0.03129568713450293,
            "ave_precision_score": 0.7502524957583395,
            "fpr": 0.20942982456140352,
            "logloss": 2.9277649041291895,
            "mae": 0.2870238630700636,
            "precision": 0.6821963394342762,
            "recall": 0.8991228070175439
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.782575085221948,
            "auditor_fn_violation": 0.02817416758141237,
            "auditor_fp_violation": 0.02826099090215633,
            "ave_precision_score": 0.7318027171632615,
            "fpr": 0.22283205268935236,
            "logloss": 3.467159862057399,
            "mae": 0.31833548780634047,
            "precision": 0.6828125,
            "recall": 0.8775100401606426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8299921092787895,
            "auditor_fn_violation": 0.03056950215450908,
            "auditor_fp_violation": 0.030506982917820873,
            "ave_precision_score": 0.8304439618146655,
            "fpr": 0.15899122807017543,
            "logloss": 0.7825451785996708,
            "mae": 0.2925786913118657,
            "precision": 0.7222222222222222,
            "recall": 0.8267543859649122
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.8235766137834744,
            "auditor_fn_violation": 0.04637870912850083,
            "auditor_fp_violation": 0.031282974035397336,
            "ave_precision_score": 0.8239418940876946,
            "fpr": 0.1756311745334797,
            "logloss": 0.9702320825119587,
            "mae": 0.3242048672036445,
            "precision": 0.714795008912656,
            "recall": 0.8052208835341366
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.7837891902274167,
            "auditor_fn_violation": 0.028778085564789172,
            "auditor_fp_violation": 0.03464768005540167,
            "ave_precision_score": 0.7845365397499723,
            "fpr": 0.15021929824561403,
            "logloss": 0.6417205277741619,
            "mae": 0.3210118308669238,
            "precision": 0.7198364008179959,
            "recall": 0.7719298245614035
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.8143421682457417,
            "auditor_fn_violation": 0.04302170261727481,
            "auditor_fp_violation": 0.04281541450605061,
            "ave_precision_score": 0.8147347601194812,
            "fpr": 0.16575192096597147,
            "logloss": 0.7171683668501013,
            "mae": 0.33229467520033007,
            "precision": 0.7214022140221402,
            "recall": 0.785140562248996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 6126,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8021751236421132,
            "auditor_fn_violation": 0.015581717451523546,
            "auditor_fp_violation": 0.02946098799630656,
            "ave_precision_score": 0.7524455956504932,
            "fpr": 0.20723684210526316,
            "logloss": 2.903431610809184,
            "mae": 0.2845755482304486,
            "precision": 0.685,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7834261515940021,
            "auditor_fn_violation": 0.023230132384642853,
            "auditor_fp_violation": 0.02910353149427365,
            "ave_precision_score": 0.732655936154853,
            "fpr": 0.21953896816684962,
            "logloss": 3.448198037055655,
            "mae": 0.31805088725773806,
            "precision": 0.6870109546165885,
            "recall": 0.8815261044176707
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8022648354613409,
            "auditor_fn_violation": 0.013944194367497694,
            "auditor_fp_violation": 0.03052862419205909,
            "ave_precision_score": 0.7525336311889816,
            "fpr": 0.21052631578947367,
            "logloss": 2.9053854904904237,
            "mae": 0.2891557334529,
            "precision": 0.6847290640394089,
            "recall": 0.9144736842105263
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7833278770988346,
            "auditor_fn_violation": 0.024389545007692682,
            "auditor_fp_violation": 0.03079658624878072,
            "ave_precision_score": 0.7325561496483894,
            "fpr": 0.2283205268935236,
            "logloss": 3.4401092478923787,
            "mae": 0.320307372465011,
            "precision": 0.6804915514592934,
            "recall": 0.8895582329317269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 6126,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.8569963877462046,
            "auditor_fn_violation": 0.01941943674976916,
            "auditor_fp_violation": 0.02923255232379195,
            "ave_precision_score": 0.8573422500442435,
            "fpr": 0.15679824561403508,
            "logloss": 0.7699982480466264,
            "mae": 0.2561258619649179,
            "precision": 0.7306967984934086,
            "recall": 0.8508771929824561
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.844599866988516,
            "auditor_fn_violation": 0.03313804063675118,
            "auditor_fp_violation": 0.026512121155742432,
            "ave_precision_score": 0.8448914770700238,
            "fpr": 0.16136114160263446,
            "logloss": 0.9735414266571065,
            "mae": 0.2851037562437844,
            "precision": 0.7356115107913669,
            "recall": 0.821285140562249
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7905701754385965,
            "auc_prc": 0.8619914218190552,
            "auditor_fn_violation": 0.032048322560787934,
            "auditor_fp_violation": 0.0347799322868575,
            "ave_precision_score": 0.8625603493603122,
            "fpr": 0.11951754385964912,
            "logloss": 0.6534281601907882,
            "mae": 0.26128494767067223,
            "precision": 0.7743271221532091,
            "recall": 0.8201754385964912
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.840912675356178,
            "auditor_fn_violation": 0.042633762271919735,
            "auditor_fp_violation": 0.03320194661428917,
            "ave_precision_score": 0.841122331664955,
            "fpr": 0.1350164654226125,
            "logloss": 0.8820398363927776,
            "mae": 0.30510924194922007,
            "precision": 0.7520161290322581,
            "recall": 0.748995983935743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7984486849662062,
            "auditor_fn_violation": 0.02088142505386273,
            "auditor_fp_violation": 0.034621229609110495,
            "ave_precision_score": 0.7487217372172876,
            "fpr": 0.22587719298245615,
            "logloss": 2.9385943908425474,
            "mae": 0.29559485437852406,
            "precision": 0.6672051696284329,
            "recall": 0.9057017543859649
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7817785020961231,
            "auditor_fn_violation": 0.027197704098501585,
            "auditor_fp_violation": 0.033063738062900846,
            "ave_precision_score": 0.7310091604716243,
            "fpr": 0.23929747530186607,
            "logloss": 3.46910890029897,
            "mae": 0.3234251768887705,
            "precision": 0.670196671709531,
            "recall": 0.8895582329317269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.7989392082516121,
            "auditor_fn_violation": 0.022028412588488765,
            "auditor_fp_violation": 0.013881675130809484,
            "ave_precision_score": 0.7999175717728505,
            "fpr": 0.1524122807017544,
            "logloss": 0.7402264519677719,
            "mae": 0.30011671296654163,
            "precision": 0.7274509803921568,
            "recall": 0.8135964912280702
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8066232575450201,
            "auditor_fn_violation": 0.027737734692887914,
            "auditor_fp_violation": 0.029061005786154163,
            "ave_precision_score": 0.8070847899856922,
            "fpr": 0.1690450054884742,
            "logloss": 0.9006969672429346,
            "mae": 0.323957308078723,
            "precision": 0.7240143369175627,
            "recall": 0.8112449799196787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8091192231977165,
            "auditor_fn_violation": 0.023391812865497078,
            "auditor_fp_violation": 0.029107513850415517,
            "ave_precision_score": 0.8098781547187354,
            "fpr": 0.12171052631578948,
            "logloss": 1.2670182221438058,
            "mae": 0.2668037363047579,
            "precision": 0.7618025751072961,
            "recall": 0.7785087719298246
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.8200313063812394,
            "auditor_fn_violation": 0.033955801251107615,
            "auditor_fp_violation": 0.027484896728975697,
            "ave_precision_score": 0.8205857522282174,
            "fpr": 0.13611416026344675,
            "logloss": 1.5442592962899762,
            "mae": 0.3013647510818176,
            "precision": 0.7464212678936605,
            "recall": 0.7329317269076305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8019276648747365,
            "auditor_fn_violation": 0.016904239766081873,
            "auditor_fp_violation": 0.02614265927977839,
            "ave_precision_score": 0.7521966141868136,
            "fpr": 0.22807017543859648,
            "logloss": 2.9111249842719404,
            "mae": 0.2942368505315104,
            "precision": 0.6677316293929713,
            "recall": 0.9166666666666666
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7842158298815358,
            "auditor_fn_violation": 0.02233742874902464,
            "auditor_fp_violation": 0.028845719388799276,
            "ave_precision_score": 0.7334409086170373,
            "fpr": 0.2349066959385291,
            "logloss": 3.4445678276522993,
            "mae": 0.3218369522194522,
            "precision": 0.6747720364741642,
            "recall": 0.891566265060241
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.7872628688969505,
            "auditor_fn_violation": 0.03766543551862112,
            "auditor_fp_violation": 0.03594134733764236,
            "ave_precision_score": 0.7887197886849344,
            "fpr": 0.13925438596491227,
            "logloss": 0.7534551592766914,
            "mae": 0.30929527594419665,
            "precision": 0.728051391862955,
            "recall": 0.7456140350877193
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7682483952109033,
            "auditor_fn_violation": 0.052795154272413485,
            "auditor_fp_violation": 0.03460529498223223,
            "ave_precision_score": 0.7678417086018989,
            "fpr": 0.145993413830955,
            "logloss": 0.9180166697172657,
            "mae": 0.3457367726129925,
            "precision": 0.7285714285714285,
            "recall": 0.7168674698795181
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7708333333333334,
            "auc_prc": 0.8599928419519305,
            "auditor_fn_violation": 0.018407106032625426,
            "auditor_fp_violation": 0.03303901200369345,
            "ave_precision_score": 0.8603281678972954,
            "fpr": 0.16228070175438597,
            "logloss": 0.7621851334763525,
            "mae": 0.25711511721456864,
            "precision": 0.7274401473296501,
            "recall": 0.8662280701754386
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8458471765652903,
            "auditor_fn_violation": 0.023488024546043672,
            "auditor_fp_violation": 0.025316085614881877,
            "ave_precision_score": 0.8460961031216292,
            "fpr": 0.17453347969264543,
            "logloss": 0.9686839388146856,
            "mae": 0.2859240763939321,
            "precision": 0.7268041237113402,
            "recall": 0.8493975903614458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8081937135734223,
            "auditor_fn_violation": 0.023045552477685444,
            "auditor_fp_violation": 0.03974780701754386,
            "ave_precision_score": 0.7536454865849184,
            "fpr": 0.23026315789473684,
            "logloss": 3.0077789479588737,
            "mae": 0.30966091058009926,
            "precision": 0.6645367412140575,
            "recall": 0.9122807017543859
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7869126722711851,
            "auditor_fn_violation": 0.0290161744673535,
            "auditor_fp_violation": 0.04059610411356491,
            "ave_precision_score": 0.7308447067120296,
            "fpr": 0.24588364434687157,
            "logloss": 3.498857677143941,
            "mae": 0.32793502537033437,
            "precision": 0.6666666666666666,
            "recall": 0.8995983935742972
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7970465824306652,
            "auditor_fn_violation": 0.02458448753462604,
            "auditor_fp_violation": 0.042618882733148684,
            "ave_precision_score": 0.7472137806244014,
            "fpr": 0.24671052631578946,
            "logloss": 2.9270502205400986,
            "mae": 0.31479990562810195,
            "precision": 0.6445497630331753,
            "recall": 0.8947368421052632
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7824220862668831,
            "auditor_fn_violation": 0.03086109531429781,
            "auditor_fp_violation": 0.03776814452361905,
            "ave_precision_score": 0.7309071679891629,
            "fpr": 0.24259055982436883,
            "logloss": 3.395958507012242,
            "mae": 0.3272108313711575,
            "precision": 0.6661631419939577,
            "recall": 0.8855421686746988
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7741228070175439,
            "auc_prc": 0.8111264248838526,
            "auditor_fn_violation": 0.009082121421975991,
            "auditor_fp_violation": 0.013251673591874426,
            "ave_precision_score": 0.7615056930600919,
            "fpr": 0.17653508771929824,
            "logloss": 2.7006538538950036,
            "mae": 0.27832949490951714,
            "precision": 0.7185314685314685,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7925148031645096,
            "auditor_fn_violation": 0.01881069833670577,
            "auditor_fp_violation": 0.011535098327410754,
            "ave_precision_score": 0.7429561678061658,
            "fpr": 0.19209659714599342,
            "logloss": 3.15155535744242,
            "mae": 0.31507599509002193,
            "precision": 0.7097844112769486,
            "recall": 0.8594377510040161
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 6126,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8046994724532065,
            "auditor_fn_violation": 0.03132213758079409,
            "auditor_fp_violation": 0.030211218836565103,
            "ave_precision_score": 0.8055436656378354,
            "fpr": 0.15350877192982457,
            "logloss": 0.7994545811396994,
            "mae": 0.29305207472928535,
            "precision": 0.7276264591439688,
            "recall": 0.8201754385964912
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8141306143500374,
            "auditor_fn_violation": 0.04852340206049225,
            "auditor_fp_violation": 0.034530874993023125,
            "ave_precision_score": 0.8145369822253392,
            "fpr": 0.16794731064763996,
            "logloss": 0.9811108143023488,
            "mae": 0.32571466930328935,
            "precision": 0.7213114754098361,
            "recall": 0.7951807228915663
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 6126,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.7999736389275468,
            "auditor_fn_violation": 0.018938519544475223,
            "auditor_fp_violation": 0.029528316405047713,
            "ave_precision_score": 0.7501846638802554,
            "fpr": 0.20614035087719298,
            "logloss": 2.931292360793286,
            "mae": 0.28479770750759875,
            "precision": 0.68561872909699,
            "recall": 0.8991228070175439
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7813753240290555,
            "auditor_fn_violation": 0.030153545025326335,
            "auditor_fp_violation": 0.028404515167059597,
            "ave_precision_score": 0.730605303492899,
            "fpr": 0.2217343578485181,
            "logloss": 3.477417557647026,
            "mae": 0.3180974041623594,
            "precision": 0.680379746835443,
            "recall": 0.8634538152610441
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 6126,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.6705828996099187,
            "auditor_fn_violation": 0.019501192674669134,
            "auditor_fp_violation": 0.03631646275777164,
            "ave_precision_score": 0.6685742890231832,
            "fpr": 0.24890350877192982,
            "logloss": 2.074152744400789,
            "mae": 0.34815045246454496,
            "precision": 0.641390205371248,
            "recall": 0.8903508771929824
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7276205201204313,
            "auditor_fn_violation": 0.020228003121156413,
            "auditor_fp_violation": 0.034238510749701676,
            "ave_precision_score": 0.7230504863208228,
            "fpr": 0.24039517014270034,
            "logloss": 1.540683635264448,
            "mae": 0.3396604902652269,
            "precision": 0.6681818181818182,
            "recall": 0.8855421686746988
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7899370019625714,
            "auditor_fn_violation": 0.02006867497691598,
            "auditor_fp_violation": 0.029915454755309327,
            "ave_precision_score": 0.7345759475008804,
            "fpr": 0.23574561403508773,
            "logloss": 3.1333012597413217,
            "mae": 0.33326490845271833,
            "precision": 0.6492659053833605,
            "recall": 0.8728070175438597
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7704954203523476,
            "auditor_fn_violation": 0.02506623640555637,
            "auditor_fp_violation": 0.025326717041911747,
            "ave_precision_score": 0.7121139422422134,
            "fpr": 0.24478594950603733,
            "logloss": 3.7306513229089755,
            "mae": 0.35303542786156494,
            "precision": 0.661608497723824,
            "recall": 0.8755020080321285
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.8555844788314262,
            "auditor_fn_violation": 0.010599415204678367,
            "auditor_fp_violation": 0.015355686365035393,
            "ave_precision_score": 0.855855267003629,
            "fpr": 0.1162280701754386,
            "logloss": 0.7165200744094025,
            "mae": 0.2566544000290122,
            "precision": 0.7720430107526882,
            "recall": 0.7872807017543859
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8383287893528842,
            "auditor_fn_violation": 0.018700488011320807,
            "auditor_fp_violation": 0.01183012042748968,
            "ave_precision_score": 0.8385490966755315,
            "fpr": 0.1350164654226125,
            "logloss": 0.9521832338551993,
            "mae": 0.29684547880933954,
            "precision": 0.7544910179640718,
            "recall": 0.7590361445783133
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8071877411971309,
            "auditor_fn_violation": 0.02902094875346261,
            "auditor_fp_violation": 0.0424962488457987,
            "ave_precision_score": 0.8077260621453797,
            "fpr": 0.17434210526315788,
            "logloss": 0.7512495192995116,
            "mae": 0.3208311315998655,
            "precision": 0.6965648854961832,
            "recall": 0.8004385964912281
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.8085053041361205,
            "auditor_fn_violation": 0.04955056229308011,
            "auditor_fp_violation": 0.04569387337438837,
            "ave_precision_score": 0.8089401534064007,
            "fpr": 0.1778265642151482,
            "logloss": 0.8464750558196772,
            "mae": 0.33942592004259386,
            "precision": 0.7043795620437956,
            "recall": 0.7751004016064257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8018741351406065,
            "auditor_fn_violation": 0.015581717451523546,
            "auditor_fp_violation": 0.02917003308710372,
            "ave_precision_score": 0.7521449221652912,
            "fpr": 0.20285087719298245,
            "logloss": 2.9005321898017544,
            "mae": 0.2824655696325451,
            "precision": 0.6895973154362416,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7831751534005094,
            "auditor_fn_violation": 0.02555336604375791,
            "auditor_fp_violation": 0.02815467663185761,
            "ave_precision_score": 0.7324051779146392,
            "fpr": 0.21844127332601537,
            "logloss": 3.445046996861175,
            "mae": 0.3168206801547522,
            "precision": 0.6871069182389937,
            "recall": 0.8775100401606426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8024690844260481,
            "auditor_fn_violation": 0.01388888888888889,
            "auditor_fp_violation": 0.027556555863342568,
            "ave_precision_score": 0.7527410060082452,
            "fpr": 0.19736842105263158,
            "logloss": 2.8860644399465114,
            "mae": 0.28375244809280925,
            "precision": 0.6964586846543002,
            "recall": 0.9057017543859649
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.784235206557906,
            "auditor_fn_violation": 0.02744237102085621,
            "auditor_fp_violation": 0.024816408544477916,
            "ave_precision_score": 0.7334618446253098,
            "fpr": 0.20856201975850713,
            "logloss": 3.443240275682574,
            "mae": 0.3148956141167823,
            "precision": 0.6945337620578779,
            "recall": 0.8674698795180723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8018887391185867,
            "auditor_fn_violation": 0.015908741151123423,
            "auditor_fp_violation": 0.026469682979378274,
            "ave_precision_score": 0.7521577137428401,
            "fpr": 0.20285087719298245,
            "logloss": 2.90013248249825,
            "mae": 0.2824792853427978,
            "precision": 0.6885521885521886,
            "recall": 0.8969298245614035
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7832575066333559,
            "auditor_fn_violation": 0.02555336604375791,
            "auditor_fp_violation": 0.02815467663185761,
            "ave_precision_score": 0.7324857687109096,
            "fpr": 0.21844127332601537,
            "logloss": 3.4441889590100416,
            "mae": 0.31671041618729134,
            "precision": 0.6871069182389937,
            "recall": 0.8775100401606426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 6126,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.813948951836601,
            "auditor_fn_violation": 0.0032029085872576175,
            "auditor_fp_violation": 0.024966816712834728,
            "ave_precision_score": 0.7639723122206606,
            "fpr": 0.2883771929824561,
            "logloss": 2.9702654311978423,
            "mae": 0.3247383244719303,
            "precision": 0.6248216833095578,
            "recall": 0.9605263157894737
        },
        "train": {
            "accuracy": 0.6695938529088913,
            "auc_prc": 0.7875518896990091,
            "auditor_fn_violation": 0.005058653935169878,
            "auditor_fp_violation": 0.020122633510789578,
            "ave_precision_score": 0.7353828634027086,
            "fpr": 0.300768386388584,
            "logloss": 3.537735173078983,
            "mae": 0.34089111493816615,
            "precision": 0.6322147651006711,
            "recall": 0.9457831325301205
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8532960774244235,
            "auditor_fn_violation": 0.007002154509079717,
            "auditor_fp_violation": 0.023661126500461687,
            "ave_precision_score": 0.8535615795521673,
            "fpr": 0.18421052631578946,
            "logloss": 0.7930370202572594,
            "mae": 0.26632702643661094,
            "precision": 0.7062937062937062,
            "recall": 0.8859649122807017
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8290304376736757,
            "auditor_fn_violation": 0.015623415726572594,
            "auditor_fp_violation": 0.023646951571192017,
            "ave_precision_score": 0.8293025696264854,
            "fpr": 0.19209659714599342,
            "logloss": 1.0385308662198411,
            "mae": 0.29448565014869493,
            "precision": 0.7107438016528925,
            "recall": 0.8634538152610441
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8019516177080963,
            "auditor_fn_violation": 0.015581717451523546,
            "auditor_fp_violation": 0.02695060018467221,
            "ave_precision_score": 0.7522205485696534,
            "fpr": 0.20394736842105263,
            "logloss": 2.900166613066822,
            "mae": 0.2827011010598362,
            "precision": 0.6884422110552764,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7832226820139152,
            "auditor_fn_violation": 0.02555336604375791,
            "auditor_fp_violation": 0.02815467663185761,
            "ave_precision_score": 0.7324509648176027,
            "fpr": 0.21844127332601537,
            "logloss": 3.444775739834601,
            "mae": 0.3169553157337975,
            "precision": 0.6871069182389937,
            "recall": 0.8775100401606426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 6126,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7974790752494881,
            "auditor_fn_violation": 0.01556728993536473,
            "auditor_fp_violation": 0.03471260387811635,
            "ave_precision_score": 0.7477068304537694,
            "fpr": 0.27631578947368424,
            "logloss": 3.0025356610090785,
            "mae": 0.32525743187098455,
            "precision": 0.6283185840707964,
            "recall": 0.9342105263157895
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.779701275978925,
            "auditor_fn_violation": 0.019414650919815375,
            "auditor_fp_violation": 0.03389033151447336,
            "ave_precision_score": 0.7288757694025374,
            "fpr": 0.2722283205268935,
            "logloss": 3.483375430468218,
            "mae": 0.3412610147993697,
            "precision": 0.6492220650636492,
            "recall": 0.9216867469879518
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8026040298438726,
            "auditor_fn_violation": 0.017178362573099414,
            "auditor_fp_violation": 0.033277066020313946,
            "ave_precision_score": 0.7528261126847737,
            "fpr": 0.19407894736842105,
            "logloss": 2.9063897065986084,
            "mae": 0.28072227464848265,
            "precision": 0.6984667802385008,
            "recall": 0.8991228070175439
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7841241849650349,
            "auditor_fn_violation": 0.024803935831140152,
            "auditor_fp_violation": 0.031035793356952834,
            "ave_precision_score": 0.733290402573194,
            "fpr": 0.21624588364434688,
            "logloss": 3.454456150034377,
            "mae": 0.31504068574744826,
            "precision": 0.6882911392405063,
            "recall": 0.8734939759036144
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.814717241494338,
            "auditor_fn_violation": 0.0021280586334256717,
            "auditor_fp_violation": 0.017269736842105265,
            "ave_precision_score": 0.7619133347851017,
            "fpr": 0.20394736842105263,
            "logloss": 2.8402289957438254,
            "mae": 0.2967760978560451,
            "precision": 0.6915422885572139,
            "recall": 0.9144736842105263
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7872151130426273,
            "auditor_fn_violation": 0.005175476880077943,
            "auditor_fp_violation": 0.017961795966968154,
            "ave_precision_score": 0.7326840594703737,
            "fpr": 0.21734357848518113,
            "logloss": 3.3605888678470883,
            "mae": 0.326164736731222,
            "precision": 0.6911076443057722,
            "recall": 0.8895582329317269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7931093446058219,
            "auditor_fn_violation": 0.03283702677746999,
            "auditor_fp_violation": 0.04116170360110804,
            "ave_precision_score": 0.7452655954983326,
            "fpr": 0.23355263157894737,
            "logloss": 3.8554054995279,
            "mae": 0.3069739420851059,
            "precision": 0.6525285481239804,
            "recall": 0.8771929824561403
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7816694372379198,
            "auditor_fn_violation": 0.03411670832616966,
            "auditor_fp_violation": 0.040048585621526515,
            "ave_precision_score": 0.7344432048497274,
            "fpr": 0.23600439077936333,
            "logloss": 4.225118866069722,
            "mae": 0.32482023863453313,
            "precision": 0.6687211093990755,
            "recall": 0.8714859437751004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8028887016200799,
            "auditor_fn_violation": 0.0182267620806402,
            "auditor_fp_violation": 0.03275046168051708,
            "ave_precision_score": 0.7531039381208494,
            "fpr": 0.20614035087719298,
            "logloss": 2.9260276197408444,
            "mae": 0.2843324741152151,
            "precision": 0.6866666666666666,
            "recall": 0.9035087719298246
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7843331712924779,
            "auditor_fn_violation": 0.025295473882357093,
            "auditor_fp_violation": 0.03240990530056375,
            "ave_precision_score": 0.7328113339453757,
            "fpr": 0.2239297475301866,
            "logloss": 3.440144876134883,
            "mae": 0.3163360387212253,
            "precision": 0.6827371695178849,
            "recall": 0.8815261044176707
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8091122993193668,
            "auditor_fn_violation": 0.019681536626654356,
            "auditor_fp_violation": 0.0307113727300708,
            "ave_precision_score": 0.7558930165782664,
            "fpr": 0.19956140350877194,
            "logloss": 2.932196557354449,
            "mae": 0.29073227790971456,
            "precision": 0.6941176470588235,
            "recall": 0.9057017543859649
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7867127204419522,
            "auditor_fn_violation": 0.02459012779989332,
            "auditor_fp_violation": 0.03661463469087797,
            "ave_precision_score": 0.7307017953530869,
            "fpr": 0.21514818880351264,
            "logloss": 3.4726558899676814,
            "mae": 0.31875496750243737,
            "precision": 0.6903633491311216,
            "recall": 0.8775100401606426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 6126,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7772466630782257,
            "auditor_fn_violation": 0.028855032317636198,
            "auditor_fp_violation": 0.05029191674361345,
            "ave_precision_score": 0.7380461588547275,
            "fpr": 0.25109649122807015,
            "logloss": 3.0904941065603895,
            "mae": 0.3260129999518949,
            "precision": 0.640502354788069,
            "recall": 0.8947368421052632
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7822653753733719,
            "auditor_fn_violation": 0.03334964446149031,
            "auditor_fp_violation": 0.048787618640081035,
            "ave_precision_score": 0.747002764521171,
            "fpr": 0.2491767288693743,
            "logloss": 3.0284491089691072,
            "mae": 0.3390950985608441,
            "precision": 0.6581325301204819,
            "recall": 0.8775100401606426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 6126,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8024086029653966,
            "auditor_fn_violation": 0.015754847645429365,
            "auditor_fp_violation": 0.029388850415512476,
            "ave_precision_score": 0.7526764116913947,
            "fpr": 0.20394736842105263,
            "logloss": 2.879027409503776,
            "mae": 0.2842829543790385,
            "precision": 0.6868686868686869,
            "recall": 0.8947368421052632
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7841663588344072,
            "auditor_fn_violation": 0.02753053928116418,
            "auditor_fp_violation": 0.02945702644301688,
            "ave_precision_score": 0.7333937157806869,
            "fpr": 0.21185510428100987,
            "logloss": 3.422326860689767,
            "mae": 0.3161743210356616,
            "precision": 0.6931637519872814,
            "recall": 0.8755020080321285
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 6126,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8220878749342357,
            "auditor_fn_violation": 0.02190096952908588,
            "auditor_fp_violation": 0.030622403047091417,
            "ave_precision_score": 0.8225888779001771,
            "fpr": 0.1611842105263158,
            "logloss": 0.8135361593059265,
            "mae": 0.2631481951552526,
            "precision": 0.7215909090909091,
            "recall": 0.8355263157894737
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.8235493726604235,
            "auditor_fn_violation": 0.03208442992607092,
            "auditor_fp_violation": 0.03265177026549331,
            "ave_precision_score": 0.8239500749883034,
            "fpr": 0.18441273326015367,
            "logloss": 0.9973516339495024,
            "mae": 0.299100185061274,
            "precision": 0.7052631578947368,
            "recall": 0.8072289156626506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 6126,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.8333513996349138,
            "auditor_fn_violation": 0.02262715450907972,
            "auditor_fp_violation": 0.04783202523853495,
            "ave_precision_score": 0.833067479539431,
            "fpr": 0.24890350877192982,
            "logloss": 0.8813122839355745,
            "mae": 0.3185035010029791,
            "precision": 0.6396825396825396,
            "recall": 0.8837719298245614
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.8401751145531489,
            "auditor_fn_violation": 0.03294407046407364,
            "auditor_fp_violation": 0.04450049569028527,
            "ave_precision_score": 0.8404095910758603,
            "fpr": 0.24039517014270034,
            "logloss": 0.999739081317821,
            "mae": 0.3282878564690564,
            "precision": 0.6651376146788991,
            "recall": 0.8734939759036144
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8019605490147313,
            "auditor_fn_violation": 0.015581717451523546,
            "auditor_fp_violation": 0.02917003308710372,
            "ave_precision_score": 0.7522312779099293,
            "fpr": 0.20285087719298245,
            "logloss": 2.899573352844165,
            "mae": 0.28256028879073974,
            "precision": 0.6895973154362416,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7832814785077338,
            "auditor_fn_violation": 0.02555336604375791,
            "auditor_fp_violation": 0.02815467663185761,
            "ave_precision_score": 0.7325114141621042,
            "fpr": 0.21844127332601537,
            "logloss": 3.4440740742216374,
            "mae": 0.3168380541648204,
            "precision": 0.6871069182389937,
            "recall": 0.8775100401606426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8016553396995209,
            "auditor_fn_violation": 0.018301304247460758,
            "auditor_fp_violation": 0.02633502616189597,
            "ave_precision_score": 0.751926208311687,
            "fpr": 0.2050438596491228,
            "logloss": 2.901720978672674,
            "mae": 0.2847831540898352,
            "precision": 0.6872909698996655,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7829007202562445,
            "auditor_fn_violation": 0.02555336604375791,
            "auditor_fp_violation": 0.025451636309512738,
            "ave_precision_score": 0.7321279496755236,
            "fpr": 0.21953896816684962,
            "logloss": 3.4473433645106595,
            "mae": 0.31853008912563724,
            "precision": 0.6860282574568289,
            "recall": 0.8775100401606426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8032132150207665,
            "auditor_fn_violation": 0.018183479532163743,
            "auditor_fp_violation": 0.03981032625423208,
            "ave_precision_score": 0.7534800861371108,
            "fpr": 0.2324561403508772,
            "logloss": 2.9292350680723245,
            "mae": 0.2958103935440594,
            "precision": 0.6634920634920635,
            "recall": 0.9166666666666666
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7831990319751169,
            "auditor_fn_violation": 0.029022787086876597,
            "auditor_fp_violation": 0.035386704868927796,
            "ave_precision_score": 0.7324272504535543,
            "fpr": 0.23819978046103182,
            "logloss": 3.4823440850331013,
            "mae": 0.32187239495071845,
            "precision": 0.6722054380664653,
            "recall": 0.893574297188755
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8053008344801764,
            "auditor_fn_violation": 0.01639206294244383,
            "auditor_fp_violation": 0.03365458602646969,
            "ave_precision_score": 0.755568372470721,
            "fpr": 0.2149122807017544,
            "logloss": 2.892026530923391,
            "mae": 0.28597286228621793,
            "precision": 0.6781609195402298,
            "recall": 0.9057017543859649
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7846600784698479,
            "auditor_fn_violation": 0.02738506165165602,
            "auditor_fp_violation": 0.02965902355658445,
            "ave_precision_score": 0.7338848900513743,
            "fpr": 0.2239297475301866,
            "logloss": 3.45185677479411,
            "mae": 0.3173430632293253,
            "precision": 0.6832298136645962,
            "recall": 0.8835341365461847
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8019561084828348,
            "auditor_fn_violation": 0.015581717451523546,
            "auditor_fp_violation": 0.02917003308710372,
            "ave_precision_score": 0.7522250402818191,
            "fpr": 0.20285087719298245,
            "logloss": 2.900242406032988,
            "mae": 0.28257709939459486,
            "precision": 0.6895973154362416,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7832696370616669,
            "auditor_fn_violation": 0.02555336604375791,
            "auditor_fp_violation": 0.02815467663185761,
            "ave_precision_score": 0.7324995852667313,
            "fpr": 0.21844127332601537,
            "logloss": 3.4451930785644005,
            "mae": 0.31687016640337246,
            "precision": 0.6871069182389937,
            "recall": 0.8775100401606426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8021505788391597,
            "auditor_fn_violation": 0.01799111265004617,
            "auditor_fp_violation": 0.029537934749153588,
            "ave_precision_score": 0.7523655602704008,
            "fpr": 0.2050438596491228,
            "logloss": 2.910742133307547,
            "mae": 0.2847377085456062,
            "precision": 0.6872909698996655,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7825891153037154,
            "auditor_fn_violation": 0.025643738510573583,
            "auditor_fp_violation": 0.029441079302472077,
            "ave_precision_score": 0.7318182473459866,
            "fpr": 0.2261251372118551,
            "logloss": 3.4466467153746048,
            "mae": 0.31882156712649656,
            "precision": 0.6801242236024845,
            "recall": 0.8795180722891566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7972493816958786,
            "auditor_fn_violation": 0.02502693136349646,
            "auditor_fp_violation": 0.0288550323176362,
            "ave_precision_score": 0.7475223514720106,
            "fpr": 0.19736842105263158,
            "logloss": 2.973393865358404,
            "mae": 0.28432554690558987,
            "precision": 0.6896551724137931,
            "recall": 0.8771929824561403
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7805193027124808,
            "auditor_fn_violation": 0.032080021513055515,
            "auditor_fp_violation": 0.029111505064546062,
            "ave_precision_score": 0.7297505502679872,
            "fpr": 0.21295279912184412,
            "logloss": 3.5171954813367683,
            "mae": 0.31690077160256663,
            "precision": 0.686084142394822,
            "recall": 0.8514056224899599
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8019676673695246,
            "auditor_fn_violation": 0.015581717451523546,
            "auditor_fp_violation": 0.02917003308710372,
            "ave_precision_score": 0.7522365920207317,
            "fpr": 0.20285087719298245,
            "logloss": 2.900083162828733,
            "mae": 0.2825597050534732,
            "precision": 0.6895973154362416,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7832578545786295,
            "auditor_fn_violation": 0.02555336604375791,
            "auditor_fp_violation": 0.02815467663185761,
            "ave_precision_score": 0.7324861156937063,
            "fpr": 0.21844127332601537,
            "logloss": 3.444985113698114,
            "mae": 0.3168626469719863,
            "precision": 0.6871069182389937,
            "recall": 0.8775100401606426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 6126,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8054702932811363,
            "auditor_fn_violation": 0.015370113881194214,
            "auditor_fp_violation": 0.026907317636195763,
            "ave_precision_score": 0.7555451070653212,
            "fpr": 0.21600877192982457,
            "logloss": 2.9331546147753564,
            "mae": 0.29036879410703925,
            "precision": 0.6801948051948052,
            "recall": 0.918859649122807
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7835118224721611,
            "auditor_fn_violation": 0.021129523582805426,
            "auditor_fp_violation": 0.02657059400440674,
            "ave_precision_score": 0.7320381894604472,
            "fpr": 0.2349066959385291,
            "logloss": 3.505814147646934,
            "mae": 0.32110821806310286,
            "precision": 0.6757575757575758,
            "recall": 0.8955823293172691
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8053953550607849,
            "auditor_fn_violation": 0.017409202831640505,
            "auditor_fp_violation": 0.03101194598337951,
            "ave_precision_score": 0.7550539548400366,
            "fpr": 0.20723684210526316,
            "logloss": 2.8559309678412985,
            "mae": 0.2929783629827983,
            "precision": 0.6844741235392321,
            "recall": 0.8991228070175439
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7842146685428708,
            "auditor_fn_violation": 0.024228637932630633,
            "auditor_fp_violation": 0.03669968610711694,
            "ave_precision_score": 0.7313540085607673,
            "fpr": 0.22502744237102085,
            "logloss": 3.371328308822037,
            "mae": 0.31916624860582116,
            "precision": 0.6811819595645412,
            "recall": 0.8795180722891566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 6126,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8120360736665234,
            "auditor_fn_violation": 0.009875634810710987,
            "auditor_fp_violation": 0.025079832256078797,
            "ave_precision_score": 0.7620734172150492,
            "fpr": 0.1962719298245614,
            "logloss": 2.7898612096403457,
            "mae": 0.2823330111472715,
            "precision": 0.6976351351351351,
            "recall": 0.9057017543859649
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7868630248052494,
            "auditor_fn_violation": 0.012563977093885972,
            "auditor_fp_violation": 0.017047493242399195,
            "ave_precision_score": 0.7335004254774226,
            "fpr": 0.20856201975850713,
            "logloss": 3.353234205503617,
            "mae": 0.3102562347389315,
            "precision": 0.6988906497622821,
            "recall": 0.8855421686746988
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8155912332453319,
            "auditor_fn_violation": 0.023468759618344107,
            "auditor_fp_violation": 0.019861880578639583,
            "ave_precision_score": 0.8161758975713699,
            "fpr": 0.11951754385964912,
            "logloss": 1.147570955038498,
            "mae": 0.2759748880730905,
            "precision": 0.7593818984547461,
            "recall": 0.7543859649122807
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8230962970201341,
            "auditor_fn_violation": 0.03922605901101662,
            "auditor_fp_violation": 0.0235911365792852,
            "ave_precision_score": 0.8234375650197003,
            "fpr": 0.11964873765093303,
            "logloss": 1.3586091837072094,
            "mae": 0.30745400031328307,
            "precision": 0.7645788336933045,
            "recall": 0.7108433734939759
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8023970424299514,
            "auditor_fn_violation": 0.015600954139735303,
            "auditor_fp_violation": 0.029432132963988934,
            "ave_precision_score": 0.7526655698949003,
            "fpr": 0.21052631578947367,
            "logloss": 2.90279516485277,
            "mae": 0.28806654025525597,
            "precision": 0.6842105263157895,
            "recall": 0.9122807017543859
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7835969044326536,
            "auditor_fn_violation": 0.025132362600787345,
            "auditor_fp_violation": 0.0313520783110915,
            "ave_precision_score": 0.7328249041861797,
            "fpr": 0.2239297475301866,
            "logloss": 3.43996957766123,
            "mae": 0.31928659134092474,
            "precision": 0.6842105263157895,
            "recall": 0.8875502008032129
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8027870386091911,
            "auditor_fn_violation": 0.01848645737149892,
            "auditor_fp_violation": 0.02646968297937828,
            "ave_precision_score": 0.753057247712803,
            "fpr": 0.20175438596491227,
            "logloss": 2.8653831671206786,
            "mae": 0.2809356559444621,
            "precision": 0.6902356902356902,
            "recall": 0.8991228070175439
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7840744597689668,
            "auditor_fn_violation": 0.026331450940975762,
            "auditor_fp_violation": 0.02370542441985632,
            "ave_precision_score": 0.7333022257264185,
            "fpr": 0.21624588364434688,
            "logloss": 3.4088372081160023,
            "mae": 0.3160203599751726,
            "precision": 0.6882911392405063,
            "recall": 0.8734939759036144
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 6126,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8022322456118859,
            "auditor_fn_violation": 0.015581717451523546,
            "auditor_fp_violation": 0.029251789012003697,
            "ave_precision_score": 0.7525009271904525,
            "fpr": 0.20065789473684212,
            "logloss": 2.888368512083573,
            "mae": 0.28189461288484396,
            "precision": 0.6919191919191919,
            "recall": 0.9013157894736842
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7836195901295497,
            "auditor_fn_violation": 0.023626889556028725,
            "auditor_fp_violation": 0.029172635769967824,
            "ave_precision_score": 0.7328476107400977,
            "fpr": 0.21953896816684962,
            "logloss": 3.428684595141771,
            "mae": 0.31616825604286275,
            "precision": 0.6860282574568289,
            "recall": 0.8775100401606426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 6126,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8135448448678559,
            "auditor_fn_violation": 0.017361111111111112,
            "auditor_fp_violation": 0.027734495229301326,
            "ave_precision_score": 0.7594494811391692,
            "fpr": 0.14692982456140352,
            "logloss": 2.8667307226286916,
            "mae": 0.273916960772387,
            "precision": 0.7392996108949417,
            "recall": 0.8333333333333334
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.787550028627577,
            "auditor_fn_violation": 0.022502744237102086,
            "auditor_fp_violation": 0.028011152366954332,
            "ave_precision_score": 0.7303380189038352,
            "fpr": 0.15697036223929747,
            "logloss": 3.405683908581865,
            "mae": 0.3120591299389906,
            "precision": 0.7361623616236163,
            "recall": 0.8012048192771084
        }
    }
]
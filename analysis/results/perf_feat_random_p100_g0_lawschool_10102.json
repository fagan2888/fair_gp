[
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(0)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8097343558816805,
            "auditor_fn_violation": 0.013026127550636249,
            "auditor_fp_violation": 0.011535284399339152,
            "ave_precision_score": 0.8102217014435542,
            "fpr": 0.13925438596491227,
            "logloss": 0.9054659275635634,
            "mae": 0.2774800887959754,
            "precision": 0.7348643006263048,
            "recall": 0.7553648068669528
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.838975064386127,
            "auditor_fn_violation": 0.013077864353709676,
            "auditor_fp_violation": 0.028584181257184975,
            "ave_precision_score": 0.8392074429023889,
            "fpr": 0.12733260153677278,
            "logloss": 0.9100807773548164,
            "mae": 0.2678436229955503,
            "precision": 0.7613168724279835,
            "recall": 0.7581967213114754
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(1)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5603070175438597,
            "auc_prc": 0.6339624107082887,
            "auditor_fn_violation": 0.019929786913636022,
            "auditor_fp_violation": 0.03412398709778932,
            "ave_precision_score": 0.6357901300573253,
            "fpr": 0.3958333333333333,
            "logloss": 0.6828160457324409,
            "mae": 0.4874128648161627,
            "precision": 0.5412960609911055,
            "recall": 0.9141630901287554
        },
        "train": {
            "accuracy": 0.5686059275521405,
            "auc_prc": 0.6461118568754372,
            "auditor_fn_violation": 0.029345341994925417,
            "auditor_fp_violation": 0.031122114009752098,
            "ave_precision_score": 0.6472201623992232,
            "fpr": 0.3677277716794731,
            "logloss": 0.6732313613274131,
            "mae": 0.4823414649156001,
            "precision": 0.5620915032679739,
            "recall": 0.8811475409836066
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(2)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.6360371638517506,
            "auditor_fn_violation": 0.007348373616444548,
            "auditor_fp_violation": 0.004105695853984737,
            "ave_precision_score": 0.6852452338264676,
            "fpr": 0.1118421052631579,
            "logloss": 0.5915438071025347,
            "mae": 0.40011612273621977,
            "precision": 0.7553956834532374,
            "recall": 0.6759656652360515
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.6358274422866403,
            "auditor_fn_violation": 0.0003509024491191523,
            "auditor_fp_violation": 0.007551517699356177,
            "ave_precision_score": 0.697226441583634,
            "fpr": 0.1119648737650933,
            "logloss": 0.5957842943959182,
            "mae": 0.40257895560413764,
            "precision": 0.7681818181818182,
            "recall": 0.6926229508196722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(3)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7342392247648333,
            "auditor_fn_violation": 0.001185904675852722,
            "auditor_fp_violation": 0.0006957556447171843,
            "ave_precision_score": 0.6989608541691621,
            "fpr": 0.4857456140350877,
            "logloss": 6.881516785653091,
            "mae": 0.48516534101589803,
            "precision": 0.5115766262403528,
            "recall": 0.9957081545064378
        },
        "train": {
            "accuracy": 0.5455543358946213,
            "auc_prc": 0.7711420005367633,
            "auditor_fn_violation": 0.0010976948408342481,
            "auditor_fp_violation": 0.004154632246278091,
            "ave_precision_score": 0.738899357819347,
            "fpr": 0.4522502744237102,
            "logloss": 6.430855499666602,
            "mae": 0.4532623154048159,
            "precision": 0.5412026726057907,
            "recall": 0.9959016393442623
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(4)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5131578947368421,
            "auc_prc": 0.7558148599045496,
            "auditor_fn_violation": 0.0006282471199457873,
            "auditor_fp_violation": 0.0006957556447171843,
            "ave_precision_score": 0.5126778932959758,
            "fpr": 0.4857456140350877,
            "logloss": 16.794927357634272,
            "mae": 0.48684210513244597,
            "precision": 0.512114537444934,
            "recall": 0.9978540772532188
        },
        "train": {
            "accuracy": 0.5455543358946213,
            "auc_prc": 0.771412680756396,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.002545717822360287,
            "ave_precision_score": 0.542825361512792,
            "fpr": 0.4544456641053787,
            "logloss": 15.620253499461148,
            "mae": 0.45444511654086484,
            "precision": 0.541019955654102,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(5)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.8433444632256375,
            "auditor_fn_violation": 0.008986051502145922,
            "auditor_fp_violation": 0.021600385492880184,
            "ave_precision_score": 0.844142392713396,
            "fpr": 0.30701754385964913,
            "logloss": 0.6274693182330439,
            "mae": 0.3576588338658902,
            "precision": 0.6105702364394993,
            "recall": 0.9420600858369099
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.859846987967649,
            "auditor_fn_violation": 0.00474393118712998,
            "auditor_fp_violation": 0.02722957911317675,
            "ave_precision_score": 0.8600636243556841,
            "fpr": 0.2843029637760702,
            "logloss": 0.6075747562379719,
            "mae": 0.34500465636347594,
            "precision": 0.6392757660167131,
            "recall": 0.9405737704918032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(6)",
        "seed": 10102,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.6336939269618741,
            "auditor_fn_violation": 0.00695071907235901,
            "auditor_fp_violation": 0.020208874203445828,
            "ave_precision_score": 0.6344150947126475,
            "fpr": 0.375,
            "logloss": 1.1966377079584694,
            "mae": 0.4389284067836247,
            "precision": 0.5676359039190898,
            "recall": 0.9635193133047211
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6559619794762569,
            "auditor_fn_violation": 0.00878830685069551,
            "auditor_fp_violation": 0.019916803554143856,
            "ave_precision_score": 0.656857606994366,
            "fpr": 0.3512623490669594,
            "logloss": 1.5085080256047265,
            "mae": 0.43221666954220583,
            "precision": 0.5892169448010269,
            "recall": 0.9405737704918032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(7)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.7139762029808092,
            "auditor_fn_violation": 0.027108745576387327,
            "auditor_fp_violation": 0.04390881913303439,
            "ave_precision_score": 0.657346164868136,
            "fpr": 0.23903508771929824,
            "logloss": 0.6682770257688121,
            "mae": 0.4392611459910093,
            "precision": 0.6273504273504273,
            "recall": 0.7875536480686696
        },
        "train": {
            "accuracy": 0.6608122941822173,
            "auc_prc": 0.7308625917186444,
            "auditor_fn_violation": 0.032739648377750986,
            "auditor_fp_violation": 0.041992666464254855,
            "ave_precision_score": 0.6818673166553116,
            "fpr": 0.21514818880351264,
            "logloss": 0.7300732071470275,
            "mae": 0.4431927051000927,
            "precision": 0.6567425569176882,
            "recall": 0.7684426229508197
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(8)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8400366275863063,
            "auditor_fn_violation": 0.007393080340335818,
            "auditor_fp_violation": 0.011867181968373849,
            "ave_precision_score": 0.8404480605984379,
            "fpr": 0.20723684210526316,
            "logloss": 0.5564257737429009,
            "mae": 0.3414262191002791,
            "precision": 0.6834170854271356,
            "recall": 0.8755364806866953
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8620746722525464,
            "auditor_fn_violation": 0.007787335120836408,
            "auditor_fp_violation": 0.017926420710361674,
            "ave_precision_score": 0.8622643744475975,
            "fpr": 0.2052689352360044,
            "logloss": 0.535572345365243,
            "mae": 0.3330054414841495,
            "precision": 0.7027027027027027,
            "recall": 0.9057377049180327
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(9)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5822368421052632,
            "auc_prc": 0.512442422432659,
            "auditor_fn_violation": 0.007727204276786388,
            "auditor_fp_violation": 0.02175281252458501,
            "ave_precision_score": 0.5144128279216976,
            "fpr": 0.3991228070175439,
            "logloss": 0.6900503365176749,
            "mae": 0.4937748032164547,
            "precision": 0.5522755227552275,
            "recall": 0.9635193133047211
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.5329939078147551,
            "auditor_fn_violation": 0.009134710550466971,
            "auditor_fp_violation": 0.022831014680046606,
            "ave_precision_score": 0.5348669912969797,
            "fpr": 0.3567508232711306,
            "logloss": 0.6758163722070273,
            "mae": 0.48743721019910113,
            "precision": 0.5865139949109415,
            "recall": 0.944672131147541
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(10)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5460526315789473,
            "auc_prc": 0.723775137238566,
            "auditor_fn_violation": 0.0024471048866802204,
            "auditor_fp_violation": 0.007483675556604542,
            "ave_precision_score": 0.7254546657376625,
            "fpr": 0.4418859649122807,
            "logloss": 1.0421486842260497,
            "mae": 0.43455234731500403,
            "precision": 0.5303030303030303,
            "recall": 0.9763948497854077
        },
        "train": {
            "accuracy": 0.5598243688254665,
            "auc_prc": 0.7489415141987441,
            "auditor_fn_violation": 0.002152651562865524,
            "auditor_fp_violation": 0.005317202668721929,
            "ave_precision_score": 0.7505238267720298,
            "fpr": 0.42371020856201974,
            "logloss": 1.0138705631790672,
            "mae": 0.4173949162395969,
            "precision": 0.5506402793946449,
            "recall": 0.9692622950819673
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(11)",
        "seed": 10102,
        "test": {
            "accuracy": 0.48135964912280704,
            "auc_prc": 0.6065157644388266,
            "auditor_fn_violation": 0.011204916798433858,
            "auditor_fp_violation": 0.010574010699394228,
            "ave_precision_score": 0.520271748370183,
            "fpr": 0.33881578947368424,
            "logloss": 0.6933017456878272,
            "mae": 0.498827609707389,
            "precision": 0.49427168576104746,
            "recall": 0.648068669527897
        },
        "train": {
            "accuracy": 0.4829857299670692,
            "auc_prc": 0.6144896078930557,
            "auditor_fn_violation": 0.002033434705151975,
            "auditor_fp_violation": 0.01326575892752879,
            "ave_precision_score": 0.5342441831586787,
            "fpr": 0.3062568605927552,
            "logloss": 0.6935567620673633,
            "mae": 0.49888067371104605,
            "precision": 0.5147826086956522,
            "recall": 0.6065573770491803
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(12)",
        "seed": 10102,
        "test": {
            "accuracy": 0.48903508771929827,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.64810285090831,
            "mae": 0.5109649122807017,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4643249176728869,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.50156188882153,
            "mae": 0.535675082327113,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(13)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7828947368421053,
            "auc_prc": 0.8694189830238563,
            "auditor_fn_violation": 0.009416647842782929,
            "auditor_fp_violation": 0.00887026984501613,
            "ave_precision_score": 0.8537036286383393,
            "fpr": 0.13267543859649122,
            "logloss": 0.5053156769553434,
            "mae": 0.3214284862213739,
            "precision": 0.7627450980392156,
            "recall": 0.8347639484978541
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8687728277560423,
            "auditor_fn_violation": 0.005837127278616545,
            "auditor_fp_violation": 0.01556235451650824,
            "ave_precision_score": 0.8484886780281932,
            "fpr": 0.1437980241492865,
            "logloss": 0.48614957190043784,
            "mae": 0.3154393865246212,
            "precision": 0.7574074074074074,
            "recall": 0.8381147540983607
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(14)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.7280505826339814,
            "auditor_fn_violation": 0.009955481514946166,
            "auditor_fp_violation": 0.025767543859649144,
            "ave_precision_score": 0.6340830476182969,
            "fpr": 0.25548245614035087,
            "logloss": 0.6292646094230306,
            "mae": 0.44538340449594616,
            "precision": 0.6324921135646687,
            "recall": 0.8605150214592274
        },
        "train": {
            "accuracy": 0.6531284302963776,
            "auc_prc": 0.7155234557873105,
            "auditor_fn_violation": 0.017792553670079723,
            "auditor_fp_violation": 0.03329155345877677,
            "ave_precision_score": 0.6307498044957107,
            "fpr": 0.23819978046103182,
            "logloss": 0.6431088047150174,
            "mae": 0.4517137236835928,
            "precision": 0.641914191419142,
            "recall": 0.7971311475409836
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(15)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8375544921189516,
            "auditor_fn_violation": 0.0004329493261049692,
            "auditor_fp_violation": 0.005917610730863031,
            "ave_precision_score": 0.8378566991729189,
            "fpr": 0.044956140350877194,
            "logloss": 0.5535383229759322,
            "mae": 0.3424445577251741,
            "precision": 0.8655737704918033,
            "recall": 0.5665236051502146
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.8485585068245002,
            "auditor_fn_violation": 0.003149124543376939,
            "auditor_fp_violation": 0.01145443268898906,
            "ave_precision_score": 0.848797160226072,
            "fpr": 0.05817782656421515,
            "logloss": 0.5657014894622211,
            "mae": 0.3522135871917451,
            "precision": 0.832807570977918,
            "recall": 0.5409836065573771
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(16)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.7619307401922919,
            "auditor_fn_violation": 0.012296702055568104,
            "auditor_fp_violation": 0.020031862166627345,
            "ave_precision_score": 0.6227682078082399,
            "fpr": 0.17543859649122806,
            "logloss": 0.6373559742091475,
            "mae": 0.4521822350981988,
            "precision": 0.6707818930041153,
            "recall": 0.6995708154506438
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7860509149558295,
            "auditor_fn_violation": 0.020743733242158682,
            "auditor_fp_violation": 0.026383601528987707,
            "ave_precision_score": 0.6585593466970444,
            "fpr": 0.1525795828759605,
            "logloss": 0.6249252946997451,
            "mae": 0.44630815051128,
            "precision": 0.7110187110187111,
            "recall": 0.7008196721311475
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(17)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7850877192982456,
            "auc_prc": 0.8716896948690283,
            "auditor_fn_violation": 0.0028400534598298287,
            "auditor_fp_violation": 0.0033165171898355736,
            "ave_precision_score": 0.8618767367833176,
            "fpr": 0.0756578947368421,
            "logloss": 0.47426769188548423,
            "mae": 0.2953247653622703,
            "precision": 0.8308823529411765,
            "recall": 0.7274678111587983
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8619239824752054,
            "auditor_fn_violation": 0.00026542621151320303,
            "auditor_fp_violation": 0.008322239608878098,
            "ave_precision_score": 0.8507553307262614,
            "fpr": 0.08342480790340286,
            "logloss": 0.4918431805950638,
            "mae": 0.2992470653443449,
            "precision": 0.8215962441314554,
            "recall": 0.7172131147540983
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(18)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8532844216497013,
            "auditor_fn_violation": 0.011101385437843536,
            "auditor_fp_violation": 0.013384076783887974,
            "ave_precision_score": 0.8400478313897274,
            "fpr": 0.19956140350877194,
            "logloss": 0.5267471581088422,
            "mae": 0.3607317880122808,
            "precision": 0.689419795221843,
            "recall": 0.8669527896995708
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8480854995664686,
            "auditor_fn_violation": 0.00993773730902809,
            "auditor_fp_violation": 0.007014347883628777,
            "ave_precision_score": 0.834005054390129,
            "fpr": 0.1712403951701427,
            "logloss": 0.5233129003365201,
            "mae": 0.3544167515491549,
            "precision": 0.7314974182444062,
            "recall": 0.8709016393442623
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(19)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.7624394203676791,
            "auditor_fn_violation": 0.00407537083050975,
            "auditor_fp_violation": 0.008004877665014565,
            "ave_precision_score": 0.7692944560843537,
            "fpr": 0.3048245614035088,
            "logloss": 0.5940722156948398,
            "mae": 0.3777902938686965,
            "precision": 0.6095505617977528,
            "recall": 0.9313304721030042
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.7791336858775614,
            "auditor_fn_violation": 0.002631768368393588,
            "auditor_fp_violation": 0.013608301998427427,
            "ave_precision_score": 0.7907235977601796,
            "fpr": 0.29198682766191,
            "logloss": 0.5843067488174502,
            "mae": 0.3704834760838361,
            "precision": 0.6305555555555555,
            "recall": 0.930327868852459
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(20)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7632294998229562,
            "auditor_fn_violation": 0.012299055041036068,
            "auditor_fp_violation": 0.015188616159232164,
            "ave_precision_score": 0.763741584595709,
            "fpr": 0.18092105263157895,
            "logloss": 0.9681253416744903,
            "mae": 0.31640875792276857,
            "precision": 0.6863117870722434,
            "recall": 0.7746781115879828
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7940432263626896,
            "auditor_fn_violation": 0.010396609742491592,
            "auditor_fp_violation": 0.025981372923008263,
            "ave_precision_score": 0.7944799925432908,
            "fpr": 0.1734357848518112,
            "logloss": 0.9162869446128532,
            "mae": 0.3005843049483415,
            "precision": 0.7074074074074074,
            "recall": 0.7827868852459017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(21)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7933692629718909,
            "auditor_fn_violation": 0.01622148181612831,
            "auditor_fp_violation": 0.006878884430807966,
            "ave_precision_score": 0.7942529722436201,
            "fpr": 0.11842105263157894,
            "logloss": 0.9986659708403096,
            "mae": 0.2916441875719843,
            "precision": 0.7452830188679245,
            "recall": 0.6781115879828327
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.829090209865048,
            "auditor_fn_violation": 0.011962174515484699,
            "auditor_fp_violation": 0.01697923721886167,
            "ave_precision_score": 0.829337508714348,
            "fpr": 0.09989023051591657,
            "logloss": 0.9929481669410835,
            "mae": 0.2850203010585227,
            "precision": 0.7893518518518519,
            "recall": 0.6987704918032787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(22)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7277662170651946,
            "auditor_fn_violation": 0.011063737670356155,
            "auditor_fp_violation": 0.020174455196286682,
            "ave_precision_score": 0.7148705144448514,
            "fpr": 0.1787280701754386,
            "logloss": 1.528671569124413,
            "mae": 0.33630949175713204,
            "precision": 0.6693711967545639,
            "recall": 0.7081545064377682
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7634783319364642,
            "auditor_fn_violation": 0.017803800543448923,
            "auditor_fp_violation": 0.026259040412297298,
            "ave_precision_score": 0.7526668037286319,
            "fpr": 0.15697036223929747,
            "logloss": 1.4132716351997334,
            "mae": 0.31225477546990765,
            "precision": 0.7105263157894737,
            "recall": 0.7192622950819673
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(23)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5076754385964912,
            "auc_prc": 0.6295882084281966,
            "auditor_fn_violation": 0.005110684436412944,
            "auditor_fp_violation": 0.000732633152387696,
            "ave_precision_score": 0.6301794816085133,
            "fpr": 0.005482456140350877,
            "logloss": 3.5555540098763725,
            "mae": 0.4818210437509126,
            "precision": 0.8148148148148148,
            "recall": 0.04721030042918455
        },
        "train": {
            "accuracy": 0.48737650933040616,
            "auc_prc": 0.63421169889075,
            "auditor_fn_violation": 0.0061407928595850255,
            "auditor_fp_violation": 0.0005709051181643843,
            "ave_precision_score": 0.634785959978073,
            "fpr": 0.0010976948408342481,
            "logloss": 3.728396924911488,
            "mae": 0.5084121931778925,
            "precision": 0.9565217391304348,
            "recall": 0.045081967213114756
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(24)",
        "seed": 10102,
        "test": {
            "accuracy": 0.48903508771929827,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.64810285090831,
            "mae": 0.5109649122807017,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4643249176728869,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.50156188882153,
            "mae": 0.535675082327113,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(25)",
        "seed": 10102,
        "test": {
            "accuracy": 0.4199561403508772,
            "auc_prc": 0.55040110446471,
            "auditor_fn_violation": 0.022955726225434844,
            "auditor_fp_violation": 0.016285107387302333,
            "ave_precision_score": 0.48141578665208934,
            "fpr": 0.2324561403508772,
            "logloss": 0.7224913334758125,
            "mae": 0.5101582359914717,
            "precision": 0.41274238227146814,
            "recall": 0.3197424892703863
        },
        "train": {
            "accuracy": 0.3809001097694841,
            "auc_prc": 0.5441602496489059,
            "auditor_fn_violation": 0.007049540227816683,
            "auditor_fp_violation": 0.013050371996584967,
            "ave_precision_score": 0.49295976566728733,
            "fpr": 0.23929747530186607,
            "logloss": 0.7312611801766278,
            "mae": 0.5146643340129622,
            "precision": 0.39444444444444443,
            "recall": 0.29098360655737704
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(26)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5175438596491229,
            "auc_prc": 0.649156009458858,
            "auditor_fn_violation": 0.002322396656878247,
            "auditor_fp_violation": 0.005342321611202907,
            "ave_precision_score": 0.6499132647276151,
            "fpr": 0.4791666666666667,
            "logloss": 2.1899336175376827,
            "mae": 0.4776744707160744,
            "precision": 0.5144444444444445,
            "recall": 0.9935622317596566
        },
        "train": {
            "accuracy": 0.5510428100987925,
            "auc_prc": 0.6432959745415465,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.006116469834152083,
            "ave_precision_score": 0.6453529776723879,
            "fpr": 0.4489571899012075,
            "logloss": 2.0794069308647503,
            "mae": 0.44872481275030895,
            "precision": 0.5440356744704571,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(27)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7750655438985773,
            "auditor_fn_violation": 0.011063737670356151,
            "auditor_fp_violation": 0.015911415309574392,
            "ave_precision_score": 0.7273422804251048,
            "fpr": 0.15789473684210525,
            "logloss": 4.3356038038762525,
            "mae": 0.2938675767827652,
            "precision": 0.7148514851485148,
            "recall": 0.7746781115879828
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8006252510731423,
            "auditor_fn_violation": 0.007377948930197409,
            "auditor_fp_violation": 0.021398561838106876,
            "ave_precision_score": 0.7543368526270678,
            "fpr": 0.14050493962678376,
            "logloss": 4.3877646452342685,
            "mae": 0.27565673519629624,
            "precision": 0.746031746031746,
            "recall": 0.7704918032786885
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(28)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8048736064671765,
            "auditor_fn_violation": 0.04508320156614713,
            "auditor_fp_violation": 0.037182361733931246,
            "ave_precision_score": 0.739750178932071,
            "fpr": 0.10526315789473684,
            "logloss": 0.571336058281968,
            "mae": 0.39098478680509224,
            "precision": 0.7725118483412322,
            "recall": 0.6995708154506438
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8025179041274746,
            "auditor_fn_violation": 0.047173885659786585,
            "auditor_fp_violation": 0.03353548564562881,
            "ave_precision_score": 0.7441878859918809,
            "fpr": 0.10647639956092206,
            "logloss": 0.5798053749542752,
            "mae": 0.3934502059718518,
            "precision": 0.7780320366132724,
            "recall": 0.6967213114754098
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(29)",
        "seed": 10102,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.8333745873306977,
            "auditor_fn_violation": 0.029929975152473455,
            "auditor_fp_violation": 0.051648178742821176,
            "ave_precision_score": 0.8338013588494203,
            "fpr": 0.2324561403508772,
            "logloss": 0.7034484496371927,
            "mae": 0.3237421983555168,
            "precision": 0.6530278232405892,
            "recall": 0.8562231759656652
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.8308850616089984,
            "auditor_fn_violation": 0.027185942308038367,
            "auditor_fp_violation": 0.04730208406318363,
            "ave_precision_score": 0.8311212679249029,
            "fpr": 0.2217343578485181,
            "logloss": 0.7082018569804144,
            "mae": 0.327681075936037,
            "precision": 0.6672158154859967,
            "recall": 0.8299180327868853
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(30)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5032894736842105,
            "auc_prc": 0.5526489657739578,
            "auditor_fn_violation": 0.006207175664483103,
            "auditor_fp_violation": 0.00045728109511448544,
            "ave_precision_score": 0.554037059328987,
            "fpr": 0.3848684210526316,
            "logloss": 0.6917568899803963,
            "mae": 0.4987696652349673,
            "precision": 0.509090909090909,
            "recall": 0.7811158798283262
        },
        "train": {
            "accuracy": 0.5126234906695939,
            "auc_prc": 0.568765048332506,
            "auditor_fn_violation": 0.004980115527883253,
            "auditor_fp_violation": 0.0013001066554561708,
            "ave_precision_score": 0.5707854195849267,
            "fpr": 0.37102085620197583,
            "logloss": 0.6899237876779386,
            "mae": 0.4978035607924184,
            "precision": 0.5305555555555556,
            "recall": 0.7827868852459017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(31)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.8109193593654964,
            "auditor_fn_violation": 0.03004291845493563,
            "auditor_fp_violation": 0.03143930453937535,
            "ave_precision_score": 0.8119880116659652,
            "fpr": 0.22587719298245615,
            "logloss": 1.8147068165885216,
            "mae": 0.34624901857532703,
            "precision": 0.6531986531986532,
            "recall": 0.8326180257510729
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.8018914029507498,
            "auditor_fn_violation": 0.031016627377589034,
            "auditor_fp_violation": 0.03489268281290143,
            "ave_precision_score": 0.8028838704487835,
            "fpr": 0.22722283205268934,
            "logloss": 1.8643940119404896,
            "mae": 0.3456282096782293,
            "precision": 0.6661290322580645,
            "recall": 0.8463114754098361
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(32)",
        "seed": 10102,
        "test": {
            "accuracy": 0.48903508771929827,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 3.6311131158173007,
            "mae": 0.5077447106013815,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4643249176728869,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 3.800845842626722,
            "mae": 0.5327868488682563,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(33)",
        "seed": 10102,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8369725380521371,
            "auditor_fn_violation": 0.0047271478051351574,
            "auditor_fp_violation": 0.0005949571237510806,
            "ave_precision_score": 0.8373921119533194,
            "fpr": 0.18530701754385964,
            "logloss": 0.5263834743269469,
            "mae": 0.337869141633028,
            "precision": 0.7065972222222222,
            "recall": 0.8733905579399142
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8525150382684691,
            "auditor_fn_violation": 0.0019502078422198633,
            "auditor_fp_violation": 0.01233674059887948,
            "ave_precision_score": 0.8527308774545483,
            "fpr": 0.18660812294182216,
            "logloss": 0.5263636950899817,
            "mae": 0.3391063525758042,
            "precision": 0.7161936560934892,
            "recall": 0.8790983606557377
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(34)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.5495176531062711,
            "auditor_fn_violation": 0.01044490249228221,
            "auditor_fp_violation": 0.03523031232790498,
            "ave_precision_score": 0.5504830538420786,
            "fpr": 0.29605263157894735,
            "logloss": 0.7110386955108655,
            "mae": 0.44877609352401476,
            "precision": 0.6081277213352685,
            "recall": 0.8991416309012875
        },
        "train": {
            "accuracy": 0.6520307354555434,
            "auc_prc": 0.5777015948593305,
            "auditor_fn_violation": 0.006694139029349844,
            "auditor_fp_violation": 0.040241025760795954,
            "ave_precision_score": 0.5776210050502972,
            "fpr": 0.27661909989023054,
            "logloss": 0.690169039407958,
            "mae": 0.4532023640482407,
            "precision": 0.6266666666666667,
            "recall": 0.8668032786885246
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(35)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7796052631578947,
            "auc_prc": 0.8578753771895149,
            "auditor_fn_violation": 0.004847150064001207,
            "auditor_fp_violation": 0.003609078750688381,
            "ave_precision_score": 0.8408179673890528,
            "fpr": 0.07894736842105263,
            "logloss": 0.4892999577194954,
            "mae": 0.2961572746729903,
            "precision": 0.823960880195599,
            "recall": 0.723175965665236
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8595222192761541,
            "auditor_fn_violation": 0.0006478199060661177,
            "auditor_fp_violation": 0.012027932830417828,
            "ave_precision_score": 0.8400945104022101,
            "fpr": 0.0889132821075741,
            "logloss": 0.4967074776210331,
            "mae": 0.29216497745589814,
            "precision": 0.8154897494305239,
            "recall": 0.7336065573770492
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(36)",
        "seed": 10102,
        "test": {
            "accuracy": 0.48903508771929827,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.853012558041378,
            "mae": 0.5049477295884699,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4643249176728869,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.8792024710785709,
            "mae": 0.5148642779247167,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(37)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.7276672836531435,
            "auditor_fn_violation": 0.0067624802349220735,
            "auditor_fp_violation": 0.010601054205019285,
            "ave_precision_score": 0.6896888304664194,
            "fpr": 0.2817982456140351,
            "logloss": 0.6356142477610179,
            "mae": 0.44653984067732827,
            "precision": 0.6064318529862175,
            "recall": 0.8497854077253219
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.7639103703253936,
            "auditor_fn_violation": 0.005990084756437714,
            "auditor_fp_violation": 0.018326054293076748,
            "ave_precision_score": 0.7283307383771069,
            "fpr": 0.2601536772777168,
            "logloss": 0.6176692770138855,
            "mae": 0.4375067449100193,
            "precision": 0.6441441441441441,
            "recall": 0.8790983606557377
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(38)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.7373985246650666,
            "auditor_fn_violation": 0.008611926812740008,
            "auditor_fp_violation": 0.023321335850837857,
            "ave_precision_score": 0.6559102406530308,
            "fpr": 0.21820175438596492,
            "logloss": 0.6044017458439394,
            "mae": 0.41630658580055624,
            "precision": 0.657487091222031,
            "recall": 0.8197424892703863
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7464180881162619,
            "auditor_fn_violation": 0.0017635097442910903,
            "auditor_fp_violation": 0.010128375800889053,
            "ave_precision_score": 0.6795503965065741,
            "fpr": 0.1942919868276619,
            "logloss": 0.6045452904858367,
            "mae": 0.41634892158692804,
            "precision": 0.6833631484794276,
            "recall": 0.7827868852459017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(39)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7263880204990242,
            "auditor_fn_violation": 0.004788325427302169,
            "auditor_fp_violation": 0.00472032098182677,
            "ave_precision_score": 0.733192825706366,
            "fpr": 0.10526315789473684,
            "logloss": 0.574396993998751,
            "mae": 0.38002948766868366,
            "precision": 0.7652811735941321,
            "recall": 0.6716738197424893
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.737679026903821,
            "auditor_fn_violation": 0.00929891490165734,
            "auditor_fp_violation": 0.008231413794624668,
            "ave_precision_score": 0.7413057275260437,
            "fpr": 0.09769484083424808,
            "logloss": 0.5657302694848745,
            "mae": 0.3703841548392592,
            "precision": 0.7954022988505747,
            "recall": 0.7090163934426229
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(40)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5153508771929824,
            "auc_prc": 0.6904242336837854,
            "auditor_fn_violation": 0.0039436036443038875,
            "auditor_fp_violation": 0.003653331759893006,
            "ave_precision_score": 0.6913570722141413,
            "fpr": 0.01644736842105263,
            "logloss": 0.7991123496736238,
            "mae": 0.46335377909749614,
            "precision": 0.7222222222222222,
            "recall": 0.08369098712446352
        },
        "train": {
            "accuracy": 0.4994511525795829,
            "auc_prc": 0.7414103811177554,
            "auditor_fn_violation": 0.006563675298267083,
            "auditor_fp_violation": 0.004325903781727403,
            "ave_precision_score": 0.7424495143791592,
            "fpr": 0.010976948408342482,
            "logloss": 0.797078329065815,
            "mae": 0.4662894432961483,
            "precision": 0.8076923076923077,
            "recall": 0.0860655737704918
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(41)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7719298245614035,
            "auc_prc": 0.857525342601124,
            "auditor_fn_violation": 0.007781322942549517,
            "auditor_fp_violation": 0.002542089528754621,
            "ave_precision_score": 0.8115478115570747,
            "fpr": 0.0756578947368421,
            "logloss": 0.5024904102236375,
            "mae": 0.33609043896655766,
            "precision": 0.8257575757575758,
            "recall": 0.7017167381974249
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8612344553769239,
            "auditor_fn_violation": 0.006529934678159468,
            "auditor_fp_violation": 0.009778047660197277,
            "ave_precision_score": 0.813374756741609,
            "fpr": 0.08562019758507135,
            "logloss": 0.5064518225471357,
            "mae": 0.3329832851886749,
            "precision": 0.8173302107728337,
            "recall": 0.7151639344262295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(42)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8418437751323654,
            "auditor_fn_violation": 0.012150816956554475,
            "auditor_fp_violation": 0.018015891747305487,
            "ave_precision_score": 0.8423887831460337,
            "fpr": 0.09100877192982457,
            "logloss": 0.5032764224168108,
            "mae": 0.3268869964185318,
            "precision": 0.7955665024630542,
            "recall": 0.6931330472103004
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8433921521736187,
            "auditor_fn_violation": 0.01610777203937306,
            "auditor_fp_violation": 0.011363606874735636,
            "ave_precision_score": 0.8459189054860464,
            "fpr": 0.0867178924259056,
            "logloss": 0.5019751233159082,
            "mae": 0.32403196952214747,
            "precision": 0.8162790697674419,
            "recall": 0.7192622950819673
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(43)",
        "seed": 10102,
        "test": {
            "accuracy": 0.48026315789473684,
            "auc_prc": 0.4814040417994181,
            "auditor_fn_violation": 0.014748512913184264,
            "auditor_fp_violation": 0.004840787506883802,
            "ave_precision_score": 0.4844670428949923,
            "fpr": 0.20723684210526316,
            "logloss": 0.6977355887895589,
            "mae": 0.5017466115389477,
            "precision": 0.4891891891891892,
            "recall": 0.388412017167382
        },
        "train": {
            "accuracy": 0.4994511525795829,
            "auc_prc": 0.5138076908161215,
            "auditor_fn_violation": 0.0010077198538806318,
            "auditor_fp_violation": 0.007328345698619191,
            "ave_precision_score": 0.513710200822134,
            "fpr": 0.19319429198682767,
            "logloss": 0.6960937884860111,
            "mae": 0.500919790515261,
            "precision": 0.5416666666666666,
            "recall": 0.4262295081967213
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(44)",
        "seed": 10102,
        "test": {
            "accuracy": 0.48903508771929827,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.64810285090831,
            "mae": 0.5109649122807017,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4643249176728869,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.50156188882153,
            "mae": 0.535675082327113,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(45)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0.60528950803834,
            "auditor_fn_violation": 0.007044838491077499,
            "auditor_fp_violation": 0.0035107387302336563,
            "ave_precision_score": 0.5982996801503063,
            "fpr": 0.020833333333333332,
            "logloss": 0.8179083870522675,
            "mae": 0.4708068150896932,
            "precision": 0.6041666666666666,
            "recall": 0.06223175965665236
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.6607109911180021,
            "auditor_fn_violation": 0.006921325871407767,
            "auditor_fp_violation": 0.0012923215856630156,
            "ave_precision_score": 0.6440878471114504,
            "fpr": 0.014270032930845226,
            "logloss": 0.8314488655406822,
            "mae": 0.4741386554145797,
            "precision": 0.74,
            "recall": 0.07581967213114754
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(46)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5833333333333334,
            "auc_prc": 0.5705030863806768,
            "auditor_fn_violation": 0.00012235524433401403,
            "auditor_fp_violation": 0.01202206750059004,
            "ave_precision_score": 0.5722320892520455,
            "fpr": 0.35855263157894735,
            "logloss": 1.1077930783833752,
            "mae": 0.44403221471727367,
            "precision": 0.5581081081081081,
            "recall": 0.8862660944206009
        },
        "train": {
            "accuracy": 0.5850713501646543,
            "auc_prc": 0.5684408105793499,
            "auditor_fn_violation": 0.007144013964117977,
            "auditor_fp_violation": 0.009482215008057552,
            "ave_precision_score": 0.5700355554206089,
            "fpr": 0.34906695938529086,
            "logloss": 1.1418629888850846,
            "mae": 0.44171887307839336,
            "precision": 0.5737265415549598,
            "recall": 0.8770491803278688
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(47)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.811091323696524,
            "auditor_fn_violation": 0.008574279045252617,
            "auditor_fp_violation": 0.015390213201164352,
            "ave_precision_score": 0.8112479120128387,
            "fpr": 0.1600877192982456,
            "logloss": 0.7301870237794867,
            "mae": 0.30696024300465197,
            "precision": 0.7176015473887815,
            "recall": 0.796137339055794
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8416835277629405,
            "auditor_fn_violation": 0.010392110993143909,
            "auditor_fp_violation": 0.0195638803901877,
            "ave_precision_score": 0.841916210216183,
            "fpr": 0.13172338090010977,
            "logloss": 0.6783532660350262,
            "mae": 0.2914460244098868,
            "precision": 0.7674418604651163,
            "recall": 0.8114754098360656
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(48)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.7686958385982241,
            "auditor_fn_violation": 0.004755383630750699,
            "auditor_fp_violation": 0.009246420423255455,
            "ave_precision_score": 0.7221369169338656,
            "fpr": 0.21600877192982457,
            "logloss": 2.922331005598562,
            "mae": 0.32613090261604255,
            "precision": 0.6791530944625407,
            "recall": 0.8948497854077253
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.795463272536658,
            "auditor_fn_violation": 0.005434489211999064,
            "auditor_fp_violation": 0.01964173108811921,
            "ave_precision_score": 0.7489046367714465,
            "fpr": 0.19758507135016465,
            "logloss": 2.823028310573603,
            "mae": 0.3062295794973107,
            "precision": 0.7063621533442088,
            "recall": 0.8872950819672131
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(49)",
        "seed": 10102,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.7286880517425824,
            "auditor_fn_violation": 0.017661508922520896,
            "auditor_fp_violation": 0.01141235937377075,
            "ave_precision_score": 0.7290871426178509,
            "fpr": 0.14035087719298245,
            "logloss": 0.7187044388498024,
            "mae": 0.3841065168544127,
            "precision": 0.6952380952380952,
            "recall": 0.6266094420600858
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7696416965874859,
            "auditor_fn_violation": 0.006219520973169466,
            "auditor_fp_violation": 0.013027016787205498,
            "ave_precision_score": 0.7696810189547127,
            "fpr": 0.11745334796926454,
            "logloss": 0.6694192558938689,
            "mae": 0.3699976484547855,
            "precision": 0.7440191387559809,
            "recall": 0.6372950819672131
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(50)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7554824561403508,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5109649122807017,
            "fpr": 0.48903508771929827,
            "logloss": 0.6953914476976136,
            "mae": 0.4989885144066392,
            "precision": 0.5109649122807017,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.535675082327113,
            "auc_prc": 0.7678375411635565,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.535675082327113,
            "fpr": 0.4643249176728869,
            "logloss": 0.6908195476562418,
            "mae": 0.49670906333839593,
            "precision": 0.535675082327113,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(51)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5372807017543859,
            "auc_prc": 0.7167355117303598,
            "auditor_fn_violation": 0.0029647616896318054,
            "auditor_fp_violation": 0.006269176303988687,
            "ave_precision_score": 0.6856996240590795,
            "fpr": 0.4506578947368421,
            "logloss": 0.7618367784596103,
            "mae": 0.42649592392211944,
            "precision": 0.5254041570438799,
            "recall": 0.9763948497854077
        },
        "train": {
            "accuracy": 0.5762897914379802,
            "auc_prc": 0.74469447294479,
            "auditor_fn_violation": 0.0007737848878011913,
            "auditor_fp_violation": 0.0060931146247726236,
            "ave_precision_score": 0.719114845063409,
            "fpr": 0.41712403951701427,
            "logloss": 0.7097102185302341,
            "mae": 0.4067526560452584,
            "precision": 0.5591647331786543,
            "recall": 0.9877049180327869
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(52)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8065074917135127,
            "auditor_fn_violation": 0.014776748738799795,
            "auditor_fp_violation": 0.023709778931634024,
            "ave_precision_score": 0.807861529533467,
            "fpr": 0.15899122807017543,
            "logloss": 0.5892182228827799,
            "mae": 0.31085676274350943,
            "precision": 0.7117296222664016,
            "recall": 0.7682403433476395
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8339145893943158,
            "auditor_fn_violation": 0.011485307084630482,
            "auditor_fp_violation": 0.022262704585146606,
            "ave_precision_score": 0.8342333522474823,
            "fpr": 0.1437980241492865,
            "logloss": 0.5692497578429188,
            "mae": 0.2989977484355834,
            "precision": 0.7385229540918163,
            "recall": 0.7581967213114754
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(53)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6162280701754386,
            "auc_prc": 0.8131121944161062,
            "auditor_fn_violation": 0.00432949326104962,
            "auditor_fp_violation": 0.011712296436157659,
            "ave_precision_score": 0.8135460350868156,
            "fpr": 0.36293859649122806,
            "logloss": 0.6726146683452335,
            "mae": 0.38785931620319997,
            "precision": 0.5745501285347043,
            "recall": 0.9592274678111588
        },
        "train": {
            "accuracy": 0.6410537870472008,
            "auc_prc": 0.8349236915650073,
            "auditor_fn_violation": 0.007949290097352936,
            "auditor_fp_violation": 0.026438097017539765,
            "ave_precision_score": 0.8351789678874839,
            "fpr": 0.3358946212952799,
            "logloss": 0.6207244127825667,
            "mae": 0.3712320277655395,
            "precision": 0.6041397153945667,
            "recall": 0.9569672131147541
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(54)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.6812542647189079,
            "auditor_fn_violation": 0.00033177095098260837,
            "auditor_fp_violation": 0.004009814334041386,
            "ave_precision_score": 0.6689601866119536,
            "fpr": 0.12609649122807018,
            "logloss": 0.5867700447576648,
            "mae": 0.3996554480767564,
            "precision": 0.7444444444444445,
            "recall": 0.7188841201716738
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.6951044633452894,
            "auditor_fn_violation": 0.006653650285220711,
            "auditor_fp_violation": 0.006822316162064393,
            "ave_precision_score": 0.6872267797513283,
            "fpr": 0.12403951701427003,
            "logloss": 0.5872829029668479,
            "mae": 0.3995679545284757,
            "precision": 0.755939524838013,
            "recall": 0.7172131147540983
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(55)",
        "seed": 10102,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8452968709012924,
            "auditor_fn_violation": 0.016292071380167156,
            "auditor_fp_violation": 0.014790339076390527,
            "ave_precision_score": 0.8445484972268797,
            "fpr": 0.15350877192982457,
            "logloss": 0.4991706850292439,
            "mae": 0.3306606368553874,
            "precision": 0.7265625,
            "recall": 0.7982832618025751
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8215404005087017,
            "auditor_fn_violation": 0.014454481654100161,
            "auditor_fp_violation": 0.011023658827101388,
            "ave_precision_score": 0.8213253046238557,
            "fpr": 0.12843029637760703,
            "logloss": 0.5031836558840339,
            "mae": 0.32756497352726505,
            "precision": 0.7692307692307693,
            "recall": 0.7991803278688525
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(56)",
        "seed": 10102,
        "test": {
            "accuracy": 0.48903508771929827,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 4.840884352046953,
            "mae": 0.5111449875670234,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4643249176728869,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 5.050272894550675,
            "mae": 0.5363133953517057,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(57)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.7073204161981153,
            "auditor_fn_violation": 0.013016715608764402,
            "auditor_fp_violation": 0.016019589332074584,
            "ave_precision_score": 0.7079523283374269,
            "fpr": 0.13596491228070176,
            "logloss": 0.6335896569334478,
            "mae": 0.38994046008794386,
            "precision": 0.7026378896882494,
            "recall": 0.628755364806867
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7359519407411392,
            "auditor_fn_violation": 0.0128079393928488,
            "auditor_fp_violation": 0.02231201002716989,
            "ave_precision_score": 0.7368121321474416,
            "fpr": 0.11525795828759605,
            "logloss": 0.6286284514547129,
            "mae": 0.3840710323714906,
            "precision": 0.742014742014742,
            "recall": 0.6188524590163934
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(58)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.702601169194534,
            "auditor_fn_violation": 0.001924742112792714,
            "auditor_fp_violation": 0.006347848320352451,
            "ave_precision_score": 0.6273171220786602,
            "fpr": 0.05592105263157895,
            "logloss": 0.8614757293506297,
            "mae": 0.4672248454053739,
            "precision": 0.7883817427385892,
            "recall": 0.40772532188841204
        },
        "train": {
            "accuracy": 0.610318331503842,
            "auc_prc": 0.504939241726607,
            "auditor_fn_violation": 0.006235266595886343,
            "auditor_fp_violation": 0.012020147760624674,
            "ave_precision_score": 0.6314686879866737,
            "fpr": 0.06256860592755215,
            "logloss": 0.9095480810026604,
            "mae": 0.4830475617302367,
            "precision": 0.7692307692307693,
            "recall": 0.38934426229508196
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(59)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.866089271371463,
            "auditor_fn_violation": 0.0044636134327234395,
            "auditor_fp_violation": 0.002532255526709153,
            "ave_precision_score": 0.8598626715168056,
            "fpr": 0.06469298245614036,
            "logloss": 0.4728558187841859,
            "mae": 0.2950583818089255,
            "precision": 0.8455497382198953,
            "recall": 0.6931330472103004
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8614484849002686,
            "auditor_fn_violation": 0.005729157294272198,
            "auditor_fp_violation": 0.012775299530560297,
            "ave_precision_score": 0.8554990990976171,
            "fpr": 0.08562019758507135,
            "logloss": 0.4905569822876539,
            "mae": 0.29869156686111065,
            "precision": 0.8181818181818182,
            "recall": 0.7192622950819673
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(60)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5789473684210527,
            "auc_prc": 0.7094466711977807,
            "auditor_fn_violation": 0.09800184474060689,
            "auditor_fp_violation": 0.09400322555267092,
            "ave_precision_score": 0.5597347740283389,
            "fpr": 0.26535087719298245,
            "logloss": 0.6787567990817724,
            "mae": 0.48615356998746856,
            "precision": 0.5724381625441696,
            "recall": 0.6952789699570815
        },
        "train": {
            "accuracy": 0.5499451152579583,
            "auc_prc": 0.7031522987132452,
            "auditor_fn_violation": 0.11187040002879199,
            "auditor_fp_violation": 0.09324697095909466,
            "ave_precision_score": 0.5632487850924981,
            "fpr": 0.2678375411635565,
            "logloss": 0.6886201814333407,
            "mae": 0.4909250787270187,
            "precision": 0.568904593639576,
            "recall": 0.6598360655737705
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(61)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.827669216301881,
            "auditor_fn_violation": 0.004920092613508024,
            "auditor_fp_violation": 0.003331268192903787,
            "ave_precision_score": 0.810183492279506,
            "fpr": 0.11513157894736842,
            "logloss": 0.5100924041216797,
            "mae": 0.34055630444434654,
            "precision": 0.7697368421052632,
            "recall": 0.7532188841201717
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8419838270994845,
            "auditor_fn_violation": 0.003630490723578849,
            "auditor_fp_violation": 0.0015414438190438412,
            "ave_precision_score": 0.814664619090563,
            "fpr": 0.11855104281009879,
            "logloss": 0.5092944557753126,
            "mae": 0.33727961414699104,
            "precision": 0.7735849056603774,
            "recall": 0.7561475409836066
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(62)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5855263157894737,
            "auc_prc": 0.5398538505856254,
            "auditor_fn_violation": 0.10701848505383632,
            "auditor_fp_violation": 0.08744886318936354,
            "ave_precision_score": 0.5412794087815349,
            "fpr": 0.20614035087719298,
            "logloss": 0.7310136109917531,
            "mae": 0.47684606893609954,
            "precision": 0.5948275862068966,
            "recall": 0.592274678111588
        },
        "train": {
            "accuracy": 0.5675082327113062,
            "auc_prc": 0.551655891534672,
            "auditor_fn_violation": 0.10780802936783575,
            "auditor_fp_violation": 0.08139030966412615,
            "ave_precision_score": 0.5537009173301362,
            "fpr": 0.19978046103183314,
            "logloss": 0.7561269208969243,
            "mae": 0.4913073841243085,
            "precision": 0.6026200873362445,
            "recall": 0.5655737704918032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(63)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5153508771929824,
            "auc_prc": 0.5871668443934336,
            "auditor_fn_violation": 0.00933194036593631,
            "auditor_fp_violation": 0.0037025017701203707,
            "ave_precision_score": 0.5309159267346781,
            "fpr": 0.08223684210526316,
            "logloss": 0.7201181246863468,
            "mae": 0.49435797157301686,
            "precision": 0.5689655172413793,
            "recall": 0.21244635193133046
        },
        "train": {
            "accuracy": 0.4884742041712404,
            "auc_prc": 0.5968106194412052,
            "auditor_fn_violation": 0.00864659624624357,
            "auditor_fp_violation": 0.009230497751412346,
            "ave_precision_score": 0.5493268728610576,
            "fpr": 0.07793633369923161,
            "logloss": 0.7075312972763381,
            "mae": 0.4955041822124292,
            "precision": 0.5670731707317073,
            "recall": 0.19057377049180327
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(64)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5164473684210527,
            "auc_prc": 0.5198236080107576,
            "auditor_fn_violation": 0.0061271741585723965,
            "auditor_fp_violation": 0.0028813625993234313,
            "ave_precision_score": 0.5213573885303203,
            "fpr": 0.4692982456140351,
            "logloss": 0.6999125633923748,
            "mae": 0.4988091127587515,
            "precision": 0.5141884222474461,
            "recall": 0.9721030042918455
        },
        "train": {
            "accuracy": 0.5466520307354555,
            "auc_prc": 0.5491558747088854,
            "auditor_fn_violation": 0.0020559284518903743,
            "auditor_fp_violation": 0.004445274851889064,
            "ave_precision_score": 0.5507081481948687,
            "fpr": 0.4434687156970362,
            "logloss": 0.6929977151543382,
            "mae": 0.49501453855296784,
            "precision": 0.5424688561721405,
            "recall": 0.9815573770491803
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(65)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.79410568906137,
            "auditor_fn_violation": 0.008569573074316696,
            "auditor_fp_violation": 0.009661907009676663,
            "ave_precision_score": 0.7831582907620976,
            "fpr": 0.13925438596491227,
            "logloss": 0.5606372096231375,
            "mae": 0.369352715580087,
            "precision": 0.7292110874200426,
            "recall": 0.7339055793991416
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7985362447719209,
            "auditor_fn_violation": 0.01192168577135557,
            "auditor_fp_violation": 0.009466644868471255,
            "ave_precision_score": 0.786772538152616,
            "fpr": 0.13391877058177826,
            "logloss": 0.5598458729940542,
            "mae": 0.37111404957300875,
            "precision": 0.7431578947368421,
            "recall": 0.7233606557377049
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(66)",
        "seed": 10102,
        "test": {
            "accuracy": 0.631578947368421,
            "auc_prc": 0.6292363319227917,
            "auditor_fn_violation": 0.0858086740456291,
            "auditor_fp_violation": 0.06758417905750926,
            "ave_precision_score": 0.6132342133716905,
            "fpr": 0.20614035087719298,
            "logloss": 0.6474700382864471,
            "mae": 0.45313819535135336,
            "precision": 0.6284584980237155,
            "recall": 0.6824034334763949
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.640728312702828,
            "auditor_fn_violation": 0.0942398013352288,
            "auditor_fp_violation": 0.07174201316714805,
            "ave_precision_score": 0.6287961593440435,
            "fpr": 0.18331503841931943,
            "logloss": 0.6419892164332638,
            "mae": 0.4482617215339031,
            "precision": 0.6506276150627615,
            "recall": 0.6372950819672131
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(67)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.8017447288053566,
            "auditor_fn_violation": 0.001750621188163542,
            "auditor_fp_violation": 0.016585044449689276,
            "ave_precision_score": 0.8021151996569319,
            "fpr": 0.4418859649122807,
            "logloss": 1.3733102346007613,
            "mae": 0.431667267015986,
            "precision": 0.5346420323325635,
            "recall": 0.9935622317596566
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.8412153605596799,
            "auditor_fn_violation": 0.001916467222112253,
            "auditor_fp_violation": 0.013320254416080854,
            "ave_precision_score": 0.8414689821264947,
            "fpr": 0.4083424807903403,
            "logloss": 1.2367254417431672,
            "mae": 0.3992191892703795,
            "precision": 0.5649122807017544,
            "recall": 0.9897540983606558
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(68)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.6660230115885432,
            "auditor_fn_violation": 0.020583916873729395,
            "auditor_fp_violation": 0.012548186610022822,
            "ave_precision_score": 0.6505261789137232,
            "fpr": 0.14364035087719298,
            "logloss": 0.6198945220515059,
            "mae": 0.41903259497331946,
            "precision": 0.6765432098765433,
            "recall": 0.5879828326180258
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7265988771649077,
            "auditor_fn_violation": 0.020910186968022892,
            "auditor_fp_violation": 0.02230681998064113,
            "ave_precision_score": 0.6998451155001412,
            "fpr": 0.11086717892425905,
            "logloss": 0.6016668439619762,
            "mae": 0.40860700689084445,
            "precision": 0.749379652605459,
            "recall": 0.6188524590163934
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(69)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.7539632144167171,
            "auditor_fn_violation": 0.009482531435885849,
            "auditor_fp_violation": 0.021531547478561886,
            "ave_precision_score": 0.7152230694463364,
            "fpr": 0.20175438596491227,
            "logloss": 2.463756866023128,
            "mae": 0.3661401695326755,
            "precision": 0.6617647058823529,
            "recall": 0.7725321888412017
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7806602463012446,
            "auditor_fn_violation": 0.010531572222922026,
            "auditor_fp_violation": 0.0291161610263836,
            "ave_precision_score": 0.7434893260628441,
            "fpr": 0.17453347969264543,
            "logloss": 2.367494947371654,
            "mae": 0.3445095822100582,
            "precision": 0.7039106145251397,
            "recall": 0.7745901639344263
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(70)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.7805857946040373,
            "auditor_fn_violation": 0.0018918003162412512,
            "auditor_fp_violation": 0.007810656124616477,
            "ave_precision_score": 0.708177271615636,
            "fpr": 0.08881578947368421,
            "logloss": 0.6132768816695374,
            "mae": 0.38303862252153,
            "precision": 0.8033980582524272,
            "recall": 0.7103004291845494
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.7717316066418688,
            "auditor_fn_violation": 0.0027982220942577955,
            "auditor_fp_violation": 0.009412149379919195,
            "ave_precision_score": 0.7073482731246263,
            "fpr": 0.09549945115257959,
            "logloss": 0.6362737633259214,
            "mae": 0.39573540035065946,
            "precision": 0.8,
            "recall": 0.7131147540983607
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(71)",
        "seed": 10102,
        "test": {
            "accuracy": 0.4155701754385965,
            "auc_prc": 0.4501749898782977,
            "auditor_fn_violation": 0.006317765981477312,
            "auditor_fp_violation": 0.021654472504130287,
            "ave_precision_score": 0.482795969137996,
            "fpr": 0.09649122807017543,
            "logloss": 3.259290889029567,
            "mae": 0.5531059477522369,
            "precision": 0.1926605504587156,
            "recall": 0.045064377682403435
        },
        "train": {
            "accuracy": 0.3951701427003293,
            "auc_prc": 0.48060223111816736,
            "auditor_fn_violation": 0.006363480952295271,
            "auditor_fp_violation": 0.02121691020960003,
            "ave_precision_score": 0.5082576421320701,
            "fpr": 0.10098792535675083,
            "logloss": 3.356444594534187,
            "mae": 0.5768109401223149,
            "precision": 0.2396694214876033,
            "recall": 0.05942622950819672
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(72)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8403202606191429,
            "auditor_fn_violation": 0.006976601912506588,
            "auditor_fp_violation": 0.008658838801038468,
            "ave_precision_score": 0.8407145405694638,
            "fpr": 0.11732456140350878,
            "logloss": 0.49822852181627875,
            "mae": 0.32935823270185083,
            "precision": 0.7723404255319148,
            "recall": 0.778969957081545
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8575115962031407,
            "auditor_fn_violation": 0.006383725324359831,
            "auditor_fp_violation": 0.014698211769468521,
            "ave_precision_score": 0.8577034012405301,
            "fpr": 0.1207464324917673,
            "logloss": 0.49714547506279744,
            "mae": 0.3292114752274248,
            "precision": 0.7745901639344263,
            "recall": 0.7745901639344263
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(73)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.654127582283028,
            "auditor_fn_violation": 0.001844740606882012,
            "auditor_fp_violation": 0.0013079222720478432,
            "ave_precision_score": 0.5404328082276353,
            "fpr": 0.4692982456140351,
            "logloss": 0.6856118405725865,
            "mae": 0.4849666767639288,
            "precision": 0.5191011235955056,
            "recall": 0.9914163090128756
        },
        "train": {
            "accuracy": 0.5543358946212953,
            "auc_prc": 0.7430313407589496,
            "auditor_fn_violation": 0.0005758399165032122,
            "auditor_fp_violation": 0.003176308475605501,
            "ave_precision_score": 0.5882047721372959,
            "fpr": 0.4445664105378705,
            "logloss": 0.6632781417377194,
            "mae": 0.47501214241974965,
            "precision": 0.5459641255605381,
            "recall": 0.9979508196721312
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(74)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8088885455126962,
            "auditor_fn_violation": 0.011934342293501996,
            "auditor_fp_violation": 0.017858547714577927,
            "ave_precision_score": 0.8097716868832923,
            "fpr": 0.1699561403508772,
            "logloss": 0.8464648031451191,
            "mae": 0.2870131316679456,
            "precision": 0.7036328871892925,
            "recall": 0.7896995708154506
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8454856005964798,
            "auditor_fn_violation": 0.010063702290763176,
            "auditor_fp_violation": 0.02315798761135894,
            "ave_precision_score": 0.8456804825495068,
            "fpr": 0.14709110867178923,
            "logloss": 0.8143343343905727,
            "mae": 0.2709117633136747,
            "precision": 0.7403100775193798,
            "recall": 0.7827868852459017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(75)",
        "seed": 10102,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.6810842216903532,
            "auditor_fn_violation": 0.00748719975905429,
            "auditor_fp_violation": 0.002679765557391238,
            "ave_precision_score": 0.7120231837311884,
            "fpr": 0.06907894736842106,
            "logloss": 0.57013573376065,
            "mae": 0.36605381848686186,
            "precision": 0.8297297297297297,
            "recall": 0.6587982832618026
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.7256003656357168,
            "auditor_fn_violation": 0.008601608752766741,
            "auditor_fp_violation": 0.010533199430132895,
            "ave_precision_score": 0.7391482155404676,
            "fpr": 0.07903402854006586,
            "logloss": 0.5531184731389016,
            "mae": 0.36159294538089715,
            "precision": 0.823960880195599,
            "recall": 0.6905737704918032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(76)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5043859649122807,
            "auc_prc": 0.491946902299312,
            "auditor_fn_violation": 0.005562457646261585,
            "auditor_fp_violation": 0.015596727244119276,
            "ave_precision_score": 0.5111914078643094,
            "fpr": 0.2675438596491228,
            "logloss": 0.6943781221750606,
            "mae": 0.49993890380127387,
            "precision": 0.5139442231075697,
            "recall": 0.5536480686695279
        },
        "train": {
            "accuracy": 0.5137211855104281,
            "auc_prc": 0.5243630967819676,
            "auditor_fn_violation": 0.0011561785823541056,
            "auditor_fp_violation": 0.012705233902421937,
            "ave_precision_score": 0.5482135705619723,
            "fpr": 0.23819978046103182,
            "logloss": 0.6928240697339881,
            "mae": 0.49914217387805526,
            "precision": 0.5469728601252609,
            "recall": 0.5368852459016393
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(77)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.6452318470331873,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5169706442937785,
            "fpr": 0.48903508771929827,
            "logloss": 0.695967470637331,
            "mae": 0.4988284558057785,
            "precision": 0.5109649122807017,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.535675082327113,
            "auc_prc": 0.6645299105228877,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5435554482808422,
            "fpr": 0.4643249176728869,
            "logloss": 0.6909039815839936,
            "mae": 0.496308744149988,
            "precision": 0.535675082327113,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(78)",
        "seed": 10102,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.6077172798138037,
            "auditor_fn_violation": 0.0069671899706347466,
            "auditor_fp_violation": 0.037526551805522786,
            "ave_precision_score": 0.49614419572881463,
            "fpr": 0.2565789473684211,
            "logloss": 0.7189701381070434,
            "mae": 0.5052485819905996,
            "precision": 0.47884187082405344,
            "recall": 0.4613733905579399
        },
        "train": {
            "accuracy": 0.47859495060373214,
            "auc_prc": 0.6318751800792483,
            "auditor_fn_violation": 0.018012992388116125,
            "auditor_fp_violation": 0.02394427966046716,
            "ave_precision_score": 0.5261585003506956,
            "fpr": 0.23161361141602635,
            "logloss": 0.7225429647929666,
            "mae": 0.5067807351587632,
            "precision": 0.5149425287356322,
            "recall": 0.45901639344262296
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(79)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5175438596491229,
            "auc_prc": 0.6251126843834629,
            "auditor_fn_violation": 0.0004729500790603119,
            "auditor_fp_violation": 0.0010178192117063915,
            "ave_precision_score": 0.6257341814131003,
            "fpr": 0.4550438596491228,
            "logloss": 0.6865510349365844,
            "mae": 0.49533495800406263,
            "precision": 0.5151869158878505,
            "recall": 0.9463519313304721
        },
        "train": {
            "accuracy": 0.5389681668496158,
            "auc_prc": 0.6642928997668875,
            "auditor_fn_violation": 0.0023483471594896617,
            "auditor_fp_violation": 0.003158143312754789,
            "ave_precision_score": 0.6646884992284499,
            "fpr": 0.43029637760702527,
            "logloss": 0.6845936619950127,
            "mae": 0.49451644697382996,
            "precision": 0.539906103286385,
            "recall": 0.9426229508196722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(80)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8492499391763947,
            "auditor_fn_violation": 0.005120096378284776,
            "auditor_fp_violation": 0.0016939068523326279,
            "ave_precision_score": 0.7701980205378092,
            "fpr": 0.0712719298245614,
            "logloss": 0.5172900591723504,
            "mae": 0.34401670065626766,
            "precision": 0.8266666666666667,
            "recall": 0.6652360515021459
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8467968049388463,
            "auditor_fn_violation": 0.006959565240863043,
            "auditor_fp_violation": 0.009054036169434256,
            "ave_precision_score": 0.7646513780298161,
            "fpr": 0.08232711306256861,
            "logloss": 0.533663989401162,
            "mae": 0.3463216184686489,
            "precision": 0.8188405797101449,
            "recall": 0.694672131147541
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(81)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7554824561403508,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5109649122807017,
            "fpr": 0.48903508771929827,
            "logloss": 0.6953914476976136,
            "mae": 0.4989885144066392,
            "precision": 0.5109649122807017,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.535675082327113,
            "auc_prc": 0.7678375411635565,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.535675082327113,
            "fpr": 0.4643249176728869,
            "logloss": 0.6908195476562418,
            "mae": 0.49670906333839593,
            "precision": 0.535675082327113,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(82)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7839912280701754,
            "auc_prc": 0.7693957708448651,
            "auditor_fn_violation": 0.004145960394548609,
            "auditor_fp_violation": 0.009106285894107466,
            "ave_precision_score": 0.800083234124941,
            "fpr": 0.07017543859649122,
            "logloss": 0.5101291287275369,
            "mae": 0.33021047841675844,
            "precision": 0.8387909319899244,
            "recall": 0.7145922746781116
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8132798957420585,
            "auditor_fn_violation": 0.006633405913156147,
            "auditor_fp_violation": 0.01040085324364933,
            "ave_precision_score": 0.8039805826470514,
            "fpr": 0.08232711306256861,
            "logloss": 0.520157170472425,
            "mae": 0.32952947979210234,
            "precision": 0.823943661971831,
            "recall": 0.7192622950819673
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(83)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5131578947368421,
            "auc_prc": 0.7555343623545768,
            "auditor_fn_violation": 0.0006282471199457873,
            "auditor_fp_violation": 0.0006957556447171843,
            "ave_precision_score": 0.5121156892210746,
            "fpr": 0.4857456140350877,
            "logloss": 16.78074505747896,
            "mae": 0.4868204070914081,
            "precision": 0.512114537444934,
            "recall": 0.9978540772532188
        },
        "train": {
            "accuracy": 0.5455543358946213,
            "auc_prc": 0.770509977827051,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.002545717822360287,
            "ave_precision_score": 0.541019955654102,
            "fpr": 0.4544456641053787,
            "logloss": 15.69724584394952,
            "mae": 0.45521087368437047,
            "precision": 0.541019955654102,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(84)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7410123774104884,
            "auditor_fn_violation": 0.018146223928921023,
            "auditor_fp_violation": 0.00705589646762647,
            "ave_precision_score": 0.7515736722741063,
            "fpr": 0.08771929824561403,
            "logloss": 0.5640837563852494,
            "mae": 0.363245732359294,
            "precision": 0.7720797720797721,
            "recall": 0.5815450643776824
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7823984985054924,
            "auditor_fn_violation": 0.014204801065303852,
            "auditor_fp_violation": 0.016535488240652076,
            "ave_precision_score": 0.7828804666283655,
            "fpr": 0.07903402854006586,
            "logloss": 0.5511417456682036,
            "mae": 0.35511060007955186,
            "precision": 0.8038147138964578,
            "recall": 0.6045081967213115
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(85)",
        "seed": 10102,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.7712022561838374,
            "auditor_fn_violation": 0.009950775544010245,
            "auditor_fp_violation": 0.0059594052395562916,
            "ave_precision_score": 0.8134492912489223,
            "fpr": 0.13596491228070176,
            "logloss": 0.5231033598074876,
            "mae": 0.3486560096947901,
            "precision": 0.7356076759061834,
            "recall": 0.740343347639485
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.7444783217241387,
            "auditor_fn_violation": 0.008115743823217145,
            "auditor_fp_violation": 0.010333382638775361,
            "ave_precision_score": 0.800451515725652,
            "fpr": 0.12952799121844127,
            "logloss": 0.5361093015042867,
            "mae": 0.35150356724874643,
            "precision": 0.7601626016260162,
            "recall": 0.7663934426229508
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(86)",
        "seed": 10102,
        "test": {
            "accuracy": 0.49780701754385964,
            "auc_prc": 0.5243994129328882,
            "auditor_fn_violation": 0.0036306565770649725,
            "auditor_fp_violation": 0.0036533317598930063,
            "ave_precision_score": 0.5275023953380079,
            "fpr": 0.047149122807017545,
            "logloss": 9.960013615098887,
            "mae": 0.4994365732573322,
            "precision": 0.5425531914893617,
            "recall": 0.10944206008583691
        },
        "train": {
            "accuracy": 0.46871569703622395,
            "auc_prc": 0.5241165122761686,
            "auditor_fn_violation": 0.003907163808461251,
            "auditor_fp_violation": 0.0004878643737041117,
            "ave_precision_score": 0.5266283833741378,
            "fpr": 0.05159165751920966,
            "logloss": 10.297851043537976,
            "mae": 0.5335814041215473,
            "precision": 0.5204081632653061,
            "recall": 0.10450819672131148
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(87)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5153508771929824,
            "auc_prc": 0.7739130416280303,
            "auditor_fn_violation": 0.0032141781492357603,
            "auditor_fp_violation": 0.00037615057823932037,
            "ave_precision_score": 0.7313364837087429,
            "fpr": 0.005482456140350877,
            "logloss": 0.9199128312191532,
            "mae": 0.42735373031086565,
            "precision": 0.8529411764705882,
            "recall": 0.06223175965665236
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.7732363701558763,
            "auditor_fn_violation": 0.0033808101347825442,
            "auditor_fp_violation": 0.002172034472289044,
            "ave_precision_score": 0.7422596514980997,
            "fpr": 0.005488474204171241,
            "logloss": 0.9247608391016868,
            "mae": 0.4447405851485962,
            "precision": 0.8076923076923077,
            "recall": 0.0430327868852459
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(88)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5131578947368421,
            "auc_prc": 0.842941302363381,
            "auditor_fn_violation": 0.0006282471199457873,
            "auditor_fp_violation": 0.0006957556447171843,
            "ave_precision_score": 0.8434349903555856,
            "fpr": 0.4857456140350877,
            "logloss": 3.809271815761,
            "mae": 0.4865842739922671,
            "precision": 0.512114537444934,
            "recall": 0.9978540772532188
        },
        "train": {
            "accuracy": 0.5455543358946213,
            "auc_prc": 0.8662149116872901,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.002545717822360287,
            "ave_precision_score": 0.8663391912875541,
            "fpr": 0.4544456641053787,
            "logloss": 3.5668488631202746,
            "mae": 0.45423066550802454,
            "precision": 0.541019955654102,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(89)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6567982456140351,
            "auc_prc": 0.7538366254878497,
            "auditor_fn_violation": 0.009948422558542282,
            "auditor_fp_violation": 0.011736881441271343,
            "ave_precision_score": 0.607149369570818,
            "fpr": 0.20723684210526316,
            "logloss": 1.0058437057966167,
            "mae": 0.4555643119128827,
            "precision": 0.6440677966101694,
            "recall": 0.7339055793991416
        },
        "train": {
            "accuracy": 0.6487376509330406,
            "auc_prc": 0.7627849776206227,
            "auditor_fn_violation": 0.012425545698295877,
            "auditor_fp_violation": 0.004676231922419191,
            "ave_precision_score": 0.619677348818976,
            "fpr": 0.21295279912184412,
            "logloss": 1.0094799505959666,
            "mae": 0.45880037983933236,
            "precision": 0.6510791366906474,
            "recall": 0.7418032786885246
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(90)",
        "seed": 10102,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.858016874908763,
            "auditor_fn_violation": 0.013139070853098412,
            "auditor_fp_violation": 0.00935951144677838,
            "ave_precision_score": 0.8466933729109916,
            "fpr": 0.10416666666666667,
            "logloss": 0.5113331049570622,
            "mae": 0.3305207584527764,
            "precision": 0.7865168539325843,
            "recall": 0.7510729613733905
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8615081285571277,
            "auditor_fn_violation": 0.009316909899048067,
            "auditor_fp_violation": 0.005677910902471242,
            "ave_precision_score": 0.8485537101406797,
            "fpr": 0.1141602634467618,
            "logloss": 0.49650945976288957,
            "mae": 0.32603751518201457,
            "precision": 0.7819706498951782,
            "recall": 0.764344262295082
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(91)",
        "seed": 10102,
        "test": {
            "accuracy": 0.4144736842105263,
            "auc_prc": 0.4361439779707592,
            "auditor_fn_violation": 0.007903678186883513,
            "auditor_fp_violation": 0.02487756667453387,
            "ave_precision_score": 0.4653904196870876,
            "fpr": 0.11074561403508772,
            "logloss": 0.7803255299377887,
            "mae": 0.5243596843721574,
            "precision": 0.2462686567164179,
            "recall": 0.07081545064377683
        },
        "train": {
            "accuracy": 0.3973655323819978,
            "auc_prc": 0.47242211257982153,
            "auditor_fn_violation": 0.006982058987601452,
            "auditor_fp_violation": 0.022421001004274,
            "ave_precision_score": 0.4960788719950213,
            "fpr": 0.11086717892425905,
            "logloss": 0.7890567377143252,
            "mae": 0.5278795751140879,
            "precision": 0.28368794326241137,
            "recall": 0.08196721311475409
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(92)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.6672314425813641,
            "auditor_fn_violation": 0.011063737670356155,
            "auditor_fp_violation": 0.020174455196286682,
            "ave_precision_score": 0.6410500782686857,
            "fpr": 0.1787280701754386,
            "logloss": 0.6414881350034212,
            "mae": 0.4262309799526344,
            "precision": 0.6693711967545639,
            "recall": 0.7081545064377682
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7544920278180519,
            "auditor_fn_violation": 0.017803800543448923,
            "auditor_fp_violation": 0.026259040412297298,
            "ave_precision_score": 0.687415638300226,
            "fpr": 0.15697036223929747,
            "logloss": 0.6221667938218591,
            "mae": 0.4176864078192758,
            "precision": 0.7105263157894737,
            "recall": 0.7192622950819673
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(93)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8382214064842577,
            "auditor_fn_violation": 0.017553271590994655,
            "auditor_fp_violation": 0.010178192117063962,
            "ave_precision_score": 0.8375232284561029,
            "fpr": 0.08333333333333333,
            "logloss": 1.8703284484272806,
            "mae": 0.30136486850953365,
            "precision": 0.8071065989847716,
            "recall": 0.6824034334763949
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8628414477755072,
            "auditor_fn_violation": 0.01677808569217758,
            "auditor_fp_violation": 0.018069146989902765,
            "ave_precision_score": 0.8612934070756153,
            "fpr": 0.08562019758507135,
            "logloss": 2.0509544123126426,
            "mae": 0.29753618698512885,
            "precision": 0.8138424821002387,
            "recall": 0.6987704918032787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(94)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8485211239938796,
            "auditor_fn_violation": 0.01272023943980122,
            "auditor_fp_violation": 0.013713515852411297,
            "ave_precision_score": 0.8488918830109904,
            "fpr": 0.13925438596491227,
            "logloss": 0.4989470394562342,
            "mae": 0.3283422531707114,
            "precision": 0.75,
            "recall": 0.8175965665236051
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8679238864334944,
            "auditor_fn_violation": 0.01373918050781883,
            "auditor_fp_violation": 0.01730880517343839,
            "ave_precision_score": 0.8680988868767979,
            "fpr": 0.13611416026344675,
            "logloss": 0.48684466159661927,
            "mae": 0.3211290947908922,
            "precision": 0.7651515151515151,
            "recall": 0.8278688524590164
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(95)",
        "seed": 10102,
        "test": {
            "accuracy": 0.48903508771929827,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.64810285090831,
            "mae": 0.5109649122807017,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4643249176728869,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.50156188882153,
            "mae": 0.535675082327113,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(96)",
        "seed": 10102,
        "test": {
            "accuracy": 0.4725877192982456,
            "auc_prc": 0.496511889964388,
            "auditor_fn_violation": 0.01384496649348694,
            "auditor_fp_violation": 0.03157698056801196,
            "ave_precision_score": 0.49424407452330604,
            "fpr": 0.3432017543859649,
            "logloss": 0.7220193496723478,
            "mae": 0.5045121932369575,
            "precision": 0.4877250409165303,
            "recall": 0.6394849785407726
        },
        "train": {
            "accuracy": 0.5082327113062569,
            "auc_prc": 0.5234793487447159,
            "auditor_fn_violation": 0.013199330586097065,
            "auditor_fp_violation": 0.03246114601417402,
            "ave_precision_score": 0.5220272892918401,
            "fpr": 0.3238199780461032,
            "logloss": 0.7117180079823132,
            "mae": 0.49974735377256224,
            "precision": 0.5317460317460317,
            "recall": 0.6864754098360656
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(97)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8282252440492102,
            "auditor_fn_violation": 0.0006541299600933784,
            "auditor_fp_violation": 0.0028764455983006838,
            "ave_precision_score": 0.8194727194314262,
            "fpr": 0.07675438596491228,
            "logloss": 0.6324024757454337,
            "mae": 0.33206709653462496,
            "precision": 0.8097826086956522,
            "recall": 0.6394849785407726
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8357473320092315,
            "auditor_fn_violation": 0.010848734051933564,
            "auditor_fp_violation": 0.004748892573821928,
            "ave_precision_score": 0.829569967310859,
            "fpr": 0.07244785949506037,
            "logloss": 0.6627391041739241,
            "mae": 0.33294763054031534,
            "precision": 0.8312020460358056,
            "recall": 0.6659836065573771
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(98)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8355954377473808,
            "auditor_fn_violation": 0.008091917024320465,
            "auditor_fp_violation": 0.008501494768310917,
            "ave_precision_score": 0.8361053217919863,
            "fpr": 0.07785087719298246,
            "logloss": 0.5097531304943651,
            "mae": 0.34817330297409443,
            "precision": 0.821608040201005,
            "recall": 0.7017167381974249
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8406086982496046,
            "auditor_fn_violation": 0.001450846664627236,
            "auditor_fp_violation": 0.014994044421608241,
            "ave_precision_score": 0.8409452334067851,
            "fpr": 0.0867178924259056,
            "logloss": 0.5089036287755452,
            "mae": 0.3442291033778311,
            "precision": 0.8145539906103286,
            "recall": 0.7110655737704918
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(99)",
        "seed": 10102,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8382565232559653,
            "auditor_fn_violation": 0.00748719975905429,
            "auditor_fp_violation": 0.002679765557391238,
            "ave_precision_score": 0.7720939629001591,
            "fpr": 0.06907894736842106,
            "logloss": 0.5308334230505001,
            "mae": 0.36008587334174336,
            "precision": 0.8297297297297297,
            "recall": 0.6587982832618026
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8431563755326786,
            "auditor_fn_violation": 0.005074589264184556,
            "auditor_fp_violation": 0.010533199430132895,
            "ave_precision_score": 0.7765416958688455,
            "fpr": 0.07903402854006586,
            "logloss": 0.5311511984716408,
            "mae": 0.35561993963501193,
            "precision": 0.824390243902439,
            "recall": 0.6926229508196722
        }
    }
]
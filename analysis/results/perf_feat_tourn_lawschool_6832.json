[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5603070175438597,
            "auc_prc": 0.6187976245883606,
            "auditor_fn_violation": 0.006482400970980912,
            "auditor_fp_violation": 0.02017543859649123,
            "ave_precision_score": 0.5713182685040216,
            "fpr": 0.4133771929824561,
            "logloss": 6.23995107900331,
            "mae": 0.4383339196409123,
            "precision": 0.5457831325301205,
            "recall": 0.949685534591195
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.6415206583861749,
            "auditor_fn_violation": 0.003976554895097653,
            "auditor_fp_violation": 0.01944488003763525,
            "ave_precision_score": 0.6036390957332783,
            "fpr": 0.3951701427003293,
            "logloss": 5.449422306075654,
            "mae": 0.409795588471035,
            "precision": 0.5636363636363636,
            "recall": 0.9748427672955975
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5603070175438597,
            "auc_prc": 0.6262839782206941,
            "auditor_fn_violation": 0.003077991099341646,
            "auditor_fp_violation": 0.01607178866706999,
            "ave_precision_score": 0.5887403543322516,
            "fpr": 0.42872807017543857,
            "logloss": 5.7211440888126175,
            "mae": 0.44280797925320353,
            "precision": 0.5442890442890443,
            "recall": 0.9790356394129979
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.6489651687739229,
            "auditor_fn_violation": 0.0014681956152038792,
            "auditor_fp_violation": 0.012029116735040757,
            "ave_precision_score": 0.6127609333440901,
            "fpr": 0.424807903402854,
            "logloss": 5.451760841445503,
            "mae": 0.4250845433059485,
            "precision": 0.5494761350407451,
            "recall": 0.989517819706499
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5427631578947368,
            "auc_prc": 0.6707956617824431,
            "auditor_fn_violation": 0.002441244620986429,
            "auditor_fp_violation": 0.008590441621294617,
            "ave_precision_score": 0.6024510477424754,
            "fpr": 0.44846491228070173,
            "logloss": 7.583674184379179,
            "mae": 0.45678919975586074,
            "precision": 0.5341685649202733,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.5609220636663008,
            "auc_prc": 0.6773465552601575,
            "auditor_fn_violation": 0.0009941387237744133,
            "auditor_fp_violation": 0.013296271378492267,
            "ave_precision_score": 0.612937470315305,
            "fpr": 0.43578485181119647,
            "logloss": 7.224382936676564,
            "mae": 0.4372120035630679,
            "precision": 0.5442020665901263,
            "recall": 0.9937106918238994
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.696698685323635,
            "auditor_fn_violation": 0.014700338372135795,
            "auditor_fp_violation": 0.0078090340794515056,
            "ave_precision_score": 0.6980964096908879,
            "fpr": 0.15570175438596492,
            "logloss": 1.4464354644201398,
            "mae": 0.3111537144103448,
            "precision": 0.7142857142857143,
            "recall": 0.7442348008385744
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7479020020736884,
            "auditor_fn_violation": 0.017570021194485294,
            "auditor_fp_violation": 0.012236515299438004,
            "ave_precision_score": 0.7475825295461447,
            "fpr": 0.15916575192096596,
            "logloss": 1.2756916160215197,
            "mae": 0.30170636415034624,
            "precision": 0.7184466019417476,
            "recall": 0.7756813417190775
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5416666666666666,
            "auc_prc": 0.6949648550336391,
            "auditor_fn_violation": 0.0020067858325057927,
            "auditor_fp_violation": 0.008247630570679584,
            "ave_precision_score": 0.6219122952507798,
            "fpr": 0.4517543859649123,
            "logloss": 7.714261175972971,
            "mae": 0.4580216866397391,
            "precision": 0.5334088335220838,
            "recall": 0.9874213836477987
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.7174359787856793,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.012939647017760408,
            "ave_precision_score": 0.6530470113695789,
            "fpr": 0.43249176728869376,
            "logloss": 7.008198121849765,
            "mae": 0.43274572726541793,
            "precision": 0.5466052934407365,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5416666666666666,
            "auc_prc": 0.6905391830250075,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.0075922565033272795,
            "ave_precision_score": 0.6138711665884089,
            "fpr": 0.4506578947368421,
            "logloss": 7.899931416668315,
            "mae": 0.4559200266514196,
            "precision": 0.5334846765039728,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.562019758507135,
            "auc_prc": 0.6998049346937494,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.011475210813052961,
            "ave_precision_score": 0.6233429674656337,
            "fpr": 0.43578485181119647,
            "logloss": 7.662598484199095,
            "mae": 0.43481785443633997,
            "precision": 0.5447247706422018,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5274122807017544,
            "auc_prc": 0.5940308663682762,
            "auditor_fn_violation": 0.0024826216484607746,
            "auditor_fp_violation": 0.004262452107279684,
            "ave_precision_score": 0.5478072229444191,
            "fpr": 0.46271929824561403,
            "logloss": 8.10126065005618,
            "mae": 0.4729215946686807,
            "precision": 0.5258426966292135,
            "recall": 0.9811320754716981
        },
        "train": {
            "accuracy": 0.5411635565312843,
            "auc_prc": 0.6112263303372135,
            "auditor_fn_violation": 0.0009895362296828652,
            "auditor_fp_violation": 0.004896629520403466,
            "ave_precision_score": 0.5653940837140563,
            "fpr": 0.4566410537870472,
            "logloss": 7.68081601196533,
            "mae": 0.4588939973879348,
            "precision": 0.5331088664421998,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.602565597870628,
            "auditor_fn_violation": 0.03133160469307441,
            "auditor_fp_violation": 0.04091046581972172,
            "ave_precision_score": 0.5821479001388996,
            "fpr": 0.19298245614035087,
            "logloss": 3.5746090584076295,
            "mae": 0.35569359820339397,
            "precision": 0.6653992395437263,
            "recall": 0.7337526205450734
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.6113628251455718,
            "auditor_fn_violation": 0.01485685092751763,
            "auditor_fp_violation": 0.02716921193604031,
            "ave_precision_score": 0.588731215550679,
            "fpr": 0.20087815587266739,
            "logloss": 3.4384840066272857,
            "mae": 0.34910025767297453,
            "precision": 0.6666666666666666,
            "recall": 0.7672955974842768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5416666666666666,
            "auc_prc": 0.6949253563080621,
            "auditor_fn_violation": 0.0020067858325057927,
            "auditor_fp_violation": 0.008247630570679584,
            "ave_precision_score": 0.6218350261624198,
            "fpr": 0.4517543859649123,
            "logloss": 7.7141244455038365,
            "mae": 0.45802682805769235,
            "precision": 0.5334088335220838,
            "recall": 0.9874213836477987
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.7173983018870949,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.012939647017760408,
            "ave_precision_score": 0.652981474152822,
            "fpr": 0.43249176728869376,
            "logloss": 7.008090820858406,
            "mae": 0.43274960628482256,
            "precision": 0.5466052934407365,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5350877192982456,
            "auc_prc": 0.6724287823289121,
            "auditor_fn_violation": 0.0020067858325057927,
            "auditor_fp_violation": 0.005041338979632999,
            "ave_precision_score": 0.6234709713866897,
            "fpr": 0.4583333333333333,
            "logloss": 6.922070279128574,
            "mae": 0.46302248628158316,
            "precision": 0.5298087739032621,
            "recall": 0.9874213836477987
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.6921658050088123,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.007011083176941338,
            "ave_precision_score": 0.6460439052914142,
            "fpr": 0.4456641053787047,
            "logloss": 6.577913141720283,
            "mae": 0.44360776130726515,
            "precision": 0.5396825396825397,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5614035087719298,
            "auc_prc": 0.6497967331458682,
            "auditor_fn_violation": 0.002018279451248667,
            "auditor_fp_violation": 0.013664549304295236,
            "ave_precision_score": 0.6008543775404219,
            "fpr": 0.4298245614035088,
            "logloss": 6.264597634902723,
            "mae": 0.4373846813323066,
            "precision": 0.5447154471544715,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.58397365532382,
            "auc_prc": 0.6576113808815574,
            "auditor_fn_violation": 0.001445183144746138,
            "auditor_fp_violation": 0.014487548498383806,
            "ave_precision_score": 0.6074384151232257,
            "fpr": 0.411635565312843,
            "logloss": 6.159556890750716,
            "mae": 0.41537578532837816,
            "precision": 0.5577830188679245,
            "recall": 0.9916142557651991
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5383771929824561,
            "auc_prc": 0.6820041486699955,
            "auditor_fn_violation": 0.0018228879326198098,
            "auditor_fp_violation": 0.006883948376688867,
            "ave_precision_score": 0.6116223771745972,
            "fpr": 0.45285087719298245,
            "logloss": 7.6308994172217925,
            "mae": 0.46075513701541476,
            "precision": 0.5317460317460317,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.5598243688254665,
            "auc_prc": 0.6845105312002397,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.010010774608345527,
            "ave_precision_score": 0.6175849111718881,
            "fpr": 0.43907793633369924,
            "logloss": 7.325333687749429,
            "mae": 0.43722423776615144,
            "precision": 0.54337899543379,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.6130432347711909,
            "auditor_fn_violation": 0.02188385008643202,
            "auditor_fp_violation": 0.036317806009276074,
            "ave_precision_score": 0.5858986305702846,
            "fpr": 0.23135964912280702,
            "logloss": 3.5770699549003386,
            "mae": 0.35783648624174086,
            "precision": 0.6429780033840947,
            "recall": 0.7966457023060797
        },
        "train": {
            "accuracy": 0.6608122941822173,
            "auc_prc": 0.6192014672658739,
            "auditor_fn_violation": 0.012074643249176734,
            "auditor_fp_violation": 0.02401270695594552,
            "ave_precision_score": 0.5895843117100465,
            "fpr": 0.2502744237102086,
            "logloss": 3.6394374364194144,
            "mae": 0.3562519320868559,
            "precision": 0.6346153846153846,
            "recall": 0.8301886792452831
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.6163885776586322,
            "auditor_fn_violation": 0.026382452462392885,
            "auditor_fp_violation": 0.04004839685420447,
            "ave_precision_score": 0.6031555922842597,
            "fpr": 0.19298245614035087,
            "logloss": 2.707445923797505,
            "mae": 0.3444948922679092,
            "precision": 0.6691729323308271,
            "recall": 0.7463312368972747
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.654224200471397,
            "auditor_fn_violation": 0.010700798762849587,
            "auditor_fp_violation": 0.033166065548063346,
            "ave_precision_score": 0.6400932264218853,
            "fpr": 0.18990120746432493,
            "logloss": 2.29120794799803,
            "mae": 0.3346325751970736,
            "precision": 0.6760299625468165,
            "recall": 0.7568134171907757
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.6821321352130706,
            "auditor_fn_violation": 0.013831420795174524,
            "auditor_fp_violation": 0.0054345634200443724,
            "ave_precision_score": 0.6823602648371814,
            "fpr": 0.15021929824561403,
            "logloss": 1.6314991438479973,
            "mae": 0.314185565497629,
            "precision": 0.7215447154471545,
            "recall": 0.7442348008385744
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7228594090074664,
            "auditor_fn_violation": 0.011830711062324677,
            "auditor_fp_violation": 0.010961772903630488,
            "ave_precision_score": 0.7223774193341757,
            "fpr": 0.14928649835345773,
            "logloss": 1.4119770473438642,
            "mae": 0.3104113352365739,
            "precision": 0.7230142566191446,
            "recall": 0.7442348008385744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5361842105263158,
            "auc_prc": 0.6987061259067752,
            "auditor_fn_violation": 0.0018228879326198098,
            "auditor_fp_violation": 0.007738455333736646,
            "ave_precision_score": 0.6195794061650267,
            "fpr": 0.4550438596491228,
            "logloss": 8.01533041782052,
            "mae": 0.461107840985243,
            "precision": 0.5305429864253394,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.5686059275521405,
            "auc_prc": 0.7092037912834194,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.013278566622994948,
            "ave_precision_score": 0.6279559219922455,
            "fpr": 0.43029637760702527,
            "logloss": 7.833535088720834,
            "mae": 0.4348111535048247,
            "precision": 0.5483870967741935,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 6832,
        "test": {
            "accuracy": 0.543859649122807,
            "auc_prc": 0.6751983217238045,
            "auditor_fn_violation": 0.002018279451248667,
            "auditor_fp_violation": 0.009089534180278289,
            "ave_precision_score": 0.6052141120139171,
            "fpr": 0.4473684210526316,
            "logloss": 7.6743608417521525,
            "mae": 0.4545918047056412,
            "precision": 0.5347776510832383,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.6854724512083739,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.011255165994729041,
            "ave_precision_score": 0.6167168305332663,
            "fpr": 0.433589462129528,
            "logloss": 7.384129127190432,
            "mae": 0.4336095166297471,
            "precision": 0.5464982778415615,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5592105263157895,
            "auc_prc": 0.6654528016866,
            "auditor_fn_violation": 0.0021355143624259814,
            "auditor_fp_violation": 0.012784835652349266,
            "ave_precision_score": 0.6042576654078023,
            "fpr": 0.4298245614035088,
            "logloss": 6.9079239056865385,
            "mae": 0.4419692586187448,
            "precision": 0.5436554132712457,
            "recall": 0.9790356394129979
        },
        "train": {
            "accuracy": 0.5773874862788145,
            "auc_prc": 0.6770105433854541,
            "auditor_fn_violation": 0.001884721330488992,
            "auditor_fp_violation": 0.015327259759114153,
            "ave_precision_score": 0.618278677682089,
            "fpr": 0.41931942919868276,
            "logloss": 6.561003484043129,
            "mae": 0.4208665232243088,
            "precision": 0.5537383177570093,
            "recall": 0.9937106918238994
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5449561403508771,
            "auc_prc": 0.6577433850006886,
            "auditor_fn_violation": 0.0016343925852366767,
            "auditor_fp_violation": 0.006652046783625747,
            "ave_precision_score": 0.6168648903900619,
            "fpr": 0.4451754385964912,
            "logloss": 5.875887643410858,
            "mae": 0.45315468799005304,
            "precision": 0.5354691075514875,
            "recall": 0.9811320754716981
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.6623175963406267,
            "auditor_fn_violation": 0.0007041815960068761,
            "auditor_fp_violation": 0.012997819785823051,
            "ave_precision_score": 0.6202286177206835,
            "fpr": 0.4270032930845225,
            "logloss": 5.8672470456108,
            "mae": 0.4263229387066834,
            "precision": 0.5497685185185185,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 6832,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.6107106399882725,
            "auditor_fn_violation": 0.022145904593769538,
            "auditor_fp_violation": 0.03678917120387175,
            "ave_precision_score": 0.5964148122723373,
            "fpr": 0.20175438596491227,
            "logloss": 3.0507341504163876,
            "mae": 0.34950845585528645,
            "precision": 0.6611418047882136,
            "recall": 0.7526205450733753
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.6494509152097474,
            "auditor_fn_violation": 0.010843476079687584,
            "auditor_fp_violation": 0.03214171898000375,
            "ave_precision_score": 0.6343146831087714,
            "fpr": 0.20087815587266739,
            "logloss": 2.6548101598184077,
            "mae": 0.33964168979981363,
            "precision": 0.6642201834862386,
            "recall": 0.7589098532494759
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.6645895875049344,
            "auditor_fn_violation": 0.002018279451248667,
            "auditor_fp_violation": 0.011887477313974595,
            "ave_precision_score": 0.6003781457964409,
            "fpr": 0.43640350877192985,
            "logloss": 7.183430309922907,
            "mae": 0.44558337534764836,
            "precision": 0.5409457900807382,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.6718894107658702,
            "auditor_fn_violation": 0.0015648479911263915,
            "auditor_fp_violation": 0.01488211162089568,
            "ave_precision_score": 0.6094340005835398,
            "fpr": 0.42371020856201974,
            "logloss": 6.929303175479722,
            "mae": 0.42596726691518844,
            "precision": 0.5501165501165501,
            "recall": 0.989517819706499
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.6470344328903685,
            "auditor_fn_violation": 0.011068354849387625,
            "auditor_fp_violation": 0.03706392417826175,
            "ave_precision_score": 0.6195429946184922,
            "fpr": 0.19517543859649122,
            "logloss": 3.403480456206734,
            "mae": 0.31488157626785945,
            "precision": 0.6798561151079137,
            "recall": 0.7924528301886793
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.6833487518446925,
            "auditor_fn_violation": 0.006342236858153434,
            "auditor_fp_violation": 0.02811515172975461,
            "ave_precision_score": 0.6582145622735838,
            "fpr": 0.2074643249176729,
            "logloss": 2.81388704647364,
            "mae": 0.29803902224678125,
            "precision": 0.6796610169491526,
            "recall": 0.8406708595387841
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5405701754385965,
            "auc_prc": 0.6690555370629885,
            "auditor_fn_violation": 0.0021585015999117294,
            "auditor_fp_violation": 0.008608086307723339,
            "ave_precision_score": 0.5970827340720058,
            "fpr": 0.44846491228070173,
            "logloss": 7.697222393143609,
            "mae": 0.45648008732729733,
            "precision": 0.5331050228310502,
            "recall": 0.9790356394129979
        },
        "train": {
            "accuracy": 0.5598243688254665,
            "auc_prc": 0.6785270838860096,
            "auditor_fn_violation": 0.001532630532485554,
            "auditor_fp_violation": 0.013167279588440321,
            "ave_precision_score": 0.609910279039111,
            "fpr": 0.43688254665203075,
            "logloss": 7.295705821468202,
            "mae": 0.43552336194786234,
            "precision": 0.5435779816513762,
            "recall": 0.9937106918238994
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5427631578947368,
            "auc_prc": 0.696048763518217,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.00828039927404719,
            "ave_precision_score": 0.6176386246100058,
            "fpr": 0.44956140350877194,
            "logloss": 7.967806296917199,
            "mae": 0.45527006034635503,
            "precision": 0.5340909090909091,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.566410537870472,
            "auc_prc": 0.706259586231826,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.010845427367505215,
            "ave_precision_score": 0.6268620533804472,
            "fpr": 0.43249176728869376,
            "logloss": 7.750909938170684,
            "mae": 0.43391685457494655,
            "precision": 0.5471264367816092,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5515350877192983,
            "auc_prc": 0.6762414374901833,
            "auditor_fn_violation": 0.00396299974254294,
            "auditor_fp_violation": 0.011872353297035702,
            "ave_precision_score": 0.6130722596620731,
            "fpr": 0.4342105263157895,
            "logloss": 7.013434049430808,
            "mae": 0.4483866989073685,
            "precision": 0.5395348837209303,
            "recall": 0.9727463312368972
        },
        "train": {
            "accuracy": 0.5773874862788145,
            "auc_prc": 0.7035338733598462,
            "auditor_fn_violation": 0.001458990627020783,
            "auditor_fp_violation": 0.012180871782160698,
            "ave_precision_score": 0.6379177324670751,
            "fpr": 0.4149286498353458,
            "logloss": 6.7770769725688735,
            "mae": 0.4211723885730888,
            "precision": 0.5542452830188679,
            "recall": 0.9853249475890985
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 6832,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.6655196644527195,
            "auditor_fn_violation": 0.015102615028136389,
            "auditor_fp_violation": 0.016606170598911073,
            "ave_precision_score": 0.663711198181627,
            "fpr": 0.1611842105263158,
            "logloss": 2.0224580329183515,
            "mae": 0.3204245927773403,
            "precision": 0.7077534791252486,
            "recall": 0.7463312368972747
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.6688098349219793,
            "auditor_fn_violation": 0.014398902765408577,
            "auditor_fp_violation": 0.004691760206791554,
            "ave_precision_score": 0.6637216480028894,
            "fpr": 0.16136114160263446,
            "logloss": 2.0893338716184306,
            "mae": 0.31300047151011967,
            "precision": 0.7156673114119922,
            "recall": 0.7756813417190775
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.6731812148836583,
            "auditor_fn_violation": 0.009227077126779218,
            "auditor_fp_violation": 0.018163944343617667,
            "ave_precision_score": 0.6734911365942701,
            "fpr": 0.1600877192982456,
            "logloss": 1.8192304789924665,
            "mae": 0.3216596946596602,
            "precision": 0.7062374245472837,
            "recall": 0.7358490566037735
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7164998817828152,
            "auditor_fn_violation": 0.0069474648311920245,
            "auditor_fp_violation": 0.011852069180067495,
            "ave_precision_score": 0.7159877927723628,
            "fpr": 0.15587266739846323,
            "logloss": 1.5370477122470227,
            "mae": 0.3140466000880409,
            "precision": 0.7131313131313132,
            "recall": 0.740041928721174
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.6179267186455155,
            "auditor_fn_violation": 0.02292287322078782,
            "auditor_fp_violation": 0.037840290381125234,
            "ave_precision_score": 0.5994136103184902,
            "fpr": 0.1962719298245614,
            "logloss": 3.178425358674783,
            "mae": 0.34054957300170996,
            "precision": 0.6691312384473198,
            "recall": 0.7589098532494759
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.6222309851452505,
            "auditor_fn_violation": 0.01182150607414158,
            "auditor_fp_violation": 0.031190720684718783,
            "ave_precision_score": 0.6007020323638705,
            "fpr": 0.21405049396267836,
            "logloss": 3.236665225285349,
            "mae": 0.34025388681019925,
            "precision": 0.6590909090909091,
            "recall": 0.790356394129979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.6033597668483447,
            "auditor_fn_violation": 0.021840174335209094,
            "auditor_fp_violation": 0.03810748134704578,
            "ave_precision_score": 0.5869618778781052,
            "fpr": 0.19846491228070176,
            "logloss": 3.302147239868365,
            "mae": 0.35029401043439395,
            "precision": 0.6648148148148149,
            "recall": 0.7526205450733753
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.615331860619992,
            "auditor_fn_violation": 0.014106644390595266,
            "auditor_fp_violation": 0.026380085691016606,
            "ave_precision_score": 0.5943511903751633,
            "fpr": 0.19978046103183314,
            "logloss": 3.378854318483969,
            "mae": 0.34605031311021583,
            "precision": 0.6654411764705882,
            "recall": 0.7589098532494759
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 6832,
        "test": {
            "accuracy": 0.543859649122807,
            "auc_prc": 0.6890358943252304,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.008590441621294617,
            "ave_precision_score": 0.6136332610647546,
            "fpr": 0.44846491228070173,
            "logloss": 7.846839142970582,
            "mae": 0.45634665230285515,
            "precision": 0.534698521046644,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5642151481888035,
            "auc_prc": 0.6960897359322551,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.010961772903630495,
            "ave_precision_score": 0.6210055426685843,
            "fpr": 0.43468715697036225,
            "logloss": 7.626745806092114,
            "mae": 0.43445156149166547,
            "precision": 0.5458715596330275,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5537280701754386,
            "auc_prc": 0.6668854066289419,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.012868017745513213,
            "ave_precision_score": 0.6003632519528247,
            "fpr": 0.43859649122807015,
            "logloss": 7.263296022678717,
            "mae": 0.44748928789846454,
            "precision": 0.5402298850574713,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5729967069154775,
            "auc_prc": 0.6690885478679507,
            "auditor_fn_violation": 0.0010355611705983475,
            "auditor_fp_violation": 0.012580493406243207,
            "ave_precision_score": 0.60671051108164,
            "fpr": 0.42371020856201974,
            "logloss": 6.990788035390265,
            "mae": 0.426238916460714,
            "precision": 0.5511627906976744,
            "recall": 0.9937106918238994
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.6371218720513294,
            "auditor_fn_violation": 0.0075099304865938465,
            "auditor_fp_violation": 0.031185722928009682,
            "ave_precision_score": 0.6094673485858544,
            "fpr": 0.26644736842105265,
            "logloss": 3.495332666744577,
            "mae": 0.37057637484453415,
            "precision": 0.6351351351351351,
            "recall": 0.8867924528301887
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.6770960304558454,
            "auditor_fn_violation": 0.00597633857787535,
            "auditor_fp_violation": 0.030634285511945653,
            "ave_precision_score": 0.6519521112152786,
            "fpr": 0.2414928649835346,
            "logloss": 2.8038979592448583,
            "mae": 0.35128682673663797,
            "precision": 0.6583850931677019,
            "recall": 0.8888888888888888
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5416666666666666,
            "auc_prc": 0.6872767669637839,
            "auditor_fn_violation": 0.0016343925852366767,
            "auditor_fp_violation": 0.004943032869530152,
            "ave_precision_score": 0.6254682101784559,
            "fpr": 0.44846491228070173,
            "logloss": 7.0185553624688,
            "mae": 0.4548839507278944,
            "precision": 0.5336374002280502,
            "recall": 0.9811320754716981
        },
        "train": {
            "accuracy": 0.5686059275521405,
            "auc_prc": 0.7183711281434322,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.012074643249176729,
            "ave_precision_score": 0.6668662525098289,
            "fpr": 0.43029637760702527,
            "logloss": 6.177136945691452,
            "mae": 0.42720689179134536,
            "precision": 0.5483870967741935,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5537280701754386,
            "auc_prc": 0.6700153173353978,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.01062462189957653,
            "ave_precision_score": 0.6026324046067858,
            "fpr": 0.43859649122807015,
            "logloss": 7.381448461657286,
            "mae": 0.44856276046820004,
            "precision": 0.5402298850574713,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5729967069154775,
            "auc_prc": 0.6752991163831517,
            "auditor_fn_violation": 0.0013255182983658844,
            "auditor_fp_violation": 0.013030700046032375,
            "ave_precision_score": 0.6113963668540464,
            "fpr": 0.4226125137211855,
            "logloss": 7.080102610168705,
            "mae": 0.42618482026292454,
            "precision": 0.5512820512820513,
            "recall": 0.9916142557651991
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5449561403508771,
            "auc_prc": 0.6809497553577715,
            "auditor_fn_violation": 0.0016343925852366767,
            "auditor_fp_violation": 0.006725146198830412,
            "ave_precision_score": 0.6233037864999892,
            "fpr": 0.4451754385964912,
            "logloss": 6.76809405349277,
            "mae": 0.4529747325131529,
            "precision": 0.5354691075514875,
            "recall": 0.9811320754716981
        },
        "train": {
            "accuracy": 0.5740944017563118,
            "auc_prc": 0.6885156187210237,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.014105631629798622,
            "ave_precision_score": 0.6309920698117918,
            "fpr": 0.424807903402854,
            "logloss": 6.561010779279944,
            "mae": 0.4231979016281167,
            "precision": 0.5515643105446119,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5449561403508771,
            "auc_prc": 0.6690493250955647,
            "auditor_fn_violation": 0.0030067306631358266,
            "auditor_fp_violation": 0.011474087517644685,
            "ave_precision_score": 0.6151757096173109,
            "fpr": 0.44298245614035087,
            "logloss": 6.6070395397627095,
            "mae": 0.4541317987447762,
            "precision": 0.535632183908046,
            "recall": 0.9769392033542977
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.7106015515478412,
            "auditor_fn_violation": 0.0019606624829995377,
            "auditor_fp_violation": 0.00976290803138295,
            "ave_precision_score": 0.6635315567968008,
            "fpr": 0.42590559824368823,
            "logloss": 5.837963683397982,
            "mae": 0.4286219954272659,
            "precision": 0.5493612078977933,
            "recall": 0.9916142557651991
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5394736842105263,
            "auc_prc": 0.6991795966743504,
            "auditor_fn_violation": 0.0018228879326198098,
            "auditor_fp_violation": 0.006977213147812081,
            "ave_precision_score": 0.6183898903345144,
            "fpr": 0.4517543859649123,
            "logloss": 8.125564442253285,
            "mae": 0.4590900228190212,
            "precision": 0.5323496027241771,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.5642151481888035,
            "auc_prc": 0.7057016737665898,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.010041125617769528,
            "ave_precision_score": 0.625439421727017,
            "fpr": 0.43468715697036225,
            "logloss": 7.8991688868371055,
            "mae": 0.43420096788644474,
            "precision": 0.5458715596330275,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 6832,
        "test": {
            "accuracy": 0.49890350877192985,
            "auc_prc": 0.5641147432245399,
            "auditor_fn_violation": 0.015268123138033773,
            "auditor_fp_violation": 0.0328241580963904,
            "ave_precision_score": 0.5502869879431008,
            "fpr": 0.1162280701754386,
            "logloss": 5.0876972509407175,
            "mae": 0.4955139122216475,
            "precision": 0.5431034482758621,
            "recall": 0.2641509433962264
        },
        "train": {
            "accuracy": 0.5214050493962679,
            "auc_prc": 0.5994160124197689,
            "auditor_fn_violation": 0.013110204419775083,
            "auditor_fp_violation": 0.01781098403031054,
            "ave_precision_score": 0.5773139352732017,
            "fpr": 0.10428100987925357,
            "logloss": 4.934320277913071,
            "mae": 0.4739189796355389,
            "precision": 0.5887445887445888,
            "recall": 0.2851153039832285
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5372807017543859,
            "auc_prc": 0.6862077848904516,
            "auditor_fn_violation": 0.0020067858325057927,
            "auditor_fp_violation": 0.006619278080258127,
            "ave_precision_score": 0.6104019152976474,
            "fpr": 0.45614035087719296,
            "logloss": 7.9878231340958425,
            "mae": 0.46023218290154855,
            "precision": 0.5310033821871477,
            "recall": 0.9874213836477987
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.6974572334270682,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.00987419506593757,
            "ave_precision_score": 0.6227527311391161,
            "fpr": 0.433589462129528,
            "logloss": 7.705949007002424,
            "mae": 0.4373268254706064,
            "precision": 0.5464982778415615,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5493421052631579,
            "auc_prc": 0.6635853629201605,
            "auditor_fn_violation": 0.004151495089926074,
            "auditor_fp_violation": 0.009195402298850578,
            "ave_precision_score": 0.5917938843270197,
            "fpr": 0.4375,
            "logloss": 7.704362553027327,
            "mae": 0.4512904302335123,
            "precision": 0.5381944444444444,
            "recall": 0.9748427672955975
        },
        "train": {
            "accuracy": 0.566410537870472,
            "auc_prc": 0.6734115923598762,
            "auditor_fn_violation": 0.0014635931211123311,
            "auditor_fp_violation": 0.013038287798388363,
            "ave_precision_score": 0.605048336390964,
            "fpr": 0.4270032930845225,
            "logloss": 7.305317045562539,
            "mae": 0.4333312974040907,
            "precision": 0.5476744186046512,
            "recall": 0.9874213836477987
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5394736842105263,
            "auc_prc": 0.6757302617055762,
            "auditor_fn_violation": 0.0020067858325057927,
            "auditor_fp_violation": 0.00834089534180279,
            "ave_precision_score": 0.6038778815344937,
            "fpr": 0.45394736842105265,
            "logloss": 7.7942912306978505,
            "mae": 0.4580190923218292,
            "precision": 0.5322033898305085,
            "recall": 0.9874213836477987
        },
        "train": {
            "accuracy": 0.562019758507135,
            "auc_prc": 0.6900815727149403,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.012357919337133956,
            "ave_precision_score": 0.6191351515151505,
            "fpr": 0.43688254665203075,
            "logloss": 7.474438132497744,
            "mae": 0.4374780518142152,
            "precision": 0.5446224256292906,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.6322984587671983,
            "auditor_fn_violation": 0.00032641877229762063,
            "auditor_fp_violation": 0.003591954022988514,
            "ave_precision_score": 0.5822256154745242,
            "fpr": 0.46271929824561403,
            "logloss": 7.886059787816896,
            "mae": 0.473094508294117,
            "precision": 0.5253093363329584,
            "recall": 0.9790356394129979
        },
        "train": {
            "accuracy": 0.5488474204171241,
            "auc_prc": 0.6288636391427929,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.005255783131920666,
            "ave_precision_score": 0.5813472473126444,
            "fpr": 0.4500548847420417,
            "logloss": 7.669479042747614,
            "mae": 0.4516026469438685,
            "precision": 0.5372460496613995,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5405701754385965,
            "auc_prc": 0.6906357402304605,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.008247630570679584,
            "ave_precision_score": 0.6140291435906037,
            "fpr": 0.4517543859649123,
            "logloss": 7.89389068038596,
            "mae": 0.4557504851289378,
            "precision": 0.5328798185941043,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.562019758507135,
            "auc_prc": 0.6998037347840622,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.011475210813052961,
            "ave_precision_score": 0.6233212374434985,
            "fpr": 0.43578485181119647,
            "logloss": 7.655517983934416,
            "mae": 0.43466158415700484,
            "precision": 0.5447247706422018,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5504385964912281,
            "auc_prc": 0.6783666353010611,
            "auditor_fn_violation": 0.003151550259296039,
            "auditor_fp_violation": 0.014080459770114944,
            "ave_precision_score": 0.6157249408906496,
            "fpr": 0.4407894736842105,
            "logloss": 6.917322576558939,
            "mae": 0.4484727250799066,
            "precision": 0.5384615384615384,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.5773874862788145,
            "auc_prc": 0.7201900816296813,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.014017107852312,
            "ave_precision_score": 0.6653744207772442,
            "fpr": 0.42151481888035125,
            "logloss": 6.097632227165662,
            "mae": 0.4224724560138165,
            "precision": 0.5534883720930233,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5405701754385965,
            "auc_prc": 0.7008125692038587,
            "auditor_fn_violation": 0.0018228879326198098,
            "auditor_fp_violation": 0.008628251663641867,
            "ave_precision_score": 0.6191074008746245,
            "fpr": 0.4506578947368421,
            "logloss": 7.99304650522329,
            "mae": 0.457434793420568,
            "precision": 0.5329545454545455,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.5675082327113062,
            "auc_prc": 0.7138795412721224,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.009702206012534964,
            "ave_precision_score": 0.6325244816087237,
            "fpr": 0.4313940724478595,
            "logloss": 7.716961726441145,
            "mae": 0.43299282585476256,
            "precision": 0.5477560414269275,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5504385964912281,
            "auc_prc": 0.6552051806180123,
            "auditor_fn_violation": 0.0029814447019015046,
            "auditor_fp_violation": 0.010473381730187543,
            "ave_precision_score": 0.5972869397457445,
            "fpr": 0.4375,
            "logloss": 6.925247737103163,
            "mae": 0.44837817183152767,
            "precision": 0.5387283236994219,
            "recall": 0.9769392033542977
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.659950592852875,
            "auditor_fn_violation": 0.0014681956152038792,
            "auditor_fp_violation": 0.011371511530854332,
            "ave_precision_score": 0.6064672142303187,
            "fpr": 0.424807903402854,
            "logloss": 6.587374086930073,
            "mae": 0.42822717765535606,
            "precision": 0.5494761350407451,
            "recall": 0.989517819706499
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.6030444851036432,
            "auditor_fn_violation": 0.029511015484203176,
            "auditor_fp_violation": 0.04091046581972172,
            "ave_precision_score": 0.5826155405366273,
            "fpr": 0.19298245614035087,
            "logloss": 3.574034063883707,
            "mae": 0.354385565628826,
            "precision": 0.6666666666666666,
            "recall": 0.7379454926624738
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.6110932234567276,
            "auditor_fn_violation": 0.01485685092751763,
            "auditor_fp_violation": 0.028588121626611774,
            "ave_precision_score": 0.5884160594311797,
            "fpr": 0.2030735455543359,
            "logloss": 3.442863647204241,
            "mae": 0.349481236935671,
            "precision": 0.6642468239564429,
            "recall": 0.7672955974842768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.5601921353312594,
            "auditor_fn_violation": 0.0035170473353194312,
            "auditor_fp_violation": 0.006137830207703168,
            "ave_precision_score": 0.50907315803616,
            "fpr": 0.4583333333333333,
            "logloss": 8.872863715535535,
            "mae": 0.4889229692183617,
            "precision": 0.5184331797235023,
            "recall": 0.9433962264150944
        },
        "train": {
            "accuracy": 0.5279912184412733,
            "auc_prc": 0.579635916304171,
            "auditor_fn_violation": 0.003645175320506182,
            "auditor_fp_violation": 0.005427772185323272,
            "ave_precision_score": 0.5310391758907723,
            "fpr": 0.4489571899012075,
            "logloss": 8.36472118451667,
            "mae": 0.4708621802242441,
            "precision": 0.5271676300578034,
            "recall": 0.9559748427672956
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.6662091809818721,
            "auditor_fn_violation": 0.013755562911471552,
            "auditor_fp_violation": 0.008406432748538013,
            "ave_precision_score": 0.6663562297272903,
            "fpr": 0.15899122807017543,
            "logloss": 1.9170410937784623,
            "mae": 0.32116894958902265,
            "precision": 0.7123015873015873,
            "recall": 0.7526205450733753
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.6966305846189758,
            "auditor_fn_violation": 0.016145549273151125,
            "auditor_fp_violation": 0.014229564918279909,
            "ave_precision_score": 0.6952302535233919,
            "fpr": 0.16355653128430298,
            "logloss": 1.671131415296606,
            "mae": 0.31814149419670124,
            "precision": 0.7043650793650794,
            "recall": 0.7442348008385744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5537280701754386,
            "auc_prc": 0.6700009821743516,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.01062462189957653,
            "ave_precision_score": 0.6026055471875644,
            "fpr": 0.43859649122807015,
            "logloss": 7.379815973518223,
            "mae": 0.44848789296359726,
            "precision": 0.5402298850574713,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.6745811826300396,
            "auditor_fn_violation": 0.0016568978729573555,
            "auditor_fp_violation": 0.013030700046032375,
            "ave_precision_score": 0.6109627012730994,
            "fpr": 0.4226125137211855,
            "logloss": 7.078499555058512,
            "mae": 0.4261680718673651,
            "precision": 0.5507584597432905,
            "recall": 0.989517819706499
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5416666666666666,
            "auc_prc": 0.6905610773002568,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.0075922565033272795,
            "ave_precision_score": 0.6138879854504964,
            "fpr": 0.4506578947368421,
            "logloss": 7.899976015181994,
            "mae": 0.45593575744967263,
            "precision": 0.5334846765039728,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.562019758507135,
            "auc_prc": 0.6998145625041752,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.011475210813052961,
            "ave_precision_score": 0.6233578325082972,
            "fpr": 0.43578485181119647,
            "logloss": 7.662482102163899,
            "mae": 0.43482762739432307,
            "precision": 0.5447247706422018,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5372807017543859,
            "auc_prc": 0.6869100828748745,
            "auditor_fn_violation": 0.0020067858325057927,
            "auditor_fp_violation": 0.006619278080258127,
            "ave_precision_score": 0.6108583597995877,
            "fpr": 0.45614035087719296,
            "logloss": 7.988400273425917,
            "mae": 0.46023729580999456,
            "precision": 0.5310033821871477,
            "recall": 0.9874213836477987
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.6974785256730052,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.00987419506593757,
            "ave_precision_score": 0.6227628940062806,
            "fpr": 0.433589462129528,
            "logloss": 7.70614481767913,
            "mae": 0.43732549397661225,
            "precision": 0.5464982778415615,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5361842105263158,
            "auc_prc": 0.6564685137952584,
            "auditor_fn_violation": 0.003151550259296039,
            "auditor_fp_violation": 0.008431639443436176,
            "ave_precision_score": 0.5901887396872425,
            "fpr": 0.4550438596491228,
            "logloss": 7.858487802253358,
            "mae": 0.46313956224935654,
            "precision": 0.5305429864253394,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.5543358946212953,
            "auc_prc": 0.666299580971436,
            "auditor_fn_violation": 0.0009941387237744133,
            "auditor_fp_violation": 0.010349694213580049,
            "ave_precision_score": 0.6033458034664195,
            "fpr": 0.442371020856202,
            "logloss": 7.4415244328165,
            "mae": 0.44584791760236275,
            "precision": 0.540478905359179,
            "recall": 0.9937106918238994
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5482456140350878,
            "auc_prc": 0.6900179681240383,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.01364942528735634,
            "ave_precision_score": 0.6127375210807182,
            "fpr": 0.4440789473684211,
            "logloss": 7.734739368519188,
            "mae": 0.45022314370709593,
            "precision": 0.5371428571428571,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5740944017563118,
            "auc_prc": 0.7084084824194337,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.01274236545650448,
            "ave_precision_score": 0.6291635906427192,
            "fpr": 0.424807903402854,
            "logloss": 7.507096583436526,
            "mae": 0.42768995698559553,
            "precision": 0.5515643105446119,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.6685762093778587,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.011199334543254702,
            "ave_precision_score": 0.6008558570211457,
            "fpr": 0.4375,
            "logloss": 7.408421356508682,
            "mae": 0.44679351612079665,
            "precision": 0.5408515535097813,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.6750693428295493,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.011761016151795521,
            "ave_precision_score": 0.6109428972722648,
            "fpr": 0.42590559824368823,
            "logloss": 7.1062158340129855,
            "mae": 0.4269242338629073,
            "precision": 0.5504055619930475,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5372807017543859,
            "auc_prc": 0.6869110856295496,
            "auditor_fn_violation": 0.0020067858325057927,
            "auditor_fp_violation": 0.006619278080258127,
            "ave_precision_score": 0.6108603653089378,
            "fpr": 0.45614035087719296,
            "logloss": 7.988439195878985,
            "mae": 0.4602374523145389,
            "precision": 0.5310033821871477,
            "recall": 0.9874213836477987
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.6974771920291195,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.00987419506593757,
            "ave_precision_score": 0.6227585615498958,
            "fpr": 0.433589462129528,
            "logloss": 7.706178148451022,
            "mae": 0.43732568857833387,
            "precision": 0.5464982778415615,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.664767795521399,
            "auditor_fn_violation": 0.002018279451248667,
            "auditor_fp_violation": 0.011887477313974595,
            "ave_precision_score": 0.5999896533724264,
            "fpr": 0.43640350877192985,
            "logloss": 7.209463049494181,
            "mae": 0.44570944903195603,
            "precision": 0.5409457900807382,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.672673526782202,
            "auditor_fn_violation": 0.0015648479911263915,
            "auditor_fp_violation": 0.01488211162089568,
            "ave_precision_score": 0.6099850755308072,
            "fpr": 0.42371020856201974,
            "logloss": 6.934270462017884,
            "mae": 0.4260925916142794,
            "precision": 0.5501165501165501,
            "recall": 0.989517819706499
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 6832,
        "test": {
            "accuracy": 0.543859649122807,
            "auc_prc": 0.6761314950144459,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.00854254890098811,
            "ave_precision_score": 0.605371653951881,
            "fpr": 0.44846491228070173,
            "logloss": 7.668931010090259,
            "mae": 0.4525405114175876,
            "precision": 0.534698521046644,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5642151481888035,
            "auc_prc": 0.6862814293127306,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.012957351773257726,
            "ave_precision_score": 0.615878500548418,
            "fpr": 0.433589462129528,
            "logloss": 7.428115404717302,
            "mae": 0.433708398853821,
            "precision": 0.5459770114942529,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5460526315789473,
            "auc_prc": 0.689275502928777,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.009588626739261957,
            "ave_precision_score": 0.612678905540826,
            "fpr": 0.44627192982456143,
            "logloss": 7.8757388002483255,
            "mae": 0.4536430379332052,
            "precision": 0.5359179019384265,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5642151481888035,
            "auc_prc": 0.6977065739193345,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.011255165994729041,
            "ave_precision_score": 0.6209177558742432,
            "fpr": 0.433589462129528,
            "logloss": 7.664096024290021,
            "mae": 0.4329242993304484,
            "precision": 0.5459770114942529,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.6760450552318619,
            "auditor_fn_violation": 0.011905090293868844,
            "auditor_fp_violation": 0.006944444444444447,
            "ave_precision_score": 0.6762644229197103,
            "fpr": 0.15899122807017543,
            "logloss": 1.8716904740041032,
            "mae": 0.31575654086529265,
            "precision": 0.7117296222664016,
            "recall": 0.750524109014675
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7141369789974573,
            "auditor_fn_violation": 0.017176507949657926,
            "auditor_fp_violation": 0.015916575192096608,
            "ave_precision_score": 0.7136094249078221,
            "fpr": 0.15477497255762898,
            "logloss": 1.5196603189843008,
            "mae": 0.31146920417365515,
            "precision": 0.7168674698795181,
            "recall": 0.7484276729559748
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.6140491099665404,
            "auditor_fn_violation": 0.027536411784177435,
            "auditor_fp_violation": 0.040464307320024206,
            "ave_precision_score": 0.596584740099195,
            "fpr": 0.18201754385964913,
            "logloss": 3.165651006960856,
            "mae": 0.34244671535625665,
            "precision": 0.6782945736434108,
            "recall": 0.7337526205450734
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.6219258699580925,
            "auditor_fn_violation": 0.010397034152807412,
            "auditor_fp_violation": 0.02779646613080274,
            "ave_precision_score": 0.6009810111296525,
            "fpr": 0.20636663007683864,
            "logloss": 3.1924688251100766,
            "mae": 0.34308941985484864,
            "precision": 0.6624775583482945,
            "recall": 0.7735849056603774
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5416666666666666,
            "auc_prc": 0.6904839998541783,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.0075922565033272795,
            "ave_precision_score": 0.6138416535423314,
            "fpr": 0.4506578947368421,
            "logloss": 7.901763721192107,
            "mae": 0.455908125564446,
            "precision": 0.5334846765039728,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.562019758507135,
            "auc_prc": 0.6997637756599255,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.011475210813052961,
            "ave_precision_score": 0.623304940458322,
            "fpr": 0.43578485181119647,
            "logloss": 7.663575681144979,
            "mae": 0.43490749753306457,
            "precision": 0.5447247706422018,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5570175438596491,
            "auc_prc": 0.6549683674466289,
            "auditor_fn_violation": 0.002018279451248667,
            "auditor_fp_violation": 0.012772232304900188,
            "ave_precision_score": 0.5989363803516113,
            "fpr": 0.4342105263157895,
            "logloss": 6.893259310173091,
            "mae": 0.4451331126851851,
            "precision": 0.5421965317919075,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.5740944017563118,
            "auc_prc": 0.6583301700911318,
            "auditor_fn_violation": 0.0018525038718481548,
            "auditor_fp_violation": 0.017209022343401447,
            "ave_precision_score": 0.6029530556976245,
            "fpr": 0.42041712403951703,
            "logloss": 6.7628455413390585,
            "mae": 0.4262026298104767,
            "precision": 0.552046783625731,
            "recall": 0.989517819706499
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5635964912280702,
            "auc_prc": 0.6914287161817001,
            "auditor_fn_violation": 0.006369763507300749,
            "auditor_fp_violation": 0.017634603750756213,
            "ave_precision_score": 0.639926442335529,
            "fpr": 0.41776315789473684,
            "logloss": 6.1526305363188705,
            "mae": 0.4349691572415807,
            "precision": 0.5469678953626635,
            "recall": 0.9643605870020965
        },
        "train": {
            "accuracy": 0.6026344676180022,
            "auc_prc": 0.7143209936651828,
            "auditor_fn_violation": 0.007410015487392619,
            "auditor_fp_violation": 0.020937138000981348,
            "ave_precision_score": 0.670283879267016,
            "fpr": 0.38199780461031835,
            "logloss": 5.394818326835533,
            "mae": 0.3976433865672687,
            "precision": 0.5709001233045623,
            "recall": 0.9706498951781971
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.5967462724666206,
            "auditor_fn_violation": 0.01047298539850675,
            "auditor_fp_violation": 0.019452006452913895,
            "ave_precision_score": 0.5569306256283396,
            "fpr": 0.31030701754385964,
            "logloss": 4.875805577870777,
            "mae": 0.37885033526652523,
            "precision": 0.5951359084406295,
            "recall": 0.8721174004192872
        },
        "train": {
            "accuracy": 0.6542261251372119,
            "auc_prc": 0.6138099052331449,
            "auditor_fn_violation": 0.004729062679065786,
            "auditor_fp_violation": 0.019879911172712438,
            "ave_precision_score": 0.5711072010956341,
            "fpr": 0.29747530186608123,
            "logloss": 4.837878310993199,
            "mae": 0.3535438706753937,
            "precision": 0.6150568181818182,
            "recall": 0.9077568134171907
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5537280701754386,
            "auc_prc": 0.6663177885134566,
            "auditor_fn_violation": 0.006015760050020229,
            "auditor_fp_violation": 0.00713097398669087,
            "ave_precision_score": 0.6084395053222352,
            "fpr": 0.4309210526315789,
            "logloss": 6.993463997772089,
            "mae": 0.4459845356326918,
            "precision": 0.5408878504672897,
            "recall": 0.9706498951781971
        },
        "train": {
            "accuracy": 0.566410537870472,
            "auc_prc": 0.6793901901524178,
            "auditor_fn_violation": 0.004059399788745521,
            "auditor_fp_violation": 0.01674364019890029,
            "ave_precision_score": 0.6284156916777358,
            "fpr": 0.41712403951701427,
            "logloss": 6.7150138512252715,
            "mae": 0.43408957442762147,
            "precision": 0.5486935866983373,
            "recall": 0.9685534591194969
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.7965805485934311,
            "auditor_fn_violation": 0.01090284673949024,
            "auditor_fp_violation": 0.007441016333938297,
            "ave_precision_score": 0.79362566069007,
            "fpr": 0.12390350877192982,
            "logloss": 1.2564745704235682,
            "mae": 0.25172719707149976,
            "precision": 0.7674897119341564,
            "recall": 0.7819706498951782
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8342217549672845,
            "auditor_fn_violation": 0.0119526771557507,
            "auditor_fp_violation": 0.004031625751819798,
            "ave_precision_score": 0.8342421944821284,
            "fpr": 0.1163556531284303,
            "logloss": 0.9545394736746129,
            "mae": 0.24479816110944697,
            "precision": 0.7777777777777778,
            "recall": 0.7777777777777778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5405701754385965,
            "auc_prc": 0.6896692189824131,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.008247630570679584,
            "ave_precision_score": 0.6126005636612744,
            "fpr": 0.4517543859649123,
            "logloss": 7.9629837881989065,
            "mae": 0.4564077303245831,
            "precision": 0.5328798185941043,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.562019758507135,
            "auc_prc": 0.6979915828554349,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.011475210813052961,
            "ave_precision_score": 0.6216731243786124,
            "fpr": 0.43578485181119647,
            "logloss": 7.7069256121065175,
            "mae": 0.4353972387722119,
            "precision": 0.5447247706422018,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.6499453854124564,
            "auditor_fn_violation": 0.011633840891537031,
            "auditor_fp_violation": 0.005242992538818312,
            "ave_precision_score": 0.6491258569915447,
            "fpr": 0.14912280701754385,
            "logloss": 2.2503490290908714,
            "mae": 0.33012263190443647,
            "precision": 0.7166666666666667,
            "recall": 0.7211740041928721
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.6619831222345509,
            "auditor_fn_violation": 0.014649738693397958,
            "auditor_fp_violation": 0.008455285375366117,
            "ave_precision_score": 0.6595232310451745,
            "fpr": 0.15477497255762898,
            "logloss": 2.0019216690700707,
            "mae": 0.3268192319490654,
            "precision": 0.7116564417177914,
            "recall": 0.7295597484276729
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5482456140350878,
            "auc_prc": 0.6858213142446484,
            "auditor_fn_violation": 0.0030205230056272758,
            "auditor_fp_violation": 0.010488505747126451,
            "ave_precision_score": 0.6144969264409627,
            "fpr": 0.4418859649122807,
            "logloss": 7.424269000119728,
            "mae": 0.4509264906814191,
            "precision": 0.5373134328358209,
            "recall": 0.9811320754716981
        },
        "train": {
            "accuracy": 0.5686059275521405,
            "auc_prc": 0.7047256864414758,
            "auditor_fn_violation": 0.001532630532485554,
            "auditor_fp_violation": 0.011963356214622104,
            "ave_precision_score": 0.6304074503898169,
            "fpr": 0.4281009879253567,
            "logloss": 7.233206402900601,
            "mae": 0.4294670465575039,
            "precision": 0.5486111111111112,
            "recall": 0.9937106918238994
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6502192982456141,
            "auc_prc": 0.6306813243498599,
            "auditor_fn_violation": 0.02277115745338189,
            "auditor_fp_violation": 0.011053135712845332,
            "ave_precision_score": 0.6290250129698113,
            "fpr": 0.1787280701754386,
            "logloss": 2.301081155685222,
            "mae": 0.3605555562985497,
            "precision": 0.6632231404958677,
            "recall": 0.6729559748427673
        },
        "train": {
            "accuracy": 0.6509330406147091,
            "auc_prc": 0.6253339950753558,
            "auditor_fn_violation": 0.02246477366084681,
            "auditor_fp_violation": 0.020565338135537493,
            "ave_precision_score": 0.620370721349099,
            "fpr": 0.20965971459934138,
            "logloss": 2.412019354237379,
            "mae": 0.3562778113139132,
            "precision": 0.6469500924214417,
            "recall": 0.7337526205450734
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5383771929824561,
            "auc_prc": 0.6929119478640637,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.007191470054446461,
            "ave_precision_score": 0.6137275902555447,
            "fpr": 0.45394736842105265,
            "logloss": 8.079476692761467,
            "mae": 0.45867409451531693,
            "precision": 0.5316742081447964,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5642151481888035,
            "auc_prc": 0.7060622389438378,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.010961772903630495,
            "ave_precision_score": 0.6260644287915954,
            "fpr": 0.43468715697036225,
            "logloss": 7.838556435428496,
            "mae": 0.43670730289443516,
            "precision": 0.5458715596330275,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 6832,
        "test": {
            "accuracy": 0.4407894736842105,
            "auc_prc": 0.6048184123334589,
            "auditor_fn_violation": 0.008240924638640632,
            "auditor_fp_violation": 0.01823200241984271,
            "ave_precision_score": 0.6011322480548266,
            "fpr": 0.05921052631578947,
            "logloss": 7.257173526364015,
            "mae": 0.5607499721561789,
            "precision": 0.28,
            "recall": 0.0440251572327044
        },
        "train": {
            "accuracy": 0.4270032930845225,
            "auc_prc": 0.5949390660167646,
            "auditor_fn_violation": 0.013211459289789165,
            "auditor_fp_violation": 0.01130575101043569,
            "ave_precision_score": 0.5845416183579957,
            "fpr": 0.07354555433589462,
            "logloss": 7.298176105088697,
            "mae": 0.5720843429053215,
            "precision": 0.24719101123595505,
            "recall": 0.04612159329140461
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.6778973212995724,
            "auditor_fn_violation": 0.013852109308911698,
            "auditor_fp_violation": 0.00862068965517242,
            "ave_precision_score": 0.6781426309266729,
            "fpr": 0.16447368421052633,
            "logloss": 1.8040928522315722,
            "mae": 0.31405245761766376,
            "precision": 0.7064579256360078,
            "recall": 0.7568134171907757
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7189110604330331,
            "auditor_fn_violation": 0.022685693377241127,
            "auditor_fp_violation": 0.016073388740787207,
            "ave_precision_score": 0.7185376541159683,
            "fpr": 0.17014270032930845,
            "logloss": 1.3535074848254687,
            "mae": 0.30738513176825566,
            "precision": 0.7053231939163498,
            "recall": 0.7777777777777778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.6756577272470232,
            "auditor_fn_violation": 0.015578450844091362,
            "auditor_fp_violation": 0.0040255091752369465,
            "ave_precision_score": 0.6738650970587331,
            "fpr": 0.16337719298245615,
            "logloss": 2.0103604008564164,
            "mae": 0.30991871700192425,
            "precision": 0.7072691552062869,
            "recall": 0.7547169811320755
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.6690674822799645,
            "auditor_fn_violation": 0.01664952237617565,
            "auditor_fp_violation": 0.01258555190781387,
            "ave_precision_score": 0.6639856365586203,
            "fpr": 0.18660812294182216,
            "logloss": 2.2360488919957833,
            "mae": 0.3143384684950443,
            "precision": 0.6869244935543278,
            "recall": 0.7819706498951782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5570175438596491,
            "auc_prc": 0.6536987380193163,
            "auditor_fn_violation": 0.0021355143624259814,
            "auditor_fp_violation": 0.012724339584593682,
            "ave_precision_score": 0.599002479993469,
            "fpr": 0.43201754385964913,
            "logloss": 6.712449869276811,
            "mae": 0.44201018206044973,
            "precision": 0.5423925667828107,
            "recall": 0.9790356394129979
        },
        "train": {
            "accuracy": 0.5773874862788145,
            "auc_prc": 0.6661652878722369,
            "auditor_fn_violation": 0.0007041815960068761,
            "auditor_fp_violation": 0.013205218350220314,
            "ave_precision_score": 0.6121850428195179,
            "fpr": 0.42041712403951703,
            "logloss": 6.450482415980466,
            "mae": 0.42060315129675124,
            "precision": 0.5536130536130536,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.6741688636313201,
            "auditor_fn_violation": 0.01923342160432528,
            "auditor_fp_violation": 0.005709316394434362,
            "ave_precision_score": 0.6724371559783984,
            "fpr": 0.16447368421052633,
            "logloss": 2.025149055850194,
            "mae": 0.3100360621035903,
            "precision": 0.708171206225681,
            "recall": 0.7631027253668763
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.6679896888487244,
            "auditor_fn_violation": 0.01821897286139359,
            "auditor_fp_violation": 0.015489131809375428,
            "ave_precision_score": 0.6629069916190831,
            "fpr": 0.18880351262349068,
            "logloss": 2.2514410788365957,
            "mae": 0.3146934695749225,
            "precision": 0.6838235294117647,
            "recall": 0.779874213836478
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5460526315789473,
            "auc_prc": 0.664252562206641,
            "auditor_fn_violation": 0.003930817610062894,
            "auditor_fp_violation": 0.009127344222625532,
            "ave_precision_score": 0.5956845569128318,
            "fpr": 0.4407894736842105,
            "logloss": 7.455536658567657,
            "mae": 0.45163797990739357,
            "precision": 0.5363321799307958,
            "recall": 0.9748427672955975
        },
        "train": {
            "accuracy": 0.5675082327113062,
            "auc_prc": 0.6771330499717145,
            "auditor_fn_violation": 0.001532630532485554,
            "auditor_fp_violation": 0.01056720978111864,
            "ave_precision_score": 0.610932423832499,
            "fpr": 0.429198682766191,
            "logloss": 7.053981903952471,
            "mae": 0.4301713308713532,
            "precision": 0.5479768786127167,
            "recall": 0.9937106918238994
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5537280701754386,
            "auc_prc": 0.6700131398125239,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.01062462189957653,
            "ave_precision_score": 0.6026136122318551,
            "fpr": 0.43859649122807015,
            "logloss": 7.382388984524295,
            "mae": 0.4485910808169111,
            "precision": 0.5402298850574713,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5729967069154775,
            "auc_prc": 0.6752951209808437,
            "auditor_fn_violation": 0.0013255182983658844,
            "auditor_fp_violation": 0.013030700046032375,
            "ave_precision_score": 0.6113893399095497,
            "fpr": 0.4226125137211855,
            "logloss": 7.08103518922938,
            "mae": 0.4262042521208176,
            "precision": 0.5512820512820513,
            "recall": 0.9916142557651991
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.6948747672825218,
            "auditor_fn_violation": 0.016605980359704304,
            "auditor_fp_violation": 0.0012149626940915546,
            "ave_precision_score": 0.6962168178968415,
            "fpr": 0.10307017543859649,
            "logloss": 1.4895115695092276,
            "mae": 0.3262471945561052,
            "precision": 0.7558441558441559,
            "recall": 0.610062893081761
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7393242383022176,
            "auditor_fn_violation": 0.0168865508218904,
            "auditor_fp_violation": 0.006995907672229333,
            "ave_precision_score": 0.7390607824561647,
            "fpr": 0.10537870472008781,
            "logloss": 1.3153765044351498,
            "mae": 0.31942823374244694,
            "precision": 0.7538461538461538,
            "recall": 0.6163522012578616
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8310391667701071,
            "auditor_fn_violation": 0.017762238405237416,
            "auditor_fp_violation": 0.018799153055051428,
            "ave_precision_score": 0.8313905004708372,
            "fpr": 0.13486842105263158,
            "logloss": 0.8195640394928795,
            "mae": 0.26786356797444694,
            "precision": 0.7525150905432596,
            "recall": 0.7840670859538784
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8329196100246035,
            "auditor_fn_violation": 0.01198949710848309,
            "auditor_fp_violation": 0.018650695291040895,
            "ave_precision_score": 0.8331786825844583,
            "fpr": 0.15697036223929747,
            "logloss": 0.9296247805864173,
            "mae": 0.27433572507526655,
            "precision": 0.7260536398467433,
            "recall": 0.7945492662473794
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5296052631578947,
            "auc_prc": 0.7604974138428633,
            "auditor_fn_violation": 0.0018688624075913053,
            "auditor_fp_violation": 0.0026441822948175195,
            "ave_precision_score": 0.6162073449829248,
            "fpr": 0.4649122807017544,
            "logloss": 11.123280189494436,
            "mae": 0.4694804533233379,
            "precision": 0.5267857142857143,
            "recall": 0.989517819706499
        },
        "train": {
            "accuracy": 0.5466520307354555,
            "auc_prc": 0.7749397591408544,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0024609610141283944,
            "ave_precision_score": 0.6254426386499194,
            "fpr": 0.45334796926454446,
            "logloss": 10.936270438483866,
            "mae": 0.45192931829208666,
            "precision": 0.5359550561797752,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.7562711837278075,
            "auditor_fn_violation": 0.00032641877229762063,
            "auditor_fp_violation": 0.003624722726356112,
            "ave_precision_score": 0.5254852505600158,
            "fpr": 0.46271929824561403,
            "logloss": 15.943339110089466,
            "mae": 0.4726996884347774,
            "precision": 0.5253093363329584,
            "recall": 0.9790356394129979
        },
        "train": {
            "accuracy": 0.5455543358946213,
            "auc_prc": 0.7695130165911032,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.003930455720406526,
            "ave_precision_score": 0.5409775106412289,
            "fpr": 0.45334796926454446,
            "logloss": 15.357262358457671,
            "mae": 0.4556429111275094,
            "precision": 0.5354330708661418,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5460526315789473,
            "auc_prc": 0.6891720031317436,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.009588626739261957,
            "ave_precision_score": 0.6125401391581,
            "fpr": 0.44627192982456143,
            "logloss": 7.873576814378859,
            "mae": 0.4535480155207099,
            "precision": 0.5359179019384265,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5642151481888035,
            "auc_prc": 0.6976615143915943,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.011255165994729041,
            "ave_precision_score": 0.620886982929273,
            "fpr": 0.433589462129528,
            "logloss": 7.661919042458776,
            "mae": 0.4328903282517378,
            "precision": 0.5459770114942529,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5372807017543859,
            "auc_prc": 0.7375657477298362,
            "auditor_fn_violation": 0.0018228879326198098,
            "auditor_fp_violation": 0.008303085299455547,
            "ave_precision_score": 0.6725997949582183,
            "fpr": 0.45394736842105265,
            "logloss": 6.906876498497003,
            "mae": 0.45925875600613697,
            "precision": 0.5311438278595696,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.5729967069154775,
            "auc_prc": 0.7528672947525983,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.013920996322469376,
            "ave_precision_score": 0.6871471472194035,
            "fpr": 0.42590559824368823,
            "logloss": 6.657829884252719,
            "mae": 0.42916661862448635,
            "precision": 0.5509259259259259,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5416666666666666,
            "auc_prc": 0.6906721596681296,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.0075922565033272795,
            "ave_precision_score": 0.6140214003945965,
            "fpr": 0.4506578947368421,
            "logloss": 7.899172253138509,
            "mae": 0.4558914909408551,
            "precision": 0.5334846765039728,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.562019758507135,
            "auc_prc": 0.6998043654299786,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.011475210813052961,
            "ave_precision_score": 0.6233426396590677,
            "fpr": 0.43578485181119647,
            "logloss": 7.662382952632349,
            "mae": 0.43480944935923227,
            "precision": 0.5447247706422018,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 6832,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.616745663071214,
            "auditor_fn_violation": 0.023295266468056942,
            "auditor_fp_violation": 0.036388384754990934,
            "ave_precision_score": 0.5969388980785114,
            "fpr": 0.19407894736842105,
            "logloss": 3.307047736985242,
            "mae": 0.3440760877293886,
            "precision": 0.665406427221172,
            "recall": 0.7379454926624738
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.6199006485328793,
            "auditor_fn_violation": 0.009651430109976595,
            "auditor_fp_violation": 0.02627385715803266,
            "ave_precision_score": 0.5981184856430152,
            "fpr": 0.21734357848518113,
            "logloss": 3.323690729976678,
            "mae": 0.3417145187884396,
            "precision": 0.6544502617801047,
            "recall": 0.7861635220125787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8202590033236038,
            "auditor_fn_violation": 0.020819540990841885,
            "auditor_fp_violation": 0.014839181286549711,
            "ave_precision_score": 0.8206286170515904,
            "fpr": 0.12719298245614036,
            "logloss": 0.9251149073074354,
            "mae": 0.26927664058431033,
            "precision": 0.7578288100208769,
            "recall": 0.7610062893081762
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8207583276462148,
            "auditor_fn_violation": 0.015457476406464669,
            "auditor_fp_violation": 0.02198677707689429,
            "ave_precision_score": 0.8211221099648784,
            "fpr": 0.150384193194292,
            "logloss": 1.0355424241817008,
            "mae": 0.2755542007939091,
            "precision": 0.730844793713163,
            "recall": 0.779874213836478
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.6770193927867829,
            "auditor_fn_violation": 0.016992165949464864,
            "auditor_fp_violation": 0.0031155474894131933,
            "ave_precision_score": 0.677210352645771,
            "fpr": 0.10855263157894737,
            "logloss": 1.8556154500802595,
            "mae": 0.3317824811595463,
            "precision": 0.7455012853470437,
            "recall": 0.6079664570230608
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7157361831851848,
            "auditor_fn_violation": 0.012774222351092062,
            "auditor_fp_violation": 0.0068239186188267355,
            "ave_precision_score": 0.7152442895855223,
            "fpr": 0.11086717892425905,
            "logloss": 1.5190481981248858,
            "mae": 0.32501807797335663,
            "precision": 0.7436548223350253,
            "recall": 0.6142557651991615
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5515350877192983,
            "auc_prc": 0.6305142615045876,
            "auditor_fn_violation": 0.002018279451248667,
            "auditor_fp_violation": 0.01150433555152249,
            "ave_precision_score": 0.5866591009838459,
            "fpr": 0.43969298245614036,
            "logloss": 6.202285181467705,
            "mae": 0.4459768868920204,
            "precision": 0.539080459770115,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.5784851811196488,
            "auc_prc": 0.63704351066759,
            "auditor_fn_violation": 0.0014774006033869756,
            "auditor_fp_violation": 0.011702843383732875,
            "ave_precision_score": 0.5937390730397932,
            "fpr": 0.41822173435784854,
            "logloss": 6.100946085630785,
            "mae": 0.42384218607215324,
            "precision": 0.5543859649122806,
            "recall": 0.9937106918238994
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.6767174253136717,
            "auditor_fn_violation": 0.02490207436831071,
            "auditor_fp_violation": 0.03607834240774349,
            "ave_precision_score": 0.6638934538247825,
            "fpr": 0.16228070175438597,
            "logloss": 2.410752358192684,
            "mae": 0.3068645630631647,
            "precision": 0.7103718199608611,
            "recall": 0.7610062893081762
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.6891128811420077,
            "auditor_fn_violation": 0.007163782053494795,
            "auditor_fp_violation": 0.040956157966887057,
            "ave_precision_score": 0.6759736944511879,
            "fpr": 0.17233809001097694,
            "logloss": 2.4628976328696455,
            "mae": 0.3054942934318999,
            "precision": 0.704331450094162,
            "recall": 0.7840670859538784
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5394736842105263,
            "auc_prc": 0.7108891181329765,
            "auditor_fn_violation": 0.002797546802015521,
            "auditor_fp_violation": 0.00785692679975802,
            "ave_precision_score": 0.6238694507570156,
            "fpr": 0.45285087719298245,
            "logloss": 8.304857477711492,
            "mae": 0.46132875709135135,
            "precision": 0.5322763306908267,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5609220636663008,
            "auc_prc": 0.7148724976515336,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.010498920009914672,
            "ave_precision_score": 0.6277240267138386,
            "fpr": 0.43798024149286496,
            "logloss": 8.182606276192418,
            "mae": 0.43904411373693236,
            "precision": 0.544,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5361842105263158,
            "auc_prc": 0.6801075116086899,
            "auditor_fn_violation": 0.0020067858325057927,
            "auditor_fp_violation": 0.006745311554748944,
            "ave_precision_score": 0.5995049501080492,
            "fpr": 0.45723684210526316,
            "logloss": 8.347371501677486,
            "mae": 0.46585740527097796,
            "precision": 0.5304054054054054,
            "recall": 0.9874213836477987
        },
        "train": {
            "accuracy": 0.5466520307354555,
            "auc_prc": 0.690667793364895,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.0059993828628083735,
            "ave_precision_score": 0.6064636770690797,
            "fpr": 0.4522502744237102,
            "logloss": 8.422585809191121,
            "mae": 0.4550810088081104,
            "precision": 0.536036036036036,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5416666666666666,
            "auc_prc": 0.6797892077646813,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.007834240774349664,
            "ave_precision_score": 0.6107157815710565,
            "fpr": 0.4506578947368421,
            "logloss": 7.498818407191639,
            "mae": 0.45401806794101457,
            "precision": 0.5334846765039728,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.6840833659744735,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.013025641544461704,
            "ave_precision_score": 0.6168883367067927,
            "fpr": 0.42590559824368823,
            "logloss": 7.286746122784902,
            "mae": 0.4285922798565075,
            "precision": 0.5504055619930475,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5625,
            "auc_prc": 0.6585473046847601,
            "auditor_fn_violation": 0.002018279451248667,
            "auditor_fp_violation": 0.01210425489009881,
            "ave_precision_score": 0.6062452840585641,
            "fpr": 0.42872807017543857,
            "logloss": 6.45117946386295,
            "mae": 0.439604513408823,
            "precision": 0.5453488372093023,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.58397365532382,
            "auc_prc": 0.661942032853335,
            "auditor_fn_violation": 0.0011736359933447935,
            "auditor_fp_violation": 0.016460364110943065,
            "ave_precision_score": 0.6100269000497323,
            "fpr": 0.4127332601536773,
            "logloss": 6.325466079671654,
            "mae": 0.416274816189772,
            "precision": 0.5576470588235294,
            "recall": 0.9937106918238994
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5471491228070176,
            "auc_prc": 0.6925576355442644,
            "auditor_fn_violation": 0.0030228217293758507,
            "auditor_fp_violation": 0.01088929219600726,
            "ave_precision_score": 0.6466635407352146,
            "fpr": 0.4440789473684211,
            "logloss": 6.025377331869575,
            "mae": 0.45002615210728236,
            "precision": 0.5366132723112128,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.5762897914379802,
            "auc_prc": 0.7059978420206976,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.018281424676382378,
            "ave_precision_score": 0.6627902326347546,
            "fpr": 0.42151481888035125,
            "logloss": 5.670334940947831,
            "mae": 0.4210950318350158,
            "precision": 0.5529685681024447,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5416666666666666,
            "auc_prc": 0.6945652980411168,
            "auditor_fn_violation": 0.0026849093383353563,
            "auditor_fp_violation": 0.0075922565033272795,
            "ave_precision_score": 0.6167389604881569,
            "fpr": 0.4506578947368421,
            "logloss": 8.00851252362678,
            "mae": 0.4562226923080889,
            "precision": 0.5334846765039728,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5642151481888035,
            "auc_prc": 0.7034296866307735,
            "auditor_fn_violation": 0.0009757287474082205,
            "auditor_fp_violation": 0.012957351773257726,
            "ave_precision_score": 0.6235490785574772,
            "fpr": 0.433589462129528,
            "logloss": 7.828205354241467,
            "mae": 0.43498114498307006,
            "precision": 0.5459770114942529,
            "recall": 0.9958071278825996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.6645871997967193,
            "auditor_fn_violation": 0.002018279451248667,
            "auditor_fp_violation": 0.011887477313974595,
            "ave_precision_score": 0.6003774117531065,
            "fpr": 0.43640350877192985,
            "logloss": 7.179900673377044,
            "mae": 0.44547550180498907,
            "precision": 0.5409457900807382,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.6718751145272376,
            "auditor_fn_violation": 0.0015648479911263915,
            "auditor_fp_violation": 0.01488211162089568,
            "ave_precision_score": 0.6094317708845921,
            "fpr": 0.42371020856201974,
            "logloss": 6.926868162155977,
            "mae": 0.42593314507601293,
            "precision": 0.5501165501165501,
            "recall": 0.989517819706499
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5350877192982456,
            "auc_prc": 0.7248592856812538,
            "auditor_fn_violation": 0.002797546802015521,
            "auditor_fp_violation": 0.005459770114942523,
            "ave_precision_score": 0.6407928003735579,
            "fpr": 0.45723684210526316,
            "logloss": 8.249986849765829,
            "mae": 0.4619425279678365,
            "precision": 0.5298759864712514,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5598243688254665,
            "auc_prc": 0.7348532627695549,
            "auditor_fn_violation": 0.0005361905616653664,
            "auditor_fp_violation": 0.008892845761228618,
            "ave_precision_score": 0.6543924686547512,
            "fpr": 0.43907793633369924,
            "logloss": 7.889800097351768,
            "mae": 0.4391979705218475,
            "precision": 0.54337899543379,
            "recall": 0.9979035639412998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5350877192982456,
            "auc_prc": 0.6507482880889996,
            "auditor_fn_violation": 0.002797546802015521,
            "auditor_fp_violation": 0.005921052631578967,
            "ave_precision_score": 0.593184423571438,
            "fpr": 0.45723684210526316,
            "logloss": 7.325578786602436,
            "mae": 0.46219015779555367,
            "precision": 0.5298759864712514,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.5565312843029637,
            "auc_prc": 0.6473943936512455,
            "auditor_fn_violation": 0.0010355611705983475,
            "auditor_fp_violation": 0.008270650068036862,
            "ave_precision_score": 0.593468250699064,
            "fpr": 0.44017563117453345,
            "logloss": 7.190373727964181,
            "mae": 0.44243473316194043,
            "precision": 0.5417142857142857,
            "recall": 0.9937106918238994
        }
    }
]
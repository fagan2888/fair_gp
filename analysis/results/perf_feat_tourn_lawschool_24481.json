[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.8055350937994666,
            "auditor_fn_violation": 0.006533355217565745,
            "auditor_fp_violation": 0.010753755037245096,
            "ave_precision_score": 0.8059535137952418,
            "fpr": 0.23464912280701755,
            "logloss": 0.9985010977707924,
            "mae": 0.2939897945945579,
            "precision": 0.6687306501547987,
            "recall": 0.8981288981288982
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7900810712643339,
            "auditor_fn_violation": 0.009180720486977349,
            "auditor_fp_violation": 0.008310402037000836,
            "ave_precision_score": 0.7908034228107127,
            "fpr": 0.23600439077936333,
            "logloss": 1.0167042949690972,
            "mae": 0.2968254595347018,
            "precision": 0.6666666666666666,
            "recall": 0.9090909090909091
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8078622703929859,
            "auditor_fn_violation": 0.004299339825655617,
            "auditor_fp_violation": 0.017218219562828183,
            "ave_precision_score": 0.8082264940182907,
            "fpr": 0.22149122807017543,
            "logloss": 1.010322498101743,
            "mae": 0.2910037296149104,
            "precision": 0.6778309409888357,
            "recall": 0.8835758835758836
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7903973495093526,
            "auditor_fn_violation": 0.010905006463171527,
            "auditor_fp_violation": 0.010225102626949168,
            "ave_precision_score": 0.7910720995639738,
            "fpr": 0.22722283205268934,
            "logloss": 1.040666627964632,
            "mae": 0.29405832890623657,
            "precision": 0.6729857819905213,
            "recall": 0.9006342494714588
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7541185168347933,
            "auditor_fn_violation": 0.011306853412116574,
            "auditor_fp_violation": 0.013152806610493756,
            "ave_precision_score": 0.7547691440966106,
            "fpr": 0.21052631578947367,
            "logloss": 1.0301032795488085,
            "mae": 0.3127063995143855,
            "precision": 0.6745762711864407,
            "recall": 0.8274428274428275
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7362323733509577,
            "auditor_fn_violation": 0.011348261673740957,
            "auditor_fp_violation": 0.004977219072823781,
            "ave_precision_score": 0.7374538186707826,
            "fpr": 0.21185510428100987,
            "logloss": 1.0468198724100535,
            "mae": 0.3133768834892119,
            "precision": 0.6666666666666666,
            "recall": 0.8160676532769556
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8008618788065079,
            "auditor_fn_violation": 0.006619980304190832,
            "auditor_fp_violation": 0.00962164692473644,
            "ave_precision_score": 0.8013192097699648,
            "fpr": 0.23574561403508773,
            "logloss": 0.9914807624352246,
            "mae": 0.2951120227746675,
            "precision": 0.6666666666666666,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7880966914833654,
            "auditor_fn_violation": 0.00958452366309819,
            "auditor_fp_violation": 8.019688334862183e-05,
            "ave_precision_score": 0.7885868471642334,
            "fpr": 0.2283205268935236,
            "logloss": 1.002751483400448,
            "mae": 0.2961982212704169,
            "precision": 0.6729559748427673,
            "recall": 0.904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8003368166883433,
            "auditor_fn_violation": 0.006619980304190832,
            "auditor_fp_violation": 0.00962164692473644,
            "ave_precision_score": 0.8007826900772715,
            "fpr": 0.23574561403508773,
            "logloss": 0.9918819366480487,
            "mae": 0.2954006515406792,
            "precision": 0.6666666666666666,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7876734146914296,
            "auditor_fn_violation": 0.00958452366309819,
            "auditor_fp_violation": 8.019688334862183e-05,
            "ave_precision_score": 0.7882471242779993,
            "fpr": 0.2283205268935236,
            "logloss": 1.002487147754547,
            "mae": 0.296550288426367,
            "precision": 0.6729559748427673,
            "recall": 0.904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8521408564697304,
            "auditor_fn_violation": 0.008270416165153009,
            "auditor_fp_violation": 0.021326881589123624,
            "ave_precision_score": 0.8525278453263234,
            "fpr": 0.16337719298245615,
            "logloss": 0.7412397596434144,
            "mae": 0.2614758898389668,
            "precision": 0.7305605786618445,
            "recall": 0.83991683991684
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8281708291156431,
            "auditor_fn_violation": 0.01590845271441601,
            "auditor_fp_violation": 0.02048027908515406,
            "ave_precision_score": 0.8285152530401905,
            "fpr": 0.18441273326015367,
            "logloss": 0.8036754224109199,
            "mae": 0.27297379558414314,
            "precision": 0.7047451669595782,
            "recall": 0.8477801268498943
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6643022558634345,
            "auditor_fn_violation": 0.009013568224094541,
            "auditor_fp_violation": 0.020716306427321207,
            "ave_precision_score": 0.6651977811100952,
            "fpr": 0.3125,
            "logloss": 1.1841514364105692,
            "mae": 0.38072143346289733,
            "precision": 0.6058091286307054,
            "recall": 0.9106029106029107
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.6672453983110054,
            "auditor_fn_violation": 0.013000605704764183,
            "auditor_fp_violation": 0.022014044479196433,
            "ave_precision_score": 0.6687233710998913,
            "fpr": 0.3106476399560922,
            "logloss": 1.1145700162471317,
            "mae": 0.37719769245182727,
            "precision": 0.6041958041958042,
            "recall": 0.9133192389006343
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.798861732732719,
            "auditor_fn_violation": 0.00600676587518693,
            "auditor_fp_violation": 0.011265111735254608,
            "ave_precision_score": 0.7993243636071423,
            "fpr": 0.22478070175438597,
            "logloss": 0.9616512113818289,
            "mae": 0.29340455272511123,
            "precision": 0.6735668789808917,
            "recall": 0.8794178794178794
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7857854991254679,
            "auditor_fn_violation": 0.00967967268735655,
            "auditor_fp_violation": 0.006034815471983723,
            "ave_precision_score": 0.7866191562687069,
            "fpr": 0.21844127332601537,
            "logloss": 0.9678043575239665,
            "mae": 0.2940623813441418,
            "precision": 0.679549114331723,
            "recall": 0.8921775898520085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8518935189707932,
            "auditor_fn_violation": 0.0054391435970383385,
            "auditor_fp_violation": 0.011613648390116832,
            "ave_precision_score": 0.8521427415692092,
            "fpr": 0.16666666666666666,
            "logloss": 0.7312975360841044,
            "mae": 0.2627036846588462,
            "precision": 0.7275985663082437,
            "recall": 0.8440748440748441
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8349955958117197,
            "auditor_fn_violation": 0.010659011424845037,
            "auditor_fp_violation": 0.014375291340240298,
            "ave_precision_score": 0.8352710871231714,
            "fpr": 0.19099890230515917,
            "logloss": 0.7797704024455565,
            "mae": 0.27699830731601244,
            "precision": 0.697391304347826,
            "recall": 0.8477801268498943
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8059084469303938,
            "auditor_fn_violation": 0.007112375533428168,
            "auditor_fp_violation": 0.008761753571864705,
            "ave_precision_score": 0.8062684936254844,
            "fpr": 0.23355263157894737,
            "logloss": 0.9664786284290997,
            "mae": 0.2932771205832771,
            "precision": 0.6682242990654206,
            "recall": 0.8918918918918919
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7906657882710089,
            "auditor_fn_violation": 0.010696142751384884,
            "auditor_fp_violation": 0.009132420091324214,
            "ave_precision_score": 0.7913058038610842,
            "fpr": 0.2283205268935236,
            "logloss": 0.979428195549181,
            "mae": 0.29490935622398684,
            "precision": 0.6724409448818898,
            "recall": 0.9027484143763214
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.7922910647365105,
            "auditor_fn_violation": 0.012772641062114747,
            "auditor_fp_violation": 0.02056875076321896,
            "ave_precision_score": 0.792752101733309,
            "fpr": 0.1699561403508772,
            "logloss": 0.9443143923980697,
            "mae": 0.29015911022501717,
            "precision": 0.7155963302752294,
            "recall": 0.8108108108108109
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7717581899771977,
            "auditor_fn_violation": 0.01244595651457521,
            "auditor_fp_violation": 0.010109819607135528,
            "ave_precision_score": 0.7725724621904257,
            "fpr": 0.18660812294182216,
            "logloss": 0.9944977237463819,
            "mae": 0.2975906743742441,
            "precision": 0.6925858951175407,
            "recall": 0.8097251585623678
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8481150263635248,
            "auditor_fn_violation": 0.0064854834591676755,
            "auditor_fp_violation": 0.020853685838726748,
            "ave_precision_score": 0.8483895689596891,
            "fpr": 0.14802631578947367,
            "logloss": 0.7860129492660387,
            "mae": 0.26121830593326,
            "precision": 0.7438330170777988,
            "recall": 0.814968814968815
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8234297585668919,
            "auditor_fn_violation": 0.006398191704397518,
            "auditor_fp_violation": 0.013949245397450747,
            "ave_precision_score": 0.8237038773055642,
            "fpr": 0.1734357848518112,
            "logloss": 0.8649695509248119,
            "mae": 0.27456403075422214,
            "precision": 0.7111517367458866,
            "recall": 0.8224101479915433
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8143393928927766,
            "auditor_fn_violation": 0.004415599810336653,
            "auditor_fp_violation": 0.009458826881589125,
            "ave_precision_score": 0.8146616521441787,
            "fpr": 0.20394736842105263,
            "logloss": 0.9052834094272668,
            "mae": 0.2838890923301676,
            "precision": 0.6910299003322259,
            "recall": 0.8648648648648649
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.7946344815783222,
            "auditor_fn_violation": 0.0075933562773988625,
            "auditor_fp_violation": 0.005834323263612168,
            "ave_precision_score": 0.7950492482335316,
            "fpr": 0.20856201975850713,
            "logloss": 0.9337831505498997,
            "mae": 0.2891747226551221,
            "precision": 0.6843853820598007,
            "recall": 0.8710359408033826
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.809968689440039,
            "auditor_fn_violation": 0.007126053178684762,
            "auditor_fp_violation": 0.02189675174013922,
            "ave_precision_score": 0.8103162497833913,
            "fpr": 0.19956140350877194,
            "logloss": 0.896602029857902,
            "mae": 0.2947087999434697,
            "precision": 0.689419795221843,
            "recall": 0.83991683991684
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7813010309570719,
            "auditor_fn_violation": 0.005363155977099258,
            "auditor_fp_violation": 0.00983414282062464,
            "ave_precision_score": 0.7820519032973878,
            "fpr": 0.1942919868276619,
            "logloss": 0.9045967693123522,
            "mae": 0.2849561323005656,
            "precision": 0.6932409012131716,
            "recall": 0.8456659619450317
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8054164453437167,
            "auditor_fn_violation": 0.007112375533428168,
            "auditor_fp_violation": 0.008761753571864705,
            "ave_precision_score": 0.8058442520677903,
            "fpr": 0.23355263157894737,
            "logloss": 0.9659695980261848,
            "mae": 0.2934789447152078,
            "precision": 0.6682242990654206,
            "recall": 0.8918918918918919
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7900099662453041,
            "auditor_fn_violation": 0.010874837260357902,
            "auditor_fp_violation": 0.009225147737696041,
            "ave_precision_score": 0.7907038117213812,
            "fpr": 0.22722283205268934,
            "logloss": 0.9786050528814745,
            "mae": 0.2950280834644256,
            "precision": 0.6724683544303798,
            "recall": 0.8985200845665962
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8009332190840612,
            "auditor_fn_violation": 0.006619980304190832,
            "auditor_fp_violation": 0.00912809866894615,
            "ave_precision_score": 0.8013177759567743,
            "fpr": 0.23464912280701755,
            "logloss": 0.9788631445424189,
            "mae": 0.2950212521948281,
            "precision": 0.6677018633540373,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7893270325582269,
            "auditor_fn_violation": 0.00958452366309819,
            "auditor_fp_violation": 0.0013458039486940417,
            "ave_precision_score": 0.789931027939045,
            "fpr": 0.22941822173435786,
            "logloss": 0.9894503589943214,
            "mae": 0.2962146813650835,
            "precision": 0.6718995290423861,
            "recall": 0.904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8022307089209557,
            "auditor_fn_violation": 0.006619980304190832,
            "auditor_fp_violation": 0.00962164692473644,
            "ave_precision_score": 0.8026751356345859,
            "fpr": 0.23574561403508773,
            "logloss": 0.9782377195118266,
            "mae": 0.2949066446030781,
            "precision": 0.6666666666666666,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7898984021714222,
            "auditor_fn_violation": 0.00958452366309819,
            "auditor_fp_violation": 0.0006315504563703838,
            "ave_precision_score": 0.7905378323418842,
            "fpr": 0.22722283205268934,
            "logloss": 0.9885624703294901,
            "mae": 0.29595786861165396,
            "precision": 0.6740157480314961,
            "recall": 0.904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8008172462879539,
            "auditor_fn_violation": 0.006619980304190832,
            "auditor_fp_violation": 0.00962164692473644,
            "ave_precision_score": 0.8011997904327482,
            "fpr": 0.23574561403508773,
            "logloss": 0.9900041917524833,
            "mae": 0.2950339055344377,
            "precision": 0.6666666666666666,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7878732141238594,
            "auditor_fn_violation": 0.00958452366309819,
            "auditor_fp_violation": 8.019688334862183e-05,
            "ave_precision_score": 0.7886049736851375,
            "fpr": 0.2283205268935236,
            "logloss": 1.0009418501268759,
            "mae": 0.29609860485425876,
            "precision": 0.6729559748427673,
            "recall": 0.904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8368194825843034,
            "auditor_fn_violation": 0.007511306853412116,
            "auditor_fp_violation": 0.023629258761753574,
            "ave_precision_score": 0.8371078001093316,
            "fpr": 0.12828947368421054,
            "logloss": 0.7721982871226694,
            "mae": 0.2728141122736972,
            "precision": 0.7607361963190185,
            "recall": 0.7733887733887734
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.8107115364227093,
            "auditor_fn_violation": 0.009686634811082772,
            "auditor_fp_violation": 0.007438260930584584,
            "ave_precision_score": 0.8110343239439581,
            "fpr": 0.1525795828759605,
            "logloss": 0.8400364167472029,
            "mae": 0.29311351575855565,
            "precision": 0.722,
            "recall": 0.7632135306553911
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7985564487325453,
            "auditor_fn_violation": 0.004276543750227963,
            "auditor_fp_violation": 0.011265111735254608,
            "ave_precision_score": 0.7989985752809712,
            "fpr": 0.22478070175438597,
            "logloss": 0.9616988999040217,
            "mae": 0.29330490164488343,
            "precision": 0.6730462519936204,
            "recall": 0.8773388773388774
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7851679857759987,
            "auditor_fn_violation": 0.00903683659663544,
            "auditor_fp_violation": 0.008104897523420002,
            "ave_precision_score": 0.7859715009405859,
            "fpr": 0.22063666300768386,
            "logloss": 0.966810577347903,
            "mae": 0.2938993900281978,
            "precision": 0.6768488745980707,
            "recall": 0.8900634249471459
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8200352353061483,
            "auditor_fn_violation": 0.006369223474486633,
            "auditor_fp_violation": 0.012007978182114225,
            "ave_precision_score": 0.8203856630260555,
            "fpr": 0.17982456140350878,
            "logloss": 0.8886539360084807,
            "mae": 0.276735923790759,
            "precision": 0.7132867132867133,
            "recall": 0.8482328482328483
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7961125428497938,
            "auditor_fn_violation": 0.0076908260095659604,
            "auditor_fp_violation": 0.0009773995158113165,
            "ave_precision_score": 0.7963777350023925,
            "fpr": 0.2030735455543359,
            "logloss": 0.9572252940671768,
            "mae": 0.28866713848597947,
            "precision": 0.6826758147512865,
            "recall": 0.8414376321353065
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8429539118229133,
            "auditor_fn_violation": 0.005477896925265349,
            "auditor_fp_violation": 0.010344160866202637,
            "ave_precision_score": 0.843207606164419,
            "fpr": 0.19956140350877194,
            "logloss": 0.8125405851004945,
            "mae": 0.2770919731362779,
            "precision": 0.7016393442622951,
            "recall": 0.8898128898128899
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8248684440694046,
            "auditor_fn_violation": 0.011002476195338626,
            "auditor_fp_violation": 0.014084577638101548,
            "ave_precision_score": 0.8251963675861385,
            "fpr": 0.21295279912184412,
            "logloss": 0.8593404980464538,
            "mae": 0.2862266195261918,
            "precision": 0.6830065359477124,
            "recall": 0.8837209302325582
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7574531005984428,
            "auditor_fn_violation": 0.009542437174016124,
            "auditor_fp_violation": 0.021680506370334193,
            "ave_precision_score": 0.7578250604264519,
            "fpr": 0.23574561403508773,
            "logloss": 1.1624354286640723,
            "mae": 0.30770411630109307,
            "precision": 0.6661490683229814,
            "recall": 0.8918918918918919
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7517601111006015,
            "auditor_fn_violation": 0.01033875373343885,
            "auditor_fp_violation": 0.017432797517906464,
            "ave_precision_score": 0.7519363572143865,
            "fpr": 0.24478594950603733,
            "logloss": 1.1519485922728312,
            "mae": 0.3123352212091607,
            "precision": 0.6547987616099071,
            "recall": 0.8942917547568711
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8381117134349307,
            "auditor_fn_violation": 0.010185286501075978,
            "auditor_fp_violation": 0.027496234786502226,
            "ave_precision_score": 0.8384389413421157,
            "fpr": 0.16557017543859648,
            "logloss": 0.7573578945598345,
            "mae": 0.27229739461940666,
            "precision": 0.7259528130671506,
            "recall": 0.8316008316008316
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8104123387513835,
            "auditor_fn_violation": 0.010403733554883584,
            "auditor_fp_violation": 0.023698179029517465,
            "ave_precision_score": 0.8108019156598243,
            "fpr": 0.18221734357848518,
            "logloss": 0.8236189335761296,
            "mae": 0.2787463202943343,
            "precision": 0.7077464788732394,
            "recall": 0.8498942917547568
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8238312502632287,
            "auditor_fn_violation": 0.019584108399897875,
            "auditor_fp_violation": 0.007800097692025899,
            "ave_precision_score": 0.8241100865301543,
            "fpr": 0.12171052631578948,
            "logloss": 1.2685493799146712,
            "mae": 0.29035804905837365,
            "precision": 0.7549668874172185,
            "recall": 0.7110187110187111
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.8072482613738335,
            "auditor_fn_violation": 0.015469838919664063,
            "auditor_fp_violation": 0.024189384940027776,
            "ave_precision_score": 0.8075518309409551,
            "fpr": 0.13611416026344675,
            "logloss": 1.2405737305533213,
            "mae": 0.29672086182751517,
            "precision": 0.7268722466960352,
            "recall": 0.6976744186046512
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8577755793923544,
            "auditor_fn_violation": 0.010668563300142254,
            "auditor_fp_violation": 0.024379757398135714,
            "ave_precision_score": 0.8580218136599309,
            "fpr": 0.15570175438596492,
            "logloss": 0.7100496846296307,
            "mae": 0.2578976630162354,
            "precision": 0.7394495412844037,
            "recall": 0.8378378378378378
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8370237242099241,
            "auditor_fn_violation": 0.014878058402935232,
            "auditor_fp_violation": 0.021304803292082064,
            "ave_precision_score": 0.8373004062451671,
            "fpr": 0.17892425905598244,
            "logloss": 0.7592366510778515,
            "mae": 0.2713239811294353,
            "precision": 0.7130281690140845,
            "recall": 0.8562367864693446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8040767178229749,
            "auditor_fn_violation": 0.005081245212824163,
            "auditor_fp_violation": 0.009270566206700043,
            "ave_precision_score": 0.8044921269031109,
            "fpr": 0.21929824561403508,
            "logloss": 0.9518278967749685,
            "mae": 0.290011297626861,
            "precision": 0.6789727126805778,
            "recall": 0.8794178794178794
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7886908471964588,
            "auditor_fn_violation": 0.01013453143746969,
            "auditor_fp_violation": 0.009117383175696341,
            "ave_precision_score": 0.7892877023520721,
            "fpr": 0.22283205268935236,
            "logloss": 0.9662127846912935,
            "mae": 0.2934961906313081,
            "precision": 0.6746794871794872,
            "recall": 0.8900634249471459
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.821109135503961,
            "auditor_fn_violation": 0.00786920523762629,
            "auditor_fp_violation": 0.00694274840232833,
            "ave_precision_score": 0.8215044104395318,
            "fpr": 0.15899122807017543,
            "logloss": 0.8118548938576009,
            "mae": 0.2711784622782461,
            "precision": 0.7349177330895795,
            "recall": 0.8357588357588358
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8029795544760701,
            "auditor_fn_violation": 0.014908227605748862,
            "auditor_fp_violation": 0.00920259236425425,
            "ave_precision_score": 0.803645444076868,
            "fpr": 0.17892425905598244,
            "logloss": 0.8494603990949235,
            "mae": 0.2812130553951119,
            "precision": 0.7057761732851986,
            "recall": 0.8266384778012685
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.812384858882095,
            "auditor_fn_violation": 0.00801510012036328,
            "auditor_fp_violation": 0.007026702487076157,
            "ave_precision_score": 0.8127786596919588,
            "fpr": 0.2324561403508772,
            "logloss": 0.9747025949058284,
            "mae": 0.2919007226136402,
            "precision": 0.6697819314641744,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7938974827206999,
            "auditor_fn_violation": 0.009480091807204874,
            "auditor_fp_violation": 0.008255266679698657,
            "ave_precision_score": 0.7941317027945509,
            "fpr": 0.22941822173435786,
            "logloss": 0.99783548532242,
            "mae": 0.29450731971875965,
            "precision": 0.6718995290423861,
            "recall": 0.904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8439811844697831,
            "auditor_fn_violation": 0.007878323667797355,
            "auditor_fp_violation": 0.013737941140554407,
            "ave_precision_score": 0.8442643554184227,
            "fpr": 0.1787280701754386,
            "logloss": 0.7537448751536094,
            "mae": 0.26922751928229005,
            "precision": 0.7179930795847751,
            "recall": 0.8627858627858628
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8237602925678567,
            "auditor_fn_violation": 0.007846313439451573,
            "auditor_fp_violation": 0.012255086236711133,
            "ave_precision_score": 0.8241184399917515,
            "fpr": 0.19758507135016465,
            "logloss": 0.801063118033185,
            "mae": 0.28170772892334117,
            "precision": 0.6923076923076923,
            "recall": 0.8562367864693446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8044486021323276,
            "auditor_fn_violation": 0.006009045482729693,
            "auditor_fp_violation": 0.010624007815362069,
            "ave_precision_score": 0.8048135927918085,
            "fpr": 0.23903508771929824,
            "logloss": 1.030921562834773,
            "mae": 0.2955553457292977,
            "precision": 0.6656441717791411,
            "recall": 0.9022869022869023
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7876230852754382,
            "auditor_fn_violation": 0.009180720486977349,
            "auditor_fp_violation": 0.009859204346671091,
            "ave_precision_score": 0.7886248826944456,
            "fpr": 0.23929747530186607,
            "logloss": 1.0513648552610673,
            "mae": 0.2985041003992407,
            "precision": 0.6635802469135802,
            "recall": 0.9090909090909091
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8603867728323508,
            "auditor_fn_violation": 0.010921599737389215,
            "auditor_fp_violation": 0.023507143729393096,
            "ave_precision_score": 0.8606242683153601,
            "fpr": 0.1611842105263158,
            "logloss": 0.7221535463851543,
            "mae": 0.2587743539248035,
            "precision": 0.7356115107913669,
            "recall": 0.8503118503118503
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8406652412278549,
            "auditor_fn_violation": 0.01538165201913192,
            "auditor_fp_violation": 0.020580525189339838,
            "ave_precision_score": 0.8409282253784913,
            "fpr": 0.18660812294182216,
            "logloss": 0.7719401844978907,
            "mae": 0.2717695279016934,
            "precision": 0.7058823529411765,
            "recall": 0.8625792811839323
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8259508275380676,
            "auditor_fn_violation": 0.008623755334281646,
            "auditor_fp_violation": 0.015160072454919201,
            "ave_precision_score": 0.8263516780092508,
            "fpr": 0.09100877192982457,
            "logloss": 1.0642533603214688,
            "mae": 0.28737374888576456,
            "precision": 0.7965686274509803,
            "recall": 0.6756756756756757
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8073380171605717,
            "auditor_fn_violation": 0.009278190219144456,
            "auditor_fp_violation": 0.012721230621174995,
            "ave_precision_score": 0.8077431887914945,
            "fpr": 0.09879253567508232,
            "logloss": 1.082170945494499,
            "mae": 0.2951270158210392,
            "precision": 0.7788697788697788,
            "recall": 0.6701902748414377
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8246536813493204,
            "auditor_fn_violation": 0.010374493927125509,
            "auditor_fp_violation": 0.010361969308421875,
            "ave_precision_score": 0.8250270479973204,
            "fpr": 0.17324561403508773,
            "logloss": 0.8458943167673315,
            "mae": 0.2728092147262277,
            "precision": 0.7188612099644128,
            "recall": 0.83991683991684
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.8016605938627477,
            "auditor_fn_violation": 0.011951645730013487,
            "auditor_fp_violation": 0.002400894195249345,
            "ave_precision_score": 0.8020135723919372,
            "fpr": 0.18221734357848518,
            "logloss": 0.8991858415923185,
            "mae": 0.28432584721707205,
            "precision": 0.7003610108303249,
            "recall": 0.8202959830866807
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8503749881268463,
            "auditor_fn_violation": 0.009686052449210345,
            "auditor_fp_violation": 0.021339601904994503,
            "ave_precision_score": 0.8506818413998487,
            "fpr": 0.15350877192982457,
            "logloss": 0.6799712031486687,
            "mae": 0.26223005078445033,
            "precision": 0.7397769516728625,
            "recall": 0.8274428274428275
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8346104736972121,
            "auditor_fn_violation": 0.015191353970615197,
            "auditor_fp_violation": 0.01580881063009689,
            "ave_precision_score": 0.8349123787566579,
            "fpr": 0.18221734357848518,
            "logloss": 0.7180650724540093,
            "mae": 0.2762081452434064,
            "precision": 0.7061946902654868,
            "recall": 0.8435517970401691
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8000211279377231,
            "auditor_fn_violation": 0.0076184484079220936,
            "auditor_fp_violation": 0.009204420564171454,
            "ave_precision_score": 0.8004816503751477,
            "fpr": 0.21710526315789475,
            "logloss": 0.9501068081820591,
            "mae": 0.2906536487768946,
            "precision": 0.6801292407108239,
            "recall": 0.8752598752598753
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.787681626840713,
            "auditor_fn_violation": 0.010313225946442706,
            "auditor_fp_violation": 0.0002907137021387585,
            "ave_precision_score": 0.7883567367506402,
            "fpr": 0.21514818880351264,
            "logloss": 0.9581864525940388,
            "mae": 0.2924962831261016,
            "precision": 0.6813008130081301,
            "recall": 0.8858350951374208
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7989561850821991,
            "auditor_fn_violation": 0.009065999197578146,
            "auditor_fp_violation": 0.01310446941018439,
            "ave_precision_score": 0.7993840670407395,
            "fpr": 0.21929824561403508,
            "logloss": 0.9721618206107092,
            "mae": 0.29276243973234184,
            "precision": 0.6784565916398714,
            "recall": 0.8773388773388774
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7852328317617587,
            "auditor_fn_violation": 0.010438544173514691,
            "auditor_fp_violation": 0.012154840132525345,
            "ave_precision_score": 0.7857897242603608,
            "fpr": 0.22502744237102085,
            "logloss": 0.9787222525232679,
            "mae": 0.2959674818103394,
            "precision": 0.6714743589743589,
            "recall": 0.8858350951374208
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8051484154727109,
            "auditor_fn_violation": 0.007112375533428168,
            "auditor_fp_violation": 0.008761753571864705,
            "ave_precision_score": 0.8055726284415134,
            "fpr": 0.23355263157894737,
            "logloss": 0.9674477727988262,
            "mae": 0.29357030903639025,
            "precision": 0.6682242990654206,
            "recall": 0.8918918918918919
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.789864381236461,
            "auditor_fn_violation": 0.010874837260357902,
            "auditor_fp_violation": 0.0059771739620769005,
            "ave_precision_score": 0.7905147014725424,
            "fpr": 0.22722283205268934,
            "logloss": 0.9797541497085108,
            "mae": 0.2950409847659226,
            "precision": 0.6724683544303798,
            "recall": 0.8985200845665962
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.6557811832529388,
            "auditor_fn_violation": 0.0020151730678046507,
            "auditor_fp_violation": 0.016330341515040505,
            "ave_precision_score": 0.6380926574628896,
            "fpr": 0.24342105263157895,
            "logloss": 2.8546080342213718,
            "mae": 0.31140725071042535,
            "precision": 0.6610687022900763,
            "recall": 0.9002079002079002
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6330316561252933,
            "auditor_fn_violation": 0.010364281520434995,
            "auditor_fp_violation": 0.0272619280333218,
            "ave_precision_score": 0.6127618996093258,
            "fpr": 0.2667398463227223,
            "logloss": 3.1662159194957162,
            "mae": 0.3222643463241788,
            "precision": 0.6394658753709199,
            "recall": 0.9112050739957717
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7897139464830368,
            "auditor_fn_violation": 0.004825929168034432,
            "auditor_fp_violation": 0.012468453616640219,
            "ave_precision_score": 0.7901994218278852,
            "fpr": 0.22478070175438597,
            "logloss": 0.9907760290135523,
            "mae": 0.29474265966842345,
            "precision": 0.6746031746031746,
            "recall": 0.8835758835758836
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7785829896459868,
            "auditor_fn_violation": 0.010071872323933695,
            "auditor_fp_violation": 0.013528211759870491,
            "ave_precision_score": 0.7793262343713335,
            "fpr": 0.23161361141602635,
            "logloss": 0.9946656665454426,
            "mae": 0.2984345022360376,
            "precision": 0.667192429022082,
            "recall": 0.8942917547568711
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7990922386253634,
            "auditor_fn_violation": 0.0040554218185797145,
            "auditor_fp_violation": 0.010239854276061387,
            "ave_precision_score": 0.7995286901864858,
            "fpr": 0.21820175438596492,
            "logloss": 0.9479596252104561,
            "mae": 0.29153144125447167,
            "precision": 0.6790322580645162,
            "recall": 0.8752598752598753
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7859486626780134,
            "auditor_fn_violation": 0.009853725780512091,
            "auditor_fp_violation": 0.006310492258494607,
            "ave_precision_score": 0.7866565798008881,
            "fpr": 0.21624588364434688,
            "logloss": 0.9550876477765431,
            "mae": 0.2931378378759694,
            "precision": 0.6796747967479675,
            "recall": 0.8837209302325582
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.773223731464314,
            "auditor_fn_violation": 0.008256738519896417,
            "auditor_fp_violation": 0.02514552041356291,
            "ave_precision_score": 0.7737789330625678,
            "fpr": 0.14802631578947367,
            "logloss": 1.0128556446190125,
            "mae": 0.28004398207685044,
            "precision": 0.7368421052631579,
            "recall": 0.7858627858627859
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7474184382585468,
            "auditor_fn_violation": 0.001213730236271274,
            "auditor_fp_violation": 0.023417489937797298,
            "ave_precision_score": 0.7478610668024632,
            "fpr": 0.1602634467618002,
            "logloss": 1.1547334630488024,
            "mae": 0.2884074581394205,
            "precision": 0.7186897880539499,
            "recall": 0.7885835095137421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.7587888867448132,
            "auditor_fn_violation": 0.012838749680854947,
            "auditor_fp_violation": 0.015648532584361136,
            "ave_precision_score": 0.7593209708288184,
            "fpr": 0.13267543859649122,
            "logloss": 1.025169835685816,
            "mae": 0.33500391725289075,
            "precision": 0.7139479905437353,
            "recall": 0.6278586278586279
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.7301074030763456,
            "auditor_fn_violation": 0.0017544551790078064,
            "auditor_fp_violation": 0.012666095263872811,
            "ave_precision_score": 0.7307790475783197,
            "fpr": 0.132821075740944,
            "logloss": 1.064634527050003,
            "mae": 0.3326948835784501,
            "precision": 0.7152941176470589,
            "recall": 0.642706131078224
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8371861838213285,
            "auditor_fn_violation": 0.007784859758543969,
            "auditor_fp_violation": 0.008085032767533685,
            "ave_precision_score": 0.8375055558557953,
            "fpr": 0.19736842105263158,
            "logloss": 0.8161137310066973,
            "mae": 0.2752638214634857,
            "precision": 0.7009966777408638,
            "recall": 0.8773388773388774
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8157618164306152,
            "auditor_fn_violation": 0.01351580286050457,
            "auditor_fp_violation": 0.010676210095785156,
            "ave_precision_score": 0.8164062870889945,
            "fpr": 0.20417124039517015,
            "logloss": 0.8565829359671593,
            "mae": 0.28284545890616536,
            "precision": 0.693069306930693,
            "recall": 0.8879492600422833
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8409288762956683,
            "auditor_fn_violation": 0.007155688076740712,
            "auditor_fp_violation": 0.01285515121911508,
            "ave_precision_score": 0.8412246882258887,
            "fpr": 0.1699561403508772,
            "logloss": 0.7881492344216403,
            "mae": 0.26756541884093327,
            "precision": 0.7256637168141593,
            "recall": 0.8523908523908524
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8171151785791325,
            "auditor_fn_violation": 0.014137752580047017,
            "auditor_fp_violation": 0.008420672751605199,
            "ave_precision_score": 0.8177528790815367,
            "fpr": 0.1877058177826564,
            "logloss": 0.8465220476313682,
            "mae": 0.2787791241668093,
            "precision": 0.7,
            "recall": 0.8435517970401691
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7964281932327482,
            "auditor_fn_violation": 0.00596117372433162,
            "auditor_fp_violation": 0.00885842797248342,
            "ave_precision_score": 0.7968056906887692,
            "fpr": 0.23135964912280702,
            "logloss": 0.9891792974938807,
            "mae": 0.29580638097320183,
            "precision": 0.6697965571205008,
            "recall": 0.8898128898128899
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7820812260611819,
            "auditor_fn_violation": 0.010236642585454267,
            "auditor_fp_violation": 0.01113232986983044,
            "ave_precision_score": 0.7827694288284307,
            "fpr": 0.23161361141602635,
            "logloss": 0.9969479848390094,
            "mae": 0.29899987443983167,
            "precision": 0.6687598116169545,
            "recall": 0.9006342494714588
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8008618788065079,
            "auditor_fn_violation": 0.006619980304190832,
            "auditor_fp_violation": 0.00962164692473644,
            "ave_precision_score": 0.8013192097699648,
            "fpr": 0.23574561403508773,
            "logloss": 0.9914766027287839,
            "mae": 0.2951117104996811,
            "precision": 0.6666666666666666,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7880966914833654,
            "auditor_fn_violation": 0.00958452366309819,
            "auditor_fp_violation": 8.019688334862183e-05,
            "ave_precision_score": 0.7885868471642334,
            "fpr": 0.2283205268935236,
            "logloss": 1.0027478990262362,
            "mae": 0.2961978989757707,
            "precision": 0.6729559748427673,
            "recall": 0.904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7671789384527402,
            "auditor_fn_violation": 0.008776489039646935,
            "auditor_fp_violation": 0.01646263280009769,
            "ave_precision_score": 0.7676043230684055,
            "fpr": 0.22916666666666666,
            "logloss": 1.1299629773484765,
            "mae": 0.30326630258680853,
            "precision": 0.669826224328594,
            "recall": 0.8814968814968815
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7602855128241912,
            "auditor_fn_violation": 0.011350582381649701,
            "auditor_fp_violation": 0.009330406147091111,
            "ave_precision_score": 0.7606074689540361,
            "fpr": 0.24039517014270034,
            "logloss": 1.1268389598731747,
            "mae": 0.30786259758177387,
            "precision": 0.6572769953051644,
            "recall": 0.8879492600422833
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8005634667305254,
            "auditor_fn_violation": 0.006619980304190832,
            "auditor_fp_violation": 0.00912809866894615,
            "ave_precision_score": 0.8010133991358193,
            "fpr": 0.23464912280701755,
            "logloss": 0.9820291054296962,
            "mae": 0.29504734510116354,
            "precision": 0.6677018633540373,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7888561181758167,
            "auditor_fn_violation": 0.00958452366309819,
            "auditor_fp_violation": 0.003541193630362545,
            "ave_precision_score": 0.7894883257373071,
            "fpr": 0.22941822173435786,
            "logloss": 0.9928400009580205,
            "mae": 0.2961601212077494,
            "precision": 0.6718995290423861,
            "recall": 0.904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7719353431485303,
            "auditor_fn_violation": 0.010392730787467637,
            "auditor_fp_violation": 0.0232654577278463,
            "ave_precision_score": 0.7724772860822036,
            "fpr": 0.17324561403508773,
            "logloss": 0.8021613580466728,
            "mae": 0.3230069625448867,
            "precision": 0.6961538461538461,
            "recall": 0.7525987525987526
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7562019341163315,
            "auditor_fn_violation": 0.015072997867269435,
            "auditor_fp_violation": 0.021816058423429523,
            "ave_precision_score": 0.7567244184978663,
            "fpr": 0.18331503841931943,
            "logloss": 0.8126511987956966,
            "mae": 0.32486705182928716,
            "precision": 0.683111954459203,
            "recall": 0.7610993657505285
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8534813775104754,
            "auditor_fn_violation": 0.003385217201006675,
            "auditor_fp_violation": 0.01787713192493997,
            "ave_precision_score": 0.8537223700289807,
            "fpr": 0.17105263157894737,
            "logloss": 0.7661816592557029,
            "mae": 0.26302426097219145,
            "precision": 0.7286956521739131,
            "recall": 0.8711018711018711
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8317360224398676,
            "auditor_fn_violation": 0.013573820558223081,
            "auditor_fp_violation": 0.01726989759860458,
            "ave_precision_score": 0.8320998897817714,
            "fpr": 0.1942919868276619,
            "logloss": 0.8273007482987811,
            "mae": 0.2771291542948973,
            "precision": 0.6994906621392191,
            "recall": 0.8710359408033826
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.765489725290851,
            "auditor_fn_violation": 0.008776489039646935,
            "auditor_fp_violation": 0.01646263280009769,
            "ave_precision_score": 0.7658869054690396,
            "fpr": 0.22916666666666666,
            "logloss": 1.134349004404407,
            "mae": 0.30384332372606127,
            "precision": 0.669826224328594,
            "recall": 0.8814968814968815
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7587884143247899,
            "auditor_fn_violation": 0.012448277222483947,
            "auditor_fp_violation": 0.010831591557273113,
            "ave_precision_score": 0.759152034217048,
            "fpr": 0.24259055982436883,
            "logloss": 1.1306909942498133,
            "mae": 0.3085254764206516,
            "precision": 0.6552262090483619,
            "recall": 0.8879492600422833
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7910696571605814,
            "auditor_fn_violation": 0.007691395849290587,
            "auditor_fp_violation": 0.013017971262262399,
            "ave_precision_score": 0.7915627703701902,
            "fpr": 0.22039473684210525,
            "logloss": 0.9727720632823121,
            "mae": 0.2980793252076175,
            "precision": 0.6773675762439807,
            "recall": 0.8773388773388774
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7826672108624491,
            "auditor_fn_violation": 0.011352903089558438,
            "auditor_fp_violation": 0.002225463512924237,
            "ave_precision_score": 0.7832940548338209,
            "fpr": 0.2283205268935236,
            "logloss": 0.9760621262602814,
            "mae": 0.2999597272394248,
            "precision": 0.6693163751987281,
            "recall": 0.8900634249471459
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8172525267637315,
            "auditor_fn_violation": 0.008106284422073898,
            "auditor_fp_violation": 0.01038995400333781,
            "ave_precision_score": 0.8176286184899608,
            "fpr": 0.18969298245614036,
            "logloss": 0.9568113269792344,
            "mae": 0.27877356709688217,
            "precision": 0.7032590051457976,
            "recall": 0.8523908523908524
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7926554972629978,
            "auditor_fn_violation": 0.012756931374346431,
            "auditor_fp_violation": 0.0024109188056679236,
            "ave_precision_score": 0.7926670115429701,
            "fpr": 0.2030735455543359,
            "logloss": 1.024975739136867,
            "mae": 0.2882946761722931,
            "precision": 0.685374149659864,
            "recall": 0.8520084566596194
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7825542925116911,
            "auditor_fn_violation": 0.013474760185286504,
            "auditor_fp_violation": 0.01473521390483168,
            "ave_precision_score": 0.7730776763599332,
            "fpr": 0.23355263157894737,
            "logloss": 1.7106981727049395,
            "mae": 0.29812709943055893,
            "precision": 0.6692546583850931,
            "recall": 0.896049896049896
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7640799461637584,
            "auditor_fn_violation": 0.013775722146283505,
            "auditor_fp_violation": 0.008781558726673997,
            "ave_precision_score": 0.7510880798227937,
            "fpr": 0.24039517014270034,
            "logloss": 1.8967475107580465,
            "mae": 0.2983201557570941,
            "precision": 0.6630769230769231,
            "recall": 0.9112050739957717
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8231506390077074,
            "auditor_fn_violation": 0.0038776124302440118,
            "auditor_fp_violation": 0.01019660520210039,
            "ave_precision_score": 0.8233963569606741,
            "fpr": 0.24561403508771928,
            "logloss": 1.092150122013999,
            "mae": 0.2969461022089109,
            "precision": 0.6621417797888386,
            "recall": 0.9126819126819127
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7965312079164777,
            "auditor_fn_violation": 0.008593581386066007,
            "auditor_fp_violation": 0.0032028630287355493,
            "ave_precision_score": 0.796634018121545,
            "fpr": 0.2502744237102086,
            "logloss": 1.1864359161938378,
            "mae": 0.3013178419734242,
            "precision": 0.6555891238670695,
            "recall": 0.9175475687103594
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8100987618349765,
            "auditor_fn_violation": 0.006993835941204366,
            "auditor_fp_violation": 0.009262934017177519,
            "ave_precision_score": 0.8104373225955279,
            "fpr": 0.23026315789473684,
            "logloss": 0.9536707547716627,
            "mae": 0.2914932931344813,
            "precision": 0.6713615023474179,
            "recall": 0.8918918918918919
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7946044779954211,
            "auditor_fn_violation": 0.010285377451537818,
            "auditor_fp_violation": 0.000992436431439187,
            "ave_precision_score": 0.7952809749840886,
            "fpr": 0.2239297475301866,
            "logloss": 0.9726846808458192,
            "mae": 0.2927634931485634,
            "precision": 0.6767036450079239,
            "recall": 0.9027484143763214
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.84803426387144,
            "auditor_fn_violation": 0.00492851150745888,
            "auditor_fp_violation": 0.0074312085317702675,
            "ave_precision_score": 0.8482978960586822,
            "fpr": 0.15570175438596492,
            "logloss": 0.7327332102024656,
            "mae": 0.26157831947057475,
            "precision": 0.7389705882352942,
            "recall": 0.8357588357588358
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8215022847877678,
            "auditor_fn_violation": 0.010545296737316754,
            "auditor_fp_violation": 0.013204918073871353,
            "ave_precision_score": 0.822105542165006,
            "fpr": 0.17672886937431395,
            "logloss": 0.7832250013449215,
            "mae": 0.27657093585724657,
            "precision": 0.7078039927404719,
            "recall": 0.8245243128964059
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8542360913745242,
            "auditor_fn_violation": 0.007126053178684761,
            "auditor_fp_violation": 0.02082315708063663,
            "ave_precision_score": 0.8545592765326252,
            "fpr": 0.125,
            "logloss": 0.6933668570166912,
            "mae": 0.2605229222454476,
            "precision": 0.768762677484787,
            "recall": 0.7879417879417879
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8285766132097274,
            "auditor_fn_violation": 0.009605410034276859,
            "auditor_fp_violation": 0.01880867529785624,
            "ave_precision_score": 0.828886962746065,
            "fpr": 0.150384193194292,
            "logloss": 0.7394196943714112,
            "mae": 0.27427670758922557,
            "precision": 0.7313725490196078,
            "recall": 0.7885835095137421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.75382573772578,
            "auditor_fn_violation": 0.00978407557354926,
            "auditor_fp_violation": 0.03068649000691985,
            "ave_precision_score": 0.7541868399050569,
            "fpr": 0.23355263157894737,
            "logloss": 1.1506299839222498,
            "mae": 0.3132479000619507,
            "precision": 0.6602870813397129,
            "recall": 0.8607068607068608
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7401201455797957,
            "auditor_fn_violation": 0.006969085849947667,
            "auditor_fp_violation": 0.016217313504653927,
            "ave_precision_score": 0.7397595830078123,
            "fpr": 0.22063666300768386,
            "logloss": 1.1216250224791215,
            "mae": 0.30311878732525555,
            "precision": 0.6747572815533981,
            "recall": 0.8816067653276956
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8008618788065079,
            "auditor_fn_violation": 0.006619980304190832,
            "auditor_fp_violation": 0.00962164692473644,
            "ave_precision_score": 0.8013192097699648,
            "fpr": 0.23574561403508773,
            "logloss": 0.9914755517578958,
            "mae": 0.29511161333384717,
            "precision": 0.6666666666666666,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7880966914833654,
            "auditor_fn_violation": 0.00958452366309819,
            "auditor_fp_violation": 8.019688334862183e-05,
            "ave_precision_score": 0.7885868471642334,
            "fpr": 0.2283205268935236,
            "logloss": 1.0027471782831325,
            "mae": 0.29619778961909204,
            "precision": 0.6729559748427673,
            "recall": 0.904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7999337001564231,
            "auditor_fn_violation": 0.0056260714155451,
            "auditor_fp_violation": 0.010789371921683569,
            "ave_precision_score": 0.8003728703372873,
            "fpr": 0.23355263157894737,
            "logloss": 1.022250433389302,
            "mae": 0.29633622674614657,
            "precision": 0.6692546583850931,
            "recall": 0.896049896049896
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7862024225695916,
            "auditor_fn_violation": 0.009491695346748574,
            "auditor_fp_violation": 0.010270213373832767,
            "ave_precision_score": 0.7868954413415258,
            "fpr": 0.23380900109769484,
            "logloss": 1.0322614158071626,
            "mae": 0.2990269834473659,
            "precision": 0.6671875,
            "recall": 0.9027484143763214
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.80086668965642,
            "auditor_fn_violation": 0.006619980304190832,
            "auditor_fp_violation": 0.00962164692473644,
            "ave_precision_score": 0.801321096455508,
            "fpr": 0.23574561403508773,
            "logloss": 0.9915368660404329,
            "mae": 0.2951154263839443,
            "precision": 0.6666666666666666,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7880854867421363,
            "auditor_fn_violation": 0.00958452366309819,
            "auditor_fp_violation": 8.019688334862183e-05,
            "ave_precision_score": 0.788591492196483,
            "fpr": 0.2283205268935236,
            "logloss": 1.002800966271277,
            "mae": 0.2962015390359035,
            "precision": 0.6729559748427673,
            "recall": 0.904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8428829142708671,
            "auditor_fn_violation": 0.0015045409782251922,
            "auditor_fp_violation": 0.02054076606830301,
            "ave_precision_score": 0.8431521825522348,
            "fpr": 0.1600877192982456,
            "logloss": 0.7586433087742168,
            "mae": 0.2686585355095061,
            "precision": 0.7326007326007326,
            "recall": 0.8316008316008316
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.824618004828104,
            "auditor_fn_violation": 0.011120832298684396,
            "auditor_fp_violation": 0.014801337283029847,
            "ave_precision_score": 0.8249460939051769,
            "fpr": 0.1800219538968167,
            "logloss": 0.813475866315709,
            "mae": 0.2775772051891138,
            "precision": 0.7066189624329159,
            "recall": 0.8350951374207188
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.820704504827658,
            "auditor_fn_violation": 0.00786920523762629,
            "auditor_fp_violation": 0.00694274840232833,
            "ave_precision_score": 0.8211029746129969,
            "fpr": 0.15899122807017543,
            "logloss": 0.8117186441768313,
            "mae": 0.27145298803594126,
            "precision": 0.7349177330895795,
            "recall": 0.8357588357588358
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8023552616513774,
            "auditor_fn_violation": 0.01557195006764864,
            "auditor_fp_violation": 0.00920259236425425,
            "ave_precision_score": 0.8027577279698158,
            "fpr": 0.17892425905598244,
            "logloss": 0.849400957098214,
            "mae": 0.28148283890953996,
            "precision": 0.705244122965642,
            "recall": 0.8245243128964059
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7602587329656788,
            "auditor_fn_violation": 0.008600959258853996,
            "auditor_fp_violation": 0.02007011438108032,
            "ave_precision_score": 0.7604718692896995,
            "fpr": 0.23464912280701755,
            "logloss": 1.1465706619070117,
            "mae": 0.3066348299979669,
            "precision": 0.665625,
            "recall": 0.8856548856548857
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.755288730277332,
            "auditor_fn_violation": 0.011434127866364358,
            "auditor_fp_violation": 0.01533264163521446,
            "ave_precision_score": 0.7555032373889864,
            "fpr": 0.24807903402854006,
            "logloss": 1.1394032853619032,
            "mae": 0.3116461256799552,
            "precision": 0.6512345679012346,
            "recall": 0.8921775898520085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8057509495391687,
            "auditor_fn_violation": 0.007112375533428168,
            "auditor_fp_violation": 0.008761753571864705,
            "ave_precision_score": 0.8061160303783773,
            "fpr": 0.23355263157894737,
            "logloss": 0.965611085992011,
            "mae": 0.2934407651799755,
            "precision": 0.6682242990654206,
            "recall": 0.8918918918918919
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7903021316856579,
            "auditor_fn_violation": 0.010696142751384884,
            "auditor_fp_violation": 0.009132420091324214,
            "ave_precision_score": 0.7909567754831068,
            "fpr": 0.2283205268935236,
            "logloss": 0.9785798295679409,
            "mae": 0.2950470304838728,
            "precision": 0.6724409448818898,
            "recall": 0.9027484143763214
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8421112550149363,
            "auditor_fn_violation": 0.00892010431484116,
            "auditor_fp_violation": 0.021980705824887047,
            "ave_precision_score": 0.8424401085946821,
            "fpr": 0.1337719298245614,
            "logloss": 0.7910145560535613,
            "mae": 0.264615775809081,
            "precision": 0.7555110220440882,
            "recall": 0.7837837837837838
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8182043758765518,
            "auditor_fn_violation": 0.008955611819829522,
            "auditor_fp_violation": 0.022074192141707895,
            "ave_precision_score": 0.8185560717465666,
            "fpr": 0.14928649835345773,
            "logloss": 0.8578624952141164,
            "mae": 0.276559820760474,
            "precision": 0.7312252964426877,
            "recall": 0.7822410147991543
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7908785675249015,
            "auditor_fn_violation": 0.006619980304190832,
            "auditor_fp_violation": 0.013732853014206051,
            "ave_precision_score": 0.7912546754334862,
            "fpr": 0.23026315789473684,
            "logloss": 1.020868884369613,
            "mae": 0.29757763516878627,
            "precision": 0.671875,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.7806043900513113,
            "auditor_fn_violation": 0.010652049301118812,
            "auditor_fp_violation": 0.0017593191284603702,
            "ave_precision_score": 0.7813251836319521,
            "fpr": 0.2283205268935236,
            "logloss": 1.0253469523686638,
            "mae": 0.2995279575652321,
            "precision": 0.6714060031595577,
            "recall": 0.8985200845665962
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.8010985933493162,
            "auditor_fn_violation": 0.008589561221140171,
            "auditor_fp_violation": 0.016254019619815208,
            "ave_precision_score": 0.801456277145382,
            "fpr": 0.21052631578947367,
            "logloss": 1.0245960780120789,
            "mae": 0.28884482371977743,
            "precision": 0.6821192052980133,
            "recall": 0.8565488565488566
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7821658932837742,
            "auditor_fn_violation": 0.007674581054204778,
            "auditor_fp_violation": 0.0007067350345097221,
            "ave_precision_score": 0.7825904270446116,
            "fpr": 0.2074643249176729,
            "logloss": 1.04511529879261,
            "mae": 0.28999222077907605,
            "precision": 0.6839464882943144,
            "recall": 0.864693446088795
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8009332190840612,
            "auditor_fn_violation": 0.006619980304190832,
            "auditor_fp_violation": 0.00912809866894615,
            "ave_precision_score": 0.8013177759567743,
            "fpr": 0.23464912280701755,
            "logloss": 0.9788640023946504,
            "mae": 0.29502131973541934,
            "precision": 0.6677018633540373,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7893270325582269,
            "auditor_fn_violation": 0.00958452366309819,
            "auditor_fp_violation": 0.0013458039486940417,
            "ave_precision_score": 0.789931027939045,
            "fpr": 0.22941822173435786,
            "logloss": 0.9894522902673455,
            "mae": 0.29621475272857023,
            "precision": 0.6718995290423861,
            "recall": 0.904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8406775713431816,
            "auditor_fn_violation": 0.008359320859320863,
            "auditor_fp_violation": 0.02214098180486018,
            "ave_precision_score": 0.8411011082108283,
            "fpr": 0.12280701754385964,
            "logloss": 0.8094710498415433,
            "mae": 0.2668530403331851,
            "precision": 0.768595041322314,
            "recall": 0.7733887733887734
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8183979805713009,
            "auditor_fn_violation": 0.011636029454424776,
            "auditor_fp_violation": 0.020708338972176697,
            "ave_precision_score": 0.818752578576102,
            "fpr": 0.1437980241492865,
            "logloss": 0.8651024155173878,
            "mae": 0.2766838542679748,
            "precision": 0.738,
            "recall": 0.7801268498942917
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7944453500514634,
            "auditor_fn_violation": 0.0075455009665535994,
            "auditor_fp_violation": 0.016526234379452102,
            "ave_precision_score": 0.7948687306856163,
            "fpr": 0.21600877192982457,
            "logloss": 0.9895391682206934,
            "mae": 0.2933657642080531,
            "precision": 0.6817447495961227,
            "recall": 0.8773388773388774
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7829878231403132,
            "auditor_fn_violation": 0.010712387706746066,
            "auditor_fp_violation": 0.010971936103133201,
            "ave_precision_score": 0.7836311989579552,
            "fpr": 0.2261251372118551,
            "logloss": 1.0043211306130984,
            "mae": 0.2976405792665554,
            "precision": 0.6719745222929936,
            "recall": 0.8921775898520085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7861066250027836,
            "auditor_fn_violation": 0.010185286501075978,
            "auditor_fp_violation": 0.015490800667562176,
            "ave_precision_score": 0.7865463673234658,
            "fpr": 0.2149122807017544,
            "logloss": 0.9142761458520994,
            "mae": 0.29984718088968926,
            "precision": 0.6771004942339374,
            "recall": 0.8544698544698545
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7835990850033177,
            "auditor_fn_violation": 0.015302747950234745,
            "auditor_fp_violation": 0.012738773689407502,
            "ave_precision_score": 0.7843072998265845,
            "fpr": 0.20965971459934138,
            "logloss": 0.8765671374231302,
            "mae": 0.29566560943033426,
            "precision": 0.6795302013422819,
            "recall": 0.8562367864693446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8544282317670788,
            "auditor_fn_violation": 0.003385217201006675,
            "auditor_fp_violation": 0.017182602678389714,
            "ave_precision_score": 0.8546668216500923,
            "fpr": 0.17763157894736842,
            "logloss": 0.7643202253589194,
            "mae": 0.2630565529684607,
            "precision": 0.7211703958691911,
            "recall": 0.8711018711018711
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8312802716615272,
            "auditor_fn_violation": 0.013928888868260379,
            "auditor_fp_violation": 0.017683412778370905,
            "ave_precision_score": 0.8316800134721687,
            "fpr": 0.19758507135016465,
            "logloss": 0.8240657157275817,
            "mae": 0.27711090999296895,
            "precision": 0.6949152542372882,
            "recall": 0.8668076109936576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8035365396026625,
            "auditor_fn_violation": 0.011842561184666449,
            "auditor_fp_violation": 0.015231306223796157,
            "ave_precision_score": 0.7975336649178583,
            "fpr": 0.2050438596491228,
            "logloss": 1.2262016866811447,
            "mae": 0.28372709373953436,
            "precision": 0.6924342105263158,
            "recall": 0.8752598752598753
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7625230655341085,
            "auditor_fn_violation": 0.01009043798720362,
            "auditor_fp_violation": 0.013603396338009823,
            "ave_precision_score": 0.7546408656043547,
            "fpr": 0.21953896816684962,
            "logloss": 1.4435936671127652,
            "mae": 0.290704669603605,
            "precision": 0.6726677577741408,
            "recall": 0.86892177589852
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7992153611862609,
            "auditor_fn_violation": 0.006396578764999821,
            "auditor_fp_violation": 0.01387277648878577,
            "ave_precision_score": 0.7996673927980614,
            "fpr": 0.21820175438596492,
            "logloss": 0.9646370067792737,
            "mae": 0.2916486825389905,
            "precision": 0.680064308681672,
            "recall": 0.8794178794178794
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.7848474854317145,
            "auditor_fn_violation": 0.011589615296249968,
            "auditor_fp_violation": 0.0006716488980447063,
            "ave_precision_score": 0.7853486791761267,
            "fpr": 0.21953896816684962,
            "logloss": 0.978198062683416,
            "mae": 0.29364400350930514,
            "precision": 0.6758508914100486,
            "recall": 0.8816067653276956
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 24481,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8484551924101398,
            "auditor_fn_violation": 0.0006816026552868674,
            "auditor_fp_violation": 0.01667379004355437,
            "ave_precision_score": 0.8487090937804411,
            "fpr": 0.1699561403508772,
            "logloss": 0.7606446691569185,
            "mae": 0.2634306424149862,
            "precision": 0.7246891651865008,
            "recall": 0.8482328482328483
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8273790630747551,
            "auditor_fn_violation": 0.008199061041580127,
            "auditor_fp_violation": 0.013838974682846396,
            "ave_precision_score": 0.8277367255008431,
            "fpr": 0.18880351262349068,
            "logloss": 0.8096613108032068,
            "mae": 0.2775274497530148,
            "precision": 0.7008695652173913,
            "recall": 0.8520084566596194
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8463910578782179,
            "auditor_fn_violation": 0.006692927745559327,
            "auditor_fp_violation": 0.00797818211421827,
            "ave_precision_score": 0.8466546427519139,
            "fpr": 0.1962719298245614,
            "logloss": 0.8033174740653173,
            "mae": 0.26987400576594545,
            "precision": 0.7026578073089701,
            "recall": 0.8794178794178794
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8223668735361629,
            "auditor_fn_violation": 0.010057948076481249,
            "auditor_fp_violation": 0.015127137121633613,
            "ave_precision_score": 0.8230229854845191,
            "fpr": 0.2052689352360044,
            "logloss": 0.85602623169727,
            "mae": 0.281058876486695,
            "precision": 0.6909090909090909,
            "recall": 0.8837209302325582
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8038830800019094,
            "auditor_fn_violation": 0.007894280920596713,
            "auditor_fp_violation": 0.010837709121992913,
            "ave_precision_score": 0.8043167133098913,
            "fpr": 0.2324561403508772,
            "logloss": 0.9836938897671692,
            "mae": 0.2946037065199947,
            "precision": 0.66875,
            "recall": 0.8898128898128899
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7905008046211544,
            "auditor_fn_violation": 0.009909422770321861,
            "auditor_fp_violation": 0.0019798605576690892,
            "ave_precision_score": 0.7911865724175702,
            "fpr": 0.2283205268935236,
            "logloss": 0.9938394795448031,
            "mae": 0.2960481816407223,
            "precision": 0.6719242902208202,
            "recall": 0.9006342494714588
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7773097146763616,
            "auditor_fn_violation": 0.016595542911332386,
            "auditor_fp_violation": 0.02695689339357675,
            "ave_precision_score": 0.7777322277113289,
            "fpr": 0.20394736842105263,
            "logloss": 1.0009885770654428,
            "mae": 0.2993986216733033,
            "precision": 0.6873949579831933,
            "recall": 0.8503118503118503
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7706482536024617,
            "auditor_fn_violation": 0.02110683842999469,
            "auditor_fp_violation": 0.013450521029126509,
            "ave_precision_score": 0.771213215481373,
            "fpr": 0.2052689352360044,
            "logloss": 0.9889553973303461,
            "mae": 0.30203427912407244,
            "precision": 0.6819727891156463,
            "recall": 0.8477801268498943
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8568856210017428,
            "auditor_fn_violation": 0.015093281540649964,
            "auditor_fp_violation": 0.007283652867668011,
            "ave_precision_score": 0.8571764836091997,
            "fpr": 0.14583333333333334,
            "logloss": 0.5414748058799469,
            "mae": 0.296060810074811,
            "precision": 0.7461832061068703,
            "recall": 0.8128898128898129
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8441827530333847,
            "auditor_fn_violation": 0.014541555756167862,
            "auditor_fp_violation": 0.013595877880195884,
            "ave_precision_score": 0.8443971192642257,
            "fpr": 0.150384193194292,
            "logloss": 0.5632516128153882,
            "mae": 0.30903293604361226,
            "precision": 0.7390476190476191,
            "recall": 0.8202959830866807
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8530687685623697,
            "auditor_fn_violation": 0.011858518437465812,
            "auditor_fp_violation": 0.022957626083770916,
            "ave_precision_score": 0.8533174756815145,
            "fpr": 0.15021929824561403,
            "logloss": 0.739195984498088,
            "mae": 0.2610278009598937,
            "precision": 0.7419962335216572,
            "recall": 0.8191268191268192
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8215163226095439,
            "auditor_fn_violation": 0.013726987280199957,
            "auditor_fp_violation": 0.02203659985263823,
            "ave_precision_score": 0.8219154392574168,
            "fpr": 0.16794731064763996,
            "logloss": 0.805518067500759,
            "mae": 0.27246496197702,
            "precision": 0.7171903881700554,
            "recall": 0.8202959830866807
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8339315455989775,
            "auditor_fn_violation": 0.0005060728744939305,
            "auditor_fp_violation": 0.013964362763056135,
            "ave_precision_score": 0.8342500991444934,
            "fpr": 0.13706140350877194,
            "logloss": 0.8084849771157362,
            "mae": 0.2654207179407179,
            "precision": 0.7549019607843137,
            "recall": 0.8004158004158004
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.8072938933362663,
            "auditor_fn_violation": 0.004232971225542641,
            "auditor_fp_violation": 0.011894200261642339,
            "ave_precision_score": 0.8076694315646272,
            "fpr": 0.1712403951701427,
            "logloss": 0.8900648199104373,
            "mae": 0.27995978171473523,
            "precision": 0.7078651685393258,
            "recall": 0.7991543340380549
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8448179569649327,
            "auditor_fn_violation": 0.005600995732574687,
            "auditor_fp_violation": 0.014928562706069121,
            "ave_precision_score": 0.8450896069160793,
            "fpr": 0.15350877192982457,
            "logloss": 0.7401109425576932,
            "mae": 0.265178023413553,
            "precision": 0.7421731123388582,
            "recall": 0.8378378378378378
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8220163908162553,
            "auditor_fn_violation": 0.01038052647579618,
            "auditor_fp_violation": 0.010570951686390093,
            "ave_precision_score": 0.8224880203752922,
            "fpr": 0.1778265642151482,
            "logloss": 0.7884658830536632,
            "mae": 0.2786261513234762,
            "precision": 0.7075812274368231,
            "recall": 0.828752642706131
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7964030982553572,
            "auditor_fn_violation": 0.0056260714155451,
            "auditor_fp_violation": 0.010239854276061385,
            "ave_precision_score": 0.7967273496992552,
            "fpr": 0.23464912280701755,
            "logloss": 1.036507821252258,
            "mae": 0.2990713716749156,
            "precision": 0.6682170542635659,
            "recall": 0.896049896049896
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7853121617106598,
            "auditor_fn_violation": 0.009180720486977349,
            "auditor_fp_violation": 0.012006977128851336,
            "ave_precision_score": 0.785675993980784,
            "fpr": 0.24478594950603733,
            "logloss": 1.0449716428261593,
            "mae": 0.30312315722191135,
            "precision": 0.6584992343032159,
            "recall": 0.9090909090909091
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8472660846398998,
            "auditor_fn_violation": 0.010287868840500425,
            "auditor_fp_violation": 0.024641795905075915,
            "ave_precision_score": 0.8475690183678861,
            "fpr": 0.17543859649122806,
            "logloss": 0.7918599883899772,
            "mae": 0.26796290860049116,
            "precision": 0.7163120567375887,
            "recall": 0.83991683991684
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8248522749522712,
            "auditor_fn_violation": 0.013142168887197351,
            "auditor_fp_violation": 0.023537785262820226,
            "ave_precision_score": 0.8251645746992924,
            "fpr": 0.19538968166849616,
            "logloss": 0.8614782845921857,
            "mae": 0.277478676313198,
            "precision": 0.6946826758147513,
            "recall": 0.8562367864693446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.80086668965642,
            "auditor_fn_violation": 0.006619980304190832,
            "auditor_fp_violation": 0.00962164692473644,
            "ave_precision_score": 0.801321096455508,
            "fpr": 0.23574561403508773,
            "logloss": 0.9915366481842915,
            "mae": 0.2951154095211327,
            "precision": 0.6666666666666666,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7880854867421363,
            "auditor_fn_violation": 0.00958452366309819,
            "auditor_fp_violation": 8.019688334862183e-05,
            "ave_precision_score": 0.788591492196483,
            "fpr": 0.2283205268935236,
            "logloss": 1.0028008165839153,
            "mae": 0.2962015202658472,
            "precision": 0.6729559748427673,
            "recall": 0.904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7830351631641255,
            "auditor_fn_violation": 0.01263130539446329,
            "auditor_fp_violation": 0.019518052672283962,
            "ave_precision_score": 0.783456271712111,
            "fpr": 0.18201754385964913,
            "logloss": 0.9705969029851336,
            "mae": 0.2938592467123682,
            "precision": 0.7046263345195729,
            "recall": 0.8232848232848233
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.7724850073689178,
            "auditor_fn_violation": 0.015690306170994404,
            "auditor_fp_violation": 0.008866767915231898,
            "ave_precision_score": 0.7729018025408751,
            "fpr": 0.18880351262349068,
            "logloss": 0.9693308470076614,
            "mae": 0.29431177037336925,
            "precision": 0.6934046345811051,
            "recall": 0.8224101479915433
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8005874397405099,
            "auditor_fn_violation": 0.006619980304190832,
            "auditor_fp_violation": 0.00962164692473644,
            "ave_precision_score": 0.8009583180558315,
            "fpr": 0.23574561403508773,
            "logloss": 0.9897241105430507,
            "mae": 0.2953456915545588,
            "precision": 0.6666666666666666,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7879200013775842,
            "auditor_fn_violation": 0.00958452366309819,
            "auditor_fp_violation": 0.0013458039486940417,
            "ave_precision_score": 0.7885010903234838,
            "fpr": 0.22941822173435786,
            "logloss": 1.000276606461518,
            "mae": 0.2964946137474723,
            "precision": 0.6718995290423861,
            "recall": 0.904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8355711534732195,
            "auditor_fn_violation": 0.003118503118503122,
            "auditor_fp_violation": 0.013750661456425289,
            "ave_precision_score": 0.8358578284377289,
            "fpr": 0.14144736842105263,
            "logloss": 0.7849103256673456,
            "mae": 0.2671281006306018,
            "precision": 0.7495145631067961,
            "recall": 0.8024948024948025
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8100763508574442,
            "auditor_fn_violation": 0.007495886545231756,
            "auditor_fp_violation": 0.0069320181044464245,
            "ave_precision_score": 0.8107570261723425,
            "fpr": 0.16465422612513722,
            "logloss": 0.8409955079522038,
            "mae": 0.27823051437894775,
            "precision": 0.715370018975332,
            "recall": 0.7970401691331924
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.7401178438556746,
            "auditor_fn_violation": 0.013422329211802897,
            "auditor_fp_violation": 0.0201260837709122,
            "ave_precision_score": 0.7265207438241927,
            "fpr": 0.2565789473684211,
            "logloss": 2.0962330596956416,
            "mae": 0.3242062643518514,
            "precision": 0.6459909228441755,
            "recall": 0.8877338877338877
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.7323688712403391,
            "auditor_fn_violation": 0.022004952390677252,
            "auditor_fp_violation": 0.013661037847916644,
            "ave_precision_score": 0.7177284758674569,
            "fpr": 0.2601536772777168,
            "logloss": 2.1326868968777597,
            "mae": 0.317790967045578,
            "precision": 0.639269406392694,
            "recall": 0.8879492600422833
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 24481,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8119075330858718,
            "auditor_fn_violation": 0.008265856950067476,
            "auditor_fp_violation": 0.007868787397728665,
            "ave_precision_score": 0.8123247005584533,
            "fpr": 0.1875,
            "logloss": 0.9264043029842853,
            "mae": 0.28314274134073647,
            "precision": 0.7041522491349481,
            "recall": 0.8461538461538461
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7903261730438383,
            "auditor_fn_violation": 0.010320188070168929,
            "auditor_fp_violation": 0.006751575116912024,
            "ave_precision_score": 0.7908122828868336,
            "fpr": 0.2052689352360044,
            "logloss": 0.9933836411192568,
            "mae": 0.292670136264912,
            "precision": 0.6786941580756014,
            "recall": 0.8350951374207188
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7676227553506572,
            "auditor_fn_violation": 0.008776489039646935,
            "auditor_fp_violation": 0.01646263280009769,
            "ave_precision_score": 0.7680310653851949,
            "fpr": 0.22916666666666666,
            "logloss": 1.1300068240444416,
            "mae": 0.3033813390523954,
            "precision": 0.669826224328594,
            "recall": 0.8814968814968815
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7609560246166466,
            "auditor_fn_violation": 0.011761347681496769,
            "auditor_fp_violation": 0.010350410257181399,
            "ave_precision_score": 0.7614395118340971,
            "fpr": 0.2414928649835346,
            "logloss": 1.1272691899268747,
            "mae": 0.30795492516112083,
            "precision": 0.656786271450858,
            "recall": 0.8900634249471459
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7944177522711297,
            "auditor_fn_violation": 0.004757540941751472,
            "auditor_fp_violation": 0.011827349696747678,
            "ave_precision_score": 0.7948101821558905,
            "fpr": 0.2225877192982456,
            "logloss": 0.9754507678882426,
            "mae": 0.2972027131704881,
            "precision": 0.6767515923566879,
            "recall": 0.8835758835758836
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7856829025482941,
            "auditor_fn_violation": 0.011691726444234553,
            "auditor_fp_violation": 0.0018144544857625609,
            "ave_precision_score": 0.7862817325855136,
            "fpr": 0.2261251372118551,
            "logloss": 0.9787113136260235,
            "mae": 0.29822777750132856,
            "precision": 0.6719745222929936,
            "recall": 0.8921775898520085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.7382908458297087,
            "auditor_fn_violation": 0.016586424481161325,
            "auditor_fp_violation": 0.018388488622949513,
            "ave_precision_score": 0.7243024602280661,
            "fpr": 0.26096491228070173,
            "logloss": 2.167833429248901,
            "mae": 0.32576245865105774,
            "precision": 0.6437125748502994,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.7345985495652998,
            "auditor_fn_violation": 0.022687240515846956,
            "auditor_fp_violation": 0.012600935296152051,
            "ave_precision_score": 0.7194055998304599,
            "fpr": 0.26344676180021953,
            "logloss": 2.1852666270419343,
            "mae": 0.31757460820189953,
            "precision": 0.6385542168674698,
            "recall": 0.8964059196617337
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8129638546778748,
            "auditor_fn_violation": 0.00980459204143415,
            "auditor_fp_violation": 0.013267289453331714,
            "ave_precision_score": 0.8133636431402804,
            "fpr": 0.1787280701754386,
            "logloss": 0.9299106443766371,
            "mae": 0.27961776675265654,
            "precision": 0.7104795737122558,
            "recall": 0.8316008316008316
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7877291346273112,
            "auditor_fn_violation": 0.011461976361269242,
            "auditor_fp_violation": 0.0007844257652537044,
            "ave_precision_score": 0.7885002508912522,
            "fpr": 0.1986827661909989,
            "logloss": 1.001102996297852,
            "mae": 0.290487081815375,
            "precision": 0.6841186736474695,
            "recall": 0.828752642706131
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8312789466266002,
            "auditor_fn_violation": 0.008965696465696468,
            "auditor_fp_violation": 0.0057012455733300845,
            "ave_precision_score": 0.831637526618809,
            "fpr": 0.1962719298245614,
            "logloss": 0.8294495973495328,
            "mae": 0.276902813467758,
            "precision": 0.7016666666666667,
            "recall": 0.8752598752598753
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8153812543630532,
            "auditor_fn_violation": 0.01148750414826539,
            "auditor_fp_violation": 0.012375381561734065,
            "ave_precision_score": 0.8158313821509577,
            "fpr": 0.20417124039517015,
            "logloss": 0.8487391251484004,
            "mae": 0.28268746231731917,
            "precision": 0.6894824707846411,
            "recall": 0.8731501057082452
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8480473495909566,
            "auditor_fn_violation": 0.0064854834591676755,
            "auditor_fp_violation": 0.02144645255830993,
            "ave_precision_score": 0.8483223231256205,
            "fpr": 0.14692982456140352,
            "logloss": 0.7862653869820094,
            "mae": 0.26125532153961556,
            "precision": 0.7452471482889734,
            "recall": 0.814968814968815
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8232701651653299,
            "auditor_fn_violation": 0.009053081551996627,
            "auditor_fp_violation": 0.015956673633770912,
            "ave_precision_score": 0.8236156369511043,
            "fpr": 0.17233809001097694,
            "logloss": 0.8652385839362261,
            "mae": 0.2746026780717551,
            "precision": 0.7119266055045872,
            "recall": 0.8202959830866807
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8054190031154984,
            "auditor_fn_violation": 0.0042446292446292454,
            "auditor_fp_violation": 0.011725587169780604,
            "ave_precision_score": 0.805845956329729,
            "fpr": 0.21162280701754385,
            "logloss": 0.9169208830304731,
            "mae": 0.28909139486834995,
            "precision": 0.6851549755301795,
            "recall": 0.8731808731808732
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7912367629693793,
            "auditor_fn_violation": 0.009853725780512091,
            "auditor_fp_violation": 0.006307986105889962,
            "ave_precision_score": 0.7919353374815503,
            "fpr": 0.21405049396267836,
            "logloss": 0.9283623479065332,
            "mae": 0.29098931390158717,
            "precision": 0.6818923327895595,
            "recall": 0.8837209302325582
        }
    }
]
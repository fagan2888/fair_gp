[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8400300495475306,
            "auditor_fn_violation": 0.01964546783625731,
            "auditor_fp_violation": 0.010560941828254851,
            "ave_precision_score": 0.8405576305169153,
            "fpr": 0.1337719298245614,
            "logloss": 0.6354104325413132,
            "mae": 0.26414947085760093,
            "precision": 0.7474120082815735,
            "recall": 0.7916666666666666
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8564845769585976,
            "auditor_fn_violation": 0.010251764467309417,
            "auditor_fp_violation": 0.010296537078430697,
            "ave_precision_score": 0.8567169846401205,
            "fpr": 0.11964873765093303,
            "logloss": 0.7262039194424094,
            "mae": 0.2647121457735067,
            "precision": 0.7845849802371542,
            "recall": 0.7971887550200804
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8114632015255947,
            "auditor_fn_violation": 0.012652931671283472,
            "auditor_fp_violation": 0.012061403508771933,
            "ave_precision_score": 0.8129039434823304,
            "fpr": 0.13596491228070176,
            "logloss": 0.7192897906828465,
            "mae": 0.2742993080297297,
            "precision": 0.7378435517970402,
            "recall": 0.7653508771929824
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8329929841970483,
            "auditor_fn_violation": 0.008675756814304426,
            "auditor_fp_violation": 0.016614262590931926,
            "ave_precision_score": 0.8333876326946912,
            "fpr": 0.12294182217343579,
            "logloss": 0.7998491295481707,
            "mae": 0.2744044988484727,
            "precision": 0.7746478873239436,
            "recall": 0.7730923694779116
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8197868819434522,
            "auditor_fn_violation": 0.01638965835641736,
            "auditor_fp_violation": 0.011368882733148664,
            "ave_precision_score": 0.821204542359512,
            "fpr": 0.1337719298245614,
            "logloss": 0.7083270326887586,
            "mae": 0.2711928880708443,
            "precision": 0.7420718816067653,
            "recall": 0.7697368421052632
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8393356434941838,
            "auditor_fn_violation": 0.018823923575751966,
            "auditor_fp_violation": 0.016967757539675162,
            "ave_precision_score": 0.8396900027838599,
            "fpr": 0.11525795828759605,
            "logloss": 0.7846971272460678,
            "mae": 0.2703895805904818,
            "precision": 0.7870182555780934,
            "recall": 0.7791164658634538
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8082278815640497,
            "auditor_fn_violation": 0.0037944367497691586,
            "auditor_fp_violation": 0.013463277162203757,
            "ave_precision_score": 0.8096617374456349,
            "fpr": 0.12390350877192982,
            "logloss": 0.7282431474740106,
            "mae": 0.2824199184376503,
            "precision": 0.7488888888888889,
            "recall": 0.7390350877192983
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8147175071495444,
            "auditor_fn_violation": 0.013485335414104285,
            "auditor_fp_violation": 0.01711128180457843,
            "ave_precision_score": 0.8153031343964622,
            "fpr": 0.1163556531284303,
            "logloss": 0.8324421958831948,
            "mae": 0.28560187638598,
            "precision": 0.7773109243697479,
            "recall": 0.7429718875502008
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 18998,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8412622402771573,
            "auditor_fn_violation": 0.01195079255155433,
            "auditor_fp_violation": 0.015117632348414905,
            "ave_precision_score": 0.8415074237248029,
            "fpr": 0.18530701754385964,
            "logloss": 0.6976251453288294,
            "mae": 0.2724033056213043,
            "precision": 0.7014134275618374,
            "recall": 0.8706140350877193
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8453579844968232,
            "auditor_fn_violation": 0.008062987405164018,
            "auditor_fp_violation": 0.02485627639583993,
            "ave_precision_score": 0.8456464195903498,
            "fpr": 0.1756311745334797,
            "logloss": 0.7768562682771529,
            "mae": 0.2669146753072407,
            "precision": 0.727427597955707,
            "recall": 0.857429718875502
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8391290852504332,
            "auditor_fn_violation": 0.01345365881809788,
            "auditor_fp_violation": 0.00909414435210834,
            "ave_precision_score": 0.8395512043632434,
            "fpr": 0.1425438596491228,
            "logloss": 0.6484693568983582,
            "mae": 0.2644515900585099,
            "precision": 0.7394789579158316,
            "recall": 0.8092105263157895
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8484034676425356,
            "auditor_fn_violation": 0.01033332010809429,
            "auditor_fp_violation": 0.023285483052176394,
            "ave_precision_score": 0.8486682498307361,
            "fpr": 0.13172338090010977,
            "logloss": 0.7440695991591563,
            "mae": 0.26503321893556675,
            "precision": 0.7683397683397684,
            "recall": 0.7991967871485943
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8286579662690968,
            "auditor_fn_violation": 0.01585584025854109,
            "auditor_fp_violation": 0.009339412126808257,
            "ave_precision_score": 0.8300294784971203,
            "fpr": 0.15021929824561403,
            "logloss": 0.6788957495770348,
            "mae": 0.26826171270446225,
            "precision": 0.7292490118577075,
            "recall": 0.8092105263157895
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8477716798793313,
            "auditor_fn_violation": 0.014329546506553112,
            "auditor_fp_violation": 0.019620298583628137,
            "ave_precision_score": 0.8480488012513665,
            "fpr": 0.13391877058177826,
            "logloss": 0.7684629832900856,
            "mae": 0.2668741310544164,
            "precision": 0.7658349328214972,
            "recall": 0.8012048192771084
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.801061803668397,
            "auditor_fn_violation": 0.007603301015697141,
            "auditor_fp_violation": 0.008829639889196677,
            "ave_precision_score": 0.8025625388687582,
            "fpr": 0.10526315789473684,
            "logloss": 0.5858279552977512,
            "mae": 0.32621701166243394,
            "precision": 0.7686746987951807,
            "recall": 0.6995614035087719
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8014546133901947,
            "auditor_fn_violation": 0.009786676894184863,
            "auditor_fp_violation": 0.011617491886892248,
            "ave_precision_score": 0.802612909620838,
            "fpr": 0.08562019758507135,
            "logloss": 0.6387377562903446,
            "mae": 0.32101324573607665,
            "precision": 0.8194444444444444,
            "recall": 0.7108433734939759
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8239706808619831,
            "auditor_fn_violation": 0.015009425977223764,
            "auditor_fp_violation": 0.010488804247460765,
            "ave_precision_score": 0.8253313602599391,
            "fpr": 0.14364035087719298,
            "logloss": 0.6818821706393884,
            "mae": 0.2729262967730681,
            "precision": 0.7304526748971193,
            "recall": 0.7785087719298246
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8439426937626088,
            "auditor_fn_violation": 0.0073245782250847494,
            "auditor_fp_violation": 0.018796362988813087,
            "ave_precision_score": 0.8442180365365705,
            "fpr": 0.1207464324917673,
            "logloss": 0.7071472004529461,
            "mae": 0.2707094462320088,
            "precision": 0.780439121756487,
            "recall": 0.785140562248996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8107607762873709,
            "auditor_fn_violation": 0.03220942982456141,
            "auditor_fp_violation": 0.03419321329639889,
            "ave_precision_score": 0.8122986376320048,
            "fpr": 0.14144736842105263,
            "logloss": 0.7757753407964334,
            "mae": 0.2710729527643357,
            "precision": 0.7323651452282157,
            "recall": 0.7741228070175439
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8245084619459478,
            "auditor_fn_violation": 0.035463478502373934,
            "auditor_fp_violation": 0.02643504330977587,
            "ave_precision_score": 0.8249007449925974,
            "fpr": 0.13391877058177826,
            "logloss": 0.8378879338069025,
            "mae": 0.2833430380462784,
            "precision": 0.7564870259481038,
            "recall": 0.7610441767068273
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.682099683510871,
            "auditor_fn_violation": 0.008930632502308404,
            "auditor_fp_violation": 0.006160549399815334,
            "ave_precision_score": 0.6762241708620687,
            "fpr": 0.1513157894736842,
            "logloss": 1.7842931084614768,
            "mae": 0.3027243839269376,
            "precision": 0.7154639175257732,
            "recall": 0.7609649122807017
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7041755401925491,
            "auditor_fn_violation": 0.01072126045344937,
            "auditor_fp_violation": 0.006636668323397384,
            "ave_precision_score": 0.6956766790408941,
            "fpr": 0.13611416026344675,
            "logloss": 1.8058067474361115,
            "mae": 0.30523665715944487,
            "precision": 0.7505030181086519,
            "recall": 0.748995983935743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8282135934891491,
            "auditor_fn_violation": 0.014456371191135733,
            "auditor_fp_violation": 0.020597683902739305,
            "ave_precision_score": 0.8296217690355709,
            "fpr": 0.15899122807017543,
            "logloss": 0.678521411782496,
            "mae": 0.2685604965131573,
            "precision": 0.7195357833655706,
            "recall": 0.8157894736842105
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8476741231130107,
            "auditor_fn_violation": 0.014957745361247408,
            "auditor_fp_violation": 0.024462913595734676,
            "ave_precision_score": 0.8479703052215244,
            "fpr": 0.14270032930845225,
            "logloss": 0.7642154554719323,
            "mae": 0.2634073172388846,
            "precision": 0.7579143389199255,
            "recall": 0.8172690763052208
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8461954216905255,
            "auditor_fn_violation": 0.013319002000615577,
            "auditor_fp_violation": 0.00802169898430287,
            "ave_precision_score": 0.8463857480345294,
            "fpr": 0.12280701754385964,
            "logloss": 0.6695687063384645,
            "mae": 0.26553099208561654,
            "precision": 0.7591397849462366,
            "recall": 0.7741228070175439
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8463322225528287,
            "auditor_fn_violation": 0.01119516485260471,
            "auditor_fp_violation": 0.01345141304954511,
            "ave_precision_score": 0.8465845979347859,
            "fpr": 0.10757409440175632,
            "logloss": 0.7872819019579461,
            "mae": 0.2685749517292012,
            "precision": 0.797938144329897,
            "recall": 0.7771084337349398
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.8034381482052364,
            "auditor_fn_violation": 0.0133406432748538,
            "auditor_fp_violation": 0.008089027393044021,
            "ave_precision_score": 0.804833981256414,
            "fpr": 0.2774122807017544,
            "logloss": 0.8542826760843638,
            "mae": 0.3398384480260066,
            "precision": 0.6195488721804512,
            "recall": 0.9035087719298246
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.8325765261966862,
            "auditor_fn_violation": 0.006916800021160382,
            "auditor_fp_violation": 0.01672855043150304,
            "ave_precision_score": 0.8328716277483845,
            "fpr": 0.23600439077936333,
            "logloss": 0.8040128735192149,
            "mae": 0.3152614850116469,
            "precision": 0.6707503828483921,
            "recall": 0.8795180722891566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8395458278880322,
            "auditor_fn_violation": 0.015307594644506003,
            "auditor_fp_violation": 0.00688673437980918,
            "ave_precision_score": 0.8403535234337869,
            "fpr": 0.15350877192982457,
            "logloss": 0.6344003392375942,
            "mae": 0.2650374323024926,
            "precision": 0.7297297297297297,
            "recall": 0.8289473684210527
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8587341449106637,
            "auditor_fn_violation": 0.01046116408554085,
            "auditor_fp_violation": 0.016725892574745577,
            "ave_precision_score": 0.8589622592992332,
            "fpr": 0.13391877058177826,
            "logloss": 0.7150532097208058,
            "mae": 0.2630407289520882,
            "precision": 0.769811320754717,
            "recall": 0.8192771084337349
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8143977453567991,
            "auditor_fn_violation": 0.01175361649738381,
            "auditor_fp_violation": 0.0103589566020314,
            "ave_precision_score": 0.8158034877364106,
            "fpr": 0.13486842105263158,
            "logloss": 0.7219931884908906,
            "mae": 0.27850386950290207,
            "precision": 0.7377398720682303,
            "recall": 0.7587719298245614
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8157619379662406,
            "auditor_fn_violation": 0.014349384365122397,
            "auditor_fp_violation": 0.015832852704236358,
            "ave_precision_score": 0.816272445414462,
            "fpr": 0.12843029637760703,
            "logloss": 0.8110917049207372,
            "mae": 0.28090474208986776,
            "precision": 0.7664670658682635,
            "recall": 0.7710843373493976
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8245289952623,
            "auditor_fn_violation": 0.015288357956294246,
            "auditor_fp_violation": 0.010750904124345954,
            "ave_precision_score": 0.8259402618721006,
            "fpr": 0.13706140350877194,
            "logloss": 0.6652394391953279,
            "mae": 0.27227852438505634,
            "precision": 0.7412008281573499,
            "recall": 0.7850877192982456
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8429674913393628,
            "auditor_fn_violation": 0.00973377593800008,
            "auditor_fp_violation": 0.015131178520264838,
            "ave_precision_score": 0.8432520941344557,
            "fpr": 0.12184412733260154,
            "logloss": 0.7546482596886405,
            "mae": 0.2717439649755074,
            "precision": 0.7810650887573964,
            "recall": 0.7951807228915663
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8199169363085612,
            "auditor_fn_violation": 0.015062326869806094,
            "auditor_fp_violation": 0.01069559864573715,
            "ave_precision_score": 0.8214393342195556,
            "fpr": 0.14912280701754385,
            "logloss": 0.7128916655914617,
            "mae": 0.26950799938018133,
            "precision": 0.7301587301587301,
            "recall": 0.8070175438596491
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8355126023232732,
            "auditor_fn_violation": 0.007749990081070719,
            "auditor_fp_violation": 0.020353867048689283,
            "ave_precision_score": 0.8360303679812882,
            "fpr": 0.12294182217343579,
            "logloss": 0.802216479861569,
            "mae": 0.2725033143954799,
            "precision": 0.7768924302788844,
            "recall": 0.7831325301204819
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8111658709038954,
            "auditor_fn_violation": 0.011421783625730993,
            "auditor_fp_violation": 0.020395698676515854,
            "ave_precision_score": 0.8126041909957769,
            "fpr": 0.15899122807017543,
            "logloss": 0.8048174572544078,
            "mae": 0.2724183508198866,
            "precision": 0.7134387351778656,
            "recall": 0.7916666666666666
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8329759300313493,
            "auditor_fn_violation": 0.011671273458267762,
            "auditor_fp_violation": 0.027033061080206144,
            "ave_precision_score": 0.8333232582162109,
            "fpr": 0.150384193194292,
            "logloss": 0.8704560461362365,
            "mae": 0.2710983729139379,
            "precision": 0.7444029850746269,
            "recall": 0.8012048192771084
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8303847986450205,
            "auditor_fn_violation": 0.014110110803324106,
            "auditor_fp_violation": 0.011570867959372122,
            "ave_precision_score": 0.8317791167347403,
            "fpr": 0.15679824561403508,
            "logloss": 0.6582443435646546,
            "mae": 0.26865076508384894,
            "precision": 0.7223300970873786,
            "recall": 0.8157894736842105
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8488478674415924,
            "auditor_fn_violation": 0.009914520871631426,
            "auditor_fp_violation": 0.021525981878732628,
            "ave_precision_score": 0.8491247988177721,
            "fpr": 0.13830954994511527,
            "logloss": 0.7423995664425592,
            "mae": 0.2662548805428358,
            "precision": 0.7631578947368421,
            "recall": 0.8152610441767069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8211948800210206,
            "auditor_fn_violation": 0.017240881809787632,
            "auditor_fp_violation": 0.011895487072945531,
            "ave_precision_score": 0.8226133618163982,
            "fpr": 0.15679824561403508,
            "logloss": 0.7004238495317353,
            "mae": 0.27641652733042915,
            "precision": 0.7190569744597249,
            "recall": 0.8026315789473685
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8329112404787619,
            "auditor_fn_violation": 0.011164305961496927,
            "auditor_fp_violation": 0.019498037172784616,
            "ave_precision_score": 0.8332222044636517,
            "fpr": 0.14050493962678376,
            "logloss": 0.7835297794761058,
            "mae": 0.27389133167887825,
            "precision": 0.7589453860640302,
            "recall": 0.8092369477911646
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.823874085291739,
            "auditor_fn_violation": 0.015747633887349954,
            "auditor_fp_violation": 0.011561249615266233,
            "ave_precision_score": 0.8252338765425282,
            "fpr": 0.14035087719298245,
            "logloss": 0.6827615535356633,
            "mae": 0.2728030153605257,
            "precision": 0.7360824742268042,
            "recall": 0.7828947368421053
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8439623277969748,
            "auditor_fn_violation": 0.007827137308840191,
            "auditor_fp_violation": 0.01646276475575625,
            "ave_precision_score": 0.844235517214136,
            "fpr": 0.11745334796926454,
            "logloss": 0.7090337786142809,
            "mae": 0.27101805043658267,
            "precision": 0.7842741935483871,
            "recall": 0.7811244979919679
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8260722295361513,
            "auditor_fn_violation": 0.014795417820867962,
            "auditor_fp_violation": 0.022891658971991385,
            "ave_precision_score": 0.8274972007859409,
            "fpr": 0.16228070175438597,
            "logloss": 0.6793862166588279,
            "mae": 0.2706646791314472,
            "precision": 0.7170172084130019,
            "recall": 0.8223684210526315
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8466154537047966,
            "auditor_fn_violation": 0.015319235228510095,
            "auditor_fp_violation": 0.025071562793194824,
            "ave_precision_score": 0.8469105455401216,
            "fpr": 0.14489571899012074,
            "logloss": 0.7621068623630394,
            "mae": 0.2654418010945289,
            "precision": 0.7564575645756457,
            "recall": 0.8232931726907631
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8247500638289657,
            "auditor_fn_violation": 0.01364843028624192,
            "auditor_fp_violation": 0.017793936595875658,
            "ave_precision_score": 0.8261436059656585,
            "fpr": 0.12719298245614036,
            "logloss": 0.6820070767718247,
            "mae": 0.2689958632668842,
            "precision": 0.75,
            "recall": 0.7631578947368421
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8432970466886814,
            "auditor_fn_violation": 0.008217281860702965,
            "auditor_fp_violation": 0.014894629268850188,
            "ave_precision_score": 0.8435809965087882,
            "fpr": 0.1119648737650933,
            "logloss": 0.7272169782849416,
            "mae": 0.26829153238140324,
            "precision": 0.7918367346938775,
            "recall": 0.7791164658634538
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8287090184603086,
            "auditor_fn_violation": 0.01499740304709141,
            "auditor_fp_violation": 0.019683941212680828,
            "ave_precision_score": 0.830115569186705,
            "fpr": 0.14692982456140352,
            "logloss": 0.6577630026484151,
            "mae": 0.268897419303699,
            "precision": 0.7303822937625755,
            "recall": 0.7960526315789473
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8487389257694509,
            "auditor_fn_violation": 0.01188948990252999,
            "auditor_fp_violation": 0.02433533647137621,
            "ave_precision_score": 0.8490203839173971,
            "fpr": 0.13391877058177826,
            "logloss": 0.7442655523171803,
            "mae": 0.2659124551392128,
            "precision": 0.767175572519084,
            "recall": 0.8072289156626506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8236940355359242,
            "auditor_fn_violation": 0.015855840258541098,
            "auditor_fp_violation": 0.0069588719606032655,
            "ave_precision_score": 0.8250515134731139,
            "fpr": 0.1162280701754386,
            "logloss": 0.7040772136148689,
            "mae": 0.2714962603463984,
            "precision": 0.7568807339449541,
            "recall": 0.7236842105263158
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8415046512008135,
            "auditor_fn_violation": 0.008217281860702972,
            "auditor_fp_violation": 0.01472452643637224,
            "ave_precision_score": 0.8417847191932842,
            "fpr": 0.10208562019758508,
            "logloss": 0.7464672063741633,
            "mae": 0.27404886693605984,
            "precision": 0.8008565310492506,
            "recall": 0.751004016064257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8299606341730076,
            "auditor_fn_violation": 0.017774699907663893,
            "auditor_fp_violation": 0.010431094182825493,
            "ave_precision_score": 0.8308046192058962,
            "fpr": 0.15021929824561403,
            "logloss": 0.668733486694875,
            "mae": 0.26942443054067056,
            "precision": 0.7287128712871287,
            "recall": 0.8070175438596491
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8463896707043294,
            "auditor_fn_violation": 0.01535891094564868,
            "auditor_fp_violation": 0.01680031256395468,
            "ave_precision_score": 0.8466696641522644,
            "fpr": 0.13391877058177826,
            "logloss": 0.7710877722462995,
            "mae": 0.2688899224911985,
            "precision": 0.7667304015296367,
            "recall": 0.8052208835341366
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5822368421052632,
            "auc_prc": 0.5857026785012094,
            "auditor_fn_violation": 0.06906932902431519,
            "auditor_fp_violation": 0.027162203755001547,
            "ave_precision_score": 0.5938498127682531,
            "fpr": 0.21820175438596492,
            "logloss": 5.87993860001577,
            "mae": 0.4211852245729204,
            "precision": 0.5792811839323467,
            "recall": 0.6008771929824561
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.6289183274430548,
            "auditor_fn_violation": 0.07845432222854096,
            "auditor_fp_violation": 0.02143827260573619,
            "ave_precision_score": 0.6342551647998679,
            "fpr": 0.21075740944017562,
            "logloss": 5.5992064445831895,
            "mae": 0.4153348702441207,
            "precision": 0.62426614481409,
            "recall": 0.6405622489959839
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8525795321850044,
            "auditor_fn_violation": 0.013821560480147743,
            "auditor_fp_violation": 0.015725992613111736,
            "ave_precision_score": 0.8528320873445032,
            "fpr": 0.14692982456140352,
            "logloss": 0.697835854548125,
            "mae": 0.2646899560687138,
            "precision": 0.7335984095427436,
            "recall": 0.8092105263157895
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8546122419768584,
            "auditor_fn_violation": 0.01383360004232077,
            "auditor_fp_violation": 0.02232599676273047,
            "ave_precision_score": 0.8548398125652652,
            "fpr": 0.141602634467618,
            "logloss": 0.8747499462255299,
            "mae": 0.26779291211335904,
            "precision": 0.7570621468926554,
            "recall": 0.8072289156626506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.7998583365676316,
            "auditor_fn_violation": 0.012205678670360116,
            "auditor_fp_violation": 0.00931296168051709,
            "ave_precision_score": 0.8007234397882288,
            "fpr": 0.14802631578947367,
            "logloss": 0.5834003716329023,
            "mae": 0.32140533377263314,
            "precision": 0.7283702213279678,
            "recall": 0.793859649122807
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8484748615222235,
            "auditor_fn_violation": 0.010333320108094292,
            "auditor_fp_violation": 0.01304210310889505,
            "ave_precision_score": 0.8487332477981923,
            "fpr": 0.12623490669593854,
            "logloss": 0.5938315106963112,
            "mae": 0.3053469993188799,
            "precision": 0.777992277992278,
            "recall": 0.8092369477911646
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8193925240313591,
            "auditor_fn_violation": 0.011748807325330875,
            "auditor_fp_violation": 0.01317713142505387,
            "ave_precision_score": 0.8207842640237788,
            "fpr": 0.12719298245614036,
            "logloss": 0.7691259305567524,
            "mae": 0.2675609026014126,
            "precision": 0.7489177489177489,
            "recall": 0.7587719298245614
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8369984610421497,
            "auditor_fn_violation": 0.014181864670537257,
            "auditor_fp_violation": 0.019721297140411924,
            "ave_precision_score": 0.8374940179520718,
            "fpr": 0.11745334796926454,
            "logloss": 0.8611486654318808,
            "mae": 0.2672990358609933,
            "precision": 0.7829614604462475,
            "recall": 0.7751004016064257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8286400557647964,
            "auditor_fn_violation": 0.01544225146198831,
            "auditor_fp_violation": 0.0034577947060634097,
            "ave_precision_score": 0.8300119919802977,
            "fpr": 0.1425438596491228,
            "logloss": 0.6763201919678281,
            "mae": 0.26783058218415573,
            "precision": 0.7352342158859471,
            "recall": 0.7916666666666666
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8474385435381973,
            "auditor_fn_violation": 0.014891619166016426,
            "auditor_fp_violation": 0.015179019941899264,
            "ave_precision_score": 0.847715296511147,
            "fpr": 0.12184412733260154,
            "logloss": 0.7691973930091223,
            "mae": 0.26772344837171325,
            "precision": 0.7806324110671937,
            "recall": 0.7931726907630522
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8259231576900772,
            "auditor_fn_violation": 0.016937903970452448,
            "auditor_fp_violation": 0.009671244998461065,
            "ave_precision_score": 0.8272820629613508,
            "fpr": 0.16885964912280702,
            "logloss": 0.684464872271782,
            "mae": 0.2743116696472406,
            "precision": 0.7105263157894737,
            "recall": 0.8289473684210527
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.845571933689699,
            "auditor_fn_violation": 0.008243732338795355,
            "auditor_fp_violation": 0.019229593640280356,
            "ave_precision_score": 0.8458535943361943,
            "fpr": 0.15148188803512624,
            "logloss": 0.7012208599250729,
            "mae": 0.2700992961322403,
            "precision": 0.7467889908256881,
            "recall": 0.8172690763052208
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8315336389926488,
            "auditor_fn_violation": 0.015781298091720526,
            "auditor_fp_violation": 0.00923361034164359,
            "ave_precision_score": 0.8329093932791848,
            "fpr": 0.13596491228070176,
            "logloss": 0.6587777118752364,
            "mae": 0.26636130548775405,
            "precision": 0.7432712215320911,
            "recall": 0.7872807017543859
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.849639412630747,
            "auditor_fn_violation": 0.011122426037850635,
            "auditor_fp_violation": 0.015436832047373637,
            "ave_precision_score": 0.8499137812336668,
            "fpr": 0.11964873765093303,
            "logloss": 0.7496165348162203,
            "mae": 0.26497259455822275,
            "precision": 0.7833001988071571,
            "recall": 0.7911646586345381
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8276568642568809,
            "auditor_fn_violation": 0.015206602031394276,
            "auditor_fp_violation": 0.012722664666051101,
            "ave_precision_score": 0.8290700802599358,
            "fpr": 0.15021929824561403,
            "logloss": 0.66065353032907,
            "mae": 0.2695561921409779,
            "precision": 0.7276341948310139,
            "recall": 0.8026315789473685
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8482976910565669,
            "auditor_fn_violation": 0.012969551091302644,
            "auditor_fp_violation": 0.018878756548294587,
            "ave_precision_score": 0.8485851440198087,
            "fpr": 0.13172338090010977,
            "logloss": 0.7458096720988511,
            "mae": 0.26582296368728764,
            "precision": 0.7709923664122137,
            "recall": 0.8112449799196787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.820679866650407,
            "auditor_fn_violation": 0.01601454293628809,
            "auditor_fp_violation": 0.012888581101877508,
            "ave_precision_score": 0.822091695293928,
            "fpr": 0.14035087719298245,
            "logloss": 0.6782107259335657,
            "mae": 0.27406307307446487,
            "precision": 0.7344398340248963,
            "recall": 0.7763157894736842
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8383118170900656,
            "auditor_fn_violation": 0.0077632153201169125,
            "auditor_fp_violation": 0.015790326996116877,
            "ave_precision_score": 0.8386416869583082,
            "fpr": 0.1163556531284303,
            "logloss": 0.769611597930472,
            "mae": 0.2730898934545439,
            "precision": 0.7888446215139442,
            "recall": 0.7951807228915663
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8181995493743359,
            "auditor_fn_violation": 0.009839566020313946,
            "auditor_fp_violation": 0.011388119421360426,
            "ave_precision_score": 0.8185785613893148,
            "fpr": 0.13925438596491227,
            "logloss": 0.5704518204726208,
            "mae": 0.31191959159976623,
            "precision": 0.735966735966736,
            "recall": 0.7763157894736842
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8565385305752489,
            "auditor_fn_violation": 0.012555160267855178,
            "auditor_fp_violation": 0.01311652309810415,
            "ave_precision_score": 0.8567707289705049,
            "fpr": 0.10757409440175632,
            "logloss": 0.6134075066037153,
            "mae": 0.2974233495823993,
            "precision": 0.802020202020202,
            "recall": 0.7971887550200804
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8304663144866183,
            "auditor_fn_violation": 0.0179093567251462,
            "auditor_fp_violation": 0.00923361034164359,
            "ave_precision_score": 0.8318450993544029,
            "fpr": 0.13596491228070176,
            "logloss": 0.6589737122929111,
            "mae": 0.26753178017317375,
            "precision": 0.7443298969072165,
            "recall": 0.7916666666666666
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8479237501608647,
            "auditor_fn_violation": 0.011788096403175819,
            "auditor_fp_violation": 0.014339137206539397,
            "ave_precision_score": 0.8481999209443929,
            "fpr": 0.11964873765093303,
            "logloss": 0.7512491618815796,
            "mae": 0.26750150995838756,
            "precision": 0.7837301587301587,
            "recall": 0.7931726907630522
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8321080730441331,
            "auditor_fn_violation": 0.015891909048938137,
            "auditor_fp_violation": 0.009493305632502315,
            "ave_precision_score": 0.8334862902648821,
            "fpr": 0.13706140350877194,
            "logloss": 0.617160164824812,
            "mae": 0.26681359674438093,
            "precision": 0.7406639004149378,
            "recall": 0.7828947368421053
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8442258141802371,
            "auditor_fn_violation": 0.003359210717733725,
            "auditor_fp_violation": 0.01386869656046758,
            "ave_precision_score": 0.8445365053697522,
            "fpr": 0.12184412733260154,
            "logloss": 0.6529451087506031,
            "mae": 0.269626982426917,
            "precision": 0.7784431137724551,
            "recall": 0.7831325301204819
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8384023860406937,
            "auditor_fn_violation": 0.020025392428439526,
            "auditor_fp_violation": 0.009464450600184676,
            "ave_precision_score": 0.8392085741657812,
            "fpr": 0.13157894736842105,
            "logloss": 0.6359035109892751,
            "mae": 0.263996383470035,
            "precision": 0.75,
            "recall": 0.7894736842105263
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8567007492291584,
            "auditor_fn_violation": 0.010251764467309417,
            "auditor_fp_violation": 0.010296537078430697,
            "ave_precision_score": 0.8569345864619187,
            "fpr": 0.11964873765093303,
            "logloss": 0.7260149642188568,
            "mae": 0.2645927802023516,
            "precision": 0.7845849802371542,
            "recall": 0.7971887550200804
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 18998,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.6767886371047034,
            "auditor_fn_violation": 0.01035414742997847,
            "auditor_fp_violation": 0.00768265235457064,
            "ave_precision_score": 0.6752329439742588,
            "fpr": 0.06469298245614036,
            "logloss": 6.792207103757805,
            "mae": 0.40238328796800554,
            "precision": 0.7389380530973452,
            "recall": 0.36622807017543857
        },
        "train": {
            "accuracy": 0.5850713501646543,
            "auc_prc": 0.6907769859822286,
            "auditor_fn_violation": 0.023126534678781004,
            "auditor_fp_violation": 0.0041276515443476715,
            "ave_precision_score": 0.6892727849882881,
            "fpr": 0.0570801317233809,
            "logloss": 7.506441424026295,
            "mae": 0.438310208617676,
            "precision": 0.7678571428571429,
            "recall": 0.3453815261044177
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8314517657064056,
            "auditor_fn_violation": 0.01808008233302555,
            "auditor_fp_violation": 0.008339104339796859,
            "ave_precision_score": 0.8328280833333684,
            "fpr": 0.14473684210526316,
            "logloss": 0.6625637456166615,
            "mae": 0.26655570520215743,
            "precision": 0.7344064386317908,
            "recall": 0.8004385964912281
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8499408726911983,
            "auditor_fn_violation": 0.011287741525928081,
            "auditor_fp_violation": 0.017486039607381407,
            "ave_precision_score": 0.8502150118567836,
            "fpr": 0.12623490669593854,
            "logloss": 0.7503411351956156,
            "mae": 0.2640658583808709,
            "precision": 0.7762645914396887,
            "recall": 0.8012048192771084
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8290509640072883,
            "auditor_fn_violation": 0.013994690674053557,
            "auditor_fp_violation": 0.014163011695906442,
            "ave_precision_score": 0.8293275751117664,
            "fpr": 0.14583333333333334,
            "logloss": 0.7340746205693074,
            "mae": 0.27554962670342187,
            "precision": 0.7246376811594203,
            "recall": 0.7675438596491229
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8206370850989921,
            "auditor_fn_violation": 0.006564126979928502,
            "auditor_fp_violation": 0.017600327447952525,
            "ave_precision_score": 0.8209905227453694,
            "fpr": 0.13721185510428102,
            "logloss": 0.8281898256250563,
            "mae": 0.2774754637980765,
            "precision": 0.758220502901354,
            "recall": 0.7871485943775101
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8314908927325707,
            "auditor_fn_violation": 0.01222972453062481,
            "auditor_fp_violation": 0.007790858725761776,
            "ave_precision_score": 0.8328600824387128,
            "fpr": 0.10526315789473684,
            "logloss": 0.6548044704507543,
            "mae": 0.26650488053749466,
            "precision": 0.7767441860465116,
            "recall": 0.7324561403508771
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8432463471885463,
            "auditor_fn_violation": 0.015409607695325763,
            "auditor_fp_violation": 0.012002881116725097,
            "ave_precision_score": 0.8436228459308696,
            "fpr": 0.10318331503841932,
            "logloss": 0.7619619203540471,
            "mae": 0.2737765719277832,
            "precision": 0.7987152034261242,
            "recall": 0.748995983935743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8412837269683604,
            "auditor_fn_violation": 0.01263850415512466,
            "auditor_fp_violation": 0.013458467990150816,
            "ave_precision_score": 0.8415210519353999,
            "fpr": 0.13267543859649122,
            "logloss": 0.7060362513991081,
            "mae": 0.26770131078286785,
            "precision": 0.7397849462365591,
            "recall": 0.7543859649122807
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8359538135441886,
            "auditor_fn_violation": 0.012006312847438057,
            "auditor_fp_violation": 0.015553777744702233,
            "ave_precision_score": 0.8364041682317751,
            "fpr": 0.1141602634467618,
            "logloss": 0.807843617157524,
            "mae": 0.26889014180375226,
            "precision": 0.7898989898989899,
            "recall": 0.785140562248996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8383888384749567,
            "auditor_fn_violation": 0.0125158702677747,
            "auditor_fp_violation": 0.010529682209910744,
            "ave_precision_score": 0.8389016300023994,
            "fpr": 0.15021929824561403,
            "logloss": 0.6357032948122604,
            "mae": 0.2658561317512483,
            "precision": 0.7292490118577075,
            "recall": 0.8092105263157895
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8579171733889299,
            "auditor_fn_violation": 0.014091492203721584,
            "auditor_fp_violation": 0.019628272153900544,
            "ave_precision_score": 0.8581527353654954,
            "fpr": 0.1350164654226125,
            "logloss": 0.7178108293739293,
            "mae": 0.2623732056477739,
            "precision": 0.7670454545454546,
            "recall": 0.8132530120481928
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8252233924427834,
            "auditor_fn_violation": 0.015314808402585417,
            "auditor_fp_violation": 0.008618036318867352,
            "ave_precision_score": 0.826658301631589,
            "fpr": 0.1425438596491228,
            "logloss": 0.6580329180244892,
            "mae": 0.27122062220775783,
            "precision": 0.7330595482546202,
            "recall": 0.7828947368421053
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.848545847500372,
            "auditor_fn_violation": 0.016518323568698503,
            "auditor_fp_violation": 0.022979829525067583,
            "ave_precision_score": 0.8488336897485129,
            "fpr": 0.1251372118551043,
            "logloss": 0.7565854869114629,
            "mae": 0.26788678283279543,
            "precision": 0.7777777777777778,
            "recall": 0.8012048192771084
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8382681816633066,
            "auditor_fn_violation": 0.012135945675592492,
            "auditor_fp_violation": 0.006218259464450603,
            "ave_precision_score": 0.8390744232291638,
            "fpr": 0.10635964912280702,
            "logloss": 0.6404925455036937,
            "mae": 0.2649807171380472,
            "precision": 0.7764976958525346,
            "recall": 0.7390350877192983
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8554157270987848,
            "auditor_fn_violation": 0.010712443627418567,
            "auditor_fp_violation": 0.013656068019870137,
            "ave_precision_score": 0.8556525223306954,
            "fpr": 0.09989023051591657,
            "logloss": 0.7425890527101621,
            "mae": 0.26988415474702243,
            "precision": 0.8051391862955032,
            "recall": 0.7550200803212851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 18998,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8191657201784596,
            "auditor_fn_violation": 0.0172985918744229,
            "auditor_fp_violation": 0.011022622345337033,
            "ave_precision_score": 0.8205718646933887,
            "fpr": 0.17763157894736842,
            "logloss": 0.7212051513818971,
            "mae": 0.2779549316306502,
            "precision": 0.697196261682243,
            "recall": 0.8179824561403509
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8415341725289917,
            "auditor_fn_violation": 0.007516344191254593,
            "auditor_fp_violation": 0.02539582131760592,
            "ave_precision_score": 0.8418309670583354,
            "fpr": 0.1602634467618002,
            "logloss": 0.730217867094996,
            "mae": 0.2727678494190111,
            "precision": 0.7355072463768116,
            "recall": 0.8152610441767069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8152020551375079,
            "auditor_fn_violation": 0.009058075561711293,
            "auditor_fp_violation": 0.012003693444136659,
            "ave_precision_score": 0.8154165450792172,
            "fpr": 0.06907894736842106,
            "logloss": 2.7441361757920015,
            "mae": 0.2779737161610778,
            "precision": 0.8179190751445087,
            "recall": 0.6206140350877193
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8329607439363658,
            "auditor_fn_violation": 0.011294354145451189,
            "auditor_fp_violation": 0.014448109333595577,
            "ave_precision_score": 0.8332683469605122,
            "fpr": 0.07025246981339188,
            "logloss": 2.5021849697535425,
            "mae": 0.27975298156925354,
            "precision": 0.8350515463917526,
            "recall": 0.6506024096385542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8336687742964208,
            "auditor_fn_violation": 0.010378193290243155,
            "auditor_fp_violation": 0.006768909664512157,
            "ave_precision_score": 0.8339855162900002,
            "fpr": 0.0581140350877193,
            "logloss": 0.728087151118754,
            "mae": 0.29961436201552516,
            "precision": 0.8317460317460318,
            "recall": 0.5745614035087719
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.8446332386092024,
            "auditor_fn_violation": 0.014737324710477482,
            "auditor_fp_violation": 0.012170326092445574,
            "ave_precision_score": 0.8449293063352501,
            "fpr": 0.06256860592755215,
            "logloss": 0.8417949203629898,
            "mae": 0.30939641031435794,
            "precision": 0.8394366197183099,
            "recall": 0.5983935742971888
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7945977667569568,
            "auditor_fn_violation": 0.00963758079409049,
            "auditor_fp_violation": 0.017053324099723,
            "ave_precision_score": 0.796204413937514,
            "fpr": 0.20942982456140352,
            "logloss": 0.6678648539340578,
            "mae": 0.31910655256617104,
            "precision": 0.6723842195540308,
            "recall": 0.8596491228070176
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8382661549586684,
            "auditor_fn_violation": 0.012105502140284519,
            "auditor_fp_violation": 0.016247478358401353,
            "ave_precision_score": 0.8385627206778491,
            "fpr": 0.17892425905598244,
            "logloss": 0.7067059588751682,
            "mae": 0.3009578796339461,
            "precision": 0.7241962774957699,
            "recall": 0.8594377510040161
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8303544776620148,
            "auditor_fn_violation": 0.0179093567251462,
            "auditor_fp_violation": 0.00923361034164359,
            "ave_precision_score": 0.8317360914545363,
            "fpr": 0.13596491228070176,
            "logloss": 0.6589716020868358,
            "mae": 0.2675330215823675,
            "precision": 0.7443298969072165,
            "recall": 0.7916666666666666
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8479162738720107,
            "auditor_fn_violation": 0.011788096403175819,
            "auditor_fp_violation": 0.014339137206539397,
            "ave_precision_score": 0.8481924582281184,
            "fpr": 0.11964873765093303,
            "logloss": 0.7512423366870514,
            "mae": 0.2674998465870796,
            "precision": 0.7837301587301587,
            "recall": 0.7931726907630522
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8163082083483016,
            "auditor_fn_violation": 0.014355378578024009,
            "auditor_fp_violation": 0.02379818790397046,
            "ave_precision_score": 0.817865024266811,
            "fpr": 0.16776315789473684,
            "logloss": 0.711077823705012,
            "mae": 0.2724954507464679,
            "precision": 0.7096774193548387,
            "recall": 0.8201754385964912
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8382075631855553,
            "auditor_fn_violation": 0.013375125088719313,
            "auditor_fp_violation": 0.025023721371560403,
            "ave_precision_score": 0.8385891711517108,
            "fpr": 0.14818880351262348,
            "logloss": 0.7805264125154197,
            "mae": 0.26894276328533684,
            "precision": 0.7509225092250923,
            "recall": 0.8172690763052208
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8320623892547971,
            "auditor_fn_violation": 0.016598857340720224,
            "auditor_fp_violation": 0.011138042474607578,
            "ave_precision_score": 0.8334497896861902,
            "fpr": 0.15789473684210525,
            "logloss": 0.6673422947108386,
            "mae": 0.2674315167476785,
            "precision": 0.7236084452975048,
            "recall": 0.8267543859649122
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8471978889679423,
            "auditor_fn_violation": 0.010979152614850178,
            "auditor_fp_violation": 0.021525981878732628,
            "ave_precision_score": 0.8474875848684529,
            "fpr": 0.13830954994511527,
            "logloss": 0.7537472190319728,
            "mae": 0.2636311199427491,
            "precision": 0.7644859813084112,
            "recall": 0.821285140562249
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8351594499005515,
            "auditor_fn_violation": 0.013763850415512474,
            "auditor_fp_violation": 0.008555517082179135,
            "ave_precision_score": 0.8353355234145853,
            "fpr": 0.08552631578947369,
            "logloss": 0.7947064391443136,
            "mae": 0.28272420307762347,
            "precision": 0.7925531914893617,
            "recall": 0.6535087719298246
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.853873896922831,
            "auditor_fn_violation": 0.02112511516979003,
            "auditor_fp_violation": 0.013353072349518798,
            "ave_precision_score": 0.8539833207591949,
            "fpr": 0.0801317233809001,
            "logloss": 0.9498451903198997,
            "mae": 0.28157816867893243,
            "precision": 0.8206388206388207,
            "recall": 0.6706827309236948
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8303741315837331,
            "auditor_fn_violation": 0.0179093567251462,
            "auditor_fp_violation": 0.00923361034164359,
            "ave_precision_score": 0.8317556623029723,
            "fpr": 0.13596491228070176,
            "logloss": 0.6589684839579112,
            "mae": 0.2675344568624558,
            "precision": 0.7443298969072165,
            "recall": 0.7916666666666666
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8479088876754847,
            "auditor_fn_violation": 0.011788096403175819,
            "auditor_fp_violation": 0.014339137206539397,
            "ave_precision_score": 0.8481850855057878,
            "fpr": 0.11964873765093303,
            "logloss": 0.7512366021666681,
            "mae": 0.26749886309148313,
            "precision": 0.7837301587301587,
            "recall": 0.7931726907630522
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8376650634426008,
            "auditor_fn_violation": 0.013162703908895047,
            "auditor_fp_violation": 0.020198522622345335,
            "ave_precision_score": 0.8382230434304732,
            "fpr": 0.15789473684210525,
            "logloss": 0.6517272775138155,
            "mae": 0.2657765728507736,
            "precision": 0.7236084452975048,
            "recall": 0.8267543859649122
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8564153956706662,
            "auditor_fn_violation": 0.0140429996605522,
            "auditor_fp_violation": 0.022894778108828603,
            "ave_precision_score": 0.856654651959909,
            "fpr": 0.1437980241492865,
            "logloss": 0.7345584242752554,
            "mae": 0.2620421898693702,
            "precision": 0.7574074074074074,
            "recall": 0.821285140562249
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8275142634476911,
            "auditor_fn_violation": 0.015420610187750075,
            "auditor_fp_violation": 0.012722664666051101,
            "ave_precision_score": 0.8289240604793346,
            "fpr": 0.15021929824561403,
            "logloss": 0.6625388988084407,
            "mae": 0.2696889607157657,
            "precision": 0.7281746031746031,
            "recall": 0.8048245614035088
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8480916018318382,
            "auditor_fn_violation": 0.012969551091302644,
            "auditor_fp_violation": 0.019723954997169394,
            "ave_precision_score": 0.8483785641183559,
            "fpr": 0.13391877058177826,
            "logloss": 0.7471617669608424,
            "mae": 0.26580577241743425,
            "precision": 0.7680608365019012,
            "recall": 0.8112449799196787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8341827753841877,
            "auditor_fn_violation": 0.009740977993228686,
            "auditor_fp_violation": 0.025623268698060957,
            "ave_precision_score": 0.8346299508676047,
            "fpr": 0.2532894736842105,
            "logloss": 0.9735619277616795,
            "mae": 0.30346319198549,
            "precision": 0.6457055214723927,
            "recall": 0.9232456140350878
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.8470350772985764,
            "auditor_fn_violation": 0.00836716790322652,
            "auditor_fp_violation": 0.030028465645872488,
            "ave_precision_score": 0.847439253015519,
            "fpr": 0.24259055982436883,
            "logloss": 0.9622460100822414,
            "mae": 0.29241141718761443,
            "precision": 0.6730769230769231,
            "recall": 0.9136546184738956
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8239150026355226,
            "auditor_fn_violation": 0.015747633887349954,
            "auditor_fp_violation": 0.010488804247460765,
            "ave_precision_score": 0.8252734760711824,
            "fpr": 0.14364035087719298,
            "logloss": 0.6808833781206765,
            "mae": 0.273263110585765,
            "precision": 0.7315573770491803,
            "recall": 0.7828947368421053
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8449628443044437,
            "auditor_fn_violation": 0.01077195720312645,
            "auditor_fp_violation": 0.019444880037635254,
            "ave_precision_score": 0.8452360882960189,
            "fpr": 0.11855104281009879,
            "logloss": 0.7041940003079405,
            "mae": 0.2703359700727142,
            "precision": 0.7835671342685371,
            "recall": 0.785140562248996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8313433419708494,
            "auditor_fn_violation": 0.016230955678670365,
            "auditor_fp_violation": 0.009094144352108341,
            "ave_precision_score": 0.8327314966374937,
            "fpr": 0.1337719298245614,
            "logloss": 0.6582508454045192,
            "mae": 0.26648925124949485,
            "precision": 0.7458333333333333,
            "recall": 0.7850877192982456
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8447963778894003,
            "auditor_fn_violation": 0.009339222973121907,
            "auditor_fp_violation": 0.01318031166028338,
            "ave_precision_score": 0.8450883085449806,
            "fpr": 0.11964873765093303,
            "logloss": 0.756873857322579,
            "mae": 0.2671343052379222,
            "precision": 0.782,
            "recall": 0.785140562248996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8240699128954417,
            "auditor_fn_violation": 0.015009425977223764,
            "auditor_fp_violation": 0.010488804247460765,
            "ave_precision_score": 0.8254303950417503,
            "fpr": 0.14364035087719298,
            "logloss": 0.6817299771583039,
            "mae": 0.27294959932331914,
            "precision": 0.7304526748971193,
            "recall": 0.7785087719298246
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8439626240495377,
            "auditor_fn_violation": 0.0073245782250847494,
            "auditor_fp_violation": 0.018796362988813087,
            "ave_precision_score": 0.8442379521818691,
            "fpr": 0.1207464324917673,
            "logloss": 0.7069376684554732,
            "mae": 0.27069495105280733,
            "precision": 0.780439121756487,
            "recall": 0.785140562248996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8287477248168064,
            "auditor_fn_violation": 0.01499740304709141,
            "auditor_fp_violation": 0.019683941212680828,
            "ave_precision_score": 0.8301542068592594,
            "fpr": 0.14692982456140352,
            "logloss": 0.6575387071709253,
            "mae": 0.2688727086165471,
            "precision": 0.7303822937625755,
            "recall": 0.7960526315789473
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8487826231742959,
            "auditor_fn_violation": 0.01188948990252999,
            "auditor_fp_violation": 0.02433533647137621,
            "ave_precision_score": 0.8490640919560621,
            "fpr": 0.13391877058177826,
            "logloss": 0.7441435742610156,
            "mae": 0.26588236923761094,
            "precision": 0.767175572519084,
            "recall": 0.8072289156626506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7980953229018034,
            "auditor_fn_violation": 0.012159991535857189,
            "auditor_fp_violation": 0.018909664512157587,
            "ave_precision_score": 0.7996276649863047,
            "fpr": 0.17543859649122806,
            "logloss": 0.8742225911218341,
            "mae": 0.28331736923954065,
            "precision": 0.6952380952380952,
            "recall": 0.8004385964912281
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8147491655113046,
            "auditor_fn_violation": 0.01352501113124287,
            "auditor_fp_violation": 0.024558596439003516,
            "ave_precision_score": 0.8152186380821257,
            "fpr": 0.14928649835345773,
            "logloss": 0.9937122605595773,
            "mae": 0.27912116413122634,
            "precision": 0.7476808905380334,
            "recall": 0.8092369477911646
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8336390140856312,
            "auditor_fn_violation": 0.014004309018159437,
            "auditor_fp_violation": 0.011859418282548481,
            "ave_precision_score": 0.8338731750114496,
            "fpr": 0.1699561403508772,
            "logloss": 0.7093238039894019,
            "mae": 0.2784218583883312,
            "precision": 0.7001934235976789,
            "recall": 0.793859649122807
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8414225083069882,
            "auditor_fn_violation": 0.008040945340087024,
            "auditor_fp_violation": 0.022995776665612384,
            "ave_precision_score": 0.8416845017914458,
            "fpr": 0.15367727771679474,
            "logloss": 0.7350513000745709,
            "mae": 0.2734285257715234,
            "precision": 0.7435897435897436,
            "recall": 0.8152610441767069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.833632430277151,
            "auditor_fn_violation": 0.017101415820252392,
            "auditor_fp_violation": 0.013446445060018464,
            "ave_precision_score": 0.8344510015755215,
            "fpr": 0.1425438596491228,
            "logloss": 0.6513156197395306,
            "mae": 0.26724040965668533,
            "precision": 0.7389558232931727,
            "recall": 0.8070175438596491
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.850519100831684,
            "auditor_fn_violation": 0.010990173647388678,
            "auditor_fp_violation": 0.016319240490852985,
            "ave_precision_score": 0.8507927065839365,
            "fpr": 0.1251372118551043,
            "logloss": 0.7443821754964389,
            "mae": 0.2665689895895647,
            "precision": 0.77734375,
            "recall": 0.7991967871485943
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8192345455082943,
            "auditor_fn_violation": 0.015218624961526626,
            "auditor_fp_violation": 0.014841104955370894,
            "ave_precision_score": 0.8206740892816958,
            "fpr": 0.18969298245614036,
            "logloss": 0.7233751960050201,
            "mae": 0.27956726833143947,
            "precision": 0.6921708185053381,
            "recall": 0.8530701754385965
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8365791786412328,
            "auditor_fn_violation": 0.006625844762144077,
            "auditor_fp_violation": 0.02322435234675463,
            "ave_precision_score": 0.836902538232823,
            "fpr": 0.17453347969264543,
            "logloss": 0.7925108954050748,
            "mae": 0.2730202546598628,
            "precision": 0.7253886010362695,
            "recall": 0.8433734939759037
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8331388081518464,
            "auditor_fn_violation": 0.01092643890427824,
            "auditor_fp_violation": 0.0075888734995383225,
            "ave_precision_score": 0.833566800482875,
            "fpr": 0.1513157894736842,
            "logloss": 0.51694850090485,
            "mae": 0.31949027432489735,
            "precision": 0.7267326732673267,
            "recall": 0.8048245614035088
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.870233187208706,
            "auditor_fn_violation": 0.006154144569496429,
            "auditor_fp_violation": 0.008922425134819788,
            "ave_precision_score": 0.8704102075753025,
            "fpr": 0.12184412733260154,
            "logloss": 0.4954504502777499,
            "mae": 0.30453414038214077,
            "precision": 0.7852998065764023,
            "recall": 0.8152610441767069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.811544727690254,
            "auditor_fn_violation": 0.013333429516774393,
            "auditor_fp_violation": 0.018034395198522624,
            "ave_precision_score": 0.812947751633226,
            "fpr": 0.17763157894736842,
            "logloss": 0.7692568416599336,
            "mae": 0.2804206405583188,
            "precision": 0.6960600375234521,
            "recall": 0.8135964912280702
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8350664259019631,
            "auditor_fn_violation": 0.010606641715049001,
            "auditor_fp_violation": 0.02636328117732424,
            "ave_precision_score": 0.8353710144938226,
            "fpr": 0.16575192096597147,
            "logloss": 0.7801772894112659,
            "mae": 0.27525026021121496,
            "precision": 0.7284172661870504,
            "recall": 0.8132530120481928
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 18998,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.8351928014188194,
            "auditor_fn_violation": 0.01572118344105879,
            "auditor_fp_violation": 0.01809931902123731,
            "ave_precision_score": 0.835994616716207,
            "fpr": 0.14144736842105263,
            "logloss": 0.7242668074840027,
            "mae": 0.265905714243004,
            "precision": 0.7435387673956262,
            "recall": 0.8201754385964912
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8504564711778072,
            "auditor_fn_violation": 0.01174401227302184,
            "auditor_fp_violation": 0.018097346661599022,
            "ave_precision_score": 0.8507099922279282,
            "fpr": 0.1251372118551043,
            "logloss": 0.8971288145461068,
            "mae": 0.26613370032812567,
            "precision": 0.7790697674418605,
            "recall": 0.8072289156626506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.6258177525414277,
            "auditor_fn_violation": 0.008141928285626349,
            "auditor_fp_violation": 0.013754232071406608,
            "ave_precision_score": 0.6258211903102774,
            "fpr": 0.3442982456140351,
            "logloss": 1.8238991354965959,
            "mae": 0.38087514778781173,
            "precision": 0.5762483130904184,
            "recall": 0.9364035087719298
        },
        "train": {
            "accuracy": 0.6300768386388584,
            "auc_prc": 0.6722209196274471,
            "auditor_fn_violation": 0.004106436723843784,
            "auditor_fp_violation": 0.012037433254572188,
            "ave_precision_score": 0.6717144610972202,
            "fpr": 0.3358946212952799,
            "logloss": 1.7705989936934383,
            "mae": 0.36942168070507475,
            "precision": 0.6041397153945667,
            "recall": 0.9377510040160643
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8285624380101776,
            "auditor_fn_violation": 0.015485534010464763,
            "auditor_fp_violation": 0.0034577947060634097,
            "ave_precision_score": 0.8299345121310429,
            "fpr": 0.1425438596491228,
            "logloss": 0.6763710939411793,
            "mae": 0.26789267087638624,
            "precision": 0.7357723577235772,
            "recall": 0.793859649122807
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8474196558648318,
            "auditor_fn_violation": 0.014891619166016426,
            "auditor_fp_violation": 0.01669665615041343,
            "ave_precision_score": 0.8476968361782604,
            "fpr": 0.1251372118551043,
            "logloss": 0.7689796706848938,
            "mae": 0.2676674686677196,
            "precision": 0.7760314341846758,
            "recall": 0.7931726907630522
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8275324725041315,
            "auditor_fn_violation": 0.01807767774699908,
            "auditor_fp_violation": 0.009320175438596492,
            "ave_precision_score": 0.8290475032282146,
            "fpr": 0.13925438596491227,
            "logloss": 0.6743917443470037,
            "mae": 0.26365859048741325,
            "precision": 0.741869918699187,
            "recall": 0.8004385964912281
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.854889009895544,
            "auditor_fn_violation": 0.012731496788471115,
            "auditor_fp_violation": 0.018456157323857185,
            "ave_precision_score": 0.8551608526176232,
            "fpr": 0.12294182217343579,
            "logloss": 0.7370900828944809,
            "mae": 0.26391663395293313,
            "precision": 0.7799607072691552,
            "recall": 0.7971887550200804
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.7814558279089048,
            "auditor_fn_violation": 0.010616247306863653,
            "auditor_fp_violation": 0.012725069252077567,
            "ave_precision_score": 0.7830711077717865,
            "fpr": 0.16447368421052633,
            "logloss": 0.6778570036487293,
            "mae": 0.3274328764558414,
            "precision": 0.7087378640776699,
            "recall": 0.8004385964912281
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8245980810886017,
            "auditor_fn_violation": 0.008979937312366919,
            "auditor_fp_violation": 0.014025510109158184,
            "ave_precision_score": 0.8250859860931316,
            "fpr": 0.141602634467618,
            "logloss": 0.7251351789948662,
            "mae": 0.30473258296582223,
            "precision": 0.7602230483271375,
            "recall": 0.821285140562249
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8205273124133345,
            "auditor_fn_violation": 0.012775565558633425,
            "auditor_fp_violation": 0.012792397660818711,
            "ave_precision_score": 0.8219453544817543,
            "fpr": 0.16666666666666666,
            "logloss": 0.6837449829569053,
            "mae": 0.27494131951125805,
            "precision": 0.708253358925144,
            "recall": 0.8092105263157895
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8399713397326323,
            "auditor_fn_violation": 0.011583105197959792,
            "auditor_fp_violation": 0.0203193149108422,
            "ave_precision_score": 0.8402982474267373,
            "fpr": 0.14489571899012074,
            "logloss": 0.7658500108732017,
            "mae": 0.270481701093768,
            "precision": 0.7532710280373832,
            "recall": 0.8092369477911646
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8250726161896503,
            "auditor_fn_violation": 0.015747633887349954,
            "auditor_fp_violation": 0.009526969836872888,
            "ave_precision_score": 0.8264292026355163,
            "fpr": 0.1425438596491228,
            "logloss": 0.6822893466035316,
            "mae": 0.27176287143503414,
            "precision": 0.7330595482546202,
            "recall": 0.7828947368421053
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8459003492438844,
            "auditor_fn_violation": 0.010560353378387321,
            "auditor_fp_violation": 0.016717919004473177,
            "ave_precision_score": 0.8461723553739144,
            "fpr": 0.11964873765093303,
            "logloss": 0.7055637163768053,
            "mae": 0.2683417230611566,
            "precision": 0.7828685258964143,
            "recall": 0.7891566265060241
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 18998,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8192181876043942,
            "auditor_fn_violation": 0.015740420129270547,
            "auditor_fp_violation": 0.01989554478301016,
            "ave_precision_score": 0.8206549152731228,
            "fpr": 0.14802631578947367,
            "logloss": 0.6992018200178935,
            "mae": 0.27766038064456183,
            "precision": 0.725609756097561,
            "recall": 0.7828947368421053
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8320466465961293,
            "auditor_fn_violation": 0.01428987078941452,
            "auditor_fp_violation": 0.014262059360572826,
            "ave_precision_score": 0.8323916052331692,
            "fpr": 0.13721185510428102,
            "logloss": 0.7800566951482174,
            "mae": 0.27333529956816516,
            "precision": 0.7623574144486692,
            "recall": 0.8052208835341366
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8287083699547805,
            "auditor_fn_violation": 0.01635599415204678,
            "auditor_fp_violation": 0.008762311480455521,
            "ave_precision_score": 0.8300797746197535,
            "fpr": 0.14364035087719298,
            "logloss": 0.6757650414323167,
            "mae": 0.2679738068153435,
            "precision": 0.733739837398374,
            "recall": 0.7916666666666666
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8477378543781283,
            "auditor_fn_violation": 0.014031978628013708,
            "auditor_fp_violation": 0.01895849225101863,
            "ave_precision_score": 0.8480162622723766,
            "fpr": 0.12623490669593854,
            "logloss": 0.7681632696059933,
            "mae": 0.26736127794251613,
            "precision": 0.7749510763209393,
            "recall": 0.7951807228915663
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8251844908017005,
            "auditor_fn_violation": 0.013403162511542012,
            "auditor_fp_violation": 0.01655557479224377,
            "ave_precision_score": 0.8267109845985374,
            "fpr": 0.16776315789473684,
            "logloss": 0.6595151349616549,
            "mae": 0.27210224414067186,
            "precision": 0.7145522388059702,
            "recall": 0.8399122807017544
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8527260235039619,
            "auditor_fn_violation": 0.012907833309087065,
            "auditor_fp_violation": 0.023163221641332863,
            "ave_precision_score": 0.8529906503594062,
            "fpr": 0.1525795828759605,
            "logloss": 0.7158904658226894,
            "mae": 0.2681645540522412,
            "precision": 0.7472727272727273,
            "recall": 0.8253012048192772
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8117872618208862,
            "auditor_fn_violation": 0.010897583871960602,
            "auditor_fp_violation": 0.006540473991997541,
            "ave_precision_score": 0.8132230169120515,
            "fpr": 0.0668859649122807,
            "logloss": 0.6384884651332519,
            "mae": 0.3295223408295262,
            "precision": 0.8262108262108262,
            "recall": 0.6359649122807017
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8449848305656908,
            "auditor_fn_violation": 0.01769977825682533,
            "auditor_fp_violation": 0.006538327623371065,
            "ave_precision_score": 0.845302205435505,
            "fpr": 0.05598243688254665,
            "logloss": 0.720267663209542,
            "mae": 0.32832132399280184,
            "precision": 0.8647214854111406,
            "recall": 0.6546184738955824
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.848635470653647,
            "auditor_fn_violation": 0.01364843028624192,
            "auditor_fp_violation": 0.010070406278855037,
            "ave_precision_score": 0.8488809559586661,
            "fpr": 0.12171052631578948,
            "logloss": 0.6584571672350135,
            "mae": 0.26377358244360594,
            "precision": 0.7581699346405228,
            "recall": 0.7631578947368421
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8489753760386898,
            "auditor_fn_violation": 0.01046557249855625,
            "auditor_fp_violation": 0.01469794786879756,
            "ave_precision_score": 0.8492201853033102,
            "fpr": 0.10757409440175632,
            "logloss": 0.7769607298194621,
            "mae": 0.26754528687974855,
            "precision": 0.7983539094650206,
            "recall": 0.7791164658634538
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8264029443862544,
            "auditor_fn_violation": 0.01351617805478609,
            "auditor_fp_violation": 0.015581717451523557,
            "ave_precision_score": 0.827201089875754,
            "fpr": 0.18421052631578946,
            "logloss": 0.7684551578346888,
            "mae": 0.27523183608531293,
            "precision": 0.6962025316455697,
            "recall": 0.8442982456140351
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8570796005072746,
            "auditor_fn_violation": 0.017836439060302682,
            "auditor_fp_violation": 0.023234983773784496,
            "ave_precision_score": 0.857201576143208,
            "fpr": 0.16355653128430298,
            "logloss": 0.9008297255276089,
            "mae": 0.2646014655123881,
            "precision": 0.7372134038800705,
            "recall": 0.8393574297188755
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 18998,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8160059919727922,
            "auditor_fn_violation": 0.010257963988919677,
            "auditor_fp_violation": 0.01732023314866113,
            "ave_precision_score": 0.8174351741292797,
            "fpr": 0.12171052631578948,
            "logloss": 0.7446384535618363,
            "mae": 0.27078206204671224,
            "precision": 0.7533333333333333,
            "recall": 0.743421052631579
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8350158765465003,
            "auditor_fn_violation": 0.009661037123246003,
            "auditor_fp_violation": 0.023296114479206258,
            "ave_precision_score": 0.8353671082910825,
            "fpr": 0.11855104281009879,
            "logloss": 0.801745333642668,
            "mae": 0.2738459445698396,
            "precision": 0.7782340862422998,
            "recall": 0.7610441767068273
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8250167967060961,
            "auditor_fn_violation": 0.014352973991997538,
            "auditor_fp_violation": 0.016937903970452445,
            "ave_precision_score": 0.826383489792891,
            "fpr": 0.16776315789473684,
            "logloss": 0.6874387483679816,
            "mae": 0.2732638333286847,
            "precision": 0.7046332046332047,
            "recall": 0.8004385964912281
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8434905182831729,
            "auditor_fn_violation": 0.008389209968303508,
            "auditor_fp_violation": 0.02249344173845095,
            "ave_precision_score": 0.8437762095043833,
            "fpr": 0.1437980241492865,
            "logloss": 0.7089690932499738,
            "mae": 0.26936307975108537,
            "precision": 0.7551401869158878,
            "recall": 0.8112449799196787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.6787874323270864,
            "auditor_fn_violation": 0.010907202216066478,
            "auditor_fp_violation": 0.009613534933825793,
            "ave_precision_score": 0.6777997685709515,
            "fpr": 0.0668859649122807,
            "logloss": 6.726097615621942,
            "mae": 0.3960818090164675,
            "precision": 0.7336244541484717,
            "recall": 0.3684210526315789
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.7022104045788682,
            "auditor_fn_violation": 0.020876039834420018,
            "auditor_fp_violation": 0.006450618350374627,
            "ave_precision_score": 0.7007364975672975,
            "fpr": 0.06256860592755215,
            "logloss": 7.4060830406177605,
            "mae": 0.4288941622730455,
            "precision": 0.7625,
            "recall": 0.3674698795180723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 18998,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8100440827326216,
            "auditor_fn_violation": 0.006833833487226851,
            "auditor_fp_violation": 0.011445829485995696,
            "ave_precision_score": 0.8114720965365982,
            "fpr": 0.12719298245614036,
            "logloss": 0.7784665316810087,
            "mae": 0.27630392823092675,
            "precision": 0.7444933920704846,
            "recall": 0.7412280701754386
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8177915901240383,
            "auditor_fn_violation": 0.009985055479877802,
            "auditor_fp_violation": 0.01990734711343467,
            "ave_precision_score": 0.8184498043181547,
            "fpr": 0.12294182217343579,
            "logloss": 0.8804219572571413,
            "mae": 0.28176549861250394,
            "precision": 0.7714285714285715,
            "recall": 0.7590361445783133
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.6619473449460533,
            "auditor_fn_violation": 0.006408221760541706,
            "auditor_fp_violation": 0.016658971991381964,
            "ave_precision_score": 0.660607162362758,
            "fpr": 0.05701754385964912,
            "logloss": 6.9082660287522195,
            "mae": 0.4069289127014459,
            "precision": 0.7438423645320197,
            "recall": 0.33114035087719296
        },
        "train": {
            "accuracy": 0.579582875960483,
            "auc_prc": 0.6879175398013927,
            "auditor_fn_violation": 0.017197219173069888,
            "auditor_fp_violation": 0.010618137746084312,
            "ave_precision_score": 0.6864000525851874,
            "fpr": 0.05598243688254665,
            "logloss": 7.668928650460638,
            "mae": 0.442629266872599,
            "precision": 0.7649769585253456,
            "recall": 0.3333333333333333
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8368656198882518,
            "auditor_fn_violation": 0.009377885503231768,
            "auditor_fp_violation": 0.014586218836565103,
            "ave_precision_score": 0.8372001200811181,
            "fpr": 0.19407894736842105,
            "logloss": 0.5969287727393726,
            "mae": 0.31156254757075763,
            "precision": 0.6910994764397905,
            "recall": 0.868421052631579
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.86262166266159,
            "auditor_fn_violation": 0.009760226416092475,
            "auditor_fp_violation": 0.01769601029122137,
            "ave_precision_score": 0.8628659901629434,
            "fpr": 0.1668496158068057,
            "logloss": 0.6344125578989215,
            "mae": 0.2949299998272327,
            "precision": 0.7397260273972602,
            "recall": 0.8674698795180723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.8259666403022656,
            "auditor_fn_violation": 0.012018120960295475,
            "auditor_fp_violation": 0.010936057248384121,
            "ave_precision_score": 0.8267451158705416,
            "fpr": 0.05701754385964912,
            "logloss": 0.8799897492136091,
            "mae": 0.299778628301289,
            "precision": 0.8272425249169435,
            "recall": 0.5460526315789473
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.848534424525748,
            "auditor_fn_violation": 0.02374812091395219,
            "auditor_fp_violation": 0.007875229572377428,
            "ave_precision_score": 0.8486756144704686,
            "fpr": 0.052689352360043906,
            "logloss": 1.0496733713186108,
            "mae": 0.3063320696068407,
            "precision": 0.8567164179104477,
            "recall": 0.5763052208835341
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.8461720474893206,
            "auditor_fn_violation": 0.0104936134195137,
            "auditor_fp_violation": 0.018587449984610645,
            "ave_precision_score": 0.8463948602907801,
            "fpr": 0.2225877192982456,
            "logloss": 0.8325553566921833,
            "mae": 0.2960431591289964,
            "precision": 0.6655683690280065,
            "recall": 0.8859649122807017
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8427590266222671,
            "auditor_fn_violation": 0.010615458541079799,
            "auditor_fp_violation": 0.028882929383403817,
            "ave_precision_score": 0.8430351087951486,
            "fpr": 0.21075740944017562,
            "logloss": 0.942891781589145,
            "mae": 0.28559529408155454,
            "precision": 0.6990595611285266,
            "recall": 0.8955823293172691
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8287046223766386,
            "auditor_fn_violation": 0.01635599415204678,
            "auditor_fp_violation": 0.008762311480455521,
            "ave_precision_score": 0.8300759778793503,
            "fpr": 0.14364035087719298,
            "logloss": 0.6756806728300748,
            "mae": 0.26798098270947196,
            "precision": 0.733739837398374,
            "recall": 0.7916666666666666
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8478355973394939,
            "auditor_fn_violation": 0.014031978628013708,
            "auditor_fp_violation": 0.01895849225101863,
            "ave_precision_score": 0.8481125250380144,
            "fpr": 0.12623490669593854,
            "logloss": 0.7674554367951113,
            "mae": 0.2673538387008555,
            "precision": 0.7749510763209393,
            "recall": 0.7951807228915663
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8499489160315581,
            "auditor_fn_violation": 0.01470885272391505,
            "auditor_fp_violation": 0.007560018467220684,
            "ave_precision_score": 0.8501899112911349,
            "fpr": 0.14473684210526316,
            "logloss": 0.6274744896176216,
            "mae": 0.2659774668982017,
            "precision": 0.7386138613861386,
            "recall": 0.8179824561403509
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8524132025273914,
            "auditor_fn_violation": 0.012841707113856089,
            "auditor_fp_violation": 0.01663818330174914,
            "ave_precision_score": 0.8526762318342829,
            "fpr": 0.12843029637760703,
            "logloss": 0.7215035012020435,
            "mae": 0.26441920730510365,
            "precision": 0.7745664739884393,
            "recall": 0.8072289156626506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.821349021847779,
            "auditor_fn_violation": 0.011101973684210535,
            "auditor_fp_violation": 0.009618344105878737,
            "ave_precision_score": 0.8216974786296215,
            "fpr": 0.11403508771929824,
            "logloss": 0.567295145386587,
            "mae": 0.30775494174586104,
            "precision": 0.7652370203160271,
            "recall": 0.743421052631579
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8581269700544115,
            "auditor_fn_violation": 0.011475099079082526,
            "auditor_fp_violation": 0.01436305791735661,
            "ave_precision_score": 0.8583544041408226,
            "fpr": 0.09001097694840834,
            "logloss": 0.6186284375489534,
            "mae": 0.29464850770902573,
            "precision": 0.8225108225108225,
            "recall": 0.7630522088353414
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.8140957621323245,
            "auditor_fn_violation": 0.00699013157894737,
            "auditor_fp_violation": 0.015480724838411824,
            "ave_precision_score": 0.8141668596621299,
            "fpr": 0.3168859649122807,
            "logloss": 1.2507741360700781,
            "mae": 0.34870469166905915,
            "precision": 0.6019283746556474,
            "recall": 0.9583333333333334
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.805358354544805,
            "auditor_fn_violation": 0.006076997341726952,
            "auditor_fp_violation": 0.02852146086438818,
            "ave_precision_score": 0.8048831876576661,
            "fpr": 0.29198682766191,
            "logloss": 1.2841336866482336,
            "mae": 0.32731256241679885,
            "precision": 0.6390773405698779,
            "recall": 0.9457831325301205
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8254210045508801,
            "auditor_fn_violation": 0.012140754847645435,
            "auditor_fp_violation": 0.007367651585103116,
            "ave_precision_score": 0.8268189846102694,
            "fpr": 0.12280701754385964,
            "logloss": 0.7450741842526356,
            "mae": 0.27045694943376025,
            "precision": 0.7516629711751663,
            "recall": 0.743421052631579
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8459311901430117,
            "auditor_fn_violation": 0.012625694876101554,
            "auditor_fp_violation": 0.015888667696143183,
            "ave_precision_score": 0.8462276357225492,
            "fpr": 0.10757409440175632,
            "logloss": 0.9113621271197456,
            "mae": 0.2709266884113645,
            "precision": 0.7962577962577962,
            "recall": 0.7690763052208835
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8319814433017219,
            "auditor_fn_violation": 0.011996479686057252,
            "auditor_fp_violation": 0.02046783625730995,
            "ave_precision_score": 0.8328327485240713,
            "fpr": 0.16666666666666666,
            "logloss": 0.6645026525715849,
            "mae": 0.2706022485418917,
            "precision": 0.7148217636022514,
            "recall": 0.8355263157894737
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8495544530606549,
            "auditor_fn_violation": 0.014087083790706187,
            "auditor_fp_violation": 0.025924734812342032,
            "ave_precision_score": 0.8498143913472676,
            "fpr": 0.150384193194292,
            "logloss": 0.7457475430845031,
            "mae": 0.26614357263983807,
            "precision": 0.7518115942028986,
            "recall": 0.8333333333333334
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8366135677964683,
            "auditor_fn_violation": 0.015004616805170823,
            "auditor_fp_violation": 0.008971510464758387,
            "ave_precision_score": 0.8368804982975296,
            "fpr": 0.15899122807017543,
            "logloss": 0.6601871816833225,
            "mae": 0.2683243345130839,
            "precision": 0.7216890595009597,
            "recall": 0.8245614035087719
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8451817312554407,
            "auditor_fn_violation": 0.010320094869048096,
            "auditor_fp_violation": 0.019311987199761863,
            "ave_precision_score": 0.8455625342952127,
            "fpr": 0.13830954994511527,
            "logloss": 0.67677355107433,
            "mae": 0.2654378799394166,
            "precision": 0.7631578947368421,
            "recall": 0.8152610441767069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 18998,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8271527076294714,
            "auditor_fn_violation": 0.016844125115420133,
            "auditor_fp_violation": 0.012927054478301014,
            "ave_precision_score": 0.8285402102749186,
            "fpr": 0.13486842105263158,
            "logloss": 0.6851815440199654,
            "mae": 0.26747278262589,
            "precision": 0.74375,
            "recall": 0.7828947368421053
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8435818459084622,
            "auditor_fn_violation": 0.011148876515943026,
            "auditor_fp_violation": 0.01614913765837504,
            "ave_precision_score": 0.8438696148583902,
            "fpr": 0.11525795828759605,
            "logloss": 0.7857810775078163,
            "mae": 0.2684601282825002,
            "precision": 0.7878787878787878,
            "recall": 0.7831325301204819
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8393130432813973,
            "auditor_fn_violation": 0.0197176054170514,
            "auditor_fp_violation": 0.014552554632194521,
            "ave_precision_score": 0.8396279110223148,
            "fpr": 0.17214912280701755,
            "logloss": 0.7289041799827497,
            "mae": 0.264715297506281,
            "precision": 0.7092592592592593,
            "recall": 0.8399122807017544
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8226256563211605,
            "auditor_fn_violation": 0.01095931475628089,
            "auditor_fp_violation": 0.027795865969599442,
            "ave_precision_score": 0.8230432383337898,
            "fpr": 0.17014270032930845,
            "logloss": 0.8708655144126881,
            "mae": 0.2714947162499306,
            "precision": 0.7313691507798961,
            "recall": 0.8473895582329317
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.763545139004748,
            "auditor_fn_violation": 0.018500019028047345,
            "auditor_fp_violation": 0.01643521219901194,
            "ave_precision_score": 0.7651188566493309,
            "fpr": 0.1513157894736842,
            "logloss": 2.4444284847349933,
            "mae": 0.3334732560377727,
            "precision": 0.7130977130977131,
            "recall": 0.7440347071583514
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7964299705576465,
            "auditor_fn_violation": 0.012887338212471861,
            "auditor_fp_violation": 0.018361440973954694,
            "ave_precision_score": 0.7978401007435003,
            "fpr": 0.145993413830955,
            "logloss": 2.4144196096386112,
            "mae": 0.3306863253891957,
            "precision": 0.7307692307692307,
            "recall": 0.7322515212981744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7460246094661014,
            "auditor_fn_violation": 0.02238174068577083,
            "auditor_fp_violation": 0.02172560003111993,
            "ave_precision_score": 0.7477236185877532,
            "fpr": 0.16447368421052633,
            "logloss": 2.470880178571794,
            "mae": 0.3377807767029699,
            "precision": 0.6855345911949685,
            "recall": 0.7093275488069414
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7888588239001219,
            "auditor_fn_violation": 0.01841589052442205,
            "auditor_fp_violation": 0.01860303888150673,
            "ave_precision_score": 0.7902418701279548,
            "fpr": 0.14489571899012074,
            "logloss": 2.4387840422445337,
            "mae": 0.328230544517956,
            "precision": 0.728952772073922,
            "recall": 0.7200811359026369
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7366266896440449,
            "auditor_fn_violation": 0.022039235833618758,
            "auditor_fp_violation": 0.01968335472828413,
            "ave_precision_score": 0.7384021197052223,
            "fpr": 0.16885964912280702,
            "logloss": 2.550966244489547,
            "mae": 0.34288540731403483,
            "precision": 0.6869918699186992,
            "recall": 0.7331887201735358
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7792488549648064,
            "auditor_fn_violation": 0.013679994121877526,
            "auditor_fp_violation": 0.018209129249628413,
            "ave_precision_score": 0.7806717465699602,
            "fpr": 0.16355653128430298,
            "logloss": 2.503966417160093,
            "mae": 0.33148361070578175,
            "precision": 0.7095516569200779,
            "recall": 0.7383367139959433
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7389119069592434,
            "auditor_fn_violation": 0.02502426076036078,
            "auditor_fp_violation": 0.01982436690395612,
            "ave_precision_score": 0.7406619469231177,
            "fpr": 0.17324561403508773,
            "logloss": 2.5779065309601936,
            "mae": 0.34140776872855466,
            "precision": 0.6858846918489065,
            "recall": 0.7483731019522777
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7802284126149441,
            "auditor_fn_violation": 0.015376634017852572,
            "auditor_fp_violation": 0.019545795933802186,
            "ave_precision_score": 0.7816093697071307,
            "fpr": 0.16355653128430298,
            "logloss": 2.5242208461565,
            "mae": 0.33162534855640696,
            "precision": 0.7117988394584139,
            "recall": 0.7464503042596349
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7427822546230138,
            "auditor_fn_violation": 0.0190399398713704,
            "auditor_fp_violation": 0.020427315517174312,
            "ave_precision_score": 0.7445289759256042,
            "fpr": 0.15789473684210525,
            "logloss": 2.527710840752855,
            "mae": 0.33533468472074,
            "precision": 0.7,
            "recall": 0.7288503253796096
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7855809739895763,
            "auditor_fn_violation": 0.018745421632826647,
            "auditor_fp_violation": 0.01685670618017953,
            "ave_precision_score": 0.7869746156176732,
            "fpr": 0.15477497255762898,
            "logloss": 2.478828696296381,
            "mae": 0.3267508962315085,
            "precision": 0.718562874251497,
            "recall": 0.7302231237322515
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8242202773798732,
            "auditor_fn_violation": 0.014218708376146442,
            "auditor_fp_violation": 0.018613607188703474,
            "ave_precision_score": 0.8223889410230152,
            "fpr": 0.16557017543859648,
            "logloss": 0.8225840311310162,
            "mae": 0.2994815476285702,
            "precision": 0.7254545454545455,
            "recall": 0.8655097613882863
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8503827376369664,
            "auditor_fn_violation": 0.02369284138198222,
            "auditor_fp_violation": 0.02854006586169046,
            "ave_precision_score": 0.848650889431717,
            "fpr": 0.1668496158068057,
            "logloss": 0.7288557812724633,
            "mae": 0.28756775576253263,
            "precision": 0.7356521739130435,
            "recall": 0.8580121703853956
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7470546977936623,
            "auditor_fn_violation": 0.0169040415572554,
            "auditor_fp_violation": 0.018900494028863738,
            "ave_precision_score": 0.748749977140043,
            "fpr": 0.17763157894736842,
            "logloss": 2.543415095318751,
            "mae": 0.336313564608932,
            "precision": 0.6804733727810651,
            "recall": 0.7483731019522777
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7861165373789429,
            "auditor_fn_violation": 0.01308104906673674,
            "auditor_fp_violation": 0.018687072936307444,
            "ave_precision_score": 0.7874987476220108,
            "fpr": 0.16575192096597147,
            "logloss": 2.4948859895216233,
            "mae": 0.3325539190269384,
            "precision": 0.7079303675048356,
            "recall": 0.742393509127789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7480431595315717,
            "auditor_fn_violation": 0.018785439738174075,
            "auditor_fp_violation": 0.01999698525693392,
            "ave_precision_score": 0.7496201086053449,
            "fpr": 0.17214912280701755,
            "logloss": 2.4508231767326505,
            "mae": 0.33696862879348866,
            "precision": 0.689108910891089,
            "recall": 0.754880694143167
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7834615873022734,
            "auditor_fn_violation": 0.013040970958957792,
            "auditor_fp_violation": 0.026536378867536074,
            "ave_precision_score": 0.7848321712214581,
            "fpr": 0.15916575192096596,
            "logloss": 2.407287523221164,
            "mae": 0.332467801625022,
            "precision": 0.7173489278752436,
            "recall": 0.7464503042596349
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.7525496215935708,
            "auditor_fn_violation": 0.025174106633177307,
            "auditor_fp_violation": 0.017198622943167235,
            "ave_precision_score": 0.7542006937998933,
            "fpr": 0.17324561403508773,
            "logloss": 2.489884986457188,
            "mae": 0.34465048716149854,
            "precision": 0.680161943319838,
            "recall": 0.7288503253796096
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7899017651575421,
            "auditor_fn_violation": 0.01566608701847824,
            "auditor_fp_violation": 0.018939175100709568,
            "ave_precision_score": 0.7913150499406872,
            "fpr": 0.15806805708013172,
            "logloss": 2.444800551901805,
            "mae": 0.33951604922176065,
            "precision": 0.7142857142857143,
            "recall": 0.7302231237322515
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7419947191082872,
            "auditor_fn_violation": 0.02106404840735244,
            "auditor_fp_violation": 0.023967207375423037,
            "ave_precision_score": 0.7437006093929208,
            "fpr": 0.17105263157894737,
            "logloss": 2.501683505697107,
            "mae": 0.3529426276787458,
            "precision": 0.6898608349900597,
            "recall": 0.7527114967462039
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7850809479708036,
            "auditor_fn_violation": 0.01604905560392142,
            "auditor_fp_violation": 0.024926601505260007,
            "ave_precision_score": 0.7864554793756133,
            "fpr": 0.16355653128430298,
            "logloss": 2.457985835184163,
            "mae": 0.3429749204165585,
            "precision": 0.7151051625239006,
            "recall": 0.7586206896551724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7726151674574524,
            "auditor_fn_violation": 0.02328081592267002,
            "auditor_fp_violation": 0.016605399307581592,
            "ave_precision_score": 0.7752141219855435,
            "fpr": 0.16447368421052633,
            "logloss": 2.3748062308916404,
            "mae": 0.3324160988931948,
            "precision": 0.6938775510204082,
            "recall": 0.737527114967462
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.8050712260429357,
            "auditor_fn_violation": 0.011464565386319564,
            "auditor_fp_violation": 0.018487492056155762,
            "ave_precision_score": 0.806385240499694,
            "fpr": 0.15697036223929747,
            "logloss": 2.391542394297746,
            "mae": 0.3320774219248069,
            "precision": 0.7157057654075547,
            "recall": 0.7302231237322515
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7421730977994949,
            "auditor_fn_violation": 0.019337253111085747,
            "auditor_fp_violation": 0.023716789201384845,
            "ave_precision_score": 0.7439043344325331,
            "fpr": 0.15789473684210525,
            "logloss": 2.5284683494098528,
            "mae": 0.3373103014919736,
            "precision": 0.6974789915966386,
            "recall": 0.720173535791757
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7853738853151035,
            "auditor_fn_violation": 0.019513585365256294,
            "auditor_fp_violation": 0.019863549703517353,
            "ave_precision_score": 0.7868094781488849,
            "fpr": 0.15148188803512624,
            "logloss": 2.4747585603611344,
            "mae": 0.32974871450858284,
            "precision": 0.7200811359026369,
            "recall": 0.7200811359026369
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7375769472548198,
            "auditor_fn_violation": 0.009547322753739011,
            "auditor_fp_violation": 0.015997588205547135,
            "ave_precision_score": 0.7152290475859842,
            "fpr": 0.19517543859649122,
            "logloss": 1.7783128647157118,
            "mae": 0.3156003559267622,
            "precision": 0.6815742397137746,
            "recall": 0.8264642082429501
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.753254957584782,
            "auditor_fn_violation": 0.00953190996675744,
            "auditor_fp_violation": 0.03278903775755125,
            "ave_precision_score": 0.731332078536774,
            "fpr": 0.18221734357848518,
            "logloss": 1.808946261201713,
            "mae": 0.3110021935187027,
            "precision": 0.7157534246575342,
            "recall": 0.847870182555781
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7456062213150296,
            "auditor_fn_violation": 0.02584484530197511,
            "auditor_fp_violation": 0.023998813552728832,
            "ave_precision_score": 0.7472966983867294,
            "fpr": 0.16666666666666666,
            "logloss": 2.47035439593915,
            "mae": 0.34251753346713587,
            "precision": 0.683991683991684,
            "recall": 0.7136659436008677
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7886039769969437,
            "auditor_fn_violation": 0.01925753078777974,
            "auditor_fp_violation": 0.018597786753081692,
            "ave_precision_score": 0.7899839026008454,
            "fpr": 0.15367727771679474,
            "logloss": 2.439550235953417,
            "mae": 0.33113935115345605,
            "precision": 0.720558882235529,
            "recall": 0.7322515212981744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7431247454358387,
            "auditor_fn_violation": 0.020702515507858586,
            "auditor_fp_violation": 0.023658439335589535,
            "ave_precision_score": 0.7448391424592788,
            "fpr": 0.1699561403508772,
            "logloss": 2.530143831185523,
            "mae": 0.34099022229364845,
            "precision": 0.6893787575150301,
            "recall": 0.7462039045553145
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7858928495170012,
            "auditor_fn_violation": 0.016906281798082044,
            "auditor_fp_violation": 0.019459135814788957,
            "ave_precision_score": 0.7872686276620939,
            "fpr": 0.16355653128430298,
            "logloss": 2.4870254788510917,
            "mae": 0.33051524881507555,
            "precision": 0.7084148727984344,
            "recall": 0.7342799188640974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 26311,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8162221909222718,
            "auditor_fn_violation": 0.014658731970925145,
            "auditor_fp_violation": 0.01865007585482554,
            "ave_precision_score": 0.8144338800137099,
            "fpr": 0.15570175438596492,
            "logloss": 0.7847613373577181,
            "mae": 0.28077377428889394,
            "precision": 0.7253384912959381,
            "recall": 0.8134490238611713
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8463200905754132,
            "auditor_fn_violation": 0.021268115861356473,
            "auditor_fp_violation": 0.01631836301661249,
            "ave_precision_score": 0.8445955911246822,
            "fpr": 0.1394072447859495,
            "logloss": 0.7108981023915611,
            "mae": 0.2758062553152945,
            "precision": 0.7599243856332704,
            "recall": 0.8154158215010142
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8164175108013643,
            "auditor_fn_violation": 0.008895612132282986,
            "auditor_fp_violation": 0.01734449760765551,
            "ave_precision_score": 0.8146256442837283,
            "fpr": 0.1611842105263158,
            "logloss": 0.7393566328122743,
            "mae": 0.28504340746439993,
            "precision": 0.7226415094339622,
            "recall": 0.8308026030368764
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8405285952856865,
            "auditor_fn_violation": 0.01484003268592346,
            "auditor_fp_violation": 0.013098808292060367,
            "ave_precision_score": 0.8388116360773435,
            "fpr": 0.1437980241492865,
            "logloss": 0.6900596302762451,
            "mae": 0.28383053794327373,
            "precision": 0.7565055762081785,
            "recall": 0.8255578093306288
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8480769414690512,
            "auditor_fn_violation": 0.025293031929063438,
            "auditor_fp_violation": 0.024711168164313225,
            "ave_precision_score": 0.8483360432144949,
            "fpr": 0.14473684210526316,
            "logloss": 0.59531578685766,
            "mae": 0.3100561340116863,
            "precision": 0.7391304347826086,
            "recall": 0.8112798264642083
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.871731847584492,
            "auditor_fn_violation": 0.02130819396913541,
            "auditor_fp_violation": 0.029721794757325407,
            "ave_precision_score": 0.8719413321738718,
            "fpr": 0.15587266739846323,
            "logloss": 0.5154987262539755,
            "mae": 0.3036858819968974,
            "precision": 0.7408759124087592,
            "recall": 0.8235294117647058
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 26311,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8160314559770239,
            "auditor_fn_violation": 0.014658731970925145,
            "auditor_fp_violation": 0.01865007585482554,
            "ave_precision_score": 0.8142438263625149,
            "fpr": 0.15570175438596492,
            "logloss": 0.7850575829886184,
            "mae": 0.2808752789062789,
            "precision": 0.7253384912959381,
            "recall": 0.8134490238611713
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8462313768066421,
            "auditor_fn_violation": 0.021268115861356473,
            "auditor_fp_violation": 0.01631836301661249,
            "ave_precision_score": 0.844507092118963,
            "fpr": 0.1394072447859495,
            "logloss": 0.7111365674819109,
            "mae": 0.2758615019457245,
            "precision": 0.7599243856332704,
            "recall": 0.8154158215010142
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7713182904062273,
            "auditor_fn_violation": 0.031924306427674395,
            "auditor_fp_violation": 0.03794200023339946,
            "ave_precision_score": 0.7729504890713413,
            "fpr": 0.20285087719298245,
            "logloss": 2.4136227017193916,
            "mae": 0.3797566274316188,
            "precision": 0.6489563567362429,
            "recall": 0.7418655097613883
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.8203364572998466,
            "auditor_fn_violation": 0.014325696969427088,
            "auditor_fp_violation": 0.03927279029826838,
            "ave_precision_score": 0.821655713008856,
            "fpr": 0.18551042810098792,
            "logloss": 2.3824373270755475,
            "mae": 0.3640479157916973,
            "precision": 0.692167577413479,
            "recall": 0.77079107505071
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7424405463397531,
            "auditor_fn_violation": 0.02223665182478974,
            "auditor_fp_violation": 0.019649317306570192,
            "ave_precision_score": 0.7442019719155368,
            "fpr": 0.1699561403508772,
            "logloss": 2.522139285112615,
            "mae": 0.33633696740454117,
            "precision": 0.6875,
            "recall": 0.7396963123644251
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.78323378434464,
            "auditor_fn_violation": 0.012248315049552136,
            "auditor_fp_violation": 0.016097773622760627,
            "ave_precision_score": 0.7846182551926104,
            "fpr": 0.15916575192096596,
            "logloss": 2.4808763714821525,
            "mae": 0.3290730440381119,
            "precision": 0.7156862745098039,
            "recall": 0.7403651115618661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7421749432710409,
            "auditor_fn_violation": 0.020141188111276024,
            "auditor_fp_violation": 0.02009423503325943,
            "ave_precision_score": 0.7439031425152621,
            "fpr": 0.16337719298245615,
            "logloss": 2.54093590100751,
            "mae": 0.33822128398895457,
            "precision": 0.6952965235173824,
            "recall": 0.737527114967462
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7832787392621939,
            "auditor_fn_violation": 0.014227728261523012,
            "auditor_fp_violation": 0.017001139711868233,
            "ave_precision_score": 0.784680090559378,
            "fpr": 0.16136114160263446,
            "logloss": 2.490693109015571,
            "mae": 0.32913597012808293,
            "precision": 0.7111984282907662,
            "recall": 0.7342799188640974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7475486959107146,
            "auditor_fn_violation": 0.01498696578757089,
            "auditor_fp_violation": 0.020760396001089202,
            "ave_precision_score": 0.7492552997952184,
            "fpr": 0.17214912280701755,
            "logloss": 2.5057425432897493,
            "mae": 0.33634230159541617,
            "precision": 0.6872509960159362,
            "recall": 0.7483731019522777
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7862297156783464,
            "auditor_fn_violation": 0.017215773852597177,
            "auditor_fp_violation": 0.019327832604162844,
            "ave_precision_score": 0.7876448410490846,
            "fpr": 0.15367727771679474,
            "logloss": 2.4669856859235684,
            "mae": 0.32854771971229463,
            "precision": 0.724950884086444,
            "recall": 0.7484787018255578
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7459099136995916,
            "auditor_fn_violation": 0.016642405906305897,
            "auditor_fp_violation": 0.022651904150620454,
            "ave_precision_score": 0.7476325828276696,
            "fpr": 0.14912280701754385,
            "logloss": 2.502452141275882,
            "mae": 0.33047750588338476,
            "precision": 0.7081545064377682,
            "recall": 0.7158351409978309
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7872373192708086,
            "auditor_fn_violation": 0.018295656201085236,
            "auditor_fp_violation": 0.017447570627996994,
            "ave_precision_score": 0.7886213871547223,
            "fpr": 0.14050493962678376,
            "logloss": 2.4689050953785405,
            "mae": 0.32578482304692497,
            "precision": 0.7322175732217573,
            "recall": 0.7099391480730223
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 26311,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7522150488680015,
            "auditor_fn_violation": 0.019608402785706133,
            "auditor_fp_violation": 0.02035194694052205,
            "ave_precision_score": 0.7538670625291878,
            "fpr": 0.14473684210526316,
            "logloss": 2.421132993151909,
            "mae": 0.3290915292500039,
            "precision": 0.7111597374179431,
            "recall": 0.7049891540130152
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7868532902326058,
            "auditor_fn_violation": 0.018716476332764072,
            "auditor_fp_violation": 0.018041061140026998,
            "ave_precision_score": 0.7882166633065527,
            "fpr": 0.13721185510428102,
            "logloss": 2.412867947615314,
            "mae": 0.327489376199517,
            "precision": 0.7340425531914894,
            "recall": 0.6997971602434077
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.776743600728723,
            "auditor_fn_violation": 0.015688625033299087,
            "auditor_fp_violation": 0.01545055821371611,
            "ave_precision_score": 0.7784666368718046,
            "fpr": 0.17982456140350878,
            "logloss": 2.5003388929128394,
            "mae": 0.33147332590807094,
            "precision": 0.6758893280632411,
            "recall": 0.7418655097613883
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.8076489619950944,
            "auditor_fn_violation": 0.016182649296517884,
            "auditor_fp_violation": 0.013256372144811688,
            "ave_precision_score": 0.8089660135980525,
            "fpr": 0.1602634467618002,
            "logloss": 2.567945218298488,
            "mae": 0.32656308695460706,
            "precision": 0.7114624505928854,
            "recall": 0.7302231237322515
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7407465861553744,
            "auditor_fn_violation": 0.020949880123301755,
            "auditor_fp_violation": 0.0185576885673163,
            "ave_precision_score": 0.7424847274981695,
            "fpr": 0.16337719298245615,
            "logloss": 2.5512789883176255,
            "mae": 0.33898737796902934,
            "precision": 0.6952965235173824,
            "recall": 0.737527114967462
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7834625978417501,
            "auditor_fn_violation": 0.01563268859532912,
            "auditor_fp_violation": 0.020622482260936256,
            "ave_precision_score": 0.7848569672065544,
            "fpr": 0.15916575192096596,
            "logloss": 2.4988312810189064,
            "mae": 0.32957735446701963,
            "precision": 0.7145669291338582,
            "recall": 0.7363083164300203
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7559317288454783,
            "auditor_fn_violation": 0.02026011340716216,
            "auditor_fp_violation": 0.019308943089430895,
            "ave_precision_score": 0.7576774915479525,
            "fpr": 0.1206140350877193,
            "logloss": 2.4155670554539026,
            "mae": 0.3267011140617581,
            "precision": 0.7411764705882353,
            "recall": 0.6832971800433839
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.8003648500522053,
            "auditor_fn_violation": 0.013205736513160106,
            "auditor_fp_violation": 0.014385579756196202,
            "ave_precision_score": 0.8017026772656799,
            "fpr": 0.1207464324917673,
            "logloss": 2.390780297326149,
            "mae": 0.32620617002683244,
            "precision": 0.7528089887640449,
            "recall": 0.6795131845841785
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7484604304197459,
            "auditor_fn_violation": 0.021711002016973024,
            "auditor_fp_violation": 0.017957171198506258,
            "ave_precision_score": 0.7501573610110985,
            "fpr": 0.16557017543859648,
            "logloss": 2.510075703769227,
            "mae": 0.33621730915534465,
            "precision": 0.693089430894309,
            "recall": 0.7396963123644251
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7893311408004648,
            "auditor_fn_violation": 0.013956087753243553,
            "auditor_fp_violation": 0.019188651200899166,
            "ave_precision_score": 0.7907030985804158,
            "fpr": 0.15916575192096596,
            "logloss": 2.4644937017067488,
            "mae": 0.32966102529926694,
            "precision": 0.7145669291338582,
            "recall": 0.7363083164300203
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7496030956810676,
            "auditor_fn_violation": 0.022567264147353203,
            "auditor_fp_violation": 0.020648558758314863,
            "ave_precision_score": 0.7514882132881004,
            "fpr": 0.13815789473684212,
            "logloss": 2.4424986509753728,
            "mae": 0.3247342492612581,
            "precision": 0.7212389380530974,
            "recall": 0.7071583514099783
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7986079850008874,
            "auditor_fn_violation": 0.018513859232326117,
            "auditor_fp_violation": 0.01924379854936213,
            "ave_precision_score": 0.7999665449599204,
            "fpr": 0.12952799121844127,
            "logloss": 2.415215500187766,
            "mae": 0.3191583893213504,
            "precision": 0.7484008528784648,
            "recall": 0.7119675456389453
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 26311,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7710842102390693,
            "auditor_fn_violation": 0.017218004338394804,
            "auditor_fp_violation": 0.023213521608900306,
            "ave_precision_score": 0.7727734041890612,
            "fpr": 0.14473684210526316,
            "logloss": 2.3987156290703315,
            "mae": 0.33447500761601706,
            "precision": 0.7111597374179431,
            "recall": 0.7049891540130152
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.8139666661140166,
            "auditor_fn_violation": 0.021105576868697438,
            "auditor_fp_violation": 0.018545265468831253,
            "ave_precision_score": 0.8152968697447454,
            "fpr": 0.14050493962678376,
            "logloss": 2.3799736007861774,
            "mae": 0.33242392412518773,
            "precision": 0.7344398340248963,
            "recall": 0.718052738336714
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7424062352665697,
            "auditor_fn_violation": 0.02223665182478974,
            "auditor_fp_violation": 0.019649317306570192,
            "ave_precision_score": 0.7441677124515951,
            "fpr": 0.1699561403508772,
            "logloss": 2.522368167548699,
            "mae": 0.3363775377391057,
            "precision": 0.6875,
            "recall": 0.7396963123644251
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7831625735047849,
            "auditor_fn_violation": 0.010787690677164164,
            "auditor_fp_violation": 0.016097773622760627,
            "ave_precision_score": 0.7845726532766644,
            "fpr": 0.15916575192096596,
            "logloss": 2.4810784251555518,
            "mae": 0.32909305401689004,
            "precision": 0.7151277013752456,
            "recall": 0.7383367139959433
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.812761114083677,
            "auditor_fn_violation": 0.013488507059405566,
            "auditor_fp_violation": 0.02112751390671802,
            "ave_precision_score": 0.8130976987986381,
            "fpr": 0.15899122807017543,
            "logloss": 0.7718142673182687,
            "mae": 0.28199050868843506,
            "precision": 0.7274436090225563,
            "recall": 0.8394793926247288
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8478102217927178,
            "auditor_fn_violation": 0.019829757104401245,
            "auditor_fp_violation": 0.017500091912247447,
            "ave_precision_score": 0.8480385115254816,
            "fpr": 0.15367727771679474,
            "logloss": 0.6582047332668487,
            "mae": 0.2737020527299659,
            "precision": 0.7440585009140768,
            "recall": 0.8255578093306288
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 26311,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7458806041636478,
            "auditor_fn_violation": 0.016642405906305897,
            "auditor_fp_violation": 0.021978449449566267,
            "ave_precision_score": 0.7476032690609075,
            "fpr": 0.15021929824561403,
            "logloss": 2.502729100005001,
            "mae": 0.33048420843190623,
            "precision": 0.7066381156316917,
            "recall": 0.7158351409978309
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7871797373510955,
            "auditor_fn_violation": 0.018295656201085236,
            "auditor_fp_violation": 0.017447570627996994,
            "ave_precision_score": 0.7885638942661903,
            "fpr": 0.14050493962678376,
            "logloss": 2.469154525765563,
            "mae": 0.32579000712899553,
            "precision": 0.7322175732217573,
            "recall": 0.7099391480730223
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7499098612305759,
            "auditor_fn_violation": 0.024907713970392358,
            "auditor_fp_violation": 0.02013313494378963,
            "ave_precision_score": 0.7516129277390182,
            "fpr": 0.15350877192982457,
            "logloss": 2.40733397286643,
            "mae": 0.33304878912446856,
            "precision": 0.7014925373134329,
            "recall": 0.7136659436008677
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7933791411700312,
            "auditor_fn_violation": 0.017062141106111244,
            "auditor_fp_violation": 0.0188131240185085,
            "ave_precision_score": 0.7947535068798901,
            "fpr": 0.14709110867178923,
            "logloss": 2.375803515774675,
            "mae": 0.3265992133604817,
            "precision": 0.7292929292929293,
            "recall": 0.7322515212981744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7420330298214799,
            "auditor_fn_violation": 0.023057236366404082,
            "auditor_fp_violation": 0.01975629206052826,
            "ave_precision_score": 0.743787214948191,
            "fpr": 0.1600877192982456,
            "logloss": 2.5373096727140574,
            "mae": 0.33587957240358757,
            "precision": 0.6983471074380165,
            "recall": 0.7331887201735358
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7848096114960097,
            "auditor_fn_violation": 0.01457284530073054,
            "auditor_fp_violation": 0.017823097810387663,
            "ave_precision_score": 0.786204362251358,
            "fpr": 0.15697036223929747,
            "logloss": 2.4877981279345374,
            "mae": 0.3275520456522912,
            "precision": 0.7162698412698413,
            "recall": 0.7322515212981744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 26311,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7459616053652874,
            "auditor_fn_violation": 0.016642405906305897,
            "auditor_fp_violation": 0.021978449449566267,
            "ave_precision_score": 0.7476846546996387,
            "fpr": 0.15021929824561403,
            "logloss": 2.5021154091459517,
            "mae": 0.3304067807780883,
            "precision": 0.7066381156316917,
            "recall": 0.7158351409978309
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7871585254303789,
            "auditor_fn_violation": 0.018295656201085236,
            "auditor_fp_violation": 0.017447570627996994,
            "ave_precision_score": 0.7885451162210031,
            "fpr": 0.14050493962678376,
            "logloss": 2.469015043001368,
            "mae": 0.3255204957603733,
            "precision": 0.7322175732217573,
            "recall": 0.7099391480730223
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7562722863589959,
            "auditor_fn_violation": 0.023154755109030714,
            "auditor_fp_violation": 0.015263352394289493,
            "ave_precision_score": 0.7581083444796254,
            "fpr": 0.16776315789473684,
            "logloss": 2.3690812732483995,
            "mae": 0.347526037424167,
            "precision": 0.6832298136645962,
            "recall": 0.7158351409978309
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7957405795671271,
            "auditor_fn_violation": 0.018803312232951775,
            "auditor_fp_violation": 0.018471735670880632,
            "ave_precision_score": 0.7971158943342433,
            "fpr": 0.15916575192096596,
            "logloss": 2.393280671110198,
            "mae": 0.34501302146241203,
            "precision": 0.7088353413654619,
            "recall": 0.716024340770791
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7331353256139388,
            "auditor_fn_violation": 0.028751379533432285,
            "auditor_fp_violation": 0.021701287587038553,
            "ave_precision_score": 0.7348944734225611,
            "fpr": 0.1425438596491228,
            "logloss": 2.537796272566629,
            "mae": 0.3513268569845977,
            "precision": 0.7085201793721974,
            "recall": 0.6854663774403471
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7776663965001414,
            "auditor_fn_violation": 0.020417569351825673,
            "auditor_fp_violation": 0.021906627660859566,
            "ave_precision_score": 0.7791378864870511,
            "fpr": 0.14270032930845225,
            "logloss": 2.4797106912317286,
            "mae": 0.34141531959095395,
            "precision": 0.7222222222222222,
            "recall": 0.6855983772819473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 26311,
        "test": {
            "accuracy": 0.631578947368421,
            "auc_prc": 0.7456999450326847,
            "auditor_fn_violation": 0.017983883243901506,
            "auditor_fp_violation": 0.01419360485470885,
            "ave_precision_score": 0.7474188824313948,
            "fpr": 0.2576754385964912,
            "logloss": 2.5843175687316178,
            "mae": 0.3876590684571327,
            "precision": 0.6050420168067226,
            "recall": 0.7809110629067245
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.7890994165611224,
            "auditor_fn_violation": 0.008369644841168231,
            "auditor_fp_violation": 0.02639194533584735,
            "ave_precision_score": 0.7904801952979409,
            "fpr": 0.23161361141602635,
            "logloss": 2.5249667550480237,
            "mae": 0.3686069014751067,
            "precision": 0.6540983606557377,
            "recall": 0.8093306288032455
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7430832807128316,
            "auditor_fn_violation": 0.023057236366404082,
            "auditor_fp_violation": 0.019094993581514775,
            "ave_precision_score": 0.7448419427369978,
            "fpr": 0.16228070175438597,
            "logloss": 2.5329589496653093,
            "mae": 0.33640986305728576,
            "precision": 0.6954732510288066,
            "recall": 0.7331887201735358
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.785990464751402,
            "auditor_fn_violation": 0.014227728261523012,
            "auditor_fp_violation": 0.017823097810387663,
            "ave_precision_score": 0.7873795073412507,
            "fpr": 0.15697036223929747,
            "logloss": 2.4834041424407824,
            "mae": 0.32772282797662206,
            "precision": 0.7168316831683168,
            "recall": 0.7342799188640974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.7539280027012786,
            "auditor_fn_violation": 0.01985338889523157,
            "auditor_fp_violation": 0.025277648111409347,
            "ave_precision_score": 0.7556173675502555,
            "fpr": 0.18311403508771928,
            "logloss": 2.4753804195268554,
            "mae": 0.34718476818451555,
            "precision": 0.6706114398422091,
            "recall": 0.737527114967462
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.8005875828510964,
            "auditor_fn_violation": 0.015552532379771241,
            "auditor_fp_violation": 0.02248961391603948,
            "ave_precision_score": 0.8019415223016297,
            "fpr": 0.17014270032930845,
            "logloss": 2.4424181417666224,
            "mae": 0.34024693492472013,
            "precision": 0.7019230769230769,
            "recall": 0.7403651115618661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6535087719298246,
            "auc_prc": 0.7046243197582212,
            "auditor_fn_violation": 0.010332229706587521,
            "auditor_fp_violation": 0.009170653907496013,
            "ave_precision_score": 0.7064299270794022,
            "fpr": 0.06907894736842106,
            "logloss": 3.1869325784444302,
            "mae": 0.3758568031917927,
            "precision": 0.7675276752767528,
            "recall": 0.4511930585683297
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.7476935948281183,
            "auditor_fn_violation": 0.01985647584292054,
            "auditor_fp_violation": 0.0046402554635266,
            "ave_precision_score": 0.7492486099209736,
            "fpr": 0.06256860592755215,
            "logloss": 3.313203446448549,
            "mae": 0.3794388482680284,
            "precision": 0.7992957746478874,
            "recall": 0.460446247464503
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7966976945903257,
            "auditor_fn_violation": 0.013450450964722001,
            "auditor_fp_violation": 0.023133290543431752,
            "ave_precision_score": 0.7949301719831083,
            "fpr": 0.14692982456140352,
            "logloss": 1.1308401675515292,
            "mae": 0.2813290791692086,
            "precision": 0.7237113402061855,
            "recall": 0.7613882863340564
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.8208575214344596,
            "auditor_fn_violation": 0.021203545576601514,
            "auditor_fp_violation": 0.01696437481289293,
            "ave_precision_score": 0.8191272763903307,
            "fpr": 0.13391877058177826,
            "logloss": 1.1257040826537326,
            "mae": 0.28650602783688844,
            "precision": 0.7484536082474227,
            "recall": 0.7363083164300203
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.759207506133706,
            "auditor_fn_violation": 0.01779598127640142,
            "auditor_fp_violation": 0.02578334694830202,
            "ave_precision_score": 0.7578891384065698,
            "fpr": 0.1425438596491228,
            "logloss": 2.5913942898127162,
            "mae": 0.3240297841888637,
            "precision": 0.7098214285714286,
            "recall": 0.6898047722342733
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7822494304341833,
            "auditor_fn_violation": 0.010772104746361245,
            "auditor_fp_violation": 0.01600323531110983,
            "ave_precision_score": 0.7784461353186872,
            "fpr": 0.12952799121844127,
            "logloss": 2.7517203329294735,
            "mae": 0.3241022290755472,
            "precision": 0.7434782608695653,
            "recall": 0.6937119675456389
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7436441239853753,
            "auditor_fn_violation": 0.021325684058301944,
            "auditor_fp_violation": 0.021745049986385035,
            "ave_precision_score": 0.7453551995867571,
            "fpr": 0.16447368421052633,
            "logloss": 2.541621362246718,
            "mae": 0.3406718926283231,
            "precision": 0.691358024691358,
            "recall": 0.7288503253796096
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7855783709859097,
            "auditor_fn_violation": 0.017307062875871422,
            "auditor_fp_violation": 0.019832036932967097,
            "ave_precision_score": 0.7869691450673585,
            "fpr": 0.15477497255762898,
            "logloss": 2.490413752413116,
            "mae": 0.33001359382045736,
            "precision": 0.718,
            "recall": 0.7281947261663286
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8158884062282433,
            "auditor_fn_violation": 0.015324713627887506,
            "auditor_fp_violation": 0.019394036643715723,
            "ave_precision_score": 0.8162056141215659,
            "fpr": 0.16776315789473684,
            "logloss": 0.7649697409664556,
            "mae": 0.2861468120253241,
            "precision": 0.71875,
            "recall": 0.8481561822125814
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8466215787679081,
            "auditor_fn_violation": 0.017104445775433456,
            "auditor_fp_violation": 0.019422370915813638,
            "ave_precision_score": 0.846852612252806,
            "fpr": 0.15477497255762898,
            "logloss": 0.6564351314546613,
            "mae": 0.2785515942772197,
            "precision": 0.7436363636363637,
            "recall": 0.8296146044624746
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8188056142959415,
            "auditor_fn_violation": 0.014730087148456828,
            "auditor_fp_violation": 0.019184949624615866,
            "ave_precision_score": 0.8160194627219403,
            "fpr": 0.14144736842105263,
            "logloss": 0.7684475612690413,
            "mae": 0.2748868807329927,
            "precision": 0.7393939393939394,
            "recall": 0.7939262472885033
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8468084005241996,
            "auditor_fn_violation": 0.02237917007145036,
            "auditor_fp_violation": 0.01800167017683916,
            "ave_precision_score": 0.8450755673604704,
            "fpr": 0.12623490669593854,
            "logloss": 0.7060981808776879,
            "mae": 0.27378074718485407,
            "precision": 0.7718253968253969,
            "recall": 0.7890466531440162
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 26311,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7322118946864479,
            "auditor_fn_violation": 0.024641321307607416,
            "auditor_fp_violation": 0.021837437273894273,
            "ave_precision_score": 0.7339778437840626,
            "fpr": 0.16337719298245615,
            "logloss": 2.5528834526369133,
            "mae": 0.35104256880924767,
            "precision": 0.6895833333333333,
            "recall": 0.7180043383947939
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7752576197074963,
            "auditor_fn_violation": 0.017781320484588862,
            "auditor_fp_violation": 0.022156103761049168,
            "ave_precision_score": 0.7767538725869607,
            "fpr": 0.1525795828759605,
            "logloss": 2.4926450271247433,
            "mae": 0.33881286458939985,
            "precision": 0.7191919191919192,
            "recall": 0.7221095334685599
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 26311,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8161471377909402,
            "auditor_fn_violation": 0.014658731970925145,
            "auditor_fp_violation": 0.01865007585482554,
            "ave_precision_score": 0.8143475172559707,
            "fpr": 0.15570175438596492,
            "logloss": 0.7846251206461319,
            "mae": 0.28086240730095724,
            "precision": 0.7253384912959381,
            "recall": 0.8134490238611713
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8463177022868206,
            "auditor_fn_violation": 0.021268115861356473,
            "auditor_fp_violation": 0.01631836301661249,
            "ave_precision_score": 0.8445932057997003,
            "fpr": 0.1394072447859495,
            "logloss": 0.710779965412709,
            "mae": 0.2758681400456444,
            "precision": 0.7599243856332704,
            "recall": 0.8154158215010142
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7388149194583027,
            "auditor_fn_violation": 0.019306332534155347,
            "auditor_fp_violation": 0.013478818998716306,
            "ave_precision_score": 0.7405763973443971,
            "fpr": 0.18092105263157895,
            "logloss": 2.5274309550077563,
            "mae": 0.34238345946226,
            "precision": 0.6789883268482491,
            "recall": 0.7570498915401301
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7762667145341334,
            "auditor_fn_violation": 0.01067413603845717,
            "auditor_fp_violation": 0.019480144328489123,
            "ave_precision_score": 0.7777041672142078,
            "fpr": 0.16575192096597147,
            "logloss": 2.480987832749655,
            "mae": 0.33584662604554405,
            "precision": 0.7067961165048544,
            "recall": 0.7383367139959433
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7798200000663418,
            "auditor_fn_violation": 0.01732979411652777,
            "auditor_fp_violation": 0.018372913992297825,
            "ave_precision_score": 0.7684868291006948,
            "fpr": 0.13267543859649122,
            "logloss": 2.789835529979903,
            "mae": 0.31172416934577196,
            "precision": 0.7323008849557522,
            "recall": 0.7180043383947939
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8106755799813701,
            "auditor_fn_violation": 0.009371597535641697,
            "auditor_fp_violation": 0.013718559446215585,
            "ave_precision_score": 0.801577976676289,
            "fpr": 0.12403951701427003,
            "logloss": 2.6391897861672753,
            "mae": 0.3075186798459592,
            "precision": 0.7590618336886994,
            "recall": 0.7221095334685599
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7548191875055072,
            "auditor_fn_violation": 0.017398770788141723,
            "auditor_fp_violation": 0.02210487415878944,
            "ave_precision_score": 0.7564886315382786,
            "fpr": 0.17763157894736842,
            "logloss": 2.4705930172636417,
            "mae": 0.3456273166846437,
            "precision": 0.6766467065868264,
            "recall": 0.735357917570499
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.8010421874060798,
            "auditor_fn_violation": 0.015267532502232131,
            "auditor_fp_violation": 0.022082573963098547,
            "ave_precision_score": 0.8023959773265517,
            "fpr": 0.17014270032930845,
            "logloss": 2.436346241486851,
            "mae": 0.33920574185444236,
            "precision": 0.7019230769230769,
            "recall": 0.7403651115618661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7572840616263254,
            "auditor_fn_violation": 0.018431042356433388,
            "auditor_fp_violation": 0.019508305130898202,
            "ave_precision_score": 0.7590270243041388,
            "fpr": 0.1425438596491228,
            "logloss": 2.4562932588732704,
            "mae": 0.3310648356450681,
            "precision": 0.7228144989339019,
            "recall": 0.735357917570499
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7994578275419554,
            "auditor_fn_violation": 0.015605969856809829,
            "auditor_fp_violation": 0.017014270032930847,
            "ave_precision_score": 0.8008714335673118,
            "fpr": 0.132821075740944,
            "logloss": 2.410670831400295,
            "mae": 0.3237672119070295,
            "precision": 0.7430997876857749,
            "recall": 0.7099391480730223
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7479574131741786,
            "auditor_fn_violation": 0.017938691631464783,
            "auditor_fp_violation": 0.0198097794375073,
            "ave_precision_score": 0.7496751904921257,
            "fpr": 0.17214912280701755,
            "logloss": 2.504101877441571,
            "mae": 0.338918856647931,
            "precision": 0.686,
            "recall": 0.7440347071583514
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7863366747187588,
            "auditor_fn_violation": 0.016737063120793195,
            "auditor_fp_violation": 0.01461142127847311,
            "ave_precision_score": 0.7877305883924722,
            "fpr": 0.16355653128430298,
            "logloss": 2.4631959491495223,
            "mae": 0.3325748178575282,
            "precision": 0.7117988394584139,
            "recall": 0.7464503042596349
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.7374035880681423,
            "auditor_fn_violation": 0.017979126232066076,
            "auditor_fp_violation": 0.017840471466915635,
            "ave_precision_score": 0.7353830130943086,
            "fpr": 0.1524122807017544,
            "logloss": 3.6945695837442822,
            "mae": 0.35683968722112597,
            "precision": 0.6848072562358276,
            "recall": 0.6550976138828634
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.8002131808534384,
            "auditor_fn_violation": 0.016999797382899574,
            "auditor_fp_violation": 0.015493778853880539,
            "ave_precision_score": 0.8013732580637218,
            "fpr": 0.14270032930845225,
            "logloss": 3.037278625848689,
            "mae": 0.3402916112782804,
            "precision": 0.7222222222222222,
            "recall": 0.6855983772819473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7362576606060058,
            "auditor_fn_violation": 0.02688663089393767,
            "auditor_fp_violation": 0.015632901544326453,
            "ave_precision_score": 0.7380237198861419,
            "fpr": 0.15350877192982457,
            "logloss": 2.5413593823939844,
            "mae": 0.345217774864168,
            "precision": 0.7033898305084746,
            "recall": 0.720173535791757
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.781275183746202,
            "auditor_fn_violation": 0.017425070637664967,
            "auditor_fp_violation": 0.021323641405679654,
            "ave_precision_score": 0.7827339814886443,
            "fpr": 0.15148188803512624,
            "logloss": 2.4831615612324627,
            "mae": 0.3352175244292661,
            "precision": 0.7217741935483871,
            "recall": 0.7261663286004056
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7678951000073061,
            "auditor_fn_violation": 0.020288655478174836,
            "auditor_fp_violation": 0.021514081767611937,
            "ave_precision_score": 0.7565720567744421,
            "fpr": 0.16337719298245615,
            "logloss": 2.8562965868768035,
            "mae": 0.33264557578417897,
            "precision": 0.6940451745379876,
            "recall": 0.7331887201735358
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7987170121582755,
            "auditor_fn_violation": 0.015959993142190453,
            "auditor_fp_violation": 0.02204843512833576,
            "ave_precision_score": 0.7896305461064103,
            "fpr": 0.15148188803512624,
            "logloss": 2.6996552582751034,
            "mae": 0.32246175519830056,
            "precision": 0.7267326732673267,
            "recall": 0.744421906693712
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7423236057384737,
            "auditor_fn_violation": 0.021268599916276595,
            "auditor_fp_violation": 0.02009423503325943,
            "ave_precision_score": 0.7440600123738056,
            "fpr": 0.16337719298245615,
            "logloss": 2.537594950965296,
            "mae": 0.3380101021229756,
            "precision": 0.694672131147541,
            "recall": 0.735357917570499
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7837207695602597,
            "auditor_fn_violation": 0.014227728261523012,
            "auditor_fp_violation": 0.016549456667314433,
            "ave_precision_score": 0.7851189523361387,
            "fpr": 0.1602634467618002,
            "logloss": 2.488837368334223,
            "mae": 0.32881944613447467,
            "precision": 0.7125984251968503,
            "recall": 0.7342799188640974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.759984744352112,
            "auditor_fn_violation": 0.015222437873425438,
            "auditor_fp_violation": 0.020111253744116393,
            "ave_precision_score": 0.761668430870361,
            "fpr": 0.13267543859649122,
            "logloss": 2.4274241054258274,
            "mae": 0.3278314149028063,
            "precision": 0.7323008849557522,
            "recall": 0.7180043383947939
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.797723220728684,
            "auditor_fn_violation": 0.010170933129677175,
            "auditor_fp_violation": 0.010604047290164342,
            "ave_precision_score": 0.7991296412291652,
            "fpr": 0.12403951701427003,
            "logloss": 2.399892947515997,
            "mae": 0.32894564748746286,
            "precision": 0.7532751091703057,
            "recall": 0.6997971602434077
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7714726717918186,
            "auditor_fn_violation": 0.009349906762568033,
            "auditor_fp_violation": 0.022068405492667365,
            "ave_precision_score": 0.7734049390968432,
            "fpr": 0.12390350877192982,
            "logloss": 2.3479892676085803,
            "mae": 0.32741557909252766,
            "precision": 0.7372093023255814,
            "recall": 0.6876355748373102
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.8115269676145361,
            "auditor_fn_violation": 0.00474480264871761,
            "auditor_fp_violation": 0.02001848749205616,
            "ave_precision_score": 0.8128572637717064,
            "fpr": 0.11964873765093303,
            "logloss": 2.366276040125586,
            "mae": 0.33506373700580205,
            "precision": 0.7435294117647059,
            "recall": 0.640973630831643
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7450177138935739,
            "auditor_fn_violation": 0.023185675685961114,
            "auditor_fp_violation": 0.018667094565682506,
            "ave_precision_score": 0.7467348902966839,
            "fpr": 0.16228070175438597,
            "logloss": 2.482234098764023,
            "mae": 0.33440628566824127,
            "precision": 0.6942148760330579,
            "recall": 0.7288503253796096
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7856147800587714,
            "auditor_fn_violation": 0.018275617147195763,
            "auditor_fp_violation": 0.01668075987794054,
            "ave_precision_score": 0.7870086299451948,
            "fpr": 0.1525795828759605,
            "logloss": 2.4258977941478537,
            "mae": 0.3273172039128107,
            "precision": 0.7208835341365462,
            "recall": 0.7281947261663286
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.742072583002639,
            "auditor_fn_violation": 0.021570670167827388,
            "auditor_fp_violation": 0.020400571828684797,
            "ave_precision_score": 0.7438328828329193,
            "fpr": 0.1962719298245614,
            "logloss": 2.561989501252734,
            "mae": 0.34614922741130644,
            "precision": 0.6629001883239172,
            "recall": 0.7635574837310195
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7846438803626946,
            "auditor_fn_violation": 0.007231871892555047,
            "auditor_fp_violation": 0.01642865771353841,
            "ave_precision_score": 0.7860833586390753,
            "fpr": 0.18551042810098792,
            "logloss": 2.502406947805535,
            "mae": 0.3356456514330681,
            "precision": 0.6904761904761905,
            "recall": 0.7647058823529411
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 26311,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.81610737183638,
            "auditor_fn_violation": 0.014658731970925145,
            "auditor_fp_violation": 0.01865007585482554,
            "ave_precision_score": 0.8143194927569561,
            "fpr": 0.15570175438596492,
            "logloss": 0.7848275149924933,
            "mae": 0.2808629295705351,
            "precision": 0.7253384912959381,
            "recall": 0.8134490238611713
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8463084870184461,
            "auditor_fn_violation": 0.021268115861356473,
            "auditor_fp_violation": 0.01631836301661249,
            "ave_precision_score": 0.844585157886638,
            "fpr": 0.1394072447859495,
            "logloss": 0.7109264827162219,
            "mae": 0.27586119351514976,
            "precision": 0.7599243856332704,
            "recall": 0.8154158215010142
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7413613828700021,
            "auditor_fn_violation": 0.021237679339346196,
            "auditor_fp_violation": 0.017524409693857705,
            "ave_precision_score": 0.7431086964902249,
            "fpr": 0.1699561403508772,
            "logloss": 2.527471621354302,
            "mae": 0.33664010662285265,
            "precision": 0.6868686868686869,
            "recall": 0.737527114967462
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7829468849163594,
            "auditor_fn_violation": 0.018329054624234342,
            "auditor_fp_violation": 0.018227511699116065,
            "ave_precision_score": 0.7843422445361329,
            "fpr": 0.15697036223929747,
            "logloss": 2.482110986051569,
            "mae": 0.3290181786879716,
            "precision": 0.718503937007874,
            "recall": 0.7403651115618661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8127400924971483,
            "auditor_fn_violation": 0.013312497621494083,
            "auditor_fp_violation": 0.018681682032131337,
            "ave_precision_score": 0.8130621894165895,
            "fpr": 0.16557017543859648,
            "logloss": 0.7682540554982893,
            "mae": 0.2846555747645764,
            "precision": 0.7193308550185874,
            "recall": 0.8394793926247288
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8420034062919858,
            "auditor_fn_violation": 0.013624330083295671,
            "auditor_fp_violation": 0.015218042111565716,
            "ave_precision_score": 0.842243766462407,
            "fpr": 0.1525795828759605,
            "logloss": 0.6650761075125529,
            "mae": 0.2793354146412492,
            "precision": 0.744954128440367,
            "recall": 0.8235294117647058
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7524451695608871,
            "auditor_fn_violation": 0.017703219545610226,
            "auditor_fp_violation": 0.01718646672112655,
            "ave_precision_score": 0.754166592172551,
            "fpr": 0.17653508771929824,
            "logloss": 2.4972103343906684,
            "mae": 0.3420153085862838,
            "precision": 0.6861598440545809,
            "recall": 0.7635574837310195
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7917237577925613,
            "auditor_fn_violation": 0.017164562937101863,
            "auditor_fp_violation": 0.017851984516725414,
            "ave_precision_score": 0.7931468415460876,
            "fpr": 0.1690450054884742,
            "logloss": 2.4507401414785317,
            "mae": 0.33506455259542284,
            "precision": 0.7094339622641509,
            "recall": 0.7626774847870182
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.783800959098752,
            "auditor_fn_violation": 0.010339365224340688,
            "auditor_fp_violation": 0.023940463686933523,
            "ave_precision_score": 0.7856296369878218,
            "fpr": 0.14912280701754385,
            "logloss": 2.2691388438971525,
            "mae": 0.3212714365718216,
            "precision": 0.7112526539278131,
            "recall": 0.7266811279826464
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.8213060653868647,
            "auditor_fn_violation": 0.013428392667487526,
            "auditor_fp_violation": 0.02770497744210842,
            "ave_precision_score": 0.822484981552625,
            "fpr": 0.14270032930845225,
            "logloss": 2.1926985809176287,
            "mae": 0.3214429783316028,
            "precision": 0.7357723577235772,
            "recall": 0.7342799188640974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7528906142179463,
            "auditor_fn_violation": 0.01738449975263539,
            "auditor_fp_violation": 0.01701871085696503,
            "ave_precision_score": 0.7546166175702029,
            "fpr": 0.11513157894736842,
            "logloss": 2.446761529965123,
            "mae": 0.3285565081771957,
            "precision": 0.7488038277511961,
            "recall": 0.6789587852494577
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7949455248501865,
            "auditor_fn_violation": 0.016670266274494968,
            "auditor_fp_violation": 0.012489561394755228,
            "ave_precision_score": 0.7963863905477884,
            "fpr": 0.08781558726673985,
            "logloss": 2.4152188380025614,
            "mae": 0.3269860222207602,
            "precision": 0.8,
            "recall": 0.6490872210953347
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7482613556952824,
            "auditor_fn_violation": 0.021711002016973024,
            "auditor_fp_violation": 0.017957171198506258,
            "ave_precision_score": 0.7499591250784088,
            "fpr": 0.16557017543859648,
            "logloss": 2.5111236432858606,
            "mae": 0.3360540574756653,
            "precision": 0.693089430894309,
            "recall": 0.7396963123644251
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.789257895286016,
            "auditor_fn_violation": 0.013956087753243553,
            "auditor_fp_violation": 0.019188651200899166,
            "ave_precision_score": 0.7906302305623736,
            "fpr": 0.15916575192096596,
            "logloss": 2.4653117118271544,
            "mae": 0.3295252819533555,
            "precision": 0.7145669291338582,
            "recall": 0.7363083164300203
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8127715812191334,
            "auditor_fn_violation": 0.013488507059405566,
            "auditor_fp_violation": 0.02112751390671802,
            "ave_precision_score": 0.8131203435029203,
            "fpr": 0.15899122807017543,
            "logloss": 0.7716674358717085,
            "mae": 0.2819992415578574,
            "precision": 0.7274436090225563,
            "recall": 0.8394793926247288
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8478355489837964,
            "auditor_fn_violation": 0.019829757104401245,
            "auditor_fp_violation": 0.017500091912247447,
            "ave_precision_score": 0.8480639224491533,
            "fpr": 0.15367727771679474,
            "logloss": 0.6580537511447682,
            "mae": 0.27370708437294533,
            "precision": 0.7440585009140768,
            "recall": 0.8255578093306288
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.7450589127880415,
            "auditor_fn_violation": 0.02396582562697417,
            "auditor_fp_violation": 0.023928307464892835,
            "ave_precision_score": 0.7467665739177853,
            "fpr": 0.16447368421052633,
            "logloss": 2.465216817582918,
            "mae": 0.33950629595944587,
            "precision": 0.6861924686192469,
            "recall": 0.7114967462039046
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7886457957788436,
            "auditor_fn_violation": 0.018573976393994527,
            "auditor_fp_violation": 0.017521100425947622,
            "ave_precision_score": 0.7900254933048789,
            "fpr": 0.14928649835345773,
            "logloss": 2.436992180696705,
            "mae": 0.32987522004507797,
            "precision": 0.7235772357723578,
            "recall": 0.7221095334685599
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7454352153350832,
            "auditor_fn_violation": 0.020088860981086122,
            "auditor_fp_violation": 0.018929668961761383,
            "ave_precision_score": 0.747150428740928,
            "fpr": 0.16337719298245615,
            "logloss": 2.523827848535876,
            "mae": 0.33574331087285764,
            "precision": 0.6959183673469388,
            "recall": 0.7396963123644251
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7898670878778357,
            "auditor_fn_violation": 0.013985033053306117,
            "auditor_fp_violation": 0.01942762304423868,
            "ave_precision_score": 0.7912157408025523,
            "fpr": 0.15806805708013172,
            "logloss": 2.4675142752924386,
            "mae": 0.3282040783597599,
            "precision": 0.7148514851485148,
            "recall": 0.7322515212981744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7632243173815559,
            "auditor_fn_violation": 0.011947235224721237,
            "auditor_fp_violation": 0.022805072548333142,
            "ave_precision_score": 0.764893555088004,
            "fpr": 0.1524122807017544,
            "logloss": 2.4437303860743063,
            "mae": 0.3237663487568777,
            "precision": 0.7151639344262295,
            "recall": 0.7570498915401301
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7990418294916628,
            "auditor_fn_violation": 0.016926320851971505,
            "auditor_fp_violation": 0.0161660512922862,
            "ave_precision_score": 0.80039728614828,
            "fpr": 0.145993413830955,
            "logloss": 2.429478765003564,
            "mae": 0.3233477501247209,
            "precision": 0.7350597609561753,
            "recall": 0.7484787018255578
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7488407960489842,
            "auditor_fn_violation": 0.015624405373520577,
            "auditor_fp_violation": 0.021552981678142142,
            "ave_precision_score": 0.7507309419807255,
            "fpr": 0.14802631578947367,
            "logloss": 2.481318282353044,
            "mae": 0.3261379005935899,
            "precision": 0.7127659574468085,
            "recall": 0.7266811279826464
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.792653868548609,
            "auditor_fn_violation": 0.011255268601251776,
            "auditor_fp_violation": 0.015845671458358503,
            "ave_precision_score": 0.7940162125894874,
            "fpr": 0.13721185510428102,
            "logloss": 2.4423739428577185,
            "mae": 0.32121907275629613,
            "precision": 0.7469635627530364,
            "recall": 0.7484787018255578
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8099628887064239,
            "auditor_fn_violation": 0.017731761616622904,
            "auditor_fp_violation": 0.016960360991169723,
            "ave_precision_score": 0.8081797686781428,
            "fpr": 0.14583333333333334,
            "logloss": 0.7824386959111513,
            "mae": 0.28426703356796534,
            "precision": 0.7307692307692307,
            "recall": 0.7830802603036876
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8414864144650366,
            "auditor_fn_violation": 0.020767139514119747,
            "auditor_fp_violation": 0.014942305369250897,
            "ave_precision_score": 0.8397652608270303,
            "fpr": 0.1350164654226125,
            "logloss": 0.7134800855224017,
            "mae": 0.27784037306285275,
            "precision": 0.7611650485436893,
            "recall": 0.795131845841785
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7456724468156748,
            "auditor_fn_violation": 0.020802412756402943,
            "auditor_fp_violation": 0.02150192554557125,
            "ave_precision_score": 0.7473776146116275,
            "fpr": 0.15679824561403508,
            "logloss": 2.521598117442954,
            "mae": 0.3359518455561141,
            "precision": 0.6995798319327731,
            "recall": 0.7223427331887202
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7876185309013477,
            "auditor_fn_violation": 0.018244445285589918,
            "auditor_fp_violation": 0.01985304544666727,
            "ave_precision_score": 0.7890082853879328,
            "fpr": 0.15148188803512624,
            "logloss": 2.4791392780787116,
            "mae": 0.3264665981065657,
            "precision": 0.7212121212121212,
            "recall": 0.7241379310344828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8339859186345109,
            "auditor_fn_violation": 0.011331202192031058,
            "auditor_fp_violation": 0.020103960010891982,
            "ave_precision_score": 0.8342213582028672,
            "fpr": 0.19188596491228072,
            "logloss": 0.7227606966216917,
            "mae": 0.2932417318929526,
            "precision": 0.6972318339100346,
            "recall": 0.8741865509761388
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8585937927486946,
            "auditor_fn_violation": 0.015706165126257176,
            "auditor_fp_violation": 0.02438038014905541,
            "ave_precision_score": 0.8587971420287641,
            "fpr": 0.1778265642151482,
            "logloss": 0.6285491616266561,
            "mae": 0.28796887739728133,
            "precision": 0.7235494880546075,
            "recall": 0.8600405679513184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 26311,
        "test": {
            "accuracy": 0.518640350877193,
            "auc_prc": 0.6174317312693811,
            "auditor_fn_violation": 0.006685980134718584,
            "auditor_fp_violation": 0.0030268992881316375,
            "ave_precision_score": 0.622198966821744,
            "fpr": 0.008771929824561403,
            "logloss": 6.531604888849577,
            "mae": 0.4692660700265273,
            "precision": 0.7894736842105263,
            "recall": 0.0650759219088937
        },
        "train": {
            "accuracy": 0.4983534577387486,
            "auc_prc": 0.6747111208135324,
            "auditor_fn_violation": 0.010743159446298684,
            "auditor_fp_violation": 0.0031906680182143816,
            "ave_precision_score": 0.6786697836031574,
            "fpr": 0.005488474204171241,
            "logloss": 6.501753337569375,
            "mae": 0.49057184312634755,
            "precision": 0.8913043478260869,
            "recall": 0.08316430020283976
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7400132830441801,
            "auditor_fn_violation": 0.02041471629181414,
            "auditor_fp_violation": 0.019911891702649086,
            "ave_precision_score": 0.741772282298357,
            "fpr": 0.17763157894736842,
            "logloss": 2.5498383749508835,
            "mae": 0.3426482848643214,
            "precision": 0.6817288801571709,
            "recall": 0.7527114967462039
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.779521775167346,
            "auditor_fn_violation": 0.02263967777201346,
            "auditor_fp_violation": 0.021357780240442444,
            "ave_precision_score": 0.7809765325136685,
            "fpr": 0.16136114160263446,
            "logloss": 2.503198461308594,
            "mae": 0.3343501263762286,
            "precision": 0.7183908045977011,
            "recall": 0.7606490872210954
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.7968728443879662,
            "auditor_fn_violation": 0.019860524412984738,
            "auditor_fp_violation": 0.021993036916015098,
            "ave_precision_score": 0.7973221320525381,
            "fpr": 0.14583333333333334,
            "logloss": 0.6713051536024924,
            "mae": 0.28161118623356424,
            "precision": 0.73558648111332,
            "recall": 0.8026030368763557
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8349096423664583,
            "auditor_fn_violation": 0.020956397245298068,
            "auditor_fp_violation": 0.01906522618291063,
            "ave_precision_score": 0.835232175908233,
            "fpr": 0.132821075740944,
            "logloss": 0.6180241168959587,
            "mae": 0.27598503079125686,
            "precision": 0.7659574468085106,
            "recall": 0.8032454361054767
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7364181443441357,
            "auditor_fn_violation": 0.02149455797846025,
            "auditor_fp_violation": 0.016367137355584087,
            "ave_precision_score": 0.7382042347458089,
            "fpr": 0.15679824561403508,
            "logloss": 2.561806304171453,
            "mae": 0.3401757822914812,
            "precision": 0.7033195020746889,
            "recall": 0.735357917570499
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.777816696480389,
            "auditor_fn_violation": 0.016276164881335414,
            "auditor_fp_violation": 0.01952216135588948,
            "ave_precision_score": 0.7792717033668246,
            "fpr": 0.1437980241492865,
            "logloss": 2.5072997236288512,
            "mae": 0.3310139671565435,
            "precision": 0.7321063394683026,
            "recall": 0.7261663286004056
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.752030620973196,
            "auditor_fn_violation": 0.015222437873425438,
            "auditor_fp_violation": 0.018560119811724434,
            "ave_precision_score": 0.7538205690500872,
            "fpr": 0.14144736842105263,
            "logloss": 2.457712447639498,
            "mae": 0.3194009436384664,
            "precision": 0.7195652173913043,
            "recall": 0.7180043383947939
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.7952066130056692,
            "auditor_fn_violation": 0.015650501087675316,
            "auditor_fp_violation": 0.01485827131445018,
            "ave_precision_score": 0.7965844737055829,
            "fpr": 0.13062568605927552,
            "logloss": 2.4345032715342776,
            "mae": 0.3163842337293733,
            "precision": 0.7494736842105263,
            "recall": 0.7221095334685599
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7489438434321891,
            "auditor_fn_violation": 0.022024964798112424,
            "auditor_fp_violation": 0.017402847473450812,
            "ave_precision_score": 0.7507533222455405,
            "fpr": 0.12719298245614036,
            "logloss": 2.4568935904288405,
            "mae": 0.3246127845727642,
            "precision": 0.7321016166281755,
            "recall": 0.6876355748373102
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.79479228726607,
            "auditor_fn_violation": 0.009901519182940981,
            "auditor_fp_violation": 0.010622429739652004,
            "ave_precision_score": 0.7961592375508753,
            "fpr": 0.11964873765093303,
            "logloss": 2.428151429270648,
            "mae": 0.3219808776603413,
            "precision": 0.7566964285714286,
            "recall": 0.6876267748478702
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7591279236461324,
            "auditor_fn_violation": 0.021544506602732435,
            "auditor_fp_violation": 0.014726047380091029,
            "ave_precision_score": 0.760769054950311,
            "fpr": 0.17324561403508773,
            "logloss": 2.488757361990922,
            "mae": 0.3450248431567838,
            "precision": 0.6908023483365949,
            "recall": 0.7657266811279827
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7947168730274552,
            "auditor_fn_violation": 0.020310694397748505,
            "auditor_fp_violation": 0.019582560832777488,
            "ave_precision_score": 0.7961328249363313,
            "fpr": 0.16136114160263446,
            "logloss": 2.4432785244169333,
            "mae": 0.3382200364411413,
            "precision": 0.7173076923076923,
            "recall": 0.7565922920892495
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7394347847208719,
            "auditor_fn_violation": 0.02212961905849222,
            "auditor_fp_violation": 0.020638833780682313,
            "ave_precision_score": 0.7411928579199218,
            "fpr": 0.16447368421052633,
            "logloss": 2.551630552319153,
            "mae": 0.33841383465299485,
            "precision": 0.6938775510204082,
            "recall": 0.737527114967462
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7818617168922433,
            "auditor_fn_violation": 0.018086359416017446,
            "auditor_fp_violation": 0.016097773622760627,
            "ave_precision_score": 0.7832662401260029,
            "fpr": 0.15916575192096596,
            "logloss": 2.4984501338802887,
            "mae": 0.32940670947443196,
            "precision": 0.7156862745098039,
            "recall": 0.7403651115618661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.7424748007297236,
            "auditor_fn_violation": 0.018485747992541005,
            "auditor_fp_violation": 0.023240265297389824,
            "ave_precision_score": 0.7441845065787498,
            "fpr": 0.17434210526315788,
            "logloss": 2.4677523363496197,
            "mae": 0.3397680166300441,
            "precision": 0.6794354838709677,
            "recall": 0.7310195227765727
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7858672210372137,
            "auditor_fn_violation": 0.018921319994745313,
            "auditor_fp_violation": 0.02157311750586926,
            "ave_precision_score": 0.7872443648362873,
            "fpr": 0.16136114160263446,
            "logloss": 2.4223338211685648,
            "mae": 0.3305863294617668,
            "precision": 0.7106299212598425,
            "recall": 0.7322515212981744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.7527644393769641,
            "auditor_fn_violation": 0.022933554058682498,
            "auditor_fp_violation": 0.019304080600614622,
            "ave_precision_score": 0.7544176938287046,
            "fpr": 0.1962719298245614,
            "logloss": 2.5598688475698284,
            "mae": 0.35275219900556576,
            "precision": 0.6616257088846881,
            "recall": 0.7592190889370932
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7929234337453729,
            "auditor_fn_violation": 0.016135891504109122,
            "auditor_fp_violation": 0.02183834999133399,
            "ave_precision_score": 0.7943399481176497,
            "fpr": 0.18111964873765093,
            "logloss": 2.4566619025030065,
            "mae": 0.34136003343634813,
            "precision": 0.6921641791044776,
            "recall": 0.7525354969574036
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7570062419678492,
            "auditor_fn_violation": 0.01767229896867984,
            "auditor_fp_violation": 0.015564826700898593,
            "ave_precision_score": 0.7586576418358376,
            "fpr": 0.1524122807017544,
            "logloss": 2.482546778167015,
            "mae": 0.3283987045336976,
            "precision": 0.7104166666666667,
            "recall": 0.7396963123644251
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7904102971247344,
            "auditor_fn_violation": 0.018972530910240624,
            "auditor_fp_violation": 0.018361440973954694,
            "ave_precision_score": 0.7917878245194649,
            "fpr": 0.145993413830955,
            "logloss": 2.451841890466414,
            "mae": 0.327286686434226,
            "precision": 0.7323943661971831,
            "recall": 0.7383367139959433
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.8311481222116874,
            "auditor_fn_violation": 0.013448072458804281,
            "auditor_fp_violation": 0.021691562609406,
            "ave_precision_score": 0.8293358251660556,
            "fpr": 0.1337719298245614,
            "logloss": 0.690357190780119,
            "mae": 0.2681118727439816,
            "precision": 0.7569721115537849,
            "recall": 0.824295010845987
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8580274672822499,
            "auditor_fn_violation": 0.01744733625309771,
            "auditor_fp_violation": 0.016108277879610715,
            "ave_precision_score": 0.8562923289809848,
            "fpr": 0.12184412733260154,
            "logloss": 0.6401607070254699,
            "mae": 0.267402737430945,
            "precision": 0.7827788649706457,
            "recall": 0.8113590263691683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7413030765701297,
            "auditor_fn_violation": 0.0148846900331088,
            "auditor_fp_violation": 0.020738514801415964,
            "ave_precision_score": 0.7429519468613669,
            "fpr": 0.14912280701754385,
            "logloss": 2.473985514071534,
            "mae": 0.33002860943241374,
            "precision": 0.708779443254818,
            "recall": 0.7180043383947939
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7847917901444121,
            "auditor_fn_violation": 0.010634057930678238,
            "auditor_fp_violation": 0.014569404251072752,
            "ave_precision_score": 0.7862142443142082,
            "fpr": 0.13830954994511527,
            "logloss": 2.4510387015669868,
            "mae": 0.3268725912928281,
            "precision": 0.7418032786885246,
            "recall": 0.7342799188640974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7422126926701845,
            "auditor_fn_violation": 0.020662080907257296,
            "auditor_fp_violation": 0.019882716769751434,
            "ave_precision_score": 0.7439565203149061,
            "fpr": 0.1600877192982456,
            "logloss": 2.5135002765357366,
            "mae": 0.3345929961127088,
            "precision": 0.6995884773662552,
            "recall": 0.737527114967462
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7861574901412263,
            "auditor_fn_violation": 0.013956087753243553,
            "auditor_fp_violation": 0.01867131655103231,
            "ave_precision_score": 0.7875498489893289,
            "fpr": 0.15806805708013172,
            "logloss": 2.476718640427533,
            "mae": 0.32629863916552865,
            "precision": 0.7159763313609467,
            "recall": 0.7363083164300203
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8324804063528901,
            "auditor_fn_violation": 0.018362065684819427,
            "auditor_fp_violation": 0.02575417201540437,
            "ave_precision_score": 0.8327277411683529,
            "fpr": 0.16885964912280702,
            "logloss": 0.6074978028420343,
            "mae": 0.29396074302872366,
            "precision": 0.7083333333333334,
            "recall": 0.8112798264642083
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8585220219938651,
            "auditor_fn_violation": 0.01681721933635107,
            "auditor_fp_violation": 0.018545265468831246,
            "ave_precision_score": 0.8587708443824469,
            "fpr": 0.16136114160263446,
            "logloss": 0.5712910173511557,
            "mae": 0.28903554971089623,
            "precision": 0.7346570397111913,
            "recall": 0.8255578093306288
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7485572081277438,
            "auditor_fn_violation": 0.019018533318110892,
            "auditor_fp_violation": 0.024759793052475988,
            "ave_precision_score": 0.7502942455364094,
            "fpr": 0.18859649122807018,
            "logloss": 2.514296018673245,
            "mae": 0.34756569247277486,
            "precision": 0.6736242884250474,
            "recall": 0.7700650759219089
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7892625428662401,
            "auditor_fn_violation": 0.01685284432104346,
            "auditor_fp_violation": 0.01855051759725629,
            "ave_precision_score": 0.7906314102962825,
            "fpr": 0.1800219538968167,
            "logloss": 2.4664868830014592,
            "mae": 0.337391476971496,
            "precision": 0.6968576709796673,
            "recall": 0.7647058823529411
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.7359437823302251,
            "auditor_fn_violation": 0.01948472047798456,
            "auditor_fp_violation": 0.024035282218850908,
            "ave_precision_score": 0.7376878011166769,
            "fpr": 0.18201754385964913,
            "logloss": 2.5710145947106264,
            "mae": 0.34999548649022405,
            "precision": 0.6706349206349206,
            "recall": 0.7331887201735358
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7769188865763993,
            "auditor_fn_violation": 0.017556437768718153,
            "auditor_fp_violation": 0.018965435742834783,
            "ave_precision_score": 0.7783903170404047,
            "fpr": 0.16794731064763996,
            "logloss": 2.514954860687421,
            "mae": 0.3377873888299964,
            "precision": 0.7063339731285988,
            "recall": 0.7464503042596349
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7452483544555988,
            "auditor_fn_violation": 0.020802412756402943,
            "auditor_fp_violation": 0.02150192554557125,
            "ave_precision_score": 0.7469500449238426,
            "fpr": 0.15679824561403508,
            "logloss": 2.5286218658658877,
            "mae": 0.3373867273379833,
            "precision": 0.6995798319327731,
            "recall": 0.7223427331887202
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.787140446364597,
            "auditor_fn_violation": 0.01688178962110603,
            "auditor_fp_violation": 0.018500622377218372,
            "ave_precision_score": 0.7885275986945608,
            "fpr": 0.1525795828759605,
            "logloss": 2.479647560435931,
            "mae": 0.3278690680475424,
            "precision": 0.7186234817813765,
            "recall": 0.7200811359026369
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 26311,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7366167561827597,
            "auditor_fn_violation": 0.02149455797846025,
            "auditor_fp_violation": 0.01968335472828413,
            "ave_precision_score": 0.7383714864793058,
            "fpr": 0.16885964912280702,
            "logloss": 2.5516643019810075,
            "mae": 0.3428811493038729,
            "precision": 0.6876267748478702,
            "recall": 0.735357917570499
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7791441896860907,
            "auditor_fn_violation": 0.013679994121877526,
            "auditor_fp_violation": 0.018209129249628413,
            "ave_precision_score": 0.7805705026618113,
            "fpr": 0.16355653128430298,
            "logloss": 2.5048801351326917,
            "mae": 0.3315062498835992,
            "precision": 0.7095516569200779,
            "recall": 0.7383367139959433
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7424374970452404,
            "auditor_fn_violation": 0.021033127830422046,
            "auditor_fp_violation": 0.01927490566771697,
            "ave_precision_score": 0.7441766589058969,
            "fpr": 0.1600877192982456,
            "logloss": 2.5214658129824348,
            "mae": 0.3298152672442851,
            "precision": 0.6995884773662552,
            "recall": 0.737527114967462
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7834562312468245,
            "auditor_fn_violation": 0.01229952596504744,
            "auditor_fp_violation": 0.014918670791338197,
            "ave_precision_score": 0.7848591738285965,
            "fpr": 0.1437980241492865,
            "logloss": 2.484361790917209,
            "mae": 0.32567401348763225,
            "precision": 0.733739837398374,
            "recall": 0.7322515212981744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.745264590700453,
            "auditor_fn_violation": 0.020802412756402943,
            "auditor_fp_violation": 0.02150192554557125,
            "ave_precision_score": 0.7469744556229215,
            "fpr": 0.15679824561403508,
            "logloss": 2.527886122705345,
            "mae": 0.337390607417387,
            "precision": 0.6995798319327731,
            "recall": 0.7223427331887202
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7872012964766879,
            "auditor_fn_violation": 0.01688178962110603,
            "auditor_fp_violation": 0.018500622377218372,
            "ave_precision_score": 0.7885913641211902,
            "fpr": 0.1525795828759605,
            "logloss": 2.4793490635214077,
            "mae": 0.3278240527450421,
            "precision": 0.7186234817813765,
            "recall": 0.7200811359026369
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 26311,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7465084073883848,
            "auditor_fn_violation": 0.02270046047874568,
            "auditor_fp_violation": 0.017259404053370682,
            "ave_precision_score": 0.7482428467600462,
            "fpr": 0.14364035087719298,
            "logloss": 2.510345280316227,
            "mae": 0.3428547125572986,
            "precision": 0.7127192982456141,
            "recall": 0.7049891540130152
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7927802971144551,
            "auditor_fn_violation": 0.018489367055350103,
            "auditor_fp_violation": 0.01745807488484709,
            "ave_precision_score": 0.7941921975897128,
            "fpr": 0.1437980241492865,
            "logloss": 2.4459695070369674,
            "mae": 0.3341807834976413,
            "precision": 0.7247899159663865,
            "recall": 0.6997971602434077
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.6727328181857264,
            "auditor_fn_violation": 0.009968914138707262,
            "auditor_fp_violation": 0.015111263907988497,
            "ave_precision_score": 0.664310847632005,
            "fpr": 0.2631578947368421,
            "logloss": 2.2381219437740745,
            "mae": 0.3424640378032468,
            "precision": 0.6506550218340611,
            "recall": 0.9103869653767821
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.6520039532251432,
            "auditor_fn_violation": 0.00559990327008746,
            "auditor_fp_violation": 0.02307119335110554,
            "ave_precision_score": 0.645641568646121,
            "fpr": 0.270032930845225,
            "logloss": 2.431986121970946,
            "mae": 0.3583215184210218,
            "precision": 0.6278366111951589,
            "recall": 0.896328293736501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.6618063833726202,
            "auditor_fn_violation": 0.010424482795583665,
            "auditor_fp_violation": 0.014105929907905166,
            "ave_precision_score": 0.6533929775862523,
            "fpr": 0.29276315789473684,
            "logloss": 2.2418221704438723,
            "mae": 0.36296120308045493,
            "precision": 0.6260504201680672,
            "recall": 0.9103869653767821
        },
        "train": {
            "accuracy": 0.6399560922063666,
            "auc_prc": 0.644291895298985,
            "auditor_fn_violation": 0.004165550400314848,
            "auditor_fp_violation": 0.023218206053002982,
            "ave_precision_score": 0.6379635776883719,
            "fpr": 0.31174533479692645,
            "logloss": 2.4348493490377296,
            "mae": 0.3826924226994081,
            "precision": 0.5960170697012802,
            "recall": 0.9049676025917927
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.6531576722267604,
            "auditor_fn_violation": 0.00911137313752814,
            "auditor_fp_violation": 0.0003672334041755188,
            "ave_precision_score": 0.6447662607794737,
            "fpr": 0.27521929824561403,
            "logloss": 2.13955869203927,
            "mae": 0.36368935959099125,
            "precision": 0.6372832369942196,
            "recall": 0.8981670061099797
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.6488528621474434,
            "auditor_fn_violation": 0.007605626456579412,
            "auditor_fp_violation": 0.01998392661125923,
            "ave_precision_score": 0.6426192456386288,
            "fpr": 0.27771679473106475,
            "logloss": 2.1715359316848817,
            "mae": 0.3656581753678824,
            "precision": 0.6189759036144579,
            "recall": 0.8876889848812095
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.6735248352311052,
            "auditor_fn_violation": 0.010587504912995323,
            "auditor_fp_violation": 0.012574488477726388,
            "ave_precision_score": 0.6651142582027756,
            "fpr": 0.28618421052631576,
            "logloss": 2.0857611052256746,
            "mae": 0.36328580146532896,
            "precision": 0.6266094420600858,
            "recall": 0.8920570264765784
        },
        "train": {
            "accuracy": 0.6564215148188803,
            "auc_prc": 0.6649343712478897,
            "auditor_fn_violation": 0.005450540905135933,
            "auditor_fp_violation": 0.026080053316606573,
            "ave_precision_score": 0.6586825369345104,
            "fpr": 0.28869374313940727,
            "logloss": 2.13051361068771,
            "mae": 0.36133502905270337,
            "precision": 0.6109467455621301,
            "recall": 0.8920086393088553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8213812517921333,
            "auditor_fn_violation": 0.0060921141958766595,
            "auditor_fp_violation": 0.0007188398549818786,
            "ave_precision_score": 0.8215109367915603,
            "fpr": 0.08662280701754387,
            "logloss": 0.8416665086760086,
            "mae": 0.27014650613130053,
            "precision": 0.8136792452830188,
            "recall": 0.7026476578411406
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.7998249066804135,
            "auditor_fn_violation": 0.012752700969432873,
            "auditor_fp_violation": 0.005032734828289168,
            "ave_precision_score": 0.8000341150485445,
            "fpr": 0.0889132821075741,
            "logloss": 0.8254380068413757,
            "mae": 0.2622731282156796,
            "precision": 0.8038740920096852,
            "recall": 0.7170626349892009
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7730263157894737,
            "auc_prc": 0.8543966013770135,
            "auditor_fn_violation": 0.020275038410690678,
            "auditor_fp_violation": 0.013918406467475102,
            "ave_precision_score": 0.8546162986222154,
            "fpr": 0.1074561403508772,
            "logloss": 0.5020385004530101,
            "mae": 0.32221840623369963,
            "precision": 0.7958333333333333,
            "recall": 0.7780040733197556
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8394212349809422,
            "auditor_fn_violation": 0.020690243792571245,
            "auditor_fp_violation": 0.012300062725419478,
            "ave_precision_score": 0.8400730822480129,
            "fpr": 0.10976948408342481,
            "logloss": 0.48687789403237497,
            "mae": 0.3161972220337916,
            "precision": 0.782608695652174,
            "recall": 0.7775377969762419
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.6848139806722637,
            "auditor_fn_violation": 0.018187015400007146,
            "auditor_fp_violation": 0.02186992124015503,
            "ave_precision_score": 0.6763787416605782,
            "fpr": 0.2817982456140351,
            "logloss": 2.2278487423793396,
            "mae": 0.3450018692419051,
            "precision": 0.6349431818181818,
            "recall": 0.9103869653767821
        },
        "train": {
            "accuracy": 0.6465422612513722,
            "auc_prc": 0.6562679161466768,
            "auditor_fn_violation": 0.013072763180043294,
            "auditor_fp_violation": 0.033494393915634316,
            "ave_precision_score": 0.649901745941168,
            "fpr": 0.29747530186608123,
            "logloss": 2.459652250364784,
            "mae": 0.36717614497525286,
            "precision": 0.6032210834553441,
            "recall": 0.8898488120950324
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7554689424562403,
            "auditor_fn_violation": 0.013740754636081038,
            "auditor_fp_violation": 0.020664041338500666,
            "ave_precision_score": 0.706561755487592,
            "fpr": 0.2050438596491228,
            "logloss": 3.3228774159626884,
            "mae": 0.30832717104216534,
            "precision": 0.6969205834683955,
            "recall": 0.8757637474541752
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.725412723320803,
            "auditor_fn_violation": 0.006019540390665564,
            "auditor_fp_violation": 0.01999862788144896,
            "ave_precision_score": 0.673518394100896,
            "fpr": 0.2305159165751921,
            "logloss": 3.470971974715212,
            "mae": 0.3248598527161478,
            "precision": 0.6579804560260586,
            "recall": 0.8725701943844493
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.6438391246278004,
            "auditor_fn_violation": 0.011663897523850358,
            "auditor_fp_violation": 0.009639225736550417,
            "ave_precision_score": 0.6354526627994855,
            "fpr": 0.3125,
            "logloss": 2.164510479180089,
            "mae": 0.38477384053954955,
            "precision": 0.6095890410958904,
            "recall": 0.9063136456211812
        },
        "train": {
            "accuracy": 0.6333699231613611,
            "auc_prc": 0.6396905597714171,
            "auditor_fn_violation": 0.004319654427645789,
            "auditor_fp_violation": 0.02338482044848675,
            "ave_precision_score": 0.6334929046901732,
            "fpr": 0.32491767288693746,
            "logloss": 2.1997429420864325,
            "mae": 0.39348499774912415,
            "precision": 0.5894590846047156,
            "recall": 0.91792656587473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.6510247072895776,
            "auditor_fn_violation": 0.008519580519526926,
            "auditor_fp_violation": 0.0012944326374130187,
            "ave_precision_score": 0.6426350790985987,
            "fpr": 0.2774122807017544,
            "logloss": 2.1582209414117663,
            "mae": 0.36406184434216765,
            "precision": 0.6380543633762518,
            "recall": 0.9083503054989817
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.6468221930055545,
            "auditor_fn_violation": 0.007465747416386712,
            "auditor_fp_violation": 0.02121883330719774,
            "ave_precision_score": 0.640591731901256,
            "fpr": 0.283205268935236,
            "logloss": 2.189312876751558,
            "mae": 0.36745196863018287,
            "precision": 0.6154992548435171,
            "recall": 0.8920086393088553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 14724,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.8107154597398155,
            "auditor_fn_violation": 0.020045020902561904,
            "auditor_fp_violation": 0.029441180147518457,
            "ave_precision_score": 0.8108618184101637,
            "fpr": 0.23903508771929824,
            "logloss": 0.9031725044051319,
            "mae": 0.32054687976494933,
            "precision": 0.669195751138088,
            "recall": 0.8981670061099797
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.8038808333902528,
            "auditor_fn_violation": 0.014239212125379038,
            "auditor_fp_violation": 0.04517455308138624,
            "ave_precision_score": 0.8051886082676935,
            "fpr": 0.27332601536772777,
            "logloss": 1.0303454541613661,
            "mae": 0.34284078728439377,
            "precision": 0.621580547112462,
            "recall": 0.8833693304535637
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.6730857184512631,
            "auditor_fn_violation": 0.008392289277164396,
            "auditor_fp_violation": 0.014691940659249078,
            "ave_precision_score": 0.6646631845510599,
            "fpr": 0.26206140350877194,
            "logloss": 2.2268972156388718,
            "mae": 0.3410640302585977,
            "precision": 0.6500732064421669,
            "recall": 0.9042769857433809
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.6520912224110166,
            "auditor_fn_violation": 0.00559990327008746,
            "auditor_fp_violation": 0.026168260937745033,
            "ave_precision_score": 0.6457287731499106,
            "fpr": 0.265642151481888,
            "logloss": 2.4208023377069425,
            "mae": 0.35666895203975274,
            "precision": 0.6316590563165906,
            "recall": 0.896328293736501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.8243691529814212,
            "auditor_fn_violation": 0.013148962018079823,
            "auditor_fp_violation": 0.014936762928699427,
            "ave_precision_score": 0.8248240981262063,
            "fpr": 0.2631578947368421,
            "logloss": 0.8680019665220727,
            "mae": 0.3303609727396785,
            "precision": 0.6536796536796536,
            "recall": 0.9226069246435845
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.8172649006854747,
            "auditor_fn_violation": 0.008670129660757767,
            "auditor_fp_violation": 0.03139211227850089,
            "ave_precision_score": 0.8176783498707717,
            "fpr": 0.27332601536772777,
            "logloss": 0.970877313560593,
            "mae": 0.3483506828596617,
            "precision": 0.6261261261261262,
            "recall": 0.9006479481641468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 14724,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8214113358130379,
            "auditor_fn_violation": 0.018233912173509132,
            "auditor_fp_violation": 0.014329916239529948,
            "ave_precision_score": 0.822689967549194,
            "fpr": 0.19298245614035087,
            "logloss": 0.6474244003574597,
            "mae": 0.32305775585777347,
            "precision": 0.705685618729097,
            "recall": 0.8594704684317719
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8276620822935268,
            "auditor_fn_violation": 0.013300362974255145,
            "auditor_fp_violation": 0.03217127959855732,
            "ave_precision_score": 0.8291206031373012,
            "fpr": 0.1964873765093304,
            "logloss": 0.6005943461216056,
            "mae": 0.3220262038403903,
            "precision": 0.6881533101045296,
            "recall": 0.8531317494600432
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.6681150040560985,
            "auditor_fn_violation": 0.011864883696001716,
            "auditor_fp_violation": 0.01406165354002584,
            "ave_precision_score": 0.6596943981002635,
            "fpr": 0.28289473684210525,
            "logloss": 2.18908558566615,
            "mae": 0.36493608041405023,
            "precision": 0.6277056277056277,
            "recall": 0.8859470468431772
        },
        "train": {
            "accuracy": 0.6498353457738749,
            "auc_prc": 0.6456643158891311,
            "auditor_fn_violation": 0.007864047056257453,
            "auditor_fp_violation": 0.026638701583816846,
            "ave_precision_score": 0.6393334294622757,
            "fpr": 0.29418221734357847,
            "logloss": 2.3889403228555093,
            "mae": 0.38034941160869423,
            "precision": 0.6058823529411764,
            "recall": 0.8898488120950324
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 14724,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7468305491470093,
            "auditor_fn_violation": 0.014062332511523211,
            "auditor_fp_violation": 0.006534671000541742,
            "ave_precision_score": 0.7481640363182789,
            "fpr": 0.18859649122807018,
            "logloss": 1.4232885878580062,
            "mae": 0.33476314081766123,
            "precision": 0.6906474820143885,
            "recall": 0.7820773930753564
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7058291202984054,
            "auditor_fn_violation": 0.008553958932462134,
            "auditor_fp_violation": 0.015994981966441904,
            "ave_precision_score": 0.7059702774762109,
            "fpr": 0.18880351262349068,
            "logloss": 1.5823014524332486,
            "mae": 0.34330895450847804,
            "precision": 0.6742424242424242,
            "recall": 0.7688984881209503
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.6775964255865335,
            "auditor_fn_violation": 0.010790724264837246,
            "auditor_fp_violation": 0.013241238488144357,
            "ave_precision_score": 0.6692483430118157,
            "fpr": 0.17214912280701755,
            "logloss": 1.9306770644626616,
            "mae": 0.3403200699361613,
            "precision": 0.7231040564373897,
            "recall": 0.835030549898167
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.6741048314263463,
            "auditor_fn_violation": 0.008494688152719464,
            "auditor_fp_violation": 0.014615512780304228,
            "ave_precision_score": 0.6678465379382152,
            "fpr": 0.17014270032930845,
            "logloss": 1.9817112266544694,
            "mae": 0.3318481420255731,
            "precision": 0.7134935304990758,
            "recall": 0.8336933045356372
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5899122807017544,
            "auc_prc": 0.6816918104231379,
            "auditor_fn_violation": 0.007003251509629472,
            "auditor_fp_violation": 0.011355586114931053,
            "ave_precision_score": 0.6732778780698345,
            "fpr": 0.39144736842105265,
            "logloss": 2.2799631408308474,
            "mae": 0.3830731780351989,
            "precision": 0.5703971119133574,
            "recall": 0.9653767820773931
        },
        "train": {
            "accuracy": 0.5905598243688255,
            "auc_prc": 0.6753979248975668,
            "auditor_fn_violation": 0.003565730109319026,
            "auditor_fp_violation": 0.018511349380586486,
            "ave_precision_score": 0.6691385869154985,
            "fpr": 0.39846322722283206,
            "logloss": 2.3347020645612946,
            "mae": 0.38998149726749426,
            "precision": 0.5551470588235294,
            "recall": 0.978401727861771
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.6504387473608897,
            "auditor_fn_violation": 0.008519580519526926,
            "auditor_fp_violation": 0.0033806309121973735,
            "ave_precision_score": 0.6420472946582716,
            "fpr": 0.27960526315789475,
            "logloss": 2.162273390188642,
            "mae": 0.36418462250730954,
            "precision": 0.6362339514978602,
            "recall": 0.9083503054989817
        },
        "train": {
            "accuracy": 0.6630076838638859,
            "auc_prc": 0.6465814536101503,
            "auditor_fn_violation": 0.0076483014179941355,
            "auditor_fp_violation": 0.022348380900109783,
            "ave_precision_score": 0.6403513442592578,
            "fpr": 0.2843029637760702,
            "logloss": 2.1930109242539664,
            "mae": 0.36772084837211,
            "precision": 0.615727002967359,
            "recall": 0.896328293736501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8331153542207548,
            "auditor_fn_violation": 0.015248150927216209,
            "auditor_fp_violation": 0.014921135975330253,
            "ave_precision_score": 0.8343547866427623,
            "fpr": 0.17543859649122806,
            "logloss": 0.625048761454191,
            "mae": 0.31706514420486137,
            "precision": 0.7264957264957265,
            "recall": 0.8655804480651731
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8355821696633554,
            "auditor_fn_violation": 0.011811480987119274,
            "auditor_fp_violation": 0.02620746432491768,
            "ave_precision_score": 0.8370324335870905,
            "fpr": 0.18441273326015367,
            "logloss": 0.5814256223062264,
            "mae": 0.3165124088850866,
            "precision": 0.7026548672566372,
            "recall": 0.857451403887689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.6568425015834075,
            "auditor_fn_violation": 0.007925554721835138,
            "auditor_fp_violation": 0.0025055215235237844,
            "ave_precision_score": 0.6484486112241471,
            "fpr": 0.2708333333333333,
            "logloss": 2.122048840960417,
            "mae": 0.36250208428164277,
            "precision": 0.6425470332850941,
            "recall": 0.9042769857433809
        },
        "train": {
            "accuracy": 0.6630076838638859,
            "auc_prc": 0.6508930388755243,
            "auditor_fn_violation": 0.007231035128605739,
            "auditor_fp_violation": 0.02242433746275678,
            "ave_precision_score": 0.6446584608981217,
            "fpr": 0.2810098792535675,
            "logloss": 2.1601089888126537,
            "mae": 0.36558310705483466,
            "precision": 0.6167664670658682,
            "recall": 0.8898488120950324
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6326754385964912,
            "auc_prc": 0.6142899226424652,
            "auditor_fn_violation": 0.010018044091899811,
            "auditor_fp_violation": 0.004216672917448017,
            "ave_precision_score": 0.6061268156876569,
            "fpr": 0.3223684210526316,
            "logloss": 2.3412685827650574,
            "mae": 0.38849795126640074,
            "precision": 0.6048387096774194,
            "recall": 0.9164969450101833
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.6060669097198739,
            "auditor_fn_violation": 0.007916205342431005,
            "auditor_fp_violation": 0.021566763368355054,
            "ave_precision_score": 0.5999415309958649,
            "fpr": 0.3216245883644347,
            "logloss": 2.3750533557332565,
            "mae": 0.39415095712608506,
            "precision": 0.5896358543417367,
            "recall": 0.9092872570194385
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.6720872245382647,
            "auditor_fn_violation": 0.010134169435809482,
            "auditor_fp_violation": 0.0070972413218318925,
            "ave_precision_score": 0.6638145444729897,
            "fpr": 0.2708333333333333,
            "logloss": 2.015328292942169,
            "mae": 0.35764695972402316,
            "precision": 0.6415094339622641,
            "recall": 0.90020366598778
        },
        "train": {
            "accuracy": 0.6717892425905598,
            "auc_prc": 0.659919790362461,
            "auditor_fn_violation": 0.007012918659152712,
            "auditor_fp_violation": 0.022473341696722594,
            "ave_precision_score": 0.6537444398301008,
            "fpr": 0.2711306256860593,
            "logloss": 2.068647038792229,
            "mae": 0.3611790142372214,
            "precision": 0.6246200607902735,
            "recall": 0.8876889848812095
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.6350340967114877,
            "auditor_fn_violation": 0.028444009718798017,
            "auditor_fp_violation": 0.017049006125765723,
            "ave_precision_score": 0.6267114070241471,
            "fpr": 0.18092105263157895,
            "logloss": 1.9853380809425627,
            "mae": 0.3972609237044756,
            "precision": 0.6966911764705882,
            "recall": 0.7718940936863544
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.6325128150109184,
            "auditor_fn_violation": 0.01860865400800867,
            "auditor_fp_violation": 0.022708562019758502,
            "ave_precision_score": 0.6262976534654798,
            "fpr": 0.19209659714599342,
            "logloss": 2.034887195168868,
            "mae": 0.3889207201800284,
            "precision": 0.6829710144927537,
            "recall": 0.8142548596112311
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6513157894736842,
            "auc_prc": 0.657831643570397,
            "auditor_fn_violation": 0.009265462536177512,
            "auditor_fp_violation": 0.01318654415135226,
            "ave_precision_score": 0.6494383846766344,
            "fpr": 0.30701754385964913,
            "logloss": 2.165127270013614,
            "mae": 0.368426711640623,
            "precision": 0.6180081855388813,
            "recall": 0.9226069246435845
        },
        "train": {
            "accuracy": 0.6520307354555434,
            "auc_prc": 0.6511610145884427,
            "auditor_fn_violation": 0.004623120819928259,
            "auditor_fp_violation": 0.015504939626783764,
            "ave_precision_score": 0.6449273871704113,
            "fpr": 0.3029637760702525,
            "logloss": 2.2056364752561093,
            "mae": 0.3733624915104731,
            "precision": 0.6045845272206304,
            "recall": 0.9114470842332614
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.6646318510817152,
            "auditor_fn_violation": 0.008349858863043558,
            "auditor_fp_violation": 0.022672104846439136,
            "ave_precision_score": 0.6563032764844223,
            "fpr": 0.2817982456140351,
            "logloss": 2.0810692031415448,
            "mae": 0.3618839724548978,
            "precision": 0.6318051575931232,
            "recall": 0.8981670061099797
        },
        "train": {
            "accuracy": 0.6695938529088913,
            "auc_prc": 0.6637287911542078,
            "auditor_fn_violation": 0.004293575284559017,
            "auditor_fp_violation": 0.01666633997177356,
            "ave_precision_score": 0.6575558101385649,
            "fpr": 0.283205268935236,
            "logloss": 2.0824594880746017,
            "mae": 0.358394546765511,
            "precision": 0.6194690265486725,
            "recall": 0.9071274298056156
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.6730595880221755,
            "auditor_fn_violation": 0.008392289277164396,
            "auditor_fp_violation": 0.012939117389673713,
            "ave_precision_score": 0.6646443826808162,
            "fpr": 0.2642543859649123,
            "logloss": 2.2075722149668415,
            "mae": 0.34176977240611195,
            "precision": 0.6481751824817519,
            "recall": 0.9042769857433809
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.6550010067339016,
            "auditor_fn_violation": 0.005692365686486024,
            "auditor_fp_violation": 0.024962756782185982,
            "ave_precision_score": 0.6487059323998597,
            "fpr": 0.2678375411635565,
            "logloss": 2.367594113047789,
            "mae": 0.3551132730126553,
            "precision": 0.6314199395770392,
            "recall": 0.9028077753779697
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7080875678097275,
            "auditor_fn_violation": 0.016849340765355346,
            "auditor_fp_violation": 0.016296307871817305,
            "ave_precision_score": 0.6997788372953871,
            "fpr": 0.1524122807017544,
            "logloss": 1.8325490318232096,
            "mae": 0.32264833802445647,
            "precision": 0.7362428842504743,
            "recall": 0.790224032586558
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.6832135345020571,
            "auditor_fn_violation": 0.01719563861894342,
            "auditor_fp_violation": 0.01738180178767446,
            "ave_precision_score": 0.6770138739468026,
            "fpr": 0.15477497255762898,
            "logloss": 1.9104625798488886,
            "mae": 0.32494095882162416,
            "precision": 0.7262135922330097,
            "recall": 0.8077753779697624
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 14724,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.6542350320863597,
            "auditor_fn_violation": 0.011344552828098762,
            "auditor_fp_violation": 0.003815581114305949,
            "ave_precision_score": 0.6458426037584011,
            "fpr": 0.24780701754385964,
            "logloss": 2.085025293215588,
            "mae": 0.3646892695309872,
            "precision": 0.6533742331288344,
            "recall": 0.8676171079429735
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.6484288584476176,
            "auditor_fn_violation": 0.007873530381016285,
            "auditor_fp_violation": 0.01708777638387958,
            "ave_precision_score": 0.6421926170936177,
            "fpr": 0.25686059275521406,
            "logloss": 2.1220120299692824,
            "mae": 0.36398899906880244,
            "precision": 0.631496062992126,
            "recall": 0.8660907127429806
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.8829118498953518,
            "auditor_fn_violation": 0.018950762854182304,
            "auditor_fp_violation": 0.021773555027711798,
            "ave_precision_score": 0.8830556753965674,
            "fpr": 0.13815789473684212,
            "logloss": 0.5344266495984942,
            "mae": 0.26828703253832636,
            "precision": 0.7666666666666667,
            "recall": 0.8431771894093686
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8544052658611694,
            "auditor_fn_violation": 0.010635548717024703,
            "auditor_fp_violation": 0.023531833150384197,
            "ave_precision_score": 0.8549140312611527,
            "fpr": 0.1602634467618002,
            "logloss": 0.5172066464043259,
            "mae": 0.2748357905877166,
            "precision": 0.7306273062730627,
            "recall": 0.8552915766738661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.6762540546171886,
            "auditor_fn_violation": 0.04064387036838533,
            "auditor_fp_violation": 0.025659457432179032,
            "ave_precision_score": 0.6678317698525041,
            "fpr": 0.21600877192982457,
            "logloss": 2.101073743390352,
            "mae": 0.3563858540268611,
            "precision": 0.6722129783693843,
            "recall": 0.8228105906313645
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.6498793178982275,
            "auditor_fn_violation": 0.03615991730540811,
            "auditor_fp_violation": 0.04075682138936804,
            "ave_precision_score": 0.6436050050111466,
            "fpr": 0.23929747530186607,
            "logloss": 2.18413913263063,
            "mae": 0.3635359910264811,
            "precision": 0.6378737541528239,
            "recall": 0.8293736501079914
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7708333333333334,
            "auc_prc": 0.8614693906157809,
            "auditor_fn_violation": 0.034893432665165966,
            "auditor_fp_violation": 0.02415145643205401,
            "ave_precision_score": 0.8617484804850217,
            "fpr": 0.13157894736842105,
            "logloss": 0.536560551039877,
            "mae": 0.3024120112185253,
            "precision": 0.7701149425287356,
            "recall": 0.8187372708757638
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8433342957310064,
            "auditor_fn_violation": 0.029275023530499558,
            "auditor_fp_violation": 0.024705484553865464,
            "ave_precision_score": 0.8440257215289373,
            "fpr": 0.132821075740944,
            "logloss": 0.5312388289367737,
            "mae": 0.30314848175530784,
            "precision": 0.7535641547861507,
            "recall": 0.7991360691144709
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.7049267699538772,
            "auditor_fn_violation": 0.01699673062493301,
            "auditor_fp_violation": 0.020096262032754098,
            "ave_precision_score": 0.6964781423499846,
            "fpr": 0.14035087719298245,
            "logloss": 2.0153500190878644,
            "mae": 0.31072136265197114,
            "precision": 0.7547892720306514,
            "recall": 0.8024439918533605
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.6763376486027911,
            "auditor_fn_violation": 0.009843691099662637,
            "auditor_fp_violation": 0.013419809471538337,
            "ave_precision_score": 0.6699520646794863,
            "fpr": 0.15477497255762898,
            "logloss": 2.2155486929718435,
            "mae": 0.3181809788676128,
            "precision": 0.7288461538461538,
            "recall": 0.8185745140388769
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.6747329735013777,
            "auditor_fn_violation": 0.011715260656733485,
            "auditor_fp_violation": 0.014509626203275422,
            "ave_precision_score": 0.6663097665203251,
            "fpr": 0.2708333333333333,
            "logloss": 2.246562948949313,
            "mae": 0.3418399222361602,
            "precision": 0.6461318051575932,
            "recall": 0.9185336048879837
        },
        "train": {
            "accuracy": 0.6717892425905598,
            "auc_prc": 0.652817886753891,
            "auditor_fn_violation": 0.006659664811886399,
            "auditor_fp_violation": 0.02767269092049554,
            "ave_precision_score": 0.646455579387125,
            "fpr": 0.27442371020856204,
            "logloss": 2.4467338618467305,
            "mae": 0.3587443438217015,
            "precision": 0.6234939759036144,
            "recall": 0.8941684665226782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.6649493624772804,
            "auditor_fn_violation": 0.011404848679744171,
            "auditor_fp_violation": 0.013183939659124065,
            "ave_precision_score": 0.6566105808607978,
            "fpr": 0.1524122807017544,
            "logloss": 1.9337129206498207,
            "mae": 0.3587638369336083,
            "precision": 0.7326923076923076,
            "recall": 0.7759674134419552
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.6615909099338583,
            "auditor_fn_violation": 0.004739291548223894,
            "auditor_fp_violation": 0.012596538340912657,
            "ave_precision_score": 0.6554100911823743,
            "fpr": 0.150384193194292,
            "logloss": 1.9415537671631136,
            "mae": 0.3512436036056554,
            "precision": 0.7276341948310139,
            "recall": 0.7904967602591793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.6535425530807126,
            "auditor_fn_violation": 0.022157608889841713,
            "auditor_fp_violation": 0.020611951493936742,
            "ave_precision_score": 0.6451523534942362,
            "fpr": 0.26096491228070173,
            "logloss": 2.1695653237154824,
            "mae": 0.36161661746377893,
            "precision": 0.6484490398818316,
            "recall": 0.8940936863543788
        },
        "train": {
            "accuracy": 0.6410537870472008,
            "auc_prc": 0.6409302588811753,
            "auditor_fn_violation": 0.025472210302209853,
            "auditor_fp_violation": 0.03850752705033716,
            "ave_precision_score": 0.6347019413143247,
            "fpr": 0.287596048298573,
            "logloss": 2.233426257014887,
            "mae": 0.3721398001602263,
            "precision": 0.603030303030303,
            "recall": 0.8596112311015118
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.7008491224812752,
            "auditor_fn_violation": 0.013445974916925715,
            "auditor_fp_violation": 0.012959953327499269,
            "ave_precision_score": 0.6932649444167193,
            "fpr": 0.15570175438596492,
            "logloss": 1.7775379581308461,
            "mae": 0.33176289348691507,
            "precision": 0.7340823970037453,
            "recall": 0.7983706720977597
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.6855080054415712,
            "auditor_fn_violation": 0.0074894557282837795,
            "auditor_fp_violation": 0.014662066802571748,
            "ave_precision_score": 0.6793826654807592,
            "fpr": 0.15806805708013172,
            "logloss": 1.8346088415048432,
            "mae": 0.3264269336141057,
            "precision": 0.7277882797731569,
            "recall": 0.8315334773218143
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.6219548322353956,
            "auditor_fn_violation": 0.01848849465823418,
            "auditor_fp_violation": 0.008032254031753968,
            "ave_precision_score": 0.613151224729536,
            "fpr": 0.17324561403508773,
            "logloss": 2.6258073199175267,
            "mae": 0.36153015476494993,
            "precision": 0.7018867924528301,
            "recall": 0.7576374745417516
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.6012105514471879,
            "auditor_fn_violation": 0.02338824968645758,
            "auditor_fp_violation": 0.014441547749725578,
            "ave_precision_score": 0.5948139963257149,
            "fpr": 0.17672886937431395,
            "logloss": 2.5673787318999133,
            "mae": 0.3641093734083273,
            "precision": 0.6811881188118812,
            "recall": 0.7429805615550756
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.655528520158333,
            "auditor_fn_violation": 0.018055257798263485,
            "auditor_fp_violation": 0.013441784389715385,
            "ave_precision_score": 0.6472032193723871,
            "fpr": 0.2708333333333333,
            "logloss": 2.095280198584641,
            "mae": 0.3657925672626956,
            "precision": 0.6409883720930233,
            "recall": 0.8981670061099797
        },
        "train": {
            "accuracy": 0.6564215148188803,
            "auc_prc": 0.6532277203541285,
            "auditor_fn_violation": 0.010372386454967247,
            "auditor_fp_violation": 0.025004410381056944,
            "ave_precision_score": 0.6470564597931653,
            "fpr": 0.28869374313940727,
            "logloss": 2.1087036485016486,
            "mae": 0.36639202317128644,
            "precision": 0.6109467455621301,
            "recall": 0.8920086393088553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8288521242285599,
            "auditor_fn_violation": 0.014352645871297389,
            "auditor_fp_violation": 0.020455681960245036,
            "ave_precision_score": 0.8292140308851506,
            "fpr": 0.23684210526315788,
            "logloss": 0.8216688306361531,
            "mae": 0.3188165192850488,
            "precision": 0.675187969924812,
            "recall": 0.9144602851323829
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.8240171291196686,
            "auditor_fn_violation": 0.00893329192281522,
            "auditor_fp_violation": 0.035996060059589156,
            "ave_precision_score": 0.824370528736299,
            "fpr": 0.2491767288693743,
            "logloss": 0.9194781995788329,
            "mae": 0.3358239092103499,
            "precision": 0.6447574334898278,
            "recall": 0.8898488120950324
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.6899659281752428,
            "auditor_fn_violation": 0.016552327866509453,
            "auditor_fp_violation": 0.016697399674959375,
            "ave_precision_score": 0.6816060220550598,
            "fpr": 0.1425438596491228,
            "logloss": 1.8997430343317558,
            "mae": 0.3339833086581182,
            "precision": 0.7425742574257426,
            "recall": 0.7637474541751528
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.6745413478181922,
            "auditor_fn_violation": 0.00426512531028253,
            "auditor_fp_violation": 0.018053159793006122,
            "ave_precision_score": 0.6683546169032974,
            "fpr": 0.14818880351262348,
            "logloss": 1.9259773526058497,
            "mae": 0.330918282534187,
            "precision": 0.73,
            "recall": 0.7883369330453563
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.6743644938414193,
            "auditor_fn_violation": 0.025657001464965877,
            "auditor_fp_violation": 0.02760240863441264,
            "ave_precision_score": 0.6660917419656589,
            "fpr": 0.1600877192982456,
            "logloss": 1.876785852023493,
            "mae": 0.3603052328073294,
            "precision": 0.7153996101364523,
            "recall": 0.7474541751527495
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.6481977919054807,
            "auditor_fn_violation": 0.00660039403214373,
            "auditor_fp_violation": 0.019611494433119027,
            "ave_precision_score": 0.642039490258765,
            "fpr": 0.18111964873765093,
            "logloss": 1.944668993855893,
            "mae": 0.36594580610798555,
            "precision": 0.6826923076923077,
            "recall": 0.7667386609071274
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 14724,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.6947593945094135,
            "auditor_fn_violation": 0.010993943616679174,
            "auditor_fp_violation": 0.02037233820894279,
            "ave_precision_score": 0.6863856764448029,
            "fpr": 0.1699561403508772,
            "logloss": 2.01339077087728,
            "mae": 0.3160976256813707,
            "precision": 0.7322970639032815,
            "recall": 0.8635437881873728
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.6777158005058135,
            "auditor_fn_violation": 0.007029514477480662,
            "auditor_fp_violation": 0.02153001019288068,
            "ave_precision_score": 0.6713962583531239,
            "fpr": 0.1877058177826564,
            "logloss": 2.180917542035658,
            "mae": 0.32153663439338803,
            "precision": 0.7,
            "recall": 0.8617710583153347
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.6510265156200183,
            "auditor_fn_violation": 0.008519580519526926,
            "auditor_fp_violation": 0.0012944326374130187,
            "ave_precision_score": 0.642636885131743,
            "fpr": 0.2774122807017544,
            "logloss": 2.158221693494056,
            "mae": 0.3640624008681666,
            "precision": 0.6380543633762518,
            "recall": 0.9083503054989817
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.6468221930055545,
            "auditor_fn_violation": 0.007465747416386712,
            "auditor_fp_violation": 0.02121883330719774,
            "ave_precision_score": 0.640591731901256,
            "fpr": 0.283205268935236,
            "logloss": 2.1893141658486255,
            "mae": 0.3674524344785525,
            "precision": 0.6154992548435171,
            "recall": 0.8920086393088553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.659334412023183,
            "auditor_fn_violation": 0.010341855147032552,
            "auditor_fp_violation": 0.012605742384464737,
            "ave_precision_score": 0.6509375558460002,
            "fpr": 0.31140350877192985,
            "logloss": 2.1989294591080406,
            "mae": 0.370633962532003,
            "precision": 0.6172506738544474,
            "recall": 0.9327902240325866
        },
        "train": {
            "accuracy": 0.6421514818880352,
            "auc_prc": 0.6520102123955958,
            "auditor_fn_violation": 0.004049379672019215,
            "auditor_fp_violation": 0.02422769327269877,
            "ave_precision_score": 0.6457766480346304,
            "fpr": 0.31613611416026344,
            "logloss": 2.240844152185553,
            "mae": 0.37651120265255733,
            "precision": 0.5960729312762973,
            "recall": 0.91792656587473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.6680299309425529,
            "auditor_fn_violation": 0.00426314002929932,
            "auditor_fp_violation": 0.01803350418802351,
            "ave_precision_score": 0.6595503763076346,
            "fpr": 0.26864035087719296,
            "logloss": 2.2659424325072486,
            "mae": 0.3455153256573044,
            "precision": 0.6444121915820029,
            "recall": 0.9042769857433809
        },
        "train": {
            "accuracy": 0.6663007683863886,
            "auc_prc": 0.6501273740909026,
            "auditor_fn_violation": 0.0026742975819892714,
            "auditor_fp_violation": 0.01615179551513252,
            "ave_precision_score": 0.6437653306680362,
            "fpr": 0.2810098792535675,
            "logloss": 2.430162513648371,
            "mae": 0.35980680895280237,
            "precision": 0.6184798807749627,
            "recall": 0.896328293736501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.6694901614410931,
            "auditor_fn_violation": 0.008863490191874799,
            "auditor_fp_violation": 0.008107784306371628,
            "ave_precision_score": 0.6610844987639138,
            "fpr": 0.24561403508771928,
            "logloss": 2.0571533504334623,
            "mae": 0.34993623753625636,
            "precision": 0.6626506024096386,
            "recall": 0.8961303462321792
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6617779357070377,
            "auditor_fn_violation": 0.005275099397097628,
            "auditor_fp_violation": 0.017670926768072776,
            "ave_precision_score": 0.6555342905565867,
            "fpr": 0.2557628979143798,
            "logloss": 2.10173940955152,
            "mae": 0.3510615851261891,
            "precision": 0.6381987577639752,
            "recall": 0.8876889848812095
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.651021687838698,
            "auditor_fn_violation": 0.008519580519526926,
            "auditor_fp_violation": 0.0012944326374130187,
            "ave_precision_score": 0.6426320614910841,
            "fpr": 0.2774122807017544,
            "logloss": 2.1582189061807626,
            "mae": 0.3640609273774827,
            "precision": 0.6380543633762518,
            "recall": 0.9083503054989817
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.6468293141843784,
            "auditor_fn_violation": 0.007465747416386712,
            "auditor_fp_violation": 0.02121883330719774,
            "ave_precision_score": 0.640615740500864,
            "fpr": 0.283205268935236,
            "logloss": 2.18931095111703,
            "mae": 0.36745120141410875,
            "precision": 0.6154992548435171,
            "recall": 0.8920086393088553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.7113296831462453,
            "auditor_fn_violation": 0.019370600636009573,
            "auditor_fp_violation": 0.020684877276326206,
            "ave_precision_score": 0.7028737796469091,
            "fpr": 0.13815789473684212,
            "logloss": 2.0091490946452,
            "mae": 0.2991952955094268,
            "precision": 0.7567567567567568,
            "recall": 0.7983706720977597
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.6800538351082425,
            "auditor_fn_violation": 0.00638227756269071,
            "auditor_fp_violation": 0.01383389524854948,
            "ave_precision_score": 0.6736963724800297,
            "fpr": 0.15148188803512624,
            "logloss": 2.200595638503114,
            "mae": 0.3086051282234711,
            "precision": 0.7299412915851272,
            "recall": 0.8056155507559395
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.6703047941569137,
            "auditor_fn_violation": 0.007818362096687749,
            "auditor_fp_violation": 0.010477872234029259,
            "ave_precision_score": 0.6618947871967586,
            "fpr": 0.32456140350877194,
            "logloss": 2.2048676484196816,
            "mae": 0.3748431139055787,
            "precision": 0.6084656084656085,
            "recall": 0.9368635437881874
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.6491768345207212,
            "auditor_fn_violation": 0.0009957490996768549,
            "auditor_fp_violation": 0.022527246354085012,
            "ave_precision_score": 0.6429371277273187,
            "fpr": 0.33479692645444564,
            "logloss": 2.2797648662432404,
            "mae": 0.3858753674833306,
            "precision": 0.5827633378932968,
            "recall": 0.9200863930885529
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.6526589945623096,
            "auditor_fn_violation": 0.008742898488583986,
            "auditor_fp_violation": 0.009623598783181241,
            "ave_precision_score": 0.6442697444483222,
            "fpr": 0.2774122807017544,
            "logloss": 2.1466494012119024,
            "mae": 0.3631409027408441,
            "precision": 0.6390870185449358,
            "recall": 0.9124236252545825
        },
        "train": {
            "accuracy": 0.6641053787047201,
            "auc_prc": 0.6461714114644049,
            "auditor_fn_violation": 0.008632196361722458,
            "auditor_fp_violation": 0.023252509016779055,
            "ave_precision_score": 0.6399437256061189,
            "fpr": 0.283205268935236,
            "logloss": 2.1874013054004746,
            "mae": 0.36881040096521667,
            "precision": 0.6166419019316494,
            "recall": 0.896328293736501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6513157894736842,
            "auc_prc": 0.6517756111470242,
            "auditor_fn_violation": 0.03192107049701648,
            "auditor_fp_violation": 0.02930835104388049,
            "ave_precision_score": 0.6433475175111524,
            "fpr": 0.27960526315789475,
            "logloss": 2.836791040198565,
            "mae": 0.36268282288372755,
            "precision": 0.6266471449487555,
            "recall": 0.8716904276985743
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.6232913975246072,
            "auditor_fn_violation": 0.02684017989867068,
            "auditor_fp_violation": 0.04480947153834091,
            "ave_precision_score": 0.6169730828727499,
            "fpr": 0.29857299670691545,
            "logloss": 2.969207914848818,
            "mae": 0.3827175522835102,
            "precision": 0.592814371257485,
            "recall": 0.8552915766738661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.6514875110716578,
            "auditor_fn_violation": 0.007621842283917532,
            "auditor_fp_violation": 0.0012944326374130187,
            "ave_precision_score": 0.6431667290042604,
            "fpr": 0.2774122807017544,
            "logloss": 2.1230773498616653,
            "mae": 0.36357829644826745,
            "precision": 0.6375358166189111,
            "recall": 0.9063136456211812
        },
        "train": {
            "accuracy": 0.6641053787047201,
            "auc_prc": 0.6492301487545848,
            "auditor_fn_violation": 0.0084022257363209,
            "auditor_fp_violation": 0.019822212639172027,
            "ave_precision_score": 0.6430667243961405,
            "fpr": 0.28210757409440174,
            "logloss": 2.133539356519818,
            "mae": 0.3656305829586991,
            "precision": 0.61698956780924,
            "recall": 0.8941684665226782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.6652962095390855,
            "auditor_fn_violation": 0.009535677278736557,
            "auditor_fp_violation": 0.006185669041963592,
            "ave_precision_score": 0.6568949023076808,
            "fpr": 0.2642543859649123,
            "logloss": 2.1015795155143437,
            "mae": 0.3564721594734134,
            "precision": 0.6502177068214804,
            "recall": 0.9124236252545825
        },
        "train": {
            "accuracy": 0.6663007683863886,
            "auc_prc": 0.6584001849048111,
            "auditor_fn_violation": 0.005211086954975544,
            "auditor_fp_violation": 0.01648012388270346,
            "ave_precision_score": 0.6521587088202276,
            "fpr": 0.2864983534577388,
            "logloss": 2.1434226681764748,
            "mae": 0.358863913024746,
            "precision": 0.6167400881057269,
            "recall": 0.9071274298056156
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.6826635819450858,
            "auditor_fn_violation": 0.017278111265944904,
            "auditor_fp_violation": 0.00805048547735135,
            "ave_precision_score": 0.6744485946751133,
            "fpr": 0.22807017543859648,
            "logloss": 1.9154008254323105,
            "mae": 0.3522448646590898,
            "precision": 0.6724409448818898,
            "recall": 0.869653767820774
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.6669107566772126,
            "auditor_fn_violation": 0.009227274990338864,
            "auditor_fp_violation": 0.02303198996393289,
            "ave_precision_score": 0.6607290263842973,
            "fpr": 0.23600439077936333,
            "logloss": 1.9950261699760392,
            "mae": 0.35222283561556045,
            "precision": 0.6504065040650406,
            "recall": 0.8639308855291576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7999003603963639,
            "auditor_fn_violation": 0.015020366598778007,
            "auditor_fp_violation": 0.02049735383589616,
            "ave_precision_score": 0.800452887522126,
            "fpr": 0.25,
            "logloss": 0.7241587667496054,
            "mae": 0.33814951324619186,
            "precision": 0.6612184249628529,
            "recall": 0.9063136456211812
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.8146334370143629,
            "auditor_fn_violation": 0.009208308340821212,
            "auditor_fp_violation": 0.03403589070095656,
            "ave_precision_score": 0.8150740768291171,
            "fpr": 0.2667398463227223,
            "logloss": 0.7096955615341118,
            "mae": 0.3445703900973394,
            "precision": 0.6284403669724771,
            "recall": 0.8876889848812095
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.6937556502953316,
            "auditor_fn_violation": 0.013113231143030696,
            "auditor_fp_violation": 0.014319498270617162,
            "ave_precision_score": 0.6708383577973016,
            "fpr": 0.20942982456140352,
            "logloss": 2.7101300539635664,
            "mae": 0.3471367189945421,
            "precision": 0.6848184818481848,
            "recall": 0.845213849287169
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.6631243757837767,
            "auditor_fn_violation": 0.010728011133423273,
            "auditor_fp_violation": 0.014975693899952975,
            "ave_precision_score": 0.6392948347199074,
            "fpr": 0.2261251372118551,
            "logloss": 2.8672151721177803,
            "mae": 0.3533638681917049,
            "precision": 0.6543624161073825,
            "recall": 0.8423326133909287
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.6524740791360712,
            "auditor_fn_violation": 0.010632168506806733,
            "auditor_fp_violation": 0.006607596782931211,
            "ave_precision_score": 0.6440847292322237,
            "fpr": 0.27960526315789475,
            "logloss": 2.1572994614034235,
            "mae": 0.3627856132449795,
            "precision": 0.6372688477951636,
            "recall": 0.9124236252545825
        },
        "train": {
            "accuracy": 0.6597145993413831,
            "auc_prc": 0.6474470699806831,
            "auditor_fn_violation": 0.008058455213813412,
            "auditor_fp_violation": 0.023443625529245744,
            "ave_precision_score": 0.6412171218815282,
            "fpr": 0.2854006586169045,
            "logloss": 2.1914671422715633,
            "mae": 0.36680391393042183,
            "precision": 0.6136701337295691,
            "recall": 0.8920086393088553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.706841842342919,
            "auditor_fn_violation": 0.0066727409154250196,
            "auditor_fp_violation": 0.01853877568029337,
            "ave_precision_score": 0.698464247952434,
            "fpr": 0.1600877192982456,
            "logloss": 1.869742434825457,
            "mae": 0.31347416726853944,
            "precision": 0.7383512544802867,
            "recall": 0.8391038696537678
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.6846865487983647,
            "auditor_fn_violation": 0.012693430189690207,
            "auditor_fp_violation": 0.012015838168417756,
            "ave_precision_score": 0.6784847957201792,
            "fpr": 0.16465422612513722,
            "logloss": 1.9235333824808978,
            "mae": 0.31590361219884994,
            "precision": 0.7185741088180112,
            "recall": 0.8272138228941684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.5577853489766096,
            "auditor_fn_violation": 0.005654410976524816,
            "auditor_fp_violation": 0.005584031337250515,
            "ave_precision_score": 0.5444578990719819,
            "fpr": 0.36293859649122806,
            "logloss": 3.459846020029198,
            "mae": 0.41372217860669225,
            "precision": 0.5841708542713567,
            "recall": 0.9470468431771895
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.5481297411992334,
            "auditor_fn_violation": 0.00440737518166494,
            "auditor_fp_violation": 0.012317214207307536,
            "ave_precision_score": 0.5362199665334436,
            "fpr": 0.3677277716794731,
            "logloss": 3.3010247351895687,
            "mae": 0.4264826316178933,
            "precision": 0.5597897503285151,
            "recall": 0.9200863930885529
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8704276043534259,
            "auditor_fn_violation": 0.01945992782363241,
            "auditor_fp_violation": 0.006550297953910906,
            "ave_precision_score": 0.8706204680541961,
            "fpr": 0.04057017543859649,
            "logloss": 0.5903787088016074,
            "mae": 0.34824289778822404,
            "precision": 0.8810289389067524,
            "recall": 0.5580448065173116
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8506935410144985,
            "auditor_fn_violation": 0.021650430424402494,
            "auditor_fp_violation": 0.008210659400972245,
            "ave_precision_score": 0.851547713791186,
            "fpr": 0.04500548847420417,
            "logloss": 0.5598813716350872,
            "mae": 0.33298997409385145,
            "precision": 0.8681672025723473,
            "recall": 0.5831533477321814
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.6691526039268705,
            "auditor_fn_violation": 0.01711955550791439,
            "auditor_fp_violation": 0.00942305288161022,
            "ave_precision_score": 0.6607537814862452,
            "fpr": 0.24013157894736842,
            "logloss": 2.091739888904758,
            "mae": 0.3449580723257289,
            "precision": 0.664624808575804,
            "recall": 0.8839103869653768
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.6529459177569026,
            "auditor_fn_violation": 0.013971308200942173,
            "auditor_fp_violation": 0.026160910302650154,
            "ave_precision_score": 0.6467135399106896,
            "fpr": 0.2491767288693743,
            "logloss": 2.1542286075590122,
            "mae": 0.35077777027634116,
            "precision": 0.6396825396825396,
            "recall": 0.8704103671706264
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 14724,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.6540760300987115,
            "auditor_fn_violation": 0.008919319684139067,
            "auditor_fp_violation": 0.004638600658415638,
            "ave_precision_score": 0.6456841722434551,
            "fpr": 0.28618421052631576,
            "logloss": 2.158334101853856,
            "mae": 0.3633848969996172,
            "precision": 0.6329113924050633,
            "recall": 0.9164969450101833
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.649531465924529,
            "auditor_fn_violation": 0.006140452781340612,
            "auditor_fp_violation": 0.022948682766191008,
            "ave_precision_score": 0.6432987963700725,
            "fpr": 0.287596048298573,
            "logloss": 2.1914734079330387,
            "mae": 0.36762264227284003,
            "precision": 0.614138438880707,
            "recall": 0.9006479481641468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.7052738446109774,
            "auditor_fn_violation": 0.012525904884410619,
            "auditor_fp_violation": 0.020515585281493522,
            "ave_precision_score": 0.6968247622379276,
            "fpr": 0.14583333333333334,
            "logloss": 2.019343540442722,
            "mae": 0.3080451546299165,
            "precision": 0.7509363295880149,
            "recall": 0.8167006109979633
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.6766951372965948,
            "auditor_fn_violation": 0.01037712811734666,
            "auditor_fp_violation": 0.016683491453661603,
            "ave_precision_score": 0.6703087515193433,
            "fpr": 0.16355653128430298,
            "logloss": 2.216427889304462,
            "mae": 0.3149659835317394,
            "precision": 0.7199248120300752,
            "recall": 0.8272138228941684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.6780731653144245,
            "auditor_fn_violation": 0.008986315074856185,
            "auditor_fp_violation": 0.015343063716297865,
            "ave_precision_score": 0.6697180598313848,
            "fpr": 0.2916666666666667,
            "logloss": 2.209249299147088,
            "mae": 0.35402347110497784,
            "precision": 0.6305555555555555,
            "recall": 0.924643584521385
        },
        "train": {
            "accuracy": 0.6531284302963776,
            "auc_prc": 0.6546271312433767,
            "auditor_fn_violation": 0.0038265215401867758,
            "auditor_fp_violation": 0.018587305943233497,
            "ave_precision_score": 0.6483641249566887,
            "fpr": 0.300768386388584,
            "logloss": 2.3827284679512646,
            "mae": 0.3706099543256356,
            "precision": 0.6057553956834533,
            "recall": 0.9092872570194385
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.6723139094595628,
            "auditor_fn_violation": 0.011469610890770717,
            "auditor_fp_violation": 0.01623119556611243,
            "ave_precision_score": 0.6656616607057291,
            "fpr": 0.26096491228070173,
            "logloss": 2.2241366543741816,
            "mae": 0.34126798008673676,
            "precision": 0.6489675516224189,
            "recall": 0.8961303462321792
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.6568755151268122,
            "auditor_fn_violation": 0.01025858655786132,
            "auditor_fp_violation": 0.024556021640269727,
            "ave_precision_score": 0.6505438914185957,
            "fpr": 0.270032930845225,
            "logloss": 2.2788428656505584,
            "mae": 0.3559766013701878,
            "precision": 0.6255707762557078,
            "recall": 0.8876889848812095
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.6662572554447773,
            "auditor_fn_violation": 0.010893450530603496,
            "auditor_fp_violation": 0.01719746218277286,
            "ave_precision_score": 0.6578540690959939,
            "fpr": 0.2543859649122807,
            "logloss": 2.071062717233391,
            "mae": 0.35399207019394163,
            "precision": 0.6532137518684604,
            "recall": 0.890020366598778
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.6590612634543517,
            "auditor_fn_violation": 0.002816547453371677,
            "auditor_fp_violation": 0.018330033714912985,
            "ave_precision_score": 0.6528172167009431,
            "fpr": 0.2557628979143798,
            "logloss": 2.110737140539972,
            "mae": 0.3531865679631347,
            "precision": 0.6393188854489165,
            "recall": 0.8920086393088553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.6823405859544055,
            "auditor_fn_violation": 0.012298120555972417,
            "auditor_fp_violation": 0.01839031962328625,
            "ave_precision_score": 0.673907671385383,
            "fpr": 0.2894736842105263,
            "logloss": 2.257178265915618,
            "mae": 0.34973385471446244,
            "precision": 0.6328233657858137,
            "recall": 0.9266802443991853
        },
        "train": {
            "accuracy": 0.6465422612513722,
            "auc_prc": 0.6566204166993779,
            "auditor_fn_violation": 0.004623120819928259,
            "auditor_fp_violation": 0.019758507135016475,
            "ave_precision_score": 0.6502604995882815,
            "fpr": 0.30735455543358947,
            "logloss": 2.4740468278478995,
            "mae": 0.3695324806483211,
            "precision": 0.6005706134094151,
            "recall": 0.9092872570194385
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.671872395325397,
            "auditor_fn_violation": 0.010134169435809482,
            "auditor_fp_violation": 0.0070972413218318925,
            "ave_precision_score": 0.6635999702635065,
            "fpr": 0.2708333333333333,
            "logloss": 2.0161294613174636,
            "mae": 0.35768118980105784,
            "precision": 0.6415094339622641,
            "recall": 0.90020366598778
        },
        "train": {
            "accuracy": 0.6717892425905598,
            "auc_prc": 0.6598805511002261,
            "auditor_fn_violation": 0.007012918659152712,
            "auditor_fp_violation": 0.022473341696722594,
            "ave_precision_score": 0.6537052207163778,
            "fpr": 0.2711306256860593,
            "logloss": 2.0693589420409038,
            "mae": 0.3612516549468024,
            "precision": 0.6246200607902735,
            "recall": 0.8876889848812095
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6535087719298246,
            "auc_prc": 0.6651260042277349,
            "auditor_fn_violation": 0.010618769428663308,
            "auditor_fp_violation": 0.012298412301537713,
            "ave_precision_score": 0.6567424230338783,
            "fpr": 0.3026315789473684,
            "logloss": 2.137926108882561,
            "mae": 0.36589285527282445,
            "precision": 0.6203576341127923,
            "recall": 0.9185336048879837
        },
        "train": {
            "accuracy": 0.6531284302963776,
            "auc_prc": 0.6551413031908714,
            "auditor_fn_violation": 0.00643917751124367,
            "auditor_fp_violation": 0.02259830249333542,
            "ave_precision_score": 0.6489032580257428,
            "fpr": 0.29747530186608123,
            "logloss": 2.1882109407364525,
            "mae": 0.3717239539360814,
            "precision": 0.6066763425253991,
            "recall": 0.9028077753779697
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6294946165479064,
            "auditor_fn_violation": 0.008633472683746026,
            "auditor_fp_violation": 0.012238508980289216,
            "ave_precision_score": 0.621115380267623,
            "fpr": 0.24232456140350878,
            "logloss": 2.3432251072060772,
            "mae": 0.36194872019736896,
            "precision": 0.6475279106858054,
            "recall": 0.8268839103869654
        },
        "train": {
            "accuracy": 0.6575192096597146,
            "auc_prc": 0.609491423407161,
            "auditor_fn_violation": 0.006462885823140741,
            "auditor_fp_violation": 0.012888113533009268,
            "ave_precision_score": 0.6031687548796465,
            "fpr": 0.2502744237102086,
            "logloss": 2.5166666367696804,
            "mae": 0.36993745711256887,
            "precision": 0.6243822075782537,
            "recall": 0.8185745140388769
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8651367233004037,
            "auditor_fn_violation": 0.014129327902240326,
            "auditor_fp_violation": 0.021823040380047516,
            "ave_precision_score": 0.8653453770429342,
            "fpr": 0.23574561403508773,
            "logloss": 0.6255518663786863,
            "mae": 0.3275787648458949,
            "precision": 0.6766917293233082,
            "recall": 0.9164969450101833
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.8597588220903281,
            "auditor_fn_violation": 0.006405985874587773,
            "auditor_fp_violation": 0.03444017563117453,
            "ave_precision_score": 0.8602482347061843,
            "fpr": 0.26125137211855104,
            "logloss": 0.6121496471395188,
            "mae": 0.3276901714341218,
            "precision": 0.6442451420029895,
            "recall": 0.9308855291576674
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5800438596491229,
            "auc_prc": 0.5651692285711685,
            "auditor_fn_violation": 0.010379819201772252,
            "auditor_fp_violation": 0.006396632912447398,
            "ave_precision_score": 0.519137617347673,
            "fpr": 0.3826754385964912,
            "logloss": 5.891191075548667,
            "mae": 0.43199889881029857,
            "precision": 0.5669975186104218,
            "recall": 0.9307535641547862
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.5509022587860639,
            "auditor_fn_violation": 0.005602274101277166,
            "auditor_fp_violation": 0.015255018033558094,
            "ave_precision_score": 0.5058688046255146,
            "fpr": 0.3973655323819978,
            "logloss": 5.609986077258177,
            "mae": 0.4499847351258537,
            "precision": 0.5423514538558787,
            "recall": 0.9265658747300216
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8320422983291789,
            "auditor_fn_violation": 0.011384750062529033,
            "auditor_fp_violation": 0.009946555819477445,
            "ave_precision_score": 0.8322950226450868,
            "fpr": 0.23464912280701755,
            "logloss": 0.7976793259949719,
            "mae": 0.3200802410219535,
            "precision": 0.6722817764165391,
            "recall": 0.8940936863543788
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.8224836672631368,
            "auditor_fn_violation": 0.008203075916385527,
            "auditor_fp_violation": 0.02963776070252469,
            "ave_precision_score": 0.8228897552781109,
            "fpr": 0.24588364434687157,
            "logloss": 0.8938798486037187,
            "mae": 0.3365033119339312,
            "precision": 0.6461295418641391,
            "recall": 0.8833693304535637
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.675863100330068,
            "auditor_fn_violation": 0.008881355629399365,
            "auditor_fp_violation": 0.014467954327624311,
            "ave_precision_score": 0.6674372937473521,
            "fpr": 0.28728070175438597,
            "logloss": 2.2526873990615055,
            "mae": 0.34752376155168013,
            "precision": 0.6340782122905028,
            "recall": 0.924643584521385
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.6538762110066162,
            "auditor_fn_violation": 0.005317774358512352,
            "auditor_fp_violation": 0.021243335424180665,
            "ave_precision_score": 0.6475127766155228,
            "fpr": 0.2864983534577388,
            "logloss": 2.4534645065343823,
            "mae": 0.3642503612037875,
            "precision": 0.6161764705882353,
            "recall": 0.9049676025917927
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.6885361605250886,
            "auditor_fn_violation": 0.015567495622967808,
            "auditor_fp_violation": 0.020843751302246118,
            "ave_precision_score": 0.6810303224519028,
            "fpr": 0.15679824561403508,
            "logloss": 1.772867866453863,
            "mae": 0.3290935983961007,
            "precision": 0.7342007434944238,
            "recall": 0.8044806517311609
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.6738621365597637,
            "auditor_fn_violation": 0.006707081435680541,
            "auditor_fp_violation": 0.01828838011604203,
            "ave_precision_score": 0.6677496367265717,
            "fpr": 0.16465422612513722,
            "logloss": 1.8614422708490335,
            "mae": 0.323512021473657,
            "precision": 0.7175141242937854,
            "recall": 0.8228941684665226
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.6775770246381954,
            "auditor_fn_violation": 0.010299424732911714,
            "auditor_fp_violation": 0.01475444847272576,
            "ave_precision_score": 0.6691500412160722,
            "fpr": 0.2598684210526316,
            "logloss": 2.2028331694919516,
            "mae": 0.3415112965566662,
            "precision": 0.6504424778761062,
            "recall": 0.8981670061099797
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.6553780652649368,
            "auditor_fn_violation": 0.007522647364939676,
            "auditor_fp_violation": 0.02394836913909363,
            "ave_precision_score": 0.6490121450041744,
            "fpr": 0.2645444566410538,
            "logloss": 2.3993004660545076,
            "mae": 0.35491422763141334,
            "precision": 0.6298003072196621,
            "recall": 0.8855291576673866
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.6960686110085479,
            "auditor_fn_violation": 0.023988816236109628,
            "auditor_fp_violation": 0.021210984706421646,
            "ave_precision_score": 0.6876968143882234,
            "fpr": 0.22039473684210525,
            "logloss": 2.0734567625296534,
            "mae": 0.3292510376576294,
            "precision": 0.6804451510333863,
            "recall": 0.8716904276985743
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.6636247387729981,
            "auditor_fn_violation": 0.018333637590002682,
            "auditor_fp_violation": 0.03347969264544457,
            "ave_precision_score": 0.6573144919955054,
            "fpr": 0.24588364434687157,
            "logloss": 2.2951663381429985,
            "mae": 0.3478225718011911,
            "precision": 0.6387096774193548,
            "recall": 0.8552915766738661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.650790346616038,
            "auditor_fn_violation": 0.008519580519526926,
            "auditor_fp_violation": 0.0008829228653581779,
            "ave_precision_score": 0.6424007241074927,
            "fpr": 0.2807017543859649,
            "logloss": 2.1628375604923633,
            "mae": 0.36479516255693806,
            "precision": 0.6353276353276354,
            "recall": 0.9083503054989817
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.6467244369731737,
            "auditor_fn_violation": 0.007754988821530943,
            "auditor_fp_violation": 0.020826799435471235,
            "ave_precision_score": 0.6404936445976945,
            "fpr": 0.2854006586169045,
            "logloss": 2.19347380747529,
            "mae": 0.36823513875352515,
            "precision": 0.6148148148148148,
            "recall": 0.896328293736501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.5834508373040483,
            "auditor_fn_violation": 0.0027043806052810247,
            "auditor_fp_violation": 0.004815706129932917,
            "ave_precision_score": 0.5699188804851296,
            "fpr": 0.33771929824561403,
            "logloss": 3.3880368979500153,
            "mae": 0.3889752247885458,
            "precision": 0.5984354628422425,
            "recall": 0.9348268839103869
        },
        "train": {
            "accuracy": 0.605927552140505,
            "auc_prc": 0.5631338287062403,
            "auditor_fn_violation": 0.0032480387298983145,
            "auditor_fp_violation": 0.014723322095029011,
            "ave_precision_score": 0.5495552081547571,
            "fpr": 0.35236004390779363,
            "logloss": 3.4400079585535512,
            "mae": 0.40973654166109347,
            "precision": 0.5697050938337802,
            "recall": 0.91792656587473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.6777148999298117,
            "auditor_fn_violation": 0.009198467145460392,
            "auditor_fp_violation": 0.012441659374088424,
            "ave_precision_score": 0.6692898370846494,
            "fpr": 0.2949561403508772,
            "logloss": 2.3023345906321784,
            "mae": 0.3452886238657428,
            "precision": 0.6289655172413793,
            "recall": 0.9287169042769857
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.6579820910368266,
            "auditor_fn_violation": 0.004326766921214913,
            "auditor_fp_violation": 0.020709189273953283,
            "ave_precision_score": 0.651615093485077,
            "fpr": 0.3106476399560922,
            "logloss": 2.4992260361557164,
            "mae": 0.3639892818299904,
            "precision": 0.5980113636363636,
            "recall": 0.9092872570194385
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.6436677368838367,
            "auditor_fn_violation": 0.011663897523850358,
            "auditor_fp_violation": 0.009639225736550417,
            "ave_precision_score": 0.635281406495167,
            "fpr": 0.3125,
            "logloss": 2.1650699891899343,
            "mae": 0.384851011608956,
            "precision": 0.6095890410958904,
            "recall": 0.9063136456211812
        },
        "train": {
            "accuracy": 0.6333699231613611,
            "auc_prc": 0.639553244185779,
            "auditor_fn_violation": 0.004319654427645789,
            "auditor_fp_violation": 0.02338482044848675,
            "ave_precision_score": 0.6333557810577356,
            "fpr": 0.32491767288693746,
            "logloss": 2.200205124320041,
            "mae": 0.39355747391184553,
            "precision": 0.5894590846047156,
            "recall": 0.91792656587473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.7936994115036498,
            "auditor_fn_violation": 0.018053024618572912,
            "auditor_fp_violation": 0.02240123765470685,
            "ave_precision_score": 0.7940703295510663,
            "fpr": 0.3059210526315789,
            "logloss": 1.1249888971963444,
            "mae": 0.3495932901046297,
            "precision": 0.6219512195121951,
            "recall": 0.9348268839103869
        },
        "train": {
            "accuracy": 0.6311745334796927,
            "auc_prc": 0.781724751634678,
            "auditor_fn_violation": 0.013848024979077417,
            "auditor_fp_violation": 0.04454239846322725,
            "ave_precision_score": 0.7820544504301428,
            "fpr": 0.3194291986827662,
            "logloss": 1.2488624907502188,
            "mae": 0.36990680849094104,
            "precision": 0.5895627644569816,
            "recall": 0.9028077753779697
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.6414070897489217,
            "auditor_fn_violation": 0.004321202701254153,
            "auditor_fp_violation": 0.008391673959244911,
            "ave_precision_score": 0.6290330880761781,
            "fpr": 0.4144736842105263,
            "logloss": 3.8582658644833945,
            "mae": 0.4275904130966713,
            "precision": 0.5604651162790698,
            "recall": 0.9816700610997964
        },
        "train": {
            "accuracy": 0.5455543358946213,
            "auc_prc": 0.6282232152621375,
            "auditor_fn_violation": 0.0017378192620550843,
            "auditor_fp_violation": 0.007879880821702994,
            "ave_precision_score": 0.6187036312716603,
            "fpr": 0.44127332601536773,
            "logloss": 3.908786448692097,
            "mae": 0.45031364377829297,
            "precision": 0.5287221570926143,
            "recall": 0.9740820734341252
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.6901448396821157,
            "auditor_fn_violation": 0.0053998284917997665,
            "auditor_fp_violation": 0.0032764512230695525,
            "ave_precision_score": 0.6925695359279935,
            "fpr": 0.30153508771929827,
            "logloss": 1.4806403822109704,
            "mae": 0.36685596112357943,
            "precision": 0.6201657458563536,
            "recall": 0.9144602851323829
        },
        "train": {
            "accuracy": 0.6377607025246982,
            "auc_prc": 0.684918247909003,
            "auditor_fn_violation": 0.0027454225176804744,
            "auditor_fp_violation": 0.01659283362082485,
            "ave_precision_score": 0.6856544257876243,
            "fpr": 0.31174533479692645,
            "logloss": 1.5823917448749372,
            "mae": 0.3843474283847144,
            "precision": 0.5948644793152639,
            "recall": 0.9006479481641468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.6739228788840267,
            "auditor_fn_violation": 0.008666970379104586,
            "auditor_fp_violation": 0.011475392757428019,
            "ave_precision_score": 0.6200124214153901,
            "fpr": 0.16447368421052633,
            "logloss": 5.790165601988593,
            "mae": 0.32492236404990155,
            "precision": 0.7191011235955056,
            "recall": 0.7820773930753564
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.6326056727596172,
            "auditor_fn_violation": 0.006892006268477667,
            "auditor_fp_violation": 0.01333405206209817,
            "ave_precision_score": 0.5848537428896681,
            "fpr": 0.16355653128430298,
            "logloss": 6.354160598728032,
            "mae": 0.3389434182043182,
            "precision": 0.703187250996016,
            "recall": 0.7624190064794817
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.6700055620826969,
            "auditor_fn_violation": 0.022050416264694325,
            "auditor_fp_violation": 0.015731133058298957,
            "ave_precision_score": 0.6615911593991872,
            "fpr": 0.15460526315789475,
            "logloss": 2.1880815475476534,
            "mae": 0.3489770948013883,
            "precision": 0.718562874251497,
            "recall": 0.7331975560081466
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.6530759889454659,
            "auditor_fn_violation": 0.012902063334384406,
            "auditor_fp_violation": 0.005488474204171249,
            "ave_precision_score": 0.6468372807003143,
            "fpr": 0.14709110867178923,
            "logloss": 2.180088070980512,
            "mae": 0.3413586000663086,
            "precision": 0.7184873949579832,
            "recall": 0.7386609071274298
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.6509772476166736,
            "auditor_fn_violation": 0.008519580519526926,
            "auditor_fp_violation": 0.00041150977205484205,
            "ave_precision_score": 0.64258339094434,
            "fpr": 0.27850877192982454,
            "logloss": 2.158602277437202,
            "mae": 0.3640733676147647,
            "precision": 0.6371428571428571,
            "recall": 0.9083503054989817
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.6468202356886951,
            "auditor_fn_violation": 0.007465747416386712,
            "auditor_fp_violation": 0.02121883330719774,
            "ave_precision_score": 0.6405897847521493,
            "fpr": 0.283205268935236,
            "logloss": 2.189686584030792,
            "mae": 0.36748567580922203,
            "precision": 0.6154992548435171,
            "recall": 0.8920086393088553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5285087719298246,
            "auc_prc": 0.531564433152753,
            "auditor_fn_violation": 0.007177439525493981,
            "auditor_fp_violation": 0.009238133933408346,
            "ave_precision_score": 0.5325072816135522,
            "fpr": 0.15679824561403508,
            "logloss": 10.81350603942108,
            "mae": 0.4941924104822515,
            "precision": 0.5878962536023055,
            "recall": 0.4154786150712831
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.54111946430876,
            "auditor_fn_violation": 0.005211086954975551,
            "auditor_fp_violation": 0.01462776383879568,
            "ave_precision_score": 0.5357465557032344,
            "fpr": 0.14709110867178923,
            "logloss": 9.73565652600781,
            "mae": 0.45634042595752855,
            "precision": 0.6,
            "recall": 0.43412526997840173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7839912280701754,
            "auc_prc": 0.8800359478278968,
            "auditor_fn_violation": 0.012159663415157038,
            "auditor_fp_violation": 0.009899674959369924,
            "ave_precision_score": 0.8802057260268272,
            "fpr": 0.08662280701754387,
            "logloss": 0.4712001158018914,
            "mae": 0.2966935511161737,
            "precision": 0.8252212389380531,
            "recall": 0.7596741344195519
        },
        "train": {
            "accuracy": 0.7903402854006586,
            "auc_prc": 0.8578599749663532,
            "auditor_fn_violation": 0.009739374527315534,
            "auditor_fp_violation": 0.008850164654226125,
            "ave_precision_score": 0.8582391244158114,
            "fpr": 0.09220636663007684,
            "logloss": 0.47623795743753855,
            "mae": 0.2940359818355541,
            "precision": 0.8090909090909091,
            "recall": 0.7688984881209503
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.6798302653397628,
            "auditor_fn_violation": 0.012711258798727983,
            "auditor_fp_violation": 0.020726549151977337,
            "ave_precision_score": 0.6708255076985752,
            "fpr": 0.2675438596491228,
            "logloss": 2.1606032253549685,
            "mae": 0.36041972423870966,
            "precision": 0.6406480117820325,
            "recall": 0.8859470468431772
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.6527987951485645,
            "auditor_fn_violation": 0.0055714532958109795,
            "auditor_fp_violation": 0.017425905598243696,
            "ave_precision_score": 0.6457101673013637,
            "fpr": 0.30735455543358947,
            "logloss": 2.3669444040795056,
            "mae": 0.37553794300909493,
            "precision": 0.5988538681948424,
            "recall": 0.9028077753779697
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7817982456140351,
            "auc_prc": 0.8871158262656784,
            "auditor_fn_violation": 0.024089309322185306,
            "auditor_fp_violation": 0.009894465974913532,
            "ave_precision_score": 0.8871778287584378,
            "fpr": 0.06907894736842106,
            "logloss": 0.6474567919040932,
            "mae": 0.2802212748019405,
            "precision": 0.8492822966507177,
            "recall": 0.7230142566191446
        },
        "train": {
            "accuracy": 0.7848518111964874,
            "auc_prc": 0.8458533225970574,
            "auditor_fn_violation": 0.012050934937279666,
            "auditor_fp_violation": 0.013632977889289638,
            "ave_precision_score": 0.8465915819480534,
            "fpr": 0.07464324917672886,
            "logloss": 0.729637539749352,
            "mae": 0.2824408657162355,
            "precision": 0.8312655086848635,
            "recall": 0.7235421166306696
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 14724,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.7256108842947899,
            "auditor_fn_violation": 0.017456765641190558,
            "auditor_fp_violation": 0.022859628286869202,
            "ave_precision_score": 0.7195648903219678,
            "fpr": 0.16666666666666666,
            "logloss": 1.6488625997229254,
            "mae": 0.31344867482364763,
            "precision": 0.7290552584670231,
            "recall": 0.8329938900203666
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.7007677417660082,
            "auditor_fn_violation": 0.006486594135037807,
            "auditor_fp_violation": 0.012055041555590408,
            "ave_precision_score": 0.6948742155936773,
            "fpr": 0.19099890230515917,
            "logloss": 1.8148372857725914,
            "mae": 0.3211091014941997,
            "precision": 0.6892857142857143,
            "recall": 0.8336933045356372
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.6810008576579358,
            "auditor_fn_violation": 0.013642494729695934,
            "auditor_fp_violation": 0.006266408301037633,
            "ave_precision_score": 0.6699076279543255,
            "fpr": 0.23574561403508773,
            "logloss": 2.2024960244624094,
            "mae": 0.3347764180405392,
            "precision": 0.6727549467275494,
            "recall": 0.90020366598778
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.6699648093494959,
            "auditor_fn_violation": 0.007299789233107235,
            "auditor_fp_violation": 0.02362984161831583,
            "ave_precision_score": 0.6592418498793484,
            "fpr": 0.2491767288693743,
            "logloss": 2.3890119141735457,
            "mae": 0.34456446778252064,
            "precision": 0.6402535657686212,
            "recall": 0.8725701943844493
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8145125417537368,
            "auditor_fn_violation": 0.016601457819702004,
            "auditor_fp_violation": 0.01672604908946952,
            "ave_precision_score": 0.8148769966094531,
            "fpr": 0.20614035087719298,
            "logloss": 0.6236638809660655,
            "mae": 0.3323631757849127,
            "precision": 0.6957928802588996,
            "recall": 0.8757637474541752
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.8199042308403621,
            "auditor_fn_violation": 0.018345491745951216,
            "auditor_fp_violation": 0.032460404578955625,
            "ave_precision_score": 0.8206809407907503,
            "fpr": 0.2217343578485181,
            "logloss": 0.6130181520875553,
            "mae": 0.337297493154229,
            "precision": 0.6638935108153078,
            "recall": 0.8617710583153347
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.7055510398360094,
            "auditor_fn_violation": 0.015397773966484441,
            "auditor_fp_violation": 0.021903779639121553,
            "ave_precision_score": 0.6971020864785018,
            "fpr": 0.1524122807017544,
            "logloss": 2.023277127522489,
            "mae": 0.30643817569331366,
            "precision": 0.7444852941176471,
            "recall": 0.824847250509165
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.6743918450153967,
            "auditor_fn_violation": 0.0109461276028763,
            "auditor_fp_violation": 0.018557903402854015,
            "ave_precision_score": 0.6680081926320275,
            "fpr": 0.1690450054884742,
            "logloss": 2.254832154706563,
            "mae": 0.31810745068874163,
            "precision": 0.7110694183864915,
            "recall": 0.8185745140388769
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.6521530533788124,
            "auditor_fn_violation": 0.009533444099045985,
            "auditor_fp_violation": 0.009821540192524075,
            "ave_precision_score": 0.6437660528103746,
            "fpr": 0.2949561403508772,
            "logloss": 2.178532805445546,
            "mae": 0.36592868515295424,
            "precision": 0.6253481894150418,
            "recall": 0.9144602851323829
        },
        "train": {
            "accuracy": 0.6586169045005489,
            "auc_prc": 0.6456355640619773,
            "auditor_fn_violation": 0.009502291408344854,
            "auditor_fp_violation": 0.02343137447075429,
            "ave_precision_score": 0.6394060469311971,
            "fpr": 0.28869374313940727,
            "logloss": 2.2166146544283274,
            "mae": 0.37069452574510336,
            "precision": 0.612094395280236,
            "recall": 0.896328293736501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.6748741269211062,
            "auditor_fn_violation": 0.009698699396148215,
            "auditor_fp_violation": 0.014691940659249078,
            "ave_precision_score": 0.6664496127371253,
            "fpr": 0.26206140350877194,
            "logloss": 2.220118029064106,
            "mae": 0.3422167497164194,
            "precision": 0.6490455212922174,
            "recall": 0.90020366598778
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6538635237660098,
            "auditor_fn_violation": 0.006311152626999499,
            "auditor_fp_violation": 0.026464736553238208,
            "ave_precision_score": 0.6474974875910096,
            "fpr": 0.2667398463227223,
            "logloss": 2.4131126786386803,
            "mae": 0.3567928522702564,
            "precision": 0.6301369863013698,
            "recall": 0.8941684665226782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.6809742871687714,
            "auditor_fn_violation": 0.011398149140672456,
            "auditor_fp_violation": 0.011446743342917873,
            "ave_precision_score": 0.6725453253389446,
            "fpr": 0.23684210526315788,
            "logloss": 2.1655946798826444,
            "mae": 0.3326445964947261,
            "precision": 0.6707317073170732,
            "recall": 0.8961303462321792
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.6603989379523405,
            "auditor_fn_violation": 0.0054173492684800395,
            "auditor_fp_violation": 0.020236298416183164,
            "ave_precision_score": 0.6540285414533676,
            "fpr": 0.2579582875960483,
            "logloss": 2.3585365238956686,
            "mae": 0.3469989927215696,
            "precision": 0.6367851622874807,
            "recall": 0.8898488120950324
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.6554129873787524,
            "auditor_fn_violation": 0.009689766677385931,
            "auditor_fp_violation": 0.0013543359586615095,
            "ave_precision_score": 0.647022757814,
            "fpr": 0.27850877192982454,
            "logloss": 2.125435686318669,
            "mae": 0.3639436632460147,
            "precision": 0.6340057636887608,
            "recall": 0.8961303462321792
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.6503839506882341,
            "auditor_fn_violation": 0.007494197390663191,
            "auditor_fp_violation": 0.01839618943076684,
            "ave_precision_score": 0.6441493210591926,
            "fpr": 0.27771679473106475,
            "logloss": 2.162338678165992,
            "mae": 0.36525361394431977,
            "precision": 0.6189759036144579,
            "recall": 0.8876889848812095
        }
    }
]
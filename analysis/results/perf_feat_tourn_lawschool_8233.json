[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.796511956059503,
            "auditor_fn_violation": 0.02703847687400321,
            "auditor_fp_violation": 0.005203209542547959,
            "ave_precision_score": 0.7970131164377421,
            "fpr": 0.01864035087719298,
            "logloss": 0.9466491947666376,
            "mae": 0.3994304808436522,
            "precision": 0.89375,
            "recall": 0.29545454545454547
        },
        "train": {
            "accuracy": 0.5850713501646543,
            "auc_prc": 0.776989754375423,
            "auditor_fn_violation": 0.03540649741924937,
            "auditor_fp_violation": 0.006917219870018993,
            "ave_precision_score": 0.7776290929581564,
            "fpr": 0.030735455543358946,
            "logloss": 0.942521322574065,
            "mae": 0.3980735172126565,
            "precision": 0.8108108108108109,
            "recall": 0.2553191489361702
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6118421052631579,
            "auc_prc": 0.799583012504328,
            "auditor_fn_violation": 0.02576754385964914,
            "auditor_fp_violation": 0.005738645679619609,
            "ave_precision_score": 0.8001167902965572,
            "fpr": 0.020833333333333332,
            "logloss": 0.903325553998663,
            "mae": 0.3971254539797532,
            "precision": 0.8869047619047619,
            "recall": 0.30785123966942146
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.7779171331515552,
            "auditor_fn_violation": 0.03377163276268773,
            "auditor_fp_violation": 0.006429355496314882,
            "ave_precision_score": 0.7784738517550229,
            "fpr": 0.029637760702524697,
            "logloss": 0.9036671700911143,
            "mae": 0.39659831984468014,
            "precision": 0.8223684210526315,
            "recall": 0.26595744680851063
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6337719298245614,
            "auc_prc": 0.802480666939337,
            "auditor_fn_violation": 0.018377555458895176,
            "auditor_fp_violation": 0.005072552877520907,
            "ave_precision_score": 0.8029336118767983,
            "fpr": 0.021929824561403508,
            "logloss": 0.8207821943945841,
            "mae": 0.39049835235448227,
            "precision": 0.8947368421052632,
            "recall": 0.3512396694214876
        },
        "train": {
            "accuracy": 0.6092206366630076,
            "auc_prc": 0.7894640533493348,
            "auditor_fn_violation": 0.031032066702477993,
            "auditor_fp_violation": 0.007056609691077309,
            "ave_precision_score": 0.7900748419534755,
            "fpr": 0.036223929747530186,
            "logloss": 0.7980212258668004,
            "mae": 0.38610962726051573,
            "precision": 0.8166666666666667,
            "recall": 0.3127659574468085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6129385964912281,
            "auc_prc": 0.7989568332786728,
            "auditor_fn_violation": 0.03050012686675368,
            "auditor_fp_violation": 0.005344113789145762,
            "ave_precision_score": 0.799399745846815,
            "fpr": 0.019736842105263157,
            "logloss": 0.9039538955896426,
            "mae": 0.39999920332599703,
            "precision": 0.8922155688622755,
            "recall": 0.30785123966942146
        },
        "train": {
            "accuracy": 0.5938529088913282,
            "auc_prc": 0.7757597435831833,
            "auditor_fn_violation": 0.03746175584464115,
            "auditor_fp_violation": 0.005784677573920165,
            "ave_precision_score": 0.7763880431870578,
            "fpr": 0.02854006586169045,
            "logloss": 0.8973752695041947,
            "mae": 0.39888732241667013,
            "precision": 0.8289473684210527,
            "recall": 0.2680851063829787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5997807017543859,
            "auc_prc": 0.7975218447829554,
            "auditor_fn_violation": 0.03234649122807017,
            "auditor_fp_violation": 0.005702779144121987,
            "ave_precision_score": 0.7980149251655688,
            "fpr": 0.019736842105263157,
            "logloss": 0.9466888696482099,
            "mae": 0.3997899690917748,
            "precision": 0.8838709677419355,
            "recall": 0.2830578512396694
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.7768554611098928,
            "auditor_fn_violation": 0.03867622673237266,
            "auditor_fp_violation": 0.0043459754922825335,
            "ave_precision_score": 0.7774802790305055,
            "fpr": 0.029637760702524697,
            "logloss": 0.9460823981939704,
            "mae": 0.3991557245092154,
            "precision": 0.8163265306122449,
            "recall": 0.2553191489361702
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8257446307142655,
            "auditor_fn_violation": 0.012847524285921414,
            "auditor_fp_violation": 0.00907423348089851,
            "ave_precision_score": 0.826308558180812,
            "fpr": 0.046052631578947366,
            "logloss": 0.6454740864334944,
            "mae": 0.34344099721439353,
            "precision": 0.864951768488746,
            "recall": 0.5557851239669421
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.8064884241610144,
            "auditor_fn_violation": 0.011584183852208245,
            "auditor_fp_violation": 0.008677016360880247,
            "ave_precision_score": 0.8069229567112284,
            "fpr": 0.05598243688254665,
            "logloss": 0.642981776654787,
            "mae": 0.34224274800885895,
            "precision": 0.8349514563106796,
            "recall": 0.548936170212766
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.7955960058693149,
            "auditor_fn_violation": 0.014752791068580552,
            "auditor_fp_violation": 0.0019470404984423693,
            "ave_precision_score": 0.7958485295005022,
            "fpr": 0.03508771929824561,
            "logloss": 0.7089989691779868,
            "mae": 0.3937520253719157,
            "precision": 0.8545454545454545,
            "recall": 0.3884297520661157
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.7424896056442716,
            "auditor_fn_violation": 0.019679099423126355,
            "auditor_fp_violation": 0.0037884162080492664,
            "ave_precision_score": 0.7442229303081173,
            "fpr": 0.04720087815587267,
            "logloss": 0.7400942799369368,
            "mae": 0.40589412160649174,
            "precision": 0.7942583732057417,
            "recall": 0.35319148936170214
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 8233,
        "test": {
            "accuracy": 0.581140350877193,
            "auc_prc": 0.7459356925764665,
            "auditor_fn_violation": 0.0165719696969697,
            "auditor_fp_violation": 0.004808677652074111,
            "ave_precision_score": 0.7465021781813536,
            "fpr": 0.044956140350877194,
            "logloss": 0.9161227101679623,
            "mae": 0.4144337659279444,
            "precision": 0.7771739130434783,
            "recall": 0.29545454545454547
        },
        "train": {
            "accuracy": 0.566410537870472,
            "auc_prc": 0.713377292229418,
            "auditor_fn_violation": 0.03604643015624637,
            "auditor_fp_violation": 0.004268813269910968,
            "ave_precision_score": 0.7152917518147406,
            "fpr": 0.04720087815587267,
            "logloss": 0.9085162519431317,
            "mae": 0.41176616689512296,
            "precision": 0.7329192546583851,
            "recall": 0.251063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5986842105263158,
            "auc_prc": 0.8221417509917511,
            "auditor_fn_violation": 0.02971853704509208,
            "auditor_fp_violation": 0.004385964912280702,
            "ave_precision_score": 0.8225624385058667,
            "fpr": 0.013157894736842105,
            "logloss": 0.9459539674019737,
            "mae": 0.3996435044541852,
            "precision": 0.9154929577464789,
            "recall": 0.26859504132231404
        },
        "train": {
            "accuracy": 0.575192096597146,
            "auc_prc": 0.7896811959048547,
            "auditor_fn_violation": 0.036656001121050066,
            "auditor_fp_violation": 0.004253878646226145,
            "ave_precision_score": 0.7903697613111068,
            "fpr": 0.024149286498353458,
            "logloss": 0.9499006498358902,
            "mae": 0.40168867169206696,
            "precision": 0.8267716535433071,
            "recall": 0.22340425531914893
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.8105900897526944,
            "auditor_fn_violation": 0.01681437581557202,
            "auditor_fp_violation": 0.005923102147893097,
            "ave_precision_score": 0.811109257141108,
            "fpr": 0.023026315789473683,
            "logloss": 0.8368447032696181,
            "mae": 0.3843694481766809,
            "precision": 0.8939393939393939,
            "recall": 0.365702479338843
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.7856427349387315,
            "auditor_fn_violation": 0.02732092393208308,
            "auditor_fp_violation": 0.003905404093580354,
            "ave_precision_score": 0.7862416233558306,
            "fpr": 0.03293084522502744,
            "logloss": 0.8336847008491574,
            "mae": 0.3845122212063088,
            "precision": 0.8387096774193549,
            "recall": 0.33191489361702126
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6326754385964912,
            "auc_prc": 0.8019362518620838,
            "auditor_fn_violation": 0.017901805132666367,
            "auditor_fp_violation": 0.005072552877520907,
            "ave_precision_score": 0.8023817419846555,
            "fpr": 0.021929824561403508,
            "logloss": 0.8187642402312463,
            "mae": 0.3909477774659376,
            "precision": 0.8941798941798942,
            "recall": 0.34917355371900827
        },
        "train": {
            "accuracy": 0.610318331503842,
            "auc_prc": 0.7862606920991664,
            "auditor_fn_violation": 0.030289371044211418,
            "auditor_fp_violation": 0.008238934066125537,
            "ave_precision_score": 0.7868769954557013,
            "fpr": 0.03732162458836443,
            "logloss": 0.7971894759822027,
            "mae": 0.38721464040027737,
            "precision": 0.8142076502732241,
            "recall": 0.3170212765957447
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8646525737465126,
            "auditor_fn_violation": 0.015844751341162828,
            "auditor_fp_violation": 0.01091111247745532,
            "ave_precision_score": 0.8649467007625797,
            "fpr": 0.06469298245614036,
            "logloss": 0.5485936847671057,
            "mae": 0.2825447299135767,
            "precision": 0.8455497382198953,
            "recall": 0.6673553719008265
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.868938018247267,
            "auditor_fn_violation": 0.014265361888969342,
            "auditor_fp_violation": 0.012881112928156994,
            "ave_precision_score": 0.8692351406238306,
            "fpr": 0.06256860592755215,
            "logloss": 0.5284892203536767,
            "mae": 0.27651402595338975,
            "precision": 0.848,
            "recall": 0.676595744680851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.8200368259662764,
            "auditor_fn_violation": 0.011300202986805872,
            "auditor_fp_violation": 0.010083620265617316,
            "ave_precision_score": 0.8204025900164276,
            "fpr": 0.03508771929824561,
            "logloss": 0.7065268444102376,
            "mae": 0.3568028838561619,
            "precision": 0.8787878787878788,
            "recall": 0.4793388429752066
        },
        "train": {
            "accuracy": 0.6630076838638859,
            "auc_prc": 0.7819475106524888,
            "auditor_fn_violation": 0.015190228180395646,
            "auditor_fp_violation": 0.00954571363854726,
            "ave_precision_score": 0.7823644129610771,
            "fpr": 0.050493962678375415,
            "logloss": 0.7345919160706448,
            "mae": 0.36798158394072855,
            "precision": 0.8196078431372549,
            "recall": 0.44468085106382976
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5899122807017544,
            "auc_prc": 0.8095601256816001,
            "auditor_fn_violation": 0.025801526025808316,
            "auditor_fp_violation": 0.0016677939006394496,
            "ave_precision_score": 0.8099541954811544,
            "fpr": 0.009868421052631578,
            "logloss": 0.9872232341891161,
            "mae": 0.4089838896543689,
            "precision": 0.9296875,
            "recall": 0.24586776859504134
        },
        "train": {
            "accuracy": 0.5543358946212953,
            "auc_prc": 0.7814887697538437,
            "auditor_fn_violation": 0.03777938669220173,
            "auditor_fp_violation": 0.004696939148875798,
            "ave_precision_score": 0.782206180616216,
            "fpr": 0.026344676180021953,
            "logloss": 0.9864971360678141,
            "mae": 0.4098865853507035,
            "precision": 0.7857142857142857,
            "recall": 0.18723404255319148
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6118421052631579,
            "auc_prc": 0.786195825105858,
            "auditor_fn_violation": 0.016248006379585338,
            "auditor_fp_violation": 0.005236514182652895,
            "ave_precision_score": 0.7870302891940102,
            "fpr": 0.02631578947368421,
            "logloss": 0.9299224733343128,
            "mae": 0.40165951328539484,
            "precision": 0.8651685393258427,
            "recall": 0.3181818181818182
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.7685356591643593,
            "auditor_fn_violation": 0.027664245509961004,
            "auditor_fp_violation": 0.0057498301186555845,
            "ave_precision_score": 0.7695708876695135,
            "fpr": 0.036223929747530186,
            "logloss": 0.9109801120254506,
            "mae": 0.3952588337705124,
            "precision": 0.8206521739130435,
            "recall": 0.32127659574468087
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 8233,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.8406775201999787,
            "auditor_fn_violation": 0.010901478903871255,
            "auditor_fp_violation": 0.006514899983603871,
            "ave_precision_score": 0.8410677105182182,
            "fpr": 0.025219298245614034,
            "logloss": 0.7642282903872867,
            "mae": 0.3585060347539088,
            "precision": 0.9083665338645418,
            "recall": 0.47107438016528924
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.8419184030122997,
            "auditor_fn_violation": 0.008639091949459336,
            "auditor_fp_violation": 0.003392648680401543,
            "ave_precision_score": 0.8423212424807061,
            "fpr": 0.025246981339187707,
            "logloss": 0.7390150706060848,
            "mae": 0.3519322369187614,
            "precision": 0.902542372881356,
            "recall": 0.4531914893617021
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.8127340941328647,
            "auditor_fn_violation": 0.013606459330143542,
            "auditor_fp_violation": 0.004918839153959666,
            "ave_precision_score": 0.8141639747544777,
            "fpr": 0.04824561403508772,
            "logloss": 0.6820168037844656,
            "mae": 0.3515411976767242,
            "precision": 0.8543046357615894,
            "recall": 0.5330578512396694
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7947854158396709,
            "auditor_fn_violation": 0.0135763832122755,
            "auditor_fp_violation": 0.007310498293719245,
            "ave_precision_score": 0.7954417621149942,
            "fpr": 0.05159165751920966,
            "logloss": 0.6820379524198127,
            "mae": 0.35088255399154167,
            "precision": 0.8401360544217688,
            "recall": 0.5255319148936171
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6008771929824561,
            "auc_prc": 0.8116172908548925,
            "auditor_fn_violation": 0.03255491518051327,
            "auditor_fp_violation": 0.0037045007378258736,
            "ave_precision_score": 0.8120313116385152,
            "fpr": 0.01425438596491228,
            "logloss": 0.9430674379893081,
            "mae": 0.4012541500490586,
            "precision": 0.910958904109589,
            "recall": 0.27479338842975204
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.7844647314287307,
            "auditor_fn_violation": 0.037295933858047046,
            "auditor_fp_violation": 0.0038506438067360134,
            "ave_precision_score": 0.7851698547863959,
            "fpr": 0.027442371020856202,
            "logloss": 0.9405953782732184,
            "mae": 0.40278316450870877,
            "precision": 0.8046875,
            "recall": 0.21914893617021278
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5997807017543859,
            "auc_prc": 0.7950003056658383,
            "auditor_fn_violation": 0.02434029288096274,
            "auditor_fp_violation": 0.005067429086735531,
            "ave_precision_score": 0.7955137103823935,
            "fpr": 0.019736842105263157,
            "logloss": 0.9631085899773552,
            "mae": 0.4028214728144265,
            "precision": 0.8838709677419355,
            "recall": 0.2830578512396694
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.7774558538007468,
            "auditor_fn_violation": 0.03446995352313334,
            "auditor_fp_violation": 0.0058021013015524545,
            "ave_precision_score": 0.7781738381670475,
            "fpr": 0.029637760702524697,
            "logloss": 0.9569725293481862,
            "mae": 0.4005716017175989,
            "precision": 0.8125,
            "recall": 0.24893617021276596
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.8243030788635947,
            "auditor_fn_violation": 0.01077914310569813,
            "auditor_fp_violation": 0.010024696671585506,
            "ave_precision_score": 0.8247598079508848,
            "fpr": 0.04057017543859649,
            "logloss": 0.7047271494222938,
            "mae": 0.34929173880689696,
            "precision": 0.8715277777777778,
            "recall": 0.518595041322314
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.8027918131873307,
            "auditor_fn_violation": 0.012041945956045496,
            "auditor_fp_violation": 0.013782168557141115,
            "ave_precision_score": 0.8032235309960256,
            "fpr": 0.05378704720087816,
            "logloss": 0.7123373576833978,
            "mae": 0.3500299015965182,
            "precision": 0.8338983050847457,
            "recall": 0.5234042553191489
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.8176795701169537,
            "auditor_fn_violation": 0.005332934609250416,
            "auditor_fp_violation": 0.004145146745368093,
            "ave_precision_score": 0.8180402833972086,
            "fpr": 0.01644736842105263,
            "logloss": 0.8571053733918461,
            "mae": 0.39217959693301135,
            "precision": 0.9112426035502958,
            "recall": 0.3181818181818182
        },
        "train": {
            "accuracy": 0.5938529088913282,
            "auc_prc": 0.7766603871649891,
            "auditor_fn_violation": 0.024319779526823456,
            "auditor_fp_violation": 0.0031835639488140676,
            "ave_precision_score": 0.7775535224681024,
            "fpr": 0.025246981339187707,
            "logloss": 0.8844885636284455,
            "mae": 0.4019298167236923,
            "precision": 0.8424657534246576,
            "recall": 0.26170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5953947368421053,
            "auc_prc": 0.803725394565463,
            "auditor_fn_violation": 0.016483616064955804,
            "auditor_fp_violation": 0.003886395310706674,
            "ave_precision_score": 0.804178636505959,
            "fpr": 0.020833333333333332,
            "logloss": 0.9251379886671182,
            "mae": 0.3991959178511024,
            "precision": 0.8758169934640523,
            "recall": 0.2768595041322314
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.7754772268010652,
            "auditor_fn_violation": 0.03959175094004718,
            "auditor_fp_violation": 0.0010454236579373786,
            "ave_precision_score": 0.7763923521643572,
            "fpr": 0.030735455543358946,
            "logloss": 0.9284151357384238,
            "mae": 0.40137206497733785,
            "precision": 0.8082191780821918,
            "recall": 0.251063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.7962652974943347,
            "auditor_fn_violation": 0.02703847687400321,
            "auditor_fp_violation": 0.005203209542547959,
            "ave_precision_score": 0.7967671479020174,
            "fpr": 0.01864035087719298,
            "logloss": 0.9478062758436295,
            "mae": 0.39964890254469265,
            "precision": 0.89375,
            "recall": 0.29545454545454547
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.7766900447658605,
            "auditor_fn_violation": 0.03774201835719458,
            "auditor_fp_violation": 0.006917219870018993,
            "ave_precision_score": 0.7773299207045875,
            "fpr": 0.030735455543358946,
            "logloss": 0.9435357366055629,
            "mae": 0.3982637962389024,
            "precision": 0.8120805369127517,
            "recall": 0.2574468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6041666666666666,
            "auc_prc": 0.7819166825455206,
            "auditor_fn_violation": 0.02654686820356679,
            "auditor_fp_violation": 0.008105837022462701,
            "ave_precision_score": 0.7824489086575785,
            "fpr": 0.02631578947368421,
            "logloss": 0.8613999883133721,
            "mae": 0.4046937371267153,
            "precision": 0.8596491228070176,
            "recall": 0.3037190082644628
        },
        "train": {
            "accuracy": 0.6026344676180022,
            "auc_prc": 0.7704949860707349,
            "auditor_fn_violation": 0.03264124062872224,
            "auditor_fp_violation": 0.007527050337149131,
            "ave_precision_score": 0.7711975791101788,
            "fpr": 0.03951701427003293,
            "logloss": 0.8251618545902455,
            "mae": 0.39650356229936495,
            "precision": 0.8,
            "recall": 0.30638297872340425
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.836047638949502,
            "auditor_fn_violation": 0.011252627954182982,
            "auditor_fp_violation": 0.010539637645515661,
            "ave_precision_score": 0.8363970867670976,
            "fpr": 0.049342105263157895,
            "logloss": 0.6025844606730683,
            "mae": 0.34925086192200533,
            "precision": 0.8575949367088608,
            "recall": 0.5599173553719008
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7999886334962703,
            "auditor_fn_violation": 0.006254525071817279,
            "auditor_fp_violation": 0.00796762173585131,
            "ave_precision_score": 0.8003973765914396,
            "fpr": 0.05159165751920966,
            "logloss": 0.6277294215056242,
            "mae": 0.3618289275538548,
            "precision": 0.8401360544217688,
            "recall": 0.5255319148936171
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6052631578947368,
            "auc_prc": 0.7929055066027728,
            "auditor_fn_violation": 0.023259659997100203,
            "auditor_fp_violation": 0.005223704705689457,
            "ave_precision_score": 0.7945759441539709,
            "fpr": 0.023026315789473683,
            "logloss": 0.8824564112931249,
            "mae": 0.3999715756995819,
            "precision": 0.8734939759036144,
            "recall": 0.29958677685950413
        },
        "train": {
            "accuracy": 0.6048298572996706,
            "auc_prc": 0.7748791484366324,
            "auditor_fn_violation": 0.03127496088002431,
            "auditor_fp_violation": 0.00599625140945511,
            "ave_precision_score": 0.7780132754866422,
            "fpr": 0.036223929747530186,
            "logloss": 0.8596559939164814,
            "mae": 0.3928193142037005,
            "precision": 0.8125,
            "recall": 0.30425531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5942982456140351,
            "auc_prc": 0.8041715753514692,
            "auditor_fn_violation": 0.017999220675656107,
            "auditor_fp_violation": 0.0025747048696507626,
            "ave_precision_score": 0.8045549059482358,
            "fpr": 0.01425438596491228,
            "logloss": 0.9512478504680989,
            "mae": 0.40648228515375084,
            "precision": 0.9071428571428571,
            "recall": 0.26239669421487605
        },
        "train": {
            "accuracy": 0.5609220636663008,
            "auc_prc": 0.7654851869731232,
            "auditor_fn_violation": 0.03545087231707033,
            "auditor_fp_violation": 0.006608570980532718,
            "ave_precision_score": 0.7663902190328039,
            "fpr": 0.029637760702524697,
            "logloss": 0.9756111069168385,
            "mae": 0.413411441892342,
            "precision": 0.782258064516129,
            "recall": 0.20638297872340425
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6008771929824561,
            "auc_prc": 0.801701842317613,
            "auditor_fn_violation": 0.023946099753516022,
            "auditor_fp_violation": 0.005702779144121987,
            "ave_precision_score": 0.8021244078211722,
            "fpr": 0.019736842105263157,
            "logloss": 0.9080389238979099,
            "mae": 0.40416166720116736,
            "precision": 0.8846153846153846,
            "recall": 0.28512396694214875
        },
        "train": {
            "accuracy": 0.5894621295279913,
            "auc_prc": 0.7733522000516946,
            "auditor_fn_violation": 0.0326342340659084,
            "auditor_fp_violation": 0.005184803522579908,
            "ave_precision_score": 0.7741061783614739,
            "fpr": 0.027442371020856202,
            "logloss": 0.9037908889343863,
            "mae": 0.4022594968486489,
            "precision": 0.8287671232876712,
            "recall": 0.2574468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.7963580433380204,
            "auditor_fn_violation": 0.02703847687400321,
            "auditor_fp_violation": 0.005203209542547959,
            "ave_precision_score": 0.7968597714603357,
            "fpr": 0.01864035087719298,
            "logloss": 0.947296147678745,
            "mae": 0.3994988110813338,
            "precision": 0.89375,
            "recall": 0.29545454545454547
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.7767949536630783,
            "auditor_fn_violation": 0.03774201835719458,
            "auditor_fp_violation": 0.006917219870018993,
            "ave_precision_score": 0.7774345752907451,
            "fpr": 0.030735455543358946,
            "logloss": 0.9428826075174659,
            "mae": 0.3980876678358365,
            "precision": 0.8120805369127517,
            "recall": 0.2574468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.8017254779653284,
            "auditor_fn_violation": 0.022881325213861103,
            "auditor_fp_violation": 0.00397606164945073,
            "ave_precision_score": 0.8021940810630384,
            "fpr": 0.017543859649122806,
            "logloss": 0.8971813937622081,
            "mae": 0.3972895633725401,
            "precision": 0.9041916167664671,
            "recall": 0.3119834710743802
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.7780487511799368,
            "auditor_fn_violation": 0.03136838171754211,
            "auditor_fp_violation": 0.005451137644959193,
            "ave_precision_score": 0.7786754131202913,
            "fpr": 0.03293084522502744,
            "logloss": 0.8910061335877931,
            "mae": 0.3965570899915016,
            "precision": 0.803921568627451,
            "recall": 0.26170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 8233,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.8141290220845208,
            "auditor_fn_violation": 0.015586486878352906,
            "auditor_fp_violation": 0.007475610755861617,
            "ave_precision_score": 0.8146475728325201,
            "fpr": 0.05482456140350877,
            "logloss": 0.6652100441605222,
            "mae": 0.3441977815197325,
            "precision": 0.8417721518987342,
            "recall": 0.5495867768595041
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7928836271266702,
            "auditor_fn_violation": 0.014818880351262356,
            "auditor_fp_violation": 0.017197219173069888,
            "ave_precision_score": 0.7933732530030486,
            "fpr": 0.06256860592755215,
            "logloss": 0.6723914048983359,
            "mae": 0.3475237203903525,
            "precision": 0.8229813664596274,
            "recall": 0.5638297872340425
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.8017177948091114,
            "auditor_fn_violation": 0.022881325213861103,
            "auditor_fp_violation": 0.00397606164945073,
            "ave_precision_score": 0.8021864128450003,
            "fpr": 0.017543859649122806,
            "logloss": 0.8972353776127799,
            "mae": 0.3973018008918779,
            "precision": 0.9041916167664671,
            "recall": 0.3119834710743802
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.7780448058636786,
            "auditor_fn_violation": 0.03136838171754211,
            "auditor_fp_violation": 0.005451137644959193,
            "ave_precision_score": 0.7786714705201442,
            "fpr": 0.03293084522502744,
            "logloss": 0.8910537984676654,
            "mae": 0.3965672945908376,
            "precision": 0.803921568627451,
            "recall": 0.26170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6096491228070176,
            "auc_prc": 0.8111090108957113,
            "auditor_fn_violation": 0.013008373205741641,
            "auditor_fp_violation": 0.004857353664535169,
            "ave_precision_score": 0.8115185017811133,
            "fpr": 0.01644736842105263,
            "logloss": 0.9299071240630018,
            "mae": 0.39441269248414246,
            "precision": 0.9050632911392406,
            "recall": 0.29545454545454547
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.7851611446230481,
            "auditor_fn_violation": 0.035499918256767175,
            "auditor_fp_violation": 0.004485365313340851,
            "ave_precision_score": 0.7860381528158664,
            "fpr": 0.02854006586169045,
            "logloss": 0.9309611158190852,
            "mae": 0.39571878039196307,
            "precision": 0.821917808219178,
            "recall": 0.2553191489361702
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.8087629060128277,
            "auditor_fn_violation": 0.006732999855009425,
            "auditor_fp_violation": 0.00556699868830956,
            "ave_precision_score": 0.8091554197888582,
            "fpr": 0.020833333333333332,
            "logloss": 0.8259551462742413,
            "mae": 0.3953335704211081,
            "precision": 0.8848484848484849,
            "recall": 0.30165289256198347
        },
        "train": {
            "accuracy": 0.601536772777168,
            "auc_prc": 0.7714623877195561,
            "auditor_fn_violation": 0.02844430950323469,
            "auditor_fp_violation": 0.0017249490355966766,
            "ave_precision_score": 0.772045727742814,
            "fpr": 0.026344676180021953,
            "logloss": 0.8457708273901064,
            "mae": 0.40217713013136225,
            "precision": 0.8451612903225807,
            "recall": 0.27872340425531916
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8343112751565803,
            "auditor_fn_violation": 0.01370614035087719,
            "auditor_fp_violation": 0.009543060337760288,
            "ave_precision_score": 0.8348572404104302,
            "fpr": 0.03399122807017544,
            "logloss": 0.6888205643945767,
            "mae": 0.34606681689330254,
            "precision": 0.8908450704225352,
            "recall": 0.5227272727272727
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.8469318178806207,
            "auditor_fn_violation": 0.010509844220753448,
            "auditor_fp_violation": 0.0020435543408728303,
            "ave_precision_score": 0.8472737337035979,
            "fpr": 0.031833150384193196,
            "logloss": 0.6581616082141877,
            "mae": 0.33899613954246954,
            "precision": 0.8933823529411765,
            "recall": 0.5170212765957447
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.8077952668717139,
            "auditor_fn_violation": 0.012460127591706546,
            "auditor_fp_violation": 0.007542220036071487,
            "ave_precision_score": 0.8082137982959166,
            "fpr": 0.02850877192982456,
            "logloss": 0.8399313527649213,
            "mae": 0.37914404470645596,
            "precision": 0.8779342723004695,
            "recall": 0.38636363636363635
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.7667491115143908,
            "auditor_fn_violation": 0.02482658757035758,
            "auditor_fp_violation": 0.01111136002150586,
            "ave_precision_score": 0.7677463219251242,
            "fpr": 0.04061470911086718,
            "logloss": 0.8597684991998991,
            "mae": 0.3862590801787553,
            "precision": 0.8121827411167513,
            "recall": 0.3404255319148936
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.8261846621843352,
            "auditor_fn_violation": 0.012061403508771938,
            "auditor_fp_violation": 0.0072066117396294505,
            "ave_precision_score": 0.8266811407440761,
            "fpr": 0.03399122807017544,
            "logloss": 0.7227770633682771,
            "mae": 0.35587471309988533,
            "precision": 0.8864468864468864,
            "recall": 0.5
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.8083020626651138,
            "auditor_fn_violation": 0.007613798257701387,
            "auditor_fp_violation": 0.007751069692421428,
            "ave_precision_score": 0.8087357341907666,
            "fpr": 0.04720087815587267,
            "logloss": 0.7072907761509715,
            "mae": 0.3521349368589847,
            "precision": 0.8442028985507246,
            "recall": 0.4957446808510638
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6052631578947368,
            "auc_prc": 0.7929111765079286,
            "auditor_fn_violation": 0.023259659997100203,
            "auditor_fp_violation": 0.005223704705689457,
            "ave_precision_score": 0.7945798362127328,
            "fpr": 0.023026315789473683,
            "logloss": 0.8812515385919878,
            "mae": 0.39964960060920895,
            "precision": 0.8734939759036144,
            "recall": 0.29958677685950413
        },
        "train": {
            "accuracy": 0.6048298572996706,
            "auc_prc": 0.774861589100712,
            "auditor_fn_violation": 0.03127496088002431,
            "auditor_fp_violation": 0.00599625140945511,
            "ave_precision_score": 0.7779956225708748,
            "fpr": 0.036223929747530186,
            "logloss": 0.8586409657944507,
            "mae": 0.3925312135867472,
            "precision": 0.8125,
            "recall": 0.30425531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.8490450497110723,
            "auditor_fn_violation": 0.017589169204001748,
            "auditor_fp_violation": 0.007844523692408593,
            "ave_precision_score": 0.8495073826016193,
            "fpr": 0.03837719298245614,
            "logloss": 0.612056438733208,
            "mae": 0.3409058155275967,
            "precision": 0.8780487804878049,
            "recall": 0.5206611570247934
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.8349416535546176,
            "auditor_fn_violation": 0.013662797486979477,
            "auditor_fp_violation": 0.005909132771293662,
            "ave_precision_score": 0.8352733210921703,
            "fpr": 0.03512623490669594,
            "logloss": 0.630458677985436,
            "mae": 0.3475613595088985,
            "precision": 0.8805970149253731,
            "recall": 0.502127659574468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.8148286191092692,
            "auditor_fn_violation": 0.026646549224300414,
            "auditor_fp_violation": 0.004319355632070832,
            "ave_precision_score": 0.815292839513013,
            "fpr": 0.01644736842105263,
            "logloss": 0.8746126751488958,
            "mae": 0.39049738842789483,
            "precision": 0.9132947976878613,
            "recall": 0.32644628099173556
        },
        "train": {
            "accuracy": 0.5993413830954994,
            "auc_prc": 0.7861244655510018,
            "auditor_fn_violation": 0.03130065161034169,
            "auditor_fp_violation": 0.004455496065971211,
            "ave_precision_score": 0.7867004717294133,
            "fpr": 0.031833150384193196,
            "logloss": 0.8712338252843543,
            "mae": 0.3910545040418841,
            "precision": 0.8220858895705522,
            "recall": 0.2851063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8268931429729685,
            "auditor_fn_violation": 0.020883173843700167,
            "auditor_fp_violation": 0.008469626168224302,
            "ave_precision_score": 0.8282159123669413,
            "fpr": 0.11074561403508772,
            "logloss": 0.5299625401482737,
            "mae": 0.31993980304194564,
            "precision": 0.7837259100642399,
            "recall": 0.756198347107438
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8498301020099268,
            "auditor_fn_violation": 0.017189434103276734,
            "auditor_fp_violation": 0.016714333007260714,
            "ave_precision_score": 0.8502063693220339,
            "fpr": 0.1163556531284303,
            "logloss": 0.505231952999232,
            "mae": 0.31619148925928203,
            "precision": 0.774468085106383,
            "recall": 0.774468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.8007759008566422,
            "auditor_fn_violation": 0.023860011599246046,
            "auditor_fp_violation": 0.004826610919822923,
            "ave_precision_score": 0.8012428434291182,
            "fpr": 0.017543859649122806,
            "logloss": 0.9009950657694247,
            "mae": 0.39943722031319084,
            "precision": 0.8987341772151899,
            "recall": 0.29338842975206614
        },
        "train": {
            "accuracy": 0.5817782656421515,
            "auc_prc": 0.7753739225327404,
            "auditor_fn_violation": 0.032300254571782254,
            "auditor_fp_violation": 0.006384551625260423,
            "ave_precision_score": 0.7760930880450414,
            "fpr": 0.03402854006586169,
            "logloss": 0.8972765951432625,
            "mae": 0.3990672884770167,
            "precision": 0.7947019867549668,
            "recall": 0.2553191489361702
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.801671772157304,
            "auditor_fn_violation": 0.022881325213861103,
            "auditor_fp_violation": 0.00397606164945073,
            "ave_precision_score": 0.8021405418498939,
            "fpr": 0.017543859649122806,
            "logloss": 0.8974034218054479,
            "mae": 0.39733994596919503,
            "precision": 0.9041916167664671,
            "recall": 0.3119834710743802
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.7780168028985961,
            "auditor_fn_violation": 0.03136838171754211,
            "auditor_fp_violation": 0.004022391979111442,
            "ave_precision_score": 0.7786434911238814,
            "fpr": 0.031833150384193196,
            "logloss": 0.8912020308511386,
            "mae": 0.39659905622896774,
            "precision": 0.8092105263157895,
            "recall": 0.26170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 8233,
        "test": {
            "accuracy": 0.569078947368421,
            "auc_prc": 0.809138957072636,
            "auditor_fn_violation": 0.012915488618239816,
            "auditor_fp_violation": 0.0034431874077717663,
            "ave_precision_score": 0.8097570090074439,
            "fpr": 0.010964912280701754,
            "logloss": 0.9849791378656404,
            "mae": 0.4091603286726974,
            "precision": 0.9099099099099099,
            "recall": 0.20867768595041322
        },
        "train": {
            "accuracy": 0.5521405049396267,
            "auc_prc": 0.7811842335355597,
            "auditor_fn_violation": 0.03506784688324731,
            "auditor_fp_violation": 0.0026757867435301965,
            "ave_precision_score": 0.7825672136729701,
            "fpr": 0.018660812294182216,
            "logloss": 0.9822072716007512,
            "mae": 0.40937441397656643,
            "precision": 0.8229166666666666,
            "recall": 0.16808510638297872
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8261682548904807,
            "auditor_fn_violation": 0.008101348412353198,
            "auditor_fp_violation": 0.012243298081652733,
            "ave_precision_score": 0.82651692303334,
            "fpr": 0.12171052631578948,
            "logloss": 0.5488902189962551,
            "mae": 0.3239938650360528,
            "precision": 0.7682672233820459,
            "recall": 0.7603305785123967
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8390129806946561,
            "auditor_fn_violation": 0.010019384823784947,
            "auditor_fp_violation": 0.008323563600339513,
            "ave_precision_score": 0.8393716859433785,
            "fpr": 0.14709110867178923,
            "logloss": 0.5265054536638762,
            "mae": 0.3209031327866819,
            "precision": 0.7341269841269841,
            "recall": 0.7872340425531915
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8309924417199512,
            "auditor_fn_violation": 0.007655049296795713,
            "auditor_fp_violation": 0.00881548204623709,
            "ave_precision_score": 0.8313815265297052,
            "fpr": 0.03618421052631579,
            "logloss": 0.6890454068022265,
            "mae": 0.34531305408547686,
            "precision": 0.886986301369863,
            "recall": 0.5351239669421488
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7965063887416144,
            "auditor_fn_violation": 0.00930938645864961,
            "auditor_fp_violation": 0.009095185824055198,
            "ave_precision_score": 0.7971950310139155,
            "fpr": 0.04610318331503842,
            "logloss": 0.7070878851667347,
            "mae": 0.3530587489872227,
            "precision": 0.8478260869565217,
            "recall": 0.4978723404255319
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8256005696307787,
            "auditor_fn_violation": 0.016098484848484855,
            "auditor_fp_violation": 0.007129754877848829,
            "ave_precision_score": 0.8260510350894382,
            "fpr": 0.04276315789473684,
            "logloss": 0.6703811187439989,
            "mae": 0.33539243762865417,
            "precision": 0.8757961783439491,
            "recall": 0.5681818181818182
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8151853719805556,
            "auditor_fn_violation": 0.0075016932526800065,
            "auditor_fp_violation": 0.00906780568063303,
            "ave_precision_score": 0.8155247024974204,
            "fpr": 0.050493962678375415,
            "logloss": 0.6739416090535907,
            "mae": 0.33611315284558174,
            "precision": 0.8525641025641025,
            "recall": 0.5659574468085107
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.8216456418489435,
            "auditor_fn_violation": 0.012786356386834868,
            "auditor_fp_violation": 0.009122909493359569,
            "ave_precision_score": 0.8220963600030873,
            "fpr": 0.044956140350877194,
            "logloss": 0.6808161657284044,
            "mae": 0.350177196811082,
            "precision": 0.8619528619528619,
            "recall": 0.5289256198347108
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7998058473153906,
            "auditor_fn_violation": 0.009846556274377,
            "auditor_fp_violation": 0.010347205109632584,
            "ave_precision_score": 0.8003552124035422,
            "fpr": 0.050493962678375415,
            "logloss": 0.6836963578977795,
            "mae": 0.35200022867728437,
            "precision": 0.8440677966101695,
            "recall": 0.5297872340425532
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5997807017543859,
            "auc_prc": 0.7942263957181029,
            "auditor_fn_violation": 0.031236407133536336,
            "auditor_fp_violation": 0.005203209542547959,
            "ave_precision_score": 0.7947319128345817,
            "fpr": 0.01864035087719298,
            "logloss": 0.9588146787468931,
            "mae": 0.4014643210402618,
            "precision": 0.8888888888888888,
            "recall": 0.2809917355371901
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.7734619489566787,
            "auditor_fn_violation": 0.037158138122708295,
            "auditor_fp_violation": 0.006917219870018993,
            "ave_precision_score": 0.774195397401843,
            "fpr": 0.030735455543358946,
            "logloss": 0.9554235179062532,
            "mae": 0.40022518864830775,
            "precision": 0.8082191780821918,
            "recall": 0.251063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.8279266959125726,
            "auditor_fn_violation": 0.026678265912715677,
            "auditor_fp_violation": 0.00917158550582063,
            "ave_precision_score": 0.8282434209002585,
            "fpr": 0.029605263157894735,
            "logloss": 0.7832152686157605,
            "mae": 0.36306005411616005,
            "precision": 0.8851063829787233,
            "recall": 0.4297520661157025
        },
        "train": {
            "accuracy": 0.6564215148188803,
            "auc_prc": 0.8191847953179296,
            "auditor_fn_violation": 0.028133685218487997,
            "auditor_fp_violation": 0.006302411194993915,
            "ave_precision_score": 0.8197802987289493,
            "fpr": 0.03293084522502744,
            "logloss": 0.7790902861089846,
            "mae": 0.36012804457688047,
            "precision": 0.8617511520737328,
            "recall": 0.39787234042553193
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.811414971286187,
            "auditor_fn_violation": 0.02009705306655068,
            "auditor_fp_violation": 0.005428656337104446,
            "ave_precision_score": 0.8121093278239837,
            "fpr": 0.05592105263157895,
            "logloss": 0.6278680040826716,
            "mae": 0.33785257940217617,
            "precision": 0.85,
            "recall": 0.5971074380165289
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8280474444346331,
            "auditor_fn_violation": 0.011497769577504278,
            "auditor_fp_violation": 0.001993772261923431,
            "ave_precision_score": 0.828535282606658,
            "fpr": 0.05598243688254665,
            "logloss": 0.5869820694459211,
            "mae": 0.3286176279693756,
            "precision": 0.8463855421686747,
            "recall": 0.597872340425532
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 8233,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.8139807360899199,
            "auditor_fn_violation": 0.00876513339132956,
            "auditor_fp_violation": 0.008228808001311691,
            "ave_precision_score": 0.8144033037633942,
            "fpr": 0.039473684210526314,
            "logloss": 0.7390319346945942,
            "mae": 0.3635036803100599,
            "precision": 0.8641509433962264,
            "recall": 0.4731404958677686
        },
        "train": {
            "accuracy": 0.6663007683863886,
            "auc_prc": 0.7886911230532478,
            "auditor_fn_violation": 0.013756218324497284,
            "auditor_fp_violation": 0.008333520016129397,
            "ave_precision_score": 0.7891623961065716,
            "fpr": 0.050493962678375415,
            "logloss": 0.7488632284249118,
            "mae": 0.3672445494517772,
            "precision": 0.8217054263565892,
            "recall": 0.451063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6052631578947368,
            "auc_prc": 0.793401497245236,
            "auditor_fn_violation": 0.023259659997100203,
            "auditor_fp_violation": 0.005223704705689457,
            "ave_precision_score": 0.7950617529758133,
            "fpr": 0.023026315789473683,
            "logloss": 0.8800198095105156,
            "mae": 0.39943726526743967,
            "precision": 0.8734939759036144,
            "recall": 0.29958677685950413
        },
        "train": {
            "accuracy": 0.6048298572996706,
            "auc_prc": 0.7756440819942998,
            "auditor_fn_violation": 0.03127496088002431,
            "auditor_fp_violation": 0.00599625140945511,
            "ave_precision_score": 0.7787696928734003,
            "fpr": 0.036223929747530186,
            "logloss": 0.8571587590401325,
            "mae": 0.3922681205187291,
            "precision": 0.8125,
            "recall": 0.30425531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8269615345400254,
            "auditor_fn_violation": 0.011386291141075845,
            "auditor_fp_violation": 0.007409001475651747,
            "ave_precision_score": 0.827396159604365,
            "fpr": 0.0581140350877193,
            "logloss": 0.5918552910474407,
            "mae": 0.33519416702237276,
            "precision": 0.8472622478386167,
            "recall": 0.6074380165289256
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8035972039916408,
            "auditor_fn_violation": 0.006551136230936318,
            "auditor_fp_violation": 0.009844406112243654,
            "ave_precision_score": 0.8040705358901208,
            "fpr": 0.06476399560922064,
            "logloss": 0.6011432873303896,
            "mae": 0.33761729118715805,
            "precision": 0.8284883720930233,
            "recall": 0.6063829787234043
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8259323795044707,
            "auditor_fn_violation": 0.014254385964912276,
            "auditor_fp_violation": 0.007808657156910971,
            "ave_precision_score": 0.8264741308711111,
            "fpr": 0.03289473684210526,
            "logloss": 0.7086568466513488,
            "mae": 0.35098155391259706,
            "precision": 0.8936170212765957,
            "recall": 0.5206611570247934
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.8365554367337763,
            "auditor_fn_violation": 0.0047971600065394564,
            "auditor_fp_violation": 0.00451025635281555,
            "ave_precision_score": 0.8369460380882775,
            "fpr": 0.03512623490669594,
            "logloss": 0.6774482643320321,
            "mae": 0.34354471332039865,
            "precision": 0.8805970149253731,
            "recall": 0.502127659574468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8104042628396717,
            "auditor_fn_violation": 0.00597179933304336,
            "auditor_fp_violation": 0.004713887522544679,
            "ave_precision_score": 0.8108297507798008,
            "fpr": 0.07017543859649122,
            "logloss": 0.6293658161788002,
            "mae": 0.34738942299345743,
            "precision": 0.8222222222222222,
            "recall": 0.6115702479338843
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.8105329132960452,
            "auditor_fn_violation": 0.0026017703248709684,
            "auditor_fp_violation": 0.008955796002996888,
            "ave_precision_score": 0.8109245799676815,
            "fpr": 0.08122941822173436,
            "logloss": 0.5957022626446979,
            "mae": 0.3415358853771767,
            "precision": 0.7932960893854749,
            "recall": 0.6042553191489362
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.8220092301393815,
            "auditor_fn_violation": 0.011778218790778602,
            "auditor_fp_violation": 0.01056781849483522,
            "ave_precision_score": 0.8224795072665856,
            "fpr": 0.03837719298245614,
            "logloss": 0.7126030827444372,
            "mae": 0.3528830591663759,
            "precision": 0.875886524822695,
            "recall": 0.5103305785123967
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.800183783345848,
            "auditor_fn_violation": 0.009827872106873442,
            "auditor_fp_violation": 0.0071213263937115285,
            "ave_precision_score": 0.8006719350395624,
            "fpr": 0.050493962678375415,
            "logloss": 0.712060016978288,
            "mae": 0.35087802573557,
            "precision": 0.8430034129692833,
            "recall": 0.5255319148936171
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.8147536259173985,
            "auditor_fn_violation": 0.025860428447150943,
            "auditor_fp_violation": 0.004319355632070832,
            "ave_precision_score": 0.8152179010916014,
            "fpr": 0.01644736842105263,
            "logloss": 0.8750617188329713,
            "mae": 0.3906616276411736,
            "precision": 0.9117647058823529,
            "recall": 0.3202479338842975
        },
        "train": {
            "accuracy": 0.5993413830954994,
            "auc_prc": 0.7860792049277181,
            "auditor_fn_violation": 0.03130065161034169,
            "auditor_fp_violation": 0.004455496065971211,
            "ave_precision_score": 0.7866552961574843,
            "fpr": 0.031833150384193196,
            "logloss": 0.8718095387905835,
            "mae": 0.39124705563692946,
            "precision": 0.8220858895705522,
            "recall": 0.2851063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.7948333801503662,
            "auditor_fn_violation": 0.014417500362476443,
            "auditor_fp_violation": 0.0032126168224299065,
            "ave_precision_score": 0.7980025621596717,
            "fpr": 0.027412280701754384,
            "logloss": 1.2693220144539772,
            "mae": 0.3659002334983619,
            "precision": 0.8983739837398373,
            "recall": 0.45661157024793386
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.7897184085499009,
            "auditor_fn_violation": 0.01772193287712825,
            "auditor_fp_violation": 0.005003098934414599,
            "ave_precision_score": 0.7926594075907168,
            "fpr": 0.026344676180021953,
            "logloss": 1.32558819292412,
            "mae": 0.36698580301105416,
            "precision": 0.8947368421052632,
            "recall": 0.4340425531914894
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8330299765381515,
            "auditor_fn_violation": 0.011662679425837328,
            "auditor_fp_violation": 0.00779840957534022,
            "ave_precision_score": 0.8335257769452511,
            "fpr": 0.03618421052631579,
            "logloss": 0.6654464320245572,
            "mae": 0.3432540593476644,
            "precision": 0.8888888888888888,
            "recall": 0.5454545454545454
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8434576985559848,
            "auditor_fn_violation": 0.007123338860732889,
            "auditor_fp_violation": 0.0031113799343374386,
            "ave_precision_score": 0.8438438898683734,
            "fpr": 0.03512623490669594,
            "logloss": 0.6341019698180403,
            "mae": 0.33592560326473014,
            "precision": 0.8885017421602788,
            "recall": 0.5425531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5964912280701754,
            "auc_prc": 0.7975295730881884,
            "auditor_fn_violation": 0.015455089169204017,
            "auditor_fp_violation": 0.006671175602557797,
            "ave_precision_score": 0.7990746282036125,
            "fpr": 0.019736842105263157,
            "logloss": 0.9302167684503996,
            "mae": 0.4007658765852906,
            "precision": 0.881578947368421,
            "recall": 0.2768595041322314
        },
        "train": {
            "accuracy": 0.5894621295279913,
            "auc_prc": 0.7752281541276449,
            "auditor_fn_violation": 0.02760352196557443,
            "auditor_fp_violation": 0.0012769103250520864,
            "ave_precision_score": 0.7775521619283939,
            "fpr": 0.029637760702524697,
            "logloss": 0.9130270066639685,
            "mae": 0.3961831896305677,
            "precision": 0.82,
            "recall": 0.26170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8327698553518176,
            "auditor_fn_violation": 0.02083786428882123,
            "auditor_fp_violation": 0.009099852434825386,
            "ave_precision_score": 0.8339160246520254,
            "fpr": 0.04824561403508772,
            "logloss": 0.5894513162365783,
            "mae": 0.3269347636527719,
            "precision": 0.8633540372670807,
            "recall": 0.5743801652892562
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8589121478036538,
            "auditor_fn_violation": 0.018499661349463997,
            "auditor_fp_violation": 0.009311737867485084,
            "ave_precision_score": 0.8591077646913399,
            "fpr": 0.042810098792535674,
            "logloss": 0.5529508341925532,
            "mae": 0.3127439302687751,
            "precision": 0.8803680981595092,
            "recall": 0.6106382978723405
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5910087719298246,
            "auc_prc": 0.8090620982915225,
            "auditor_fn_violation": 0.030701754385964928,
            "auditor_fp_violation": 0.00784708558780128,
            "ave_precision_score": 0.809536234810141,
            "fpr": 0.023026315789473683,
            "logloss": 0.927913783989178,
            "mae": 0.3945870193258503,
            "precision": 0.8627450980392157,
            "recall": 0.2727272727272727
        },
        "train": {
            "accuracy": 0.5905598243688255,
            "auc_prc": 0.7907425155072784,
            "auditor_fn_violation": 0.042310297311815424,
            "auditor_fp_violation": 0.005406333773904734,
            "ave_precision_score": 0.7912937381791768,
            "fpr": 0.027442371020856202,
            "logloss": 0.9339749779418853,
            "mae": 0.3946931288607263,
            "precision": 0.8299319727891157,
            "recall": 0.25957446808510637
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.83251662976387,
            "auditor_fn_violation": 0.010330578512396705,
            "auditor_fp_violation": 0.00779840957534022,
            "ave_precision_score": 0.8330152091906121,
            "fpr": 0.03618421052631579,
            "logloss": 0.6760463278960693,
            "mae": 0.3454997087683615,
            "precision": 0.8873720136518771,
            "recall": 0.5371900826446281
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8429910154331184,
            "auditor_fn_violation": 0.011570170726580573,
            "auditor_fp_violation": 0.002150585810614038,
            "ave_precision_score": 0.8433781963182491,
            "fpr": 0.03293084522502744,
            "logloss": 0.6442733460522518,
            "mae": 0.33803113039138777,
            "precision": 0.8936170212765957,
            "recall": 0.5361702127659574
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.8089309377787715,
            "auditor_fn_violation": 0.006732999855009425,
            "auditor_fp_violation": 0.00556699868830956,
            "ave_precision_score": 0.8093233929412187,
            "fpr": 0.020833333333333332,
            "logloss": 0.8250368065730723,
            "mae": 0.3952819864706518,
            "precision": 0.8848484848484849,
            "recall": 0.30165289256198347
        },
        "train": {
            "accuracy": 0.601536772777168,
            "auc_prc": 0.7717500546586579,
            "auditor_fn_violation": 0.02844430950323469,
            "auditor_fp_violation": 0.0017249490355966766,
            "ave_precision_score": 0.7723329838202451,
            "fpr": 0.026344676180021953,
            "logloss": 0.8446967512057932,
            "mae": 0.40209666688260226,
            "precision": 0.8451612903225807,
            "recall": 0.27872340425531916
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8495270014431248,
            "auditor_fn_violation": 0.014909109032912872,
            "auditor_fp_violation": 0.00795212329890146,
            "ave_precision_score": 0.8497394198493805,
            "fpr": 0.05701754385964912,
            "logloss": 0.5598816917126456,
            "mae": 0.3252551766054239,
            "precision": 0.8535211267605634,
            "recall": 0.6260330578512396
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8530355869270141,
            "auditor_fn_violation": 0.0013779573533876759,
            "auditor_fp_violation": 0.009401345609594004,
            "ave_precision_score": 0.8536748493286037,
            "fpr": 0.05598243688254665,
            "logloss": 0.5281298891491075,
            "mae": 0.3166697507097491,
            "precision": 0.8547008547008547,
            "recall": 0.6382978723404256
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6052631578947368,
            "auc_prc": 0.8044437687348175,
            "auditor_fn_violation": 0.022736334638248513,
            "auditor_fp_violation": 0.004826610919822923,
            "ave_precision_score": 0.8048835248938094,
            "fpr": 0.017543859649122806,
            "logloss": 0.8978719564001718,
            "mae": 0.39902605061046126,
            "precision": 0.8974358974358975,
            "recall": 0.2892561983471074
        },
        "train": {
            "accuracy": 0.5850713501646543,
            "auc_prc": 0.7779150536837418,
            "auditor_fn_violation": 0.030873251278697725,
            "auditor_fp_violation": 0.004022391979111442,
            "ave_precision_score": 0.778633495922655,
            "fpr": 0.031833150384193196,
            "logloss": 0.8920381640839585,
            "mae": 0.3982100774744642,
            "precision": 0.8066666666666666,
            "recall": 0.2574468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8399683694819884,
            "auditor_fn_violation": 0.012759170653907496,
            "auditor_fp_violation": 0.010893179209706514,
            "ave_precision_score": 0.8404613329090005,
            "fpr": 0.07236842105263158,
            "logloss": 0.5596508820269162,
            "mae": 0.3118086645784053,
            "precision": 0.8249336870026526,
            "recall": 0.6425619834710744
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8343322020985093,
            "auditor_fn_violation": 0.010818132984562209,
            "auditor_fp_violation": 0.01172616869653094,
            "ave_precision_score": 0.8347994378933514,
            "fpr": 0.06147091108671789,
            "logloss": 0.5549380667653592,
            "mae": 0.31248927802976084,
            "precision": 0.8409090909090909,
            "recall": 0.6297872340425532
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 8233,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.8077554800128809,
            "auditor_fn_violation": 0.006569885457445267,
            "auditor_fp_violation": 0.00556699868830956,
            "ave_precision_score": 0.8081498455639793,
            "fpr": 0.020833333333333332,
            "logloss": 0.8306208139780193,
            "mae": 0.3963004069914301,
            "precision": 0.8834355828220859,
            "recall": 0.2975206611570248
        },
        "train": {
            "accuracy": 0.5982436882546652,
            "auc_prc": 0.7701151531197834,
            "auditor_fn_violation": 0.028311184809771826,
            "auditor_fp_violation": 0.0017249490355966766,
            "ave_precision_score": 0.770700000974819,
            "fpr": 0.026344676180021953,
            "logloss": 0.850880876947639,
            "mae": 0.40318659646065985,
            "precision": 0.8421052631578947,
            "recall": 0.2723404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.8069472508864124,
            "auditor_fn_violation": 0.014970276931999432,
            "auditor_fp_violation": 0.00568996966715855,
            "ave_precision_score": 0.8073840349986595,
            "fpr": 0.020833333333333332,
            "logloss": 0.8563089573160112,
            "mae": 0.3912767994183684,
            "precision": 0.8972972972972973,
            "recall": 0.34297520661157027
        },
        "train": {
            "accuracy": 0.610318331503842,
            "auc_prc": 0.780567617081207,
            "auditor_fn_violation": 0.028481677838241827,
            "auditor_fp_violation": 0.005899176355503783,
            "ave_precision_score": 0.7812820281804586,
            "fpr": 0.03293084522502744,
            "logloss": 0.8526753737923028,
            "mae": 0.39123926893041217,
            "precision": 0.8285714285714286,
            "recall": 0.30851063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8412100339442368,
            "auditor_fn_violation": 0.02639961215021024,
            "auditor_fp_violation": 0.014208271847843908,
            "ave_precision_score": 0.8417279110924002,
            "fpr": 0.05043859649122807,
            "logloss": 0.5633972576180946,
            "mae": 0.33233199773199057,
            "precision": 0.8674351585014409,
            "recall": 0.621900826446281
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8424004018281778,
            "auditor_fn_violation": 0.018205385711282905,
            "auditor_fp_violation": 0.009339118010907253,
            "ave_precision_score": 0.8428678714257368,
            "fpr": 0.06147091108671789,
            "logloss": 0.5525247682476799,
            "mae": 0.33138602467240513,
            "precision": 0.8404558404558404,
            "recall": 0.6276595744680851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8288744613795811,
            "auditor_fn_violation": 0.00328947368421053,
            "auditor_fp_violation": 0.005994835218888344,
            "ave_precision_score": 0.8291925646711276,
            "fpr": 0.07017543859649122,
            "logloss": 0.5871712616977568,
            "mae": 0.32943580080973334,
            "precision": 0.827027027027027,
            "recall": 0.6322314049586777
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8285581534010931,
            "auditor_fn_violation": 0.0055445267066819275,
            "auditor_fp_violation": 0.013640289632135328,
            "ave_precision_score": 0.8289441171203242,
            "fpr": 0.08342480790340286,
            "logloss": 0.5623141320635237,
            "mae": 0.3236511698325955,
            "precision": 0.8,
            "recall": 0.6468085106382979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8616321657272648,
            "auditor_fn_violation": 0.010382684500507474,
            "auditor_fp_violation": 0.012935009837678308,
            "ave_precision_score": 0.8618307710883265,
            "fpr": 0.08442982456140351,
            "logloss": 0.49835986152126605,
            "mae": 0.3024361039319876,
            "precision": 0.8200934579439252,
            "recall": 0.7252066115702479
        },
        "train": {
            "accuracy": 0.7848518111964874,
            "auc_prc": 0.8655611198824189,
            "auditor_fn_violation": 0.009421491463670972,
            "auditor_fp_violation": 0.002576222585631399,
            "ave_precision_score": 0.8661215875477039,
            "fpr": 0.07903402854006586,
            "logloss": 0.4710058291879307,
            "mae": 0.2955919360071339,
            "precision": 0.8277511961722488,
            "recall": 0.7361702127659574
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.809884438936202,
            "auditor_fn_violation": 0.013755980861244032,
            "auditor_fp_violation": 0.004857353664535169,
            "ave_precision_score": 0.8102895761040886,
            "fpr": 0.01644736842105263,
            "logloss": 0.9144903678609593,
            "mae": 0.3927553371359113,
            "precision": 0.9112426035502958,
            "recall": 0.3181818181818182
        },
        "train": {
            "accuracy": 0.5905598243688255,
            "auc_prc": 0.7827444337975307,
            "auditor_fn_violation": 0.03420136861526965,
            "auditor_fp_violation": 0.0032433024435533473,
            "ave_precision_score": 0.7836260978362108,
            "fpr": 0.031833150384193196,
            "logloss": 0.9152522916759575,
            "mae": 0.3945065922922691,
            "precision": 0.8129032258064516,
            "recall": 0.2680851063829787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6041666666666666,
            "auc_prc": 0.8003091301801426,
            "auditor_fn_violation": 0.0243674786138901,
            "auditor_fp_violation": 0.004826610919822923,
            "ave_precision_score": 0.8007762610340523,
            "fpr": 0.017543859649122806,
            "logloss": 0.9049982027139671,
            "mae": 0.40006265208141584,
            "precision": 0.896774193548387,
            "recall": 0.2871900826446281
        },
        "train": {
            "accuracy": 0.5806805708013172,
            "auc_prc": 0.7746316340568893,
            "auditor_fn_violation": 0.03266226031716375,
            "auditor_fp_violation": 0.006384551625260423,
            "ave_precision_score": 0.7753524658810778,
            "fpr": 0.03402854006586169,
            "logloss": 0.9015462249615199,
            "mae": 0.39973379798378167,
            "precision": 0.7933333333333333,
            "recall": 0.2531914893617021
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.8301764924671997,
            "auditor_fn_violation": 0.008216887777294474,
            "auditor_fp_violation": 0.0060845015576323986,
            "ave_precision_score": 0.8306937758283992,
            "fpr": 0.04057017543859649,
            "logloss": 0.7000412113774096,
            "mae": 0.34604314631698735,
            "precision": 0.8706293706293706,
            "recall": 0.5144628099173554
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.8072023603320526,
            "auditor_fn_violation": 0.009757806478735087,
            "auditor_fp_violation": 0.012781548770258198,
            "ave_precision_score": 0.8076813943314813,
            "fpr": 0.048298572996706916,
            "logloss": 0.7049647686973459,
            "mae": 0.3486738925348378,
            "precision": 0.8445229681978799,
            "recall": 0.5085106382978724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5953947368421053,
            "auc_prc": 0.8045750081882734,
            "auditor_fn_violation": 0.017716035957662767,
            "auditor_fp_violation": 0.007444868011149368,
            "ave_precision_score": 0.8050812285545919,
            "fpr": 0.019736842105263157,
            "logloss": 0.9272724590776046,
            "mae": 0.3971028002191528,
            "precision": 0.8807947019867549,
            "recall": 0.27479338842975204
        },
        "train": {
            "accuracy": 0.5982436882546652,
            "auc_prc": 0.7884845297379932,
            "auditor_fn_violation": 0.03485297895695636,
            "auditor_fp_violation": 0.0028823823711702033,
            "ave_precision_score": 0.7889783601400113,
            "fpr": 0.027442371020856202,
            "logloss": 0.9130189366898329,
            "mae": 0.39224754031630016,
            "precision": 0.8376623376623377,
            "recall": 0.274468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 8233,
        "test": {
            "accuracy": 0.581140350877193,
            "auc_prc": 0.8136342547972042,
            "auditor_fn_violation": 0.01836622807017544,
            "auditor_fp_violation": 0.006932488932611905,
            "ave_precision_score": 0.8140525065304637,
            "fpr": 0.020833333333333332,
            "logloss": 0.9072684555942199,
            "mae": 0.39999509983687875,
            "precision": 0.8642857142857143,
            "recall": 0.25
        },
        "train": {
            "accuracy": 0.5784851811196488,
            "auc_prc": 0.7887864392314424,
            "auditor_fn_violation": 0.03780974846439501,
            "auditor_fp_violation": 0.0036067116198839583,
            "ave_precision_score": 0.7893776702133843,
            "fpr": 0.02305159165751921,
            "logloss": 0.9175239668874505,
            "mae": 0.401464698864832,
            "precision": 0.8359375,
            "recall": 0.2276595744680851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.798865479390615,
            "auditor_fn_violation": 0.029791032332898378,
            "auditor_fp_violation": 0.0033150926381374,
            "ave_precision_score": 0.7992565446802693,
            "fpr": 0.010964912280701754,
            "logloss": 0.9924884352322803,
            "mae": 0.4191788766709802,
            "precision": 0.9107142857142857,
            "recall": 0.21074380165289255
        },
        "train": {
            "accuracy": 0.557628979143798,
            "auc_prc": 0.7747303159327446,
            "auditor_fn_violation": 0.037728005231566934,
            "auditor_fp_violation": 0.0003260726171185644,
            "ave_precision_score": 0.7756419460853374,
            "fpr": 0.020856201975850714,
            "logloss": 0.9790592562959827,
            "mae": 0.417010527478304,
            "precision": 0.819047619047619,
            "recall": 0.1829787234042553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8287930459588805,
            "auditor_fn_violation": 0.008346020008699434,
            "auditor_fp_violation": 0.006635309067060175,
            "ave_precision_score": 0.8292249248706571,
            "fpr": 0.05482456140350877,
            "logloss": 0.6077085303232189,
            "mae": 0.32871229407804714,
            "precision": 0.8554913294797688,
            "recall": 0.6115702479338843
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8371521578400339,
            "auditor_fn_violation": 0.0053203166966391885,
            "auditor_fp_violation": 0.007250759798979967,
            "ave_precision_score": 0.8375408002551071,
            "fpr": 0.06586169045005488,
            "logloss": 0.5680232760392964,
            "mae": 0.3221678153090854,
            "precision": 0.8275862068965517,
            "recall": 0.6127659574468085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.8123661966121294,
            "auditor_fn_violation": 0.025101493402928814,
            "auditor_fp_violation": 0.002116125594359731,
            "ave_precision_score": 0.8128019814953373,
            "fpr": 0.010964912280701754,
            "logloss": 0.9474960609274254,
            "mae": 0.402114331478866,
            "precision": 0.9315068493150684,
            "recall": 0.2809917355371901
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7837641054562591,
            "auditor_fn_violation": 0.03623794287315787,
            "auditor_fp_violation": 0.001732416347439086,
            "ave_precision_score": 0.7843267671149126,
            "fpr": 0.025246981339187707,
            "logloss": 0.9450172461242718,
            "mae": 0.40466857424085606,
            "precision": 0.816,
            "recall": 0.2170212765957447
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8303627474735567,
            "auditor_fn_violation": 0.007655049296795713,
            "auditor_fp_violation": 0.006089625348417774,
            "ave_precision_score": 0.8308305163705934,
            "fpr": 0.044956140350877194,
            "logloss": 0.6564434576471692,
            "mae": 0.34419778921233446,
            "precision": 0.8633333333333333,
            "recall": 0.5351239669421488
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.8065511027563207,
            "auditor_fn_violation": 0.0030174930518252073,
            "auditor_fp_violation": 0.00703171865160261,
            "ave_precision_score": 0.8069914195060923,
            "fpr": 0.05159165751920966,
            "logloss": 0.6546178993750772,
            "mae": 0.3457087688086683,
            "precision": 0.8459016393442623,
            "recall": 0.548936170212766
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 8233,
        "test": {
            "accuracy": 0.569078947368421,
            "auc_prc": 0.7954404884801205,
            "auditor_fn_violation": 0.03206783746556474,
            "auditor_fp_violation": 0.003350959173635022,
            "ave_precision_score": 0.7960043670127547,
            "fpr": 0.013157894736842105,
            "logloss": 1.008398556765923,
            "mae": 0.4160047828184983,
            "precision": 0.8956521739130435,
            "recall": 0.2128099173553719
        },
        "train": {
            "accuracy": 0.5598243688254665,
            "auc_prc": 0.7705748866773285,
            "auditor_fn_violation": 0.035541957633650194,
            "auditor_fp_violation": 0.0012271282461026856,
            "ave_precision_score": 0.771511489670216,
            "fpr": 0.025246981339187707,
            "logloss": 1.0063602155663345,
            "mae": 0.4142242530402988,
            "precision": 0.8,
            "recall": 0.19574468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6096491228070176,
            "auc_prc": 0.8111063146060138,
            "auditor_fn_violation": 0.013008373205741641,
            "auditor_fp_violation": 0.004857353664535169,
            "ave_precision_score": 0.8115156910993999,
            "fpr": 0.01644736842105263,
            "logloss": 0.9306570574367864,
            "mae": 0.3944536500444851,
            "precision": 0.9050632911392406,
            "recall": 0.29545454545454547
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.7852046788994266,
            "auditor_fn_violation": 0.034801597496321564,
            "auditor_fp_violation": 0.004485365313340851,
            "ave_precision_score": 0.7860809910999855,
            "fpr": 0.02854006586169045,
            "logloss": 0.9317292309322632,
            "mae": 0.39574154252894367,
            "precision": 0.8206896551724138,
            "recall": 0.2531914893617021
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6019736842105263,
            "auc_prc": 0.7873347409993335,
            "auditor_fn_violation": 0.02479338842975207,
            "auditor_fp_violation": 0.005223704705689457,
            "ave_precision_score": 0.7880577970921528,
            "fpr": 0.023026315789473683,
            "logloss": 0.900072465614569,
            "mae": 0.40372175032168517,
            "precision": 0.8711656441717791,
            "recall": 0.29338842975206614
        },
        "train": {
            "accuracy": 0.6004390779363337,
            "auc_prc": 0.7666173784230212,
            "auditor_fn_violation": 0.03545320783800828,
            "auditor_fp_violation": 0.007136261017396348,
            "ave_precision_score": 0.7675312416897492,
            "fpr": 0.03732162458836443,
            "logloss": 0.8813698357459459,
            "mae": 0.3976830234945344,
            "precision": 0.8045977011494253,
            "recall": 0.2978723404255319
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8479936785852416,
            "auditor_fn_violation": 0.018028671886327396,
            "auditor_fp_violation": 0.0076472577471716685,
            "ave_precision_score": 0.8483507384207372,
            "fpr": 0.051535087719298246,
            "logloss": 0.5549802699613167,
            "mae": 0.32074468156714464,
            "precision": 0.867231638418079,
            "recall": 0.6342975206611571
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.847907480234671,
            "auditor_fn_violation": 0.013354508723170701,
            "auditor_fp_violation": 0.008087098725329872,
            "ave_precision_score": 0.8482522986853971,
            "fpr": 0.05598243688254665,
            "logloss": 0.5467316824534384,
            "mae": 0.32009621812403816,
            "precision": 0.8571428571428571,
            "recall": 0.6510638297872341
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5888157894736842,
            "auc_prc": 0.8155390532900786,
            "auditor_fn_violation": 0.036802685950413236,
            "auditor_fp_violation": 0.00825955074602394,
            "ave_precision_score": 0.8159098498179698,
            "fpr": 0.021929824561403508,
            "logloss": 0.8788684717455105,
            "mae": 0.39319603282395593,
            "precision": 0.8657718120805369,
            "recall": 0.2665289256198347
        },
        "train": {
            "accuracy": 0.5894621295279913,
            "auc_prc": 0.805312197970367,
            "auditor_fn_violation": 0.03636172548286896,
            "auditor_fp_violation": 0.005802101301552456,
            "ave_precision_score": 0.8057979991043386,
            "fpr": 0.027442371020856202,
            "logloss": 0.8669077074864665,
            "mae": 0.38962524742272436,
            "precision": 0.8287671232876712,
            "recall": 0.2574468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.8003864884404234,
            "auditor_fn_violation": 0.027187998405103688,
            "auditor_fp_violation": 0.004826610919822923,
            "ave_precision_score": 0.8008620987776436,
            "fpr": 0.017543859649122806,
            "logloss": 0.9013801019158227,
            "mae": 0.39934191270337277,
            "precision": 0.89937106918239,
            "recall": 0.29545454545454547
        },
        "train": {
            "accuracy": 0.58397365532382,
            "auc_prc": 0.7734527878663935,
            "auditor_fn_violation": 0.031938248826400735,
            "auditor_fp_violation": 0.005451137644959193,
            "ave_precision_score": 0.774182944568767,
            "fpr": 0.03293084522502744,
            "logloss": 0.8972708776424936,
            "mae": 0.39926458341192195,
            "precision": 0.8013245033112583,
            "recall": 0.2574468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.8279817201270293,
            "auditor_fn_violation": 0.012013828476149053,
            "auditor_fp_violation": 0.0072450401705197575,
            "ave_precision_score": 0.8283373419214022,
            "fpr": 0.021929824561403508,
            "logloss": 0.8051369201778844,
            "mae": 0.37135784715805165,
            "precision": 0.908675799086758,
            "recall": 0.41115702479338845
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.795347603020689,
            "auditor_fn_violation": 0.020076137982577017,
            "auditor_fp_violation": 0.003746101440942275,
            "ave_precision_score": 0.7958109904649152,
            "fpr": 0.030735455543358946,
            "logloss": 0.8293076683790481,
            "mae": 0.37892284878781607,
            "precision": 0.8613861386138614,
            "recall": 0.3702127659574468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7890798295220307,
            "auditor_fn_violation": 0.014417500362476446,
            "auditor_fp_violation": 0.0004918839153959693,
            "ave_precision_score": 0.7897214210782518,
            "fpr": 0.08881578947368421,
            "logloss": 0.6150125952810249,
            "mae": 0.3465029133863585,
            "precision": 0.7959697732997482,
            "recall": 0.6528925619834711
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.8071178157702108,
            "auditor_fn_violation": 0.001810028726907539,
            "auditor_fp_violation": 0.0048512635936189315,
            "ave_precision_score": 0.807494674597125,
            "fpr": 0.11306256860592755,
            "logloss": 0.574398751076563,
            "mae": 0.3357111564924292,
            "precision": 0.7535885167464115,
            "recall": 0.6702127659574468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5899122807017544,
            "auc_prc": 0.8016822359224759,
            "auditor_fn_violation": 0.03941931274467159,
            "auditor_fp_violation": 0.007073393179209707,
            "ave_precision_score": 0.8021497129783962,
            "fpr": 0.023026315789473683,
            "logloss": 0.9458954521273325,
            "mae": 0.4018221226043878,
            "precision": 0.8618421052631579,
            "recall": 0.2706611570247934
        },
        "train": {
            "accuracy": 0.579582875960483,
            "auc_prc": 0.7789397008858605,
            "auditor_fn_violation": 0.043197795268234605,
            "auditor_fp_violation": 0.006730537073958746,
            "ave_precision_score": 0.77962354124237,
            "fpr": 0.027442371020856202,
            "logloss": 0.9410735587472796,
            "mae": 0.4014117666471619,
            "precision": 0.8175182481751825,
            "recall": 0.23829787234042554
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8460255854396148,
            "auditor_fn_violation": 0.012967594606350592,
            "auditor_fp_violation": 0.007629324479422857,
            "ave_precision_score": 0.8464829307423857,
            "fpr": 0.03618421052631579,
            "logloss": 0.6096656902449054,
            "mae": 0.32653593768261263,
            "precision": 0.8925081433224755,
            "recall": 0.5661157024793388
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8546654442837196,
            "auditor_fn_violation": 0.009848891795314956,
            "auditor_fp_violation": 0.0046795154212435075,
            "ave_precision_score": 0.8559373678296107,
            "fpr": 0.03732162458836443,
            "logloss": 0.5850858065501386,
            "mae": 0.3205419777699314,
            "precision": 0.8855218855218855,
            "recall": 0.5595744680851064
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5833333333333334,
            "auc_prc": 0.7807784573872549,
            "auditor_fn_violation": 0.013520371175873574,
            "auditor_fp_violation": 0.003883833415313986,
            "ave_precision_score": 0.7813478435105841,
            "fpr": 0.02631578947368421,
            "logloss": 0.8514150769547445,
            "mae": 0.4139367660166483,
            "precision": 0.8421052631578947,
            "recall": 0.2644628099173554
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.7533660950354482,
            "auditor_fn_violation": 0.03231193217647198,
            "auditor_fp_violation": 0.004268813269910965,
            "ave_precision_score": 0.7541785704872093,
            "fpr": 0.031833150384193196,
            "logloss": 0.846085190602016,
            "mae": 0.4131102394304953,
            "precision": 0.8092105263157895,
            "recall": 0.26170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8127034439153746,
            "auditor_fn_violation": 0.0037606930549514373,
            "auditor_fp_violation": 0.0075960198393179214,
            "ave_precision_score": 0.8131474383567996,
            "fpr": 0.07346491228070176,
            "logloss": 0.6254724734708677,
            "mae": 0.3449330182063517,
            "precision": 0.814404432132964,
            "recall": 0.6074380165289256
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.8091080595007243,
            "auditor_fn_violation": 0.003374827755330831,
            "auditor_fp_violation": 0.0038581111185784236,
            "ave_precision_score": 0.8095490214940506,
            "fpr": 0.0801317233809001,
            "logloss": 0.5957634394196996,
            "mae": 0.33924268519313666,
            "precision": 0.7972222222222223,
            "recall": 0.6106382978723405
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 8233,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.8218629082563321,
            "auditor_fn_violation": 0.018712846164999273,
            "auditor_fp_violation": 0.007757419249057222,
            "ave_precision_score": 0.8223199424509733,
            "fpr": 0.039473684210526314,
            "logloss": 0.6923071924638777,
            "mae": 0.35203244329188654,
            "precision": 0.8732394366197183,
            "recall": 0.512396694214876
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.7967517531623451,
            "auditor_fn_violation": 0.013242403718149335,
            "auditor_fp_violation": 0.011188522243877426,
            "ave_precision_score": 0.7972781713629844,
            "fpr": 0.05159165751920966,
            "logloss": 0.7136315698528566,
            "mae": 0.3589747054631434,
            "precision": 0.8284671532846716,
            "recall": 0.4829787234042553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5877192982456141,
            "auc_prc": 0.8061426196765992,
            "auditor_fn_violation": 0.026469841960272602,
            "auditor_fp_violation": 0.003166502705361535,
            "ave_precision_score": 0.8065378778600527,
            "fpr": 0.013157894736842105,
            "logloss": 0.978993115914345,
            "mae": 0.408459415122898,
            "precision": 0.9090909090909091,
            "recall": 0.24793388429752067
        },
        "train": {
            "accuracy": 0.5609220636663008,
            "auc_prc": 0.7730930668746675,
            "auditor_fn_violation": 0.03460774925847211,
            "auditor_fp_violation": 0.003870556638315772,
            "ave_precision_score": 0.7738203336631739,
            "fpr": 0.025246981339187707,
            "logloss": 0.9822361503305218,
            "mae": 0.41124157498392067,
            "precision": 0.8017241379310345,
            "recall": 0.19787234042553192
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6052631578947368,
            "auc_prc": 0.7929044657606925,
            "auditor_fn_violation": 0.023259659997100203,
            "auditor_fp_violation": 0.005223704705689457,
            "ave_precision_score": 0.794574814739641,
            "fpr": 0.023026315789473683,
            "logloss": 0.8806496072101543,
            "mae": 0.3996420710410396,
            "precision": 0.8734939759036144,
            "recall": 0.29958677685950413
        },
        "train": {
            "accuracy": 0.6048298572996706,
            "auc_prc": 0.7749187177217096,
            "auditor_fn_violation": 0.03127496088002431,
            "auditor_fp_violation": 0.00599625140945511,
            "ave_precision_score": 0.7780527034376874,
            "fpr": 0.036223929747530186,
            "logloss": 0.8579240991895484,
            "mae": 0.39250396120256686,
            "precision": 0.8125,
            "recall": 0.30425531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5877192982456141,
            "auc_prc": 0.8088639424047599,
            "auditor_fn_violation": 0.026469841960272602,
            "auditor_fp_violation": 0.003166502705361535,
            "ave_precision_score": 0.8092653870826344,
            "fpr": 0.013157894736842105,
            "logloss": 0.978897366775296,
            "mae": 0.4074565925524471,
            "precision": 0.9090909090909091,
            "recall": 0.24793388429752067
        },
        "train": {
            "accuracy": 0.5609220636663008,
            "auc_prc": 0.777748623430627,
            "auditor_fn_violation": 0.03460774925847211,
            "auditor_fp_violation": 0.002543864234314289,
            "ave_precision_score": 0.7784698033704326,
            "fpr": 0.025246981339187707,
            "logloss": 0.9823987833402462,
            "mae": 0.40960061370278067,
            "precision": 0.8017241379310345,
            "recall": 0.19787234042553192
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 8233,
        "test": {
            "accuracy": 0.6096491228070176,
            "auc_prc": 0.8111344540125499,
            "auditor_fn_violation": 0.013008373205741641,
            "auditor_fp_violation": 0.004857353664535169,
            "ave_precision_score": 0.8115437899222422,
            "fpr": 0.01644736842105263,
            "logloss": 0.9305871655460277,
            "mae": 0.39442642906326736,
            "precision": 0.9050632911392406,
            "recall": 0.29545454545454547
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.7852082668970759,
            "auditor_fn_violation": 0.034801597496321564,
            "auditor_fp_violation": 0.004485365313340851,
            "ave_precision_score": 0.7860845741680066,
            "fpr": 0.02854006586169045,
            "logloss": 0.9316612370993017,
            "mae": 0.39571655096436886,
            "precision": 0.8206896551724138,
            "recall": 0.2531914893617021
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 8233,
        "test": {
            "accuracy": 0.5997807017543859,
            "auc_prc": 0.8130791080752244,
            "auditor_fn_violation": 0.032369146005509636,
            "auditor_fp_violation": 0.004836858501393671,
            "ave_precision_score": 0.8134782139001607,
            "fpr": 0.019736842105263157,
            "logloss": 0.9215034000079328,
            "mae": 0.40005030182264656,
            "precision": 0.8838709677419355,
            "recall": 0.2830578512396694
        },
        "train": {
            "accuracy": 0.5806805708013172,
            "auc_prc": 0.789729841704469,
            "auditor_fn_violation": 0.031749071630427166,
            "auditor_fp_violation": 0.0055581691147004,
            "ave_precision_score": 0.7906239426181515,
            "fpr": 0.021953896816684963,
            "logloss": 0.9263667286948493,
            "mae": 0.3986159595764374,
            "precision": 0.84375,
            "recall": 0.2297872340425532
        }
    }
]
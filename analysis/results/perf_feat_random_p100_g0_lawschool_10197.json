[
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(0)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8058952800730547,
            "auditor_fn_violation": 0.016049890350877193,
            "auditor_fp_violation": 0.01749817251461989,
            "ave_precision_score": 0.8064555433890964,
            "fpr": 0.13486842105263158,
            "logloss": 1.0830080598673741,
            "mae": 0.2799268519143599,
            "precision": 0.7463917525773196,
            "recall": 0.7541666666666667
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8376408670181449,
            "auditor_fn_violation": 0.01824165034019277,
            "auditor_fp_violation": 0.023385672696033987,
            "ave_precision_score": 0.837879394899671,
            "fpr": 0.12952799121844127,
            "logloss": 0.9569673141928355,
            "mae": 0.26405327975598764,
            "precision": 0.7536534446764092,
            "recall": 0.7616033755274262
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(1)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.7760828569784594,
            "auditor_fn_violation": 0.01356907894736842,
            "auditor_fp_violation": 0.021807992202729054,
            "ave_precision_score": 0.7741432084037554,
            "fpr": 0.14035087719298245,
            "logloss": 3.6340711820603984,
            "mae": 0.3157666970947818,
            "precision": 0.717439293598234,
            "recall": 0.6770833333333334
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7846639167541735,
            "auditor_fn_violation": 0.014617404715919357,
            "auditor_fp_violation": 0.02449090319939112,
            "ave_precision_score": 0.7861566161939387,
            "fpr": 0.150384193194292,
            "logloss": 3.136212110875895,
            "mae": 0.3043269346853662,
            "precision": 0.7121848739495799,
            "recall": 0.7151898734177216
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(2)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7730263157894737,
            "auc_prc": 0.8115872556865896,
            "auditor_fn_violation": 0.0045915570175438635,
            "auditor_fp_violation": 0.01006132228719948,
            "ave_precision_score": 0.8047805366916229,
            "fpr": 0.10087719298245613,
            "logloss": 0.5158883810519237,
            "mae": 0.3322747680603674,
            "precision": 0.7986870897155361,
            "recall": 0.7604166666666666
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8234964107231857,
            "auditor_fn_violation": 0.007016910058497415,
            "auditor_fp_violation": 0.010514761106938587,
            "ave_precision_score": 0.8152817228592442,
            "fpr": 0.07574094401756312,
            "logloss": 0.5190048904356719,
            "mae": 0.3393771692243549,
            "precision": 0.8244274809160306,
            "recall": 0.6835443037974683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(3)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5098684210526315,
            "auc_prc": 0.4838535621770549,
            "auditor_fn_violation": 0.012712445175438618,
            "auditor_fp_violation": 0.009944566276803131,
            "ave_precision_score": 0.4923455959580025,
            "fpr": 0.1337719298245614,
            "logloss": 8.088152325321458,
            "mae": 0.5267258698870402,
            "precision": 0.5595667870036101,
            "recall": 0.3229166666666667
        },
        "train": {
            "accuracy": 0.4983534577387486,
            "auc_prc": 0.4755045609208353,
            "auditor_fn_violation": 0.00016673845683558983,
            "auditor_fp_violation": 0.010921686883174634,
            "ave_precision_score": 0.4831031631616978,
            "fpr": 0.16136114160263446,
            "logloss": 7.872489397902593,
            "mae": 0.5285260880054273,
            "precision": 0.5273311897106109,
            "recall": 0.3459915611814346
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(4)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.7370703604302395,
            "auditor_fn_violation": 0.007675438596491229,
            "auditor_fp_violation": 0.012741634178037692,
            "ave_precision_score": 0.7317751223361578,
            "fpr": 0.3399122807017544,
            "logloss": 0.6751384376101048,
            "mae": 0.401316300618642,
            "precision": 0.5822102425876011,
            "recall": 0.9
        },
        "train": {
            "accuracy": 0.610318331503842,
            "auc_prc": 0.7511897740152713,
            "auditor_fn_violation": 0.007364281843571538,
            "auditor_fp_violation": 0.025729263740652653,
            "ave_precision_score": 0.7411902169346027,
            "fpr": 0.33699231613611413,
            "logloss": 0.6529611161896716,
            "mae": 0.3936945913087589,
            "precision": 0.5811732605729877,
            "recall": 0.8987341772151899
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(5)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6030701754385965,
            "auc_prc": 0.6916230308029258,
            "auditor_fn_violation": 0.013304093567251464,
            "auditor_fp_violation": 0.00830490578297596,
            "ave_precision_score": 0.6918064093354541,
            "fpr": 0.03728070175438596,
            "logloss": 4.487466597050642,
            "mae": 0.42625664722258483,
            "precision": 0.8172043010752689,
            "recall": 0.31666666666666665
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.6913793943476874,
            "auditor_fn_violation": 0.0029966605992395143,
            "auditor_fp_violation": 0.0029966817966024227,
            "ave_precision_score": 0.6909755173598248,
            "fpr": 0.027442371020856202,
            "logloss": 4.280726541787988,
            "mae": 0.4311075560818644,
            "precision": 0.8355263157894737,
            "recall": 0.2679324894514768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(6)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.7763846189960306,
            "auditor_fn_violation": 0.011586257309941513,
            "auditor_fp_violation": 0.003614359974009098,
            "ave_precision_score": 0.7656842529612786,
            "fpr": 0.043859649122807015,
            "logloss": 0.5824719122687388,
            "mae": 0.38284607484054406,
            "precision": 0.8305084745762712,
            "recall": 0.4083333333333333
        },
        "train": {
            "accuracy": 0.6114160263446762,
            "auc_prc": 0.7574798764025013,
            "auditor_fn_violation": 0.010492943721139214,
            "auditor_fp_violation": 0.0010524808656969123,
            "ave_precision_score": 0.7683799431671481,
            "fpr": 0.027442371020856202,
            "logloss": 0.6106209617046906,
            "mae": 0.3914715376008879,
            "precision": 0.8529411764705882,
            "recall": 0.3059071729957806
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(7)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8044159120369789,
            "auditor_fn_violation": 0.007309941520467837,
            "auditor_fp_violation": 0.006284519168291099,
            "ave_precision_score": 0.8047932806231315,
            "fpr": 0.09100877192982457,
            "logloss": 0.7773322033499032,
            "mae": 0.2932615376262012,
            "precision": 0.794044665012407,
            "recall": 0.6666666666666666
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.8149203741042793,
            "auditor_fn_violation": 0.016685424743060683,
            "auditor_fp_violation": 0.012931196889278514,
            "ave_precision_score": 0.8152274055299508,
            "fpr": 0.07025246981339188,
            "logloss": 0.8701940232293538,
            "mae": 0.3052307320507026,
            "precision": 0.8123167155425219,
            "recall": 0.5843881856540084
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(8)",
        "seed": 10197,
        "test": {
            "accuracy": 0.631578947368421,
            "auc_prc": 0.7421052631578947,
            "auditor_fn_violation": 0.01770833333333334,
            "auditor_fp_violation": 0.01541179337231969,
            "ave_precision_score": 0.6067105263157895,
            "fpr": 0.18421052631578946,
            "logloss": 0.6584916727074809,
            "mae": 0.46571253161681325,
            "precision": 0.65,
            "recall": 0.65
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.7327377949817448,
            "auditor_fn_violation": 0.021787158359849384,
            "auditor_fp_violation": 0.015965556998495386,
            "ave_precision_score": 0.6006863233821595,
            "fpr": 0.17014270032930845,
            "logloss": 0.6605469489512291,
            "mae": 0.4666325019429204,
            "precision": 0.651685393258427,
            "recall": 0.6118143459915611
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(9)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7839912280701754,
            "auc_prc": 0.8609475887986799,
            "auditor_fn_violation": 0.010028326023391817,
            "auditor_fp_violation": 0.006431733268356074,
            "ave_precision_score": 0.8591450525664737,
            "fpr": 0.07675438596491228,
            "logloss": 0.48936400441057737,
            "mae": 0.29853757988996477,
            "precision": 0.83451536643026,
            "recall": 0.7354166666666667
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8740411378295714,
            "auditor_fn_violation": 0.008878822826494744,
            "auditor_fp_violation": 0.009160853740326094,
            "ave_precision_score": 0.8716889025490359,
            "fpr": 0.0570801317233809,
            "logloss": 0.48983042626708556,
            "mae": 0.3001205775719116,
            "precision": 0.8605898123324397,
            "recall": 0.6772151898734177
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(10)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.84163315838934,
            "auditor_fn_violation": 0.0018548976608187189,
            "auditor_fp_violation": 0.010774549220272905,
            "ave_precision_score": 0.8206817714378837,
            "fpr": 0.15460526315789475,
            "logloss": 0.5270951731957927,
            "mae": 0.324460660955019,
            "precision": 0.7349624060150376,
            "recall": 0.8145833333333333
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8479747139857603,
            "auditor_fn_violation": 0.011421584293237365,
            "auditor_fp_violation": 0.008329418975300613,
            "ave_precision_score": 0.835236497327411,
            "fpr": 0.13172338090010977,
            "logloss": 0.5112449706789421,
            "mae": 0.32763562744031494,
            "precision": 0.7535934291581109,
            "recall": 0.7742616033755274
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(11)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.7613132376688531,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5514980886381987,
            "fpr": 0.47368421052631576,
            "logloss": 0.6915595604158608,
            "mae": 0.4982786362892703,
            "precision": 0.5263157894736842,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5203073545554336,
            "auc_prc": 0.7563115599576609,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5388702004685715,
            "fpr": 0.4796926454445664,
            "logloss": 0.692359592011484,
            "mae": 0.49867535507639726,
            "precision": 0.5203073545554336,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(12)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6162280701754386,
            "auc_prc": 0.5132705273401104,
            "auditor_fn_violation": 0.006578947368421055,
            "auditor_fp_violation": 0.019442413905133203,
            "ave_precision_score": 0.5147269709930553,
            "fpr": 0.33771929824561403,
            "logloss": 0.683087478896127,
            "mae": 0.4783835350617505,
            "precision": 0.5871313672922251,
            "recall": 0.9125
        },
        "train": {
            "accuracy": 0.6344676180021954,
            "auc_prc": 0.5277330752740219,
            "auditor_fn_violation": 0.0046478344842918505,
            "auditor_fp_violation": 0.037427123863684894,
            "ave_precision_score": 0.529516352940934,
            "fpr": 0.30954994511525796,
            "logloss": 0.6716870804814489,
            "mae": 0.47442612002893025,
            "precision": 0.6,
            "recall": 0.8924050632911392
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(13)",
        "seed": 10197,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.7114037371021198,
            "auditor_fn_violation": 0.0006373355263157887,
            "auditor_fp_violation": 0.01783828784925277,
            "ave_precision_score": 0.7124260758618246,
            "fpr": 0.32785087719298245,
            "logloss": 0.9865536805374802,
            "mae": 0.40322370270446856,
            "precision": 0.5904109589041096,
            "recall": 0.8979166666666667
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.6983597783441559,
            "auditor_fn_violation": 0.012496121015066676,
            "auditor_fp_violation": 0.014548852444192144,
            "ave_precision_score": 0.6985456979668603,
            "fpr": 0.3040614709110867,
            "logloss": 0.9487497630600583,
            "mae": 0.39935871296685954,
            "precision": 0.5979680696661829,
            "recall": 0.869198312236287
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(14)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0.5645521205975326,
            "auditor_fn_violation": 0.00973135964912282,
            "auditor_fp_violation": 0.006274366471734895,
            "ave_precision_score": 0.5489339619750303,
            "fpr": 0.05701754385964912,
            "logloss": 9.951054970202023,
            "mae": 0.5100545768507663,
            "precision": 0.59375,
            "recall": 0.15833333333333333
        },
        "train": {
            "accuracy": 0.4961580680570801,
            "auc_prc": 0.5629187114154025,
            "auditor_fn_violation": 0.005356472925843067,
            "auditor_fp_violation": 0.004104424187467189,
            "ave_precision_score": 0.5443512280315924,
            "fpr": 0.06366630076838639,
            "logloss": 9.324174205805923,
            "mae": 0.5034934811999282,
            "precision": 0.5572519083969466,
            "recall": 0.1540084388185654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(15)",
        "seed": 10197,
        "test": {
            "accuracy": 0.4956140350877193,
            "auc_prc": 0.5403649414468603,
            "auditor_fn_violation": 0.012774122807017554,
            "auditor_fp_violation": 0.004314896036387264,
            "ave_precision_score": 0.554448795315027,
            "fpr": 0.05701754385964912,
            "logloss": 0.732175823475545,
            "mae": 0.5007067499355528,
            "precision": 0.5806451612903226,
            "recall": 0.15
        },
        "train": {
            "accuracy": 0.4862788144895719,
            "auc_prc": 0.5437255817795228,
            "auditor_fn_violation": 0.0035848768219650198,
            "auditor_fp_violation": 0.006357587281811172,
            "ave_precision_score": 0.55506626847811,
            "fpr": 0.05598243688254665,
            "logloss": 0.7269241690878953,
            "mae": 0.49744733675709146,
            "precision": 0.5277777777777778,
            "recall": 0.12025316455696203
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(16)",
        "seed": 10197,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.7847513730450837,
            "auditor_fn_violation": 0.004687500000000001,
            "auditor_fp_violation": 0.014571657732293708,
            "ave_precision_score": 0.7789612490970649,
            "fpr": 0.20942982456140352,
            "logloss": 1.2456245126080379,
            "mae": 0.30671677889495436,
            "precision": 0.6904376012965965,
            "recall": 0.8875
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.807312476192515,
            "auditor_fn_violation": 0.0056876340276137425,
            "auditor_fp_violation": 0.015812331860529962,
            "ave_precision_score": 0.8025747952299374,
            "fpr": 0.1964873765093304,
            "logloss": 1.1202199045745835,
            "mae": 0.2951424884464294,
            "precision": 0.7075163398692811,
            "recall": 0.9135021097046413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(17)",
        "seed": 10197,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7030511416583297,
            "auditor_fn_violation": 0.01841191520467837,
            "auditor_fp_violation": 0.015515858512020798,
            "ave_precision_score": 0.6717802263912368,
            "fpr": 0.16557017543859648,
            "logloss": 0.6431206924630702,
            "mae": 0.4135035654701489,
            "precision": 0.6924643584521385,
            "recall": 0.7083333333333334
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7423872188872105,
            "auditor_fn_violation": 0.012093169744380683,
            "auditor_fp_violation": 0.013853059604578671,
            "ave_precision_score": 0.7073428336878701,
            "fpr": 0.14709110867178923,
            "logloss": 0.6067572392824805,
            "mae": 0.3941159997294881,
            "precision": 0.7214137214137214,
            "recall": 0.7320675105485233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(18)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.8043799430088205,
            "auditor_fn_violation": 0.0006921600877193004,
            "auditor_fp_violation": 0.017462638076673167,
            "ave_precision_score": 0.8049241966133094,
            "fpr": 0.21820175438596492,
            "logloss": 0.5651231102172831,
            "mae": 0.3809781247635552,
            "precision": 0.6726973684210527,
            "recall": 0.8520833333333333
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.7852941110055962,
            "auditor_fn_violation": 0.005530158818380138,
            "auditor_fp_violation": 0.006420384469501918,
            "ave_precision_score": 0.7856220580166764,
            "fpr": 0.20417124039517015,
            "logloss": 0.5780539192371892,
            "mae": 0.39127872320368445,
            "precision": 0.6702127659574468,
            "recall": 0.7974683544303798
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(19)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5778508771929824,
            "auc_prc": 0.577258540336018,
            "auditor_fn_violation": 0.008011239035087724,
            "auditor_fp_violation": 0.017147904483430804,
            "ave_precision_score": 0.5789043150273078,
            "fpr": 0.2149122807017544,
            "logloss": 0.774596769528607,
            "mae": 0.46240025923358635,
            "precision": 0.5975359342915811,
            "recall": 0.60625
        },
        "train": {
            "accuracy": 0.535675082327113,
            "auc_prc": 0.5310828470736313,
            "auditor_fn_violation": 0.01586331151838523,
            "auditor_fp_violation": 0.012368534087569437,
            "ave_precision_score": 0.5327149602690873,
            "fpr": 0.23929747530186607,
            "logloss": 0.841705597017608,
            "mae": 0.48198435970599446,
            "precision": 0.5523613963039015,
            "recall": 0.5675105485232067
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(20)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8408614401591659,
            "auditor_fn_violation": 0.0011992872807017518,
            "auditor_fp_violation": 0.010157772904483437,
            "ave_precision_score": 0.8412952211719529,
            "fpr": 0.10307017543859649,
            "logloss": 0.5074079019505802,
            "mae": 0.31186220898566636,
            "precision": 0.7887640449438202,
            "recall": 0.73125
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8599877899414418,
            "auditor_fn_violation": 0.010636524058969834,
            "auditor_fp_violation": 0.011333636434425922,
            "ave_precision_score": 0.8602409089646127,
            "fpr": 0.06586169045005488,
            "logloss": 0.5066957174592525,
            "mae": 0.3186714625574902,
            "precision": 0.841688654353562,
            "recall": 0.6729957805907173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(21)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8109387058305211,
            "auditor_fn_violation": 0.009089455409356734,
            "auditor_fp_violation": 0.011665448343079922,
            "ave_precision_score": 0.8113059724297725,
            "fpr": 0.11293859649122807,
            "logloss": 1.0800213845953586,
            "mae": 0.30616625662003516,
            "precision": 0.7770562770562771,
            "recall": 0.7479166666666667
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8173538534617816,
            "auditor_fn_violation": 0.006176270338618022,
            "auditor_fp_violation": 0.01493819500787477,
            "ave_precision_score": 0.8177844448217824,
            "fpr": 0.08342480790340286,
            "logloss": 1.0973893393595118,
            "mae": 0.313389440830654,
            "precision": 0.8085642317380353,
            "recall": 0.6772151898734177
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(22)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.7934334415837087,
            "auditor_fn_violation": 0.005267726608187139,
            "auditor_fp_violation": 0.013320337881741393,
            "ave_precision_score": 0.7950620163750832,
            "fpr": 0.09758771929824561,
            "logloss": 0.5502798533381396,
            "mae": 0.35259005299610036,
            "precision": 0.7915690866510539,
            "recall": 0.7041666666666667
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8042488712998865,
            "auditor_fn_violation": 0.004654781919993343,
            "auditor_fp_violation": 0.008399751825514248,
            "ave_precision_score": 0.7994825354972985,
            "fpr": 0.06256860592755215,
            "logloss": 0.5756654122430158,
            "mae": 0.36266474255271725,
            "precision": 0.836676217765043,
            "recall": 0.6160337552742616
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(23)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5942982456140351,
            "auc_prc": 0.7688377563547301,
            "auditor_fn_violation": 0.026644736842105263,
            "auditor_fp_violation": 0.07039372157244965,
            "ave_precision_score": 0.7693153533514876,
            "fpr": 0.36622807017543857,
            "logloss": 0.7589546333054822,
            "mae": 0.3987462069969951,
            "precision": 0.570694087403599,
            "recall": 0.925
        },
        "train": {
            "accuracy": 0.579582875960483,
            "auc_prc": 0.7224817858219956,
            "auditor_fn_violation": 0.03951701427003293,
            "auditor_fp_violation": 0.07894360058979119,
            "ave_precision_score": 0.7229694948915759,
            "fpr": 0.3611416026344676,
            "logloss": 0.7857650403251166,
            "mae": 0.415354423331246,
            "precision": 0.5607476635514018,
            "recall": 0.8860759493670886
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(24)",
        "seed": 10197,
        "test": {
            "accuracy": 0.42214912280701755,
            "auc_prc": 0.4273420563693834,
            "auditor_fn_violation": 0.02915752923976609,
            "auditor_fp_violation": 0.015503167641325546,
            "ave_precision_score": 0.4286483723657155,
            "fpr": 0.3300438596491228,
            "logloss": 1.2626979091931578,
            "mae": 0.5616890741256882,
            "precision": 0.45765765765765765,
            "recall": 0.5291666666666667
        },
        "train": {
            "accuracy": 0.40175631174533477,
            "auc_prc": 0.4086566171596805,
            "auditor_fn_violation": 0.012936125276160582,
            "auditor_fp_violation": 0.017620890866023455,
            "ave_precision_score": 0.40985750392917375,
            "fpr": 0.32821075740944017,
            "logloss": 1.3350413227843891,
            "mae": 0.5801716753732491,
            "precision": 0.43263757115749524,
            "recall": 0.4810126582278481
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(25)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5241228070175439,
            "auc_prc": 0.7629537699847402,
            "auditor_fn_violation": 0.0027412280701754384,
            "auditor_fp_violation": 0.002375730994152063,
            "ave_precision_score": 0.5415196766347107,
            "fpr": 0.46710526315789475,
            "logloss": 0.6948762166761311,
            "mae": 0.49503808579685393,
            "precision": 0.5256124721603563,
            "recall": 0.9833333333333333
        },
        "train": {
            "accuracy": 0.5225027442371021,
            "auc_prc": 0.7603184200347749,
            "auditor_fn_violation": 0.0020564409676388446,
            "auditor_fp_violation": 0.003255406209888311,
            "ave_precision_score": 0.5325947975988247,
            "fpr": 0.47310647639956094,
            "logloss": 0.6963631322189073,
            "mae": 0.49571505388496473,
            "precision": 0.5216426193118757,
            "recall": 0.9915611814345991
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(26)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.7903919378111803,
            "auditor_fn_violation": 0.0007264254385964957,
            "auditor_fp_violation": 0.010322754223521765,
            "ave_precision_score": 0.7636101909958385,
            "fpr": 0.11293859649122807,
            "logloss": 0.5441909602517957,
            "mae": 0.354698075488079,
            "precision": 0.7765726681127982,
            "recall": 0.7458333333333333
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7954877501030809,
            "auditor_fn_violation": 0.010571681325756003,
            "auditor_fp_violation": 0.00870117832642983,
            "ave_precision_score": 0.7520970604386082,
            "fpr": 0.09659714599341383,
            "logloss": 0.5553281528236942,
            "mae": 0.3719319473015621,
            "precision": 0.7805486284289277,
            "recall": 0.6603375527426161
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(27)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.7730495604643299,
            "auditor_fn_violation": 0.0028965643274853807,
            "auditor_fp_violation": 0.015899122807017548,
            "ave_precision_score": 0.6429674292798377,
            "fpr": 0.15460526315789475,
            "logloss": 0.6360389250515093,
            "mae": 0.45783627660650955,
            "precision": 0.6980728051391863,
            "recall": 0.6791666666666667
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7762524739905556,
            "auditor_fn_violation": 0.00990009587461269,
            "auditor_fp_violation": 0.01262474661334767,
            "ave_precision_score": 0.6431737992271145,
            "fpr": 0.1602634467618002,
            "logloss": 0.6325608222104193,
            "mae": 0.4560297327821524,
            "precision": 0.6951983298538622,
            "recall": 0.7025316455696202
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(28)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6326754385964912,
            "auc_prc": 0.7437060552150803,
            "auditor_fn_violation": 0.06572322733918129,
            "auditor_fp_violation": 0.03192007797270955,
            "ave_precision_score": 0.7443319148539005,
            "fpr": 0.10526315789473684,
            "logloss": 0.6264208489489494,
            "mae": 0.42631544850832015,
            "precision": 0.7151335311572701,
            "recall": 0.5020833333333333
        },
        "train": {
            "accuracy": 0.6300768386388584,
            "auc_prc": 0.7453406409725153,
            "auditor_fn_violation": 0.053127040809237316,
            "auditor_fp_violation": 0.024279904648750215,
            "ave_precision_score": 0.7458483593249513,
            "fpr": 0.07683863885839737,
            "logloss": 0.6371452547759868,
            "mae": 0.42862051251854516,
            "precision": 0.7472924187725631,
            "recall": 0.43670886075949367
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(29)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5570175438596491,
            "auc_prc": 0.7178922385076615,
            "auditor_fn_violation": 0.08962445175438598,
            "auditor_fp_violation": 0.1040397579597141,
            "ave_precision_score": 0.5512512491672218,
            "fpr": 0.3048245614035088,
            "logloss": 0.6867402170562881,
            "mae": 0.49359473546868876,
            "precision": 0.560126582278481,
            "recall": 0.7375
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7225354649465978,
            "auditor_fn_violation": 0.09219015594677338,
            "auditor_fp_violation": 0.10481855380588637,
            "ave_precision_score": 0.5554708919049447,
            "fpr": 0.29527991218441274,
            "logloss": 0.683176147989335,
            "mae": 0.49181094818005316,
            "precision": 0.567524115755627,
            "recall": 0.7447257383966245
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(30)",
        "seed": 10197,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.8130127493446881,
            "auditor_fn_violation": 0.022765899122807028,
            "auditor_fp_violation": 0.017845902371669917,
            "ave_precision_score": 0.8134813279959965,
            "fpr": 0.10635964912280702,
            "logloss": 1.0330455506905858,
            "mae": 0.3132586488459507,
            "precision": 0.7610837438423645,
            "recall": 0.64375
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.8103896790008325,
            "auditor_fn_violation": 0.0198696661062402,
            "auditor_fp_violation": 0.02367202787190379,
            "ave_precision_score": 0.8107031108676275,
            "fpr": 0.09989023051591657,
            "logloss": 1.0820649930276667,
            "mae": 0.3070243721011459,
            "precision": 0.7736318407960199,
            "recall": 0.6561181434599156
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(31)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6491228070175439,
            "auc_prc": 0.7156222180624062,
            "auditor_fn_violation": 0.06704130116959066,
            "auditor_fp_violation": 0.027899610136452247,
            "ave_precision_score": 0.7165174224601905,
            "fpr": 0.0800438596491228,
            "logloss": 0.9979019846494342,
            "mae": 0.4304834390070602,
            "precision": 0.761437908496732,
            "recall": 0.48541666666666666
        },
        "train": {
            "accuracy": 0.6564215148188803,
            "auc_prc": 0.7187730914000497,
            "auditor_fn_violation": 0.06647769641558636,
            "auditor_fp_violation": 0.0193239505961965,
            "ave_precision_score": 0.7194663879529382,
            "fpr": 0.06147091108671789,
            "logloss": 0.9691402457551949,
            "mae": 0.421309411507197,
            "precision": 0.7948717948717948,
            "recall": 0.4578059071729958
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(32)",
        "seed": 10197,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8328669046693525,
            "auditor_fn_violation": 0.015805464181286557,
            "auditor_fp_violation": 0.01275940139701105,
            "ave_precision_score": 0.8120575159534817,
            "fpr": 0.06469298245614036,
            "logloss": 0.5230310877352,
            "mae": 0.31748500219832193,
            "precision": 0.8405405405405405,
            "recall": 0.6479166666666667
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8498256360614871,
            "auditor_fn_violation": 0.006822381858855894,
            "auditor_fp_violation": 0.01103220993351034,
            "ave_precision_score": 0.8317782624218722,
            "fpr": 0.048298572996706916,
            "logloss": 0.509800178567275,
            "mae": 0.3163440548799499,
            "precision": 0.8658536585365854,
            "recall": 0.5991561181434599
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(33)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8066012311485113,
            "auditor_fn_violation": 0.035302448830409366,
            "auditor_fp_violation": 0.017787524366471737,
            "ave_precision_score": 0.7453817752819671,
            "fpr": 0.09649122807017543,
            "logloss": 0.5830214378164935,
            "mae": 0.39136294331074806,
            "precision": 0.7810945273631841,
            "recall": 0.6541666666666667
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7855275547782851,
            "auditor_fn_violation": 0.03256031532094837,
            "auditor_fp_violation": 0.01921845132087605,
            "ave_precision_score": 0.7281554816476178,
            "fpr": 0.07135016465422613,
            "logloss": 0.5832398373146903,
            "mae": 0.3938602281225238,
            "precision": 0.8104956268221575,
            "recall": 0.5864978902953587
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(34)",
        "seed": 10197,
        "test": {
            "accuracy": 0.4956140350877193,
            "auc_prc": 0.57079662567558,
            "auditor_fn_violation": 0.012431469298245632,
            "auditor_fp_violation": 0.01350308641975309,
            "ave_precision_score": 0.5701967051290564,
            "fpr": 0.05043859649122807,
            "logloss": 2.102424527358745,
            "mae": 0.4899001566268373,
            "precision": 0.5892857142857143,
            "recall": 0.1375
        },
        "train": {
            "accuracy": 0.49396267837541163,
            "auc_prc": 0.5680435861738325,
            "auditor_fn_violation": 0.00010189572362176243,
            "auditor_fp_violation": 0.007744149186022855,
            "ave_precision_score": 0.5697688805072105,
            "fpr": 0.04061470911086718,
            "logloss": 2.4307982556223555,
            "mae": 0.4842501617272913,
            "precision": 0.5747126436781609,
            "recall": 0.10548523206751055
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(35)",
        "seed": 10197,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.178303365742465,
            "mae": 0.5263157894736842,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.97077937561763,
            "mae": 0.5203073545554336,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(36)",
        "seed": 10197,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.178303365742465,
            "mae": 0.5263157894736842,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.97077937561763,
            "mae": 0.5203073545554336,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(37)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7707167455439803,
            "auditor_fn_violation": 0.019188596491228074,
            "auditor_fp_violation": 0.021076998050682267,
            "ave_precision_score": 0.7317171860926721,
            "fpr": 0.13596491228070176,
            "logloss": 4.247217749961011,
            "mae": 0.3072286553398516,
            "precision": 0.7262693156732892,
            "recall": 0.6854166666666667
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.7883212729390668,
            "auditor_fn_violation": 0.011435479164640334,
            "auditor_fp_violation": 0.016234328961811786,
            "ave_precision_score": 0.756183621130986,
            "fpr": 0.12623490669593854,
            "logloss": 3.5378879140074453,
            "mae": 0.2898359674395238,
            "precision": 0.7510822510822511,
            "recall": 0.7320675105485233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(38)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8439323467249209,
            "auditor_fn_violation": 0.0038171600877193114,
            "auditor_fp_violation": 0.010777087394411959,
            "ave_precision_score": 0.7888639450178488,
            "fpr": 0.10307017543859649,
            "logloss": 5.373719566074895,
            "mae": 0.2399294017348265,
            "precision": 0.7915742793791575,
            "recall": 0.74375
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8407516246199263,
            "auditor_fn_violation": 0.008346186089381091,
            "auditor_fp_violation": 0.008143539299736007,
            "ave_precision_score": 0.8043123253465316,
            "fpr": 0.07244785949506037,
            "logloss": 5.422207437323706,
            "mae": 0.2498151579149666,
            "precision": 0.8290155440414507,
            "recall": 0.6751054852320675
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(39)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5361842105263158,
            "auc_prc": 0.6152681190601277,
            "auditor_fn_violation": 0.0002764071637426901,
            "auditor_fp_violation": 0.003898635477582865,
            "ave_precision_score": 0.5557233169030512,
            "fpr": 0.45614035087719296,
            "logloss": 6.097819128446704,
            "mae": 0.4540095712127687,
            "precision": 0.532058492688414,
            "recall": 0.9854166666666667
        },
        "train": {
            "accuracy": 0.5345773874862788,
            "auc_prc": 0.5993005078005456,
            "auditor_fn_violation": 0.0013686448331920687,
            "auditor_fp_violation": 0.00048730617648019706,
            "ave_precision_score": 0.5351447349598897,
            "fpr": 0.4621295279912184,
            "logloss": 6.499126572198503,
            "mae": 0.4622643366558263,
            "precision": 0.5280269058295964,
            "recall": 0.9936708860759493
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(40)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.4621066587275035,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.482791973382661,
            "fpr": 0.47368421052631576,
            "logloss": 7.22598237842261,
            "mae": 0.47368532458418294,
            "precision": 0.5263157894736842,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5203073545554336,
            "auc_prc": 0.46677841411745435,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.4716781575102689,
            "fpr": 0.4796926454445664,
            "logloss": 8.003601870546243,
            "mae": 0.4796937656978352,
            "precision": 0.5203073545554336,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(41)",
        "seed": 10197,
        "test": {
            "accuracy": 0.3399122807017544,
            "auc_prc": 0.38814817942809177,
            "auditor_fn_violation": 0.016008771929824587,
            "auditor_fp_violation": 0.012274610136452247,
            "ave_precision_score": 0.3851847573543864,
            "fpr": 0.3574561403508772,
            "logloss": 6.5268876890930185,
            "mae": 0.6572281718723597,
            "precision": 0.3849056603773585,
            "recall": 0.425
        },
        "train": {
            "accuracy": 0.3896816684961581,
            "auc_prc": 0.40150130667151623,
            "auditor_fn_violation": 0.0185126003325506,
            "auditor_fp_violation": 0.030240613704355882,
            "ave_precision_score": 0.39167388500152206,
            "fpr": 0.3578485181119649,
            "logloss": 6.177248750734404,
            "mae": 0.629917547437865,
            "precision": 0.4280701754385965,
            "recall": 0.5147679324894515
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(42)",
        "seed": 10197,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7577253072169925,
            "auditor_fn_violation": 0.006133497807017554,
            "auditor_fp_violation": 0.02639193469785575,
            "ave_precision_score": 0.7579148768093824,
            "fpr": 0.14692982456140352,
            "logloss": 0.7491212898596229,
            "mae": 0.37873158925172073,
            "precision": 0.7142857142857143,
            "recall": 0.6979166666666666
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7437319207549429,
            "auditor_fn_violation": 0.00858703052703247,
            "auditor_fp_violation": 0.020401550336969717,
            "ave_precision_score": 0.745179226604806,
            "fpr": 0.12843029637760703,
            "logloss": 0.7361907187758593,
            "mae": 0.38598561523110825,
            "precision": 0.71875,
            "recall": 0.630801687763713
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(43)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8410019416275127,
            "auditor_fn_violation": 0.0022409539473684245,
            "auditor_fp_violation": 0.006472344054580901,
            "ave_precision_score": 0.8414620934268494,
            "fpr": 0.14144736842105263,
            "logloss": 0.49979954494910583,
            "mae": 0.3218209694278541,
            "precision": 0.7556818181818182,
            "recall": 0.83125
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8592423113451855,
            "auditor_fn_violation": 0.002834553766204892,
            "auditor_fp_violation": 0.014722172682218607,
            "ave_precision_score": 0.8594613721216698,
            "fpr": 0.11086717892425905,
            "logloss": 0.49033770688893186,
            "mae": 0.3279772581150254,
            "precision": 0.7837259100642399,
            "recall": 0.7721518987341772
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(44)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7831610119889924,
            "auditor_fn_violation": 0.011225328947368421,
            "auditor_fp_violation": 0.018960160818713455,
            "ave_precision_score": 0.7375215130047783,
            "fpr": 0.13815789473684212,
            "logloss": 4.55912015161377,
            "mae": 0.29158222758710006,
            "precision": 0.7396694214876033,
            "recall": 0.7458333333333333
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.7971063569925088,
            "auditor_fn_violation": 0.012528542381673594,
            "auditor_fp_violation": 0.016508124700143435,
            "ave_precision_score": 0.7571421704327076,
            "fpr": 0.132821075740944,
            "logloss": 3.8611212098939958,
            "mae": 0.27639328673380914,
            "precision": 0.7489626556016598,
            "recall": 0.7616033755274262
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(45)",
        "seed": 10197,
        "test": {
            "accuracy": 0.3991228070175439,
            "auc_prc": 0.5729118993135012,
            "auditor_fn_violation": 0.017589546783625738,
            "auditor_fp_violation": 0.02363547758284602,
            "ave_precision_score": 0.4853890160183066,
            "fpr": 0.2894736842105263,
            "logloss": 0.7907547936489537,
            "mae": 0.5271013911094582,
            "precision": 0.4260869565217391,
            "recall": 0.4083333333333333
        },
        "train": {
            "accuracy": 0.37980241492864986,
            "auc_prc": 0.5478561379172524,
            "auditor_fn_violation": 0.022356848087370974,
            "auditor_fp_violation": 0.02082103555074391,
            "ave_precision_score": 0.47463046882247534,
            "fpr": 0.29308452250274425,
            "logloss": 0.8019626183840142,
            "mae": 0.5327557019815748,
            "precision": 0.3972911963882618,
            "recall": 0.37130801687763715
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(46)",
        "seed": 10197,
        "test": {
            "accuracy": 0.631578947368421,
            "auc_prc": 0.720535862407207,
            "auditor_fn_violation": 0.014601608187134508,
            "auditor_fp_violation": 0.025094927712800526,
            "ave_precision_score": 0.6706345150220259,
            "fpr": 0.18311403508771928,
            "logloss": 0.6300012445563032,
            "mae": 0.4430676371251282,
            "precision": 0.6506276150627615,
            "recall": 0.6479166666666667
        },
        "train": {
            "accuracy": 0.6114160263446762,
            "auc_prc": 0.6874859777134446,
            "auditor_fn_violation": 0.011847693682928293,
            "auditor_fp_violation": 0.035460315945210714,
            "ave_precision_score": 0.6449310494713473,
            "fpr": 0.19209659714599342,
            "logloss": 0.6409694921697637,
            "mae": 0.44580965135021605,
            "precision": 0.6276595744680851,
            "recall": 0.6223628691983122
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(47)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7796052631578947,
            "auc_prc": 0.8565766497230196,
            "auditor_fn_violation": 0.005765716374269013,
            "auditor_fp_violation": 0.016861090805718005,
            "ave_precision_score": 0.8547159033432603,
            "fpr": 0.09100877192982457,
            "logloss": 0.4966574936403727,
            "mae": 0.2834836860859293,
            "precision": 0.8134831460674158,
            "recall": 0.7541666666666667
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8680263414898034,
            "auditor_fn_violation": 0.01294770433566304,
            "auditor_fp_violation": 0.0061892908187999715,
            "ave_precision_score": 0.8648493447994847,
            "fpr": 0.06805708013172337,
            "logloss": 0.5077194715871574,
            "mae": 0.29065595734578226,
            "precision": 0.8414322250639387,
            "recall": 0.6940928270042194
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(48)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8268383981198666,
            "auditor_fn_violation": 0.002316337719298253,
            "auditor_fp_violation": 0.015107212475633524,
            "ave_precision_score": 0.8243634162848195,
            "fpr": 0.10526315789473684,
            "logloss": 0.5251069030374076,
            "mae": 0.30888973747935605,
            "precision": 0.7922077922077922,
            "recall": 0.7625
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8480442833069853,
            "auditor_fn_violation": 0.011166844984183017,
            "auditor_fp_violation": 0.01030627444380531,
            "ave_precision_score": 0.842013259100952,
            "fpr": 0.07903402854006586,
            "logloss": 0.5267256354713034,
            "mae": 0.31962726296270194,
            "precision": 0.817258883248731,
            "recall": 0.679324894514768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(49)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7719280485414206,
            "auditor_fn_violation": 0.021498081140350882,
            "auditor_fp_violation": 0.018010883690708253,
            "ave_precision_score": 0.7726477471512809,
            "fpr": 0.11403508771929824,
            "logloss": 0.652721996884975,
            "mae": 0.3299425941231715,
            "precision": 0.7609195402298851,
            "recall": 0.6895833333333333
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8122423108495502,
            "auditor_fn_violation": 0.021062309234994702,
            "auditor_fp_violation": 0.019718316934894394,
            "ave_precision_score": 0.8125572496443514,
            "fpr": 0.0867178924259056,
            "logloss": 0.6320283399615332,
            "mae": 0.32398042740111427,
            "precision": 0.7942708333333334,
            "recall": 0.6434599156118144
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(50)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5350877192982456,
            "auc_prc": 0.6562830008409405,
            "auditor_fn_violation": 0.004769736842105265,
            "auditor_fp_violation": 0.0014797555230669295,
            "ave_precision_score": 0.5471832651313758,
            "fpr": 0.42214912280701755,
            "logloss": 0.7106821421470022,
            "mae": 0.4912988772909892,
            "precision": 0.5338983050847458,
            "recall": 0.91875
        },
        "train": {
            "accuracy": 0.5290889132821076,
            "auc_prc": 0.5963991788260178,
            "auditor_fn_violation": 0.002709499923578207,
            "auditor_fp_violation": 0.0016980359551577867,
            "ave_precision_score": 0.5344836803156257,
            "fpr": 0.4127332601536773,
            "logloss": 0.7266697336328329,
            "mae": 0.4938179732808428,
            "precision": 0.5282308657465495,
            "recall": 0.8881856540084389
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(51)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7776472971564172,
            "auditor_fn_violation": 0.013473135964912283,
            "auditor_fp_violation": 0.02305931205328135,
            "ave_precision_score": 0.7714209077528547,
            "fpr": 0.14583333333333334,
            "logloss": 1.3316118184129995,
            "mae": 0.2949959086253218,
            "precision": 0.7268993839835729,
            "recall": 0.7375
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8061167777962759,
            "auditor_fn_violation": 0.013186232961413948,
            "auditor_fp_violation": 0.019761019022524095,
            "ave_precision_score": 0.8034810182625474,
            "fpr": 0.141602634467618,
            "logloss": 1.064891547234481,
            "mae": 0.27429970523855046,
            "precision": 0.7372708757637475,
            "recall": 0.7637130801687764
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(52)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.6881519898799908,
            "auditor_fn_violation": 0.009231085526315803,
            "auditor_fp_violation": 0.010949683235867449,
            "ave_precision_score": 0.6872993115410385,
            "fpr": 0.08552631578947369,
            "logloss": 6.223009286474015,
            "mae": 0.4089945946103285,
            "precision": 0.7263157894736842,
            "recall": 0.43125
        },
        "train": {
            "accuracy": 0.6201975850713501,
            "auc_prc": 0.6929350084841394,
            "auditor_fn_violation": 0.015603940585529875,
            "auditor_fp_violation": 0.010806140057823653,
            "ave_precision_score": 0.6923344857664986,
            "fpr": 0.09001097694840834,
            "logloss": 6.106652472520955,
            "mae": 0.4000888798576672,
            "precision": 0.7191780821917808,
            "recall": 0.4430379746835443
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(53)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6129385964912281,
            "auc_prc": 0.7221307072723704,
            "auditor_fn_violation": 0.06624862938596492,
            "auditor_fp_violation": 0.08429783950617283,
            "ave_precision_score": 0.6763166200991446,
            "fpr": 0.2850877192982456,
            "logloss": 0.6337286894129998,
            "mae": 0.43942772085664045,
            "precision": 0.598145285935085,
            "recall": 0.80625
        },
        "train": {
            "accuracy": 0.6092206366630076,
            "auc_prc": 0.7265696408730316,
            "auditor_fn_violation": 0.07607905255503528,
            "auditor_fp_violation": 0.09469313526263042,
            "ave_precision_score": 0.6770506208406284,
            "fpr": 0.27661909989023054,
            "logloss": 0.6407659072585834,
            "mae": 0.4445410064461464,
            "precision": 0.594855305466238,
            "recall": 0.7805907172995781
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(54)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8293733422654084,
            "auditor_fn_violation": 0.035012335526315795,
            "auditor_fp_violation": 0.034483633853151396,
            "ave_precision_score": 0.8298402600576619,
            "fpr": 0.12938596491228072,
            "logloss": 0.5291074912995999,
            "mae": 0.31843682629793574,
            "precision": 0.751578947368421,
            "recall": 0.74375
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8663377140956625,
            "auditor_fn_violation": 0.02468423904736762,
            "auditor_fp_violation": 0.028504899436583634,
            "ave_precision_score": 0.8665268266768721,
            "fpr": 0.09001097694840834,
            "logloss": 0.481163926930988,
            "mae": 0.3105513156441773,
            "precision": 0.8079625292740047,
            "recall": 0.7278481012658228
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(55)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7763157894736842,
            "auc_prc": 0.838572445468802,
            "auditor_fn_violation": 0.0029353983918128652,
            "auditor_fp_violation": 0.012662950779727095,
            "ave_precision_score": 0.8365246821504589,
            "fpr": 0.09978070175438597,
            "logloss": 0.506999029665307,
            "mae": 0.30132331706485466,
            "precision": 0.8013100436681223,
            "recall": 0.7645833333333333
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8513265121903775,
            "auditor_fn_violation": 0.007915445075889156,
            "auditor_fp_violation": 0.007497984210275126,
            "ave_precision_score": 0.8483124079541127,
            "fpr": 0.07135016465422613,
            "logloss": 0.5057055426703313,
            "mae": 0.30495515854213423,
            "precision": 0.8307291666666666,
            "recall": 0.6729957805907173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(56)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7660101182726168,
            "auditor_fn_violation": 0.027460252192982457,
            "auditor_fp_violation": 0.028412321312540616,
            "ave_precision_score": 0.736891546009127,
            "fpr": 0.14364035087719298,
            "logloss": 3.203174732227602,
            "mae": 0.2964867369601844,
            "precision": 0.7282157676348547,
            "recall": 0.73125
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7806290385332091,
            "auditor_fn_violation": 0.03759952201642374,
            "auditor_fp_violation": 0.036070704609564776,
            "ave_precision_score": 0.7578746339395663,
            "fpr": 0.1437980241492865,
            "logloss": 3.1532227660426564,
            "mae": 0.28493554369195184,
            "precision": 0.7265135699373695,
            "recall": 0.7341772151898734
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(57)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5219298245614035,
            "auc_prc": 0.6127098753960519,
            "auditor_fn_violation": 0.007076937134502946,
            "auditor_fp_violation": 0.005284478557504874,
            "ave_precision_score": 0.5854639136614467,
            "fpr": 0.025219298245614034,
            "logloss": 0.7157694324866506,
            "mae": 0.4937046459108068,
            "precision": 0.7444444444444445,
            "recall": 0.13958333333333334
        },
        "train": {
            "accuracy": 0.5170142700329309,
            "auc_prc": 0.6561069374421641,
            "auditor_fn_violation": 0.007702390381043691,
            "auditor_fp_violation": 0.000987171790498535,
            "ave_precision_score": 0.6053831975642832,
            "fpr": 0.01646542261251372,
            "logloss": 0.709625631871397,
            "mae": 0.4901788560213556,
            "precision": 0.765625,
            "recall": 0.10337552742616034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(58)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5833333333333334,
            "auc_prc": 0.6431439223333508,
            "auditor_fn_violation": 0.012312682748538013,
            "auditor_fp_violation": 0.020338389376218336,
            "ave_precision_score": 0.6433425081494208,
            "fpr": 0.3892543859649123,
            "logloss": 0.9586410486394442,
            "mae": 0.45984249937409183,
            "precision": 0.5617283950617284,
            "recall": 0.9479166666666666
        },
        "train": {
            "accuracy": 0.605927552140505,
            "auc_prc": 0.6434742768583036,
            "auditor_fn_violation": 0.010337784323806085,
            "auditor_fp_violation": 0.028108021210378125,
            "ave_precision_score": 0.6405981289893038,
            "fpr": 0.3633369923161361,
            "logloss": 1.2007906793154977,
            "mae": 0.45043354258339346,
            "precision": 0.574002574002574,
            "recall": 0.9409282700421941
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(59)",
        "seed": 10197,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8424572838533424,
            "auditor_fn_violation": 0.0218156067251462,
            "auditor_fp_violation": 0.04067170240415855,
            "ave_precision_score": 0.8428938322640266,
            "fpr": 0.16666666666666666,
            "logloss": 0.5390730739967748,
            "mae": 0.3020999962273779,
            "precision": 0.7295373665480427,
            "recall": 0.8541666666666666
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8762523299061339,
            "auditor_fn_violation": 0.03565424002000861,
            "auditor_fp_violation": 0.038562497017133594,
            "ave_precision_score": 0.8764426992916277,
            "fpr": 0.12843029637760703,
            "logloss": 0.48244232218989985,
            "mae": 0.2934754802063353,
            "precision": 0.7692307692307693,
            "recall": 0.8227848101265823
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(60)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.6905473777031627,
            "auditor_fn_violation": 0.017365679824561407,
            "auditor_fp_violation": 0.02271412037037038,
            "ave_precision_score": 0.6908475919628319,
            "fpr": 0.19188596491228072,
            "logloss": 0.6580681255112437,
            "mae": 0.4154150614651786,
            "precision": 0.6647509578544061,
            "recall": 0.7229166666666667
        },
        "train": {
            "accuracy": 0.6355653128430296,
            "auc_prc": 0.7027432398758189,
            "auditor_fn_violation": 0.021259153246536704,
            "auditor_fp_violation": 0.032039125159818854,
            "ave_precision_score": 0.7051514920590258,
            "fpr": 0.1800219538968167,
            "logloss": 0.6717593284398625,
            "mae": 0.4201806061885818,
            "precision": 0.6510638297872341,
            "recall": 0.6455696202531646
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(61)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.5865819318946732,
            "auditor_fn_violation": 0.06434576023391814,
            "auditor_fp_violation": 0.09418402777777778,
            "ave_precision_score": 0.5911536001667816,
            "fpr": 0.32785087719298245,
            "logloss": 0.8398951891482009,
            "mae": 0.4572357818429705,
            "precision": 0.5672937771345875,
            "recall": 0.8166666666666667
        },
        "train": {
            "accuracy": 0.575192096597146,
            "auc_prc": 0.5680341221296631,
            "auditor_fn_violation": 0.07238070094994603,
            "auditor_fp_violation": 0.09979226690311901,
            "ave_precision_score": 0.573787041040119,
            "fpr": 0.3205268935236004,
            "logloss": 0.8583146499777615,
            "mae": 0.4565069752358239,
            "precision": 0.5648286140089419,
            "recall": 0.79957805907173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(62)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.7087510551730838,
            "auditor_fn_violation": 0.0037828947368421123,
            "auditor_fp_violation": 0.004954515919428207,
            "ave_precision_score": 0.7091887766014763,
            "fpr": 0.14912280701754385,
            "logloss": 2.323379528413238,
            "mae": 0.4135572131240473,
            "precision": 0.6625310173697271,
            "recall": 0.55625
        },
        "train": {
            "accuracy": 0.6201975850713501,
            "auc_prc": 0.7084435198907342,
            "auditor_fn_violation": 0.005555632749285579,
            "auditor_fp_violation": 0.014295151805921526,
            "ave_precision_score": 0.7088286417241993,
            "fpr": 0.12403951701427003,
            "logloss": 2.7531426412380613,
            "mae": 0.42138811323541175,
            "precision": 0.6807909604519774,
            "recall": 0.5084388185654009
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(63)",
        "seed": 10197,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7668124369135668,
            "auditor_fn_violation": 0.0074287280701754495,
            "auditor_fp_violation": 0.024282711988304097,
            "ave_precision_score": 0.7251376622666045,
            "fpr": 0.16776315789473684,
            "logloss": 2.780977872812031,
            "mae": 0.3503038903837207,
            "precision": 0.6909090909090909,
            "recall": 0.7125
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7745438579434853,
            "auditor_fn_violation": 0.016217630739160847,
            "auditor_fp_violation": 0.02390312152260574,
            "ave_precision_score": 0.7405404995544304,
            "fpr": 0.1690450054884742,
            "logloss": 2.4571409766573082,
            "mae": 0.33880447411682746,
            "precision": 0.6962524654832347,
            "recall": 0.7447257383966245
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(64)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8467742977808868,
            "auditor_fn_violation": 0.007127192982456143,
            "auditor_fp_violation": 0.012906615497076024,
            "ave_precision_score": 0.8408338539752399,
            "fpr": 0.0668859649122807,
            "logloss": 0.5077946547775541,
            "mae": 0.3256174702801856,
            "precision": 0.8427835051546392,
            "recall": 0.68125
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8578590121243186,
            "auditor_fn_violation": 0.0036636144265818128,
            "auditor_fp_violation": 0.006648966232696239,
            "ave_precision_score": 0.8503812294198868,
            "fpr": 0.048298572996706916,
            "logloss": 0.5091371666893556,
            "mae": 0.32987128957926115,
            "precision": 0.8694362017804155,
            "recall": 0.6181434599156118
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(65)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.7588588676591256,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5256241941155785,
            "fpr": 0.47368421052631576,
            "logloss": 0.6922006716386903,
            "mae": 0.49939441406413126,
            "precision": 0.5263157894736842,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5203073545554336,
            "auc_prc": 0.7587972900549059,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5216313512548593,
            "fpr": 0.4796926454445664,
            "logloss": 0.69247710274367,
            "mae": 0.4995326032099425,
            "precision": 0.5203073545554336,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(66)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7850877192982456,
            "auc_prc": 0.8630442620489083,
            "auditor_fn_violation": 0.005857090643274861,
            "auditor_fp_violation": 0.010949683235867449,
            "ave_precision_score": 0.8635822216774491,
            "fpr": 0.08552631578947369,
            "logloss": 0.4765019150361034,
            "mae": 0.29226041083919035,
            "precision": 0.8227272727272728,
            "recall": 0.7541666666666667
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8786892840216002,
            "auditor_fn_violation": 0.01273696545271807,
            "auditor_fp_violation": 0.010467035244293625,
            "ave_precision_score": 0.878856620069624,
            "fpr": 0.06366630076838639,
            "logloss": 0.4790243374933635,
            "mae": 0.3000575927663523,
            "precision": 0.8497409326424871,
            "recall": 0.6919831223628692
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(67)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6469298245614035,
            "auc_prc": 0.608212076395464,
            "auditor_fn_violation": 0.012815241228070184,
            "auditor_fp_violation": 0.012023330896686167,
            "ave_precision_score": 0.59775081404683,
            "fpr": 0.19407894736842105,
            "logloss": 2.795361373268326,
            "mae": 0.38201304478449566,
            "precision": 0.654296875,
            "recall": 0.6979166666666666
        },
        "train": {
            "accuracy": 0.6465422612513722,
            "auc_prc": 0.5930287137679675,
            "auditor_fn_violation": 0.00948093392062324,
            "auditor_fp_violation": 0.008555488850987307,
            "ave_precision_score": 0.5858174619631716,
            "fpr": 0.2074643249176729,
            "logloss": 2.954911714244817,
            "mae": 0.3864572079276748,
            "precision": 0.6433962264150943,
            "recall": 0.7194092827004219
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(68)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7977278672385004,
            "auditor_fn_violation": 0.00765716374269006,
            "auditor_fp_violation": 0.025026397011046135,
            "ave_precision_score": 0.7842420084921284,
            "fpr": 0.20614035087719298,
            "logloss": 2.4840536687882566,
            "mae": 0.29914355205642157,
            "precision": 0.6824324324324325,
            "recall": 0.8416666666666667
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7746225774377805,
            "auditor_fn_violation": 0.013968977383780983,
            "auditor_fp_violation": 0.013320539452961138,
            "ave_precision_score": 0.7627099964468521,
            "fpr": 0.20636663007683864,
            "logloss": 2.769177584056151,
            "mae": 0.32138649913702544,
            "precision": 0.6654804270462633,
            "recall": 0.7890295358649789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(69)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8152798517975881,
            "auditor_fn_violation": 0.009030062134502928,
            "auditor_fp_violation": 0.012655336257309944,
            "ave_precision_score": 0.6951474980277246,
            "fpr": 0.125,
            "logloss": 0.5789278397100861,
            "mae": 0.4013551122655994,
            "precision": 0.7558886509635975,
            "recall": 0.7354166666666667
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7935759993909073,
            "auditor_fn_violation": 0.004142987489984128,
            "auditor_fp_violation": 0.00520965469082433,
            "ave_precision_score": 0.6730627052730536,
            "fpr": 0.10867178924259056,
            "logloss": 0.6032534075527531,
            "mae": 0.4129788862494292,
            "precision": 0.7561576354679803,
            "recall": 0.6476793248945147
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(70)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.7626179345336488,
            "auditor_fn_violation": 0.008694261695906431,
            "auditor_fp_violation": 0.004005238791423011,
            "ave_precision_score": 0.7630522582506664,
            "fpr": 0.3717105263157895,
            "logloss": 0.7036695726781604,
            "mae": 0.4297978187775893,
            "precision": 0.5597402597402598,
            "recall": 0.8979166666666667
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.7250820040504324,
            "auditor_fn_violation": 0.004305094323018706,
            "auditor_fp_violation": 0.007631114248179539,
            "ave_precision_score": 0.7265294630861928,
            "fpr": 0.3885839736553238,
            "logloss": 0.7368415975093887,
            "mae": 0.43917220373639,
            "precision": 0.5432258064516129,
            "recall": 0.8881856540084389
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(71)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.6955408017947341,
            "auditor_fn_violation": 0.01145833333333334,
            "auditor_fp_violation": 0.016960079597141006,
            "ave_precision_score": 0.6624348935718256,
            "fpr": 0.17324561403508773,
            "logloss": 0.6627853564215165,
            "mae": 0.4379298588783856,
            "precision": 0.6814516129032258,
            "recall": 0.7041666666666667
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.6908176514819261,
            "auditor_fn_violation": 0.010944527041735569,
            "auditor_fp_violation": 0.022431155443134645,
            "ave_precision_score": 0.6637523372524669,
            "fpr": 0.1778265642151482,
            "logloss": 0.6408510928111214,
            "mae": 0.4311515243987744,
            "precision": 0.68359375,
            "recall": 0.7383966244725738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(72)",
        "seed": 10197,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 7.6686761943125905,
            "mae": 0.5150065559963889,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 7.635745456979897,
            "mae": 0.5058271234136575,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(73)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8052710759035422,
            "auditor_fn_violation": 0.039601608187134506,
            "auditor_fp_violation": 0.05180921052631579,
            "ave_precision_score": 0.8055379518692914,
            "fpr": 0.19078947368421054,
            "logloss": 0.6494891682864949,
            "mae": 0.3834864203082888,
            "precision": 0.6925795053003534,
            "recall": 0.8166666666666667
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7985929174393294,
            "auditor_fn_violation": 0.05249250834850191,
            "auditor_fp_violation": 0.05110937511774473,
            "ave_precision_score": 0.7989075319065394,
            "fpr": 0.17233809001097694,
            "logloss": 0.6446535787781966,
            "mae": 0.38992820092927744,
            "precision": 0.6957364341085271,
            "recall": 0.7573839662447257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(74)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6326754385964912,
            "auc_prc": 0.7402657815384512,
            "auditor_fn_violation": 0.0008680555555555563,
            "auditor_fp_violation": 0.014548814165042244,
            "ave_precision_score": 0.7408558904607639,
            "fpr": 0.2576754385964912,
            "logloss": 0.9053838037899786,
            "mae": 0.38244780982049587,
            "precision": 0.6178861788617886,
            "recall": 0.7916666666666666
        },
        "train": {
            "accuracy": 0.6564215148188803,
            "auc_prc": 0.7234544784496889,
            "auditor_fn_violation": 0.006509747252289183,
            "auditor_fp_violation": 0.01510146769587071,
            "ave_precision_score": 0.7240117722015911,
            "fpr": 0.24368825466520308,
            "logloss": 0.9457484352681558,
            "mae": 0.3897059678074747,
            "precision": 0.6330578512396694,
            "recall": 0.8080168776371308
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(75)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5493421052631579,
            "auc_prc": 0.7533259209055655,
            "auditor_fn_violation": 0.0009594298245614036,
            "auditor_fp_violation": 0.004363121345029247,
            "ave_precision_score": 0.7541036887647781,
            "fpr": 0.44846491228070173,
            "logloss": 0.9320333506865541,
            "mae": 0.4459335394203663,
            "precision": 0.5388951521984217,
            "recall": 0.9958333333333333
        },
        "train": {
            "accuracy": 0.5378704720087816,
            "auc_prc": 0.7379625239678532,
            "auditor_fn_violation": 0.0005928478465265138,
            "auditor_fp_violation": 0.0017256667177417155,
            "ave_precision_score": 0.7384938925214282,
            "fpr": 0.45773874862788144,
            "logloss": 0.9552384800201935,
            "mae": 0.4563714803966288,
            "precision": 0.5298759864712514,
            "recall": 0.9915611814345991
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(76)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8513137579514226,
            "auditor_fn_violation": 0.004084429824561406,
            "auditor_fp_violation": 0.014358451104613387,
            "ave_precision_score": 0.8516559205372045,
            "fpr": 0.18969298245614036,
            "logloss": 0.5262787533864474,
            "mae": 0.3240654891236326,
            "precision": 0.7121464226289518,
            "recall": 0.8916666666666667
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8580857638213455,
            "auditor_fn_violation": 0.004108250311476703,
            "auditor_fp_violation": 0.007251819234527403,
            "ave_precision_score": 0.8582599073418153,
            "fpr": 0.15916575192096596,
            "logloss": 0.5050418646259276,
            "mae": 0.3288801472928502,
            "precision": 0.7368421052631579,
            "recall": 0.8565400843881856
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(77)",
        "seed": 10197,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.8583038956973366,
            "mae": 0.5193790738193089,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.8528009061760525,
            "mae": 0.5178706879869643,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(78)",
        "seed": 10197,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.6528766487129285,
            "auditor_fn_violation": 0.020712262426900586,
            "auditor_fp_violation": 0.009594298245614037,
            "ave_precision_score": 0.6488354561415568,
            "fpr": 0.19736842105263158,
            "logloss": 1.574605845276994,
            "mae": 0.4115446737644481,
            "precision": 0.6257796257796258,
            "recall": 0.6270833333333333
        },
        "train": {
            "accuracy": 0.6037321624588364,
            "auc_prc": 0.6705594794325037,
            "auditor_fn_violation": 0.02252590235610703,
            "auditor_fp_violation": 0.02010263572356176,
            "ave_precision_score": 0.6683365736905189,
            "fpr": 0.19758507135016465,
            "logloss": 1.4868211623931205,
            "mae": 0.40465636520953596,
            "precision": 0.6194503171247357,
            "recall": 0.6181434599156118
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(79)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.6544823335928183,
            "auditor_fn_violation": 0.01888706140350877,
            "auditor_fp_violation": 0.012000487329434713,
            "ave_precision_score": 0.6562299455882414,
            "fpr": 0.3026315789473684,
            "logloss": 0.6542272795169497,
            "mae": 0.47108828385040524,
            "precision": 0.5941176470588235,
            "recall": 0.8416666666666667
        },
        "train": {
            "accuracy": 0.605927552140505,
            "auc_prc": 0.652751604655788,
            "auditor_fn_violation": 0.014121820969213602,
            "auditor_fp_violation": 0.00847259656323552,
            "ave_precision_score": 0.6538600673747104,
            "fpr": 0.3040614709110867,
            "logloss": 0.6581588931133515,
            "mae": 0.47180853435017256,
            "precision": 0.585949177877429,
            "recall": 0.8270042194092827
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(80)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8545186173231141,
            "auditor_fn_violation": 0.009649122807017544,
            "auditor_fp_violation": 0.005254020467836257,
            "ave_precision_score": 0.8547551330374916,
            "fpr": 0.049342105263157895,
            "logloss": 0.6282683751096515,
            "mae": 0.33755163853652304,
            "precision": 0.8598130841121495,
            "recall": 0.575
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.861671630770255,
            "auditor_fn_violation": 0.026789312064916845,
            "auditor_fp_violation": 0.00620436214384575,
            "ave_precision_score": 0.8618700884874455,
            "fpr": 0.020856201975850714,
            "logloss": 0.6900393936533139,
            "mae": 0.35131706014830494,
            "precision": 0.9221311475409836,
            "recall": 0.47468354430379744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(81)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8080536660485595,
            "auditor_fn_violation": 0.007220851608187131,
            "auditor_fp_violation": 0.015036143599740088,
            "ave_precision_score": 0.7982744316769792,
            "fpr": 0.09978070175438597,
            "logloss": 0.5202022072103002,
            "mae": 0.3449701541185118,
            "precision": 0.7977777777777778,
            "recall": 0.7479166666666667
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8294768927994536,
            "auditor_fn_violation": 0.003376453750920537,
            "auditor_fp_violation": 0.007093570321546724,
            "ave_precision_score": 0.8165776111796565,
            "fpr": 0.06805708013172337,
            "logloss": 0.5269024189129792,
            "mae": 0.35054872504769774,
            "precision": 0.8393782383419689,
            "recall": 0.6835443037974683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(82)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7684784630990553,
            "auditor_fn_violation": 0.0014117324561403546,
            "auditor_fp_violation": 0.022059271442495126,
            "ave_precision_score": 0.766961563338039,
            "fpr": 0.2138157894736842,
            "logloss": 1.1487193637876434,
            "mae": 0.33478452127198116,
            "precision": 0.6813725490196079,
            "recall": 0.86875
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7904382763401787,
            "auditor_fn_violation": 0.0054815267684697605,
            "auditor_fp_violation": 0.00861577415117042,
            "ave_precision_score": 0.789795169387043,
            "fpr": 0.2074643249176729,
            "logloss": 1.0215821599371704,
            "mae": 0.32518867823232933,
            "precision": 0.6881188118811881,
            "recall": 0.879746835443038
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(83)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8384935350130192,
            "auditor_fn_violation": 0.015768914473684212,
            "auditor_fp_violation": 0.026087353801169593,
            "ave_precision_score": 0.8388911286514461,
            "fpr": 0.14692982456140352,
            "logloss": 0.5145883621605094,
            "mae": 0.3137251068061839,
            "precision": 0.7485928705440901,
            "recall": 0.83125
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8434820364823494,
            "auditor_fn_violation": 0.014033820116994817,
            "auditor_fp_violation": 0.01962537709711208,
            "ave_precision_score": 0.844746992750079,
            "fpr": 0.1163556531284303,
            "logloss": 0.5085524103989582,
            "mae": 0.31845690171639573,
            "precision": 0.7754237288135594,
            "recall": 0.7721518987341772
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(84)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7835985698328328,
            "auditor_fn_violation": 0.0071820175438596515,
            "auditor_fp_violation": 0.018691114359974014,
            "ave_precision_score": 0.7321236638779655,
            "fpr": 0.15350877192982457,
            "logloss": 5.085623425790824,
            "mae": 0.29905392938048986,
            "precision": 0.7216699801192843,
            "recall": 0.75625
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.7954496046607046,
            "auditor_fn_violation": 0.013457182953771766,
            "auditor_fp_violation": 0.016894955376318428,
            "ave_precision_score": 0.7461965224516459,
            "fpr": 0.132821075740944,
            "logloss": 4.50732999911856,
            "mae": 0.27801647081227254,
            "precision": 0.753061224489796,
            "recall": 0.7784810126582279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(85)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.5847264600304318,
            "auditor_fn_violation": 0.0007743969298245628,
            "auditor_fp_violation": 0.013203581871345052,
            "ave_precision_score": 0.5680502874626026,
            "fpr": 0.3355263157894737,
            "logloss": 3.0029911400299527,
            "mae": 0.40545415407324437,
            "precision": 0.5881561238223418,
            "recall": 0.9104166666666667
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.5781400231229072,
            "auditor_fn_violation": 0.008114604899331657,
            "auditor_fp_violation": 0.010421821269156292,
            "ave_precision_score": 0.5540508165548721,
            "fpr": 0.3216245883644347,
            "logloss": 3.3583207502489616,
            "mae": 0.4100996774655044,
            "precision": 0.5861581920903954,
            "recall": 0.8755274261603375
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(86)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5098684210526315,
            "auc_prc": 0.5451038193058748,
            "auditor_fn_violation": 0.011759868421052635,
            "auditor_fp_violation": 0.0199906595191683,
            "ave_precision_score": 0.5417805145698608,
            "fpr": 0.18640350877192982,
            "logloss": 10.453984388849111,
            "mae": 0.5000167021756682,
            "precision": 0.5442359249329759,
            "recall": 0.42291666666666666
        },
        "train": {
            "accuracy": 0.5301866081229418,
            "auc_prc": 0.5352381109140771,
            "auditor_fn_violation": 0.0015284358543261774,
            "auditor_fp_violation": 0.019821304322707213,
            "ave_precision_score": 0.5282315927847365,
            "fpr": 0.18221734357848518,
            "logloss": 9.924851729987292,
            "mae": 0.48772740076195154,
            "precision": 0.5608465608465608,
            "recall": 0.4472573839662447
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(87)",
        "seed": 10197,
        "test": {
            "accuracy": 0.4725877192982456,
            "auc_prc": 0.5748630408892851,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.000558398310591293,
            "ave_precision_score": 0.5694261175575852,
            "fpr": 0.0010964912280701754,
            "logloss": 13.200694991674187,
            "mae": 0.5116814796866871,
            "precision": 0.0,
            "recall": 0.0
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 12.771960137589522,
            "mae": 0.503543411005745,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(88)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5449561403508771,
            "auc_prc": 0.6556504853519277,
            "auditor_fn_violation": 0.01577576754385967,
            "auditor_fp_violation": 0.012066479857050033,
            "ave_precision_score": 0.6452721131958692,
            "fpr": 0.0537280701754386,
            "logloss": 0.6845904411885413,
            "mae": 0.47319576032296345,
            "precision": 0.6993865030674846,
            "recall": 0.2375
        },
        "train": {
            "accuracy": 0.5488474204171241,
            "auc_prc": 0.6606140721071522,
            "auditor_fn_violation": 0.01322791757562285,
            "auditor_fp_violation": 0.007392484934954671,
            "ave_precision_score": 0.6470478364334078,
            "fpr": 0.048298572996706916,
            "logloss": 0.6734856433852499,
            "mae": 0.46402612999163395,
            "precision": 0.7086092715231788,
            "recall": 0.22573839662447256
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(89)",
        "seed": 10197,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.827990044770917,
            "auditor_fn_violation": 0.0009183114035087733,
            "auditor_fp_violation": 0.012388827972709553,
            "ave_precision_score": 0.805325774993078,
            "fpr": 0.10197368421052631,
            "logloss": 0.5371284195131868,
            "mae": 0.34887157030926463,
            "precision": 0.7956043956043956,
            "recall": 0.7541666666666667
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8189431762865466,
            "auditor_fn_violation": 0.009295668968583701,
            "auditor_fp_violation": 0.004491254863642186,
            "ave_precision_score": 0.7936071608783919,
            "fpr": 0.07793633369923161,
            "logloss": 0.5507046554172741,
            "mae": 0.35614541892686086,
            "precision": 0.8174807197943444,
            "recall": 0.6708860759493671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(90)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8217500253606238,
            "auditor_fn_violation": 0.018471308479532163,
            "auditor_fp_violation": 0.012670565302144249,
            "ave_precision_score": 0.8157939994139284,
            "fpr": 0.0800438596491228,
            "logloss": 0.5275307061829256,
            "mae": 0.3088097155804893,
            "precision": 0.8223844282238443,
            "recall": 0.7041666666666667
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8541799505114609,
            "auditor_fn_violation": 0.013630868846308828,
            "auditor_fp_violation": 0.01242379561273728,
            "ave_precision_score": 0.8443955101162826,
            "fpr": 0.05817782656421515,
            "logloss": 0.5159442243169658,
            "mae": 0.30617535790647293,
            "precision": 0.8539944903581267,
            "recall": 0.6540084388185654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(91)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8455645127987913,
            "auditor_fn_violation": 0.0024122807017543896,
            "auditor_fp_violation": 0.01664534600389864,
            "ave_precision_score": 0.8426407750756318,
            "fpr": 0.1118421052631579,
            "logloss": 0.5046111211719267,
            "mae": 0.30588571986657354,
            "precision": 0.7829787234042553,
            "recall": 0.7666666666666667
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8624321721143532,
            "auditor_fn_violation": 0.015798468785171396,
            "auditor_fp_violation": 0.008959902739715706,
            "ave_precision_score": 0.8588660394487553,
            "fpr": 0.09440175631174534,
            "logloss": 0.49490936572787997,
            "mae": 0.3090937404548821,
            "precision": 0.7976470588235294,
            "recall": 0.7151898734177216
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(92)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7677374247207807,
            "auditor_fn_violation": 0.003837719298245615,
            "auditor_fp_violation": 0.010624796946068878,
            "ave_precision_score": 0.7761264451522798,
            "fpr": 0.25548245614035087,
            "logloss": 0.6695499659387781,
            "mae": 0.32918803630143467,
            "precision": 0.6537890044576523,
            "recall": 0.9166666666666666
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7996658218897917,
            "auditor_fn_violation": 0.009735673229677597,
            "auditor_fp_violation": 0.004722348514344137,
            "ave_precision_score": 0.7950276172502233,
            "fpr": 0.23929747530186607,
            "logloss": 0.6248276400275358,
            "mae": 0.32028927669413676,
            "precision": 0.6635802469135802,
            "recall": 0.9071729957805907
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(93)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.772244918869825,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.6111825327772292,
            "fpr": 0.47368421052631576,
            "logloss": 0.6944908651303581,
            "mae": 0.48499773272819685,
            "precision": 0.5263157894736842,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5203073545554336,
            "auc_prc": 0.7590608388096594,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.598117032928742,
            "fpr": 0.4796926454445664,
            "logloss": 0.6975769090900816,
            "mae": 0.486742733011654,
            "precision": 0.5203073545554336,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(94)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7556522607675518,
            "auditor_fn_violation": 0.009676535087719299,
            "auditor_fp_violation": 0.03200383771929825,
            "ave_precision_score": 0.7342309686074243,
            "fpr": 0.21600877192982457,
            "logloss": 0.6057182000975212,
            "mae": 0.3947896270547062,
            "precision": 0.6677908937605397,
            "recall": 0.825
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.7285409910197097,
            "auditor_fn_violation": 0.02551329970774454,
            "auditor_fp_violation": 0.03258420474897453,
            "ave_precision_score": 0.7148500573908415,
            "fpr": 0.1986827661909989,
            "logloss": 0.633573744190553,
            "mae": 0.4138825209698876,
            "precision": 0.6610486891385767,
            "recall": 0.7447257383966245
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(95)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8007483774169988,
            "auditor_fn_violation": 0.0029057017543859705,
            "auditor_fp_violation": 0.019462719298245616,
            "ave_precision_score": 0.7996174036080028,
            "fpr": 0.11842105263157894,
            "logloss": 0.5140314958427914,
            "mae": 0.3380036180136366,
            "precision": 0.775,
            "recall": 0.775
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8263689606990483,
            "auditor_fn_violation": 0.006801539551751449,
            "auditor_fp_violation": 0.0014317758793490204,
            "ave_precision_score": 0.8218490443770653,
            "fpr": 0.08342480790340286,
            "logloss": 0.5096551319912495,
            "mae": 0.33930897476349914,
            "precision": 0.8168674698795181,
            "recall": 0.7151898734177216
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(96)",
        "seed": 10197,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.735524245160103,
            "auditor_fn_violation": 0.0016538742690058576,
            "auditor_fp_violation": 0.01219085038986355,
            "ave_precision_score": 0.7865675258021134,
            "fpr": 0.09539473684210527,
            "logloss": 0.5369882230589023,
            "mae": 0.3535671178414895,
            "precision": 0.8036117381489842,
            "recall": 0.7416666666666667
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.7415296267860381,
            "auditor_fn_violation": 0.011194634726988938,
            "auditor_fp_violation": 0.004518885626226116,
            "ave_precision_score": 0.7727737993344335,
            "fpr": 0.07244785949506037,
            "logloss": 0.5512823528016144,
            "mae": 0.3655742349023353,
            "precision": 0.824468085106383,
            "recall": 0.6540084388185654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(97)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5515350877192983,
            "auc_prc": 0.6231224609658688,
            "auditor_fn_violation": 0.006167763157894737,
            "auditor_fp_violation": 0.016906777940220937,
            "ave_precision_score": 0.5320865614066395,
            "fpr": 0.32785087719298245,
            "logloss": 7.385742389078401,
            "mae": 0.45665260307131367,
            "precision": 0.5530642750373692,
            "recall": 0.7708333333333334
        },
        "train": {
            "accuracy": 0.5323819978046103,
            "auc_prc": 0.6122012022170916,
            "auditor_fn_violation": 0.004048039202063852,
            "auditor_fp_violation": 0.007394996822462302,
            "ave_precision_score": 0.5204649587600801,
            "fpr": 0.3446761800219539,
            "logloss": 7.504189334879757,
            "mae": 0.4702069078864475,
            "precision": 0.5355029585798816,
            "recall": 0.7637130801687764
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(98)",
        "seed": 10197,
        "test": {
            "accuracy": 0.4550438596491228,
            "auc_prc": 0.5154550153420118,
            "auditor_fn_violation": 0.003686951754385978,
            "auditor_fp_violation": 0.0048276072124756346,
            "ave_precision_score": 0.5819920554949899,
            "fpr": 0.03289473684210526,
            "logloss": 0.7059949563362631,
            "mae": 0.4877503384885035,
            "precision": 0.3023255813953488,
            "recall": 0.027083333333333334
        },
        "train": {
            "accuracy": 0.4610318331503842,
            "auc_prc": 0.5168574779864394,
            "auditor_fn_violation": 0.0020008614820270056,
            "auditor_fp_violation": 0.005141833728118322,
            "ave_precision_score": 0.5786233717291565,
            "fpr": 0.03402854006586169,
            "logloss": 0.7068057014509018,
            "mae": 0.4868265081250183,
            "precision": 0.3111111111111111,
            "recall": 0.029535864978902954
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(99)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8368005777371834,
            "auditor_fn_violation": 0.001153600146198831,
            "auditor_fp_violation": 0.0036168981481481525,
            "ave_precision_score": 0.8086077419687037,
            "fpr": 0.0756578947368421,
            "logloss": 0.5240046875258497,
            "mae": 0.3446617743425202,
            "precision": 0.8275,
            "recall": 0.6895833333333333
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8404235147566714,
            "auditor_fn_violation": 0.017428800363119307,
            "auditor_fp_violation": 0.006274694994059386,
            "ave_precision_score": 0.7987761400641141,
            "fpr": 0.05159165751920966,
            "logloss": 0.5440362119569095,
            "mae": 0.36351771981472764,
            "precision": 0.8588588588588588,
            "recall": 0.6033755274261603
        }
    }
]
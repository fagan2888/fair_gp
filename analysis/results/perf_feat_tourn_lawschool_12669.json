[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.7106944722758841,
            "auditor_fn_violation": 0.07927886576907385,
            "auditor_fp_violation": 0.09251238860248573,
            "ave_precision_score": 0.600538204633839,
            "fpr": 0.24451754385964913,
            "logloss": 10.20614956498632,
            "mae": 0.3796940026440742,
            "precision": 0.6121739130434782,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7192226778821487,
            "auditor_fn_violation": 0.08487622977194877,
            "auditor_fp_violation": 0.07834988384856918,
            "ave_precision_score": 0.6168801962717492,
            "fpr": 0.23710208562019758,
            "logloss": 9.926136997601649,
            "mae": 0.37642793059337726,
            "precision": 0.6210526315789474,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 12669,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8542504873565677,
            "auditor_fn_violation": 0.016829865361077112,
            "auditor_fp_violation": 0.014034588178875436,
            "ave_precision_score": 0.8377270655908503,
            "fpr": 0.10526315789473684,
            "logloss": 3.090715940970517,
            "mae": 0.23605755749349647,
            "precision": 0.7857142857142857,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.7848518111964874,
            "auc_prc": 0.8783332604725371,
            "auditor_fn_violation": 0.02343726822321773,
            "auditor_fp_violation": 0.008562019758507138,
            "ave_precision_score": 0.8656941443278545,
            "fpr": 0.09440175631174534,
            "logloss": 2.493124181524605,
            "mae": 0.2145087487857617,
            "precision": 0.811816192560175,
            "recall": 0.7713097713097713
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.7114049018192218,
            "auditor_fn_violation": 0.0771229739252995,
            "auditor_fp_violation": 0.0909738041002278,
            "ave_precision_score": 0.6049984721332424,
            "fpr": 0.24342105263157895,
            "logloss": 9.931942386112901,
            "mae": 0.3779664708255309,
            "precision": 0.6152512998266898,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7167414169565675,
            "auditor_fn_violation": 0.08498348893519037,
            "auditor_fp_violation": 0.07766573915707248,
            "ave_precision_score": 0.6167631351001153,
            "fpr": 0.23819978046103182,
            "logloss": 9.706231272311744,
            "mae": 0.3759860201186156,
            "precision": 0.6206293706293706,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.6780645318100842,
            "auditor_fn_violation": 0.03301296316902193,
            "auditor_fp_violation": 0.05311363545538105,
            "ave_precision_score": 0.6643030344466772,
            "fpr": 0.17982456140350878,
            "logloss": 2.877422454521885,
            "mae": 0.33149179176580507,
            "precision": 0.6765285996055227,
            "recall": 0.7251585623678647
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.709251260333432,
            "auditor_fn_violation": 0.04209351629768754,
            "auditor_fp_violation": 0.046746483547341275,
            "ave_precision_score": 0.6945575993633891,
            "fpr": 0.1668496158068057,
            "logloss": 2.703872345209961,
            "mae": 0.30046458983523555,
            "precision": 0.7093690248565966,
            "recall": 0.7713097713097713
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6491228070175439,
            "auc_prc": 0.7301045137205646,
            "auditor_fn_violation": 0.07041420941359741,
            "auditor_fp_violation": 0.08602835391439875,
            "ave_precision_score": 0.6205991425136937,
            "fpr": 0.22916666666666666,
            "logloss": 9.718713749514558,
            "mae": 0.3520936991297823,
            "precision": 0.6339754816112084,
            "recall": 0.7653276955602537
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.7422299115567009,
            "auditor_fn_violation": 0.07787243462325791,
            "auditor_fp_violation": 0.07359150435248768,
            "ave_precision_score": 0.6389800809355035,
            "fpr": 0.2217343578485181,
            "logloss": 9.491564566220822,
            "mae": 0.3547820463106023,
            "precision": 0.6418439716312057,
            "recall": 0.7525987525987526
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.7100436893041742,
            "auditor_fn_violation": 0.07784160453989096,
            "auditor_fp_violation": 0.09278214043080367,
            "ave_precision_score": 0.5976388221412965,
            "fpr": 0.24671052631578946,
            "logloss": 10.338329580458193,
            "mae": 0.3805570087987716,
            "precision": 0.6113989637305699,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7158746419428433,
            "auditor_fn_violation": 0.08417333993623785,
            "auditor_fp_violation": 0.07890638960508514,
            "ave_precision_score": 0.6100087578514547,
            "fpr": 0.23710208562019758,
            "logloss": 10.085163990948203,
            "mae": 0.37535966436854723,
            "precision": 0.6217162872154116,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 12669,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7957576370870096,
            "auditor_fn_violation": 0.050058417714476465,
            "auditor_fp_violation": 0.04827558646045638,
            "ave_precision_score": 0.7005727477582036,
            "fpr": 0.15679824561403508,
            "logloss": 8.454029704784473,
            "mae": 0.2914634528250208,
            "precision": 0.7099391480730223,
            "recall": 0.7399577167019028
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.8018080232754248,
            "auditor_fn_violation": 0.06016098002925665,
            "auditor_fp_violation": 0.04224082914252164,
            "ave_precision_score": 0.7064476765660344,
            "fpr": 0.15477497255762898,
            "logloss": 8.627780959733002,
            "mae": 0.28938235286803743,
            "precision": 0.717434869739479,
            "recall": 0.7442827442827443
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 12669,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7062886062175826,
            "auditor_fn_violation": 0.02455166722302586,
            "auditor_fp_violation": 0.043934580186228665,
            "ave_precision_score": 0.7016220961632653,
            "fpr": 0.16776315789473684,
            "logloss": 2.0747450699787064,
            "mae": 0.3147246590163583,
            "precision": 0.694,
            "recall": 0.733615221987315
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7336381629879252,
            "auditor_fn_violation": 0.036730558135607536,
            "auditor_fp_violation": 0.04295560717841371,
            "ave_precision_score": 0.727243838954778,
            "fpr": 0.16136114160263446,
            "logloss": 1.8953357637404795,
            "mae": 0.29195961679776317,
            "precision": 0.7156673114119922,
            "recall": 0.7692307692307693
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.7149159499411541,
            "auditor_fn_violation": 0.08331942435369608,
            "auditor_fp_violation": 0.08263147903928386,
            "ave_precision_score": 0.6079990899861745,
            "fpr": 0.21929824561403508,
            "logloss": 10.445471407173448,
            "mae": 0.3761451442429998,
            "precision": 0.6240601503759399,
            "recall": 0.7019027484143763
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.7251117691688214,
            "auditor_fn_violation": 0.08394056473090503,
            "auditor_fp_violation": 0.07114594235825696,
            "ave_precision_score": 0.6237536912211681,
            "fpr": 0.20856201975850713,
            "logloss": 10.0778791665262,
            "mae": 0.3714526037799071,
            "precision": 0.6353166986564299,
            "recall": 0.6881496881496881
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.7106948952569707,
            "auditor_fn_violation": 0.07927886576907385,
            "auditor_fp_violation": 0.09251238860248573,
            "ave_precision_score": 0.6005385801296093,
            "fpr": 0.24451754385964913,
            "logloss": 10.206711577229475,
            "mae": 0.3796981556630579,
            "precision": 0.6121739130434782,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7192313333681062,
            "auditor_fn_violation": 0.08487622977194877,
            "auditor_fp_violation": 0.07834988384856918,
            "ave_precision_score": 0.6168888490439136,
            "fpr": 0.23710208562019758,
            "logloss": 9.926308998504012,
            "mae": 0.37642216245693727,
            "precision": 0.6210526315789474,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 12669,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.6733907320674707,
            "auditor_fn_violation": 0.03778142502132711,
            "auditor_fp_violation": 0.054727151021060634,
            "ave_precision_score": 0.6600178002501493,
            "fpr": 0.17543859649122806,
            "logloss": 2.9879614545430937,
            "mae": 0.33413407950612584,
            "precision": 0.6741344195519349,
            "recall": 0.6997885835095138
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7089541135267778,
            "auditor_fn_violation": 0.049679249459710494,
            "auditor_fp_violation": 0.04846450361218186,
            "ave_precision_score": 0.6924627261455465,
            "fpr": 0.15916575192096596,
            "logloss": 2.89727157032779,
            "mae": 0.3002218742542375,
            "precision": 0.7123015873015873,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.7089078037132857,
            "auditor_fn_violation": 0.0771229739252995,
            "auditor_fp_violation": 0.09320425208807898,
            "ave_precision_score": 0.6034544612941249,
            "fpr": 0.24780701754385964,
            "logloss": 9.87492329651423,
            "mae": 0.3797621953950591,
            "precision": 0.6110154905335629,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6344676180021954,
            "auc_prc": 0.7183614369571985,
            "auditor_fn_violation": 0.07864835197436734,
            "auditor_fp_violation": 0.07890638960508514,
            "ave_precision_score": 0.6177631441739896,
            "fpr": 0.23710208562019758,
            "logloss": 9.618113511490046,
            "mae": 0.3697070888047012,
            "precision": 0.6275862068965518,
            "recall": 0.7567567567567568
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.7102916057596178,
            "auditor_fn_violation": 0.07927886576907385,
            "auditor_fp_violation": 0.08985233585101708,
            "ave_precision_score": 0.5997415452881352,
            "fpr": 0.24342105263157895,
            "logloss": 10.22714660459654,
            "mae": 0.37964338482502397,
            "precision": 0.6132404181184669,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7192213552914304,
            "auditor_fn_violation": 0.08487622977194877,
            "auditor_fp_violation": 0.07793633369923163,
            "ave_precision_score": 0.6168759290870103,
            "fpr": 0.23600439077936333,
            "logloss": 9.929445924632523,
            "mae": 0.3764251703372996,
            "precision": 0.6221441124780316,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.7101847318558342,
            "auditor_fn_violation": 0.07784160453989096,
            "auditor_fp_violation": 0.09180054349998001,
            "ave_precision_score": 0.5995460497009694,
            "fpr": 0.24561403508771928,
            "logloss": 10.238493803160935,
            "mae": 0.38011431837203546,
            "precision": 0.6124567474048442,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7183476246602715,
            "auditor_fn_violation": 0.08498348893519037,
            "auditor_fp_violation": 0.07890638960508514,
            "ave_precision_score": 0.6151469841107191,
            "fpr": 0.23710208562019758,
            "logloss": 9.949149821000665,
            "mae": 0.37614732444476157,
            "precision": 0.6217162872154116,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.7071261335529042,
            "auditor_fn_violation": 0.07927886576907385,
            "auditor_fp_violation": 0.0921077608600088,
            "ave_precision_score": 0.5958866680186459,
            "fpr": 0.24780701754385964,
            "logloss": 10.282718405454911,
            "mae": 0.3823649126306315,
            "precision": 0.6089965397923875,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.717088274692209,
            "auditor_fn_violation": 0.08518203249268014,
            "auditor_fp_violation": 0.07848518111964875,
            "ave_precision_score": 0.6115014256219118,
            "fpr": 0.23600439077936333,
            "logloss": 10.015449857874238,
            "mae": 0.3756747152894316,
            "precision": 0.6234676007005254,
            "recall": 0.7401247401247402
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.707175447620743,
            "auditor_fn_violation": 0.08161557434813249,
            "auditor_fp_violation": 0.09098129720656997,
            "ave_precision_score": 0.6022018035009236,
            "fpr": 0.23793859649122806,
            "logloss": 9.882117750343163,
            "mae": 0.37606765156722594,
            "precision": 0.6172839506172839,
            "recall": 0.7399577167019028
        },
        "train": {
            "accuracy": 0.6212952799121844,
            "auc_prc": 0.7092908341209747,
            "auditor_fn_violation": 0.08707161945361727,
            "auditor_fp_violation": 0.07868174507951907,
            "ave_precision_score": 0.6069481325468655,
            "fpr": 0.23929747530186607,
            "logloss": 10.06343142055808,
            "mae": 0.37804878525771934,
            "precision": 0.6188811188811189,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.7010071536498621,
            "auditor_fn_violation": 0.07834696413337784,
            "auditor_fp_violation": 0.09180553890420814,
            "ave_precision_score": 0.5829987673799173,
            "fpr": 0.2543859649122807,
            "logloss": 10.781136689415133,
            "mae": 0.3904197600850579,
            "precision": 0.6,
            "recall": 0.7357293868921776
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7135923412413689,
            "auditor_fn_violation": 0.08131842050612634,
            "auditor_fp_violation": 0.08154851555918619,
            "ave_precision_score": 0.6038407140504631,
            "fpr": 0.24698133918770582,
            "logloss": 10.2277458795528,
            "mae": 0.37536454860059926,
            "precision": 0.6179966044142614,
            "recall": 0.7567567567567568
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.7114291347675501,
            "auditor_fn_violation": 0.07784160453989096,
            "auditor_fp_violation": 0.09180054349998001,
            "ave_precision_score": 0.6043629054731566,
            "fpr": 0.24561403508771928,
            "logloss": 10.03301600489532,
            "mae": 0.37900773728111387,
            "precision": 0.6124567474048442,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.718110288407937,
            "auditor_fn_violation": 0.08498348893519037,
            "auditor_fp_violation": 0.07834988384856918,
            "ave_precision_score": 0.6161286685958659,
            "fpr": 0.23710208562019758,
            "logloss": 9.876069801005974,
            "mae": 0.37617574500174233,
            "precision": 0.6217162872154116,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.6895288541334256,
            "auditor_fn_violation": 0.028107729683617074,
            "auditor_fp_violation": 0.047311473444431124,
            "ave_precision_score": 0.6829832149783881,
            "fpr": 0.1699561403508772,
            "logloss": 2.323408362019311,
            "mae": 0.3220794911085669,
            "precision": 0.6855983772819473,
            "recall": 0.7145877378435518
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7179949749563805,
            "auditor_fn_violation": 0.03985704863860737,
            "auditor_fp_violation": 0.04406096035534681,
            "ave_precision_score": 0.7099062133109678,
            "fpr": 0.15916575192096596,
            "logloss": 2.139391100079294,
            "mae": 0.2964170979678168,
            "precision": 0.7173489278752436,
            "recall": 0.7650727650727651
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.7088054882071646,
            "auditor_fn_violation": 0.07893809576796114,
            "auditor_fp_violation": 0.09404847540262959,
            "ave_precision_score": 0.5951658698394067,
            "fpr": 0.25,
            "logloss": 10.331597890298633,
            "mae": 0.38188449136851804,
            "precision": 0.6082474226804123,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6256860592755215,
            "auc_prc": 0.7151016751664452,
            "auditor_fn_violation": 0.08385156244651305,
            "auditor_fp_violation": 0.08017001506139436,
            "ave_precision_score": 0.6080179759494077,
            "fpr": 0.24039517014270034,
            "logloss": 10.062982946833873,
            "mae": 0.37399983122974806,
            "precision": 0.6211072664359861,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6502192982456141,
            "auc_prc": 0.707249472005648,
            "auditor_fn_violation": 0.03793906012388264,
            "auditor_fp_violation": 0.025004495863805307,
            "ave_precision_score": 0.7001584609007396,
            "fpr": 0.08114035087719298,
            "logloss": 3.685166424147069,
            "mae": 0.3550721776124205,
            "precision": 0.7549668874172185,
            "recall": 0.4820295983086681
        },
        "train": {
            "accuracy": 0.6399560922063666,
            "auc_prc": 0.7157878616505828,
            "auditor_fn_violation": 0.04387812620523927,
            "auditor_fp_violation": 0.024624103336481764,
            "ave_precision_score": 0.7046024781720304,
            "fpr": 0.09001097694840834,
            "logloss": 3.8764336465620235,
            "mae": 0.35866243743983917,
            "precision": 0.7413249211356467,
            "recall": 0.4885654885654886
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 12669,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.7105816439772322,
            "auditor_fn_violation": 0.0782194651533697,
            "auditor_fp_violation": 0.09180054349998001,
            "ave_precision_score": 0.6008614506265748,
            "fpr": 0.24561403508771928,
            "logloss": 10.18405533674814,
            "mae": 0.38001729812387974,
            "precision": 0.613126079447323,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7199412919052086,
            "auditor_fn_violation": 0.08498348893519037,
            "auditor_fp_violation": 0.07766573915707248,
            "ave_precision_score": 0.618028754882628,
            "fpr": 0.23819978046103182,
            "logloss": 9.8642940061397,
            "mae": 0.37608286148282183,
            "precision": 0.6206293706293706,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.6715151499276343,
            "auditor_fn_violation": 0.041726938911761435,
            "auditor_fp_violation": 0.05814650521520202,
            "ave_precision_score": 0.6560566839630281,
            "fpr": 0.17763157894736842,
            "logloss": 3.177102825857056,
            "mae": 0.33631613199402194,
            "precision": 0.6713995943204868,
            "recall": 0.6997885835095138
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7120893308713662,
            "auditor_fn_violation": 0.05292440967523296,
            "auditor_fp_violation": 0.04512291629438644,
            "ave_precision_score": 0.692913514348383,
            "fpr": 0.16136114160263446,
            "logloss": 3.1578924231446046,
            "mae": 0.3020778955867798,
            "precision": 0.7094861660079052,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7354035407952304,
            "auditor_fn_violation": 0.04792338934015801,
            "auditor_fp_violation": 0.03448077768453023,
            "ave_precision_score": 0.7294428099596546,
            "fpr": 0.09539473684210527,
            "logloss": 4.757874137356565,
            "mae": 0.3328601921388252,
            "precision": 0.75,
            "recall": 0.5517970401691332
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.7427989918547997,
            "auditor_fn_violation": 0.0760467467382945,
            "auditor_fp_violation": 0.02887192709264034,
            "ave_precision_score": 0.737647240974874,
            "fpr": 0.09330406147091108,
            "logloss": 5.106334624595675,
            "mae": 0.3129411014451949,
            "precision": 0.7702702702702703,
            "recall": 0.5925155925155925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.6774973253437165,
            "auditor_fn_violation": 0.07621425392233226,
            "auditor_fp_violation": 0.07736132757862768,
            "ave_precision_score": 0.581369222518265,
            "fpr": 0.24232456140350878,
            "logloss": 9.01887579134244,
            "mae": 0.3744463008809414,
            "precision": 0.6156521739130435,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6814770944675017,
            "auditor_fn_violation": 0.07348621947963331,
            "auditor_fp_violation": 0.07629744977407908,
            "ave_precision_score": 0.5911469325431454,
            "fpr": 0.24039517014270034,
            "logloss": 9.192151601665591,
            "mae": 0.3842828939507986,
            "precision": 0.6157894736842106,
            "recall": 0.7297297297297297
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.7120376620442048,
            "auditor_fn_violation": 0.0771229739252995,
            "auditor_fp_violation": 0.0909738041002278,
            "ave_precision_score": 0.6054421127341791,
            "fpr": 0.24342105263157895,
            "logloss": 9.974403775376596,
            "mae": 0.37847240741370486,
            "precision": 0.6152512998266898,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7178619479284385,
            "auditor_fn_violation": 0.08498348893519037,
            "auditor_fp_violation": 0.07834988384856918,
            "ave_precision_score": 0.6170531688491694,
            "fpr": 0.23710208562019758,
            "logloss": 9.797538287353543,
            "mae": 0.3759615185442298,
            "precision": 0.6217162872154116,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.7133735104132815,
            "auditor_fn_violation": 0.03577157746374393,
            "auditor_fp_violation": 0.052481716820525125,
            "ave_precision_score": 0.6755773502977069,
            "fpr": 0.1875,
            "logloss": 3.976702444710947,
            "mae": 0.3470198183532903,
            "precision": 0.6627218934911243,
            "recall": 0.7103594080338267
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7509610389729396,
            "auditor_fn_violation": 0.0501105682225331,
            "auditor_fp_violation": 0.04233528195440739,
            "ave_precision_score": 0.7099576202302588,
            "fpr": 0.1690450054884742,
            "logloss": 3.6811703305076073,
            "mae": 0.30947282987540115,
            "precision": 0.7044145873320538,
            "recall": 0.762993762993763
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.709636099620278,
            "auditor_fn_violation": 0.07886623270650199,
            "auditor_fp_violation": 0.0879940454781601,
            "ave_precision_score": 0.5974188893287695,
            "fpr": 0.24890350877192982,
            "logloss": 10.087028947800087,
            "mae": 0.37574750635291987,
            "precision": 0.6132879045996593,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.6366630076838639,
            "auc_prc": 0.721040971837815,
            "auditor_fn_violation": 0.07885830608113814,
            "auditor_fp_violation": 0.07665228601332554,
            "ave_precision_score": 0.6152269444919448,
            "fpr": 0.24039517014270034,
            "logloss": 9.831087704628137,
            "mae": 0.36287107004672314,
            "precision": 0.6275510204081632,
            "recall": 0.7671517671517671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.7101487931635726,
            "auditor_fn_violation": 0.07784160453989096,
            "auditor_fp_violation": 0.09278214043080367,
            "ave_precision_score": 0.5995034708190267,
            "fpr": 0.24671052631578946,
            "logloss": 10.246701242753135,
            "mae": 0.37994769520171146,
            "precision": 0.6113989637305699,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7178374662586271,
            "auditor_fn_violation": 0.08417333993623785,
            "auditor_fp_violation": 0.07725218900773494,
            "ave_precision_score": 0.6135535008401013,
            "fpr": 0.23710208562019758,
            "logloss": 9.982443972747172,
            "mae": 0.3757470542800558,
            "precision": 0.6217162872154116,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.7071048955677199,
            "auditor_fn_violation": 0.07449417677385854,
            "auditor_fp_violation": 0.0908289373776126,
            "ave_precision_score": 0.5969416934815246,
            "fpr": 0.24890350877192982,
            "logloss": 10.017099791192667,
            "mae": 0.37833603111642483,
            "precision": 0.6106346483704974,
            "recall": 0.7526427061310782
        },
        "train": {
            "accuracy": 0.6388583973655324,
            "auc_prc": 0.7191968644865098,
            "auditor_fn_violation": 0.07808010662017249,
            "auditor_fp_violation": 0.07793633369923161,
            "ave_precision_score": 0.6164545234674239,
            "fpr": 0.23600439077936333,
            "logloss": 9.69352068803649,
            "mae": 0.3642354960160893,
            "precision": 0.6305841924398625,
            "recall": 0.762993762993763
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.7266458277137005,
            "auditor_fn_violation": 0.04684080709172509,
            "auditor_fp_violation": 0.05364814370778884,
            "ave_precision_score": 0.7172330674659655,
            "fpr": 0.15570175438596492,
            "logloss": 3.274414289089752,
            "mae": 0.3172122911633584,
            "precision": 0.6978723404255319,
            "recall": 0.693446088794926
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7395881747981343,
            "auditor_fn_violation": 0.06116967258569894,
            "auditor_fp_violation": 0.04912822607408164,
            "ave_precision_score": 0.7301993771754514,
            "fpr": 0.14818880351262348,
            "logloss": 3.4379650099594867,
            "mae": 0.3075478662104482,
            "precision": 0.7175732217573222,
            "recall": 0.7130977130977131
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.7102850993709925,
            "auditor_fn_violation": 0.07965672638255257,
            "auditor_fp_violation": 0.09251238860248573,
            "ave_precision_score": 0.5997383087719961,
            "fpr": 0.24451754385964913,
            "logloss": 10.228430274257654,
            "mae": 0.3796118319667045,
            "precision": 0.6128472222222222,
            "recall": 0.7463002114164905
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7192102861386578,
            "auditor_fn_violation": 0.08487622977194877,
            "auditor_fp_violation": 0.07834988384856918,
            "ave_precision_score": 0.6168647632460427,
            "fpr": 0.23710208562019758,
            "logloss": 9.930291909262786,
            "mae": 0.3764857729094496,
            "precision": 0.6210526315789474,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6337719298245614,
            "auc_prc": 0.7296762832592424,
            "auditor_fn_violation": 0.06559242980601611,
            "auditor_fp_violation": 0.08068077368820686,
            "ave_precision_score": 0.6234225158414666,
            "fpr": 0.24232456140350878,
            "logloss": 9.594614233113798,
            "mae": 0.36601689083082956,
            "precision": 0.6196213425129088,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.6465422612513722,
            "auc_prc": 0.7377228263488904,
            "auditor_fn_violation": 0.07057196519326048,
            "auditor_fp_violation": 0.07011972532101192,
            "ave_precision_score": 0.6371455973120685,
            "fpr": 0.22941822173435786,
            "logloss": 9.430568438594472,
            "mae": 0.357824654423943,
            "precision": 0.6377816291161178,
            "recall": 0.7650727650727651
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.7045123109875535,
            "auditor_fn_violation": 0.07680538555691556,
            "auditor_fp_violation": 0.09421831914638533,
            "ave_precision_score": 0.5863194683683942,
            "fpr": 0.2532894736842105,
            "logloss": 10.659608490682738,
            "mae": 0.38584528986392685,
            "precision": 0.6037735849056604,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.716507146705498,
            "auditor_fn_violation": 0.08045578298048112,
            "auditor_fp_violation": 0.08154851555918619,
            "ave_precision_score": 0.6065751636313128,
            "fpr": 0.24698133918770582,
            "logloss": 10.266506857905718,
            "mae": 0.37806249387517005,
            "precision": 0.6173469387755102,
            "recall": 0.7546777546777547
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.7476688977939936,
            "auditor_fn_violation": 0.0523580356811691,
            "auditor_fp_violation": 0.07880000399632339,
            "ave_precision_score": 0.60827914573366,
            "fpr": 0.29605263157894735,
            "logloss": 10.789920741956717,
            "mae": 0.3786402295895917,
            "precision": 0.5964125560538116,
            "recall": 0.8435517970401691
        },
        "train": {
            "accuracy": 0.6114160263446762,
            "auc_prc": 0.7419144555930864,
            "auditor_fn_violation": 0.059633812652473465,
            "auditor_fp_violation": 0.06586424322875452,
            "ave_precision_score": 0.6114661071470546,
            "fpr": 0.29308452250274425,
            "logloss": 11.139920712952836,
            "mae": 0.3923886865667861,
            "precision": 0.5960665658093798,
            "recall": 0.8191268191268192
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 12669,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.7165636831438822,
            "auditor_fn_violation": 0.028054411928340933,
            "auditor_fp_violation": 0.04698677216960397,
            "ave_precision_score": 0.5639305080386969,
            "fpr": 0.2993421052631579,
            "logloss": 13.650726585036965,
            "mae": 0.43000410020548846,
            "precision": 0.5652866242038217,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.5609220636663008,
            "auc_prc": 0.713098166777297,
            "auditor_fn_violation": 0.03439139553299817,
            "auditor_fp_violation": 0.035662318433614996,
            "ave_precision_score": 0.5586384847955186,
            "fpr": 0.30735455543358947,
            "logloss": 14.266106550271852,
            "mae": 0.4408837670818465,
            "precision": 0.5631825273010921,
            "recall": 0.7505197505197505
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.7407359920953138,
            "auditor_fn_violation": 0.060207336523126,
            "auditor_fp_violation": 0.07911471446269433,
            "ave_precision_score": 0.615883010615627,
            "fpr": 0.27850877192982454,
            "logloss": 10.110251371749424,
            "mae": 0.3706275027365367,
            "precision": 0.6037441497659907,
            "recall": 0.8181818181818182
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.7384438951622239,
            "auditor_fn_violation": 0.06197982158465145,
            "auditor_fp_violation": 0.06376330635897176,
            "ave_precision_score": 0.6187150749829728,
            "fpr": 0.2722283205268935,
            "logloss": 10.44387805691833,
            "mae": 0.3794911072087543,
            "precision": 0.608214849921011,
            "recall": 0.8004158004158004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.7102884587938232,
            "auditor_fn_violation": 0.07927886576907385,
            "auditor_fp_violation": 0.08985233585101708,
            "ave_precision_score": 0.5997383997235298,
            "fpr": 0.24342105263157895,
            "logloss": 10.227029507464348,
            "mae": 0.37964723620316465,
            "precision": 0.6132404181184669,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.719230703482808,
            "auditor_fn_violation": 0.08487622977194877,
            "auditor_fp_violation": 0.07793633369923163,
            "ave_precision_score": 0.6168852738950092,
            "fpr": 0.23600439077936333,
            "logloss": 9.929455969799303,
            "mae": 0.37643275390814895,
            "precision": 0.6221441124780316,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.6737238098423528,
            "auditor_fn_violation": 0.03854873706464894,
            "auditor_fp_violation": 0.05184730048355513,
            "ave_precision_score": 0.6572410130087558,
            "fpr": 0.1787280701754386,
            "logloss": 3.1878831871124595,
            "mae": 0.337969335650935,
            "precision": 0.6700404858299596,
            "recall": 0.6997885835095138
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7056566073359213,
            "auditor_fn_violation": 0.04569924987049027,
            "auditor_fp_violation": 0.04777780614198557,
            "ave_precision_score": 0.6877311836611937,
            "fpr": 0.1668496158068057,
            "logloss": 3.1258860299597773,
            "mae": 0.30387055932174645,
            "precision": 0.7059961315280464,
            "recall": 0.7588357588357588
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.7038208697909707,
            "auditor_fn_violation": 0.08043099291569304,
            "auditor_fp_violation": 0.094520541102186,
            "ave_precision_score": 0.5870977337250736,
            "fpr": 0.2412280701754386,
            "logloss": 10.584196574702661,
            "mae": 0.3851105522539056,
            "precision": 0.6140350877192983,
            "recall": 0.7399577167019028
        },
        "train": {
            "accuracy": 0.6114160263446762,
            "auc_prc": 0.7058212079424296,
            "auditor_fn_violation": 0.08715833962815303,
            "auditor_fp_violation": 0.08648814234294032,
            "ave_precision_score": 0.5957615884477667,
            "fpr": 0.2524698133918771,
            "logloss": 10.436420024921045,
            "mae": 0.39554265463031363,
            "precision": 0.6081771720613288,
            "recall": 0.7422037422037422
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6107456140350878,
            "auc_prc": 0.7039879276151334,
            "auditor_fn_violation": 0.0776468788249694,
            "auditor_fp_violation": 0.09548465411821125,
            "ave_precision_score": 0.5848822313086182,
            "fpr": 0.2565789473684211,
            "logloss": 10.767018745776173,
            "mae": 0.3876829047262896,
            "precision": 0.6006825938566553,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.7151527251341958,
            "auditor_fn_violation": 0.07974604681520159,
            "auditor_fp_violation": 0.08238072141526052,
            "ave_precision_score": 0.6034441954380713,
            "fpr": 0.2491767288693743,
            "logloss": 10.405791499617386,
            "mae": 0.3789240590223419,
            "precision": 0.6159052453468697,
            "recall": 0.7567567567567568
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.7060379165966957,
            "auditor_fn_violation": 0.07927886576907385,
            "auditor_fp_violation": 0.09320425208807898,
            "ave_precision_score": 0.5965876484689593,
            "fpr": 0.24780701754385964,
            "logloss": 10.30909811320048,
            "mae": 0.38343005720597834,
            "precision": 0.6089965397923875,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7200117829182069,
            "auditor_fn_violation": 0.08417333993623785,
            "auditor_fp_violation": 0.07725218900773494,
            "ave_precision_score": 0.617532585942552,
            "fpr": 0.23710208562019758,
            "logloss": 9.874017666724892,
            "mae": 0.37631085728239316,
            "precision": 0.6217162872154116,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.6390133082973506,
            "auditor_fn_violation": 0.04109639850153927,
            "auditor_fp_violation": 0.058451224873116735,
            "ave_precision_score": 0.6134409455618768,
            "fpr": 0.19188596491228072,
            "logloss": 4.0259896352216735,
            "mae": 0.36149288961880804,
            "precision": 0.6471774193548387,
            "recall": 0.678646934460888
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.6841958489943448,
            "auditor_fn_violation": 0.05140908873071332,
            "auditor_fp_violation": 0.05331223036274985,
            "ave_precision_score": 0.6559344624494362,
            "fpr": 0.17453347969264543,
            "logloss": 3.807554124972312,
            "mae": 0.3164713304439571,
            "precision": 0.6924564796905223,
            "recall": 0.7442827442827443
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 12669,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7923381035777505,
            "auditor_fn_violation": 0.00877656615110715,
            "auditor_fp_violation": 0.03052441753586701,
            "ave_precision_score": 0.7928375322202103,
            "fpr": 0.15570175438596492,
            "logloss": 0.7327997551540579,
            "mae": 0.3358941446210354,
            "precision": 0.7182539682539683,
            "recall": 0.7653276955602537
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8142428187737527,
            "auditor_fn_violation": 0.004153439938291753,
            "auditor_fp_violation": 0.00816889183876651,
            "ave_precision_score": 0.8147611163720374,
            "fpr": 0.13611416026344675,
            "logloss": 0.5573356459998042,
            "mae": 0.31897251969155693,
            "precision": 0.7524950099800399,
            "recall": 0.7837837837837838
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 12669,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8056753255409181,
            "auditor_fn_violation": 0.022553410481807055,
            "auditor_fp_violation": 0.02638572513287775,
            "ave_precision_score": 0.806083055815988,
            "fpr": 0.14144736842105263,
            "logloss": 1.120535693767444,
            "mae": 0.28361710608695967,
            "precision": 0.73125,
            "recall": 0.7420718816067653
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8409176375915619,
            "auditor_fn_violation": 0.02263168344397763,
            "auditor_fp_violation": 0.0209583131238353,
            "ave_precision_score": 0.8411754186473479,
            "fpr": 0.13172338090010977,
            "logloss": 0.9499104176079061,
            "mae": 0.2582653047689023,
            "precision": 0.7595190380761523,
            "recall": 0.7879417879417879
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.7071998287519154,
            "auditor_fn_violation": 0.07886623270650199,
            "auditor_fp_violation": 0.0879940454781601,
            "ave_precision_score": 0.5906894840808644,
            "fpr": 0.24890350877192982,
            "logloss": 10.34448562045317,
            "mae": 0.37731146427129825,
            "precision": 0.6132879045996593,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.6388583973655324,
            "auc_prc": 0.7176739394979862,
            "auditor_fn_violation": 0.07810520982859073,
            "auditor_fp_violation": 0.07628723865928062,
            "ave_precision_score": 0.6089077463068315,
            "fpr": 0.23929747530186607,
            "logloss": 10.03887367802666,
            "mae": 0.3638390542142631,
            "precision": 0.6292517006802721,
            "recall": 0.7692307692307693
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6162280701754386,
            "auc_prc": 0.703950096083537,
            "auditor_fn_violation": 0.07784160453989096,
            "auditor_fp_violation": 0.09421831914638533,
            "ave_precision_score": 0.5892650383589415,
            "fpr": 0.2532894736842105,
            "logloss": 10.447487487373571,
            "mae": 0.3870185029598816,
            "precision": 0.6051282051282051,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7152104476603727,
            "auditor_fn_violation": 0.07974604681520159,
            "auditor_fp_violation": 0.08092308477778064,
            "ave_precision_score": 0.6075791481885291,
            "fpr": 0.24807903402854006,
            "logloss": 10.107797350317178,
            "mae": 0.3778966838522515,
            "precision": 0.6169491525423729,
            "recall": 0.7567567567567568
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.7076736082646712,
            "auditor_fn_violation": 0.07893809576796114,
            "auditor_fp_violation": 0.09180054349998001,
            "ave_precision_score": 0.5981448750377314,
            "fpr": 0.24561403508771928,
            "logloss": 10.200592303349817,
            "mae": 0.3809647610285566,
            "precision": 0.6124567474048442,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.7186975390927098,
            "auditor_fn_violation": 0.08568409666104507,
            "auditor_fp_violation": 0.07766573915707248,
            "ave_precision_score": 0.6164798602373619,
            "fpr": 0.23819978046103182,
            "logloss": 9.902139269497576,
            "mae": 0.37649467457006014,
            "precision": 0.6199649737302977,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7137803058717017,
            "auditor_fn_violation": 0.040129724416750126,
            "auditor_fp_violation": 0.05659293450025976,
            "ave_precision_score": 0.7070437626212926,
            "fpr": 0.15789473684210525,
            "logloss": 2.9921281207177444,
            "mae": 0.3243926792713032,
            "precision": 0.6929637526652452,
            "recall": 0.6871035940803383
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7479042641783357,
            "auditor_fn_violation": 0.0554712442747569,
            "auditor_fp_violation": 0.04751742271462488,
            "ave_precision_score": 0.7397628093577413,
            "fpr": 0.14489571899012074,
            "logloss": 3.0635833670998864,
            "mae": 0.30207038638638506,
            "precision": 0.7215189873417721,
            "recall": 0.7110187110187111
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 12669,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.707664645418526,
            "auditor_fn_violation": 0.07893809576796114,
            "auditor_fp_violation": 0.09251238860248573,
            "ave_precision_score": 0.5981330916711759,
            "fpr": 0.24451754385964913,
            "logloss": 10.2006366418004,
            "mae": 0.3808387947724519,
            "precision": 0.6135181975736569,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7186925150647427,
            "auditor_fn_violation": 0.08498348893519037,
            "auditor_fp_violation": 0.07766573915707248,
            "ave_precision_score": 0.6164780502354748,
            "fpr": 0.23819978046103182,
            "logloss": 9.902326969001331,
            "mae": 0.3765456778306631,
            "precision": 0.6206293706293706,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 12669,
        "test": {
            "accuracy": 0.5252192982456141,
            "auc_prc": 0.5391900571180146,
            "auditor_fn_violation": 0.05394597752308891,
            "auditor_fp_violation": 0.04645975702353835,
            "ave_precision_score": 0.5075338740952535,
            "fpr": 0.15789473684210525,
            "logloss": 13.249366925942235,
            "mae": 0.47849016469547434,
            "precision": 0.5609756097560976,
            "recall": 0.3890063424947146
        },
        "train": {
            "accuracy": 0.5137211855104281,
            "auc_prc": 0.5302532513340078,
            "auditor_fn_violation": 0.06196384681565802,
            "auditor_fp_violation": 0.031728486457509,
            "ave_precision_score": 0.506111629235261,
            "fpr": 0.16575192096597147,
            "logloss": 13.587774038229202,
            "mae": 0.49131132340601275,
            "precision": 0.5558823529411765,
            "recall": 0.39293139293139295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.7098027362380894,
            "auditor_fn_violation": 0.0771229739252995,
            "auditor_fp_violation": 0.08803900411621308,
            "ave_precision_score": 0.6084479664450793,
            "fpr": 0.24451754385964913,
            "logloss": 9.58612759023262,
            "mae": 0.37861452835030923,
            "precision": 0.6141868512110726,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6300768386388584,
            "auc_prc": 0.7179912168174089,
            "auditor_fn_violation": 0.08275386760567881,
            "auditor_fp_violation": 0.07848518111964875,
            "ave_precision_score": 0.6243386567714162,
            "fpr": 0.23600439077936333,
            "logloss": 9.242007611023613,
            "mae": 0.3717506406733165,
            "precision": 0.6254355400696864,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.7218297162070407,
            "auditor_fn_violation": 0.0774011535180446,
            "auditor_fp_violation": 0.06634895895775886,
            "ave_precision_score": 0.6261588923980527,
            "fpr": 0.17543859649122806,
            "logloss": 10.153854436599243,
            "mae": 0.35781443065002355,
            "precision": 0.6588486140724946,
            "recall": 0.653276955602537
        },
        "train": {
            "accuracy": 0.6333699231613611,
            "auc_prc": 0.7188629762001221,
            "auditor_fn_violation": 0.07672909758529957,
            "auditor_fp_violation": 0.06040640236897863,
            "ave_precision_score": 0.6276866200000895,
            "fpr": 0.17672886937431395,
            "logloss": 10.421723816711232,
            "mae": 0.3676330632000606,
            "precision": 0.6567164179104478,
            "recall": 0.6403326403326404
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7365964727429759,
            "auditor_fn_violation": 0.034348225214198294,
            "auditor_fp_violation": 0.06248251608520161,
            "ave_precision_score": 0.6950371583272719,
            "fpr": 0.16557017543859648,
            "logloss": 4.8692926574910045,
            "mae": 0.3358892996355847,
            "precision": 0.6967871485943775,
            "recall": 0.733615221987315
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7744084652148652,
            "auditor_fn_violation": 0.03916100513246507,
            "auditor_fp_violation": 0.05537487555203839,
            "ave_precision_score": 0.7370126828529611,
            "fpr": 0.16575192096597147,
            "logloss": 4.132722127769088,
            "mae": 0.30611330481948273,
            "precision": 0.7140151515151515,
            "recall": 0.7837837837837838
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.7056403251661016,
            "auditor_fn_violation": 0.07804328474463113,
            "auditor_fp_violation": 0.09124355592854574,
            "ave_precision_score": 0.5917070993104048,
            "fpr": 0.25,
            "logloss": 10.38994733253591,
            "mae": 0.3822875070503228,
            "precision": 0.6095890410958904,
            "recall": 0.7526427061310782
        },
        "train": {
            "accuracy": 0.6289791437980241,
            "auc_prc": 0.7179896056827767,
            "auditor_fn_violation": 0.07935808813964687,
            "auditor_fp_violation": 0.08056569575983459,
            "ave_precision_score": 0.6109647970590452,
            "fpr": 0.2414928649835346,
            "logloss": 9.975991384626049,
            "mae": 0.36979386743393716,
            "precision": 0.6226415094339622,
            "recall": 0.7546777546777547
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.7126362122800396,
            "auditor_fn_violation": 0.0782194651533697,
            "auditor_fp_violation": 0.09251238860248573,
            "ave_precision_score": 0.6058311034304388,
            "fpr": 0.24451754385964913,
            "logloss": 10.007025784500547,
            "mae": 0.37886815453443695,
            "precision": 0.6141868512110726,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7188945220055686,
            "auditor_fn_violation": 0.08498348893519037,
            "auditor_fp_violation": 0.07834988384856918,
            "ave_precision_score": 0.6178044241642451,
            "fpr": 0.23710208562019758,
            "logloss": 9.837433496343575,
            "mae": 0.37625674712598717,
            "precision": 0.6217162872154116,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.7114162544032938,
            "auditor_fn_violation": 0.0771229739252995,
            "auditor_fp_violation": 0.08803900411621308,
            "ave_precision_score": 0.6061260728495848,
            "fpr": 0.24451754385964913,
            "logloss": 9.799991983389155,
            "mae": 0.37807247728804144,
            "precision": 0.6141868512110726,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6289791437980241,
            "auc_prc": 0.7162806709751443,
            "auditor_fn_violation": 0.08275386760567881,
            "auditor_fp_violation": 0.07999897888852016,
            "ave_precision_score": 0.6176925878832509,
            "fpr": 0.23710208562019758,
            "logloss": 9.524412693685392,
            "mae": 0.37221906928513165,
            "precision": 0.6243478260869565,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.7056353857217261,
            "auditor_fn_violation": 0.07582712065576203,
            "auditor_fp_violation": 0.09312182791831515,
            "ave_precision_score": 0.6011195440638755,
            "fpr": 0.2532894736842105,
            "logloss": 9.900551787781536,
            "mae": 0.38736069078842966,
            "precision": 0.6044520547945206,
            "recall": 0.7463002114164905
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7146240161354726,
            "auditor_fn_violation": 0.07701664342718131,
            "auditor_fp_violation": 0.07825543103668343,
            "ave_precision_score": 0.6161118802756799,
            "fpr": 0.24698133918770582,
            "logloss": 9.556129566309815,
            "mae": 0.3771518316816983,
            "precision": 0.6179966044142614,
            "recall": 0.7567567567567568
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.7074732223889689,
            "auditor_fn_violation": 0.0771229739252995,
            "auditor_fp_violation": 0.09362636374535428,
            "ave_precision_score": 0.5926594694231156,
            "fpr": 0.24890350877192982,
            "logloss": 10.412566288635578,
            "mae": 0.3811585335196124,
            "precision": 0.6099656357388317,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.7143013440506759,
            "auditor_fn_violation": 0.08386753721550648,
            "auditor_fp_violation": 0.08041508181655734,
            "ave_precision_score": 0.6069680787956689,
            "fpr": 0.23819978046103182,
            "logloss": 10.098178177130634,
            "mae": 0.3727840725159074,
            "precision": 0.6232638888888888,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6718434135914764,
            "auditor_fn_violation": 0.03854873706464894,
            "auditor_fp_violation": 0.052414278863445636,
            "ave_precision_score": 0.6559418595433266,
            "fpr": 0.17982456140350878,
            "logloss": 3.1692465737118396,
            "mae": 0.3380106901042704,
            "precision": 0.6686868686868687,
            "recall": 0.6997885835095138
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7058713228023277,
            "auditor_fn_violation": 0.04569924987049027,
            "auditor_fp_violation": 0.04777780614198557,
            "ave_precision_score": 0.6879394077328685,
            "fpr": 0.1668496158068057,
            "logloss": 3.1031349139686952,
            "mae": 0.3041433413854634,
            "precision": 0.7059961315280464,
            "recall": 0.7588357588357588
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 12669,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8030318632544111,
            "auditor_fn_violation": 0.026325062126775718,
            "auditor_fp_violation": 0.021877372817008352,
            "ave_precision_score": 0.8036564362067816,
            "fpr": 0.11074561403508772,
            "logloss": 1.414586895891448,
            "mae": 0.2833005597128796,
            "precision": 0.764018691588785,
            "recall": 0.6913319238900634
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8322197534889488,
            "auditor_fn_violation": 0.0332252373964778,
            "auditor_fp_violation": 0.02084088530365303,
            "ave_precision_score": 0.8325279250302406,
            "fpr": 0.1119648737650933,
            "logloss": 1.2582986037825492,
            "mae": 0.2677399548000905,
            "precision": 0.7738359201773836,
            "recall": 0.7255717255717256
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 12669,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.6953154872291991,
            "auditor_fn_violation": 0.055003059975520206,
            "auditor_fp_violation": 0.05438746353354914,
            "ave_precision_score": 0.6783428876119425,
            "fpr": 0.16557017543859648,
            "logloss": 3.8657711847111096,
            "mae": 0.32401252502742456,
            "precision": 0.6880165289256198,
            "recall": 0.7040169133192389
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.7023106285917567,
            "auditor_fn_violation": 0.0714391669386181,
            "auditor_fp_violation": 0.04904398437699436,
            "ave_precision_score": 0.686031193476353,
            "fpr": 0.1668496158068057,
            "logloss": 4.212063329530907,
            "mae": 0.324669255007933,
            "precision": 0.689795918367347,
            "recall": 0.7027027027027027
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6107456140350878,
            "auc_prc": 0.7011361466573585,
            "auditor_fn_violation": 0.0778578316828011,
            "auditor_fp_violation": 0.09421831914638533,
            "ave_precision_score": 0.5834916108309015,
            "fpr": 0.2532894736842105,
            "logloss": 10.770488270775747,
            "mae": 0.38952272387948467,
            "precision": 0.6017241379310345,
            "recall": 0.7378435517970402
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7151720886699887,
            "auditor_fn_violation": 0.08045578298048112,
            "auditor_fp_violation": 0.08154851555918619,
            "ave_precision_score": 0.6055332146804648,
            "fpr": 0.24698133918770582,
            "logloss": 10.26273469413879,
            "mae": 0.37861481516376605,
            "precision": 0.6173469387755102,
            "recall": 0.7546777546777547
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 12669,
        "test": {
            "accuracy": 0.3925438596491228,
            "auc_prc": 0.48853622835879085,
            "auditor_fn_violation": 0.012872760654278404,
            "auditor_fp_violation": 0.013317747672141642,
            "ave_precision_score": 0.4388805077218263,
            "fpr": 0.2719298245614035,
            "logloss": 18.184604695089764,
            "mae": 0.613168067153806,
            "precision": 0.40240963855421685,
            "recall": 0.35306553911205074
        },
        "train": {
            "accuracy": 0.36443468715697036,
            "auc_prc": 0.4838755706466775,
            "auditor_fn_violation": 0.014842842504752507,
            "auditor_fp_violation": 0.010849309473361757,
            "ave_precision_score": 0.4414889513760702,
            "fpr": 0.2722283205268935,
            "logloss": 18.98107124688596,
            "mae": 0.6340775591921098,
            "precision": 0.3768844221105528,
            "recall": 0.31185031185031187
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7263589671530182,
            "auditor_fn_violation": 0.042844293609287494,
            "auditor_fp_violation": 0.05182981656875675,
            "ave_precision_score": 0.7214463671400965,
            "fpr": 0.14802631578947367,
            "logloss": 2.7671999585367355,
            "mae": 0.3192999136874554,
            "precision": 0.7071583514099783,
            "recall": 0.6892177589852009
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7452966254440445,
            "auditor_fn_violation": 0.056258572175147376,
            "auditor_fp_violation": 0.044214127077323674,
            "ave_precision_score": 0.7395065768190847,
            "fpr": 0.150384193194292,
            "logloss": 2.782810418062248,
            "mae": 0.3084828693577175,
            "precision": 0.7157676348547718,
            "recall": 0.7172557172557172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6206140350877193,
            "auc_prc": 0.7056009836412702,
            "auditor_fn_violation": 0.07927886576907385,
            "auditor_fp_violation": 0.09222015745514128,
            "ave_precision_score": 0.5957644625082759,
            "fpr": 0.24671052631578946,
            "logloss": 10.341500805544664,
            "mae": 0.383484935930823,
            "precision": 0.610051993067591,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7194610100964179,
            "auditor_fn_violation": 0.08498348893519037,
            "auditor_fp_violation": 0.07725218900773494,
            "ave_precision_score": 0.6165573116050905,
            "fpr": 0.23710208562019758,
            "logloss": 9.920159569555999,
            "mae": 0.3768256511854521,
            "precision": 0.6217162872154116,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.7094429168841986,
            "auditor_fn_violation": 0.0748789918771559,
            "auditor_fp_violation": 0.0908289373776126,
            "ave_precision_score": 0.5955465469192459,
            "fpr": 0.24890350877192982,
            "logloss": 10.191142621795727,
            "mae": 0.37823992747079227,
            "precision": 0.6113013698630136,
            "recall": 0.7547568710359408
        },
        "train": {
            "accuracy": 0.6355653128430296,
            "auc_prc": 0.7169797382102645,
            "auditor_fn_violation": 0.07880353544458923,
            "auditor_fp_violation": 0.07876343399790672,
            "ave_precision_score": 0.6119691772037048,
            "fpr": 0.23819978046103182,
            "logloss": 9.846591369278732,
            "mae": 0.3647546361872911,
            "precision": 0.62778730703259,
            "recall": 0.760914760914761
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.7111910386635617,
            "auditor_fn_violation": 0.08014585883312933,
            "auditor_fp_violation": 0.09012708308356314,
            "ave_precision_score": 0.6233770243221971,
            "fpr": 0.2412280701754386,
            "logloss": 8.918631219770027,
            "mae": 0.37538422726841914,
            "precision": 0.6153846153846154,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6256860592755215,
            "auc_prc": 0.7112592667993549,
            "auditor_fn_violation": 0.08518431460253635,
            "auditor_fp_violation": 0.07941694534500805,
            "ave_precision_score": 0.6225138816484401,
            "fpr": 0.2327113062568606,
            "logloss": 9.177111227615582,
            "mae": 0.37693146034873104,
            "precision": 0.624113475177305,
            "recall": 0.7318087318087318
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.7207089821888102,
            "auditor_fn_violation": 0.02521698008234116,
            "auditor_fp_violation": 0.0648078767533869,
            "ave_precision_score": 0.5828355644044211,
            "fpr": 0.32785087719298245,
            "logloss": 9.047356909792267,
            "mae": 0.3731318744222379,
            "precision": 0.5909712722298222,
            "recall": 0.9133192389006343
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.7327542772986779,
            "auditor_fn_violation": 0.021139183598020042,
            "auditor_fp_violation": 0.05042248487478621,
            "ave_precision_score": 0.5975101312364457,
            "fpr": 0.3205268935236004,
            "logloss": 8.76309548772952,
            "mae": 0.3584647804425155,
            "precision": 0.6054054054054054,
            "recall": 0.9313929313929314
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.7101564195954321,
            "auditor_fn_violation": 0.07784160453989096,
            "auditor_fp_violation": 0.09278214043080367,
            "ave_precision_score": 0.5995186548489261,
            "fpr": 0.24671052631578946,
            "logloss": 10.24189592642715,
            "mae": 0.379986867801756,
            "precision": 0.6113989637305699,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.717859997872738,
            "auditor_fn_violation": 0.08417333993623785,
            "auditor_fp_violation": 0.07725218900773494,
            "ave_precision_score": 0.6135789032900412,
            "fpr": 0.23710208562019758,
            "logloss": 9.976032794485159,
            "mae": 0.37559454728031005,
            "precision": 0.6217162872154116,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.7100987604624923,
            "auditor_fn_violation": 0.0789983680130559,
            "auditor_fp_violation": 0.09070405227190985,
            "ave_precision_score": 0.6022187008262025,
            "fpr": 0.24561403508771928,
            "logloss": 10.098300782986593,
            "mae": 0.3818789980470792,
            "precision": 0.6111111111111112,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.7164749247201151,
            "auditor_fn_violation": 0.08428288120933566,
            "auditor_fp_violation": 0.08122941822173435,
            "ave_precision_score": 0.614192469487431,
            "fpr": 0.23600439077936333,
            "logloss": 9.936280851283053,
            "mae": 0.37722403795902737,
            "precision": 0.6234676007005254,
            "recall": 0.7401247401247402
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.7043615634158171,
            "auditor_fn_violation": 0.07608907310559698,
            "auditor_fp_violation": 0.09421831914638533,
            "ave_precision_score": 0.5870901717573772,
            "fpr": 0.2532894736842105,
            "logloss": 10.616714838117504,
            "mae": 0.3861736605366824,
            "precision": 0.6044520547945206,
            "recall": 0.7463002114164905
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.7151891943421151,
            "auditor_fn_violation": 0.0800678243049264,
            "auditor_fp_violation": 0.08264621040002043,
            "ave_precision_score": 0.6055618164482556,
            "fpr": 0.24698133918770582,
            "logloss": 10.261791051429308,
            "mae": 0.3784579166683798,
            "precision": 0.616695059625213,
            "recall": 0.7525987525987526
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.7101760313138838,
            "auditor_fn_violation": 0.07784160453989096,
            "auditor_fp_violation": 0.09180054349998001,
            "ave_precision_score": 0.5995341065220788,
            "fpr": 0.24561403508771928,
            "logloss": 10.240056900875096,
            "mae": 0.3801429085736874,
            "precision": 0.6124567474048442,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7183358642743903,
            "auditor_fn_violation": 0.08498348893519037,
            "auditor_fp_violation": 0.07890638960508514,
            "ave_precision_score": 0.6151352186289709,
            "fpr": 0.23710208562019758,
            "logloss": 9.950999352997947,
            "mae": 0.3761326526911558,
            "precision": 0.6217162872154116,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 12669,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7040649780038996,
            "auditor_fn_violation": 0.031021660917621752,
            "auditor_fp_violation": 0.05038864244894697,
            "ave_precision_score": 0.697686779448875,
            "fpr": 0.17105263157894737,
            "logloss": 2.3295368470412523,
            "mae": 0.3097134987675796,
            "precision": 0.6953125,
            "recall": 0.7526427061310782
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7355043111200892,
            "auditor_fn_violation": 0.037129927360443286,
            "auditor_fp_violation": 0.04367804355040462,
            "ave_precision_score": 0.7270729237629967,
            "fpr": 0.16136114160263446,
            "logloss": 2.090434473012049,
            "mae": 0.28081320553329064,
            "precision": 0.7226415094339622,
            "recall": 0.7962577962577962
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.7106828798092437,
            "auditor_fn_violation": 0.07927886576907385,
            "auditor_fp_violation": 0.09251238860248573,
            "ave_precision_score": 0.6005280336742882,
            "fpr": 0.24451754385964913,
            "logloss": 10.20762694909104,
            "mae": 0.37970238839655285,
            "precision": 0.6121739130434782,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7192218770845524,
            "auditor_fn_violation": 0.08487622977194877,
            "auditor_fp_violation": 0.07834988384856918,
            "ave_precision_score": 0.6168793619153763,
            "fpr": 0.23710208562019758,
            "logloss": 9.926470674492526,
            "mae": 0.3764164554986873,
            "precision": 0.6210526315789474,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.7674675514234046,
            "auditor_fn_violation": 0.006226586550943959,
            "auditor_fp_violation": 0.01366243056388123,
            "ave_precision_score": 0.7628523002125744,
            "fpr": 0.1962719298245614,
            "logloss": 1.356288203443316,
            "mae": 0.3194319536412073,
            "precision": 0.6733576642335767,
            "recall": 0.7801268498942917
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8013978582242935,
            "auditor_fn_violation": 0.011440216709151949,
            "auditor_fp_violation": 0.010925892834350194,
            "ave_precision_score": 0.7961951897077838,
            "fpr": 0.1756311745334797,
            "logloss": 1.2354659693974674,
            "mae": 0.2842812340618081,
            "precision": 0.718804920913884,
            "recall": 0.8503118503118503
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.6942150261775614,
            "auditor_fn_violation": 0.08068135454916361,
            "auditor_fp_violation": 0.08909053670623027,
            "ave_precision_score": 0.6082146407094355,
            "fpr": 0.24890350877192982,
            "logloss": 8.855323275198474,
            "mae": 0.38360877069504695,
            "precision": 0.6099656357388317,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6344676180021954,
            "auc_prc": 0.7105196226850943,
            "auditor_fn_violation": 0.08164019799585114,
            "auditor_fp_violation": 0.07741301406581066,
            "ave_precision_score": 0.6283956106315844,
            "fpr": 0.24259055982436883,
            "logloss": 8.742472965021928,
            "mae": 0.37832156867004313,
            "precision": 0.6254237288135593,
            "recall": 0.7671517671517671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.7046953538917349,
            "auditor_fn_violation": 0.07839332739883537,
            "auditor_fp_violation": 0.09259481277224954,
            "ave_precision_score": 0.5868265854648049,
            "fpr": 0.25219298245614036,
            "logloss": 10.657830375418852,
            "mae": 0.38429314841188505,
            "precision": 0.6061643835616438,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6201975850713501,
            "auc_prc": 0.7191298883017906,
            "auditor_fn_violation": 0.08324223911490652,
            "auditor_fp_violation": 0.08196461848722335,
            "ave_precision_score": 0.608949287296892,
            "fpr": 0.24807903402854006,
            "logloss": 10.258548345175,
            "mae": 0.3796632586062791,
            "precision": 0.6149914821124361,
            "recall": 0.7505197505197505
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.7070065240190959,
            "auditor_fn_violation": 0.08033826638477802,
            "auditor_fp_violation": 0.0911236662270711,
            "ave_precision_score": 0.5950181697921532,
            "fpr": 0.24671052631578946,
            "logloss": 10.482256162755222,
            "mae": 0.38506494730852675,
            "precision": 0.60801393728223,
            "recall": 0.7378435517970402
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7164126669011301,
            "auditor_fn_violation": 0.08487622977194877,
            "auditor_fp_violation": 0.07725218900773494,
            "ave_precision_score": 0.609812344399794,
            "fpr": 0.23710208562019758,
            "logloss": 10.18523702812123,
            "mae": 0.3784680983992617,
            "precision": 0.6210526315789474,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 12669,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.7122849413209495,
            "auditor_fn_violation": 0.07893809576796114,
            "auditor_fp_violation": 0.09251238860248573,
            "ave_precision_score": 0.6028798490140066,
            "fpr": 0.24451754385964913,
            "logloss": 10.152440337931559,
            "mae": 0.3798829707909764,
            "precision": 0.6135181975736569,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7170482833314512,
            "auditor_fn_violation": 0.08498348893519037,
            "auditor_fp_violation": 0.07834988384856918,
            "ave_precision_score": 0.6136513972551942,
            "fpr": 0.23710208562019758,
            "logloss": 9.999712880336556,
            "mae": 0.37644727464046496,
            "precision": 0.6217162872154116,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.706541824554167,
            "auditor_fn_violation": 0.07767237862097104,
            "auditor_fp_violation": 0.09218269192343045,
            "ave_precision_score": 0.5999120405270735,
            "fpr": 0.25109649122807015,
            "logloss": 9.951815476590987,
            "mae": 0.38349019184589667,
            "precision": 0.6078767123287672,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.7180842271414678,
            "auditor_fn_violation": 0.08278353503380946,
            "auditor_fp_violation": 0.07999897888852016,
            "ave_precision_score": 0.6174756948463358,
            "fpr": 0.23710208562019758,
            "logloss": 9.629274917044405,
            "mae": 0.37046526089899884,
            "precision": 0.6262975778546713,
            "recall": 0.7525987525987526
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.7053437011125673,
            "auditor_fn_violation": 0.07927886576907385,
            "auditor_fp_violation": 0.09404847540262959,
            "ave_precision_score": 0.5927941420050163,
            "fpr": 0.25,
            "logloss": 10.395338569893674,
            "mae": 0.3851073019337643,
            "precision": 0.6068965517241379,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6289791437980241,
            "auc_prc": 0.7143782377410892,
            "auditor_fn_violation": 0.08351837440750724,
            "auditor_fp_violation": 0.07974880657595794,
            "ave_precision_score": 0.6075868830217397,
            "fpr": 0.23929747530186607,
            "logloss": 10.023944045725676,
            "mae": 0.3727541919159466,
            "precision": 0.6234887737478411,
            "recall": 0.7505197505197505
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.7079034105284945,
            "auditor_fn_violation": 0.0771229739252995,
            "auditor_fp_violation": 0.09362636374535428,
            "ave_precision_score": 0.5934803478410837,
            "fpr": 0.24890350877192982,
            "logloss": 10.388420526566438,
            "mae": 0.3810545183033281,
            "precision": 0.6099656357388317,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.714387891689672,
            "auditor_fn_violation": 0.08386753721550648,
            "auditor_fp_violation": 0.08041508181655734,
            "ave_precision_score": 0.6070229194592309,
            "fpr": 0.23819978046103182,
            "logloss": 10.093518318769762,
            "mae": 0.3727582773631552,
            "precision": 0.6232638888888888,
            "recall": 0.7463617463617463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6041666666666666,
            "auc_prc": 0.7125030935748189,
            "auditor_fn_violation": 0.07874337005303958,
            "auditor_fp_violation": 0.09392359029692683,
            "ave_precision_score": 0.5830301632533864,
            "fpr": 0.2631578947368421,
            "logloss": 11.65243549053344,
            "mae": 0.3932983236768791,
            "precision": 0.5945945945945946,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6180021953896817,
            "auc_prc": 0.7253634471416032,
            "auditor_fn_violation": 0.08351381018779483,
            "auditor_fp_violation": 0.08075204860490644,
            "ave_precision_score": 0.6056296320404312,
            "fpr": 0.2535675082327113,
            "logloss": 10.911160407997418,
            "mae": 0.3831187063737724,
            "precision": 0.611764705882353,
            "recall": 0.7567567567567568
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.7071789181938077,
            "auditor_fn_violation": 0.07893809576796114,
            "auditor_fp_violation": 0.09010460376453663,
            "ave_precision_score": 0.5918291452600055,
            "fpr": 0.25,
            "logloss": 10.432702805102696,
            "mae": 0.383810133088041,
            "precision": 0.6082474226804123,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.7145985620857547,
            "auditor_fn_violation": 0.08424180323192398,
            "auditor_fp_violation": 0.07974880657595794,
            "ave_precision_score": 0.6071099902161968,
            "fpr": 0.23929747530186607,
            "logloss": 10.100455301954195,
            "mae": 0.3732975585610386,
            "precision": 0.6228373702422145,
            "recall": 0.7484407484407485
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.7088105380311598,
            "auditor_fn_violation": 0.07780451392752495,
            "auditor_fp_violation": 0.08613325740318908,
            "ave_precision_score": 0.6063560790852383,
            "fpr": 0.24232456140350878,
            "logloss": 9.822072962799908,
            "mae": 0.37989704065717694,
            "precision": 0.6136363636363636,
            "recall": 0.7420718816067653
        },
        "train": {
            "accuracy": 0.6256860592755215,
            "auc_prc": 0.7167445162947543,
            "auditor_fn_violation": 0.08377853493111452,
            "auditor_fp_violation": 0.07642508870905981,
            "ave_precision_score": 0.6200000324789715,
            "fpr": 0.2349066959385291,
            "logloss": 9.508858105031118,
            "mae": 0.37552531555889024,
            "precision": 0.6232394366197183,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 12669,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.8048444790607344,
            "auditor_fn_violation": 0.005405956752345982,
            "auditor_fp_violation": 0.022002257922711106,
            "ave_precision_score": 0.8052158312950839,
            "fpr": 0.1611842105263158,
            "logloss": 1.0853840505717804,
            "mae": 0.28771832407531905,
            "precision": 0.7106299212598425,
            "recall": 0.7632135306553911
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8401623656585984,
            "auditor_fn_violation": 0.01210659278716359,
            "auditor_fp_violation": 0.017139356189211957,
            "ave_precision_score": 0.8404478058740283,
            "fpr": 0.15477497255762898,
            "logloss": 0.9169726045995775,
            "mae": 0.2588074875978418,
            "precision": 0.7349624060150376,
            "recall": 0.8128898128898129
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 12669,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.7117438399653946,
            "auditor_fn_violation": 0.0771229739252995,
            "auditor_fp_violation": 0.08900311713223835,
            "ave_precision_score": 0.6068586037397502,
            "fpr": 0.24561403508771928,
            "logloss": 9.782091397174163,
            "mae": 0.3780856652074699,
            "precision": 0.613126079447323,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.7157759620871909,
            "auditor_fn_violation": 0.08346816799067075,
            "auditor_fp_violation": 0.07999897888852016,
            "ave_precision_score": 0.6167751871047511,
            "fpr": 0.23710208562019758,
            "logloss": 9.544541314600085,
            "mae": 0.37208990479078147,
            "precision": 0.6236933797909407,
            "recall": 0.7442827442827443
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.6872338060597066,
            "auditor_fn_violation": 0.06499666184488706,
            "auditor_fp_violation": 0.06431582943691803,
            "ave_precision_score": 0.6511079061897244,
            "fpr": 0.1962719298245614,
            "logloss": 5.589423206111983,
            "mae": 0.33936807408698194,
            "precision": 0.657088122605364,
            "recall": 0.7251585623678647
        },
        "train": {
            "accuracy": 0.6586169045005489,
            "auc_prc": 0.7098538277268136,
            "auditor_fn_violation": 0.07723344386352071,
            "auditor_fp_violation": 0.05817782656421515,
            "ave_precision_score": 0.674329965969823,
            "fpr": 0.18880351262349068,
            "logloss": 5.342937098630366,
            "mae": 0.3459164929866529,
            "precision": 0.6653696498054474,
            "recall": 0.7110187110187111
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 12669,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.711244923827469,
            "auditor_fn_violation": 0.07640434331070806,
            "auditor_fp_violation": 0.08941024257682932,
            "ave_precision_score": 0.6065157171954737,
            "fpr": 0.24671052631578946,
            "logloss": 9.777823592727627,
            "mae": 0.3775730321455108,
            "precision": 0.612736660929432,
            "recall": 0.7526427061310782
        },
        "train": {
            "accuracy": 0.6311745334796927,
            "auc_prc": 0.7160740557550045,
            "auditor_fn_violation": 0.08314410839108974,
            "auditor_fp_violation": 0.07848518111964875,
            "ave_precision_score": 0.6170575670573017,
            "fpr": 0.23600439077936333,
            "logloss": 9.533146862239615,
            "mae": 0.3707758619771719,
            "precision": 0.6260869565217392,
            "recall": 0.7484407484407485
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 12669,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8102177248223925,
            "auditor_fn_violation": 0.0019472571492155366,
            "auditor_fp_violation": 0.02104563801302802,
            "ave_precision_score": 0.8105706690333944,
            "fpr": 0.14692982456140352,
            "logloss": 0.9628904484923448,
            "mae": 0.28625225350696754,
            "precision": 0.728744939271255,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8383500644592776,
            "auditor_fn_violation": 0.014240365502714571,
            "auditor_fp_violation": 0.017315497919485362,
            "ave_precision_score": 0.838636408152367,
            "fpr": 0.1394072447859495,
            "logloss": 0.8327306617503434,
            "mae": 0.2613554802214958,
            "precision": 0.7509803921568627,
            "recall": 0.7962577962577962
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.7106828798092437,
            "auditor_fn_violation": 0.07927886576907385,
            "auditor_fp_violation": 0.09251238860248573,
            "ave_precision_score": 0.6005280336742882,
            "fpr": 0.24451754385964913,
            "logloss": 10.20723001210948,
            "mae": 0.37970020592669573,
            "precision": 0.6121739130434782,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7192238013117283,
            "auditor_fn_violation": 0.08487622977194877,
            "auditor_fp_violation": 0.07834988384856918,
            "ave_precision_score": 0.616881347912605,
            "fpr": 0.23710208562019758,
            "logloss": 9.926384398667237,
            "mae": 0.3764193841493724,
            "precision": 0.6210526315789474,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6282894736842105,
            "auc_prc": 0.7120344010140518,
            "auditor_fn_violation": 0.07750083453877823,
            "auditor_fp_violation": 0.0909738041002278,
            "ave_precision_score": 0.6054312473902296,
            "fpr": 0.24342105263157895,
            "logloss": 9.979762486484793,
            "mae": 0.37856742309222063,
            "precision": 0.615916955017301,
            "recall": 0.7526427061310782
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7184095941458968,
            "auditor_fn_violation": 0.08498348893519037,
            "auditor_fp_violation": 0.07834988384856918,
            "ave_precision_score": 0.6174622239281061,
            "fpr": 0.23710208562019758,
            "logloss": 9.805790549376201,
            "mae": 0.376093925609265,
            "precision": 0.6217162872154116,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6019736842105263,
            "auc_prc": 0.6938921694140171,
            "auditor_fn_violation": 0.08334492414969771,
            "auditor_fp_violation": 0.09726801342764657,
            "ave_precision_score": 0.5755194760090631,
            "fpr": 0.2675438596491228,
            "logloss": 10.602068847370376,
            "mae": 0.4063974684270189,
            "precision": 0.5919732441471572,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.7082622282120578,
            "auditor_fn_violation": 0.07838590934090385,
            "auditor_fp_violation": 0.08662854517141912,
            "ave_precision_score": 0.5976619439036266,
            "fpr": 0.2601536772777168,
            "logloss": 10.21772253705909,
            "mae": 0.39718734084513824,
            "precision": 0.6121112929623568,
            "recall": 0.7775467775467776
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.709706109969582,
            "auditor_fn_violation": 0.07886623270650199,
            "auditor_fp_violation": 0.0879940454781601,
            "ave_precision_score": 0.5974934946467834,
            "fpr": 0.24890350877192982,
            "logloss": 10.086346575673419,
            "mae": 0.375739192237595,
            "precision": 0.6132879045996593,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.6366630076838639,
            "auc_prc": 0.721045619698016,
            "auditor_fn_violation": 0.07885830608113814,
            "auditor_fp_violation": 0.07665228601332554,
            "ave_precision_score": 0.6152308502736599,
            "fpr": 0.24039517014270034,
            "logloss": 9.830677309198869,
            "mae": 0.3628572012523038,
            "precision": 0.6275510204081632,
            "recall": 0.7671517671517671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 12669,
        "test": {
            "accuracy": 0.5899122807017544,
            "auc_prc": 0.7157956504870675,
            "auditor_fn_violation": 0.019808705166722305,
            "auditor_fp_violation": 0.044266774567398005,
            "ave_precision_score": 0.5615590777849555,
            "fpr": 0.3574561403508772,
            "logloss": 10.875155203860546,
            "mae": 0.4109824023537302,
            "precision": 0.5659121171770972,
            "recall": 0.8985200845665962
        },
        "train": {
            "accuracy": 0.6158068057080132,
            "auc_prc": 0.7339344821684697,
            "auditor_fn_violation": 0.0179008697120662,
            "auditor_fp_violation": 0.043343629540755116,
            "ave_precision_score": 0.5843813406449515,
            "fpr": 0.34357848518111966,
            "logloss": 10.17416128450711,
            "mae": 0.38689660177315993,
            "precision": 0.5865257595772787,
            "recall": 0.9230769230769231
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.7097264116077429,
            "auditor_fn_violation": 0.07886623270650199,
            "auditor_fp_violation": 0.0879940454781601,
            "ave_precision_score": 0.5975115111635181,
            "fpr": 0.24890350877192982,
            "logloss": 10.085199303697667,
            "mae": 0.37572036713404733,
            "precision": 0.6132879045996593,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.6366630076838639,
            "auc_prc": 0.7214703221699698,
            "auditor_fn_violation": 0.07885830608113814,
            "auditor_fp_violation": 0.07665228601332554,
            "ave_precision_score": 0.6160720185479136,
            "fpr": 0.24039517014270034,
            "logloss": 9.81017172192324,
            "mae": 0.3628318443103724,
            "precision": 0.6275510204081632,
            "recall": 0.7671517671517671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.7053402658395626,
            "auditor_fn_violation": 0.07821714699009681,
            "auditor_fp_violation": 0.09133347320465172,
            "ave_precision_score": 0.5834409525674666,
            "fpr": 0.2532894736842105,
            "logloss": 11.004054793772138,
            "mae": 0.3855779570879391,
            "precision": 0.60580204778157,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.7192708158072982,
            "auditor_fn_violation": 0.08397707848860429,
            "auditor_fp_violation": 0.08093840144997831,
            "ave_precision_score": 0.6048478700029076,
            "fpr": 0.24368825466520308,
            "logloss": 10.558120129242203,
            "mae": 0.37815104852364545,
            "precision": 0.6205128205128205,
            "recall": 0.7546777546777547
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.6718651125020376,
            "auditor_fn_violation": 0.038606691146470834,
            "auditor_fp_violation": 0.052414278863445636,
            "ave_precision_score": 0.6559700011979077,
            "fpr": 0.17982456140350878,
            "logloss": 3.169011034758307,
            "mae": 0.33799330303464553,
            "precision": 0.6680161943319838,
            "recall": 0.6976744186046512
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7059143890685085,
            "auditor_fn_violation": 0.04569924987049027,
            "auditor_fp_violation": 0.04777780614198557,
            "ave_precision_score": 0.6879954873748826,
            "fpr": 0.1668496158068057,
            "logloss": 3.103036740823861,
            "mae": 0.30415514025090323,
            "precision": 0.7059961315280464,
            "recall": 0.7588357588357588
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 12669,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.7241836396280628,
            "auditor_fn_violation": 0.06282686102147546,
            "auditor_fp_violation": 0.07717399992007354,
            "ave_precision_score": 0.6288775748110939,
            "fpr": 0.23464912280701755,
            "logloss": 8.791595680244674,
            "mae": 0.36094890273848834,
            "precision": 0.627177700348432,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.7350033437692168,
            "auditor_fn_violation": 0.06836288285245476,
            "auditor_fp_violation": 0.0689913971357823,
            "ave_precision_score": 0.645405908703717,
            "fpr": 0.22941822173435786,
            "logloss": 8.701975791692838,
            "mae": 0.35671161003543056,
            "precision": 0.6384083044982699,
            "recall": 0.7671517671517671
        }
    }
]
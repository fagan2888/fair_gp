[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 871,
        "test": {
            "accuracy": 0.6282894736842105,
            "auc_prc": 0.6449371576601826,
            "auditor_fn_violation": 0.06882192039237879,
            "auditor_fp_violation": 0.018510341850151105,
            "ave_precision_score": 0.6324317186579389,
            "fpr": 0.07456140350877193,
            "logloss": 7.685732922006764,
            "mae": 0.38440276401822554,
            "precision": 0.7404580152671756,
            "recall": 0.4172043010752688
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.6221321957819934,
            "auditor_fn_violation": 0.07862323476527512,
            "auditor_fp_violation": 0.010909317920518575,
            "ave_precision_score": 0.6112597568510929,
            "fpr": 0.07464324917672886,
            "logloss": 9.720447398422527,
            "mae": 0.42499753831407017,
            "precision": 0.7322834645669292,
            "recall": 0.3803680981595092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 871,
        "test": {
            "accuracy": 0.6359649122807017,
            "auc_prc": 0.6460725310000157,
            "auditor_fn_violation": 0.07316072439162422,
            "auditor_fp_violation": 0.019655893088425764,
            "ave_precision_score": 0.633565336876409,
            "fpr": 0.07785087719298246,
            "logloss": 7.670365288390263,
            "mae": 0.38031991194508,
            "precision": 0.7418181818181818,
            "recall": 0.43870967741935485
        },
        "train": {
            "accuracy": 0.5993413830954994,
            "auc_prc": 0.6227880887817719,
            "auditor_fn_violation": 0.08280749485385396,
            "auditor_fp_violation": 0.011361921954417053,
            "ave_precision_score": 0.6119145599547354,
            "fpr": 0.07683863885839737,
            "logloss": 9.710021323680692,
            "mae": 0.4213167845091568,
            "precision": 0.7348484848484849,
            "recall": 0.3967280163599182
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 871,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.827492457065216,
            "auditor_fn_violation": 0.014905206564799095,
            "auditor_fp_violation": 0.007977157659248797,
            "ave_precision_score": 0.8276298293922497,
            "fpr": 0.08333333333333333,
            "logloss": 1.0387230763287614,
            "mae": 0.2880272277157134,
            "precision": 0.8,
            "recall": 0.6537634408602151
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.8453142380963791,
            "auditor_fn_violation": 0.010034143023576867,
            "auditor_fp_violation": 0.01049313030314066,
            "ave_precision_score": 0.8452652122493174,
            "fpr": 0.09220636663007684,
            "logloss": 1.2328470087648944,
            "mae": 0.2950598816970664,
            "precision": 0.7894736842105263,
            "recall": 0.6441717791411042
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 871,
        "test": {
            "accuracy": 0.631578947368421,
            "auc_prc": 0.6488775382592212,
            "auditor_fn_violation": 0.07099132239200151,
            "auditor_fp_violation": 0.018409768829231916,
            "ave_precision_score": 0.6333334738683325,
            "fpr": 0.07675438596491228,
            "logloss": 7.680983541865947,
            "mae": 0.3792186544785379,
            "precision": 0.7397769516728625,
            "recall": 0.42795698924731185
        },
        "train": {
            "accuracy": 0.5949506037321625,
            "auc_prc": 0.6273185504119132,
            "auditor_fn_violation": 0.08228446234278158,
            "auditor_fp_violation": 0.012540253146118273,
            "ave_precision_score": 0.6127005157852046,
            "fpr": 0.0801317233809001,
            "logloss": 9.715994516314563,
            "mae": 0.4211836492312306,
            "precision": 0.7255639097744361,
            "recall": 0.3946830265848671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 871,
        "test": {
            "accuracy": 0.6392543859649122,
            "auc_prc": 0.6505342551742443,
            "auditor_fn_violation": 0.07142520279192606,
            "auditor_fp_violation": 0.019969877153734446,
            "ave_precision_score": 0.6349898799441304,
            "fpr": 0.07017543859649122,
            "logloss": 7.663843883560038,
            "mae": 0.3781834355054407,
            "precision": 0.7575757575757576,
            "recall": 0.43010752688172044
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.6286480526960105,
            "auditor_fn_violation": 0.07914626727634749,
            "auditor_fp_violation": 0.009952086400549367,
            "ave_precision_score": 0.614028297899923,
            "fpr": 0.07574094401756312,
            "logloss": 9.706252694024865,
            "mae": 0.4211066792768999,
            "precision": 0.73046875,
            "recall": 0.3824130879345603
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 871,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.7947316492054022,
            "auditor_fn_violation": 0.025504621769477452,
            "auditor_fp_violation": 0.009451410965893482,
            "ave_precision_score": 0.7949973370842143,
            "fpr": 0.07675438596491228,
            "logloss": 0.7325469280871673,
            "mae": 0.34983359148898796,
            "precision": 0.7734627831715211,
            "recall": 0.513978494623656
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.8155619715336282,
            "auditor_fn_violation": 0.028984531257365657,
            "auditor_fp_violation": 0.007439353660630215,
            "ave_precision_score": 0.8158200473455043,
            "fpr": 0.06695938529088913,
            "logloss": 0.771073801497839,
            "mae": 0.34473399793032156,
            "precision": 0.8087774294670846,
            "recall": 0.5276073619631901
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 871,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.6352269483461427,
            "auditor_fn_violation": 0.010681946802490096,
            "auditor_fp_violation": 0.06162673181836023,
            "ave_precision_score": 0.5410040731718075,
            "fpr": 0.30372807017543857,
            "logloss": 7.827035407468301,
            "mae": 0.3790530903363391,
            "precision": 0.593841642228739,
            "recall": 0.8709677419354839
        },
        "train": {
            "accuracy": 0.6366630076838639,
            "auc_prc": 0.6792213827460967,
            "auditor_fn_violation": 0.017475571239048308,
            "auditor_fp_violation": 0.056892847295560836,
            "ave_precision_score": 0.579278250957039,
            "fpr": 0.2864983534577388,
            "logloss": 7.9348061158731715,
            "mae": 0.36413640473040193,
            "precision": 0.6161764705882353,
            "recall": 0.8568507157464212
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 871,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7815757074078695,
            "auditor_fn_violation": 0.005694680249009624,
            "auditor_fp_violation": 0.02314896581498489,
            "ave_precision_score": 0.7819145030516647,
            "fpr": 0.1962719298245614,
            "logloss": 0.8014430627394139,
            "mae": 0.3198916991997912,
            "precision": 0.6727605118829981,
            "recall": 0.7913978494623656
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7622149314844737,
            "auditor_fn_violation": 0.003991209462174425,
            "auditor_fp_violation": 0.023519802727069366,
            "ave_precision_score": 0.7630908523097981,
            "fpr": 0.18221734357848518,
            "logloss": 0.8584802149180986,
            "mae": 0.314264419644367,
            "precision": 0.7003610108303249,
            "recall": 0.7934560327198364
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 871,
        "test": {
            "accuracy": 0.6282894736842105,
            "auc_prc": 0.6468985426442078,
            "auditor_fn_violation": 0.06491699679305792,
            "auditor_fp_violation": 0.020274049217002234,
            "ave_precision_score": 0.6313546649709313,
            "fpr": 0.06469298245614036,
            "logloss": 7.712007828814471,
            "mae": 0.3912454191885062,
            "precision": 0.7581967213114754,
            "recall": 0.3978494623655914
        },
        "train": {
            "accuracy": 0.5850713501646543,
            "auc_prc": 0.6285368494305121,
            "auditor_fn_violation": 0.07234684463240691,
            "auditor_fp_violation": 0.008198896062344908,
            "ave_precision_score": 0.6139148076770298,
            "fpr": 0.06915477497255763,
            "logloss": 9.729282934189602,
            "mae": 0.4295400870131616,
            "precision": 0.7341772151898734,
            "recall": 0.3558282208588957
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 871,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8591423050782668,
            "auditor_fn_violation": 0.004954253914355784,
            "auditor_fp_violation": 0.01584638329604773,
            "ave_precision_score": 0.8593368090686964,
            "fpr": 0.14035087719298245,
            "logloss": 0.5664634271158878,
            "mae": 0.26736290640451843,
            "precision": 0.7398373983739838,
            "recall": 0.7827956989247312
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8594179351862272,
            "auditor_fn_violation": 0.0135270124966609,
            "auditor_fp_violation": 0.01880127561504727,
            "ave_precision_score": 0.859618520915997,
            "fpr": 0.1207464324917673,
            "logloss": 0.6183461973623876,
            "mae": 0.2642566884513985,
            "precision": 0.780439121756487,
            "recall": 0.7995910020449898
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 871,
        "test": {
            "accuracy": 0.6348684210526315,
            "auc_prc": 0.6460862171142517,
            "auditor_fn_violation": 0.07229296359177514,
            "auditor_fp_violation": 0.019506260057302095,
            "ave_precision_score": 0.6335790108318229,
            "fpr": 0.07675438596491228,
            "logloss": 7.67137303587851,
            "mae": 0.38051872670581477,
            "precision": 0.7426470588235294,
            "recall": 0.4344086021505376
        },
        "train": {
            "accuracy": 0.5993413830954994,
            "auc_prc": 0.6228115702710473,
            "auditor_fn_violation": 0.08280749485385396,
            "auditor_fp_violation": 0.011361921954417053,
            "ave_precision_score": 0.6119379725375407,
            "fpr": 0.07683863885839737,
            "logloss": 9.710892870606065,
            "mae": 0.42154303508204094,
            "precision": 0.7348484848484849,
            "recall": 0.3967280163599182
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 871,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.642059371456116,
            "auditor_fn_violation": 0.07012356159215243,
            "auditor_fp_violation": 0.01954550806546568,
            "ave_precision_score": 0.629560111808569,
            "fpr": 0.0800438596491228,
            "logloss": 7.687366184682289,
            "mae": 0.38268177934966274,
            "precision": 0.7296296296296296,
            "recall": 0.4236559139784946
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.623479062827659,
            "auditor_fn_violation": 0.0801923322984922,
            "auditor_fp_violation": 0.01164805094136437,
            "ave_precision_score": 0.6126038955188092,
            "fpr": 0.07903402854006586,
            "logloss": 9.603808652084329,
            "mae": 0.42153910146107865,
            "precision": 0.7241379310344828,
            "recall": 0.38650306748466257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 871,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6587312908352214,
            "auditor_fn_violation": 0.07576400679117148,
            "auditor_fp_violation": 0.0190083009537266,
            "ave_precision_score": 0.6431632059339769,
            "fpr": 0.0800438596491228,
            "logloss": 7.610508794960256,
            "mae": 0.374715314172545,
            "precision": 0.7420494699646644,
            "recall": 0.45161290322580644
        },
        "train": {
            "accuracy": 0.5982436882546652,
            "auc_prc": 0.6346360882602029,
            "auditor_fn_violation": 0.08385355987599866,
            "auditor_fp_violation": 0.011970596344832251,
            "ave_precision_score": 0.6200052213670935,
            "fpr": 0.0801317233809001,
            "logloss": 9.648169755659753,
            "mae": 0.41502226357266475,
            "precision": 0.7286245353159851,
            "recall": 0.40081799591002043
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 871,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6626237893546498,
            "auditor_fn_violation": 0.07272684399169968,
            "auditor_fp_violation": 0.01796332273637113,
            "ave_precision_score": 0.6470503658556861,
            "fpr": 0.07236842105263158,
            "logloss": 7.612350570162984,
            "mae": 0.36913269813274796,
            "precision": 0.7546468401486989,
            "recall": 0.43655913978494626
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.6319125028783622,
            "auditor_fn_violation": 0.07862323476527512,
            "auditor_fp_violation": 0.010290238839668923,
            "ave_precision_score": 0.6172885657034561,
            "fpr": 0.07574094401756312,
            "logloss": 9.679666344653093,
            "mae": 0.41387008989196306,
            "precision": 0.7294117647058823,
            "recall": 0.3803680981595092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 871,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8495588901211991,
            "auditor_fn_violation": 0.005058007923033393,
            "auditor_fp_violation": 0.020938812355272972,
            "ave_precision_score": 0.8498471345489791,
            "fpr": 0.15350877192982457,
            "logloss": 0.5958199336646406,
            "mae": 0.2776941614605388,
            "precision": 0.7281553398058253,
            "recall": 0.8064516129032258
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8571826682034203,
            "auditor_fn_violation": 0.009921904287295249,
            "auditor_fp_violation": 0.017219762669011197,
            "ave_precision_score": 0.8573841248270693,
            "fpr": 0.14050493962678376,
            "logloss": 0.6171842799347143,
            "mae": 0.2771164711186717,
            "precision": 0.7504873294346979,
            "recall": 0.787321063394683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 871,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.6562481417803754,
            "auditor_fn_violation": 0.07749952839086964,
            "auditor_fp_violation": 0.018409768829231916,
            "ave_precision_score": 0.6406902084997361,
            "fpr": 0.07675438596491228,
            "logloss": 7.666737467404717,
            "mae": 0.36984406327092056,
            "precision": 0.7535211267605634,
            "recall": 0.46021505376344085
        },
        "train": {
            "accuracy": 0.6026344676180022,
            "auc_prc": 0.630876359971664,
            "auditor_fn_violation": 0.08542265740921572,
            "auditor_fp_violation": 0.011835335369184432,
            "ave_precision_score": 0.6162529782779296,
            "fpr": 0.07903402854006586,
            "logloss": 9.72169472108864,
            "mae": 0.4144274094446445,
            "precision": 0.7343173431734318,
            "recall": 0.4069529652351738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 871,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8557703625001746,
            "auditor_fn_violation": 0.0063808715336728964,
            "auditor_fp_violation": 0.01548824522155501,
            "ave_precision_score": 0.8559746851025165,
            "fpr": 0.13596491228070176,
            "logloss": 0.5798588334387655,
            "mae": 0.26815630387639094,
            "precision": 0.7448559670781894,
            "recall": 0.7784946236559139
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8559895692322632,
            "auditor_fn_violation": 0.011154285611667446,
            "auditor_fp_violation": 0.013838238277815643,
            "ave_precision_score": 0.8561986313301929,
            "fpr": 0.11964873765093303,
            "logloss": 0.6365361905181175,
            "mae": 0.26535459159803926,
            "precision": 0.782435129740519,
            "recall": 0.8016359918200409
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 871,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.6488016149656313,
            "auditor_fn_violation": 0.0636153555932843,
            "auditor_fp_violation": 0.018225793790965113,
            "ave_precision_score": 0.633257684939488,
            "fpr": 0.06469298245614036,
            "logloss": 7.71233131093194,
            "mae": 0.3852022459296657,
            "precision": 0.7551867219917012,
            "recall": 0.3913978494623656
        },
        "train": {
            "accuracy": 0.5806805708013172,
            "auc_prc": 0.6267930096830272,
            "auditor_fn_violation": 0.07130077961026221,
            "auditor_fp_violation": 0.006708424157610254,
            "ave_precision_score": 0.6121759532398681,
            "fpr": 0.07135016465422613,
            "logloss": 9.744264902591189,
            "mae": 0.42733655899059775,
            "precision": 0.7257383966244726,
            "recall": 0.35173824130879344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 871,
        "test": {
            "accuracy": 0.6293859649122807,
            "auc_prc": 0.6466949194729613,
            "auditor_fn_violation": 0.06752027919260517,
            "auditor_fp_violation": 0.01761499666391931,
            "ave_precision_score": 0.6311562979136937,
            "fpr": 0.07017543859649122,
            "logloss": 7.703594978578238,
            "mae": 0.384044084503661,
            "precision": 0.7490196078431373,
            "recall": 0.410752688172043
        },
        "train": {
            "accuracy": 0.5817782656421515,
            "auc_prc": 0.6251869246964452,
            "auditor_fn_violation": 0.07443897467669633,
            "auditor_fp_violation": 0.010987353098776931,
            "ave_precision_score": 0.6105726568222757,
            "fpr": 0.07683863885839737,
            "logloss": 9.736014355104091,
            "mae": 0.42613850688410093,
            "precision": 0.717741935483871,
            "recall": 0.36400817995910023
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 871,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.7433506784454511,
            "auditor_fn_violation": 0.014098754951895867,
            "auditor_fp_violation": 0.02056840927822914,
            "ave_precision_score": 0.6603621491887071,
            "fpr": 0.17214912280701755,
            "logloss": 8.658555130283343,
            "mae": 0.34132453587224615,
            "precision": 0.6652452025586354,
            "recall": 0.6709677419354839
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.7653150116890873,
            "auditor_fn_violation": 0.01994931298669523,
            "auditor_fp_violation": 0.00997809812663549,
            "ave_precision_score": 0.6795689996545174,
            "fpr": 0.16245883644346873,
            "logloss": 9.048806611314724,
            "mae": 0.3332730506702822,
            "precision": 0.6935817805383023,
            "recall": 0.6850715746421268
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 871,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8595276277473038,
            "auditor_fn_violation": 0.006777023203169212,
            "auditor_fp_violation": 0.021566780485890353,
            "ave_precision_score": 0.8597153475842678,
            "fpr": 0.21162280701754385,
            "logloss": 0.7492268599132863,
            "mae": 0.27683433356849074,
            "precision": 0.6836065573770492,
            "recall": 0.896774193548387
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8659755284579894,
            "auditor_fn_violation": 0.008108126308984264,
            "auditor_fp_violation": 0.021017474677584653,
            "ave_precision_score": 0.8660307858548081,
            "fpr": 0.18111964873765093,
            "logloss": 0.9701865217997638,
            "mae": 0.2687097318733339,
            "precision": 0.7226890756302521,
            "recall": 0.8793456032719836
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 871,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8492678586174807,
            "auditor_fn_violation": 0.0072604225617807945,
            "auditor_fp_violation": 0.021377899446603087,
            "ave_precision_score": 0.8495168020771044,
            "fpr": 0.15460526315789475,
            "logloss": 0.59790803747252,
            "mae": 0.2791661569531323,
            "precision": 0.7262135922330097,
            "recall": 0.8043010752688172
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8549694247498145,
            "auditor_fn_violation": 0.007021655341778177,
            "auditor_fp_violation": 0.014592578334313114,
            "ave_precision_score": 0.8551743548324597,
            "fpr": 0.14270032930845225,
            "logloss": 0.620527883971848,
            "mae": 0.27875757468754425,
            "precision": 0.749034749034749,
            "recall": 0.7934560327198364
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 871,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6504168350966928,
            "auditor_fn_violation": 0.07142520279192606,
            "auditor_fp_violation": 0.019435123042505595,
            "ave_precision_score": 0.6348727109597585,
            "fpr": 0.06907894736842106,
            "logloss": 7.6636608289175925,
            "mae": 0.3782222428496813,
            "precision": 0.7604562737642585,
            "recall": 0.43010752688172044
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.6286580449267998,
            "auditor_fn_violation": 0.07862323476527512,
            "auditor_fp_violation": 0.009952086400549367,
            "ave_precision_score": 0.6140382871358915,
            "fpr": 0.07574094401756312,
            "logloss": 9.705991980344058,
            "mae": 0.42113566481172576,
            "precision": 0.7294117647058823,
            "recall": 0.3803680981595092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 871,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8169446132653365,
            "auditor_fn_violation": 0.011389360498019254,
            "auditor_fp_violation": 0.008489834765885636,
            "ave_precision_score": 0.8171161805142965,
            "fpr": 0.08771929824561403,
            "logloss": 1.040396214928329,
            "mae": 0.2963813730278,
            "precision": 0.7927461139896373,
            "recall": 0.6580645161290323
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.8359019540196904,
            "auditor_fn_violation": 0.012849090529519908,
            "auditor_fp_violation": 0.01083908626008605,
            "ave_precision_score": 0.8358704324002668,
            "fpr": 0.0867178924259056,
            "logloss": 1.2368799538113007,
            "mae": 0.30542519873837,
            "precision": 0.7963917525773195,
            "recall": 0.6319018404907976
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 871,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.6467242133821782,
            "auditor_fn_violation": 0.06752027919260517,
            "auditor_fp_violation": 0.019435123042505595,
            "ave_precision_score": 0.6311856153525494,
            "fpr": 0.06907894736842106,
            "logloss": 7.704488007593155,
            "mae": 0.38417585113593933,
            "precision": 0.7519685039370079,
            "recall": 0.410752688172043
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.6253095024306174,
            "auditor_fn_violation": 0.07443897467669633,
            "auditor_fp_violation": 0.010110757929674702,
            "ave_precision_score": 0.6106950733156749,
            "fpr": 0.07574094401756312,
            "logloss": 9.736690229881464,
            "mae": 0.42634926753372093,
            "precision": 0.7206477732793523,
            "recall": 0.36400817995910023
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 871,
        "test": {
            "accuracy": 0.48026315789473684,
            "auc_prc": 0.4040208370746446,
            "auditor_fn_violation": 0.007034050179211474,
            "auditor_fp_violation": 0.013020526708269557,
            "ave_precision_score": 0.49215778270596844,
            "fpr": 0.05043859649122807,
            "logloss": 16.346858031937835,
            "mae": 0.5261296954959855,
            "precision": 0.4457831325301205,
            "recall": 0.07956989247311828
        },
        "train": {
            "accuracy": 0.4665203073545554,
            "auc_prc": 0.449538317360366,
            "auditor_fn_violation": 0.012916433771288918,
            "auditor_fp_violation": 0.013885059384770655,
            "ave_precision_score": 0.5221208632124757,
            "fpr": 0.050493962678375415,
            "logloss": 16.722686062018365,
            "mae": 0.5382165808008379,
            "precision": 0.5157894736842106,
            "recall": 0.10020449897750511
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 871,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8447024272478127,
            "auditor_fn_violation": 0.009104414261460107,
            "auditor_fp_violation": 0.011009066289885789,
            "ave_precision_score": 0.8449060763145049,
            "fpr": 0.07236842105263158,
            "logloss": 0.8293191949860447,
            "mae": 0.2894221305161006,
            "precision": 0.8125,
            "recall": 0.6150537634408603
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.843551213331966,
            "auditor_fn_violation": 0.006366181121893514,
            "auditor_fp_violation": 0.005243963978961717,
            "ave_precision_score": 0.843802634275387,
            "fpr": 0.07464324917672886,
            "logloss": 1.0938214056600932,
            "mae": 0.30120077098373305,
            "precision": 0.8116343490304709,
            "recall": 0.5991820040899796
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 871,
        "test": {
            "accuracy": 0.5942982456140351,
            "auc_prc": 0.6265131706531755,
            "auditor_fn_violation": 0.036879833993586124,
            "auditor_fp_violation": 0.07737990109501944,
            "ave_precision_score": 0.5239535883331922,
            "fpr": 0.3125,
            "logloss": 7.969127713578245,
            "mae": 0.42354482200465904,
            "precision": 0.5714285714285714,
            "recall": 0.8172043010752689
        },
        "train": {
            "accuracy": 0.6092206366630076,
            "auc_prc": 0.6699338500820485,
            "auditor_fn_violation": 0.04131956837471576,
            "auditor_fp_violation": 0.07474729608107336,
            "ave_precision_score": 0.5594868261089055,
            "fpr": 0.3040614709110867,
            "logloss": 8.033358458781166,
            "mae": 0.41113226637373207,
            "precision": 0.5967976710334789,
            "recall": 0.8384458077709611
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 871,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8441988298766739,
            "auditor_fn_violation": 0.010872948500282972,
            "auditor_fp_violation": 0.01782104870677813,
            "ave_precision_score": 0.8454514405241825,
            "fpr": 0.1162280701754386,
            "logloss": 0.6060891639127838,
            "mae": 0.272878454901777,
            "precision": 0.7665198237885462,
            "recall": 0.7483870967741936
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8540367818426657,
            "auditor_fn_violation": 0.011686297221642328,
            "auditor_fp_violation": 0.006232409570234264,
            "ave_precision_score": 0.8542856577349777,
            "fpr": 0.10318331503841932,
            "logloss": 0.6650963579617848,
            "mae": 0.275863257034234,
            "precision": 0.7906458797327395,
            "recall": 0.7259713701431493
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 871,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8516589160892387,
            "auditor_fn_violation": 0.007913601207319378,
            "auditor_fp_violation": 0.018044271753208527,
            "ave_precision_score": 0.8518758241484702,
            "fpr": 0.13157894736842105,
            "logloss": 0.6818580759557282,
            "mae": 0.26763097352984255,
            "precision": 0.7494780793319415,
            "recall": 0.7720430107526882
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8624739004043905,
            "auditor_fn_violation": 0.013758224293401037,
            "auditor_fp_violation": 0.013994308634332363,
            "ave_precision_score": 0.8626538291571658,
            "fpr": 0.11525795828759605,
            "logloss": 0.7363669435252401,
            "mae": 0.2665795061598997,
            "precision": 0.78125,
            "recall": 0.7668711656441718
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 871,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.6471723308275582,
            "auditor_fn_violation": 0.007458498396528956,
            "auditor_fp_violation": 0.058187625103026025,
            "ave_precision_score": 0.552408176516357,
            "fpr": 0.2631578947368421,
            "logloss": 7.3067300218292255,
            "mae": 0.3714582009401178,
            "precision": 0.6141479099678456,
            "recall": 0.821505376344086
        },
        "train": {
            "accuracy": 0.6509330406147091,
            "auc_prc": 0.6513778179512713,
            "auditor_fn_violation": 0.020537443964810918,
            "auditor_fp_violation": 0.05629457759558009,
            "ave_precision_score": 0.5683444458004705,
            "fpr": 0.24588364434687157,
            "logloss": 7.083865833769663,
            "mae": 0.36089101542140983,
            "precision": 0.6381260096930533,
            "recall": 0.8077709611451943
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 871,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7828812860835551,
            "auditor_fn_violation": 0.0028414450103754034,
            "auditor_fp_violation": 0.021162035401703368,
            "ave_precision_score": 0.7832194608782241,
            "fpr": 0.18530701754385964,
            "logloss": 0.7669659656447286,
            "mae": 0.3204592320684114,
            "precision": 0.6799242424242424,
            "recall": 0.7720430107526882
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7664283535964409,
            "auditor_fn_violation": 0.008395457473865211,
            "auditor_fp_violation": 0.018780466234178374,
            "ave_precision_score": 0.7671054251392465,
            "fpr": 0.17453347969264543,
            "logloss": 0.8230110482051782,
            "mae": 0.3141424019983631,
            "precision": 0.7050092764378478,
            "recall": 0.7770961145194274
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 871,
        "test": {
            "accuracy": 0.6535087719298246,
            "auc_prc": 0.6713734896222225,
            "auditor_fn_violation": 0.08252452367477835,
            "auditor_fp_violation": 0.0219592605675262,
            "ave_precision_score": 0.6603586299368867,
            "fpr": 0.0756578947368421,
            "logloss": 7.514538774544145,
            "mae": 0.36614004783408094,
            "precision": 0.759581881533101,
            "recall": 0.46881720430107526
        },
        "train": {
            "accuracy": 0.6114160263446762,
            "auc_prc": 0.64768935202302,
            "auditor_fn_violation": 0.08646872243136042,
            "auditor_fp_violation": 0.014748648690829829,
            "ave_precision_score": 0.636568407580947,
            "fpr": 0.07244785949506037,
            "logloss": 9.429985368511936,
            "mae": 0.4042006799499906,
            "precision": 0.7528089887640449,
            "recall": 0.4110429447852761
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 871,
        "test": {
            "accuracy": 0.6337719298245614,
            "auc_prc": 0.6490158874128873,
            "auditor_fn_violation": 0.07099132239200151,
            "auditor_fp_violation": 0.01860600887004985,
            "ave_precision_score": 0.6334716907985389,
            "fpr": 0.07456140350877193,
            "logloss": 7.68137769051934,
            "mae": 0.37969773465516377,
            "precision": 0.7453183520599251,
            "recall": 0.42795698924731185
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.6274644788622068,
            "auditor_fn_violation": 0.08071536480956454,
            "auditor_fp_violation": 0.01164805094136437,
            "ave_precision_score": 0.6128461766427078,
            "fpr": 0.07903402854006586,
            "logloss": 9.716298322977549,
            "mae": 0.4217248825434358,
            "precision": 0.7251908396946565,
            "recall": 0.3885480572597137
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 871,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.6499891073640545,
            "auditor_fn_violation": 0.05797491039426524,
            "auditor_fp_violation": 0.006956709446995567,
            "ave_precision_score": 0.6344425991138958,
            "fpr": 0.05043859649122807,
            "logloss": 7.748478888314117,
            "mae": 0.3926500831199674,
            "precision": 0.786046511627907,
            "recall": 0.3634408602150538
        },
        "train": {
            "accuracy": 0.566410537870472,
            "auc_prc": 0.6267759667481304,
            "auditor_fn_violation": 0.06084012938881518,
            "auditor_fp_violation": 0.0045156356485503675,
            "ave_precision_score": 0.6121593509993897,
            "fpr": 0.06366630076838639,
            "logloss": 9.778815520608099,
            "mae": 0.43526363604284524,
            "precision": 0.7238095238095238,
            "recall": 0.310838445807771
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 871,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.8577734784195854,
            "auditor_fn_violation": 0.006604885870590455,
            "auditor_fp_violation": 0.01930266101495351,
            "ave_precision_score": 0.8580279649844833,
            "fpr": 0.26096491228070173,
            "logloss": 0.9052728479765373,
            "mae": 0.310220537092455,
            "precision": 0.6468842729970327,
            "recall": 0.9376344086021505
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.8699118222466053,
            "auditor_fn_violation": 0.01019352202909677,
            "auditor_fp_violation": 0.023509398036634932,
            "ave_precision_score": 0.8700833455660941,
            "fpr": 0.2414928649835346,
            "logloss": 0.8566644437450377,
            "mae": 0.29400321391160056,
            "precision": 0.6711509715994021,
            "recall": 0.918200408997955
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 871,
        "test": {
            "accuracy": 0.6469298245614035,
            "auc_prc": 0.6649769531367872,
            "auditor_fn_violation": 0.07576400679117148,
            "auditor_fp_violation": 0.015326347187880219,
            "ave_precision_score": 0.6494018132870014,
            "fpr": 0.07346491228070176,
            "logloss": 7.582656279152234,
            "mae": 0.36537011159653754,
            "precision": 0.7581227436823105,
            "recall": 0.45161290322580644
        },
        "train": {
            "accuracy": 0.601536772777168,
            "auc_prc": 0.6380017234035826,
            "auditor_fn_violation": 0.08280749485385396,
            "auditor_fp_violation": 0.009385030771871963,
            "ave_precision_score": 0.6233656491906783,
            "fpr": 0.07464324917672886,
            "logloss": 9.65097000865277,
            "mae": 0.40904024374646425,
            "precision": 0.7404580152671756,
            "recall": 0.3967280163599182
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 871,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8614771895747915,
            "auditor_fn_violation": 0.0070104697226938345,
            "auditor_fp_violation": 0.010945288276619957,
            "ave_precision_score": 0.8616451947666244,
            "fpr": 0.1206140350877193,
            "logloss": 0.6214510506221087,
            "mae": 0.26280196214091184,
            "precision": 0.7674418604651163,
            "recall": 0.7806451612903226
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8639274593444043,
            "auditor_fn_violation": 0.020384799283467907,
            "auditor_fp_violation": 0.0038393307703112546,
            "ave_precision_score": 0.86398749159695,
            "fpr": 0.1163556531284303,
            "logloss": 0.8787604538341098,
            "mae": 0.26426313627447623,
            "precision": 0.7849898580121704,
            "recall": 0.7914110429447853
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 871,
        "test": {
            "accuracy": 0.6513157894736842,
            "auc_prc": 0.6596732387191787,
            "auditor_fn_violation": 0.07966893039049236,
            "auditor_fp_violation": 0.01848581184504887,
            "ave_precision_score": 0.6441100717507302,
            "fpr": 0.07894736842105263,
            "logloss": 7.603445719742461,
            "mae": 0.3675707946259039,
            "precision": 0.7525773195876289,
            "recall": 0.47096774193548385
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.6358604591986238,
            "auditor_fn_violation": 0.09065298251993921,
            "auditor_fp_violation": 0.011460766513544308,
            "ave_precision_score": 0.6212284734678788,
            "fpr": 0.07903402854006586,
            "logloss": 9.658609581442274,
            "mae": 0.410708075069944,
            "precision": 0.7437722419928826,
            "recall": 0.4274028629856851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 871,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.6670108131623219,
            "auditor_fn_violation": 0.07706564799094512,
            "auditor_fp_violation": 0.018507888849640886,
            "ave_precision_score": 0.6501736970460299,
            "fpr": 0.08223684210526316,
            "logloss": 7.581287830117863,
            "mae": 0.362478066509156,
            "precision": 0.7395833333333334,
            "recall": 0.45806451612903226
        },
        "train": {
            "accuracy": 0.601536772777168,
            "auc_prc": 0.636519361675284,
            "auditor_fn_violation": 0.08437659238707099,
            "auditor_fp_violation": 0.011135619937467816,
            "ave_precision_score": 0.6204768707296913,
            "fpr": 0.07793633369923161,
            "logloss": 9.674060730884156,
            "mae": 0.40758428970885463,
            "precision": 0.7350746268656716,
            "recall": 0.40286298568507156
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 871,
        "test": {
            "accuracy": 0.6326754385964912,
            "auc_prc": 0.6478087252026894,
            "auditor_fn_violation": 0.0831399735898887,
            "auditor_fp_violation": 0.02770664076298128,
            "ave_precision_score": 0.6352934529217882,
            "fpr": 0.10635964912280702,
            "logloss": 7.685863236103024,
            "mae": 0.37584354332351294,
            "precision": 0.7006172839506173,
            "recall": 0.4881720430107527
        },
        "train": {
            "accuracy": 0.5883644346871569,
            "auc_prc": 0.6164520925674506,
            "auditor_fn_violation": 0.09169904754208391,
            "auditor_fp_violation": 0.025127327399191556,
            "ave_precision_score": 0.6055869240036593,
            "fpr": 0.10647639956092206,
            "logloss": 9.718346477346744,
            "mae": 0.42010469114265997,
            "precision": 0.685064935064935,
            "recall": 0.43149284253578735
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 871,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.6654684448935624,
            "auditor_fn_violation": 0.08183833239011506,
            "auditor_fp_violation": 0.01972457710271204,
            "ave_precision_score": 0.6498912126616663,
            "fpr": 0.0800438596491228,
            "logloss": 7.54578714547174,
            "mae": 0.3626593931433106,
            "precision": 0.7542087542087542,
            "recall": 0.4817204301075269
        },
        "train": {
            "accuracy": 0.6004390779363337,
            "auc_prc": 0.6358084678267888,
            "auditor_fn_violation": 0.08646872243136042,
            "auditor_fp_violation": 0.013983903943897912,
            "ave_precision_score": 0.6211766429803579,
            "fpr": 0.08342480790340286,
            "logloss": 9.61038366115046,
            "mae": 0.4090284494563607,
            "precision": 0.7256317689530686,
            "recall": 0.4110429447852761
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 871,
        "test": {
            "accuracy": 0.5789473684210527,
            "auc_prc": 0.5200414156431237,
            "auditor_fn_violation": 0.07505423504999058,
            "auditor_fp_violation": 0.025138349228776648,
            "ave_precision_score": 0.5075194485953249,
            "fpr": 0.15350877192982457,
            "logloss": 9.897315196966883,
            "mae": 0.443210090888964,
            "precision": 0.6121883656509696,
            "recall": 0.4752688172043011
        },
        "train": {
            "accuracy": 0.5455543358946213,
            "auc_prc": 0.5363474423114317,
            "auditor_fn_violation": 0.07376778703373223,
            "auditor_fp_violation": 0.025278195410491047,
            "ave_precision_score": 0.5211256056177413,
            "fpr": 0.141602634467618,
            "logloss": 11.352797341881836,
            "mae": 0.4750510144257581,
            "precision": 0.6126126126126126,
            "recall": 0.4171779141104294
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 871,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8605486711522891,
            "auditor_fn_violation": 0.010224485946047912,
            "auditor_fp_violation": 0.01790445072412575,
            "ave_precision_score": 0.8606037439827912,
            "fpr": 0.13157894736842105,
            "logloss": 0.768683466441714,
            "mae": 0.26316297497288627,
            "precision": 0.7535934291581109,
            "recall": 0.789247311827957
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8690568180164269,
            "auditor_fn_violation": 0.010173319056566077,
            "auditor_fp_violation": 0.01351569287434776,
            "ave_precision_score": 0.8689472197616788,
            "fpr": 0.1207464324917673,
            "logloss": 0.9604358678394859,
            "mae": 0.26448286428881335,
            "precision": 0.7764227642276422,
            "recall": 0.7811860940695297
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 871,
        "test": {
            "accuracy": 0.6381578947368421,
            "auc_prc": 0.6504727245469412,
            "auditor_fn_violation": 0.07142520279192606,
            "auditor_fp_violation": 0.0181129557674948,
            "ave_precision_score": 0.6349285305496652,
            "fpr": 0.0712719298245614,
            "logloss": 7.661870809991846,
            "mae": 0.3777264241664114,
            "precision": 0.7547169811320755,
            "recall": 0.43010752688172044
        },
        "train": {
            "accuracy": 0.5949506037321625,
            "auc_prc": 0.6286904569881747,
            "auditor_fn_violation": 0.0801923322984922,
            "auditor_fp_violation": 0.009952086400549367,
            "ave_precision_score": 0.6140706485751974,
            "fpr": 0.07574094401756312,
            "logloss": 9.704338257283775,
            "mae": 0.42062673092781416,
            "precision": 0.7325581395348837,
            "recall": 0.38650306748466257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 871,
        "test": {
            "accuracy": 0.5142543859649122,
            "auc_prc": 0.5302918393844736,
            "auditor_fn_violation": 0.03064044519901905,
            "auditor_fp_violation": 0.018323913811374076,
            "ave_precision_score": 0.5177414100621356,
            "fpr": 0.09210526315789473,
            "logloss": 8.242242371250864,
            "mae": 0.49971315066325767,
            "precision": 0.5578947368421052,
            "recall": 0.22795698924731184
        },
        "train": {
            "accuracy": 0.45554335894621295,
            "auc_prc": 0.5281091716518396,
            "auditor_fn_violation": 0.028297630191322164,
            "auditor_fp_violation": 0.025720394753955086,
            "ave_precision_score": 0.5173160891242905,
            "fpr": 0.10647639956092206,
            "logloss": 10.08898928663077,
            "mae": 0.5380690328743329,
            "precision": 0.48128342245989303,
            "recall": 0.18404907975460122
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 871,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8435023032403008,
            "auditor_fn_violation": 0.010830503678551222,
            "auditor_fp_violation": 0.01067055221947486,
            "ave_precision_score": 0.8437107548710092,
            "fpr": 0.09210526315789473,
            "logloss": 0.8960998343184753,
            "mae": 0.2666498860591103,
            "precision": 0.7951219512195122,
            "recall": 0.7010752688172043
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8664444931119099,
            "auditor_fn_violation": 0.014043310683556354,
            "auditor_fp_violation": 0.004245113697254724,
            "ave_precision_score": 0.8663688436006118,
            "fpr": 0.09330406147091108,
            "logloss": 1.108049268222981,
            "mae": 0.26772983133177314,
            "precision": 0.8014018691588785,
            "recall": 0.7014314928425358
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 871,
        "test": {
            "accuracy": 0.5910087719298246,
            "auc_prc": 0.6065090952295094,
            "auditor_fn_violation": 0.06882192039237879,
            "auditor_fp_violation": 0.03120952549158131,
            "ave_precision_score": 0.5947874899014547,
            "fpr": 0.1118421052631579,
            "logloss": 7.778017962159546,
            "mae": 0.412499298534933,
            "precision": 0.6554054054054054,
            "recall": 0.4172043010752688
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.5940658269911188,
            "auditor_fn_violation": 0.07810020225420279,
            "auditor_fp_violation": 0.027249884247818917,
            "ave_precision_score": 0.5844303774449481,
            "fpr": 0.11086717892425905,
            "logloss": 9.792016222697923,
            "mae": 0.446983441944675,
            "precision": 0.6468531468531469,
            "recall": 0.3783231083844581
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 871,
        "test": {
            "accuracy": 0.6348684210526315,
            "auc_prc": 0.6480648118222871,
            "auditor_fn_violation": 0.06838803999245426,
            "auditor_fp_violation": 0.020310844224655598,
            "ave_precision_score": 0.632524477700452,
            "fpr": 0.0668859649122807,
            "logloss": 7.688400831552192,
            "mae": 0.38465468628130717,
            "precision": 0.7598425196850394,
            "recall": 0.4150537634408602
        },
        "train": {
            "accuracy": 0.5806805708013172,
            "auc_prc": 0.627171973241122,
            "auditor_fn_violation": 0.07077774709918987,
            "auditor_fp_violation": 0.007382127863240751,
            "ave_precision_score": 0.6125546097062039,
            "fpr": 0.07025246981339188,
            "logloss": 9.723399106957588,
            "mae": 0.42723043826864976,
            "precision": 0.7276595744680852,
            "recall": 0.3496932515337423
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 871,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.6550779731557925,
            "auditor_fn_violation": 0.09919354838709678,
            "auditor_fp_violation": 0.05127016366419405,
            "ave_precision_score": 0.6382562127745641,
            "fpr": 0.15570175438596492,
            "logloss": 7.70141946714485,
            "mae": 0.38306359395366196,
            "precision": 0.6502463054187192,
            "recall": 0.567741935483871
        },
        "train": {
            "accuracy": 0.579582875960483,
            "auc_prc": 0.6281837534240493,
            "auditor_fn_violation": 0.10425182780782036,
            "auditor_fp_violation": 0.042107782188210444,
            "ave_precision_score": 0.6121505550557951,
            "fpr": 0.141602634467618,
            "logloss": 9.744863997995138,
            "mae": 0.42294074632113254,
            "precision": 0.6456043956043956,
            "recall": 0.48057259713701433
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 871,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8249166290829213,
            "auditor_fn_violation": 0.011188926617619319,
            "auditor_fp_violation": 0.013592075827151774,
            "ave_precision_score": 0.8251017929702558,
            "fpr": 0.12280701754385964,
            "logloss": 0.9352926616730781,
            "mae": 0.2887556706752281,
            "precision": 0.7494407158836689,
            "recall": 0.7204301075268817
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8456510096485069,
            "auditor_fn_violation": 0.008487493237616144,
            "auditor_fp_violation": 0.01334921782739659,
            "ave_precision_score": 0.8455986703816403,
            "fpr": 0.11855104281009879,
            "logloss": 1.109310046676069,
            "mae": 0.29462481621881986,
            "precision": 0.7626373626373626,
            "recall": 0.7096114519427403
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 871,
        "test": {
            "accuracy": 0.6392543859649122,
            "auc_prc": 0.6504017597508984,
            "auditor_fn_violation": 0.07142520279192606,
            "auditor_fp_violation": 0.019969877153734446,
            "ave_precision_score": 0.634857716427552,
            "fpr": 0.07017543859649122,
            "logloss": 7.661564453395573,
            "mae": 0.37809233130376635,
            "precision": 0.7575757575757576,
            "recall": 0.43010752688172044
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.6286779379286314,
            "auditor_fn_violation": 0.07862323476527512,
            "auditor_fp_violation": 0.009952086400549367,
            "ave_precision_score": 0.6140581011908045,
            "fpr": 0.07574094401756312,
            "logloss": 9.703435846960838,
            "mae": 0.42118883223153103,
            "precision": 0.7294117647058823,
            "recall": 0.3803680981595092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 871,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6546775342207465,
            "auditor_fn_violation": 0.08140445199019053,
            "auditor_fp_violation": 0.023666548922642177,
            "ave_precision_score": 0.639122278801084,
            "fpr": 0.09210526315789473,
            "logloss": 7.595356730788221,
            "mae": 0.3728813617656976,
            "precision": 0.7263843648208469,
            "recall": 0.47956989247311826
        },
        "train": {
            "accuracy": 0.5949506037321625,
            "auc_prc": 0.6319544722069944,
            "auditor_fn_violation": 0.08803781996457745,
            "auditor_fp_violation": 0.01823421998636986,
            "ave_precision_score": 0.6173285144623022,
            "fpr": 0.09220636663007684,
            "logloss": 9.637671736779508,
            "mae": 0.41521323209643846,
            "precision": 0.7083333333333334,
            "recall": 0.4171779141104294
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 871,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.85635354966108,
            "auditor_fn_violation": 0.006383229579324657,
            "auditor_fp_violation": 0.01548824522155501,
            "ave_precision_score": 0.8565551348708602,
            "fpr": 0.13596491228070176,
            "logloss": 0.5770021253007496,
            "mae": 0.2679039023957892,
            "precision": 0.7443298969072165,
            "recall": 0.7763440860215054
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8568811047547933,
            "auditor_fn_violation": 0.010945521562183633,
            "auditor_fp_violation": 0.01530529962907279,
            "ave_precision_score": 0.8570842569462779,
            "fpr": 0.12403951701427003,
            "logloss": 0.6308525697510797,
            "mae": 0.2655213052912087,
            "precision": 0.7762376237623763,
            "recall": 0.8016359918200409
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 871,
        "test": {
            "accuracy": 0.6293859649122807,
            "auc_prc": 0.6481030580939682,
            "auditor_fn_violation": 0.07576400679117148,
            "auditor_fp_violation": 0.022690254719572988,
            "ave_precision_score": 0.6325601805598873,
            "fpr": 0.09100877192982457,
            "logloss": 7.668177177327889,
            "mae": 0.3767902755260524,
            "precision": 0.7167235494880546,
            "recall": 0.45161290322580644
        },
        "train": {
            "accuracy": 0.5960482985729967,
            "auc_prc": 0.6260229565108566,
            "auditor_fn_violation": 0.08699175494243275,
            "auditor_fp_violation": 0.01583854001383824,
            "ave_precision_score": 0.6114068660169729,
            "fpr": 0.0889132821075741,
            "logloss": 9.70984516901256,
            "mae": 0.4188551923290498,
            "precision": 0.7137809187279152,
            "recall": 0.4130879345603272
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 871,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.6866488014330416,
            "auditor_fn_violation": 0.0061898698358800255,
            "auditor_fp_violation": 0.005960791239844582,
            "ave_precision_score": 0.679271408564423,
            "fpr": 0.18969298245614036,
            "logloss": 1.9222903293226432,
            "mae": 0.3019403914837091,
            "precision": 0.6860254083484574,
            "recall": 0.8129032258064516
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.6990093985198182,
            "auditor_fn_violation": 0.011764864337039457,
            "auditor_fp_violation": 0.007324902065851292,
            "ave_precision_score": 0.6926857816121699,
            "fpr": 0.17453347969264543,
            "logloss": 1.9655470536623805,
            "mae": 0.29697156847610806,
            "precision": 0.7165775401069518,
            "recall": 0.8220858895705522
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 871,
        "test": {
            "accuracy": 0.6392543859649122,
            "auc_prc": 0.6561031865104968,
            "auditor_fn_violation": 0.07359460479154876,
            "auditor_fp_violation": 0.018500529848110205,
            "ave_precision_score": 0.6405418138174532,
            "fpr": 0.0756578947368421,
            "logloss": 7.704580102094967,
            "mae": 0.3719613340457711,
            "precision": 0.7481751824817519,
            "recall": 0.44086021505376344
        },
        "train": {
            "accuracy": 0.5971459934138309,
            "auc_prc": 0.6291442817613293,
            "auditor_fn_violation": 0.08228446234278158,
            "auditor_fp_violation": 0.010212203661410565,
            "ave_precision_score": 0.6145234836026198,
            "fpr": 0.07793633369923161,
            "logloss": 9.757836099786006,
            "mae": 0.4168904304405913,
            "precision": 0.7310606060606061,
            "recall": 0.3946830265848671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 871,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.8277641193611978,
            "auditor_fn_violation": 0.007882946613846453,
            "auditor_fp_violation": 0.003885552808194984,
            "ave_precision_score": 0.8285104550523246,
            "fpr": 0.01425438596491228,
            "logloss": 1.4408416447992969,
            "mae": 0.3573296862531839,
            "precision": 0.9235294117647059,
            "recall": 0.33763440860215055
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.823046654520589,
            "auditor_fn_violation": 0.015762808123390773,
            "auditor_fp_violation": 0.001667351642120268,
            "ave_precision_score": 0.8234535623479511,
            "fpr": 0.025246981339187707,
            "logloss": 1.556470156590269,
            "mae": 0.3793410223868939,
            "precision": 0.875,
            "recall": 0.3292433537832311
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 871,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8503619132943245,
            "auditor_fn_violation": 0.006880777211846824,
            "auditor_fp_violation": 0.016822677499116923,
            "ave_precision_score": 0.8505396492632544,
            "fpr": 0.14692982456140352,
            "logloss": 0.8124857217104124,
            "mae": 0.2677409023988848,
            "precision": 0.7325349301397206,
            "recall": 0.789247311827957
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8520878547166133,
            "auditor_fn_violation": 0.006065381308658774,
            "auditor_fp_violation": 0.009333007319699722,
            "ave_precision_score": 0.8522931214525198,
            "fpr": 0.12623490669593854,
            "logloss": 0.8153611346422569,
            "mae": 0.2595441101960099,
            "precision": 0.7731755424063116,
            "recall": 0.8016359918200409
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 871,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.6371675422472582,
            "auditor_fn_violation": 0.08373184304848143,
            "auditor_fp_violation": 0.02126015542211233,
            "ave_precision_score": 0.6198992200439041,
            "fpr": 0.11293859649122807,
            "logloss": 7.802217568136419,
            "mae": 0.368467280146978,
            "precision": 0.7005813953488372,
            "recall": 0.5182795698924731
        },
        "train": {
            "accuracy": 0.5938529088913282,
            "auc_prc": 0.605248074360931,
            "auditor_fn_violation": 0.09222208005315627,
            "auditor_fp_violation": 0.010930127301387468,
            "ave_precision_score": 0.5931778666951995,
            "fpr": 0.10208562019758508,
            "logloss": 10.046320772236937,
            "mae": 0.4193933754926218,
            "precision": 0.6950819672131148,
            "recall": 0.4335378323108384
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 871,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.8458177931567203,
            "auditor_fn_violation": 0.012179305791360123,
            "auditor_fp_violation": 0.018198810785352654,
            "ave_precision_score": 0.8461126175446787,
            "fpr": 0.24890350877192982,
            "logloss": 0.7972611117899031,
            "mae": 0.3311741547114224,
            "precision": 0.651840490797546,
            "recall": 0.9139784946236559
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8788450462790749,
            "auditor_fn_violation": 0.003935090094033615,
            "auditor_fp_violation": 0.01625732880382476,
            "ave_precision_score": 0.8790067344155352,
            "fpr": 0.21405049396267836,
            "logloss": 0.6868718837315193,
            "mae": 0.3098575729704694,
            "precision": 0.6938775510204082,
            "recall": 0.9038854805725971
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 871,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.6547861277148395,
            "auditor_fn_violation": 0.06665251839275609,
            "auditor_fp_violation": 0.017602731661368186,
            "ave_precision_score": 0.6422660383421703,
            "fpr": 0.06030701754385965,
            "logloss": 7.623774501596775,
            "mae": 0.3790115181936475,
            "precision": 0.7745901639344263,
            "recall": 0.4064516129032258
        },
        "train": {
            "accuracy": 0.5883644346871569,
            "auc_prc": 0.6301630793026832,
            "auditor_fn_violation": 0.07130077961026221,
            "auditor_fp_violation": 0.005035870170272758,
            "ave_precision_score": 0.6192778331094024,
            "fpr": 0.06366630076838639,
            "logloss": 9.682812189228827,
            "mae": 0.42243094966503414,
            "precision": 0.7478260869565218,
            "recall": 0.35173824130879344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 871,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8656403515512904,
            "auditor_fn_violation": 0.006272401433691761,
            "auditor_fp_violation": 0.01520860316338946,
            "ave_precision_score": 0.8658077178364157,
            "fpr": 0.13925438596491227,
            "logloss": 0.5887623214215514,
            "mae": 0.266942405911163,
            "precision": 0.7475149105367793,
            "recall": 0.8086021505376344
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8712577046394482,
            "auditor_fn_violation": 0.00800486667160517,
            "auditor_fp_violation": 0.012449212104816853,
            "ave_precision_score": 0.8713034948172635,
            "fpr": 0.13172338090010977,
            "logloss": 0.8290525714283612,
            "mae": 0.267788263753033,
            "precision": 0.7683397683397684,
            "recall": 0.8139059304703476
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 871,
        "test": {
            "accuracy": 0.5592105263157895,
            "auc_prc": 0.6860816790413417,
            "auditor_fn_violation": 0.11090831918505943,
            "auditor_fp_violation": 0.08941186859766868,
            "ave_precision_score": 0.5427075005448427,
            "fpr": 0.25,
            "logloss": 15.09852754777184,
            "mae": 0.44160796629241256,
            "precision": 0.5606936416184971,
            "recall": 0.6258064516129033
        },
        "train": {
            "accuracy": 0.5345773874862788,
            "auc_prc": 0.6756443548962869,
            "auditor_fn_violation": 0.12151190067320794,
            "auditor_fp_violation": 0.08137508388781663,
            "ave_precision_score": 0.5548212389934741,
            "fpr": 0.22283205268935236,
            "logloss": 15.972470204981681,
            "mae": 0.4660940390649809,
            "precision": 0.5690021231422505,
            "recall": 0.5480572597137015
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 871,
        "test": {
            "accuracy": 0.6008771929824561,
            "auc_prc": 0.6453103750582183,
            "auditor_fn_violation": 0.04626013959630259,
            "auditor_fp_violation": 0.006019663252089957,
            "ave_precision_score": 0.6327900121470342,
            "fpr": 0.044956140350877194,
            "logloss": 7.61678396244605,
            "mae": 0.40815941615109524,
            "precision": 0.7759562841530054,
            "recall": 0.3053763440860215
        },
        "train": {
            "accuracy": 0.557628979143798,
            "auc_prc": 0.6144243992646372,
            "auditor_fn_violation": 0.049333414145223464,
            "auditor_fp_violation": 0.0022422107886235143,
            "ave_precision_score": 0.6035572314062765,
            "fpr": 0.048298572996706916,
            "logloss": 9.664017838503062,
            "mae": 0.454610790914231,
            "precision": 0.7471264367816092,
            "recall": 0.2658486707566462
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 871,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8329329851432241,
            "auditor_fn_violation": 0.00575363139030372,
            "auditor_fp_violation": 0.014168530947054437,
            "ave_precision_score": 0.8336625153817162,
            "fpr": 0.17653508771929824,
            "logloss": 0.6929256136043347,
            "mae": 0.2902596507386382,
            "precision": 0.7088607594936709,
            "recall": 0.843010752688172
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8387254307215943,
            "auditor_fn_violation": 0.004909322324958081,
            "auditor_fp_violation": 0.0196336508498031,
            "ave_precision_score": 0.8389650203764435,
            "fpr": 0.16575192096597147,
            "logloss": 0.7054451735195022,
            "mae": 0.28572365384100107,
            "precision": 0.7284172661870504,
            "recall": 0.8282208588957055
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 871,
        "test": {
            "accuracy": 0.5570175438596491,
            "auc_prc": 0.7412711145855608,
            "auditor_fn_violation": 0.004739671760045274,
            "auditor_fp_violation": 0.012951842693983286,
            "ave_precision_score": 0.5296540536636923,
            "fpr": 0.42653508771929827,
            "logloss": 14.170128977161115,
            "mae": 0.4447453615747084,
            "precision": 0.5363528009535161,
            "recall": 0.967741935483871
        },
        "train": {
            "accuracy": 0.5883644346871569,
            "auc_prc": 0.752764035256042,
            "auditor_fn_violation": 0.006500867605431458,
            "auditor_fp_violation": 0.014472924394316963,
            "ave_precision_score": 0.5638989067549177,
            "fpr": 0.38748627881448955,
            "logloss": 12.552165181987755,
            "mae": 0.41351013777364126,
            "precision": 0.5695121951219512,
            "recall": 0.9550102249488752
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 871,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.817968247918928,
            "auditor_fn_violation": 0.013610639501980758,
            "auditor_fp_violation": 0.010408081164880885,
            "ave_precision_score": 0.8181301275375121,
            "fpr": 0.09100877192982457,
            "logloss": 1.061222688563853,
            "mae": 0.2935076437969601,
            "precision": 0.7866323907455013,
            "recall": 0.6580645161290323
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.8357840584879797,
            "auditor_fn_violation": 0.019736059387760154,
            "auditor_fp_violation": 0.009411042497958084,
            "ave_precision_score": 0.8357525102719003,
            "fpr": 0.09440175631174534,
            "logloss": 1.2545686427970713,
            "mae": 0.3015592803207569,
            "precision": 0.7844611528822055,
            "recall": 0.6400817995910021
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 871,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.645866499418541,
            "auditor_fn_violation": 0.0636153555932843,
            "auditor_fp_violation": 0.01741875662310138,
            "ave_precision_score": 0.6333569281784504,
            "fpr": 0.0668859649122807,
            "logloss": 7.696196554896118,
            "mae": 0.38638696815781615,
            "precision": 0.7489711934156379,
            "recall": 0.3913978494623656
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.6239160822121308,
            "auditor_fn_violation": 0.07339290965455163,
            "auditor_fp_violation": 0.007517388838888572,
            "ave_precision_score": 0.6130417487565752,
            "fpr": 0.07354555433589462,
            "logloss": 9.620379352271883,
            "mae": 0.4252282183006171,
            "precision": 0.7242798353909465,
            "recall": 0.35991820040899797
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 871,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.6038431126539548,
            "auditor_fn_violation": 0.05238398415393323,
            "auditor_fp_violation": 0.03288001883904392,
            "ave_precision_score": 0.5731367794867819,
            "fpr": 0.10635964912280702,
            "logloss": 8.13436236167882,
            "mae": 0.4006283184334217,
            "precision": 0.6891025641025641,
            "recall": 0.46236559139784944
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.6135276842135038,
            "auditor_fn_violation": 0.056173242734225415,
            "auditor_fp_violation": 0.030766669614662293,
            "ave_precision_score": 0.5799830574045727,
            "fpr": 0.10647639956092206,
            "logloss": 9.72024985040982,
            "mae": 0.4278401620097336,
            "precision": 0.6830065359477124,
            "recall": 0.4274028629856851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 871,
        "test": {
            "accuracy": 0.6041666666666666,
            "auc_prc": 0.6185696758286915,
            "auditor_fn_violation": 0.0748962459913224,
            "auditor_fp_violation": 0.0328162408257781,
            "ave_precision_score": 0.6068266417904777,
            "fpr": 0.11403508771929824,
            "logloss": 7.582312310293212,
            "mae": 0.41092780942735885,
            "precision": 0.6666666666666666,
            "recall": 0.44731182795698926
        },
        "train": {
            "accuracy": 0.5762897914379802,
            "auc_prc": 0.5993611878977739,
            "auditor_fn_violation": 0.08646872243136042,
            "auditor_fp_violation": 0.025657966611348396,
            "ave_precision_score": 0.5897147601143529,
            "fpr": 0.10757409440175632,
            "logloss": 9.513889334777911,
            "mae": 0.4447772801405557,
            "precision": 0.6722408026755853,
            "recall": 0.4110429447852761
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 871,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.6280606196796663,
            "auditor_fn_violation": 0.055805508394642524,
            "auditor_fp_violation": 0.013498861807763256,
            "ave_precision_score": 0.6155883396669349,
            "fpr": 0.06140350877192982,
            "logloss": 7.636227965887853,
            "mae": 0.4094336610312931,
            "precision": 0.7454545454545455,
            "recall": 0.35268817204301073
        },
        "train": {
            "accuracy": 0.5642151481888035,
            "auc_prc": 0.6157638781490967,
            "auditor_fn_violation": 0.061363161899887546,
            "auditor_fp_violation": 0.011499784102673489,
            "ave_precision_score": 0.6049026460886316,
            "fpr": 0.06695938529088913,
            "logloss": 9.547580949418396,
            "mae": 0.4453330689594935,
            "precision": 0.7149532710280374,
            "recall": 0.3128834355828221
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 871,
        "test": {
            "accuracy": 0.6118421052631579,
            "auc_prc": 0.632744908794854,
            "auditor_fn_violation": 0.04192133559705716,
            "auditor_fp_violation": 0.08901448251501236,
            "ave_precision_score": 0.5385422671620952,
            "fpr": 0.24342105263157895,
            "logloss": 7.94570658153424,
            "mae": 0.39410724872107616,
            "precision": 0.6,
            "recall": 0.7161290322580646
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.6784140442187269,
            "auditor_fn_violation": 0.04019718101189954,
            "auditor_fp_violation": 0.07811841578183444,
            "ave_precision_score": 0.5784991877930551,
            "fpr": 0.24259055982436883,
            "logloss": 8.034623719443571,
            "mae": 0.3789770126678085,
            "precision": 0.6291946308724832,
            "recall": 0.7668711656441718
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 871,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.7232701125121446,
            "auditor_fn_violation": 0.012504716091303537,
            "auditor_fp_violation": 0.0057302091918835125,
            "ave_precision_score": 0.7235002615211564,
            "fpr": 0.01864035087719298,
            "logloss": 2.8301408805672073,
            "mae": 0.4287136186902209,
            "precision": 0.8425925925925926,
            "recall": 0.1956989247311828
        },
        "train": {
            "accuracy": 0.5729967069154775,
            "auc_prc": 0.7635192256868246,
            "auditor_fn_violation": 0.010824303726999494,
            "auditor_fp_violation": 0.0015919176364705212,
            "ave_precision_score": 0.7637248076095066,
            "fpr": 0.010976948408342482,
            "logloss": 3.284154454773935,
            "mae": 0.43433596667599816,
            "precision": 0.9166666666666666,
            "recall": 0.2249488752556237
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 871,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6503373797948067,
            "auditor_fn_violation": 0.0718590831918506,
            "auditor_fp_violation": 0.019969877153734446,
            "ave_precision_score": 0.6347935764786701,
            "fpr": 0.07017543859649122,
            "logloss": 7.662380775540954,
            "mae": 0.37779651982659995,
            "precision": 0.7584905660377359,
            "recall": 0.432258064516129
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.6288070546069389,
            "auditor_fn_violation": 0.07862323476527512,
            "auditor_fp_violation": 0.009952086400549367,
            "ave_precision_score": 0.6141869838453244,
            "fpr": 0.07574094401756312,
            "logloss": 9.704730913620102,
            "mae": 0.4208305478597109,
            "precision": 0.7294117647058823,
            "recall": 0.3803680981595092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 871,
        "test": {
            "accuracy": 0.6348684210526315,
            "auc_prc": 0.6480391108348107,
            "auditor_fn_violation": 0.06838803999245426,
            "auditor_fp_violation": 0.020310844224655598,
            "ave_precision_score": 0.6324989772834185,
            "fpr": 0.0668859649122807,
            "logloss": 7.6869878842076345,
            "mae": 0.3844144047035806,
            "precision": 0.7598425196850394,
            "recall": 0.4150537634408602
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.6271488958806672,
            "auditor_fn_violation": 0.07182381212133457,
            "auditor_fp_violation": 0.007382127863240751,
            "ave_precision_score": 0.6125316173406546,
            "fpr": 0.07025246981339188,
            "logloss": 9.7222544526564,
            "mae": 0.4270205730853789,
            "precision": 0.729957805907173,
            "recall": 0.3537832310838446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 871,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8538084342729765,
            "auditor_fn_violation": 0.006508205998868142,
            "auditor_fp_violation": 0.012858628674594764,
            "ave_precision_score": 0.8538710666255565,
            "fpr": 0.12390350877192982,
            "logloss": 0.8406436399988366,
            "mae": 0.2634123348147659,
            "precision": 0.754880694143167,
            "recall": 0.7483870967741936
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8600412168822364,
            "auditor_fn_violation": 0.011239587051241475,
            "auditor_fp_violation": 0.013635346814343906,
            "ave_precision_score": 0.8599840517042638,
            "fpr": 0.10976948408342481,
            "logloss": 1.051610551310758,
            "mae": 0.26321454535862976,
            "precision": 0.7867803837953091,
            "recall": 0.754601226993865
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 871,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7883228883328534,
            "auditor_fn_violation": 0.0033319185059422747,
            "auditor_fp_violation": 0.026813748577259714,
            "ave_precision_score": 0.7886350854334152,
            "fpr": 0.19956140350877194,
            "logloss": 0.8855430810983166,
            "mae": 0.3151287838854363,
            "precision": 0.6755793226381461,
            "recall": 0.8150537634408602
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7771201898848565,
            "auditor_fn_violation": 0.007789368297944461,
            "auditor_fp_violation": 0.019076999911560134,
            "ave_precision_score": 0.7775773875847527,
            "fpr": 0.18221734357848518,
            "logloss": 0.90672325222135,
            "mae": 0.306984602662931,
            "precision": 0.7040998217468806,
            "recall": 0.8077709611451943
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 871,
        "test": {
            "accuracy": 0.6392543859649122,
            "auc_prc": 0.6490256410666457,
            "auditor_fn_violation": 0.07099132239200151,
            "auditor_fp_violation": 0.02037707523843165,
            "ave_precision_score": 0.6365215037468184,
            "fpr": 0.06907894736842106,
            "logloss": 7.529541714966474,
            "mae": 0.3844346484583105,
            "precision": 0.7595419847328244,
            "recall": 0.42795698924731185
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.6258239155826105,
            "auditor_fn_violation": 0.07443897467669633,
            "auditor_fp_violation": 0.0074653653867163315,
            "ave_precision_score": 0.6149489429773561,
            "fpr": 0.06695938529088913,
            "logloss": 9.476922437416292,
            "mae": 0.42700041010495426,
            "precision": 0.7447698744769874,
            "recall": 0.36400817995910023
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 871,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8266115398096852,
            "auditor_fn_violation": 0.004209111488398422,
            "auditor_fp_violation": 0.006937085442913775,
            "ave_precision_score": 0.8269444303963238,
            "fpr": 0.09978070175438597,
            "logloss": 0.7167894914430584,
            "mae": 0.2887899471075467,
            "precision": 0.7719298245614035,
            "recall": 0.6623655913978495
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8316232160708721,
            "auditor_fn_violation": 0.00791732045730551,
            "auditor_fp_violation": 0.009179538135791613,
            "ave_precision_score": 0.8319696706283881,
            "fpr": 0.0867178924259056,
            "logloss": 0.767233281426328,
            "mae": 0.2827299702168221,
            "precision": 0.8058968058968059,
            "recall": 0.6707566462167689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 871,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8364765914609018,
            "auditor_fn_violation": 0.008955857385398981,
            "auditor_fp_violation": 0.026745064562973445,
            "ave_precision_score": 0.8377886612010237,
            "fpr": 0.25109649122807015,
            "logloss": 1.0675961408514287,
            "mae": 0.30026689328157946,
            "precision": 0.653555219364599,
            "recall": 0.9290322580645162
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8540695213456818,
            "auditor_fn_violation": 0.00864687224313604,
            "auditor_fp_violation": 0.02734872880694618,
            "ave_precision_score": 0.8542696317577044,
            "fpr": 0.2283205268935236,
            "logloss": 1.0441221404966445,
            "mae": 0.29047896717992583,
            "precision": 0.6829268292682927,
            "recall": 0.9161554192229039
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 871,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8362921037704829,
            "auditor_fn_violation": 0.008503112620260333,
            "auditor_fp_violation": 0.01209083951489462,
            "ave_precision_score": 0.8366360662742603,
            "fpr": 0.07346491228070176,
            "logloss": 0.7303849162786239,
            "mae": 0.2953435179039281,
            "precision": 0.8133704735376045,
            "recall": 0.6279569892473118
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.8477342238033726,
            "auditor_fn_violation": 0.006646777962597566,
            "auditor_fp_violation": 0.0029133133216454007,
            "ave_precision_score": 0.847973935526737,
            "fpr": 0.07135016465422613,
            "logloss": 0.7662219836282843,
            "mae": 0.3029055722902887,
            "precision": 0.8204419889502762,
            "recall": 0.6073619631901841
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 871,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8578560056982557,
            "auditor_fn_violation": 0.009757592906998677,
            "auditor_fp_violation": 0.014423643000117746,
            "ave_precision_score": 0.8580528860042831,
            "fpr": 0.13925438596491227,
            "logloss": 0.5774864798022193,
            "mae": 0.2684606164409655,
            "precision": 0.742914979757085,
            "recall": 0.789247311827957
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8586581339191919,
            "auditor_fn_violation": 0.013607824386783665,
            "auditor_fp_violation": 0.016314554601214233,
            "ave_precision_score": 0.858856477519039,
            "fpr": 0.1207464324917673,
            "logloss": 0.6280443736525847,
            "mae": 0.2648491157846671,
            "precision": 0.7817460317460317,
            "recall": 0.8057259713701431
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 871,
        "test": {
            "accuracy": 0.6293859649122807,
            "auc_prc": 0.6432353350589864,
            "auditor_fn_violation": 0.07142520279192606,
            "auditor_fp_violation": 0.019925723144550413,
            "ave_precision_score": 0.6296466581237252,
            "fpr": 0.0800438596491228,
            "logloss": 7.628390871430381,
            "mae": 0.3865364511228314,
            "precision": 0.7326007326007326,
            "recall": 0.43010752688172044
        },
        "train": {
            "accuracy": 0.5773874862788145,
            "auc_prc": 0.6249257510581853,
            "auditor_fn_violation": 0.07548503969884103,
            "auditor_fp_violation": 0.013500085838696085,
            "ave_precision_score": 0.6138590449642933,
            "fpr": 0.08342480790340286,
            "logloss": 9.677264337949678,
            "mae": 0.4276892144724613,
            "precision": 0.703125,
            "recall": 0.36809815950920244
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 871,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.6622406616714476,
            "auditor_fn_violation": 0.07966893039049236,
            "auditor_fp_violation": 0.021797362533851415,
            "ave_precision_score": 0.64666784511854,
            "fpr": 0.08881578947368421,
            "logloss": 7.594188281041509,
            "mae": 0.3651729281245768,
            "precision": 0.73,
            "recall": 0.47096774193548385
        },
        "train": {
            "accuracy": 0.6037321624588364,
            "auc_prc": 0.6317441941720199,
            "auditor_fn_violation": 0.0885608524756498,
            "auditor_fp_violation": 0.015011367124299634,
            "ave_precision_score": 0.6171203079882832,
            "fpr": 0.0845225027442371,
            "logloss": 9.661850115943121,
            "mae": 0.40916413265128604,
            "precision": 0.7269503546099291,
            "recall": 0.41922290388548056
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 871,
        "test": {
            "accuracy": 0.6041666666666666,
            "auc_prc": 0.6365065451674814,
            "auditor_fn_violation": 0.05146670439539709,
            "auditor_fp_violation": 0.014806311079712704,
            "ave_precision_score": 0.6209884165209822,
            "fpr": 0.05482456140350877,
            "logloss": 7.676161486656746,
            "mae": 0.40367604469825846,
            "precision": 0.7549019607843137,
            "recall": 0.3311827956989247
        },
        "train": {
            "accuracy": 0.5686059275521405,
            "auc_prc": 0.6194943823681534,
            "auditor_fn_violation": 0.06188619441095988,
            "auditor_fp_violation": 0.006175183772844799,
            "ave_precision_score": 0.6048892480785331,
            "fpr": 0.06366630076838639,
            "logloss": 9.696601431763947,
            "mae": 0.4438635052839879,
            "precision": 0.7264150943396226,
            "recall": 0.3149284253578732
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 871,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8572569467846469,
            "auditor_fn_violation": 0.006253537068477648,
            "auditor_fp_violation": 0.01474989206797755,
            "ave_precision_score": 0.8574535671589335,
            "fpr": 0.14035087719298245,
            "logloss": 0.5838476137983022,
            "mae": 0.26736260428982095,
            "precision": 0.7382413087934561,
            "recall": 0.7763440860215054
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8569647740628155,
            "auditor_fn_violation": 0.013509054298855844,
            "auditor_fp_violation": 0.014077546157807944,
            "ave_precision_score": 0.8571723883971011,
            "fpr": 0.1207464324917673,
            "logloss": 0.6388448073446535,
            "mae": 0.26408673091781826,
            "precision": 0.7813121272365805,
            "recall": 0.803680981595092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 871,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8221511138550123,
            "auditor_fn_violation": 0.006734578381437469,
            "auditor_fp_violation": 0.016901173515444096,
            "ave_precision_score": 0.8223091607180886,
            "fpr": 0.1337719298245614,
            "logloss": 0.9815951916240889,
            "mae": 0.27945085657107677,
            "precision": 0.7431578947368421,
            "recall": 0.7591397849462366
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8402722260263723,
            "auditor_fn_violation": 0.009084603314634362,
            "auditor_fp_violation": 0.014556161917792543,
            "ave_precision_score": 0.8401971140489957,
            "fpr": 0.11525795828759605,
            "logloss": 1.1806362046505001,
            "mae": 0.2828044176785083,
            "precision": 0.7741935483870968,
            "recall": 0.7361963190184049
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 871,
        "test": {
            "accuracy": 0.49451754385964913,
            "auc_prc": 0.5178592762209111,
            "auditor_fn_violation": 0.03237596679871722,
            "auditor_fp_violation": 0.03195033164566898,
            "ave_precision_score": 0.5020885474581194,
            "fpr": 0.1162280701754386,
            "logloss": 9.721324065977932,
            "mae": 0.5097374388400363,
            "precision": 0.5092592592592593,
            "recall": 0.23655913978494625
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0.5242228995302102,
            "auditor_fn_violation": 0.03474911275278971,
            "auditor_fp_violation": 0.02860509517690575,
            "ave_precision_score": 0.5098573841647174,
            "fpr": 0.11306256860592755,
            "logloss": 11.62149028586403,
            "mae": 0.5409082555132169,
            "precision": 0.5,
            "recall": 0.21063394683026584
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 871,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.6485233374151393,
            "auditor_fn_violation": 0.06708639879268062,
            "auditor_fp_violation": 0.01890036893127674,
            "ave_precision_score": 0.6329812885113468,
            "fpr": 0.06798245614035088,
            "logloss": 7.687461225638,
            "mae": 0.3823590818057597,
            "precision": 0.753968253968254,
            "recall": 0.40860215053763443
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.6275601754913375,
            "auditor_fn_violation": 0.07548503969884103,
            "auditor_fp_violation": 0.010024919233590505,
            "ave_precision_score": 0.6129421939972758,
            "fpr": 0.07464324917672886,
            "logloss": 9.72494632759369,
            "mae": 0.4240194356387673,
            "precision": 0.7258064516129032,
            "recall": 0.36809815950920244
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 871,
        "test": {
            "accuracy": 0.6008771929824561,
            "auc_prc": 0.6504119557536021,
            "auditor_fn_violation": 0.025419732126013977,
            "auditor_fp_violation": 0.007650908591388986,
            "ave_precision_score": 0.6470980670776062,
            "fpr": 0.09100877192982457,
            "logloss": 9.389469473605818,
            "mae": 0.40162479322349515,
            "precision": 0.6891385767790262,
            "recall": 0.3956989247311828
        },
        "train": {
            "accuracy": 0.6158068057080132,
            "auc_prc": 0.7022292341446026,
            "auditor_fn_violation": 0.015170187595823836,
            "auditor_fp_violation": 2.0809380868899204e-05,
            "ave_precision_score": 0.7004132301609265,
            "fpr": 0.06586169045005488,
            "logloss": 9.676660157049048,
            "mae": 0.3932326291031848,
            "precision": 0.7683397683397684,
            "recall": 0.4069529652351738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 871,
        "test": {
            "accuracy": 0.6293859649122807,
            "auc_prc": 0.6466025788210646,
            "auditor_fn_violation": 0.06752027919260517,
            "auditor_fp_violation": 0.01761499666391931,
            "ave_precision_score": 0.6310641884451804,
            "fpr": 0.07017543859649122,
            "logloss": 7.703456145791773,
            "mae": 0.38426818535602114,
            "precision": 0.7490196078431373,
            "recall": 0.410752688172043
        },
        "train": {
            "accuracy": 0.5817782656421515,
            "auc_prc": 0.6252311344561342,
            "auditor_fn_violation": 0.07443897467669633,
            "auditor_fp_violation": 0.010987353098776931,
            "ave_precision_score": 0.6106167916786036,
            "fpr": 0.07683863885839737,
            "logloss": 9.735610389304536,
            "mae": 0.42639687942304,
            "precision": 0.717741935483871,
            "recall": 0.36400817995910023
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 871,
        "test": {
            "accuracy": 0.6282894736842105,
            "auc_prc": 0.6499912972214723,
            "auditor_fn_violation": 0.06491699679305792,
            "auditor_fp_violation": 0.018225793790965113,
            "ave_precision_score": 0.6344453835784314,
            "fpr": 0.06469298245614036,
            "logloss": 7.700148874186207,
            "mae": 0.38308280418834617,
            "precision": 0.7581967213114754,
            "recall": 0.3978494623655914
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.6272794195901131,
            "auditor_fn_violation": 0.07339290965455163,
            "auditor_fp_violation": 0.007691667403665575,
            "ave_precision_score": 0.6126618779933432,
            "fpr": 0.07354555433589462,
            "logloss": 9.735650500817025,
            "mae": 0.4254386562526914,
            "precision": 0.7242798353909465,
            "recall": 0.35991820040899797
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 871,
        "test": {
            "accuracy": 0.5449561403508771,
            "auc_prc": 0.5180200549135031,
            "auditor_fn_violation": 0.08191614789662328,
            "auditor_fp_violation": 0.020443306252207704,
            "ave_precision_score": 0.5350971348198501,
            "fpr": 0.1074561403508772,
            "logloss": 11.083527747076658,
            "mae": 0.4570655455178515,
            "precision": 0.6016260162601627,
            "recall": 0.31827956989247314
        },
        "train": {
            "accuracy": 0.5729967069154775,
            "auc_prc": 0.5949039821640978,
            "auditor_fn_violation": 0.08652259702477559,
            "auditor_fp_violation": 0.017739997190733586,
            "ave_precision_score": 0.6007638604706993,
            "fpr": 0.09220636663007684,
            "logloss": 10.189285874060463,
            "mae": 0.436906634808567,
            "precision": 0.6865671641791045,
            "recall": 0.37627811860940696
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 871,
        "test": {
            "accuracy": 0.6052631578947368,
            "auc_prc": 0.6475367361951427,
            "auditor_fn_violation": 0.047995661196000766,
            "auditor_fp_violation": 0.010022960084775698,
            "ave_precision_score": 0.6319936490745541,
            "fpr": 0.044956140350877194,
            "logloss": 7.831854928537896,
            "mae": 0.40531827674199167,
            "precision": 0.7807486631016043,
            "recall": 0.3139784946236559
        },
        "train": {
            "accuracy": 0.5521405049396267,
            "auc_prc": 0.6251516907439874,
            "auditor_fn_violation": 0.04985644665629581,
            "auditor_fp_violation": 0.0089584384640596,
            "ave_precision_score": 0.6105376123327013,
            "fpr": 0.054884742041712405,
            "logloss": 9.849183223957004,
            "mae": 0.44728799922478457,
            "precision": 0.7237569060773481,
            "recall": 0.26789366053169733
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 871,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.6621598706469936,
            "auditor_fn_violation": 0.07966893039049236,
            "auditor_fp_violation": 0.021797362533851415,
            "ave_precision_score": 0.6465872715882071,
            "fpr": 0.08881578947368421,
            "logloss": 7.5946504265696335,
            "mae": 0.36515968271430377,
            "precision": 0.73,
            "recall": 0.47096774193548385
        },
        "train": {
            "accuracy": 0.6037321624588364,
            "auc_prc": 0.6317678269735291,
            "auditor_fn_violation": 0.0885608524756498,
            "auditor_fp_violation": 0.015011367124299634,
            "ave_precision_score": 0.61714391723398,
            "fpr": 0.0845225027442371,
            "logloss": 9.662349333567715,
            "mae": 0.40918193890509386,
            "precision": 0.7269503546099291,
            "recall": 0.41922290388548056
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 871,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.6488913963804606,
            "auditor_fn_violation": 0.07706564799094512,
            "auditor_fp_violation": 0.023472761882334475,
            "ave_precision_score": 0.6333489302993653,
            "fpr": 0.08662280701754387,
            "logloss": 7.658845893820951,
            "mae": 0.37624485370744337,
            "precision": 0.7294520547945206,
            "recall": 0.45806451612903226
        },
        "train": {
            "accuracy": 0.5971459934138309,
            "auc_prc": 0.6276496803861134,
            "auditor_fn_violation": 0.08489962489814336,
            "auditor_fp_violation": 0.013895464075205104,
            "ave_precision_score": 0.6130314860101931,
            "fpr": 0.08342480790340286,
            "logloss": 9.694731450778809,
            "mae": 0.4183252037053951,
            "precision": 0.7226277372262774,
            "recall": 0.4049079754601227
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 871,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7132389538458253,
            "auditor_fn_violation": 0.007515091492171293,
            "auditor_fp_violation": 0.004145570862278744,
            "ave_precision_score": 0.7131430487428485,
            "fpr": 0.15679824561403508,
            "logloss": 1.8886566227781136,
            "mae": 0.3026856226043332,
            "precision": 0.709349593495935,
            "recall": 0.7505376344086021
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.6794649542152038,
            "auditor_fn_violation": 0.009387647902594734,
            "auditor_fp_violation": 0.0024477034247038635,
            "ave_precision_score": 0.6792529348928898,
            "fpr": 0.1525795828759605,
            "logloss": 2.4809107212615014,
            "mae": 0.31210528718489333,
            "precision": 0.7258382642998028,
            "recall": 0.7525562372188139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 871,
        "test": {
            "accuracy": 0.6282894736842105,
            "auc_prc": 0.6497548250543388,
            "auditor_fn_violation": 0.06491699679305792,
            "auditor_fp_violation": 0.018225793790965113,
            "ave_precision_score": 0.6342097569319758,
            "fpr": 0.06469298245614036,
            "logloss": 7.698548702032145,
            "mae": 0.3831616740855102,
            "precision": 0.7581967213114754,
            "recall": 0.3978494623655914
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.6276605155624101,
            "auditor_fn_violation": 0.07234684463240691,
            "auditor_fp_violation": 0.007975195218004279,
            "ave_precision_score": 0.6130423462076968,
            "fpr": 0.07135016465422613,
            "logloss": 9.733827026590518,
            "mae": 0.4254569722555711,
            "precision": 0.7280334728033473,
            "recall": 0.3558282208588957
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 871,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.6658034624124526,
            "auditor_fn_violation": 0.08704489718920959,
            "auditor_fp_violation": 0.02497890419561207,
            "ave_precision_score": 0.6502254282823567,
            "fpr": 0.10087719298245613,
            "logloss": 7.559763873643511,
            "mae": 0.36122701902215515,
            "precision": 0.7195121951219512,
            "recall": 0.5075268817204301
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.6353012779903886,
            "auditor_fn_violation": 0.09274511256422861,
            "auditor_fp_violation": 0.016637100004682114,
            "ave_precision_score": 0.6204911859691296,
            "fpr": 0.0889132821075741,
            "logloss": 9.630402506174244,
            "mae": 0.4043115701108554,
            "precision": 0.7244897959183674,
            "recall": 0.43558282208588955
        }
    }
]
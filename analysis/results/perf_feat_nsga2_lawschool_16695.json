[
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(0)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.9357326726484,
            "mae": 0.5482456140350878,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.21251864246921,
            "mae": 0.4983534577387486,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(1)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.9357326726484,
            "mae": 0.5482456140350878,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.21251864246921,
            "mae": 0.4983534577387486,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(2)",
        "seed": 16695,
        "test": {
            "accuracy": 0.543859649122807,
            "auc_prc": 0.6624443557752218,
            "auditor_fn_violation": 0.0008333333333333334,
            "auditor_fp_violation": 0.0006573624595469328,
            "ave_precision_score": 0.6632964551230899,
            "fpr": 0.4506578947368421,
            "logloss": 0.6930581410830667,
            "mae": 0.49995524903530614,
            "precision": 0.5463576158940397,
            "recall": 0.99
        },
        "train": {
            "accuracy": 0.4961580680570801,
            "auc_prc": 0.6058181401718319,
            "auditor_fn_violation": 0.0010251599394575357,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.6087134646450656,
            "fpr": 0.5016465422612514,
            "logloss": 0.6931876251200223,
            "mae": 0.5000198506039663,
            "precision": 0.49724972497249725,
            "recall": 0.9955947136563876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(3)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6942598197706454,
            "mae": 0.5000623905737149,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6915959863076911,
            "mae": 0.49884719435129676,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(4)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.7709363605910287,
            "auditor_fn_violation": 0.02335745614035088,
            "auditor_fp_violation": 0.020301056038153638,
            "ave_precision_score": 0.7717328323564216,
            "fpr": 0.20065789473684212,
            "logloss": 0.6547153055367511,
            "mae": 0.39368098819415953,
            "precision": 0.6696750902527075,
            "recall": 0.742
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.80269816519668,
            "auditor_fn_violation": 0.005952697572982202,
            "auditor_fp_violation": 0.025993990300893294,
            "ave_precision_score": 0.8029723958802593,
            "fpr": 0.19099890230515917,
            "logloss": 0.6042924428839167,
            "mae": 0.37523494840371624,
            "precision": 0.6647398843930635,
            "recall": 0.7599118942731278
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(5)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5054824561403509,
            "auc_prc": 0.6759178384945651,
            "auditor_fn_violation": 0.010109649122807034,
            "auditor_fp_violation": 0.01113790240163516,
            "ave_precision_score": 0.6225564709919883,
            "fpr": 0.13267543859649122,
            "logloss": 2.1386106091601893,
            "mae": 0.49603327091777794,
            "precision": 0.584192439862543,
            "recall": 0.34
        },
        "train": {
            "accuracy": 0.5214050493962679,
            "auc_prc": 0.6596829896147312,
            "auditor_fn_violation": 0.004581787936962335,
            "auditor_fp_violation": 0.01339331823302356,
            "ave_precision_score": 0.6031507691221891,
            "fpr": 0.15697036223929747,
            "logloss": 1.875131527790454,
            "mae": 0.46555938685867343,
            "precision": 0.5296052631578947,
            "recall": 0.35462555066079293
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(6)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7262680580874752,
            "auditor_fn_violation": 0.018894736842105266,
            "auditor_fp_violation": 0.02545350025549311,
            "ave_precision_score": 0.7208043812021466,
            "fpr": 0.15350877192982457,
            "logloss": 1.8247943514129121,
            "mae": 0.32190419209400895,
            "precision": 0.717741935483871,
            "recall": 0.712
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.6959400329935583,
            "auditor_fn_violation": 0.012611401519364405,
            "auditor_fp_violation": 0.020774535401259106,
            "ave_precision_score": 0.6895421425048774,
            "fpr": 0.16136114160263446,
            "logloss": 1.8159158225636083,
            "mae": 0.3016998374157237,
            "precision": 0.6962809917355371,
            "recall": 0.7422907488986784
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(7)",
        "seed": 16695,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.6094100340846174,
            "auditor_fn_violation": 0.00019956140350876894,
            "auditor_fp_violation": 0.0005801822517458695,
            "ave_precision_score": 0.6098315723167583,
            "fpr": 0.003289473684210526,
            "logloss": 1.7438131731976327,
            "mae": 0.504487524227762,
            "precision": 0.875,
            "recall": 0.042
        },
        "train": {
            "accuracy": 0.5257958287596048,
            "auc_prc": 0.5519103422458459,
            "auditor_fn_violation": 0.003293084522502742,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5530753561824766,
            "fpr": 0.0,
            "logloss": 1.5407871119200236,
            "mae": 0.4700959331788425,
            "precision": 1.0,
            "recall": 0.048458149779735685
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(8)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5054824561403509,
            "auc_prc": 0.6418321487920498,
            "auditor_fn_violation": 0.010109649122807034,
            "auditor_fp_violation": 0.01113790240163516,
            "ave_precision_score": 0.6262460649650579,
            "fpr": 0.13267543859649122,
            "logloss": 2.2870271762892522,
            "mae": 0.4971839408386554,
            "precision": 0.584192439862543,
            "recall": 0.34
        },
        "train": {
            "accuracy": 0.5214050493962679,
            "auc_prc": 0.6222660789477642,
            "auditor_fn_violation": 0.004581787936962335,
            "auditor_fp_violation": 0.01339331823302356,
            "ave_precision_score": 0.605929102335458,
            "fpr": 0.15697036223929747,
            "logloss": 2.0066739687973123,
            "mae": 0.46658137442261244,
            "precision": 0.5296052631578947,
            "recall": 0.35462555066079293
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(9)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.6617824278863371,
            "auditor_fn_violation": 0.0006206140350877193,
            "auditor_fp_violation": 0.0031697112927951037,
            "ave_precision_score": 0.662371624963583,
            "fpr": 0.4440789473684211,
            "logloss": 0.6925566799448452,
            "mae": 0.49969860097687496,
            "precision": 0.5519911504424779,
            "recall": 0.998
        },
        "train": {
            "accuracy": 0.5038419319429198,
            "auc_prc": 0.6299429667699552,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0015804884141552462,
            "ave_precision_score": 0.6314422827019626,
            "fpr": 0.4961580680570801,
            "logloss": 0.6930798822076287,
            "mae": 0.4999605446366918,
            "precision": 0.5011037527593819,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(10)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6937249972650331,
            "mae": 0.5000061284619988,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6918131868829105,
            "mae": 0.4991139319084347,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(11)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5021929824561403,
            "auc_prc": 0.6549533933207744,
            "auditor_fn_violation": 0.008287280701754399,
            "auditor_fp_violation": 0.0006413941406915346,
            "ave_precision_score": 0.6555294379423293,
            "fpr": 0.005482456140350877,
            "logloss": 0.6769126827997843,
            "mae": 0.4765216352973591,
            "precision": 0.9107142857142857,
            "recall": 0.102
        },
        "train": {
            "accuracy": 0.562019758507135,
            "auc_prc": 0.621122204229618,
            "auditor_fn_violation": 0.0030247053874089035,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6216765972118945,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6579029075830197,
            "mae": 0.4691967764617845,
            "precision": 0.9661016949152542,
            "recall": 0.12555066079295155
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(12)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6491228070175439,
            "auc_prc": 0.7690383960211841,
            "auditor_fn_violation": 0.02235964912280702,
            "auditor_fp_violation": 0.015297649463464487,
            "ave_precision_score": 0.7696067402413986,
            "fpr": 0.10526315789473684,
            "logloss": 0.6527694373616761,
            "mae": 0.38975548103859947,
            "precision": 0.7419354838709677,
            "recall": 0.552
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.8040953781895092,
            "auditor_fn_violation": 0.009743855084938373,
            "auditor_fp_violation": 0.011020183653714509,
            "ave_precision_score": 0.8043583011977182,
            "fpr": 0.08562019758507135,
            "logloss": 0.5874871010673711,
            "mae": 0.36630922368809155,
            "precision": 0.7796610169491526,
            "recall": 0.6079295154185022
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(13)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.6642215970953125,
            "auditor_fn_violation": 0.009574561403508776,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.6622156579950388,
            "fpr": 0.006578947368421052,
            "logloss": 0.6610594335648149,
            "mae": 0.46543552131767857,
            "precision": 0.925,
            "recall": 0.148
        },
        "train": {
            "accuracy": 0.58397365532382,
            "auc_prc": 0.6352565523728005,
            "auditor_fn_violation": 0.0020575733690527506,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6336628750615032,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6399782083161912,
            "mae": 0.4586842054527756,
            "precision": 0.9746835443037974,
            "recall": 0.1696035242290749
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(14)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6936013942461231,
            "mae": 0.5000015226633925,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.691915069312837,
            "mae": 0.49920831213405703,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(15)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.6646852215431416,
            "auditor_fn_violation": 0.009574561403508776,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.662638124466852,
            "fpr": 0.006578947368421052,
            "logloss": 0.6608908359687725,
            "mae": 0.4654508450378974,
            "precision": 0.925,
            "recall": 0.148
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.63413330514078,
            "auditor_fn_violation": 0.0018085368743260373,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6326145963533004,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6409315412381934,
            "mae": 0.45937655521014914,
            "precision": 0.9743589743589743,
            "recall": 0.16740088105726872
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(16)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5296052631578947,
            "auc_prc": 0.6692173162208649,
            "auditor_fn_violation": 0.00949780701754388,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.6670021263449093,
            "fpr": 0.006578947368421052,
            "logloss": 0.6591789345296124,
            "mae": 0.46376919383673293,
            "precision": 0.927710843373494,
            "recall": 0.154
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.647897511061304,
            "auditor_fn_violation": 0.0023186990140089143,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6457706620408116,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6335315501616109,
            "mae": 0.4540201964349569,
            "precision": 0.9770114942528736,
            "recall": 0.18722466960352424
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(17)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5416666666666666,
            "auc_prc": 0.5811354536091908,
            "auditor_fn_violation": 0.01228508771929825,
            "auditor_fp_violation": 0.0003832396525293817,
            "ave_precision_score": 0.5799419993645666,
            "fpr": 0.013157894736842105,
            "logloss": 0.6780418180489872,
            "mae": 0.4625387097333084,
            "precision": 0.8867924528301887,
            "recall": 0.188
        },
        "train": {
            "accuracy": 0.6092206366630076,
            "auc_prc": 0.5964844243087131,
            "auditor_fn_violation": 0.004543102656228078,
            "auditor_fp_violation": 0.002882349691468485,
            "ave_precision_score": 0.595190845790585,
            "fpr": 0.005488474204171241,
            "logloss": 0.6292166282137591,
            "mae": 0.4425405132718196,
            "precision": 0.9537037037037037,
            "recall": 0.22687224669603523
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(18)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5054824561403509,
            "auc_prc": 0.6581915793891805,
            "auditor_fn_violation": 0.008469298245614038,
            "auditor_fp_violation": 0.0006413941406915346,
            "ave_precision_score": 0.6587473608910267,
            "fpr": 0.005482456140350877,
            "logloss": 0.6754861081074679,
            "mae": 0.4749411409230609,
            "precision": 0.9152542372881356,
            "recall": 0.108
        },
        "train": {
            "accuracy": 0.5642151481888035,
            "auc_prc": 0.6233998969163077,
            "auditor_fn_violation": 0.004158667678931515,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.623949794219246,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6563267123377154,
            "mae": 0.46778724778496733,
            "precision": 0.9672131147540983,
            "recall": 0.1299559471365639
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(19)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6939337089897101,
            "mae": 0.5002415192623934,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6923844078808812,
            "mae": 0.4994975892105165,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(20)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.8220474651172618,
            "auditor_fn_violation": 0.0526140350877193,
            "auditor_fp_violation": 0.05338741270652359,
            "ave_precision_score": 0.8224299938584747,
            "fpr": 0.17105263157894737,
            "logloss": 0.5820483438146048,
            "mae": 0.38828126615599695,
            "precision": 0.7028571428571428,
            "recall": 0.738
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8573773295487764,
            "auditor_fn_violation": 0.04238214287441308,
            "auditor_fp_violation": 0.04632656541612724,
            "ave_precision_score": 0.8579068093833848,
            "fpr": 0.1668496158068057,
            "logloss": 0.5195152979575927,
            "mae": 0.3629446977383482,
            "precision": 0.7037037037037037,
            "recall": 0.7951541850220264
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(21)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5767543859649122,
            "auc_prc": 0.7783696114596159,
            "auditor_fn_violation": 0.0020614035087719302,
            "auditor_fp_violation": 0.010581672628172374,
            "ave_precision_score": 0.5654954550499152,
            "fpr": 0.41228070175438597,
            "logloss": 0.6838909189348045,
            "mae": 0.49438368404905003,
            "precision": 0.5658198614318707,
            "recall": 0.98
        },
        "train": {
            "accuracy": 0.5181119648737651,
            "auc_prc": 0.7509227188234501,
            "auditor_fn_violation": 0.0021953896816684962,
            "auditor_fp_violation": 0.006845580517237668,
            "ave_precision_score": 0.5083787855650793,
            "fpr": 0.47530186608122943,
            "logloss": 0.6898965628018571,
            "mae": 0.49758258789090765,
            "precision": 0.5085130533484676,
            "recall": 0.986784140969163
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(22)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.7959682427244612,
            "auditor_fn_violation": 0.036771929824561414,
            "auditor_fp_violation": 0.007260262306251068,
            "ave_precision_score": 0.7769377017857806,
            "fpr": 0.07675438596491228,
            "logloss": 3.9207758572636284,
            "mae": 0.3234403353367365,
            "precision": 0.7953216374269005,
            "recall": 0.544
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7785138707216432,
            "auditor_fn_violation": 0.030861182705745255,
            "auditor_fp_violation": 0.004823131817057266,
            "ave_precision_score": 0.7540405092869176,
            "fpr": 0.08122941822173436,
            "logloss": 3.600031893631214,
            "mae": 0.2895081057860182,
            "precision": 0.7804154302670623,
            "recall": 0.579295154185022
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(23)",
        "seed": 16695,
        "test": {
            "accuracy": 0.518640350877193,
            "auc_prc": 0.6543591075809583,
            "auditor_fn_violation": 0.005317982456140362,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.6527640339146012,
            "fpr": 0.006578947368421052,
            "logloss": 0.6664879724072406,
            "mae": 0.4695400570829709,
            "precision": 0.9178082191780822,
            "recall": 0.134
        },
        "train": {
            "accuracy": 0.5784851811196488,
            "auc_prc": 0.6271302297275695,
            "auditor_fn_violation": 0.00018859074357945925,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6258642329282962,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6438437331575663,
            "mae": 0.46150192948779994,
            "precision": 0.972972972972973,
            "recall": 0.15859030837004406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(24)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6546052631578947,
            "auc_prc": 0.7667926696187056,
            "auditor_fn_violation": 0.02421052631578948,
            "auditor_fp_violation": 0.01181921733946518,
            "ave_precision_score": 0.7673632522838053,
            "fpr": 0.13706140350877194,
            "logloss": 0.6516377170458401,
            "mae": 0.39296533076680806,
            "precision": 0.7126436781609196,
            "recall": 0.62
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.8032470332494912,
            "auditor_fn_violation": 0.010798028984946587,
            "auditor_fp_violation": 0.015134737838285778,
            "ave_precision_score": 0.8035090815080894,
            "fpr": 0.11745334796926454,
            "logloss": 0.5921465728259268,
            "mae": 0.3688145780283651,
            "precision": 0.7311557788944724,
            "recall": 0.6409691629955947
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(25)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.8218746385540607,
            "auditor_fn_violation": 0.0526140350877193,
            "auditor_fp_violation": 0.05338741270652359,
            "ave_precision_score": 0.8222581986633855,
            "fpr": 0.17105263157894737,
            "logloss": 0.5841953406040342,
            "mae": 0.3893896721695613,
            "precision": 0.7028571428571428,
            "recall": 0.738
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8574300329165705,
            "auditor_fn_violation": 0.04238214287441308,
            "auditor_fp_violation": 0.04632656541612724,
            "ave_precision_score": 0.8579594712734688,
            "fpr": 0.1668496158068057,
            "logloss": 0.5221075715930326,
            "mae": 0.3640034917157255,
            "precision": 0.7037037037037037,
            "recall": 0.7951541850220264
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(26)",
        "seed": 16695,
        "test": {
            "accuracy": 0.43201754385964913,
            "auc_prc": 0.6247124782466114,
            "auditor_fn_violation": 0.1041140350877193,
            "auditor_fp_violation": 0.07616888094021461,
            "ave_precision_score": 0.6262077496914916,
            "fpr": 0.2543859649122807,
            "logloss": 7.288104519656949,
            "mae": 0.5603786283372773,
            "precision": 0.4798206278026906,
            "recall": 0.428
        },
        "train": {
            "accuracy": 0.44127332601536773,
            "auc_prc": 0.6224929548850253,
            "auditor_fn_violation": 0.0986716441727878,
            "auditor_fp_violation": 0.081858731237705,
            "ave_precision_score": 0.6240404898297939,
            "fpr": 0.2678375411635565,
            "logloss": 6.766633151444377,
            "mae": 0.5455355464012165,
            "precision": 0.43648960739030024,
            "recall": 0.41629955947136565
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(27)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5219298245614035,
            "auc_prc": 0.6577065981152068,
            "auditor_fn_violation": 0.0063596491228070364,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.6559957246810252,
            "fpr": 0.006578947368421052,
            "logloss": 0.6642026487465195,
            "mae": 0.4679008271022324,
            "precision": 0.9210526315789473,
            "recall": 0.14
        },
        "train": {
            "accuracy": 0.5806805708013172,
            "auc_prc": 0.6298694543392254,
            "auditor_fn_violation": 0.002707969651397266,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6285640500193701,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6422135323472333,
            "mae": 0.4603476416070166,
            "precision": 0.9736842105263158,
            "recall": 0.16299559471365638
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(28)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.7670855227645317,
            "auditor_fn_violation": 0.02235964912280702,
            "auditor_fp_violation": 0.01434221171861693,
            "ave_precision_score": 0.7676548494613579,
            "fpr": 0.10635964912280702,
            "logloss": 0.6501532222450951,
            "mae": 0.3923272109067623,
            "precision": 0.739946380697051,
            "recall": 0.552
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.8038499312629099,
            "auditor_fn_violation": 0.009743855084938373,
            "auditor_fp_violation": 0.011020183653714509,
            "ave_precision_score": 0.8041114831650281,
            "fpr": 0.08562019758507135,
            "logloss": 0.5890214095713074,
            "mae": 0.36736636195008765,
            "precision": 0.7796610169491526,
            "recall": 0.6079295154185022
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(29)",
        "seed": 16695,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.6146769693330303,
            "auditor_fn_violation": 0.0009298245614035163,
            "auditor_fp_violation": 0.0005801822517458695,
            "ave_precision_score": 0.6147965812317546,
            "fpr": 0.003289473684210526,
            "logloss": 1.7608339361066263,
            "mae": 0.5029414919089615,
            "precision": 0.8888888888888888,
            "recall": 0.048
        },
        "train": {
            "accuracy": 0.5279912184412733,
            "auc_prc": 0.5563202480714527,
            "auditor_fn_violation": 0.0024855292871753592,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5571522809607767,
            "fpr": 0.0,
            "logloss": 1.5587066809085315,
            "mae": 0.46937747560155246,
            "precision": 1.0,
            "recall": 0.05286343612334802
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(30)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5482456140350878,
            "auc_prc": 0.7929114439433751,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.794530592692686,
            "fpr": 0.4517543859649123,
            "logloss": 0.6922771550090487,
            "mae": 0.4970710527217179,
            "precision": 0.5482456140350878,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.4983534577387486,
            "auc_prc": 0.813105524546827,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8138291856387194,
            "fpr": 0.5016465422612514,
            "logloss": 0.7023964263014811,
            "mae": 0.5016734634089548,
            "precision": 0.4983534577387486,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(31)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.6592478098954069,
            "auditor_fn_violation": 0.009574561403508776,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.6597578084990722,
            "fpr": 0.006578947368421052,
            "logloss": 0.6618915135300898,
            "mae": 0.4649926337429829,
            "precision": 0.925,
            "recall": 0.148
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.6272161359211572,
            "auditor_fn_violation": 0.0018085368743260373,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.627959762729878,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6389068708338179,
            "mae": 0.4573676976100281,
            "precision": 0.9743589743589743,
            "recall": 0.16740088105726872
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(32)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.7233329717733377,
            "mae": 0.5050456958374605,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.7023321149579043,
            "mae": 0.4972390841688086,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(33)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6864932819947623,
            "mae": 0.4939602965064216,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6883886131651161,
            "mae": 0.4958317073064107,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(34)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.794374552986438,
            "auditor_fn_violation": 0.03881578947368422,
            "auditor_fp_violation": 0.008489822858116168,
            "ave_precision_score": 0.7762506466080135,
            "fpr": 0.07785087719298246,
            "logloss": 3.8219049774823683,
            "mae": 0.3207239823784256,
            "precision": 0.7947976878612717,
            "recall": 0.55
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7742321830687697,
            "auditor_fn_violation": 0.034294501370909634,
            "auditor_fp_violation": 0.005291513641920897,
            "ave_precision_score": 0.7492392621976902,
            "fpr": 0.08562019758507135,
            "logloss": 3.579691059459811,
            "mae": 0.29120417115810276,
            "precision": 0.7771428571428571,
            "recall": 0.5991189427312775
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(35)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.7941361459719465,
            "auditor_fn_violation": 0.03881578947368422,
            "auditor_fp_violation": 0.008489822858116168,
            "ave_precision_score": 0.7760159931699399,
            "fpr": 0.07785087719298246,
            "logloss": 3.823362906456974,
            "mae": 0.3210825505715762,
            "precision": 0.7947976878612717,
            "recall": 0.55
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7738969887199056,
            "auditor_fn_violation": 0.03386170979269526,
            "auditor_fp_violation": 0.005291513641920897,
            "ave_precision_score": 0.7489027813302506,
            "fpr": 0.08562019758507135,
            "logloss": 3.582433144411493,
            "mae": 0.2917405089836867,
            "precision": 0.7765042979942693,
            "recall": 0.5969162995594713
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(36)",
        "seed": 16695,
        "test": {
            "accuracy": 0.506578947368421,
            "auc_prc": 0.6755156066347952,
            "auditor_fn_violation": 0.005296052631578951,
            "auditor_fp_violation": 0.0008676119911429059,
            "ave_precision_score": 0.6163811731897009,
            "fpr": 0.005482456140350877,
            "logloss": 2.3919386337054047,
            "mae": 0.500445406466816,
            "precision": 0.9166666666666666,
            "recall": 0.11
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.6587418732011096,
            "auditor_fn_violation": 0.007463841351663716,
            "auditor_fp_violation": 0.0006821560936475415,
            "ave_precision_score": 0.5956646678415478,
            "fpr": 0.0010976948408342481,
            "logloss": 2.090085423807853,
            "mae": 0.46349028018358485,
            "precision": 0.9833333333333333,
            "recall": 0.1299559471365639
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(37)",
        "seed": 16695,
        "test": {
            "accuracy": 0.43201754385964913,
            "auc_prc": 0.6247124782466114,
            "auditor_fn_violation": 0.1041140350877193,
            "auditor_fp_violation": 0.07616888094021461,
            "ave_precision_score": 0.6262077496914916,
            "fpr": 0.2543859649122807,
            "logloss": 7.288105946105921,
            "mae": 0.5603786270301575,
            "precision": 0.4798206278026906,
            "recall": 0.428
        },
        "train": {
            "accuracy": 0.44127332601536773,
            "auc_prc": 0.6224929548850253,
            "auditor_fn_violation": 0.0986716441727878,
            "auditor_fp_violation": 0.081858731237705,
            "ave_precision_score": 0.6240404898297939,
            "fpr": 0.2678375411635565,
            "logloss": 6.766634432189135,
            "mae": 0.5455355423447016,
            "precision": 0.43648960739030024,
            "recall": 0.41629955947136565
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(38)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.8220196279569367,
            "auditor_fn_violation": 0.0526140350877193,
            "auditor_fp_violation": 0.05338741270652359,
            "ave_precision_score": 0.8224023530172093,
            "fpr": 0.17105263157894737,
            "logloss": 0.5820490577895286,
            "mae": 0.3882816535616795,
            "precision": 0.7028571428571428,
            "recall": 0.738
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8573538061017515,
            "auditor_fn_violation": 0.04238214287441308,
            "auditor_fp_violation": 0.04632656541612724,
            "ave_precision_score": 0.8578834113423657,
            "fpr": 0.1668496158068057,
            "logloss": 0.5195177604761018,
            "mae": 0.36294606386077666,
            "precision": 0.7037037037037037,
            "recall": 0.7951541850220264
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(39)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5307017543859649,
            "auc_prc": 0.7286978141009484,
            "auditor_fn_violation": 0.008210526315789491,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.7262981188486639,
            "fpr": 0.006578947368421052,
            "logloss": 0.6570661957645157,
            "mae": 0.4623699218949728,
            "precision": 0.9285714285714286,
            "recall": 0.156
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.7180603559099523,
            "auditor_fn_violation": 0.0023186990140089143,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.7146374566285728,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6317478851869102,
            "mae": 0.4529819627886415,
            "precision": 0.9770114942528736,
            "recall": 0.18722466960352424
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(40)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.6984565854803098,
            "auditor_fn_violation": 0.042127192982456145,
            "auditor_fp_violation": 0.011510496508260944,
            "ave_precision_score": 0.6923645570484329,
            "fpr": 0.06030701754385965,
            "logloss": 3.719128996822205,
            "mae": 0.4302630161835152,
            "precision": 0.7477064220183486,
            "recall": 0.326
        },
        "train": {
            "accuracy": 0.5949506037321625,
            "auc_prc": 0.6450139374605758,
            "auditor_fn_violation": 0.047379797579268565,
            "auditor_fp_violation": 0.004729455452084541,
            "ave_precision_score": 0.6334292648747998,
            "fpr": 0.07574094401756312,
            "logloss": 3.586795165267344,
            "mae": 0.40544487020182063,
            "precision": 0.6905829596412556,
            "recall": 0.3392070484581498
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(41)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.7959506487821082,
            "auditor_fn_violation": 0.036771929824561414,
            "auditor_fp_violation": 0.007260262306251068,
            "ave_precision_score": 0.7769219838193464,
            "fpr": 0.07675438596491228,
            "logloss": 3.9209809197387133,
            "mae": 0.3234586828310359,
            "precision": 0.7953216374269005,
            "recall": 0.544
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7784909525833886,
            "auditor_fn_violation": 0.030861182705745255,
            "auditor_fp_violation": 0.004823131817057266,
            "ave_precision_score": 0.754017610171173,
            "fpr": 0.08122941822173436,
            "logloss": 3.6002100130426955,
            "mae": 0.28952852063259366,
            "precision": 0.7804154302670623,
            "recall": 0.579295154185022
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(42)",
        "seed": 16695,
        "test": {
            "accuracy": 0.518640350877193,
            "auc_prc": 0.6545020561807229,
            "auditor_fn_violation": 0.005317982456140362,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.6529023073230626,
            "fpr": 0.006578947368421052,
            "logloss": 0.6665705231099621,
            "mae": 0.4695341459949288,
            "precision": 0.9178082191780822,
            "recall": 0.134
        },
        "train": {
            "accuracy": 0.5784851811196488,
            "auc_prc": 0.6272225372226565,
            "auditor_fn_violation": 0.00018859074357945925,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6259536040045672,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6438356585914955,
            "mae": 0.46146468800195095,
            "precision": 0.972972972972973,
            "recall": 0.15859030837004406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(43)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6941826853372778,
            "mae": 0.5003478777382457,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6923908217211834,
            "mae": 0.49948259559996694,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(44)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5054824561403509,
            "auc_prc": 0.6759178384945651,
            "auditor_fn_violation": 0.010109649122807034,
            "auditor_fp_violation": 0.01113790240163516,
            "ave_precision_score": 0.6225564709919883,
            "fpr": 0.13267543859649122,
            "logloss": 2.4330844873351114,
            "mae": 0.4978863954071098,
            "precision": 0.584192439862543,
            "recall": 0.34
        },
        "train": {
            "accuracy": 0.5214050493962679,
            "auc_prc": 0.6596829896147312,
            "auditor_fn_violation": 0.004581787936962335,
            "auditor_fp_violation": 0.01339331823302356,
            "ave_precision_score": 0.6031507691221891,
            "fpr": 0.15697036223929747,
            "logloss": 2.1357675118915926,
            "mae": 0.4672246074281973,
            "precision": 0.5296052631578947,
            "recall": 0.35462555066079293
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(45)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7294248979363458,
            "auditor_fn_violation": 0.015712719298245616,
            "auditor_fp_violation": 0.027907298586271508,
            "ave_precision_score": 0.7273207816952655,
            "fpr": 0.15460526315789475,
            "logloss": 1.5655742571381954,
            "mae": 0.3137451036587912,
            "precision": 0.7213438735177866,
            "recall": 0.73
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7052105366304275,
            "auditor_fn_violation": 0.01517913702810002,
            "auditor_fp_violation": 0.016393363870227013,
            "ave_precision_score": 0.703339430829607,
            "fpr": 0.15697036223929747,
            "logloss": 1.5583472390433546,
            "mae": 0.28237343888401756,
            "precision": 0.7099391480730223,
            "recall": 0.7709251101321586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(46)",
        "seed": 16695,
        "test": {
            "accuracy": 0.43201754385964913,
            "auc_prc": 0.6253062176054925,
            "auditor_fn_violation": 0.1041140350877193,
            "auditor_fp_violation": 0.07616888094021461,
            "ave_precision_score": 0.6267969928633519,
            "fpr": 0.2543859649122807,
            "logloss": 7.2662635313978665,
            "mae": 0.560529313421054,
            "precision": 0.4798206278026906,
            "recall": 0.428
        },
        "train": {
            "accuracy": 0.44127332601536773,
            "auc_prc": 0.6225324836621267,
            "auditor_fn_violation": 0.0986716441727878,
            "auditor_fp_violation": 0.081858731237705,
            "ave_precision_score": 0.6240794683373961,
            "fpr": 0.2678375411635565,
            "logloss": 6.747309712867613,
            "mae": 0.5457381634706047,
            "precision": 0.43648960739030024,
            "recall": 0.41629955947136565
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(47)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5482456140350878,
            "auc_prc": 0.7718797481781074,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.7734905156965208,
            "fpr": 0.4517543859649123,
            "logloss": 0.6914107227269222,
            "mae": 0.4985575451793378,
            "precision": 0.5482456140350878,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.4983534577387486,
            "auc_prc": 0.7921986624281324,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.793424829113942,
            "fpr": 0.5016465422612514,
            "logloss": 0.6951146307542106,
            "mae": 0.5004730946277028,
            "precision": 0.4983534577387486,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(48)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6935112868123724,
            "mae": 0.5000084831395692,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.692038234969045,
            "mae": 0.4993092837762885,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(49)",
        "seed": 16695,
        "test": {
            "accuracy": 0.506578947368421,
            "auc_prc": 0.6599399175802994,
            "auditor_fn_violation": 0.007313596491228084,
            "auditor_fp_violation": 0.0006413941406915346,
            "ave_precision_score": 0.6604877325092382,
            "fpr": 0.005482456140350877,
            "logloss": 0.6753234852702051,
            "mae": 0.4744802142182986,
            "precision": 0.9166666666666666,
            "recall": 0.11
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.6260241155996289,
            "auditor_fn_violation": 0.00354453884727536,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.626575052982576,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6558474492054979,
            "mae": 0.46718252617886247,
            "precision": 0.967741935483871,
            "recall": 0.13215859030837004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(50)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5032894736842105,
            "auc_prc": 0.6473976094565692,
            "auditor_fn_violation": 0.006289473684210532,
            "auditor_fp_violation": 0.0006413941406915346,
            "ave_precision_score": 0.6487643586581271,
            "fpr": 0.005482456140350877,
            "logloss": 0.6761712501411311,
            "mae": 0.4760566657049614,
            "precision": 0.9122807017543859,
            "recall": 0.104
        },
        "train": {
            "accuracy": 0.562019758507135,
            "auc_prc": 0.615026657555808,
            "auditor_fn_violation": 0.0030247053874089035,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6175979500905203,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6577647800019945,
            "mae": 0.46919347965756575,
            "precision": 0.9661016949152542,
            "recall": 0.12555066079295155
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(51)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7431255933839318,
            "auditor_fn_violation": 0.01451535087719298,
            "auditor_fp_violation": 0.020673650144779426,
            "ave_precision_score": 0.7342864707046514,
            "fpr": 0.15679824561403508,
            "logloss": 1.7480056402113902,
            "mae": 0.3074568519703211,
            "precision": 0.7217898832684825,
            "recall": 0.742
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7156169839526445,
            "auditor_fn_violation": 0.011591077239998648,
            "auditor_fp_violation": 0.021612818769861195,
            "ave_precision_score": 0.706611512047562,
            "fpr": 0.15697036223929747,
            "logloss": 1.785221694878998,
            "mae": 0.28899094789957136,
            "precision": 0.7099391480730223,
            "recall": 0.7709251101321586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(52)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.7671098049630941,
            "auditor_fn_violation": 0.02235964912280702,
            "auditor_fp_violation": 0.01434221171861693,
            "ave_precision_score": 0.7676779346212805,
            "fpr": 0.10635964912280702,
            "logloss": 0.6497270631823255,
            "mae": 0.39210665266421674,
            "precision": 0.739946380697051,
            "recall": 0.552
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.8038755990340858,
            "auditor_fn_violation": 0.009743855084938373,
            "auditor_fp_violation": 0.011020183653714509,
            "ave_precision_score": 0.8041368929205652,
            "fpr": 0.08562019758507135,
            "logloss": 0.5884304781402376,
            "mae": 0.3670455931464024,
            "precision": 0.7796610169491526,
            "recall": 0.6079295154185022
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(53)",
        "seed": 16695,
        "test": {
            "accuracy": 0.506578947368421,
            "auc_prc": 0.6755156066347952,
            "auditor_fn_violation": 0.005296052631578951,
            "auditor_fp_violation": 0.0008676119911429059,
            "ave_precision_score": 0.6163811731897009,
            "fpr": 0.005482456140350877,
            "logloss": 2.391990065091953,
            "mae": 0.5004456852413983,
            "precision": 0.9166666666666666,
            "recall": 0.11
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.6587418732011096,
            "auditor_fn_violation": 0.007463841351663716,
            "auditor_fp_violation": 0.0006821560936475415,
            "ave_precision_score": 0.5956646678415478,
            "fpr": 0.0010976948408342481,
            "logloss": 2.09013081073166,
            "mae": 0.4634901870894403,
            "precision": 0.9833333333333333,
            "recall": 0.1299559471365639
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(54)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0.652706654325468,
            "auditor_fn_violation": 0.008414473684210532,
            "auditor_fp_violation": 0.0006413941406915346,
            "ave_precision_score": 0.653277689692694,
            "fpr": 0.005482456140350877,
            "logloss": 0.678031536550924,
            "mae": 0.47759335455402996,
            "precision": 0.9074074074074074,
            "recall": 0.098
        },
        "train": {
            "accuracy": 0.5565312843029637,
            "auc_prc": 0.6140185631773423,
            "auditor_fn_violation": 0.002824025493600014,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6146098216685802,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6618298149560591,
            "mae": 0.4721234429680817,
            "precision": 0.9629629629629629,
            "recall": 0.1145374449339207
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(55)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7293642426571916,
            "auditor_fn_violation": 0.020236842105263164,
            "auditor_fp_violation": 0.02315140095384091,
            "ave_precision_score": 0.7289449638609656,
            "fpr": 0.15899122807017543,
            "logloss": 1.4824384327054205,
            "mae": 0.3078446863041686,
            "precision": 0.7216890595009597,
            "recall": 0.752
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.7123573222851781,
            "auditor_fn_violation": 0.013085296208358924,
            "auditor_fp_violation": 0.01977772279962625,
            "ave_precision_score": 0.7093257807724884,
            "fpr": 0.15477497255762898,
            "logloss": 1.5338580763241112,
            "mae": 0.2746500658022507,
            "precision": 0.717434869739479,
            "recall": 0.788546255506608
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(56)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5482456140350878,
            "auc_prc": 0.7755526493362871,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.7768035342877995,
            "fpr": 0.4517543859649123,
            "logloss": 1.6853495127705704,
            "mae": 0.4425345273002198,
            "precision": 0.5482456140350878,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.4983534577387486,
            "auc_prc": 0.8111941577478594,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8126789112945099,
            "fpr": 0.5016465422612514,
            "logloss": 1.770871094520283,
            "mae": 0.4838185808922928,
            "precision": 0.4983534577387486,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(57)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5054824561403509,
            "auc_prc": 0.6757781171197406,
            "auditor_fn_violation": 0.010109649122807034,
            "auditor_fp_violation": 0.01113790240163516,
            "ave_precision_score": 0.6222882079998696,
            "fpr": 0.13267543859649122,
            "logloss": 2.1498127729732075,
            "mae": 0.4961382600289845,
            "precision": 0.584192439862543,
            "recall": 0.34
        },
        "train": {
            "accuracy": 0.5214050493962679,
            "auc_prc": 0.6595664820240468,
            "auditor_fn_violation": 0.004581787936962335,
            "auditor_fp_violation": 0.01339331823302356,
            "ave_precision_score": 0.6029279797948354,
            "fpr": 0.15697036223929747,
            "logloss": 1.8849422744752136,
            "mae": 0.46562207484611823,
            "precision": 0.5296052631578947,
            "recall": 0.35462555066079293
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(58)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5394736842105263,
            "auc_prc": 0.6339439997467238,
            "auditor_fn_violation": 0.04701754385964912,
            "auditor_fp_violation": 0.010464571623232841,
            "ave_precision_score": 0.6354883254030956,
            "fpr": 0.05482456140350877,
            "logloss": 9.884788129459498,
            "mae": 0.5070122072571026,
            "precision": 0.7222222222222222,
            "recall": 0.26
        },
        "train": {
            "accuracy": 0.5949506037321625,
            "auc_prc": 0.6389017596482176,
            "auditor_fn_violation": 0.05723970850640967,
            "auditor_fp_violation": 0.011113860018687239,
            "ave_precision_score": 0.6399366733899763,
            "fpr": 0.04720087815587267,
            "logloss": 9.109273987292141,
            "mae": 0.4640489284410942,
            "precision": 0.7485380116959064,
            "recall": 0.28193832599118945
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(59)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7266850601996759,
            "auditor_fn_violation": 0.016622807017543866,
            "auditor_fp_violation": 0.025038323965252938,
            "ave_precision_score": 0.7243183956188864,
            "fpr": 0.15789473684210525,
            "logloss": 1.5998287503049242,
            "mae": 0.31440827027322554,
            "precision": 0.7165354330708661,
            "recall": 0.728
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7025894941825601,
            "auditor_fn_violation": 0.008641324584012344,
            "auditor_fp_violation": 0.017106745418865456,
            "ave_precision_score": 0.7006750860588469,
            "fpr": 0.15806805708013172,
            "logloss": 1.5933050741842965,
            "mae": 0.28299267908678527,
            "precision": 0.708502024291498,
            "recall": 0.7709251101321586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(60)",
        "seed": 16695,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.6146518494460567,
            "auditor_fn_violation": 0.0009298245614035163,
            "auditor_fp_violation": 0.0005801822517458695,
            "ave_precision_score": 0.6147715803646319,
            "fpr": 0.003289473684210526,
            "logloss": 1.626202010185604,
            "mae": 0.49816099957669185,
            "precision": 0.8888888888888888,
            "recall": 0.048
        },
        "train": {
            "accuracy": 0.5279912184412733,
            "auc_prc": 0.5563204575109912,
            "auditor_fn_violation": 0.0024855292871753592,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5571524911088922,
            "fpr": 0.0,
            "logloss": 1.440361820210768,
            "mae": 0.4646409620785458,
            "precision": 1.0,
            "recall": 0.05286343612334802
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(61)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5515350877192983,
            "auc_prc": 0.783720419469425,
            "auditor_fn_violation": 0.0032850877192982467,
            "auditor_fp_violation": 0.004684040197581348,
            "ave_precision_score": 0.6179192182830373,
            "fpr": 0.43859649122807015,
            "logloss": 0.6854267914350327,
            "mae": 0.49293079805609424,
            "precision": 0.5510662177328844,
            "recall": 0.982
        },
        "train": {
            "accuracy": 0.5071350164654226,
            "auc_prc": 0.7766303334629752,
            "auditor_fn_violation": 0.0011750654023027413,
            "auditor_fp_violation": 0.0021257328974580185,
            "ave_precision_score": 0.5949912221103947,
            "fpr": 0.4884742041712404,
            "logloss": 0.6940723453579717,
            "mae": 0.49767546905317106,
            "precision": 0.5027932960893855,
            "recall": 0.9911894273127754
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(62)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7421583822519917,
            "auditor_fn_violation": 0.017087719298245617,
            "auditor_fp_violation": 0.01884261624936127,
            "ave_precision_score": 0.7343397092396262,
            "fpr": 0.14912280701754385,
            "logloss": 1.7316009493247217,
            "mae": 0.3097719275281893,
            "precision": 0.728,
            "recall": 0.728
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7118005522658688,
            "auditor_fn_violation": 0.013206187710653445,
            "auditor_fp_violation": 0.02356561068583109,
            "ave_precision_score": 0.7045525961291894,
            "fpr": 0.14928649835345773,
            "logloss": 1.7623937459806744,
            "mae": 0.2884676467550084,
            "precision": 0.7142857142857143,
            "recall": 0.748898678414097
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(63)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7331019265715194,
            "auditor_fn_violation": 0.018333333333333344,
            "auditor_fp_violation": 0.02432507238971215,
            "ave_precision_score": 0.7254239671339956,
            "fpr": 0.15789473684210525,
            "logloss": 1.8170061766566235,
            "mae": 0.31914493701015845,
            "precision": 0.7159763313609467,
            "recall": 0.726
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7005565834524785,
            "auditor_fn_violation": 0.010846385585864397,
            "auditor_fp_violation": 0.01989541874536122,
            "ave_precision_score": 0.6932808922994149,
            "fpr": 0.16575192096597147,
            "logloss": 1.8239980106692346,
            "mae": 0.29947490426095696,
            "precision": 0.6937119675456389,
            "recall": 0.7533039647577092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(64)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7417015890359526,
            "auditor_fn_violation": 0.016508771929824567,
            "auditor_fp_violation": 0.021024953159598028,
            "ave_precision_score": 0.7334518700337962,
            "fpr": 0.15789473684210525,
            "logloss": 1.7370103823281022,
            "mae": 0.3079450369034619,
            "precision": 0.722007722007722,
            "recall": 0.748
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7136025478675219,
            "auditor_fn_violation": 0.010889906526690425,
            "auditor_fp_violation": 0.02128855442957099,
            "ave_precision_score": 0.7050837877405912,
            "fpr": 0.16136114160263446,
            "logloss": 1.7797948069182745,
            "mae": 0.29086027061002145,
            "precision": 0.7024291497975709,
            "recall": 0.76431718061674
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(65)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.751469297963739,
            "auditor_fn_violation": 0.029320175438596496,
            "auditor_fp_violation": 0.01481327712485096,
            "ave_precision_score": 0.7525260096479821,
            "fpr": 0.06359649122807018,
            "logloss": 0.6603418696548125,
            "mae": 0.3984572259431476,
            "precision": 0.8141025641025641,
            "recall": 0.508
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7765508695520021,
            "auditor_fn_violation": 0.013585787027858238,
            "auditor_fp_violation": 0.005080141331213205,
            "ave_precision_score": 0.7769943571977214,
            "fpr": 0.050493962678375415,
            "logloss": 0.5921997966419658,
            "mae": 0.37076911619907627,
            "precision": 0.8461538461538461,
            "recall": 0.5572687224669604
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(66)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7280496563170347,
            "auditor_fn_violation": 0.01778947368421053,
            "auditor_fp_violation": 0.02545350025549311,
            "ave_precision_score": 0.7224531526640088,
            "fpr": 0.15350877192982457,
            "logloss": 1.7836122696363303,
            "mae": 0.3215074581708521,
            "precision": 0.7188755020080321,
            "recall": 0.716
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.6967752238584038,
            "auditor_fn_violation": 0.01275163566202605,
            "auditor_fp_violation": 0.021521544362964697,
            "ave_precision_score": 0.6901202228159381,
            "fpr": 0.1602634467618002,
            "logloss": 1.8004820146495222,
            "mae": 0.3018650657631465,
            "precision": 0.6970954356846473,
            "recall": 0.7400881057268722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(67)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5317982456140351,
            "auc_prc": 0.6276853822302979,
            "auditor_fn_violation": 0.03719517543859649,
            "auditor_fp_violation": 0.009357434849259067,
            "ave_precision_score": 0.6291611179098524,
            "fpr": 0.04824561403508772,
            "logloss": 7.922212137581337,
            "mae": 0.5029413647963284,
            "precision": 0.7267080745341615,
            "recall": 0.234
        },
        "train": {
            "accuracy": 0.5850713501646543,
            "auc_prc": 0.6219816312823,
            "auditor_fn_violation": 0.047379797579268565,
            "auditor_fp_violation": 0.008747931313606853,
            "ave_precision_score": 0.6235190687367318,
            "fpr": 0.043907793633369926,
            "logloss": 7.280777599043133,
            "mae": 0.4599423125139228,
            "precision": 0.7435897435897436,
            "recall": 0.2555066079295154
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(68)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.6646852215431416,
            "auditor_fn_violation": 0.009574561403508776,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.662638124466852,
            "fpr": 0.006578947368421052,
            "logloss": 0.6610949782153164,
            "mae": 0.46544424692789715,
            "precision": 0.925,
            "recall": 0.148
        },
        "train": {
            "accuracy": 0.58397365532382,
            "auc_prc": 0.6357558872922046,
            "auditor_fn_violation": 0.0020575733690527506,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6341328236826749,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6400649322215649,
            "mae": 0.4587096221229771,
            "precision": 0.9746835443037974,
            "recall": 0.1696035242290749
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(69)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6381578947368421,
            "auc_prc": 0.8271753461382798,
            "auditor_fn_violation": 0.003201754385964913,
            "auditor_fp_violation": 0.024293135752001375,
            "ave_precision_score": 0.8273871652359498,
            "fpr": 0.3092105263157895,
            "logloss": 0.6789894807957721,
            "mae": 0.3701137974449809,
            "precision": 0.6158038147138964,
            "recall": 0.904
        },
        "train": {
            "accuracy": 0.6520307354555434,
            "auc_prc": 0.8487125105446452,
            "auditor_fn_violation": 0.008692099014976041,
            "auditor_fp_violation": 0.03312300187112537,
            "ave_precision_score": 0.8492095614123196,
            "fpr": 0.3227222832052689,
            "logloss": 0.6356897697331925,
            "mae": 0.3567817886037963,
            "precision": 0.5944827586206897,
            "recall": 0.9493392070484582
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(70)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5164473684210527,
            "auc_prc": 0.6516505905944371,
            "auditor_fn_violation": 0.007116228070175454,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.6501924631769249,
            "fpr": 0.006578947368421052,
            "logloss": 0.667415733147707,
            "mae": 0.47065831292747407,
            "precision": 0.9154929577464789,
            "recall": 0.13
        },
        "train": {
            "accuracy": 0.5762897914379802,
            "auc_prc": 0.6241848486159205,
            "auditor_fn_violation": 0.000512579969728769,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6230029721363753,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6456405716400653,
            "mae": 0.4629523400483095,
            "precision": 0.9722222222222222,
            "recall": 0.15418502202643172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(71)",
        "seed": 16695,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.6151263768430467,
            "auditor_fn_violation": 0.0009298245614035163,
            "auditor_fp_violation": 0.0005801822517458695,
            "ave_precision_score": 0.6152300609880759,
            "fpr": 0.003289473684210526,
            "logloss": 1.9015210629774564,
            "mae": 0.5058744604829004,
            "precision": 0.8888888888888888,
            "recall": 0.048
        },
        "train": {
            "accuracy": 0.5323819978046103,
            "auc_prc": 0.564774913445573,
            "auditor_fn_violation": 0.004376272383061655,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5653389187530459,
            "fpr": 0.0,
            "logloss": 1.6571766112786415,
            "mae": 0.46870477731583965,
            "precision": 1.0,
            "recall": 0.06167400881057269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(72)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.7843892129180579,
            "auditor_fn_violation": 0.0006206140350877193,
            "auditor_fp_violation": 0.0031697112927951037,
            "ave_precision_score": 0.618024660611931,
            "fpr": 0.4440789473684211,
            "logloss": 0.6914250472604245,
            "mae": 0.489254227586858,
            "precision": 0.5519911504424779,
            "recall": 0.998
        },
        "train": {
            "accuracy": 0.5038419319429198,
            "auc_prc": 0.7770034562604106,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0015804884141552462,
            "ave_precision_score": 0.5948311992839846,
            "fpr": 0.4961580680570801,
            "logloss": 0.6992726458720914,
            "mae": 0.4970332276091477,
            "precision": 0.5011037527593819,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(73)",
        "seed": 16695,
        "test": {
            "accuracy": 0.543859649122807,
            "auc_prc": 0.6624443557752218,
            "auditor_fn_violation": 0.0008333333333333334,
            "auditor_fp_violation": 0.0006573624595469328,
            "ave_precision_score": 0.6632964551230899,
            "fpr": 0.4506578947368421,
            "logloss": 0.6930581408228289,
            "mae": 0.4999552489045942,
            "precision": 0.5463576158940397,
            "recall": 0.99
        },
        "train": {
            "accuracy": 0.4961580680570801,
            "auc_prc": 0.6058181401718319,
            "auditor_fn_violation": 0.0010251599394575357,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.6087134646450656,
            "fpr": 0.5016465422612514,
            "logloss": 0.6931876253822085,
            "mae": 0.5000198507348217,
            "precision": 0.49724972497249725,
            "recall": 0.9955947136563876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(74)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7121361814624141,
            "auditor_fn_violation": 0.013684210526315792,
            "auditor_fp_violation": 0.02658192812127406,
            "ave_precision_score": 0.7115494165825005,
            "fpr": 0.2149122807017544,
            "logloss": 1.0596878257273188,
            "mae": 0.3566283136483674,
            "precision": 0.6807817589576547,
            "recall": 0.836
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.7618079697350135,
            "auditor_fn_violation": 0.015290357210210982,
            "auditor_fp_violation": 0.01470718930071795,
            "ave_precision_score": 0.7621195026931582,
            "fpr": 0.1942919868276619,
            "logloss": 0.8082928718641572,
            "mae": 0.32434786709984426,
            "precision": 0.6953528399311532,
            "recall": 0.8898678414096917
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(75)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5482456140350878,
            "auc_prc": 0.775126143097125,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.7763417634626717,
            "fpr": 0.4517543859649123,
            "logloss": 1.6512452292249038,
            "mae": 0.44182011720381287,
            "precision": 0.5482456140350878,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.4983534577387486,
            "auc_prc": 0.8201421496108507,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.821481356881018,
            "fpr": 0.5016465422612514,
            "logloss": 1.7321200707328674,
            "mae": 0.48262832357121876,
            "precision": 0.4983534577387486,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(76)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5635964912280702,
            "auc_prc": 0.6961037473368166,
            "auditor_fn_violation": 0.042324561403508786,
            "auditor_fp_violation": 0.010858456821665818,
            "ave_precision_score": 0.6900117555323789,
            "fpr": 0.05921052631578947,
            "logloss": 3.8581969687299287,
            "mae": 0.43552902943374233,
            "precision": 0.7428571428571429,
            "recall": 0.312
        },
        "train": {
            "accuracy": 0.5894621295279913,
            "auc_prc": 0.6419025861606864,
            "auditor_fn_violation": 0.04536332732099596,
            "auditor_fp_violation": 0.004729455452084541,
            "ave_precision_score": 0.6305341462712342,
            "fpr": 0.07574094401756312,
            "logloss": 3.7082760706095335,
            "mae": 0.4096481259005834,
            "precision": 0.6834862385321101,
            "recall": 0.32819383259911894
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(77)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6939337155944849,
            "mae": 0.5002415224321579,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6923844099924599,
            "mae": 0.4994975900283629,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(78)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.7843892129180579,
            "auditor_fn_violation": 0.0006206140350877193,
            "auditor_fp_violation": 0.0031697112927951037,
            "ave_precision_score": 0.618024660611931,
            "fpr": 0.4440789473684211,
            "logloss": 0.7304979214772388,
            "mae": 0.4767360967347039,
            "precision": 0.5519911504424779,
            "recall": 0.998
        },
        "train": {
            "accuracy": 0.5038419319429198,
            "auc_prc": 0.7770034562604106,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0015804884141552462,
            "ave_precision_score": 0.5948311992839846,
            "fpr": 0.4961580680570801,
            "logloss": 0.7702933953519178,
            "mae": 0.4968937470523475,
            "precision": 0.5011037527593819,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(79)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.728835502125021,
            "auditor_fn_violation": 0.018267543859649127,
            "auditor_fp_violation": 0.025205991313234543,
            "ave_precision_score": 0.7283838901938922,
            "fpr": 0.1611842105263158,
            "logloss": 1.4809993854574621,
            "mae": 0.308242126645404,
            "precision": 0.72,
            "recall": 0.756
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.7120574243883868,
            "auditor_fn_violation": 0.011209060092747962,
            "auditor_fp_violation": 0.018973066844091308,
            "ave_precision_score": 0.7090352200215162,
            "fpr": 0.15587266739846323,
            "logloss": 1.5342460171622456,
            "mae": 0.27510169420753644,
            "precision": 0.7176938369781312,
            "recall": 0.7951541850220264
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(80)",
        "seed": 16695,
        "test": {
            "accuracy": 0.643640350877193,
            "auc_prc": 0.6782628308069674,
            "auditor_fn_violation": 0.02335964912280702,
            "auditor_fp_violation": 0.023332375234202014,
            "ave_precision_score": 0.6795855150858023,
            "fpr": 0.14144736842105263,
            "logloss": 0.7687942861736285,
            "mae": 0.4041944563331638,
            "precision": 0.7020785219399538,
            "recall": 0.608
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7194236082546122,
            "auditor_fn_violation": 0.009165993703970565,
            "auditor_fp_violation": 0.014981012521407454,
            "ave_precision_score": 0.7207138293107018,
            "fpr": 0.11855104281009879,
            "logloss": 0.6686589973718446,
            "mae": 0.3824316239276846,
            "precision": 0.7346437346437347,
            "recall": 0.6585903083700441
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(81)",
        "seed": 16695,
        "test": {
            "accuracy": 0.49780701754385964,
            "auc_prc": 0.6506221642036907,
            "auditor_fn_violation": 0.008241228070175442,
            "auditor_fp_violation": 0.0006413941406915346,
            "ave_precision_score": 0.6512137369565694,
            "fpr": 0.005482456140350877,
            "logloss": 0.6801020174870083,
            "mae": 0.47875086675610456,
            "precision": 0.9038461538461539,
            "recall": 0.094
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.6075770035222786,
            "auditor_fn_violation": 0.0028312789837376955,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6081963866580471,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6640169596248394,
            "mae": 0.4735916972683761,
            "precision": 0.9607843137254902,
            "recall": 0.10792951541850221
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(82)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5219298245614035,
            "auc_prc": 0.653120514586108,
            "auditor_fn_violation": 0.0063596491228070364,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.6517413070739366,
            "fpr": 0.006578947368421052,
            "logloss": 0.6642212247244806,
            "mae": 0.46789149632840826,
            "precision": 0.9210526315789473,
            "recall": 0.14
        },
        "train": {
            "accuracy": 0.5806805708013172,
            "auc_prc": 0.6269931003677629,
            "auditor_fn_violation": 0.002707969651397266,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6258981494790173,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6421483116010324,
            "mae": 0.46030920092936534,
            "precision": 0.9736842105263158,
            "recall": 0.16299559471365638
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(83)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.783898546779665,
            "auditor_fn_violation": 0.022675438596491227,
            "auditor_fp_violation": 0.02273888605007665,
            "ave_precision_score": 0.7843757063217479,
            "fpr": 0.12719298245614036,
            "logloss": 0.6084680740145247,
            "mae": 0.38755913520141977,
            "precision": 0.7296037296037297,
            "recall": 0.626
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.8267166878974433,
            "auditor_fn_violation": 0.017785557817569887,
            "auditor_fp_violation": 0.014627924684202563,
            "ave_precision_score": 0.8269743408641448,
            "fpr": 0.1207464324917673,
            "logloss": 0.5465409053350173,
            "mae": 0.3683298509921181,
            "precision": 0.7380952380952381,
            "recall": 0.6828193832599119
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(84)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.7969117292237834,
            "auditor_fn_violation": 0.03437938596491229,
            "auditor_fp_violation": 0.0071538068472151274,
            "ave_precision_score": 0.7777322082390399,
            "fpr": 0.07456140350877193,
            "logloss": 4.041332139966576,
            "mae": 0.32235500693140734,
            "precision": 0.799410029498525,
            "recall": 0.542
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7818698106556771,
            "auditor_fn_violation": 0.0324908001566754,
            "auditor_fp_violation": 0.005034504127764956,
            "ave_precision_score": 0.7568903748206746,
            "fpr": 0.08232711306256861,
            "logloss": 3.686766824081566,
            "mae": 0.28788893211738087,
            "precision": 0.7787610619469026,
            "recall": 0.5814977973568282
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(85)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5394736842105263,
            "auc_prc": 0.6256743043061106,
            "auditor_fn_violation": 0.046276315789473686,
            "auditor_fp_violation": 0.01104475387497871,
            "ave_precision_score": 0.6271522302051435,
            "fpr": 0.0537280701754386,
            "logloss": 9.912442396521032,
            "mae": 0.5070965321440821,
            "precision": 0.7247191011235955,
            "recall": 0.258
        },
        "train": {
            "accuracy": 0.5938529088913282,
            "auc_prc": 0.6384045812952179,
            "auditor_fn_violation": 0.05778372026673502,
            "auditor_fp_violation": 0.011113860018687239,
            "ave_precision_score": 0.6394391868118667,
            "fpr": 0.04720087815587267,
            "logloss": 9.129720934428777,
            "mae": 0.464062849339791,
            "precision": 0.7470588235294118,
            "recall": 0.27973568281938327
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(86)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.6474434409209419,
            "auditor_fn_violation": 0.009574561403508776,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.646096331572293,
            "fpr": 0.006578947368421052,
            "logloss": 0.6610204113429657,
            "mae": 0.4654857718565485,
            "precision": 0.925,
            "recall": 0.148
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.6320222617692985,
            "auditor_fn_violation": 0.0018085368743260373,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.631721628321248,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6410032218665497,
            "mae": 0.45941521863644263,
            "precision": 0.9743589743589743,
            "recall": 0.16740088105726872
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(87)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5164473684210527,
            "auc_prc": 0.5524642645913479,
            "auditor_fn_violation": 0.0071491228070175526,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.5508772400103475,
            "fpr": 0.006578947368421052,
            "logloss": 0.6750039200389192,
            "mae": 0.4732201314416894,
            "precision": 0.9154929577464789,
            "recall": 0.13
        },
        "train": {
            "accuracy": 0.5784851811196488,
            "auc_prc": 0.5558803258715342,
            "auditor_fn_violation": 0.0005899505311972763,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.5541596667006571,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6442140446927399,
            "mae": 0.46143458363253514,
            "precision": 0.972972972972973,
            "recall": 0.15859030837004406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(88)",
        "seed": 16695,
        "test": {
            "accuracy": 0.506578947368421,
            "auc_prc": 0.6597921632557608,
            "auditor_fn_violation": 0.007313596491228084,
            "auditor_fp_violation": 0.0006413941406915346,
            "ave_precision_score": 0.6603397576293251,
            "fpr": 0.005482456140350877,
            "logloss": 0.6749263221832859,
            "mae": 0.4743139708185928,
            "precision": 0.9166666666666666,
            "recall": 0.11
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.625469302797089,
            "auditor_fn_violation": 0.00354453884727536,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6260194882473538,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6557241096003436,
            "mae": 0.4672172365609953,
            "precision": 0.967741935483871,
            "recall": 0.13215859030837004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(89)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.733003580323537,
            "auditor_fn_violation": 0.018333333333333344,
            "auditor_fp_violation": 0.025168731902571972,
            "ave_precision_score": 0.7253080069546638,
            "fpr": 0.15679824561403508,
            "logloss": 1.8188122526016177,
            "mae": 0.3196398120546185,
            "precision": 0.717391304347826,
            "recall": 0.726
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7008445011920607,
            "auditor_fn_violation": 0.010931009637470561,
            "auditor_fp_violation": 0.020649633581295488,
            "ave_precision_score": 0.6936689629933486,
            "fpr": 0.16355653128430298,
            "logloss": 1.8035055282504757,
            "mae": 0.29955316783092034,
            "precision": 0.6959183673469388,
            "recall": 0.751101321585903
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(90)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5317982456140351,
            "auc_prc": 0.6723525606703322,
            "auditor_fn_violation": 0.006037280701754412,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.6700272723504481,
            "fpr": 0.006578947368421052,
            "logloss": 0.6587344941787031,
            "mae": 0.4626172547996567,
            "precision": 0.9294117647058824,
            "recall": 0.158
        },
        "train": {
            "accuracy": 0.6004390779363337,
            "auc_prc": 0.6595359863196145,
            "auditor_fn_violation": 0.005130635357379462,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.6570159126846434,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6274474963321284,
            "mae": 0.44940591879131764,
            "precision": 0.9787234042553191,
            "recall": 0.2026431718061674
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(91)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7341183005415111,
            "auditor_fn_violation": 0.018333333333333344,
            "auditor_fp_violation": 0.02432507238971215,
            "ave_precision_score": 0.7259571457336562,
            "fpr": 0.15789473684210525,
            "logloss": 1.8208681878048112,
            "mae": 0.3195563376279936,
            "precision": 0.7159763313609467,
            "recall": 0.726
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7008383295737421,
            "auditor_fn_violation": 0.010846385585864397,
            "auditor_fp_violation": 0.020649633581295488,
            "ave_precision_score": 0.6936686013330262,
            "fpr": 0.16355653128430298,
            "logloss": 1.8052523220290149,
            "mae": 0.2995262974206657,
            "precision": 0.6965376782077393,
            "recall": 0.7533039647577092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(92)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7952714280028058,
            "auditor_fn_violation": 0.03447149122807018,
            "auditor_fp_violation": 0.007260262306251068,
            "ave_precision_score": 0.7754962728691049,
            "fpr": 0.07675438596491228,
            "logloss": 3.9604240823008805,
            "mae": 0.32373540920808574,
            "precision": 0.7947214076246334,
            "recall": 0.542
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7784760074402572,
            "auditor_fn_violation": 0.030861182705745255,
            "auditor_fp_violation": 0.004823131817057266,
            "ave_precision_score": 0.7539566328515342,
            "fpr": 0.08122941822173436,
            "logloss": 3.613927089508935,
            "mae": 0.28961715414387035,
            "precision": 0.7804154302670623,
            "recall": 0.579295154185022
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(93)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7235702872739385,
            "auditor_fn_violation": 0.016140350877192993,
            "auditor_fp_violation": 0.026981136092658835,
            "ave_precision_score": 0.7217463348196483,
            "fpr": 0.15679824561403508,
            "logloss": 1.5981902182702463,
            "mae": 0.3147912976887519,
            "precision": 0.717391304347826,
            "recall": 0.726
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7009413955804156,
            "auditor_fn_violation": 0.014494891125113032,
            "auditor_fp_violation": 0.017106745418865456,
            "ave_precision_score": 0.6991136400311604,
            "fpr": 0.15806805708013172,
            "logloss": 1.612820551310585,
            "mae": 0.2836605982250519,
            "precision": 0.7090909090909091,
            "recall": 0.7731277533039648
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(94)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.5863404641702836,
            "auditor_fn_violation": 0.0019736842105263336,
            "auditor_fp_violation": 0.0005801822517458695,
            "ave_precision_score": 0.5915770277145592,
            "fpr": 0.003289473684210526,
            "logloss": 1.6657578918050124,
            "mae": 0.4960946091123971,
            "precision": 0.8928571428571429,
            "recall": 0.05
        },
        "train": {
            "accuracy": 0.5345773874862788,
            "auc_prc": 0.5857346379887322,
            "auditor_fn_violation": 0.0037911575119561705,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5822957552936515,
            "fpr": 0.0,
            "logloss": 1.4455730888916531,
            "mae": 0.4570135590175635,
            "precision": 1.0,
            "recall": 0.06607929515418502
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(95)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.7423990127619847,
            "auditor_fn_violation": 0.006864035087719298,
            "auditor_fp_violation": 0.0168731902571964,
            "ave_precision_score": 0.743042204661319,
            "fpr": 0.40460526315789475,
            "logloss": 0.973959098561172,
            "mae": 0.3968487540423347,
            "precision": 0.5638297872340425,
            "recall": 0.954
        },
        "train": {
            "accuracy": 0.531284302963776,
            "auc_prc": 0.777835496558569,
            "auditor_fn_violation": 0.004777632170679459,
            "auditor_fp_violation": 0.015665570573131223,
            "ave_precision_score": 0.779648719077428,
            "fpr": 0.44017563117453345,
            "logloss": 0.8821437731240321,
            "mae": 0.4024622040247682,
            "precision": 0.5162846803377563,
            "recall": 0.9427312775330396
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(96)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7299805428842037,
            "auditor_fn_violation": 0.013833333333333338,
            "auditor_fp_violation": 0.027907298586271508,
            "ave_precision_score": 0.7279324288835083,
            "fpr": 0.15460526315789475,
            "logloss": 1.55857814644854,
            "mae": 0.31298906320549547,
            "precision": 0.7207920792079208,
            "recall": 0.728
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.706831529777846,
            "auditor_fn_violation": 0.015943171322601396,
            "auditor_fp_violation": 0.016393363870227013,
            "ave_precision_score": 0.705013099832281,
            "fpr": 0.15697036223929747,
            "logloss": 1.547363408815789,
            "mae": 0.28066564178957276,
            "precision": 0.7105263157894737,
            "recall": 0.7731277533039648
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(97)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6502192982456141,
            "auc_prc": 0.7692159420260097,
            "auditor_fn_violation": 0.02212719298245615,
            "auditor_fp_violation": 0.015297649463464487,
            "ave_precision_score": 0.7697841176570586,
            "fpr": 0.10526315789473684,
            "logloss": 0.6522479453409405,
            "mae": 0.38981613926329084,
            "precision": 0.7426273458445041,
            "recall": 0.554
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.8041406012212302,
            "auditor_fn_violation": 0.009743855084938373,
            "auditor_fp_violation": 0.011020183653714509,
            "ave_precision_score": 0.8044025242783356,
            "fpr": 0.08562019758507135,
            "logloss": 0.5877029968965344,
            "mae": 0.3665952871416355,
            "precision": 0.7796610169491526,
            "recall": 0.6079295154185022
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(98)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5164473684210527,
            "auc_prc": 0.6522943933973299,
            "auditor_fn_violation": 0.007116228070175454,
            "auditor_fp_violation": 0.00028742973939703656,
            "ave_precision_score": 0.6507951026603382,
            "fpr": 0.006578947368421052,
            "logloss": 0.6682176467778368,
            "mae": 0.47066828680404443,
            "precision": 0.9154929577464789,
            "recall": 0.13
        },
        "train": {
            "accuracy": 0.5762897914379802,
            "auc_prc": 0.624460783299194,
            "auditor_fn_violation": 0.000512579969728769,
            "auditor_fp_violation": 0.001143332044282499,
            "ave_precision_score": 0.623271429003847,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6455758633128478,
            "mae": 0.462619282346133,
            "precision": 0.9722222222222222,
            "recall": 0.15418502202643172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(99)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7264092173393553,
            "auditor_fn_violation": 0.016359649122807022,
            "auditor_fp_violation": 0.019917816385624256,
            "ave_precision_score": 0.724381637385009,
            "fpr": 0.15570175438596492,
            "logloss": 1.5745798477681152,
            "mae": 0.3126491456220145,
            "precision": 0.7215686274509804,
            "recall": 0.736
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7052874164883375,
            "auditor_fn_violation": 0.010923756147332892,
            "auditor_fp_violation": 0.017070716047722107,
            "ave_precision_score": 0.7033867209282331,
            "fpr": 0.15916575192096596,
            "logloss": 1.5685014019117167,
            "mae": 0.2824119609767996,
            "precision": 0.7070707070707071,
            "recall": 0.7709251101321586
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6293859649122807,
            "auc_prc": 0.6326520270259625,
            "auditor_fn_violation": 0.042964600959601515,
            "auditor_fp_violation": 0.04818990316437746,
            "ave_precision_score": 0.6319132174316511,
            "fpr": 0.18311403508771928,
            "logloss": 3.5524533846088406,
            "mae": 0.3813700292237857,
            "precision": 0.6484210526315789,
            "recall": 0.6430062630480167
        },
        "train": {
            "accuracy": 0.6125137211855104,
            "auc_prc": 0.6331936779646412,
            "auditor_fn_violation": 0.0329493327170836,
            "auditor_fp_violation": 0.044582523489662546,
            "ave_precision_score": 0.6295555937880115,
            "fpr": 0.19319429198682767,
            "logloss": 4.1563167273105,
            "mae": 0.4011068620792268,
            "precision": 0.6286919831223629,
            "recall": 0.6273684210526316
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6337719298245614,
            "auc_prc": 0.6614448228300764,
            "auditor_fn_violation": 0.04879271508625427,
            "auditor_fp_violation": 0.0429429520683927,
            "ave_precision_score": 0.6606152519233587,
            "fpr": 0.14473684210526316,
            "logloss": 3.2952045331684623,
            "mae": 0.3708471694750933,
            "precision": 0.6772616136919315,
            "recall": 0.5782881002087683
        },
        "train": {
            "accuracy": 0.6344676180021954,
            "auc_prc": 0.665067481791282,
            "auditor_fn_violation": 0.03454618984343405,
            "auditor_fp_violation": 0.0352168702605263,
            "ave_precision_score": 0.6619255144598488,
            "fpr": 0.1394072447859495,
            "logloss": 3.878541850704666,
            "mae": 0.3797425587602463,
            "precision": 0.6792929292929293,
            "recall": 0.5663157894736842
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6652252507461593,
            "auditor_fn_violation": 0.022046569973995536,
            "auditor_fp_violation": 0.03352528260605324,
            "ave_precision_score": 0.660745988193252,
            "fpr": 0.21271929824561403,
            "logloss": 2.2396404438582755,
            "mae": 0.3525294831900388,
            "precision": 0.6541889483065954,
            "recall": 0.7661795407098121
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6641880826859584,
            "auditor_fn_violation": 0.022330579467329144,
            "auditor_fp_violation": 0.03405371655303679,
            "ave_precision_score": 0.6597114790265435,
            "fpr": 0.21185510428100987,
            "logloss": 2.643651745526364,
            "mae": 0.3895486975346424,
            "precision": 0.6295585412667947,
            "recall": 0.6905263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.662233611965181,
            "auditor_fn_violation": 0.01904552613265941,
            "auditor_fp_violation": 0.03323659900328188,
            "ave_precision_score": 0.6576238245404527,
            "fpr": 0.20833333333333334,
            "logloss": 2.348833171901676,
            "mae": 0.3535171194250068,
            "precision": 0.6557971014492754,
            "recall": 0.755741127348643
        },
        "train": {
            "accuracy": 0.6212952799121844,
            "auc_prc": 0.6607994734460939,
            "auditor_fn_violation": 0.023550753943035418,
            "auditor_fp_violation": 0.0331775747993434,
            "ave_precision_score": 0.6563556260846158,
            "fpr": 0.20965971459934138,
            "logloss": 2.7409130944034974,
            "mae": 0.3920613007089413,
            "precision": 0.626953125,
            "recall": 0.6757894736842105
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6381578947368421,
            "auc_prc": 0.6702235330937354,
            "auditor_fn_violation": 0.00028156246566312043,
            "auditor_fp_violation": 0.039235646853855194,
            "ave_precision_score": 0.670445523991865,
            "fpr": 0.20065789473684212,
            "logloss": 2.8853953443356213,
            "mae": 0.3770823170088178,
            "precision": 0.6446601941747573,
            "recall": 0.6931106471816284
        },
        "train": {
            "accuracy": 0.5949506037321625,
            "auc_prc": 0.6613821341870441,
            "auditor_fn_violation": 0.004222080998324578,
            "auditor_fp_violation": 0.024703169216205606,
            "ave_precision_score": 0.658380059037537,
            "fpr": 0.21075740944017562,
            "logloss": 3.373381542143535,
            "mae": 0.41145866322681657,
            "precision": 0.6081632653061224,
            "recall": 0.6273684210526316
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6502192982456141,
            "auc_prc": 0.6844617600921317,
            "auditor_fn_violation": 0.031674632824231774,
            "auditor_fp_violation": 0.050912138892265306,
            "ave_precision_score": 0.6846484487319014,
            "fpr": 0.18859649122807018,
            "logloss": 2.5650217706256084,
            "mae": 0.3564231072939008,
            "precision": 0.6587301587301587,
            "recall": 0.6931106471816284
        },
        "train": {
            "accuracy": 0.6311745334796927,
            "auc_prc": 0.687178419648679,
            "auditor_fn_violation": 0.03159974579698425,
            "auditor_fp_violation": 0.03836896645484849,
            "ave_precision_score": 0.6839092332624696,
            "fpr": 0.1800219538968167,
            "logloss": 2.859637676051316,
            "mae": 0.37916877609219346,
            "precision": 0.6488222698072805,
            "recall": 0.6378947368421053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.6645997440033757,
            "auditor_fn_violation": 0.023115591693220524,
            "auditor_fp_violation": 0.034735727887848955,
            "ave_precision_score": 0.6601196652496824,
            "fpr": 0.21929824561403508,
            "logloss": 2.250932858403078,
            "mae": 0.3537966933981288,
            "precision": 0.6491228070175439,
            "recall": 0.7724425887265136
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.6632947270235365,
            "auditor_fn_violation": 0.02168582818187071,
            "auditor_fp_violation": 0.033610610378755075,
            "ave_precision_score": 0.6587409237591713,
            "fpr": 0.21624588364434688,
            "logloss": 2.6769659561419386,
            "mae": 0.3907443174268914,
            "precision": 0.6254752851711026,
            "recall": 0.6926315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5219298245614035,
            "auc_prc": 0.575001792210194,
            "auditor_fn_violation": 0.10230102919093143,
            "auditor_fp_violation": 0.07008933997812082,
            "ave_precision_score": 0.574663875943782,
            "fpr": 0.15350877192982457,
            "logloss": 5.0068646666754155,
            "mae": 0.47044017042011727,
            "precision": 0.56656346749226,
            "recall": 0.38204592901878914
        },
        "train": {
            "accuracy": 0.5060373216245884,
            "auc_prc": 0.5820323144757804,
            "auditor_fn_violation": 0.09420301577214167,
            "auditor_fp_violation": 0.07084411726200666,
            "ave_precision_score": 0.578473610490253,
            "fpr": 0.16575192096597147,
            "logloss": 5.623375081534826,
            "mae": 0.48471139025769855,
            "precision": 0.5382262996941896,
            "recall": 0.3705263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5460526315789473,
            "auc_prc": 0.6933351599422939,
            "auditor_fn_violation": 0.11476531882943267,
            "auditor_fp_violation": 0.08497427170698109,
            "ave_precision_score": 0.6134065507494368,
            "fpr": 0.17434210526315788,
            "logloss": 9.48695295717884,
            "mae": 0.4515442505032555,
            "precision": 0.5848563968668408,
            "recall": 0.46764091858037576
        },
        "train": {
            "accuracy": 0.5521405049396267,
            "auc_prc": 0.6858481208797386,
            "auditor_fn_violation": 0.10507597203766827,
            "auditor_fp_violation": 0.09191180173012821,
            "ave_precision_score": 0.5970293537435929,
            "fpr": 0.1964873765093304,
            "logloss": 9.858230778042333,
            "mae": 0.44802126319443825,
            "precision": 0.5788235294117647,
            "recall": 0.5178947368421053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.6542598838041032,
            "auditor_fn_violation": 0.015836171849247335,
            "auditor_fp_violation": 0.02315546371702929,
            "ave_precision_score": 0.6526895652461148,
            "fpr": 0.18092105263157895,
            "logloss": 2.417477056078256,
            "mae": 0.3858235576660489,
            "precision": 0.6604938271604939,
            "recall": 0.6701461377870563
        },
        "train": {
            "accuracy": 0.5938529088913282,
            "auc_prc": 0.6368434882074246,
            "auditor_fn_violation": 0.01797446415159743,
            "auditor_fp_violation": 0.03549884691688739,
            "ave_precision_score": 0.6332866373241486,
            "fpr": 0.18880351262349068,
            "logloss": 2.9444701908152,
            "mae": 0.42315104664370107,
            "precision": 0.6169265033407573,
            "recall": 0.5831578947368421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.6225104391552039,
            "auditor_fn_violation": 0.04122257627366957,
            "auditor_fp_violation": 0.05208966411409588,
            "ave_precision_score": 0.6217224887310213,
            "fpr": 0.18421052631578946,
            "logloss": 3.785342669051245,
            "mae": 0.38909488790358077,
            "precision": 0.6455696202531646,
            "recall": 0.6388308977035491
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.6242463935252105,
            "auditor_fn_violation": 0.038481714714888204,
            "auditor_fp_violation": 0.051747751739695265,
            "ave_precision_score": 0.6205695349088516,
            "fpr": 0.18221734357848518,
            "logloss": 4.41036973714875,
            "mae": 0.4101504957399716,
            "precision": 0.6351648351648351,
            "recall": 0.608421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6780016505551268,
            "auditor_fn_violation": 0.007753268871552578,
            "auditor_fp_violation": 0.03190207041854058,
            "ave_precision_score": 0.6739607931703238,
            "fpr": 0.22149122807017543,
            "logloss": 2.4926481726791803,
            "mae": 0.37036647231441444,
            "precision": 0.6360360360360361,
            "recall": 0.7369519832985386
        },
        "train": {
            "accuracy": 0.5993413830954994,
            "auc_prc": 0.6690384301639933,
            "auditor_fn_violation": 0.017036223929747532,
            "auditor_fp_violation": 0.024743451595685764,
            "ave_precision_score": 0.6648076321550884,
            "fpr": 0.21295279912184412,
            "logloss": 2.9354290829931036,
            "mae": 0.39802943418453757,
            "precision": 0.6104417670682731,
            "recall": 0.64
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6469298245614035,
            "auc_prc": 0.6936942817032307,
            "auditor_fn_violation": 0.00998516646522361,
            "auditor_fp_violation": 0.038164478748835134,
            "ave_precision_score": 0.6939578227136961,
            "fpr": 0.18311403508771928,
            "logloss": 2.477041179790709,
            "mae": 0.36214605007481626,
            "precision": 0.659877800407332,
            "recall": 0.6764091858037579
        },
        "train": {
            "accuracy": 0.605927552140505,
            "auc_prc": 0.6788276198572867,
            "auditor_fn_violation": 0.016456178866485642,
            "auditor_fp_violation": 0.03386237525050605,
            "ave_precision_score": 0.6776517574510617,
            "fpr": 0.18111964873765093,
            "logloss": 3.174207070635898,
            "mae": 0.4056118002076481,
            "precision": 0.6300448430493274,
            "recall": 0.5915789473684211
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.6398901713589471,
            "auditor_fn_violation": 0.056088158810387145,
            "auditor_fp_violation": 0.055110712693975134,
            "ave_precision_score": 0.6392483309120929,
            "fpr": 0.1600877192982456,
            "logloss": 3.2465866753737975,
            "mae": 0.38538536944689733,
            "precision": 0.65807962529274,
            "recall": 0.5866388308977035
        },
        "train": {
            "accuracy": 0.610318331503842,
            "auc_prc": 0.6427759878700441,
            "auditor_fn_violation": 0.048148362123750656,
            "auditor_fp_violation": 0.059340980271704646,
            "ave_precision_score": 0.6397098532312717,
            "fpr": 0.1690450054884742,
            "logloss": 3.6767295179325616,
            "mae": 0.39890350627177273,
            "precision": 0.6401869158878505,
            "recall": 0.5768421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.679248011536918,
            "auditor_fn_violation": 0.02034574955133136,
            "auditor_fp_violation": 0.03632095539078643,
            "ave_precision_score": 0.6776568226618556,
            "fpr": 0.1875,
            "logloss": 2.077409597846258,
            "mae": 0.34964965553462596,
            "precision": 0.6749049429657795,
            "recall": 0.7411273486430062
        },
        "train": {
            "accuracy": 0.6212952799121844,
            "auc_prc": 0.6801377924882247,
            "auditor_fn_violation": 0.015046507597203766,
            "auditor_fp_violation": 0.030342702343427435,
            "ave_precision_score": 0.6762253036694702,
            "fpr": 0.18880351262349068,
            "logloss": 2.4954269627591037,
            "mae": 0.38625477022236676,
            "precision": 0.6371308016877637,
            "recall": 0.6357894736842106
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.6745164417468494,
            "auditor_fn_violation": 0.0470781599091675,
            "auditor_fp_violation": 0.03302135245735586,
            "ave_precision_score": 0.6571240684474953,
            "fpr": 0.20065789473684212,
            "logloss": 3.0715058143705383,
            "mae": 0.33548842164588744,
            "precision": 0.6678765880217786,
            "recall": 0.7682672233820459
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.7002848941941611,
            "auditor_fn_violation": 0.04195736322144549,
            "auditor_fp_violation": 0.04403871136668043,
            "ave_precision_score": 0.6861834674584318,
            "fpr": 0.18441273326015367,
            "logloss": 3.0221208484139934,
            "mae": 0.32728224358666896,
            "precision": 0.6830188679245283,
            "recall": 0.7621052631578947
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6019736842105263,
            "auc_prc": 0.7281807930115997,
            "auditor_fn_violation": 0.003081163242134564,
            "auditor_fp_violation": 0.01876190186783355,
            "ave_precision_score": 0.7230892036749389,
            "fpr": 0.38596491228070173,
            "logloss": 2.6558939884729265,
            "mae": 0.3926105449584144,
            "precision": 0.5707317073170731,
            "recall": 0.9770354906054279
        },
        "train": {
            "accuracy": 0.5982436882546652,
            "auc_prc": 0.7543551114899656,
            "auditor_fn_violation": 0.004265988791957941,
            "auditor_fp_violation": 0.025136204795617297,
            "ave_precision_score": 0.7485254211104094,
            "fpr": 0.3896816684961581,
            "logloss": 2.6621080161501727,
            "mae": 0.3967675375446317,
            "precision": 0.5665445665445665,
            "recall": 0.9768421052631578
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 9675,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7628571336939791,
            "auditor_fn_violation": 0.012690913086474017,
            "auditor_fp_violation": 0.004231493861674971,
            "ave_precision_score": 0.763072388345311,
            "fpr": 0.05701754385964912,
            "logloss": 1.371368170119398,
            "mae": 0.33451614592689893,
            "precision": 0.8424242424242424,
            "recall": 0.5803757828810021
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8210761502915345,
            "auditor_fn_violation": 0.008735339996533599,
            "auditor_fp_violation": 0.009023253003554922,
            "ave_precision_score": 0.8202185225161905,
            "fpr": 0.052689352360043906,
            "logloss": 0.9614533992183533,
            "mae": 0.3151052592814663,
            "precision": 0.8579881656804734,
            "recall": 0.6105263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.6279759082729651,
            "auditor_fn_violation": 0.010603230414240194,
            "auditor_fp_violation": 0.00993172886025688,
            "ave_precision_score": 0.616054039017338,
            "fpr": 0.1425438596491228,
            "logloss": 4.962712170430131,
            "mae": 0.3782715985068475,
            "precision": 0.6708860759493671,
            "recall": 0.5532359081419624
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.6349223186784638,
            "auditor_fn_violation": 0.010868334392512566,
            "auditor_fp_violation": 0.010548948126365833,
            "ave_precision_score": 0.619797221539238,
            "fpr": 0.12623490669593854,
            "logloss": 5.586275978218278,
            "mae": 0.3851566522200114,
            "precision": 0.6857923497267759,
            "recall": 0.5284210526315789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.6563534549297803,
            "auditor_fn_violation": 0.016573270336593053,
            "auditor_fp_violation": 0.023104817470929056,
            "ave_precision_score": 0.6566064841095391,
            "fpr": 0.10855263157894737,
            "logloss": 2.4920471935773727,
            "mae": 0.36999889147999115,
            "precision": 0.7195467422096318,
            "recall": 0.5302713987473904
        },
        "train": {
            "accuracy": 0.6212952799121844,
            "auc_prc": 0.6612998037443973,
            "auditor_fn_violation": 0.008954878964700458,
            "auditor_fp_violation": 0.01443871539491838,
            "ave_precision_score": 0.6580799209913225,
            "fpr": 0.10428100987925357,
            "logloss": 2.952101202632359,
            "mae": 0.39301491960617696,
            "precision": 0.703125,
            "recall": 0.47368421052631576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6567982456140351,
            "auc_prc": 0.7527866223544541,
            "auditor_fn_violation": 0.026617954070981217,
            "auditor_fp_violation": 0.06899031643774564,
            "ave_precision_score": 0.7523261529720879,
            "fpr": 0.26973684210526316,
            "logloss": 1.8586607434825526,
            "mae": 0.3585743634223627,
            "precision": 0.6261398176291794,
            "recall": 0.860125260960334
        },
        "train": {
            "accuracy": 0.6564215148188803,
            "auc_prc": 0.7630275506102037,
            "auditor_fn_violation": 0.02401756311745335,
            "auditor_fp_violation": 0.07496802586128763,
            "ave_precision_score": 0.7622298221831385,
            "fpr": 0.2864983534577388,
            "logloss": 1.8809633668187937,
            "mae": 0.35819238356673533,
            "precision": 0.618421052631579,
            "recall": 0.8905263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6502192982456141,
            "auc_prc": 0.759110523358518,
            "auditor_fn_violation": 0.016383272900413876,
            "auditor_fp_violation": 0.05380150723228394,
            "ave_precision_score": 0.759082101946019,
            "fpr": 0.3026315789473684,
            "logloss": 1.783629028408473,
            "mae": 0.3602124590290631,
            "precision": 0.6123595505617978,
            "recall": 0.9102296450939458
        },
        "train": {
            "accuracy": 0.6608122941822173,
            "auc_prc": 0.7677207877796333,
            "auditor_fn_violation": 0.01734357848518112,
            "auditor_fp_violation": 0.05836413256931087,
            "ave_precision_score": 0.7678638819091169,
            "fpr": 0.300768386388584,
            "logloss": 1.811058358908706,
            "mae": 0.35353672231228356,
            "precision": 0.6162464985994398,
            "recall": 0.9263157894736842
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5307017543859649,
            "auc_prc": 0.627998748653599,
            "auditor_fn_violation": 0.001348295059151023,
            "auditor_fp_violation": 0.011023155463717032,
            "ave_precision_score": 0.6218964629185142,
            "fpr": 0.047149122807017545,
            "logloss": 3.30497135174894,
            "mae": 0.4582079780538308,
            "precision": 0.6861313868613139,
            "recall": 0.19624217118997914
        },
        "train": {
            "accuracy": 0.5422612513721186,
            "auc_prc": 0.6233826851103365,
            "auditor_fn_violation": 0.009574209948581667,
            "auditor_fp_violation": 0.00727096949616814,
            "ave_precision_score": 0.6171311370010728,
            "fpr": 0.03293084522502744,
            "logloss": 3.8020213113800527,
            "mae": 0.4583898654454648,
            "precision": 0.7457627118644068,
            "recall": 0.18526315789473685
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.6954884439687757,
            "auditor_fn_violation": 0.012381881111965719,
            "auditor_fp_violation": 0.025563692719095674,
            "ave_precision_score": 0.6504604167112173,
            "fpr": 0.2982456140350877,
            "logloss": 3.6757968272601707,
            "mae": 0.363685123525654,
            "precision": 0.6185133239831697,
            "recall": 0.9206680584551148
        },
        "train": {
            "accuracy": 0.6663007683863886,
            "auc_prc": 0.7335425556422469,
            "auditor_fn_violation": 0.012125483852331158,
            "auditor_fp_violation": 0.03518665847591616,
            "ave_precision_score": 0.6914097305465599,
            "fpr": 0.2854006586169045,
            "logloss": 3.2594514249293183,
            "mae": 0.3585673507583653,
            "precision": 0.6237337192474675,
            "recall": 0.9073684210526316
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.7194360245112766,
            "auditor_fn_violation": 0.04000247225579607,
            "auditor_fp_violation": 0.06603257566549169,
            "ave_precision_score": 0.7161130916345925,
            "fpr": 0.23684210526315788,
            "logloss": 1.7364260418389008,
            "mae": 0.34703304289685716,
            "precision": 0.6393989983305509,
            "recall": 0.7995824634655533
        },
        "train": {
            "accuracy": 0.6553238199780461,
            "auc_prc": 0.7529735564744714,
            "auditor_fn_violation": 0.040103992142815875,
            "auditor_fp_violation": 0.07469611979979657,
            "ave_precision_score": 0.7503374223856372,
            "fpr": 0.24698133918770582,
            "logloss": 1.5662276653317277,
            "mae": 0.3430827973850528,
            "precision": 0.6317512274959084,
            "recall": 0.8126315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.673828465856772,
            "auditor_fn_violation": 0.026446269640698825,
            "auditor_fp_violation": 0.03577650824520888,
            "ave_precision_score": 0.6678784205386337,
            "fpr": 0.22149122807017543,
            "logloss": 2.247975669581416,
            "mae": 0.3450896987765508,
            "precision": 0.6541095890410958,
            "recall": 0.7974947807933194
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.681187954232755,
            "auditor_fn_violation": 0.02695938529088914,
            "auditor_fp_violation": 0.035320093857944206,
            "ave_precision_score": 0.6747364198391785,
            "fpr": 0.21844127332601537,
            "logloss": 2.5695556506886397,
            "mae": 0.3764794531497676,
            "precision": 0.6368613138686131,
            "recall": 0.7347368421052631
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6651816166536643,
            "auditor_fn_violation": 0.022046569973995536,
            "auditor_fp_violation": 0.03352528260605324,
            "ave_precision_score": 0.6607024229027911,
            "fpr": 0.21271929824561403,
            "logloss": 2.2398412313902214,
            "mae": 0.3525434488442523,
            "precision": 0.6541889483065954,
            "recall": 0.7661795407098121
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6642114395921542,
            "auditor_fn_violation": 0.022330579467329144,
            "auditor_fp_violation": 0.03405371655303679,
            "ave_precision_score": 0.6597396076888322,
            "fpr": 0.21185510428100987,
            "logloss": 2.6439249787535295,
            "mae": 0.38955983902669894,
            "precision": 0.6295585412667947,
            "recall": 0.6905263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.6693742922780893,
            "auditor_fn_violation": 0.004365362780646814,
            "auditor_fp_violation": 0.01804019286090515,
            "ave_precision_score": 0.6249222042977312,
            "fpr": 0.36403508771929827,
            "logloss": 6.655220789465186,
            "mae": 0.42444712569212817,
            "precision": 0.5602649006622517,
            "recall": 0.8830897703549061
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.6789101916757039,
            "auditor_fn_violation": 0.014383268819689179,
            "auditor_fp_violation": 0.01695384646371061,
            "ave_precision_score": 0.6395601235496983,
            "fpr": 0.36663007683863885,
            "logloss": 6.776744213265796,
            "mae": 0.4490616651837711,
            "precision": 0.5461956521739131,
            "recall": 0.8463157894736842
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6469298245614035,
            "auc_prc": 0.6830173873767003,
            "auditor_fn_violation": 0.003195619528989485,
            "auditor_fp_violation": 0.04227188930756453,
            "ave_precision_score": 0.6832415639111347,
            "fpr": 0.20723684210526316,
            "logloss": 2.4293868043910676,
            "mae": 0.3659221849017146,
            "precision": 0.6467289719626168,
            "recall": 0.7223382045929019
        },
        "train": {
            "accuracy": 0.6026344676180022,
            "auc_prc": 0.6844859516122092,
            "auditor_fn_violation": 0.0058951990294066675,
            "auditor_fp_violation": 0.030634749594658565,
            "ave_precision_score": 0.6819247918730551,
            "fpr": 0.21185510428100987,
            "logloss": 2.677482797868535,
            "mae": 0.40600616696906694,
            "precision": 0.6132264529058116,
            "recall": 0.6442105263157895
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 9675,
        "test": {
            "accuracy": 0.569078947368421,
            "auc_prc": 0.6116771433924435,
            "auditor_fn_violation": 0.06465406731860968,
            "auditor_fp_violation": 0.042160467566144,
            "ave_precision_score": 0.6119935978156723,
            "fpr": 0.10307017543859649,
            "logloss": 3.1633720258940023,
            "mae": 0.43389657846829616,
            "precision": 0.656934306569343,
            "recall": 0.3757828810020877
        },
        "train": {
            "accuracy": 0.5598243688254665,
            "auc_prc": 0.6169826995639853,
            "auditor_fn_violation": 0.060537292737882026,
            "auditor_fp_violation": 0.03148571486117685,
            "ave_precision_score": 0.6143424446561305,
            "fpr": 0.09879253567508232,
            "logloss": 3.482644776486838,
            "mae": 0.43690749076728796,
            "precision": 0.6456692913385826,
            "recall": 0.3452631578947368
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.7079631671916077,
            "auditor_fn_violation": 0.024832435996044393,
            "auditor_fp_violation": 0.012750192455735193,
            "ave_precision_score": 0.6973733805348927,
            "fpr": 0.17105263157894737,
            "logloss": 3.1056364365426927,
            "mae": 0.31673319395665994,
            "precision": 0.6929133858267716,
            "recall": 0.7348643006263048
        },
        "train": {
            "accuracy": 0.6641053787047201,
            "auc_prc": 0.7067947717036056,
            "auditor_fn_violation": 0.024033739673002482,
            "auditor_fp_violation": 0.019119024360768993,
            "ave_precision_score": 0.6969593032280239,
            "fpr": 0.16575192096597147,
            "logloss": 3.6861243799824774,
            "mae": 0.33710878692667917,
            "precision": 0.6794055201698513,
            "recall": 0.6736842105263158
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.6391584110094102,
            "auditor_fn_violation": 0.03566457898399444,
            "auditor_fp_violation": 0.04574875410234594,
            "ave_precision_score": 0.6383499966347564,
            "fpr": 0.1787280701754386,
            "logloss": 3.5687703728341216,
            "mae": 0.3847100370170868,
            "precision": 0.6471861471861472,
            "recall": 0.6242171189979123
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.6379697639379245,
            "auditor_fn_violation": 0.02575076549771796,
            "auditor_fp_violation": 0.045657559492039194,
            "ave_precision_score": 0.6343094883689582,
            "fpr": 0.17453347969264543,
            "logloss": 4.254185557404764,
            "mae": 0.39986710240790246,
            "precision": 0.6402714932126696,
            "recall": 0.5957894736842105
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.6308890106484026,
            "auditor_fn_violation": 0.04848139398600886,
            "auditor_fp_violation": 0.05256573882743812,
            "ave_precision_score": 0.630134101733108,
            "fpr": 0.17324561403508773,
            "logloss": 3.5676436312448025,
            "mae": 0.3872183766902131,
            "precision": 0.6496674057649667,
            "recall": 0.6116910229645094
        },
        "train": {
            "accuracy": 0.610318331503842,
            "auc_prc": 0.6322037693177602,
            "auditor_fn_violation": 0.040464498237910915,
            "auditor_fp_violation": 0.052273940321654805,
            "ave_precision_score": 0.6285815888489507,
            "fpr": 0.17892425905598244,
            "logloss": 4.160779623247564,
            "mae": 0.403754495253249,
            "precision": 0.6345291479820628,
            "recall": 0.5957894736842105
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 9675,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8738243135485251,
            "auditor_fn_violation": 0.0052741456982749145,
            "auditor_fp_violation": 0.012929986629391035,
            "ave_precision_score": 0.8732849538244776,
            "fpr": 0.19736842105263158,
            "logloss": 1.043796341087319,
            "mae": 0.2511473221442138,
            "precision": 0.712,
            "recall": 0.9290187891440501
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8790032519743878,
            "auditor_fn_violation": 0.008629036917210701,
            "auditor_fp_violation": 0.020951872627116085,
            "ave_precision_score": 0.8782302397974441,
            "fpr": 0.1877058177826564,
            "logloss": 0.9886815035309439,
            "mae": 0.2525242375986989,
            "precision": 0.7159468438538206,
            "recall": 0.9073684210526316
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6392543859649122,
            "auc_prc": 0.6690375884084645,
            "auditor_fn_violation": 0.0020121415229095692,
            "auditor_fp_violation": 0.035016814553705276,
            "ave_precision_score": 0.6692489191484872,
            "fpr": 0.18530701754385964,
            "logloss": 2.7562575607822497,
            "mae": 0.38013218553553135,
            "precision": 0.6536885245901639,
            "recall": 0.6659707724425887
        },
        "train": {
            "accuracy": 0.5960482985729967,
            "auc_prc": 0.6636455467348177,
            "auditor_fn_violation": 0.011485354439886774,
            "auditor_fp_violation": 0.030959526279217314,
            "ave_precision_score": 0.6606009122759381,
            "fpr": 0.1877058177826564,
            "logloss": 3.2144765073691106,
            "mae": 0.4146866530130042,
            "precision": 0.6191536748329621,
            "recall": 0.5852631578947368
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.6673991272408333,
            "auditor_fn_violation": 0.02478436435556533,
            "auditor_fp_violation": 0.030823305376605485,
            "ave_precision_score": 0.6636386598070354,
            "fpr": 0.16666666666666666,
            "logloss": 2.17377229619733,
            "mae": 0.34783481657731835,
            "precision": 0.6941649899396378,
            "recall": 0.7202505219206681
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.6677034822652781,
            "auditor_fn_violation": 0.02195620775319199,
            "auditor_fp_violation": 0.025116063605877204,
            "ave_precision_score": 0.6632015690959712,
            "fpr": 0.1778265642151482,
            "logloss": 2.628353827416071,
            "mae": 0.3832284022292194,
            "precision": 0.6478260869565218,
            "recall": 0.6273684210526316
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6293859649122807,
            "auc_prc": 0.6344098837069763,
            "auditor_fn_violation": 0.03435977731384831,
            "auditor_fp_violation": 0.04324176492038411,
            "ave_precision_score": 0.6336218287644932,
            "fpr": 0.20175438596491227,
            "logloss": 3.484490183577882,
            "mae": 0.3780585578336383,
            "precision": 0.6385068762278978,
            "recall": 0.6784968684759917
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.6365649380186671,
            "auditor_fn_violation": 0.030816338321104624,
            "auditor_fp_violation": 0.04293094593097615,
            "ave_precision_score": 0.6329500193883904,
            "fpr": 0.20197585071350166,
            "logloss": 4.062905989842845,
            "mae": 0.39943850655611407,
            "precision": 0.6260162601626016,
            "recall": 0.6484210526315789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.6591793561345247,
            "auditor_fn_violation": 0.0201351499835183,
            "auditor_fp_violation": 0.025837182448036955,
            "ave_precision_score": 0.6388715629974803,
            "fpr": 0.2916666666666667,
            "logloss": 3.0964432051204396,
            "mae": 0.35187857766784125,
            "precision": 0.6194563662374821,
            "recall": 0.9039665970772442
        },
        "train": {
            "accuracy": 0.6355653128430296,
            "auc_prc": 0.6745479590848559,
            "auditor_fn_violation": 0.02990351840083194,
            "auditor_fp_violation": 0.025634699241684218,
            "ave_precision_score": 0.6572383317395838,
            "fpr": 0.287596048298573,
            "logloss": 3.271142158328853,
            "mae": 0.37181272916172226,
            "precision": 0.6071964017991005,
            "recall": 0.8526315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6546052631578947,
            "auc_prc": 0.7748437709495117,
            "auditor_fn_violation": 0.03161282642933011,
            "auditor_fp_violation": 0.0683698999230177,
            "ave_precision_score": 0.7754442376863864,
            "fpr": 0.25877192982456143,
            "logloss": 1.3664923053370495,
            "mae": 0.34779883346934193,
            "precision": 0.6289308176100629,
            "recall": 0.8350730688935282
        },
        "train": {
            "accuracy": 0.6608122941822173,
            "auc_prc": 0.7885767367517662,
            "auditor_fn_violation": 0.028523889306141317,
            "auditor_fp_violation": 0.07759896877108531,
            "ave_precision_score": 0.7885337250683118,
            "fpr": 0.270032930845225,
            "logloss": 1.3120854314541963,
            "mae": 0.3415657613193495,
            "precision": 0.6261398176291794,
            "recall": 0.8673684210526316
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.6239416850710793,
            "auditor_fn_violation": 0.039585851371644144,
            "auditor_fp_violation": 0.04019286090514972,
            "ave_precision_score": 0.5704393885845047,
            "fpr": 0.31469298245614036,
            "logloss": 6.126020788514238,
            "mae": 0.3931788033124022,
            "precision": 0.5905848787446505,
            "recall": 0.8643006263048016
        },
        "train": {
            "accuracy": 0.5971459934138309,
            "auc_prc": 0.65298849161631,
            "auditor_fn_violation": 0.04675717834652494,
            "auditor_fp_violation": 0.04489471193063375,
            "ave_precision_score": 0.5993266218102216,
            "fpr": 0.30735455543358947,
            "logloss": 6.338041344671238,
            "mae": 0.40326267117169656,
            "precision": 0.5808383233532934,
            "recall": 0.8168421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.64325079605859,
            "auditor_fn_violation": 0.03885104201003553,
            "auditor_fp_violation": 0.04813165998136219,
            "ave_precision_score": 0.6426173740206337,
            "fpr": 0.20065789473684212,
            "logloss": 3.0258607508872237,
            "mae": 0.3671547055639366,
            "precision": 0.6467181467181468,
            "recall": 0.6993736951983298
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.6478131553441004,
            "auditor_fn_violation": 0.029355826448668323,
            "auditor_fp_violation": 0.04753824308401897,
            "ave_precision_score": 0.6448199086600455,
            "fpr": 0.20856201975850713,
            "logloss": 3.363746871716819,
            "mae": 0.39088312397488834,
            "precision": 0.62890625,
            "recall": 0.6778947368421052
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.6662305953520494,
            "auditor_fn_violation": 0.016216166721605692,
            "auditor_fp_violation": 0.007789392650216769,
            "ave_precision_score": 0.657670029139322,
            "fpr": 0.1513157894736842,
            "logloss": 2.41767670228725,
            "mae": 0.35091904125780876,
            "precision": 0.7094736842105264,
            "recall": 0.7035490605427975
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.6739998402406118,
            "auditor_fn_violation": 0.020957883182159572,
            "auditor_fp_violation": 0.01756311745334797,
            "ave_precision_score": 0.6653421293986979,
            "fpr": 0.14489571899012074,
            "logloss": 2.7519961800227737,
            "mae": 0.37846570792524387,
            "precision": 0.6826923076923077,
            "recall": 0.5978947368421053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.7559746440532773,
            "auditor_fn_violation": 0.020821887704647844,
            "auditor_fp_violation": 0.062421498318544634,
            "ave_precision_score": 0.7559361579893012,
            "fpr": 0.27960526315789475,
            "logloss": 1.7297482931074668,
            "mae": 0.3556390849305172,
            "precision": 0.6222222222222222,
            "recall": 0.8768267223382046
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.7652202578706512,
            "auditor_fn_violation": 0.019571321277947893,
            "auditor_fp_violation": 0.0678254564497125,
            "ave_precision_score": 0.7651831580299608,
            "fpr": 0.2854006586169045,
            "logloss": 1.75077477780365,
            "mae": 0.3501859708858728,
            "precision": 0.6242774566473989,
            "recall": 0.9094736842105263
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6652124422585214,
            "auditor_fn_violation": 0.022046569973995536,
            "auditor_fp_violation": 0.03352528260605324,
            "ave_precision_score": 0.6607242383566406,
            "fpr": 0.21271929824561403,
            "logloss": 2.239921772197254,
            "mae": 0.35254271698787365,
            "precision": 0.6541889483065954,
            "recall": 0.7661795407098121
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6642627654288191,
            "auditor_fn_violation": 0.022330579467329144,
            "auditor_fp_violation": 0.03405371655303679,
            "ave_precision_score": 0.659770538879525,
            "fpr": 0.21185510428100987,
            "logloss": 2.6443165136860425,
            "mae": 0.3895344028591896,
            "precision": 0.6295585412667947,
            "recall": 0.6905263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 9675,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7801292494926664,
            "auditor_fn_violation": 0.023028604915210783,
            "auditor_fp_violation": 0.03074480369515013,
            "ave_precision_score": 0.754998241940091,
            "fpr": 0.19188596491228072,
            "logloss": 2.9888284033870662,
            "mae": 0.2874109863597313,
            "precision": 0.6929824561403509,
            "recall": 0.824634655532359
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7813573970765401,
            "auditor_fn_violation": 0.015321509041539087,
            "auditor_fp_violation": 0.035508917511757415,
            "ave_precision_score": 0.7501845141320457,
            "fpr": 0.19978046103183314,
            "logloss": 3.5525111371425413,
            "mae": 0.29960146048013886,
            "precision": 0.6840277777777778,
            "recall": 0.8294736842105264
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 9675,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.6478612941983277,
            "auditor_fn_violation": 0.02877431051532799,
            "auditor_fp_violation": 0.02643227583971476,
            "ave_precision_score": 0.620915899280708,
            "fpr": 0.18530701754385964,
            "logloss": 3.6091315534171486,
            "mae": 0.34313209128799943,
            "precision": 0.6858736059479554,
            "recall": 0.7703549060542797
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.656012924665695,
            "auditor_fn_violation": 0.03306256860592756,
            "auditor_fp_violation": 0.028336136315572164,
            "ave_precision_score": 0.6294114001159322,
            "fpr": 0.17233809001097694,
            "logloss": 3.9146580444241406,
            "mae": 0.3575138286833417,
            "precision": 0.6847389558232931,
            "recall": 0.7178947368421053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.6529672587928146,
            "auditor_fn_violation": 0.011935501593231512,
            "auditor_fp_violation": 0.030185162675742474,
            "ave_precision_score": 0.6500064418373391,
            "fpr": 0.21600877192982457,
            "logloss": 2.219947243758698,
            "mae": 0.3682608315116446,
            "precision": 0.6431159420289855,
            "recall": 0.7411273486430062
        },
        "train": {
            "accuracy": 0.6092206366630076,
            "auc_prc": 0.6524599671674962,
            "auditor_fn_violation": 0.019388757293893354,
            "auditor_fp_violation": 0.029275219287203297,
            "ave_precision_score": 0.648462898958786,
            "fpr": 0.21514818880351264,
            "logloss": 2.72497885100847,
            "mae": 0.4055280001321275,
            "precision": 0.6164383561643836,
            "recall": 0.6631578947368421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.6414810027305801,
            "auditor_fn_violation": 0.004026572171556257,
            "auditor_fp_violation": 0.029473582918034116,
            "ave_precision_score": 0.6390947832087392,
            "fpr": 0.22916666666666666,
            "logloss": 3.161674286418333,
            "mae": 0.3832977229411987,
            "precision": 0.62,
            "recall": 0.7118997912317327
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.6083897383257109,
            "auditor_fn_violation": 0.008434918250621072,
            "auditor_fp_violation": 0.027510347536228975,
            "ave_precision_score": 0.6028556242398982,
            "fpr": 0.24259055982436883,
            "logloss": 3.7378471125938524,
            "mae": 0.4247952684213995,
            "precision": 0.5798479087452472,
            "recall": 0.6421052631578947
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6652814879007709,
            "auditor_fn_violation": 0.022046569973995536,
            "auditor_fp_violation": 0.03352528260605324,
            "ave_precision_score": 0.6607932073237627,
            "fpr": 0.21271929824561403,
            "logloss": 2.2398624592010417,
            "mae": 0.35246488426304684,
            "precision": 0.6541889483065954,
            "recall": 0.7661795407098121
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6642948513852013,
            "auditor_fn_violation": 0.022330579467329144,
            "auditor_fp_violation": 0.03405371655303679,
            "ave_precision_score": 0.6598005915421228,
            "fpr": 0.21185510428100987,
            "logloss": 2.644083088004186,
            "mae": 0.38945303976007273,
            "precision": 0.6295585412667947,
            "recall": 0.6905263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6651280164964815,
            "auditor_fn_violation": 0.02576182104530638,
            "auditor_fp_violation": 0.0316488391880394,
            "ave_precision_score": 0.6606395110644124,
            "fpr": 0.2138157894736842,
            "logloss": 2.2415679619380917,
            "mae": 0.3528315071235517,
            "precision": 0.6536412078152753,
            "recall": 0.7682672233820459
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.6642347266648275,
            "auditor_fn_violation": 0.02168582818187071,
            "auditor_fp_violation": 0.03405371655303679,
            "ave_precision_score": 0.6597804022256416,
            "fpr": 0.21185510428100987,
            "logloss": 2.645742191231968,
            "mae": 0.38981264918571656,
            "precision": 0.6302681992337165,
            "recall": 0.6926315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.595263370105124,
            "auditor_fn_violation": 0.09764265831593599,
            "auditor_fp_violation": 0.04748845265588915,
            "ave_precision_score": 0.5964689890431063,
            "fpr": 0.09320175438596491,
            "logloss": 11.565324730121253,
            "mae": 0.4535584531935492,
            "precision": 0.6502057613168725,
            "recall": 0.3298538622129436
        },
        "train": {
            "accuracy": 0.5598243688254665,
            "auc_prc": 0.6112093515691488,
            "auditor_fn_violation": 0.08895256802819344,
            "auditor_fp_violation": 0.043255722615534904,
            "ave_precision_score": 0.6138406384415016,
            "fpr": 0.0867178924259056,
            "logloss": 11.053286197561416,
            "mae": 0.44859581107766666,
            "precision": 0.6594827586206896,
            "recall": 0.32210526315789473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5789473684210527,
            "auc_prc": 0.6031710162675264,
            "auditor_fn_violation": 0.03833827784492548,
            "auditor_fp_violation": 0.041238705887119656,
            "ave_precision_score": 0.6020383858561698,
            "fpr": 0.19846491228070176,
            "logloss": 4.320018648278355,
            "mae": 0.41761741151944537,
            "precision": 0.6039387308533917,
            "recall": 0.5762004175365344
        },
        "train": {
            "accuracy": 0.5631174533479693,
            "auc_prc": 0.6026514410640826,
            "auditor_fn_violation": 0.030968860130567922,
            "auditor_fp_violation": 0.051621869303819776,
            "ave_precision_score": 0.599356234575841,
            "fpr": 0.19319429198682767,
            "logloss": 4.836914028973197,
            "mae": 0.44282608610103186,
            "precision": 0.5897435897435898,
            "recall": 0.5326315789473685
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.631356556958007,
            "auditor_fn_violation": 0.04065716221660623,
            "auditor_fp_violation": 0.04407236335642803,
            "ave_precision_score": 0.6306292204931335,
            "fpr": 0.19517543859649122,
            "logloss": 3.551898494707401,
            "mae": 0.38006622336104295,
            "precision": 0.6396761133603239,
            "recall": 0.6597077244258872
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.6321077559124824,
            "auditor_fn_violation": 0.03185163787624935,
            "auditor_fp_violation": 0.042800028197665645,
            "ave_precision_score": 0.6284966185028351,
            "fpr": 0.1986827661909989,
            "logloss": 4.14938803425656,
            "mae": 0.40112809035922925,
            "precision": 0.6221294363256785,
            "recall": 0.6273684210526316
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6381578947368421,
            "auc_prc": 0.637537321700951,
            "auditor_fn_violation": 0.04083342489836282,
            "auditor_fp_violation": 0.049410477695393216,
            "ave_precision_score": 0.6369059457080033,
            "fpr": 0.18530701754385964,
            "logloss": 3.276870873480506,
            "mae": 0.3742042385125623,
            "precision": 0.6529774127310062,
            "recall": 0.6638830897703549
        },
        "train": {
            "accuracy": 0.6125137211855104,
            "auc_prc": 0.641001971384768,
            "auditor_fn_violation": 0.03341845282800856,
            "auditor_fp_violation": 0.05024471545534195,
            "ave_precision_score": 0.6379494700908004,
            "fpr": 0.2030735455543359,
            "logloss": 3.717150870202574,
            "mae": 0.39697471497140496,
            "precision": 0.6239837398373984,
            "recall": 0.6463157894736842
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.6283817159257687,
            "auditor_fn_violation": 0.03206607332527562,
            "auditor_fp_violation": 0.053327964831246716,
            "ave_precision_score": 0.6285722652598151,
            "fpr": 0.22587719298245615,
            "logloss": 3.7018290424734865,
            "mae": 0.38702971648258405,
            "precision": 0.6240875912408759,
            "recall": 0.7139874739039666
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.6476458814257273,
            "auditor_fn_violation": 0.029836501242128385,
            "auditor_fp_violation": 0.05145570448846414,
            "ave_precision_score": 0.6442761467938309,
            "fpr": 0.2349066959385291,
            "logloss": 3.599081548571218,
            "mae": 0.41205659377042636,
            "precision": 0.5908221797323135,
            "recall": 0.6505263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.6738159286379566,
            "auditor_fn_violation": 0.03184860638025126,
            "auditor_fp_violation": 0.04282646570236215,
            "ave_precision_score": 0.6678570603382786,
            "fpr": 0.2149122807017544,
            "logloss": 2.3457896665011417,
            "mae": 0.34792161358446244,
            "precision": 0.6537102473498233,
            "recall": 0.7724425887265136
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.6820249981502315,
            "auditor_fn_violation": 0.03200878155872668,
            "auditor_fp_violation": 0.03481404646572474,
            "ave_precision_score": 0.675376973561543,
            "fpr": 0.21075740944017562,
            "logloss": 2.6837761987017017,
            "mae": 0.3718608598777555,
            "precision": 0.6404494382022472,
            "recall": 0.72
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5942982456140351,
            "auc_prc": 0.6624878748482514,
            "auditor_fn_violation": 0.07096747610152732,
            "auditor_fp_violation": 0.07128712369839148,
            "ave_precision_score": 0.568081820063475,
            "fpr": 0.24671052631578946,
            "logloss": 9.162129808175147,
            "mae": 0.41120857076920164,
            "precision": 0.5974955277280859,
            "recall": 0.697286012526096
        },
        "train": {
            "accuracy": 0.6048298572996706,
            "auc_prc": 0.6804422206811027,
            "auditor_fn_violation": 0.06320873533999653,
            "auditor_fp_violation": 0.07009637559290628,
            "ave_precision_score": 0.5909649289825849,
            "fpr": 0.24478594950603733,
            "logloss": 8.770497343916466,
            "mae": 0.3991247772455704,
            "precision": 0.6024955436720143,
            "recall": 0.7115789473684211
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 9675,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.6909473398596141,
            "auditor_fn_violation": 0.02047851884408307,
            "auditor_fp_violation": 0.026371500344394484,
            "ave_precision_score": 0.6818746450114578,
            "fpr": 0.23464912280701755,
            "logloss": 2.5571457485769575,
            "mae": 0.31973420232682764,
            "precision": 0.6624605678233438,
            "recall": 0.8768267223382046
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.7034435578173353,
            "auditor_fn_violation": 0.028431451845860534,
            "auditor_fp_violation": 0.03652101229619634,
            "ave_precision_score": 0.6955469610881275,
            "fpr": 0.24368825466520308,
            "logloss": 2.8842424018589905,
            "mae": 0.35604875134435954,
            "precision": 0.6287625418060201,
            "recall": 0.791578947368421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 9675,
        "test": {
            "accuracy": 0.555921052631579,
            "auc_prc": 0.5943543305701572,
            "auditor_fn_violation": 0.08951855107497346,
            "auditor_fp_violation": 0.040040922166849,
            "ave_precision_score": 0.5962569961291478,
            "fpr": 0.07785087719298246,
            "logloss": 11.556899530059642,
            "mae": 0.45169367783156217,
            "precision": 0.6712962962962963,
            "recall": 0.302713987473904
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.610187265453188,
            "auditor_fn_violation": 0.0766283436362586,
            "auditor_fp_violation": 0.033610610378755075,
            "ave_precision_score": 0.6128199061273956,
            "fpr": 0.06695938529088913,
            "logloss": 11.091518893260877,
            "mae": 0.4490224841759515,
            "precision": 0.6822916666666666,
            "recall": 0.27578947368421053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6652336174708287,
            "auditor_fn_violation": 0.022046569973995536,
            "auditor_fp_violation": 0.03352528260605324,
            "ave_precision_score": 0.6607543645793431,
            "fpr": 0.21271929824561403,
            "logloss": 2.2395721118963485,
            "mae": 0.3525186974791774,
            "precision": 0.6541889483065954,
            "recall": 0.7661795407098121
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6642109216845207,
            "auditor_fn_violation": 0.022330579467329144,
            "auditor_fp_violation": 0.03405371655303679,
            "ave_precision_score": 0.6597343017266997,
            "fpr": 0.21185510428100987,
            "logloss": 2.6435853884127534,
            "mae": 0.3895327986845925,
            "precision": 0.6295585412667947,
            "recall": 0.6905263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.6254086639375576,
            "auditor_fn_violation": 0.0400917481595429,
            "auditor_fp_violation": 0.046936408573396544,
            "ave_precision_score": 0.6247169849166168,
            "fpr": 0.19188596491228072,
            "logloss": 3.708782328526292,
            "mae": 0.38611109892123424,
            "precision": 0.6391752577319587,
            "recall": 0.6471816283924844
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.6270855170959234,
            "auditor_fn_violation": 0.0339823213357213,
            "auditor_fp_violation": 0.04747781951479875,
            "ave_precision_score": 0.6234891782111478,
            "fpr": 0.19978046103183314,
            "logloss": 4.323017141398719,
            "mae": 0.40404614382399007,
            "precision": 0.6216216216216216,
            "recall": 0.6294736842105263
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.6651779494256927,
            "auditor_fn_violation": 0.024630992931179728,
            "auditor_fp_violation": 0.03273266885458449,
            "ave_precision_score": 0.6592304239894726,
            "fpr": 0.21710526315789475,
            "logloss": 2.320568670306482,
            "mae": 0.3534219014720979,
            "precision": 0.6507936507936508,
            "recall": 0.7703549060542797
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6700270148547816,
            "auditor_fn_violation": 0.027548674100179107,
            "auditor_fp_violation": 0.03700943614739323,
            "ave_precision_score": 0.6635765531819773,
            "fpr": 0.21514818880351264,
            "logloss": 2.6808780139033264,
            "mae": 0.38742979947202794,
            "precision": 0.6280834914611005,
            "recall": 0.6968421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6293859649122807,
            "auc_prc": 0.6909919622273643,
            "auditor_fn_violation": 0.008355308940409478,
            "auditor_fp_violation": 0.027070418540577783,
            "ave_precision_score": 0.6901273190183016,
            "fpr": 0.24342105263157895,
            "logloss": 3.2792921524101732,
            "mae": 0.3658625366841619,
            "precision": 0.6205128205128205,
            "recall": 0.7578288100208769
        },
        "train": {
            "accuracy": 0.5905598243688255,
            "auc_prc": 0.6943640901141124,
            "auditor_fn_violation": 0.010373793980010407,
            "auditor_fp_violation": 0.030692655515161288,
            "ave_precision_score": 0.6911626339451962,
            "fpr": 0.24039517014270034,
            "logloss": 3.871416351158404,
            "mae": 0.4027511711937411,
            "precision": 0.5944444444444444,
            "recall": 0.6757894736842105
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.6384812479279723,
            "auditor_fn_violation": 0.04933294876020951,
            "auditor_fp_violation": 0.0501043312669665,
            "ave_precision_score": 0.6376462807523514,
            "fpr": 0.14912280701754385,
            "logloss": 3.60837761898083,
            "mae": 0.3937241454242775,
            "precision": 0.6591478696741855,
            "recall": 0.5490605427974948
        },
        "train": {
            "accuracy": 0.605927552140505,
            "auc_prc": 0.638190158715873,
            "auditor_fn_violation": 0.036813218556820154,
            "auditor_fp_violation": 0.04326579321040494,
            "ave_precision_score": 0.6344786070679038,
            "fpr": 0.145993413830955,
            "logloss": 4.298498968123458,
            "mae": 0.40376870940024623,
            "precision": 0.6518324607329843,
            "recall": 0.5242105263157895
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 9675,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7176720430208798,
            "auditor_fn_violation": 0.02749468922828993,
            "auditor_fp_violation": 0.04050686763097121,
            "ave_precision_score": 0.7106643152488277,
            "fpr": 0.2324561403508772,
            "logloss": 1.7495827666031911,
            "mae": 0.31223535315561873,
            "precision": 0.6640253565768621,
            "recall": 0.8747390396659708
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.7278607882511448,
            "auditor_fn_violation": 0.02824195505228494,
            "auditor_fp_violation": 0.04920996183244544,
            "ave_precision_score": 0.719117537613613,
            "fpr": 0.22941822173435786,
            "logloss": 1.8008019496473786,
            "mae": 0.3228542135395187,
            "precision": 0.6579378068739771,
            "recall": 0.8463157894736842
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5789473684210527,
            "auc_prc": 0.6122632617697837,
            "auditor_fn_violation": 0.067357524814123,
            "auditor_fp_violation": 0.06778240346825494,
            "ave_precision_score": 0.6116197792345068,
            "fpr": 0.1611842105263158,
            "logloss": 3.5698045181379765,
            "mae": 0.4259423368902791,
            "precision": 0.622107969151671,
            "recall": 0.5052192066805845
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.6089019799122404,
            "auditor_fn_violation": 0.05470217805765787,
            "auditor_fp_violation": 0.06264917068651246,
            "ave_precision_score": 0.6059208051945775,
            "fpr": 0.1712403951701427,
            "logloss": 4.038242592872431,
            "mae": 0.4394752218242762,
            "precision": 0.6050632911392405,
            "recall": 0.5031578947368421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.6629363861281448,
            "auditor_fn_violation": 0.02293246163425265,
            "auditor_fp_violation": 0.03475092176167903,
            "ave_precision_score": 0.6530451410231259,
            "fpr": 0.2543859649122807,
            "logloss": 2.761024404161704,
            "mae": 0.3584691180636369,
            "precision": 0.6293929712460063,
            "recall": 0.8225469728601252
        },
        "train": {
            "accuracy": 0.6114160263446762,
            "auc_prc": 0.6700640664148653,
            "auditor_fn_violation": 0.028646368941013346,
            "auditor_fp_violation": 0.035101058419520846,
            "ave_precision_score": 0.6615047666153449,
            "fpr": 0.2601536772777168,
            "logloss": 3.132834744703998,
            "mae": 0.38618959214652504,
            "precision": 0.6016806722689075,
            "recall": 0.7536842105263157
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.66765008631247,
            "auditor_fn_violation": 0.01748663150569535,
            "auditor_fp_violation": 0.035763846683683806,
            "ave_precision_score": 0.6653130037707158,
            "fpr": 0.2149122807017544,
            "logloss": 2.219353087575512,
            "mae": 0.35964037476123917,
            "precision": 0.6481149012567325,
            "recall": 0.7536534446764092
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.6675857184647404,
            "auditor_fn_violation": 0.015501762089086605,
            "auditor_fp_violation": 0.029003313225712247,
            "ave_precision_score": 0.6637019183327224,
            "fpr": 0.21295279912184412,
            "logloss": 2.655616294066179,
            "mae": 0.3939464808367786,
            "precision": 0.624031007751938,
            "recall": 0.6778947368421052
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 9675,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7966404724267176,
            "auditor_fn_violation": 0.016213877595868585,
            "auditor_fp_violation": 0.02085865645638345,
            "ave_precision_score": 0.7611707727921578,
            "fpr": 0.19298245614035087,
            "logloss": 3.649670480394864,
            "mae": 0.2790914620564839,
            "precision": 0.6944444444444444,
            "recall": 0.8350730688935282
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7918512893307812,
            "auditor_fn_violation": 0.00889710555202496,
            "auditor_fp_violation": 0.02569008751346943,
            "ave_precision_score": 0.7521252825123814,
            "fpr": 0.20197585071350166,
            "logloss": 4.262161892232355,
            "mae": 0.3018542483329938,
            "precision": 0.6766256590509666,
            "recall": 0.8105263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.6662089761800377,
            "auditor_fn_violation": 0.02290270299967037,
            "auditor_fp_violation": 0.03606265953567522,
            "ave_precision_score": 0.6602615637048452,
            "fpr": 0.2225877192982456,
            "logloss": 2.3139760009732466,
            "mae": 0.35190800181857185,
            "precision": 0.6469565217391304,
            "recall": 0.7766179540709812
        },
        "train": {
            "accuracy": 0.6344676180021954,
            "auc_prc": 0.6719007550828169,
            "auditor_fn_violation": 0.02651799641804842,
            "auditor_fp_violation": 0.03414435190686714,
            "ave_precision_score": 0.6652649577831193,
            "fpr": 0.21734357848518113,
            "logloss": 2.686008516135624,
            "mae": 0.38388680941369435,
            "precision": 0.6319702602230484,
            "recall": 0.7157894736842105
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.6350255446263399,
            "auditor_fn_violation": 0.044631084496209214,
            "auditor_fp_violation": 0.04733144929297841,
            "ave_precision_score": 0.6343031944680947,
            "fpr": 0.17763157894736842,
            "logloss": 3.4887406794715567,
            "mae": 0.38034878227422786,
            "precision": 0.6523605150214592,
            "recall": 0.6346555323590815
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6360570988157153,
            "auditor_fn_violation": 0.033420763764515575,
            "auditor_fp_violation": 0.045748194845869554,
            "ave_precision_score": 0.6324699719378171,
            "fpr": 0.1877058177826564,
            "logloss": 4.073629701478851,
            "mae": 0.3985362711679956,
            "precision": 0.6346153846153846,
            "recall": 0.6252631578947369
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.6196780191110571,
            "auditor_fn_violation": 0.03196306266710619,
            "auditor_fp_violation": 0.039635752198047075,
            "ave_precision_score": 0.6168361758054944,
            "fpr": 0.20833333333333334,
            "logloss": 3.425994957487366,
            "mae": 0.37978102832445243,
            "precision": 0.6360153256704981,
            "recall": 0.6931106471816284
        },
        "train": {
            "accuracy": 0.6344676180021954,
            "auc_prc": 0.6511969140334817,
            "auditor_fn_violation": 0.02441273326015368,
            "auditor_fp_violation": 0.031823079789323166,
            "ave_precision_score": 0.6448855491370016,
            "fpr": 0.1986827661909989,
            "logloss": 3.836701790570999,
            "mae": 0.38406830996989627,
            "precision": 0.6408730158730159,
            "recall": 0.68
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.6672436151817592,
            "auditor_fn_violation": 0.025535197597333633,
            "auditor_fp_violation": 0.03117276447469714,
            "ave_precision_score": 0.6635869756452207,
            "fpr": 0.17105263157894737,
            "logloss": 2.1575490143360283,
            "mae": 0.3462530783810822,
            "precision": 0.688622754491018,
            "recall": 0.7202505219206681
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.6678740110799217,
            "auditor_fn_violation": 0.02270957305448033,
            "auditor_fp_violation": 0.029038560307757384,
            "ave_precision_score": 0.6638683631776442,
            "fpr": 0.17453347969264543,
            "logloss": 2.630362060790532,
            "mae": 0.38169713271942124,
            "precision": 0.6466666666666666,
            "recall": 0.6126315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6648832723246676,
            "auditor_fn_violation": 0.022046569973995536,
            "auditor_fp_violation": 0.03352528260605324,
            "ave_precision_score": 0.6603953616392394,
            "fpr": 0.21271929824561403,
            "logloss": 2.2460444505523918,
            "mae": 0.35266877881859154,
            "precision": 0.6541889483065954,
            "recall": 0.7661795407098121
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.6639269792500763,
            "auditor_fn_violation": 0.022330579467329144,
            "auditor_fp_violation": 0.033615645676190097,
            "ave_precision_score": 0.6594330923711283,
            "fpr": 0.21075740944017562,
            "logloss": 2.6526870511219283,
            "mae": 0.38979902115382464,
            "precision": 0.6307692307692307,
            "recall": 0.6905263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6502192982456141,
            "auc_prc": 0.6412433674523673,
            "auditor_fn_violation": 0.011047320807237306,
            "auditor_fp_violation": 0.03583221911591913,
            "ave_precision_score": 0.6397597496437823,
            "fpr": 0.23574561403508773,
            "logloss": 3.071461503611913,
            "mae": 0.36517982717009095,
            "precision": 0.635593220338983,
            "recall": 0.7828810020876826
        },
        "train": {
            "accuracy": 0.6125137211855104,
            "auc_prc": 0.6472506786913476,
            "auditor_fn_violation": 0.018334970246692478,
            "auditor_fp_violation": 0.032933362873744966,
            "ave_precision_score": 0.6440291765416054,
            "fpr": 0.24039517014270034,
            "logloss": 3.4553063298511537,
            "mae": 0.3986839681465842,
            "precision": 0.6089285714285714,
            "recall": 0.7178947368421053
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5964912280701754,
            "auc_prc": 0.6774828888076713,
            "auditor_fn_violation": 0.004891861700179469,
            "auditor_fp_violation": 0.024259551882014507,
            "ave_precision_score": 0.6777889604570867,
            "fpr": 0.34100877192982454,
            "logloss": 3.036625215850262,
            "mae": 0.40099692957911365,
            "precision": 0.5757162346521146,
            "recall": 0.8810020876826722
        },
        "train": {
            "accuracy": 0.5587266739846323,
            "auc_prc": 0.6752162532323396,
            "auditor_fn_violation": 0.009511814662892139,
            "auditor_fp_violation": 0.016148198874107494,
            "ave_precision_score": 0.6719422727291904,
            "fpr": 0.34577387486278816,
            "logloss": 3.3004999152252834,
            "mae": 0.43895424915514253,
            "precision": 0.5519203413940256,
            "recall": 0.8168421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5942982456140351,
            "auc_prc": 0.6881709348505363,
            "auditor_fn_violation": 0.005475588763139583,
            "auditor_fp_violation": 0.014274644463352386,
            "ave_precision_score": 0.666015892826448,
            "fpr": 0.3442982456140351,
            "logloss": 5.657538608945773,
            "mae": 0.40683222092143184,
            "precision": 0.5739484396200815,
            "recall": 0.8830897703549061
        },
        "train": {
            "accuracy": 0.5466520307354555,
            "auc_prc": 0.7100123463714787,
            "auditor_fn_violation": 0.012968975677393265,
            "auditor_fp_violation": 0.01610539884590982,
            "ave_precision_score": 0.6922198758238921,
            "fpr": 0.3611416026344676,
            "logloss": 5.8247729568165365,
            "mae": 0.4518422094162519,
            "precision": 0.5430555555555555,
            "recall": 0.8231578947368421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6652210610245877,
            "auditor_fn_violation": 0.022046569973995536,
            "auditor_fp_violation": 0.03352528260605324,
            "ave_precision_score": 0.6607328458778866,
            "fpr": 0.21271929824561403,
            "logloss": 2.239983800358,
            "mae": 0.3525343590871154,
            "precision": 0.6541889483065954,
            "recall": 0.7661795407098121
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6642389551805751,
            "auditor_fn_violation": 0.022330579467329144,
            "auditor_fp_violation": 0.03405371655303679,
            "ave_precision_score": 0.659744694278773,
            "fpr": 0.21185510428100987,
            "logloss": 2.644331128342042,
            "mae": 0.389530416872327,
            "precision": 0.6295585412667947,
            "recall": 0.6905263157894737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6381578947368421,
            "auc_prc": 0.6578212261625498,
            "auditor_fn_violation": 0.006759788301651838,
            "auditor_fp_violation": 0.037113569142255186,
            "ave_precision_score": 0.6570821890349254,
            "fpr": 0.19298245614035087,
            "logloss": 3.0356123553073027,
            "mae": 0.37760600638706265,
            "precision": 0.6487025948103793,
            "recall": 0.6784968684759917
        },
        "train": {
            "accuracy": 0.5894621295279913,
            "auc_prc": 0.646938126092083,
            "auditor_fn_violation": 0.011758044947715068,
            "auditor_fp_violation": 0.02858034824117061,
            "ave_precision_score": 0.642913384789297,
            "fpr": 0.19538968166849616,
            "logloss": 3.7741667209777785,
            "mae": 0.4181322645638612,
            "precision": 0.6105032822757112,
            "recall": 0.5873684210526315
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6293859649122807,
            "auc_prc": 0.6336898076437064,
            "auditor_fn_violation": 0.0417742555763103,
            "auditor_fp_violation": 0.046483124670799396,
            "ave_precision_score": 0.6329307167100251,
            "fpr": 0.18201754385964913,
            "logloss": 3.5628255822316306,
            "mae": 0.38147722260131145,
            "precision": 0.6490486257928119,
            "recall": 0.6409185803757829
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.6338740421763975,
            "auditor_fn_violation": 0.032260673637991795,
            "auditor_fp_violation": 0.04412431142307577,
            "ave_precision_score": 0.6302735817724903,
            "fpr": 0.19099890230515917,
            "logloss": 4.185810120638011,
            "mae": 0.40054183895953427,
            "precision": 0.6313559322033898,
            "recall": 0.6273684210526316
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.7467443315874749,
            "auditor_fn_violation": 0.07107506501117095,
            "auditor_fp_violation": 0.08316366840889754,
            "ave_precision_score": 0.7482683124188274,
            "fpr": 0.19078947368421054,
            "logloss": 1.4904427239120572,
            "mae": 0.365109182659155,
            "precision": 0.6419753086419753,
            "recall": 0.651356993736952
        },
        "train": {
            "accuracy": 0.6256860592755215,
            "auc_prc": 0.7590762441197396,
            "auditor_fn_violation": 0.06704257900514184,
            "auditor_fp_violation": 0.08595756251321766,
            "ave_precision_score": 0.7595281389216537,
            "fpr": 0.20856201975850713,
            "logloss": 1.4134240026932485,
            "mae": 0.36227271103590164,
            "precision": 0.6303501945525292,
            "recall": 0.6821052631578948
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6337719298245614,
            "auc_prc": 0.7555484194449424,
            "auditor_fn_violation": 0.05536021682598982,
            "auditor_fp_violation": 0.07588073821968316,
            "ave_precision_score": 0.7587369146709426,
            "fpr": 0.21710526315789475,
            "logloss": 1.270291664987101,
            "mae": 0.3505225263556549,
            "precision": 0.634011090573013,
            "recall": 0.7160751565762005
        },
        "train": {
            "accuracy": 0.6377607025246982,
            "auc_prc": 0.7743760855227357,
            "auditor_fn_violation": 0.05656017100930152,
            "auditor_fp_violation": 0.08003353508091723,
            "ave_precision_score": 0.7748264405229127,
            "fpr": 0.22502744237102085,
            "logloss": 1.2320887261158564,
            "mae": 0.3473720167940205,
            "precision": 0.6306306306306306,
            "recall": 0.7368421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6567982456140351,
            "auc_prc": 0.6950007064616108,
            "auditor_fn_violation": 0.036026260850455995,
            "auditor_fp_violation": 0.07992484097078724,
            "ave_precision_score": 0.6964867146229874,
            "fpr": 0.24013157894736842,
            "logloss": 1.4498168888838674,
            "mae": 0.3652412691468332,
            "precision": 0.6374172185430463,
            "recall": 0.8037578288100209
        },
        "train": {
            "accuracy": 0.6542261251372119,
            "auc_prc": 0.7126059717346906,
            "auditor_fn_violation": 0.03962331734935583,
            "auditor_fp_violation": 0.08404918478534529,
            "ave_precision_score": 0.7130599596365617,
            "fpr": 0.2502744237102086,
            "logloss": 1.464090487295543,
            "mae": 0.36859062839656637,
            "precision": 0.6298701298701299,
            "recall": 0.8168421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6634060905317672,
            "auditor_fn_violation": 0.02576182104530638,
            "auditor_fp_violation": 0.0316488391880394,
            "ave_precision_score": 0.658783930611022,
            "fpr": 0.2138157894736842,
            "logloss": 2.3327607975130507,
            "mae": 0.3528366466937646,
            "precision": 0.6536412078152753,
            "recall": 0.7682672233820459
        },
        "train": {
            "accuracy": 0.6256860592755215,
            "auc_prc": 0.6632065123663989,
            "auditor_fn_violation": 0.02252238719741176,
            "auditor_fp_violation": 0.03251795083535585,
            "ave_precision_score": 0.6581801703299323,
            "fpr": 0.21075740944017562,
            "logloss": 2.7425737362578064,
            "mae": 0.39042831701974834,
            "precision": 0.6293436293436293,
            "recall": 0.6863157894736842
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6096491228070176,
            "auc_prc": 0.6817771575526466,
            "auditor_fn_violation": 0.006897135845877744,
            "auditor_fp_violation": 0.03400388963170051,
            "ave_precision_score": 0.6820192981212844,
            "fpr": 0.2993421052631579,
            "logloss": 2.5060243897952943,
            "mae": 0.3809794836698422,
            "precision": 0.5919282511210763,
            "recall": 0.826722338204593
        },
        "train": {
            "accuracy": 0.5675082327113062,
            "auc_prc": 0.6788133938620508,
            "auditor_fn_violation": 0.012758680455254497,
            "auditor_fp_violation": 0.024159357093223498,
            "ave_precision_score": 0.675553463240207,
            "fpr": 0.30954994511525796,
            "logloss": 2.7986286181901754,
            "mae": 0.42115931111594707,
            "precision": 0.5627906976744186,
            "recall": 0.7642105263157895
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.7233029609425394,
            "auditor_fn_violation": 0.02521471999413984,
            "auditor_fp_violation": 0.05996009075807301,
            "ave_precision_score": 0.7205748568815219,
            "fpr": 0.25548245614035087,
            "logloss": 1.8357789151344077,
            "mae": 0.34490849364914167,
            "precision": 0.6387596899224807,
            "recall": 0.860125260960334
        },
        "train": {
            "accuracy": 0.6586169045005489,
            "auc_prc": 0.7580780723569751,
            "auditor_fn_violation": 0.024114622450748163,
            "auditor_fp_violation": 0.06340446530176538,
            "ave_precision_score": 0.7550737118325219,
            "fpr": 0.2810098792535675,
            "logloss": 1.6968561882641329,
            "mae": 0.345171927897571,
            "precision": 0.621301775147929,
            "recall": 0.8842105263157894
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5460526315789473,
            "auc_prc": 0.6172131095269148,
            "auditor_fn_violation": 0.0125604329194594,
            "auditor_fp_violation": 0.013081925367691748,
            "ave_precision_score": 0.6182705041484201,
            "fpr": 0.06578947368421052,
            "logloss": 8.418674593290136,
            "mae": 0.4628367085527138,
            "precision": 0.6756756756756757,
            "recall": 0.2609603340292276
        },
        "train": {
            "accuracy": 0.5257958287596048,
            "auc_prc": 0.6135332471708462,
            "auditor_fn_violation": 0.016361430469697853,
            "auditor_fp_violation": 0.024421192559844512,
            "ave_precision_score": 0.6139873250041207,
            "fpr": 0.07683863885839737,
            "logloss": 9.208052627847362,
            "mae": 0.47314205401661885,
            "precision": 0.6174863387978142,
            "recall": 0.23789473684210527
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6107456140350878,
            "auc_prc": 0.6881514357465333,
            "auditor_fn_violation": 0.05415613668827602,
            "auditor_fp_violation": 0.03937239171832584,
            "ave_precision_score": 0.6173564750062771,
            "fpr": 0.17763157894736842,
            "logloss": 7.792989848625674,
            "mae": 0.38999039690763687,
            "precision": 0.6383928571428571,
            "recall": 0.5970772442588727
        },
        "train": {
            "accuracy": 0.6289791437980241,
            "auc_prc": 0.7256510426202636,
            "auditor_fn_violation": 0.04538679299786239,
            "auditor_fp_violation": 0.029703219569179956,
            "ave_precision_score": 0.6630954040119676,
            "fpr": 0.1690450054884742,
            "logloss": 6.942383048120595,
            "mae": 0.36820100975013315,
            "precision": 0.6539325842696629,
            "recall": 0.6126315789473684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.6263696535991559,
            "auditor_fn_violation": 0.0414698018532762,
            "auditor_fp_violation": 0.048075949110651925,
            "ave_precision_score": 0.6257071334195181,
            "fpr": 0.19078947368421054,
            "logloss": 3.608637242360961,
            "mae": 0.38464203467786495,
            "precision": 0.6419753086419753,
            "recall": 0.651356993736952
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.6265887912030242,
            "auditor_fn_violation": 0.03534115200184875,
            "auditor_fp_violation": 0.047462713622493685,
            "ave_precision_score": 0.6230200301164781,
            "fpr": 0.19758507135016465,
            "logloss": 4.204077137745374,
            "mae": 0.4055633037214183,
            "precision": 0.6226415094339622,
            "recall": 0.6252631578947369
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.691479988225157,
            "auditor_fn_violation": 0.0074854411603120525,
            "auditor_fp_violation": 0.027070418540577783,
            "ave_precision_score": 0.6906150805431263,
            "fpr": 0.24342105263157895,
            "logloss": 3.2652091684196227,
            "mae": 0.36466100493909864,
            "precision": 0.621160409556314,
            "recall": 0.7599164926931107
        },
        "train": {
            "accuracy": 0.5905598243688255,
            "auc_prc": 0.6951347885294595,
            "auditor_fn_violation": 0.00960194118666591,
            "auditor_fp_violation": 0.03018660812294182,
            "ave_precision_score": 0.6919327711784196,
            "fpr": 0.23929747530186607,
            "logloss": 3.853750138952043,
            "mae": 0.40156282820984096,
            "precision": 0.5947955390334573,
            "recall": 0.6736842105263158
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.6280786974054797,
            "auditor_fn_violation": 0.0755022341867194,
            "auditor_fp_violation": 0.045677849357805604,
            "ave_precision_score": 0.627298122970656,
            "fpr": 0.10964912280701754,
            "logloss": 3.866504618155107,
            "mae": 0.42403101089339773,
            "precision": 0.6563573883161512,
            "recall": 0.3987473903966597
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.636415157556762,
            "auditor_fn_violation": 0.06590790918019529,
            "auditor_fp_violation": 0.04299388714891389,
            "ave_precision_score": 0.6330536605717997,
            "fpr": 0.10647639956092206,
            "logloss": 4.396449209527372,
            "mae": 0.42792555305039476,
            "precision": 0.6498194945848376,
            "recall": 0.37894736842105264
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6513157894736842,
            "auc_prc": 0.6816256334311634,
            "auditor_fn_violation": 0.009362524264732823,
            "auditor_fp_violation": 0.04099053928122848,
            "ave_precision_score": 0.6818593296616919,
            "fpr": 0.19407894736842105,
            "logloss": 2.4097309378126397,
            "mae": 0.3634625228071812,
            "precision": 0.6563106796116505,
            "recall": 0.7056367432150313
        },
        "train": {
            "accuracy": 0.6114160263446762,
            "auc_prc": 0.6861590120752618,
            "auditor_fn_violation": 0.011323588884395403,
            "auditor_fp_violation": 0.035604588163022796,
            "ave_precision_score": 0.6833170262422019,
            "fpr": 0.1964873765093304,
            "logloss": 2.641086401341513,
            "mae": 0.3979338290707251,
            "precision": 0.6263048016701461,
            "recall": 0.631578947368421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.6313974411216449,
            "auditor_fn_violation": 0.03826273669560122,
            "auditor_fp_violation": 0.042887241197682426,
            "ave_precision_score": 0.6306862108429196,
            "fpr": 0.19298245614035087,
            "logloss": 3.601638203314869,
            "mae": 0.3804232119224347,
            "precision": 0.6437246963562753,
            "recall": 0.6638830897703549
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.6318138453766544,
            "auditor_fn_violation": 0.032884626494887055,
            "auditor_fp_violation": 0.042800028197665645,
            "ave_precision_score": 0.6282026263630663,
            "fpr": 0.1986827661909989,
            "logloss": 4.225189454487628,
            "mae": 0.40122152726263044,
            "precision": 0.6229166666666667,
            "recall": 0.6294736842105263
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.7626230567345453,
            "auditor_fn_violation": 0.011164066219829322,
            "auditor_fp_violation": 0.027774401361371104,
            "ave_precision_score": 0.763029301605207,
            "fpr": 0.31359649122807015,
            "logloss": 1.4264426206416654,
            "mae": 0.3409102335063029,
            "precision": 0.6150740242261103,
            "recall": 0.954070981210856
        },
        "train": {
            "accuracy": 0.6509330406147091,
            "auc_prc": 0.7884466749484413,
            "auditor_fn_violation": 0.007697729504881855,
            "auditor_fp_violation": 0.034652916947804126,
            "ave_precision_score": 0.7899408730291875,
            "fpr": 0.3205268935236004,
            "logloss": 1.3711230874194358,
            "mae": 0.3464426675422,
            "precision": 0.6059379217273954,
            "recall": 0.9452631578947368
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5372807017543859,
            "auc_prc": 0.6063451624287761,
            "auditor_fn_violation": 0.11004971981100979,
            "auditor_fp_violation": 0.08263441513715004,
            "ave_precision_score": 0.6067891459363899,
            "fpr": 0.20614035087719298,
            "logloss": 4.449428971353328,
            "mae": 0.4676116794203679,
            "precision": 0.5658198614318707,
            "recall": 0.511482254697286
        },
        "train": {
            "accuracy": 0.5323819978046103,
            "auc_prc": 0.6128568919248992,
            "auditor_fn_violation": 0.10154023918192849,
            "auditor_fp_violation": 0.0925311433146356,
            "ave_precision_score": 0.6097166213540901,
            "fpr": 0.19209659714599342,
            "logloss": 4.778113179210246,
            "mae": 0.47184415803024465,
            "precision": 0.5614035087719298,
            "recall": 0.47157894736842104
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.6614371740813539,
            "auditor_fn_violation": 0.02138959088744827,
            "auditor_fp_violation": 0.033383473116972574,
            "ave_precision_score": 0.6568285324318713,
            "fpr": 0.21929824561403508,
            "logloss": 2.3595482543256847,
            "mae": 0.3551095695658585,
            "precision": 0.647887323943662,
            "recall": 0.7682672233820459
        },
        "train": {
            "accuracy": 0.6212952799121844,
            "auc_prc": 0.6598194843628312,
            "auditor_fn_violation": 0.02142469235657751,
            "auditor_fp_violation": 0.03427023434274263,
            "ave_precision_score": 0.6554354461724969,
            "fpr": 0.21514818880351264,
            "logloss": 2.749689780625933,
            "mae": 0.3938213306702483,
            "precision": 0.6245210727969349,
            "recall": 0.6863157894736842
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6206140350877193,
            "auc_prc": 0.7530255038169708,
            "auditor_fn_violation": 0.016037614914112017,
            "auditor_fp_violation": 0.007966654511567604,
            "ave_precision_score": 0.7544032285821713,
            "fpr": 0.03179824561403509,
            "logloss": 2.236257811299224,
            "mae": 0.3859926853824295,
            "precision": 0.8481675392670157,
            "recall": 0.33820459290187893
        },
        "train": {
            "accuracy": 0.6201975850713501,
            "auc_prc": 0.694197378011177,
            "auditor_fn_violation": 0.001617655554913646,
            "auditor_fp_violation": 0.0033837198763330962,
            "ave_precision_score": 0.6955760936946673,
            "fpr": 0.03402854006586169,
            "logloss": 3.182561181667705,
            "mae": 0.4083926323762141,
            "precision": 0.837696335078534,
            "recall": 0.3368421052631579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 9675,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.633345577214647,
            "auditor_fn_violation": 0.034728326557521165,
            "auditor_fp_violation": 0.04172997447429198,
            "ave_precision_score": 0.6325977501323748,
            "fpr": 0.2050438596491228,
            "logloss": 3.6155990613584663,
            "mae": 0.3836745289752739,
            "precision": 0.6318897637795275,
            "recall": 0.6701461377870563
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.6347418927105065,
            "auditor_fn_violation": 0.02567681552949333,
            "auditor_fp_violation": 0.037505412944742654,
            "ave_precision_score": 0.6311131385962272,
            "fpr": 0.2030735455543359,
            "logloss": 4.256466680170046,
            "mae": 0.401471281215222,
            "precision": 0.6201232032854209,
            "recall": 0.6357894736842106
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5997807017543859,
            "auc_prc": 0.618515275262107,
            "auditor_fn_violation": 0.020627312016994477,
            "auditor_fp_violation": 0.0275844779384952,
            "ave_precision_score": 0.6183157981062886,
            "fpr": 0.22478070175438597,
            "logloss": 4.277254137526432,
            "mae": 0.4004175167320126,
            "precision": 0.6087786259541985,
            "recall": 0.6659707724425887
        },
        "train": {
            "accuracy": 0.5675082327113062,
            "auc_prc": 0.6190204057627262,
            "auditor_fn_violation": 0.018018371945230805,
            "auditor_fp_violation": 0.03594698838860411,
            "ave_precision_score": 0.6159074225295594,
            "fpr": 0.21734357848518113,
            "logloss": 4.775495080365126,
            "mae": 0.4354903479433837,
            "precision": 0.5849056603773585,
            "recall": 0.5873684210526315
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 9675,
        "test": {
            "accuracy": 0.5822368421052632,
            "auc_prc": 0.6570695849344224,
            "auditor_fn_violation": 0.01068563894077574,
            "auditor_fp_violation": 0.044320529962319194,
            "ave_precision_score": 0.659412023703088,
            "fpr": 0.3059210526315789,
            "logloss": 3.54206206312321,
            "mae": 0.4256312038791411,
            "precision": 0.5746951219512195,
            "recall": 0.7870563674321504
        },
        "train": {
            "accuracy": 0.5817782656421515,
            "auc_prc": 0.6758730082123358,
            "auditor_fn_violation": 0.013981165867467796,
            "auditor_fp_violation": 0.05723622594386651,
            "ave_precision_score": 0.6777299585159484,
            "fpr": 0.29637760702524696,
            "logloss": 3.7698019534370673,
            "mae": 0.4255408087970281,
            "precision": 0.5741324921135647,
            "recall": 0.7663157894736842
        }
    }
]
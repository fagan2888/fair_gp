[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7335634473808839,
            "auditor_fn_violation": 0.006377082874550396,
            "auditor_fp_violation": 0.032207534966448376,
            "ave_precision_score": 0.6857754924026536,
            "fpr": 0.21600877192982457,
            "logloss": 3.451202321802682,
            "mae": 0.3098059544266302,
            "precision": 0.6786296900489397,
            "recall": 0.8702928870292888
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7337095025646051,
            "auditor_fn_violation": 0.006932081284764182,
            "auditor_fp_violation": 0.03312010295620576,
            "ave_precision_score": 0.6866953296747986,
            "fpr": 0.20636663007683864,
            "logloss": 3.486645329726216,
            "mae": 0.3175384181549252,
            "precision": 0.6769759450171822,
            "recall": 0.8277310924369747
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7338716235484767,
            "auditor_fn_violation": 0.006138515745430526,
            "auditor_fp_violation": 0.03272293637319105,
            "ave_precision_score": 0.6874037658138207,
            "fpr": 0.21271929824561403,
            "logloss": 3.3938721358600956,
            "mae": 0.3085470751572294,
            "precision": 0.680921052631579,
            "recall": 0.8661087866108786
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7354269261388655,
            "auditor_fn_violation": 0.007743822007397912,
            "auditor_fp_violation": 0.03374086831447066,
            "ave_precision_score": 0.6896808357640891,
            "fpr": 0.20087815587266739,
            "logloss": 3.4188490970133754,
            "mae": 0.31624092501502327,
            "precision": 0.6833910034602076,
            "recall": 0.8298319327731093
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7336226744700566,
            "auditor_fn_violation": 0.006377082874550396,
            "auditor_fp_violation": 0.03151528013582343,
            "ave_precision_score": 0.6858263102110922,
            "fpr": 0.2149122807017544,
            "logloss": 3.4478857522286517,
            "mae": 0.309025923361528,
            "precision": 0.6797385620915033,
            "recall": 0.8702928870292888
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7330680540181773,
            "auditor_fn_violation": 0.00788449298489978,
            "auditor_fp_violation": 0.03308982171921723,
            "ave_precision_score": 0.6861699239189738,
            "fpr": 0.20417124039517015,
            "logloss": 3.4847107176006893,
            "mae": 0.3176217253735659,
            "precision": 0.6798623063683304,
            "recall": 0.8298319327731093
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.731439031635702,
            "auditor_fn_violation": 0.006734933568230198,
            "auditor_fp_violation": 0.0326117713638936,
            "ave_precision_score": 0.6836119681616755,
            "fpr": 0.2149122807017544,
            "logloss": 3.461477568880496,
            "mae": 0.3104719695167453,
            "precision": 0.6807817589576547,
            "recall": 0.8744769874476988
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7319682762065091,
            "auditor_fn_violation": 0.004416146260919297,
            "auditor_fp_violation": 0.03116696317044551,
            "ave_precision_score": 0.6840931417158106,
            "fpr": 0.2074643249176729,
            "logloss": 3.5184183257615307,
            "mae": 0.3182380801897408,
            "precision": 0.6774744027303754,
            "recall": 0.8340336134453782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.7890855382697003,
            "auditor_fn_violation": 0.011919180797181243,
            "auditor_fp_violation": 0.015846066779852865,
            "ave_precision_score": 0.7648282091436613,
            "fpr": 0.16885964912280702,
            "logloss": 1.9471636941916401,
            "mae": 0.28600933246498006,
            "precision": 0.7225225225225225,
            "recall": 0.8389121338912134
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8126033285039219,
            "auditor_fn_violation": 0.02171867649364905,
            "auditor_fp_violation": 0.01983925710031922,
            "ave_precision_score": 0.7952696696121281,
            "fpr": 0.1525795828759605,
            "logloss": 1.6272271369510678,
            "mae": 0.27762530274285274,
            "precision": 0.7337164750957854,
            "recall": 0.8046218487394958
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7296735781658832,
            "auditor_fn_violation": 0.006030701754385966,
            "auditor_fp_violation": 0.033743633276740244,
            "ave_precision_score": 0.6823829521744483,
            "fpr": 0.23026315789473684,
            "logloss": 3.380124915194676,
            "mae": 0.3081988482798758,
            "precision": 0.6713615023474179,
            "recall": 0.897489539748954
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7281768475036294,
            "auditor_fn_violation": 0.008758497910690074,
            "auditor_fp_violation": 0.029150737474292497,
            "ave_precision_score": 0.6797357646859309,
            "fpr": 0.21514818880351264,
            "logloss": 3.442212901881751,
            "mae": 0.31857492442220087,
            "precision": 0.6765676567656765,
            "recall": 0.8613445378151261
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7114326084519239,
            "auditor_fn_violation": 0.01177925200029362,
            "auditor_fp_violation": 0.03169466003718976,
            "ave_precision_score": 0.6884977980779325,
            "fpr": 0.15679824561403508,
            "logloss": 2.215078047940381,
            "mae": 0.32000207792539703,
            "precision": 0.7228682170542635,
            "recall": 0.7803347280334728
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.6998205907983777,
            "auditor_fn_violation": 0.013746552408010407,
            "auditor_fp_violation": 0.03732162458836444,
            "ave_precision_score": 0.6767492433841967,
            "fpr": 0.14818880351262348,
            "logloss": 2.3309100145464874,
            "mae": 0.324143161902794,
            "precision": 0.734251968503937,
            "recall": 0.7836134453781513
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7356113255996366,
            "auditor_fn_violation": 0.004177218674300819,
            "auditor_fp_violation": 0.02802874120785837,
            "ave_precision_score": 0.69222037190848,
            "fpr": 0.23903508771929824,
            "logloss": 3.3223143255048706,
            "mae": 0.3238767363887233,
            "precision": 0.6599063962558502,
            "recall": 0.8849372384937239
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7336372475488465,
            "auditor_fn_violation": 0.003551365661522571,
            "auditor_fp_violation": 0.02312729474999054,
            "ave_precision_score": 0.6898132650424496,
            "fpr": 0.23600439077936333,
            "logloss": 3.409504195639966,
            "mae": 0.3308684601278375,
            "precision": 0.6565495207667732,
            "recall": 0.8634453781512605
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8532843522711279,
            "auditor_fn_violation": 0.021998642002495784,
            "auditor_fp_violation": 0.02174791818255316,
            "ave_precision_score": 0.8525125301245657,
            "fpr": 0.13157894736842105,
            "logloss": 0.6673417219167821,
            "mae": 0.291758069454533,
            "precision": 0.7609561752988048,
            "recall": 0.799163179916318
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8742003488828518,
            "auditor_fn_violation": 0.0222629117508694,
            "auditor_fp_violation": 0.01981906960899353,
            "ave_precision_score": 0.8743625574053024,
            "fpr": 0.1251372118551043,
            "logloss": 0.7394839618578224,
            "mae": 0.2850978530645128,
            "precision": 0.7696969696969697,
            "recall": 0.8004201680672269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7334971487148939,
            "auditor_fn_violation": 0.006377082874550396,
            "auditor_fp_violation": 0.032207534966448376,
            "ave_precision_score": 0.6857069321799156,
            "fpr": 0.21600877192982457,
            "logloss": 3.451439806977686,
            "mae": 0.3098650862071759,
            "precision": 0.6786296900489397,
            "recall": 0.8702928870292888
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7336570622206352,
            "auditor_fn_violation": 0.006932081284764182,
            "auditor_fp_violation": 0.033344688797204036,
            "ave_precision_score": 0.686624260215646,
            "fpr": 0.2052689352360044,
            "logloss": 3.4872235289279634,
            "mae": 0.3175104315991769,
            "precision": 0.6781411359724613,
            "recall": 0.8277310924369747
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 29756,
        "test": {
            "accuracy": 0.5625,
            "auc_prc": 0.7574919115457013,
            "auditor_fn_violation": 0.008326910372164723,
            "auditor_fp_violation": 0.022500808472794894,
            "ave_precision_score": 0.543672766186155,
            "fpr": 0.4232456140350877,
            "logloss": 14.331079505526063,
            "mae": 0.4401344541198195,
            "precision": 0.5464159811985899,
            "recall": 0.9728033472803347
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7590351880128774,
            "auditor_fn_violation": 0.003413000765619091,
            "auditor_fp_violation": 0.007709098249996854,
            "ave_precision_score": 0.54294709261097,
            "fpr": 0.4226125137211855,
            "logloss": 14.419533220409033,
            "mae": 0.43364020432759504,
            "precision": 0.5497076023391813,
            "recall": 0.9873949579831933
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.7887239042183181,
            "auditor_fn_violation": 0.007755725611098879,
            "auditor_fp_violation": 0.014092691405934195,
            "ave_precision_score": 0.7667521698027477,
            "fpr": 0.15679824561403508,
            "logloss": 1.840611434533656,
            "mae": 0.2696152444709867,
            "precision": 0.7351851851851852,
            "recall": 0.8305439330543933
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8082337956278811,
            "auditor_fn_violation": 0.017830622918761356,
            "auditor_fp_violation": 0.022468677845489993,
            "ave_precision_score": 0.7945411662790685,
            "fpr": 0.13830954994511527,
            "logloss": 1.595475550136127,
            "mae": 0.27268592268154423,
            "precision": 0.7504950495049505,
            "recall": 0.7962184873949579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 29756,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.6637968297909488,
            "auditor_fn_violation": 0.008946267341995157,
            "auditor_fp_violation": 0.0182007437949713,
            "ave_precision_score": 0.6447005096603093,
            "fpr": 0.24342105263157895,
            "logloss": 2.4748221715445538,
            "mae": 0.34405325150337857,
            "precision": 0.6563467492260062,
            "recall": 0.8870292887029289
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.6628897426834843,
            "auditor_fn_violation": 0.0074440313996070465,
            "auditor_fp_violation": 0.014807524887391657,
            "ave_precision_score": 0.6415185368506122,
            "fpr": 0.24368825466520308,
            "logloss": 2.7047675053095097,
            "mae": 0.3511359992137528,
            "precision": 0.6525821596244131,
            "recall": 0.8760504201680672
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 29756,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.7199524258331257,
            "auditor_fn_violation": 0.002940798649343022,
            "auditor_fp_violation": 0.022354272778720995,
            "ave_precision_score": 0.6727779352690524,
            "fpr": 0.20833333333333334,
            "logloss": 3.496812728585566,
            "mae": 0.35209383178432097,
            "precision": 0.6607142857142857,
            "recall": 0.7740585774058577
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7310259841618431,
            "auditor_fn_violation": 0.010040679279395628,
            "auditor_fp_violation": 0.028143886344423844,
            "ave_precision_score": 0.6859137607190573,
            "fpr": 0.2052689352360044,
            "logloss": 3.2974033896936445,
            "mae": 0.34526182972846325,
            "precision": 0.6648745519713262,
            "recall": 0.7794117647058824
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 29756,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.6935340116558939,
            "auditor_fn_violation": 0.003271122366585922,
            "auditor_fp_violation": 0.017533753739186673,
            "ave_precision_score": 0.6725024236503923,
            "fpr": 0.24890350877192982,
            "logloss": 2.383226208217259,
            "mae": 0.3432587725682217,
            "precision": 0.650231124807396,
            "recall": 0.8828451882845189
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.6888647256474665,
            "auditor_fn_violation": 0.007231871892555046,
            "auditor_fp_violation": 0.022869904235588025,
            "ave_precision_score": 0.6679545069119746,
            "fpr": 0.24368825466520308,
            "logloss": 2.472098046009103,
            "mae": 0.3430321599793084,
            "precision": 0.653125,
            "recall": 0.8781512605042017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7350662058722535,
            "auditor_fn_violation": 0.005571918813770832,
            "auditor_fp_violation": 0.0311110437383782,
            "ave_precision_score": 0.6868273510808918,
            "fpr": 0.21600877192982457,
            "logloss": 3.4718292502643084,
            "mae": 0.31047751906629467,
            "precision": 0.6781045751633987,
            "recall": 0.8682008368200836
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7326889211526705,
            "auditor_fn_violation": 0.010008394137018147,
            "auditor_fp_violation": 0.031873525366844575,
            "ave_precision_score": 0.6834381626955288,
            "fpr": 0.20636663007683864,
            "logloss": 3.5873625155964097,
            "mae": 0.31733990538351364,
            "precision": 0.6758620689655173,
            "recall": 0.8235294117647058
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.7818108160975888,
            "auditor_fn_violation": 0.016364787491741906,
            "auditor_fp_violation": 0.01845339154337457,
            "ave_precision_score": 0.7549494398961213,
            "fpr": 0.17763157894736842,
            "logloss": 2.052843778364977,
            "mae": 0.2895991516088623,
            "precision": 0.7127659574468085,
            "recall": 0.8410041841004184
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.814754828465083,
            "auditor_fn_violation": 0.017876744550729184,
            "auditor_fp_violation": 0.025701199894015668,
            "ave_precision_score": 0.7973246893417631,
            "fpr": 0.150384193194292,
            "logloss": 1.614294282427315,
            "mae": 0.27400123808892934,
            "precision": 0.740530303030303,
            "recall": 0.8214285714285714
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 29756,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.7225341423881337,
            "auditor_fn_violation": 0.0017204360273067622,
            "auditor_fp_violation": 0.026697287573773147,
            "ave_precision_score": 0.6748895716359011,
            "fpr": 0.22697368421052633,
            "logloss": 3.475000226689121,
            "mae": 0.338203652622147,
            "precision": 0.6526845637583892,
            "recall": 0.8138075313807531
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7315227519385694,
            "auditor_fn_violation": 0.013458292208211497,
            "auditor_fp_violation": 0.029334948332639395,
            "ave_precision_score": 0.684729817870983,
            "fpr": 0.21734357848518113,
            "logloss": 3.3982706505575493,
            "mae": 0.34063699254789664,
            "precision": 0.657439446366782,
            "recall": 0.7983193277310925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 29756,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7278285460379195,
            "auditor_fn_violation": 0.0026402958232401106,
            "auditor_fp_violation": 0.02418849543212872,
            "ave_precision_score": 0.6788592102388578,
            "fpr": 0.23903508771929824,
            "logloss": 3.5392305449119266,
            "mae": 0.32618521588035615,
            "precision": 0.6588419405320813,
            "recall": 0.8807531380753139
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7252151600755024,
            "auditor_fn_violation": 0.004563735483216342,
            "auditor_fp_violation": 0.027578636587304587,
            "ave_precision_score": 0.6764561612023756,
            "fpr": 0.2327113062568606,
            "logloss": 3.603412391986131,
            "mae": 0.3345009201866931,
            "precision": 0.6564019448946515,
            "recall": 0.8508403361344538
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.707998279102741,
            "auditor_fn_violation": 0.012075166996990388,
            "auditor_fp_violation": 0.02959010429299055,
            "ave_precision_score": 0.6850148628781523,
            "fpr": 0.18859649122807018,
            "logloss": 2.306353629182465,
            "mae": 0.322843142168335,
            "precision": 0.696113074204947,
            "recall": 0.8242677824267782
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.7029476589278456,
            "auditor_fn_violation": 0.014069403831785185,
            "auditor_fp_violation": 0.033316930996631224,
            "ave_precision_score": 0.6806683570937864,
            "fpr": 0.1778265642151482,
            "logloss": 2.3849604784915117,
            "mae": 0.32302743828649827,
            "precision": 0.7049180327868853,
            "recall": 0.8130252100840336
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.853913087143505,
            "auditor_fn_violation": 0.016346436174117303,
            "auditor_fp_violation": 0.018150214245290654,
            "ave_precision_score": 0.8532011471916039,
            "fpr": 0.12390350877192982,
            "logloss": 0.652392504455878,
            "mae": 0.2925969295269042,
            "precision": 0.771255060728745,
            "recall": 0.797071129707113
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8733045976668534,
            "auditor_fn_violation": 0.014818880351262349,
            "auditor_fp_violation": 0.013611416026344676,
            "ave_precision_score": 0.873468870780619,
            "fpr": 0.1350164654226125,
            "logloss": 0.7421649877996984,
            "mae": 0.29061163093998993,
            "precision": 0.7525150905432596,
            "recall": 0.7857142857142857
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.7323250850933392,
            "auditor_fn_violation": 0.015694964398443808,
            "auditor_fp_violation": 0.011899708949793843,
            "ave_precision_score": 0.6901607760404618,
            "fpr": 0.17543859649122806,
            "logloss": 3.6861172859300533,
            "mae": 0.3026367590299947,
            "precision": 0.7101449275362319,
            "recall": 0.8200836820083682
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.723145483839966,
            "auditor_fn_violation": 0.02729939396175595,
            "auditor_fp_violation": 0.02493155178722384,
            "ave_precision_score": 0.6779084067194613,
            "fpr": 0.1800219538968167,
            "logloss": 4.003390579987474,
            "mae": 0.32377809625475096,
            "precision": 0.690566037735849,
            "recall": 0.7689075630252101
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.728599163704202,
            "auditor_fn_violation": 0.004964031417455775,
            "auditor_fp_violation": 0.033819427601261214,
            "ave_precision_score": 0.6794161131934745,
            "fpr": 0.21271929824561403,
            "logloss": 3.5412619792155104,
            "mae": 0.3091165302683836,
            "precision": 0.6845528455284553,
            "recall": 0.8807531380753139
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7256064401910419,
            "auditor_fn_violation": 0.007651578743462264,
            "auditor_fp_violation": 0.030785924271673173,
            "ave_precision_score": 0.6760504614372655,
            "fpr": 0.20856201975850713,
            "logloss": 3.661988849223043,
            "mae": 0.31852739168911487,
            "precision": 0.6774193548387096,
            "recall": 0.8382352941176471
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.737124735277393,
            "auditor_fn_violation": 0.010781399104455702,
            "auditor_fp_violation": 0.030577956989247312,
            "ave_precision_score": 0.6901941063966404,
            "fpr": 0.19188596491228072,
            "logloss": 3.334242697168476,
            "mae": 0.30177902116834626,
            "precision": 0.697754749568221,
            "recall": 0.8451882845188284
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7358848611413719,
            "auditor_fn_violation": 0.00929350884151685,
            "auditor_fp_violation": 0.035565312843029655,
            "ave_precision_score": 0.6879339757206067,
            "fpr": 0.19099890230515917,
            "logloss": 3.4316341589608195,
            "mae": 0.3142037734449016,
            "precision": 0.6876122082585279,
            "recall": 0.8046218487394958
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7351109801097576,
            "auditor_fn_violation": 0.005581094472583131,
            "auditor_fp_violation": 0.03052490096208263,
            "ave_precision_score": 0.6844676391157484,
            "fpr": 0.22807017543859648,
            "logloss": 3.6151442670166527,
            "mae": 0.3112984742560575,
            "precision": 0.673469387755102,
            "recall": 0.897489539748954
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.729788937282384,
            "auditor_fn_violation": 0.010149065114520016,
            "auditor_fp_violation": 0.029322331150560838,
            "ave_precision_score": 0.676885029999762,
            "fpr": 0.22502744237102085,
            "logloss": 3.7843877950115083,
            "mae": 0.3246171534292579,
            "precision": 0.6639344262295082,
            "recall": 0.8508403361344538
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 29756,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.7220428025242575,
            "auditor_fn_violation": 0.0056751449754092334,
            "auditor_fp_violation": 0.026697287573773147,
            "ave_precision_score": 0.6749551995027384,
            "fpr": 0.22697368421052633,
            "logloss": 3.4521381730941454,
            "mae": 0.3393763094694643,
            "precision": 0.6521008403361345,
            "recall": 0.8117154811715481
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7299524624567502,
            "auditor_fn_violation": 0.01304780968369785,
            "auditor_fp_violation": 0.029443456098514956,
            "ave_precision_score": 0.6834446793672109,
            "fpr": 0.21624588364434688,
            "logloss": 3.393308994485301,
            "mae": 0.3406624795044498,
            "precision": 0.6591695501730104,
            "recall": 0.8004201680672269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7353479871290793,
            "auditor_fn_violation": 0.007980529252000294,
            "auditor_fp_violation": 0.030734598593257348,
            "ave_precision_score": 0.6887770595172624,
            "fpr": 0.23135964912280702,
            "logloss": 3.39563660850612,
            "mae": 0.3099803792388351,
            "precision": 0.6682389937106918,
            "recall": 0.8891213389121339
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7347691183707975,
            "auditor_fn_violation": 0.010727891595716226,
            "auditor_fp_violation": 0.030205533896059657,
            "ave_precision_score": 0.688914633311131,
            "fpr": 0.21844127332601537,
            "logloss": 3.428585509365989,
            "mae": 0.31942544017555086,
            "precision": 0.6737704918032786,
            "recall": 0.8634453781512605
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7347451337501361,
            "auditor_fn_violation": 0.0052071863759818,
            "auditor_fp_violation": 0.032207534966448376,
            "ave_precision_score": 0.6867373936286354,
            "fpr": 0.21600877192982457,
            "logloss": 3.462687248353856,
            "mae": 0.3091995957232997,
            "precision": 0.6796747967479675,
            "recall": 0.8744769874476988
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7317181596494788,
            "auditor_fn_violation": 0.009540259572544714,
            "auditor_fp_violation": 0.029054846890495485,
            "ave_precision_score": 0.6837237281911475,
            "fpr": 0.21075740944017562,
            "logloss": 3.5447422691269415,
            "mae": 0.3203783836778452,
            "precision": 0.6740237691001698,
            "recall": 0.8340336134453782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7323231234668489,
            "auditor_fn_violation": 0.006734933568230198,
            "auditor_fp_violation": 0.0326117713638936,
            "ave_precision_score": 0.6843063546243432,
            "fpr": 0.2149122807017544,
            "logloss": 3.4525546502511135,
            "mae": 0.3097138301767642,
            "precision": 0.6807817589576547,
            "recall": 0.8744769874476988
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7314724510709268,
            "auditor_fn_violation": 0.004123273897923606,
            "auditor_fp_violation": 0.03040488537290082,
            "ave_precision_score": 0.6824978238905572,
            "fpr": 0.20965971459934138,
            "logloss": 3.549025348525279,
            "mae": 0.31778532038805063,
            "precision": 0.6757215619694398,
            "recall": 0.8361344537815126
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7324076516216345,
            "auditor_fn_violation": 0.005491631799163181,
            "auditor_fp_violation": 0.036002304147465435,
            "ave_precision_score": 0.6843775480117763,
            "fpr": 0.22587719298245615,
            "logloss": 3.503496807597962,
            "mae": 0.3080966920840056,
            "precision": 0.6766091051805337,
            "recall": 0.9016736401673641
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.728417326445541,
            "auditor_fn_violation": 0.008486380282079901,
            "auditor_fp_violation": 0.03236307203149249,
            "ave_precision_score": 0.6803669800427454,
            "fpr": 0.22502744237102085,
            "logloss": 3.5890420933494256,
            "mae": 0.32221350067634696,
            "precision": 0.6666666666666666,
            "recall": 0.8613445378151261
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7316518871903062,
            "auditor_fn_violation": 0.006381670703956545,
            "auditor_fp_violation": 0.0326117713638936,
            "ave_precision_score": 0.6837816824163674,
            "fpr": 0.2149122807017544,
            "logloss": 3.4545372658968385,
            "mae": 0.3096740345675917,
            "precision": 0.6813008130081301,
            "recall": 0.8765690376569037
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.731576078790193,
            "auditor_fn_violation": 0.007042773201486966,
            "auditor_fp_violation": 0.03193156440440592,
            "ave_precision_score": 0.6831864489551133,
            "fpr": 0.2074643249176729,
            "logloss": 3.5320411393795794,
            "mae": 0.31756173377654173,
            "precision": 0.6763698630136986,
            "recall": 0.8298319327731093
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7371166232400312,
            "auditor_fn_violation": 0.010781399104455702,
            "auditor_fp_violation": 0.030577956989247312,
            "ave_precision_score": 0.6901800157569306,
            "fpr": 0.19188596491228072,
            "logloss": 3.3345197583529855,
            "mae": 0.3018268875412944,
            "precision": 0.697754749568221,
            "recall": 0.8451882845188284
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7353396065551963,
            "auditor_fn_violation": 0.00929350884151685,
            "auditor_fp_violation": 0.03580756273893788,
            "ave_precision_score": 0.6874914590674172,
            "fpr": 0.19209659714599342,
            "logloss": 3.431543814063421,
            "mae": 0.31421709150179544,
            "precision": 0.6863799283154122,
            "recall": 0.8046218487394958
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 29756,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.7511247866596271,
            "auditor_fn_violation": 0.0030509065550906557,
            "auditor_fp_violation": 0.030067608537472725,
            "ave_precision_score": 0.6940722170801547,
            "fpr": 0.2817982456140351,
            "logloss": 4.175273999613068,
            "mae": 0.3329639163119192,
            "precision": 0.6275362318840579,
            "recall": 0.9058577405857741
        },
        "train": {
            "accuracy": 0.6641053787047201,
            "auc_prc": 0.747248864535732,
            "auditor_fn_violation": 0.012789528544678028,
            "auditor_fp_violation": 0.03409414941267019,
            "ave_precision_score": 0.6924359440396222,
            "fpr": 0.2678375411635565,
            "logloss": 3.979508437028899,
            "mae": 0.34273067951386355,
            "precision": 0.6291793313069909,
            "recall": 0.8697478991596639
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7363359956786804,
            "auditor_fn_violation": 0.005771489392938417,
            "auditor_fp_violation": 0.02714700056593096,
            "ave_precision_score": 0.6863006922998978,
            "fpr": 0.20723684210526316,
            "logloss": 3.479071578711957,
            "mae": 0.3060041609069689,
            "precision": 0.6839464882943144,
            "recall": 0.8556485355648535
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7355254676265195,
            "auditor_fn_violation": 0.005645287752861849,
            "auditor_fp_violation": 0.03395031353697466,
            "ave_precision_score": 0.6832967627536266,
            "fpr": 0.1986827661909989,
            "logloss": 3.5906184154764054,
            "mae": 0.31402138111869826,
            "precision": 0.6835664335664335,
            "recall": 0.8214285714285714
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.7366184315420488,
            "auditor_fn_violation": 0.008611355795346108,
            "auditor_fp_violation": 0.0308710283773951,
            "ave_precision_score": 0.6898015635336346,
            "fpr": 0.19407894736842105,
            "logloss": 3.33397003140326,
            "mae": 0.3008779960505827,
            "precision": 0.6974358974358974,
            "recall": 0.8535564853556485
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7357006640742845,
            "auditor_fn_violation": 0.00903983986569381,
            "auditor_fp_violation": 0.03580756273893788,
            "ave_precision_score": 0.6877050454518117,
            "fpr": 0.19209659714599342,
            "logloss": 3.434332032789416,
            "mae": 0.3141141653020283,
            "precision": 0.6869409660107334,
            "recall": 0.8067226890756303
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7339257821522102,
            "auditor_fn_violation": 0.008698524554062982,
            "auditor_fp_violation": 0.03461779448621553,
            "ave_precision_score": 0.6864256495913167,
            "fpr": 0.18859649122807018,
            "logloss": 3.3694416680012695,
            "mae": 0.3018851101279066,
            "precision": 0.7003484320557491,
            "recall": 0.8410041841004184
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7308688956919543,
            "auditor_fn_violation": 0.005008809231705855,
            "auditor_fp_violation": 0.035678867481736626,
            "ave_precision_score": 0.6817971903044725,
            "fpr": 0.18880351262349068,
            "logloss": 3.493196697644018,
            "mae": 0.31614930245424244,
            "precision": 0.6838235294117647,
            "recall": 0.7815126050420168
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7331445341010012,
            "auditor_fn_violation": 0.007221243485282245,
            "auditor_fp_violation": 0.030615854151507804,
            "ave_precision_score": 0.6856963701048148,
            "fpr": 0.2050438596491228,
            "logloss": 3.3952937857351952,
            "mae": 0.30742512249774573,
            "precision": 0.6857142857142857,
            "recall": 0.8535564853556485
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7305300853133423,
            "auditor_fn_violation": 0.008103570736746949,
            "auditor_fp_violation": 0.03560821126209673,
            "ave_precision_score": 0.6823539969053942,
            "fpr": 0.19538968166849616,
            "logloss": 3.4978019064829793,
            "mae": 0.3165535508159454,
            "precision": 0.6860670194003528,
            "recall": 0.8172268907563025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7301155861910664,
            "auditor_fn_violation": 0.008001174484327977,
            "auditor_fp_violation": 0.03151528013582343,
            "ave_precision_score": 0.68456603221224,
            "fpr": 0.19956140350877194,
            "logloss": 3.338175806577174,
            "mae": 0.30898377978449215,
            "precision": 0.6856649395509499,
            "recall": 0.8305439330543933
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7370015897037201,
            "auditor_fn_violation": 0.008716988441919031,
            "auditor_fp_violation": 0.03647374995268557,
            "ave_precision_score": 0.6926003108356233,
            "fpr": 0.18441273326015367,
            "logloss": 3.3066085144759256,
            "mae": 0.3136432943613876,
            "precision": 0.6934306569343066,
            "recall": 0.7983193277310925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 29756,
        "test": {
            "accuracy": 0.5515350877192983,
            "auc_prc": 0.7589994102572077,
            "auditor_fn_violation": 0.0011331938633193863,
            "auditor_fp_violation": 0.004105525911553082,
            "ave_precision_score": 0.5382936325710842,
            "fpr": 0.44627192982456143,
            "logloss": 14.935988094625952,
            "mae": 0.4517268920482876,
            "precision": 0.5390713476783692,
            "recall": 0.99581589958159
        },
        "train": {
            "accuracy": 0.5543358946212953,
            "auc_prc": 0.7630574880961842,
            "auditor_fn_violation": 0.0012083867575570294,
            "auditor_fp_violation": 0.009152503879783497,
            "ave_precision_score": 0.5396939227476192,
            "fpr": 0.44127332601536773,
            "logloss": 14.99038395335959,
            "mae": 0.45066908702794073,
            "precision": 0.540045766590389,
            "recall": 0.9915966386554622
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.732145913852686,
            "auditor_fn_violation": 0.005950414739778319,
            "auditor_fp_violation": 0.03301600776133884,
            "ave_precision_score": 0.6840053861755265,
            "fpr": 0.2138157894736842,
            "logloss": 3.4814323206632762,
            "mae": 0.3084840761015621,
            "precision": 0.6808510638297872,
            "recall": 0.8702928870292888
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7300587579281003,
            "auditor_fn_violation": 0.006498537944266624,
            "auditor_fp_violation": 0.03122247877159116,
            "ave_precision_score": 0.6822562732421872,
            "fpr": 0.20636663007683864,
            "logloss": 3.5531904109612005,
            "mae": 0.3191615341894316,
            "precision": 0.6764199655765921,
            "recall": 0.8256302521008403
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 29756,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.7884630675189217,
            "auditor_fn_violation": 0.002718288923144682,
            "auditor_fp_violation": 0.008857830059018522,
            "ave_precision_score": 0.7012736939211056,
            "fpr": 0.42105263157894735,
            "logloss": 8.039445646569689,
            "mae": 0.4317352745530648,
            "precision": 0.5519253208868145,
            "recall": 0.9895397489539749
        },
        "train": {
            "accuracy": 0.5740944017563118,
            "auc_prc": 0.7793610540936613,
            "auditor_fn_violation": 0.0009408812921436414,
            "auditor_fp_violation": 0.010363753359324728,
            "ave_precision_score": 0.6893880398925536,
            "fpr": 0.42041712403951703,
            "logloss": 8.155147482981372,
            "mae": 0.4321225839661911,
            "precision": 0.5515222482435597,
            "recall": 0.9894957983193278
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.853415987046132,
            "auditor_fn_violation": 0.029341462967041036,
            "auditor_fp_violation": 0.023662988115449916,
            "ave_precision_score": 0.8526642714436437,
            "fpr": 0.10087719298245613,
            "logloss": 0.9554176180148448,
            "mae": 0.3068402007331382,
            "precision": 0.7894736842105263,
            "recall": 0.7217573221757322
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8626546061024615,
            "auditor_fn_violation": 0.04307529817635068,
            "auditor_fp_violation": 0.01430536104066518,
            "ave_precision_score": 0.8628197317694918,
            "fpr": 0.09769484083424808,
            "logloss": 0.9512982098752633,
            "mae": 0.3092245800413763,
            "precision": 0.7949308755760369,
            "recall": 0.7247899159663865
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.736127721943507,
            "auditor_fn_violation": 0.005418226528664759,
            "auditor_fp_violation": 0.029610316112862807,
            "ave_precision_score": 0.6885116909639659,
            "fpr": 0.21710526315789475,
            "logloss": 3.4508794387033195,
            "mae": 0.30787142982457993,
            "precision": 0.6796116504854369,
            "recall": 0.8786610878661087
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7362563552126051,
            "auditor_fn_violation": 0.006553883902628012,
            "auditor_fp_violation": 0.0322469939563698,
            "ave_precision_score": 0.6887906657315553,
            "fpr": 0.2052689352360044,
            "logloss": 3.5087326461683324,
            "mae": 0.3173996067783482,
            "precision": 0.6781411359724613,
            "recall": 0.8277310924369747
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7365328621878984,
            "auditor_fn_violation": 0.00757909417896205,
            "auditor_fp_violation": 0.02536583393968794,
            "ave_precision_score": 0.6841840609682248,
            "fpr": 0.22149122807017543,
            "logloss": 3.5644015119604693,
            "mae": 0.3029013806628842,
            "precision": 0.678343949044586,
            "recall": 0.891213389121339
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.729173091858814,
            "auditor_fn_violation": 0.015206302059792084,
            "auditor_fp_violation": 0.03416480563231009,
            "ave_precision_score": 0.674585807605974,
            "fpr": 0.21185510428100987,
            "logloss": 3.7302745847567813,
            "mae": 0.31631644830192923,
            "precision": 0.67779632721202,
            "recall": 0.8529411764705882
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7347506373316568,
            "auditor_fn_violation": 0.0052071863759818,
            "auditor_fp_violation": 0.032399547255234876,
            "ave_precision_score": 0.6867374953673296,
            "fpr": 0.21710526315789475,
            "logloss": 3.4627173902069934,
            "mae": 0.30911570385184517,
            "precision": 0.6785714285714286,
            "recall": 0.8744769874476988
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7305202943263144,
            "auditor_fn_violation": 0.008495604608473466,
            "auditor_fp_violation": 0.030377127572327998,
            "ave_precision_score": 0.682749032671341,
            "fpr": 0.21185510428100987,
            "logloss": 3.5469139259433256,
            "mae": 0.32026634458731096,
            "precision": 0.6734348561759729,
            "recall": 0.8361344537815126
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7315432401223779,
            "auditor_fn_violation": 0.005055788005578801,
            "auditor_fp_violation": 0.03180329856900315,
            "ave_precision_score": 0.6836801257533461,
            "fpr": 0.21710526315789475,
            "logloss": 3.4627498007947377,
            "mae": 0.31004106089230227,
            "precision": 0.6790923824959482,
            "recall": 0.8765690376569037
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7314939074895404,
            "auditor_fn_violation": 0.004123273897923606,
            "auditor_fp_violation": 0.03183315038419319,
            "ave_precision_score": 0.6825213208403804,
            "fpr": 0.21075740944017562,
            "logloss": 3.5598596197622054,
            "mae": 0.3178384513373995,
            "precision": 0.6745762711864407,
            "recall": 0.8361344537815126
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7019488861788821,
            "auditor_fn_violation": 0.009255945826910372,
            "auditor_fp_violation": 0.027472916161371173,
            "ave_precision_score": 0.6792769748045498,
            "fpr": 0.19736842105263158,
            "logloss": 2.339707752236851,
            "mae": 0.32677354163024575,
            "precision": 0.6912521440823327,
            "recall": 0.8430962343096234
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7020677665997295,
            "auditor_fn_violation": 0.006549271739431231,
            "auditor_fp_violation": 0.030003658982802776,
            "ave_precision_score": 0.6795329269376034,
            "fpr": 0.19319429198682767,
            "logloss": 2.414605891819758,
            "mae": 0.32439507773681486,
            "precision": 0.6912280701754386,
            "recall": 0.8277310924369747
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7177867118345946,
            "auditor_fn_violation": 0.005959590398590622,
            "auditor_fp_violation": 0.033400032338911805,
            "ave_precision_score": 0.6788525573175913,
            "fpr": 0.21710526315789475,
            "logloss": 3.0981855686385877,
            "mae": 0.3110941478705826,
            "precision": 0.6826923076923077,
            "recall": 0.891213389121339
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7196276528670358,
            "auditor_fn_violation": 0.010273593520833143,
            "auditor_fp_violation": 0.03121995533517544,
            "ave_precision_score": 0.6789772139830963,
            "fpr": 0.21075740944017562,
            "logloss": 3.183690195630777,
            "mae": 0.319624111085208,
            "precision": 0.6805324459234608,
            "recall": 0.8592436974789915
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.7148018106533094,
            "auditor_fn_violation": 0.015295823240108645,
            "auditor_fp_violation": 0.028640148758994264,
            "ave_precision_score": 0.6918025752476555,
            "fpr": 0.16228070175438597,
            "logloss": 2.1723549089084893,
            "mae": 0.32111957734205615,
            "precision": 0.7196969696969697,
            "recall": 0.7949790794979079
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.703483447159249,
            "auditor_fn_violation": 0.016057246169598468,
            "auditor_fp_violation": 0.038419319429198684,
            "ave_precision_score": 0.680730517902798,
            "fpr": 0.14818880351262348,
            "logloss": 2.265413313144904,
            "mae": 0.3238497998099774,
            "precision": 0.7373540856031129,
            "recall": 0.7962184873949579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7335159241295033,
            "auditor_fn_violation": 0.005571918813770832,
            "auditor_fp_violation": 0.032207534966448376,
            "ave_precision_score": 0.6857331734942487,
            "fpr": 0.21600877192982457,
            "logloss": 3.4545766734255006,
            "mae": 0.30928035296874296,
            "precision": 0.6781045751633987,
            "recall": 0.8682008368200836
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7334344197729856,
            "auditor_fn_violation": 0.009685542713243367,
            "auditor_fp_violation": 0.033344688797204036,
            "ave_precision_score": 0.686432567867271,
            "fpr": 0.2052689352360044,
            "logloss": 3.492372179910935,
            "mae": 0.3172240875974249,
            "precision": 0.6770293609671848,
            "recall": 0.8235294117647058
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7346447546909795,
            "auditor_fn_violation": 0.007051493797254645,
            "auditor_fp_violation": 0.032015522677661896,
            "ave_precision_score": 0.6879649402173408,
            "fpr": 0.21052631578947367,
            "logloss": 3.3679264176590538,
            "mae": 0.3139438142261548,
            "precision": 0.679465776293823,
            "recall": 0.8514644351464435
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7374628215602964,
            "auditor_fn_violation": 0.009268141943934547,
            "auditor_fp_violation": 0.0358580314672521,
            "ave_precision_score": 0.6921808978323862,
            "fpr": 0.2052689352360044,
            "logloss": 3.3314940714587173,
            "mae": 0.32199017832144883,
            "precision": 0.6775862068965517,
            "recall": 0.8256302521008403
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 29756,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.7598562065094252,
            "auditor_fn_violation": 0.0004220803053659255,
            "auditor_fp_violation": 0.006518311908804284,
            "ave_precision_score": 0.540895563792576,
            "fpr": 0.43969298245614036,
            "logloss": 14.754064558744743,
            "mae": 0.4473066957668524,
            "precision": 0.5411899313501144,
            "recall": 0.9895397489539749
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7637851850644426,
            "auditor_fn_violation": 0.0010008394137018149,
            "auditor_fp_violation": 0.007090856328147669,
            "ave_precision_score": 0.5430198441270768,
            "fpr": 0.43688254665203075,
            "logloss": 14.763217458329393,
            "mae": 0.44450750909543785,
            "precision": 0.5409457900807382,
            "recall": 0.9852941176470589
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7332454348211132,
            "auditor_fn_violation": 0.004262093518314617,
            "auditor_fp_violation": 0.02804895302773062,
            "ave_precision_score": 0.6867750140367106,
            "fpr": 0.23026315789473684,
            "logloss": 3.408676032124647,
            "mae": 0.31643537866368565,
            "precision": 0.6703296703296703,
            "recall": 0.893305439330544
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.735242565345732,
            "auditor_fn_violation": 0.0072895239325148275,
            "auditor_fp_violation": 0.029610002901951885,
            "ave_precision_score": 0.6893742918241517,
            "fpr": 0.21844127332601537,
            "logloss": 3.4235125580393952,
            "mae": 0.32342536653532317,
            "precision": 0.6726973684210527,
            "recall": 0.8592436974789915
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.6829945992892295,
            "auditor_fn_violation": 0.007030848564926963,
            "auditor_fp_violation": 0.026207150941870816,
            "ave_precision_score": 0.6616493389599694,
            "fpr": 0.22697368421052633,
            "logloss": 2.4814249822625407,
            "mae": 0.31222326671786127,
            "precision": 0.6661290322580645,
            "recall": 0.8640167364016736
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.6663675517061984,
            "auditor_fn_violation": 0.008942984438561376,
            "auditor_fp_violation": 0.028532495552443318,
            "ave_precision_score": 0.644682664363403,
            "fpr": 0.2239297475301866,
            "logloss": 2.640278726971442,
            "mae": 0.322711859319456,
            "precision": 0.6655737704918033,
            "recall": 0.8529411764705882
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7324506485893602,
            "auditor_fn_violation": 0.0052071863759818,
            "auditor_fp_violation": 0.032399547255234876,
            "ave_precision_score": 0.6849203891895317,
            "fpr": 0.21710526315789475,
            "logloss": 3.467218522654604,
            "mae": 0.30960106638488943,
            "precision": 0.6785714285714286,
            "recall": 0.8744769874476988
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7313909968047485,
            "auditor_fn_violation": 0.008495604608473466,
            "auditor_fp_violation": 0.030765736780347476,
            "ave_precision_score": 0.6835282345408236,
            "fpr": 0.21075740944017562,
            "logloss": 3.5473018218121792,
            "mae": 0.32062921038655123,
            "precision": 0.6745762711864407,
            "recall": 0.8361344537815126
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.8124022081044777,
            "auditor_fn_violation": 0.021819716655655873,
            "auditor_fp_violation": 0.02850371897485651,
            "ave_precision_score": 0.8117301863638741,
            "fpr": 0.17982456140350878,
            "logloss": 0.7185388099178079,
            "mae": 0.3343012794661295,
            "precision": 0.6974169741697417,
            "recall": 0.7907949790794979
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8345840941100048,
            "auditor_fn_violation": 0.02885138687747327,
            "auditor_fp_violation": 0.022983458874295024,
            "ave_precision_score": 0.834831475628886,
            "fpr": 0.15806805708013172,
            "logloss": 0.7977918357439968,
            "mae": 0.31926561108433554,
            "precision": 0.7298311444652908,
            "recall": 0.8172268907563025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7329672677278396,
            "auditor_fn_violation": 0.007888772663877268,
            "auditor_fp_violation": 0.03071691325086911,
            "ave_precision_score": 0.6868434550509662,
            "fpr": 0.20175438596491227,
            "logloss": 3.3553832564353576,
            "mae": 0.3060895468107363,
            "precision": 0.6876061120543294,
            "recall": 0.8472803347280334
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7329772897547546,
            "auditor_fn_violation": 0.006881347489599576,
            "auditor_fp_violation": 0.03540381291242414,
            "ave_precision_score": 0.6874194028447288,
            "fpr": 0.19209659714599342,
            "logloss": 3.3975124464274695,
            "mae": 0.316216782783438,
            "precision": 0.6891651865008881,
            "recall": 0.8151260504201681
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.848101426882427,
            "auditor_fn_violation": 0.024879798869558833,
            "auditor_fp_violation": 0.023162745573611455,
            "ave_precision_score": 0.8473733832621858,
            "fpr": 0.14144736842105263,
            "logloss": 0.6775033855952561,
            "mae": 0.2943405984865661,
            "precision": 0.7455621301775148,
            "recall": 0.7907949790794979
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8695095223004952,
            "auditor_fn_violation": 0.02336521875490043,
            "auditor_fp_violation": 0.02331655248116886,
            "ave_precision_score": 0.8696803318514905,
            "fpr": 0.13172338090010977,
            "logloss": 0.7549415689874024,
            "mae": 0.2889211229170068,
            "precision": 0.76,
            "recall": 0.7983193277310925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.7372416040309335,
            "auditor_fn_violation": 0.007207479997063793,
            "auditor_fp_violation": 0.030469318457433912,
            "ave_precision_score": 0.6903163572130712,
            "fpr": 0.19517543859649122,
            "logloss": 3.3365166980335332,
            "mae": 0.3020402398840001,
            "precision": 0.6972789115646258,
            "recall": 0.8577405857740585
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7359771747131203,
            "auditor_fn_violation": 0.01115221060982022,
            "auditor_fp_violation": 0.03565615655399524,
            "ave_precision_score": 0.6880415645538449,
            "fpr": 0.19538968166849616,
            "logloss": 3.431830002004523,
            "mae": 0.314244596269659,
            "precision": 0.6855123674911661,
            "recall": 0.8151260504201681
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.733757090436564,
            "auditor_fn_violation": 0.005120017617264922,
            "auditor_fp_violation": 0.032207534966448376,
            "ave_precision_score": 0.6859855558631442,
            "fpr": 0.21600877192982457,
            "logloss": 3.4563881890238566,
            "mae": 0.3091237649756599,
            "precision": 0.6791530944625407,
            "recall": 0.8723849372384938
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7320161729751238,
            "auditor_fn_violation": 0.010008394137018147,
            "auditor_fp_violation": 0.03045030722838362,
            "ave_precision_score": 0.6840368906920399,
            "fpr": 0.20636663007683864,
            "logloss": 3.5372433296341086,
            "mae": 0.31970627863891654,
            "precision": 0.678082191780822,
            "recall": 0.8319327731092437
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7316035250908042,
            "auditor_fn_violation": 0.006734933568230198,
            "auditor_fp_violation": 0.0326117713638936,
            "ave_precision_score": 0.6837237602881293,
            "fpr": 0.2149122807017544,
            "logloss": 3.456927024795615,
            "mae": 0.3095878770231865,
            "precision": 0.6807817589576547,
            "recall": 0.8744769874476988
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7320199308584884,
            "auditor_fn_violation": 0.005608390447287588,
            "auditor_fp_violation": 0.03193156440440592,
            "ave_precision_score": 0.6835086063411673,
            "fpr": 0.2074643249176729,
            "logloss": 3.535311752013371,
            "mae": 0.3175191221223387,
            "precision": 0.676923076923077,
            "recall": 0.8319327731092437
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7334964342807697,
            "auditor_fn_violation": 0.006377082874550396,
            "auditor_fp_violation": 0.032207534966448376,
            "ave_precision_score": 0.6857004682105662,
            "fpr": 0.21600877192982457,
            "logloss": 3.451352793246331,
            "mae": 0.30989364960196736,
            "precision": 0.6786296900489397,
            "recall": 0.8702928870292888
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.733667076009607,
            "auditor_fn_violation": 0.006932081284764182,
            "auditor_fp_violation": 0.033344688797204036,
            "ave_precision_score": 0.6866373664772425,
            "fpr": 0.2052689352360044,
            "logloss": 3.4869598927067775,
            "mae": 0.31752541528763256,
            "precision": 0.6781411359724613,
            "recall": 0.8277310924369747
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.73165412714325,
            "auditor_fn_violation": 0.006734933568230198,
            "auditor_fp_violation": 0.0326117713638936,
            "ave_precision_score": 0.683781323290869,
            "fpr": 0.2149122807017544,
            "logloss": 3.4546639368111975,
            "mae": 0.3096657795321019,
            "precision": 0.6807817589576547,
            "recall": 0.8744769874476988
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7315844419182269,
            "auditor_fn_violation": 0.005608390447287588,
            "auditor_fp_violation": 0.03193156440440592,
            "ave_precision_score": 0.6831971170927074,
            "fpr": 0.2074643249176729,
            "logloss": 3.5321405419812333,
            "mae": 0.3175457275759089,
            "precision": 0.676923076923077,
            "recall": 0.8319327731092437
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.735842335897084,
            "auditor_fn_violation": 0.006824396241650153,
            "auditor_fp_violation": 0.025855970571590267,
            "ave_precision_score": 0.6884601024925335,
            "fpr": 0.2149122807017544,
            "logloss": 3.357073001987228,
            "mae": 0.3083021529775834,
            "precision": 0.6802610114192496,
            "recall": 0.8723849372384938
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7366795409806719,
            "auditor_fn_violation": 0.01407632207658036,
            "auditor_fp_violation": 0.03246905636095235,
            "ave_precision_score": 0.6879814012312053,
            "fpr": 0.20636663007683864,
            "logloss": 3.4292983132479935,
            "mae": 0.3172307592033563,
            "precision": 0.6758620689655173,
            "recall": 0.8235294117647058
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7322747603779235,
            "auditor_fn_violation": 0.00821680246641709,
            "auditor_fp_violation": 0.03295537230172205,
            "ave_precision_score": 0.6838812265694852,
            "fpr": 0.19736842105263158,
            "logloss": 3.4507192155503104,
            "mae": 0.3059639270137755,
            "precision": 0.6928327645051194,
            "recall": 0.8493723849372385
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7340311551966019,
            "auditor_fn_violation": 0.006231032478853236,
            "auditor_fp_violation": 0.033044399863734426,
            "ave_precision_score": 0.6851514768400668,
            "fpr": 0.19758507135016465,
            "logloss": 3.4983192056737713,
            "mae": 0.3234910771876074,
            "precision": 0.6768402154398564,
            "recall": 0.792016806722689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7332304167809565,
            "auditor_fn_violation": 0.006799163179916318,
            "auditor_fp_violation": 0.031348532621877284,
            "ave_precision_score": 0.6869320765685227,
            "fpr": 0.20614035087719298,
            "logloss": 3.349571161690347,
            "mae": 0.30994605899455996,
            "precision": 0.6850921273031826,
            "recall": 0.8556485355648535
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7346815136777487,
            "auditor_fn_violation": 0.007845289597727127,
            "auditor_fp_violation": 0.036428328097202776,
            "ave_precision_score": 0.6889302306170316,
            "fpr": 0.1942919868276619,
            "logloss": 3.3820468515882713,
            "mae": 0.31744001550177525,
            "precision": 0.6850533807829181,
            "recall": 0.8088235294117647
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7380662833793024,
            "auditor_fn_violation": 0.007019378991411583,
            "auditor_fp_violation": 0.028705837173579107,
            "ave_precision_score": 0.6876297411739718,
            "fpr": 0.21710526315789475,
            "logloss": 3.538999037650647,
            "mae": 0.30474646811301553,
            "precision": 0.6790923824959482,
            "recall": 0.8765690376569037
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7313955230138406,
            "auditor_fn_violation": 0.009233550719958674,
            "auditor_fp_violation": 0.03189371285817026,
            "ave_precision_score": 0.6787531181083086,
            "fpr": 0.21075740944017562,
            "logloss": 3.7137913739089834,
            "mae": 0.3204155919513695,
            "precision": 0.6745762711864407,
            "recall": 0.8361344537815126
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7330782045618398,
            "auditor_fn_violation": 0.0054618109080232,
            "auditor_fp_violation": 0.030994825774112698,
            "ave_precision_score": 0.6868333320440339,
            "fpr": 0.21271929824561403,
            "logloss": 3.3714883104095046,
            "mae": 0.31063473990938645,
            "precision": 0.6814449917898193,
            "recall": 0.8682008368200836
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7346265833067448,
            "auditor_fn_violation": 0.008075897757566257,
            "auditor_fp_violation": 0.032665884401377805,
            "ave_precision_score": 0.68910304328491,
            "fpr": 0.19758507135016465,
            "logloss": 3.3936783666696395,
            "mae": 0.31771685659145504,
            "precision": 0.6847635726795096,
            "recall": 0.8214285714285714
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7245377982342507,
            "auditor_fn_violation": 0.005473280481538579,
            "auditor_fp_violation": 0.031858881073651875,
            "ave_precision_score": 0.6776893578723688,
            "fpr": 0.22587719298245615,
            "logloss": 3.4691121506834874,
            "mae": 0.32573888020386466,
            "precision": 0.667741935483871,
            "recall": 0.8661087866108786
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.726978185934106,
            "auditor_fn_violation": 0.008043612615188775,
            "auditor_fp_violation": 0.02807323012478393,
            "ave_precision_score": 0.6814681350742855,
            "fpr": 0.21953896816684962,
            "logloss": 3.461031957823213,
            "mae": 0.3327894943658126,
            "precision": 0.6644295302013423,
            "recall": 0.8319327731092437
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 29756,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7460403715509708,
            "auditor_fn_violation": 0.004986970564486533,
            "auditor_fp_violation": 0.02458262591963781,
            "ave_precision_score": 0.6887214950108501,
            "fpr": 0.19188596491228072,
            "logloss": 4.786202547033294,
            "mae": 0.3196293680733231,
            "precision": 0.6777163904235728,
            "recall": 0.7698744769874477
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7407239888295847,
            "auditor_fn_violation": 0.013080094826075334,
            "auditor_fp_violation": 0.0311644397340298,
            "ave_precision_score": 0.6815497705275576,
            "fpr": 0.18990120746432493,
            "logloss": 4.816194544229563,
            "mae": 0.32204650240835914,
            "precision": 0.6760299625468165,
            "recall": 0.7584033613445378
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.7914442498756733,
            "auditor_fn_violation": 0.01208663657050576,
            "auditor_fp_violation": 0.01343075430511764,
            "ave_precision_score": 0.7690726554418759,
            "fpr": 0.16228070175438597,
            "logloss": 1.8799206657048995,
            "mae": 0.28080354410441005,
            "precision": 0.728440366972477,
            "recall": 0.8305439330543933
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8143681771790519,
            "auditor_fn_violation": 0.017636912064496496,
            "auditor_fp_violation": 0.01634934453739102,
            "ave_precision_score": 0.8004856800456717,
            "fpr": 0.1394072447859495,
            "logloss": 1.6168120891573867,
            "mae": 0.275441999361356,
            "precision": 0.748015873015873,
            "recall": 0.792016806722689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 29756,
        "test": {
            "accuracy": 0.4594298245614035,
            "auc_prc": 0.459830060603774,
            "auditor_fn_violation": 0.015928943698157522,
            "auditor_fp_violation": 0.01972673619532703,
            "ave_precision_score": 0.5185555484941632,
            "fpr": 0.043859649122807015,
            "logloss": 18.189264543303842,
            "mae": 0.5390231388492223,
            "precision": 0.38461538461538464,
            "recall": 0.05230125523012552
        },
        "train": {
            "accuracy": 0.4610318331503842,
            "auc_prc": 0.4642300057730899,
            "auditor_fn_violation": 0.014749697903310646,
            "auditor_fp_violation": 0.02206745145539195,
            "ave_precision_score": 0.5137138960835717,
            "fpr": 0.042810098792535674,
            "logloss": 18.270411237448712,
            "mae": 0.5389075510849937,
            "precision": 0.38095238095238093,
            "recall": 0.05042016806722689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.6987539524729179,
            "auditor_fn_violation": 0.009244476253394997,
            "auditor_fp_violation": 0.030494583232274247,
            "ave_precision_score": 0.6765903912158426,
            "fpr": 0.19736842105263158,
            "logloss": 2.347846555416385,
            "mae": 0.326258024096636,
            "precision": 0.6923076923076923,
            "recall": 0.8472803347280334
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.6986850134100807,
            "auditor_fn_violation": 0.004789731479858684,
            "auditor_fp_violation": 0.030003658982802776,
            "ave_precision_score": 0.676434746654351,
            "fpr": 0.18990120746432493,
            "logloss": 2.451735223838782,
            "mae": 0.32463684312240115,
            "precision": 0.6964912280701754,
            "recall": 0.8340336134453782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7324730101959349,
            "auditor_fn_violation": 0.007719022975849671,
            "auditor_fp_violation": 0.03278357183280783,
            "ave_precision_score": 0.6862159239543184,
            "fpr": 0.22039473684210525,
            "logloss": 3.3840687371098563,
            "mae": 0.3145445074596079,
            "precision": 0.6778846153846154,
            "recall": 0.8849372384937239
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7345767464041416,
            "auditor_fn_violation": 0.009097491905653591,
            "auditor_fp_violation": 0.030359463517418032,
            "ave_precision_score": 0.6888344813211995,
            "fpr": 0.21295279912184412,
            "logloss": 3.4042689734903337,
            "mae": 0.321180790528171,
            "precision": 0.6739495798319328,
            "recall": 0.842436974789916
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8501953405124838,
            "auditor_fn_violation": 0.023514919621228807,
            "auditor_fp_violation": 0.018855101463335768,
            "ave_precision_score": 0.8494651562183589,
            "fpr": 0.13486842105263158,
            "logloss": 0.6686961219982555,
            "mae": 0.29433651689211265,
            "precision": 0.754,
            "recall": 0.7887029288702929
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8711938950907707,
            "auditor_fn_violation": 0.021405049396267837,
            "auditor_fp_violation": 0.02149210795260987,
            "ave_precision_score": 0.8713609565176694,
            "fpr": 0.1251372118551043,
            "logloss": 0.7450325922539777,
            "mae": 0.2877318717635918,
            "precision": 0.7663934426229508,
            "recall": 0.7857142857142857
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 29756,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.7828605713189811,
            "auditor_fn_violation": 0.0019223005211774207,
            "auditor_fp_violation": 0.006591579755841229,
            "ave_precision_score": 0.6979137781071205,
            "fpr": 0.41776315789473684,
            "logloss": 7.9123689349144355,
            "mae": 0.43107162742070876,
            "precision": 0.5533411488862837,
            "recall": 0.9874476987447699
        },
        "train": {
            "accuracy": 0.575192096597146,
            "auc_prc": 0.7734666979528466,
            "auditor_fn_violation": 0.0009408812921436414,
            "auditor_fp_violation": 0.009697566145577045,
            "ave_precision_score": 0.6852172927482276,
            "fpr": 0.41931942919868276,
            "logloss": 8.054165925666174,
            "mae": 0.4310700895160469,
            "precision": 0.552168815943728,
            "recall": 0.9894957983193278
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.6719151777454409,
            "auditor_fn_violation": 0.010568065037069663,
            "auditor_fp_violation": 0.032101422912119,
            "ave_precision_score": 0.6475027007370995,
            "fpr": 0.21929824561403508,
            "logloss": 2.713893923416369,
            "mae": 0.32320802965560896,
            "precision": 0.677938808373591,
            "recall": 0.8807531380753139
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.67994784148828,
            "auditor_fn_violation": 0.0040679279395622134,
            "auditor_fp_violation": 0.029632713829693288,
            "ave_precision_score": 0.6545738062179318,
            "fpr": 0.21295279912184412,
            "logloss": 2.7168477099717556,
            "mae": 0.3269280956794746,
            "precision": 0.6766666666666666,
            "recall": 0.8529411764705882
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7002648105115857,
            "auditor_fn_violation": 0.01002899508184688,
            "auditor_fp_violation": 0.03358951815021424,
            "ave_precision_score": 0.6778320629141094,
            "fpr": 0.20285087719298245,
            "logloss": 2.3452735709043666,
            "mae": 0.3277079018952096,
            "precision": 0.688026981450253,
            "recall": 0.8535564853556485
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7021487678428135,
            "auditor_fn_violation": 0.00714193471021779,
            "auditor_fp_violation": 0.029271862422246615,
            "ave_precision_score": 0.679587967828386,
            "fpr": 0.1986827661909989,
            "logloss": 2.4177608193302302,
            "mae": 0.3241895238663223,
            "precision": 0.690068493150685,
            "recall": 0.8466386554621849
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.7392021098994164,
            "auditor_fn_violation": 0.009588563458856347,
            "auditor_fp_violation": 0.028195488721804513,
            "ave_precision_score": 0.6881737984015572,
            "fpr": 0.20394736842105263,
            "logloss": 3.4953743283640346,
            "mae": 0.30149642452421466,
            "precision": 0.6920529801324503,
            "recall": 0.8744769874476988
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7323324120300237,
            "auditor_fn_violation": 0.011408185667241652,
            "auditor_fp_violation": 0.03263560316438928,
            "ave_precision_score": 0.6778023997380316,
            "fpr": 0.20417124039517015,
            "logloss": 3.703917876841881,
            "mae": 0.3185317350995423,
            "precision": 0.6776429809358753,
            "recall": 0.8214285714285714
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7334992802738021,
            "auditor_fn_violation": 0.005571918813770832,
            "auditor_fp_violation": 0.032207534966448376,
            "ave_precision_score": 0.6857161497496043,
            "fpr": 0.21600877192982457,
            "logloss": 3.454516949561732,
            "mae": 0.30934625438654256,
            "precision": 0.6781045751633987,
            "recall": 0.8682008368200836
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7334605235443061,
            "auditor_fn_violation": 0.009685542713243367,
            "auditor_fp_violation": 0.033344688797204036,
            "ave_precision_score": 0.6864559595874706,
            "fpr": 0.2052689352360044,
            "logloss": 3.4920773794320747,
            "mae": 0.31721904687900804,
            "precision": 0.6770293609671848,
            "recall": 0.8235294117647058
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7352397244552652,
            "auditor_fn_violation": 0.0072487704617191515,
            "auditor_fp_violation": 0.03090387258468753,
            "ave_precision_score": 0.6878131721968225,
            "fpr": 0.21162280701754385,
            "logloss": 3.427250644233847,
            "mae": 0.3070522408754865,
            "precision": 0.6830870279146142,
            "recall": 0.8702928870292888
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7347295321226021,
            "auditor_fn_violation": 0.009104410150448765,
            "auditor_fp_violation": 0.0322924158118526,
            "ave_precision_score": 0.6881527082304786,
            "fpr": 0.1986827661909989,
            "logloss": 3.46796578858871,
            "mae": 0.3169020760825882,
            "precision": 0.6841186736474695,
            "recall": 0.8235294117647058
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.7339985460639058,
            "auditor_fn_violation": 0.008937091683182853,
            "auditor_fp_violation": 0.030858395989974943,
            "ave_precision_score": 0.6875253741580536,
            "fpr": 0.20833333333333334,
            "logloss": 3.360080216909484,
            "mae": 0.3103133610970126,
            "precision": 0.6812080536912751,
            "recall": 0.8493723849372385
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7373237978003222,
            "auditor_fn_violation": 0.010820134859651873,
            "auditor_fp_violation": 0.03485370377379916,
            "ave_precision_score": 0.6926292676629806,
            "fpr": 0.19978046103183314,
            "logloss": 3.310081128335844,
            "mae": 0.3190771594169961,
            "precision": 0.6823734729493892,
            "recall": 0.8214285714285714
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 29756,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.7224591581804887,
            "auditor_fn_violation": 0.0017204360273067622,
            "auditor_fp_violation": 0.025669011237771856,
            "ave_precision_score": 0.6748144594433219,
            "fpr": 0.22587719298245615,
            "logloss": 3.4752086317418955,
            "mae": 0.3384368015995836,
            "precision": 0.653781512605042,
            "recall": 0.8138075313807531
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.7314913686806263,
            "auditor_fn_violation": 0.012196865573891469,
            "auditor_fp_violation": 0.029334948332639395,
            "ave_precision_score": 0.6846901013234963,
            "fpr": 0.21734357848518113,
            "logloss": 3.3981062531573496,
            "mae": 0.3408006281724247,
            "precision": 0.6568457538994801,
            "recall": 0.7962184873949579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 29756,
        "test": {
            "accuracy": 0.5614035087719298,
            "auc_prc": 0.7572915699165893,
            "auditor_fn_violation": 0.006129340086618219,
            "auditor_fp_violation": 0.018857627940819802,
            "ave_precision_score": 0.54232749546401,
            "fpr": 0.4276315789473684,
            "logloss": 14.442526496288473,
            "mae": 0.4398591203606992,
            "precision": 0.5454545454545454,
            "recall": 0.9790794979079498
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7624492933025085,
            "auditor_fn_violation": 0.003413000765619091,
            "auditor_fp_violation": 0.007709098249996854,
            "ave_precision_score": 0.5449548785028955,
            "fpr": 0.4226125137211855,
            "logloss": 14.46828234071562,
            "mae": 0.4341779057014198,
            "precision": 0.5497076023391813,
            "recall": 0.9873949579831933
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.6968647952852222,
            "auditor_fn_violation": 0.006884038023930121,
            "auditor_fp_violation": 0.02707373271889401,
            "ave_precision_score": 0.6747079461129004,
            "fpr": 0.20614035087719298,
            "logloss": 2.3563623762510693,
            "mae": 0.32965428799211066,
            "precision": 0.6829679595278246,
            "recall": 0.8472803347280334
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.6982170987695404,
            "auditor_fn_violation": 0.005613002610484368,
            "auditor_fp_violation": 0.02424013020931905,
            "ave_precision_score": 0.6762055754754778,
            "fpr": 0.1986827661909989,
            "logloss": 2.431662178863245,
            "mae": 0.32486248508223786,
            "precision": 0.6895368782161235,
            "recall": 0.8445378151260504
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 29756,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7091734106107953,
            "auditor_fn_violation": 0.003853776701167143,
            "auditor_fp_violation": 0.031388956261621796,
            "ave_precision_score": 0.671059093251264,
            "fpr": 0.18640350877192982,
            "logloss": 3.1991578917122943,
            "mae": 0.31201511249108743,
            "precision": 0.6909090909090909,
            "recall": 0.7949790794979079
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.7111421084523517,
            "auditor_fn_violation": 0.015653681889879992,
            "auditor_fp_violation": 0.03653683586307834,
            "ave_precision_score": 0.6724142498441725,
            "fpr": 0.18551042810098792,
            "logloss": 3.2881815842387923,
            "mae": 0.3255110218504863,
            "precision": 0.6780952380952381,
            "recall": 0.7478991596638656
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7351378860307245,
            "auditor_fn_violation": 0.004385964912280706,
            "auditor_fp_violation": 0.032914948661977535,
            "ave_precision_score": 0.6864851272707186,
            "fpr": 0.18969298245614036,
            "logloss": 3.4139743904761546,
            "mae": 0.30087614893255915,
            "precision": 0.6980802792321117,
            "recall": 0.8368200836820083
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7311662367535172,
            "auditor_fn_violation": 0.010903153797193964,
            "auditor_fp_violation": 0.03386451669884049,
            "ave_precision_score": 0.6817199374994011,
            "fpr": 0.19209659714599342,
            "logloss": 3.5235790165353955,
            "mae": 0.31660550823024175,
            "precision": 0.6806569343065694,
            "recall": 0.7836134453781513
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7266600614101411,
            "auditor_fn_violation": 0.00612016442780592,
            "auditor_fp_violation": 0.023241066375616466,
            "ave_precision_score": 0.678634810391229,
            "fpr": 0.21600877192982457,
            "logloss": 3.505443294843437,
            "mae": 0.3047953891255914,
            "precision": 0.6754530477759473,
            "recall": 0.8577405857740585
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7246526035596278,
            "auditor_fn_violation": 0.014980306063149743,
            "auditor_fp_violation": 0.0218479124872251,
            "ave_precision_score": 0.676196383561306,
            "fpr": 0.22063666300768386,
            "logloss": 3.5618116022356268,
            "mae": 0.3171030136702092,
            "precision": 0.665,
            "recall": 0.8382352941176471
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7314734492233552,
            "auditor_fn_violation": 0.006578947368421052,
            "auditor_fp_violation": 0.03128284420729243,
            "ave_precision_score": 0.6840214316210387,
            "fpr": 0.22149122807017543,
            "logloss": 3.4616734827402014,
            "mae": 0.3081792860180924,
            "precision": 0.6752411575562701,
            "recall": 0.8786610878661087
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7301796668854108,
            "auditor_fn_violation": 0.007667721314650996,
            "auditor_fp_violation": 0.03140416619352234,
            "ave_precision_score": 0.681793276352542,
            "fpr": 0.20856201975850713,
            "logloss": 3.5679568751971136,
            "mae": 0.3182920988069703,
            "precision": 0.6785109983079526,
            "recall": 0.842436974789916
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 29756,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.784169245495122,
            "auditor_fn_violation": 0.0022847390442633777,
            "auditor_fp_violation": 0.00419395262349422,
            "ave_precision_score": 0.6988609829192174,
            "fpr": 0.4243421052631579,
            "logloss": 7.96791716895939,
            "mae": 0.43317232126439503,
            "precision": 0.5505226480836237,
            "recall": 0.9916317991631799
        },
        "train": {
            "accuracy": 0.5740944017563118,
            "auc_prc": 0.7742184933638668,
            "auditor_fn_violation": 0.0009408812921436414,
            "auditor_fp_violation": 0.009659714599341389,
            "ave_precision_score": 0.6859414097789959,
            "fpr": 0.42041712403951703,
            "logloss": 8.104462200940048,
            "mae": 0.43325150599570295,
            "precision": 0.5515222482435597,
            "recall": 0.9894957983193278
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.7366011071360604,
            "auditor_fn_violation": 0.0006858804962196304,
            "auditor_fp_violation": 0.034875495189586866,
            "ave_precision_score": 0.6900325418629293,
            "fpr": 0.17982456140350878,
            "logloss": 3.3020369803818626,
            "mae": 0.29890759774550607,
            "precision": 0.7066189624329159,
            "recall": 0.8263598326359832
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7362002830037322,
            "auditor_fn_violation": 0.012930199522179896,
            "auditor_fp_violation": 0.03592111737764489,
            "ave_precision_score": 0.6881240715574108,
            "fpr": 0.18111964873765093,
            "logloss": 3.4133617750675427,
            "mae": 0.315261959441759,
            "precision": 0.6921641791044776,
            "recall": 0.7794117647058824
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7354371227769685,
            "auditor_fn_violation": 0.007147838214783823,
            "auditor_fp_violation": 0.029850331473845904,
            "ave_precision_score": 0.6885918839151982,
            "fpr": 0.20285087719298245,
            "logloss": 3.403012529350304,
            "mae": 0.30544519150709387,
            "precision": 0.6895973154362416,
            "recall": 0.8598326359832636
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7320932861364171,
            "auditor_fn_violation": 0.008103570736746949,
            "auditor_fp_violation": 0.03337497003419257,
            "ave_precision_score": 0.6852068533638287,
            "fpr": 0.19978046103183314,
            "logloss": 3.4965178505692087,
            "mae": 0.3183826595157177,
            "precision": 0.681260945709282,
            "recall": 0.8172268907563025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 29756,
        "test": {
            "accuracy": 0.5723684210526315,
            "auc_prc": 0.7896329067389545,
            "auditor_fn_violation": 0.002376495632386405,
            "auditor_fp_violation": 0.006584000323389124,
            "ave_precision_score": 0.7025170317677252,
            "fpr": 0.4232456140350877,
            "logloss": 7.9918261161806745,
            "mae": 0.4320412399296001,
            "precision": 0.5511627906976744,
            "recall": 0.9916317991631799
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.7800070793893406,
            "auditor_fn_violation": 0.00022138383344556277,
            "auditor_fp_violation": 0.009208019480929126,
            "ave_precision_score": 0.6886188269138436,
            "fpr": 0.42371020856201974,
            "logloss": 8.171248133185694,
            "mae": 0.43372280214878434,
            "precision": 0.5490654205607477,
            "recall": 0.9873949579831933
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 29756,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.658266950109765,
            "auditor_fn_violation": 0.0013763488218454106,
            "auditor_fp_violation": 0.03787442396313365,
            "ave_precision_score": 0.6359349147394353,
            "fpr": 0.22039473684210525,
            "logloss": 2.973155298840761,
            "mae": 0.36625432332030966,
            "precision": 0.6575809199318569,
            "recall": 0.8075313807531381
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.6518566305695215,
            "auditor_fn_violation": 0.013769613223994323,
            "auditor_fp_violation": 0.025655778038532872,
            "ave_precision_score": 0.6299333390894767,
            "fpr": 0.20087815587266739,
            "logloss": 3.3802612921845667,
            "mae": 0.36431504698133793,
            "precision": 0.6778169014084507,
            "recall": 0.8088235294117647
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 29756,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7328343839117615,
            "auditor_fn_violation": 0.0034454598840196735,
            "auditor_fp_violation": 0.025999979788180135,
            "ave_precision_score": 0.6871787293860204,
            "fpr": 0.2817982456140351,
            "logloss": 3.460322468024313,
            "mae": 0.33169899744125975,
            "precision": 0.6385372714486639,
            "recall": 0.9497907949790795
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7317862696612054,
            "auditor_fn_violation": 0.005359333634661329,
            "auditor_fp_violation": 0.024729676873966978,
            "ave_precision_score": 0.6840687300594451,
            "fpr": 0.2722283205268935,
            "logloss": 3.515468254965188,
            "mae": 0.33533963810342865,
            "precision": 0.6395348837209303,
            "recall": 0.9243697478991597
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7007985223220643,
            "auditor_fn_violation": 0.008459957424943109,
            "auditor_fp_violation": 0.027205109548063704,
            "ave_precision_score": 0.6786236522576466,
            "fpr": 0.19736842105263158,
            "logloss": 2.331138143830024,
            "mae": 0.3263206970183538,
            "precision": 0.6912521440823327,
            "recall": 0.8430962343096234
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7011128874846375,
            "auditor_fn_violation": 0.006224114234058061,
            "auditor_fp_violation": 0.029213823384685273,
            "ave_precision_score": 0.6790946223349034,
            "fpr": 0.18441273326015367,
            "logloss": 2.409908232307994,
            "mae": 0.32335890699092046,
            "precision": 0.7015985790408525,
            "recall": 0.8298319327731093
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.736643508862934,
            "auditor_fn_violation": 0.011696671070982899,
            "auditor_fp_violation": 0.0312727382973563,
            "ave_precision_score": 0.6898366861228185,
            "fpr": 0.19298245614035087,
            "logloss": 3.333677224855083,
            "mae": 0.30159029823588485,
            "precision": 0.6970740103270223,
            "recall": 0.8472803347280334
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7358058729857445,
            "auditor_fn_violation": 0.00929350884151685,
            "auditor_fp_violation": 0.03580756273893788,
            "ave_precision_score": 0.6878377300965975,
            "fpr": 0.19209659714599342,
            "logloss": 3.432814192989477,
            "mae": 0.31409808531114264,
            "precision": 0.6863799283154122,
            "recall": 0.8046218487394958
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 29756,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.7225233089596408,
            "auditor_fn_violation": 0.0017204360273067622,
            "auditor_fp_violation": 0.026697287573773147,
            "ave_precision_score": 0.6748759689605182,
            "fpr": 0.22697368421052633,
            "logloss": 3.4751676785462933,
            "mae": 0.33830324243985294,
            "precision": 0.6526845637583892,
            "recall": 0.8138075313807531
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7314923756382574,
            "auditor_fn_violation": 0.013458292208211497,
            "auditor_fp_violation": 0.029334948332639395,
            "ave_precision_score": 0.6846935837602889,
            "fpr": 0.21734357848518113,
            "logloss": 3.3982119712282888,
            "mae": 0.3407293372261233,
            "precision": 0.657439446366782,
            "recall": 0.7983193277310925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.848764986824822,
            "auditor_fn_violation": 0.024347610658445276,
            "auditor_fp_violation": 0.022293637319104213,
            "ave_precision_score": 0.8480536469597312,
            "fpr": 0.13925438596491227,
            "logloss": 0.6736787521477103,
            "mae": 0.2946011484176811,
            "precision": 0.7490118577075099,
            "recall": 0.7928870292887029
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8699065869307596,
            "auditor_fn_violation": 0.024015533765646767,
            "auditor_fp_violation": 0.02070479579090806,
            "ave_precision_score": 0.8700778350137793,
            "fpr": 0.13172338090010977,
            "logloss": 0.7539918173590662,
            "mae": 0.28828488623096715,
            "precision": 0.7604790419161677,
            "recall": 0.8004201680672269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 29756,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7343682251435025,
            "auditor_fn_violation": 0.004840160023489687,
            "auditor_fp_violation": 0.03165170991996119,
            "ave_precision_score": 0.6877844562861751,
            "fpr": 0.20942982456140352,
            "logloss": 3.387422902142644,
            "mae": 0.30977530666146946,
            "precision": 0.6858552631578947,
            "recall": 0.8723849372384938
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7343806672270164,
            "auditor_fn_violation": 0.007231871892555051,
            "auditor_fp_violation": 0.033536469964798074,
            "ave_precision_score": 0.6885340199448517,
            "fpr": 0.20197585071350166,
            "logloss": 3.4222163334489477,
            "mae": 0.31855304253561084,
            "precision": 0.6805555555555556,
            "recall": 0.8235294117647058
        }
    }
]
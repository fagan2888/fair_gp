[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6348684210526315,
            "auc_prc": 0.6735930014413607,
            "auditor_fn_violation": 0.013964482100774323,
            "auditor_fp_violation": 0.015029107758108593,
            "ave_precision_score": 0.6497970170753407,
            "fpr": 0.29276315789473684,
            "logloss": 3.271610684483713,
            "mae": 0.37360926782842174,
            "precision": 0.6014925373134329,
            "recall": 0.8592750533049041
        },
        "train": {
            "accuracy": 0.6575192096597146,
            "auc_prc": 0.7021173403156158,
            "auditor_fn_violation": 0.012149331764120093,
            "auditor_fp_violation": 0.01762495941621187,
            "ave_precision_score": 0.6824541912808836,
            "fpr": 0.283205268935236,
            "logloss": 2.8066741611839383,
            "mae": 0.35194296875030273,
            "precision": 0.625544267053701,
            "recall": 0.8886597938144329
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8365039402301249,
            "auditor_fn_violation": 0.025186567164179104,
            "auditor_fp_violation": 0.021954576056393806,
            "ave_precision_score": 0.8372351573218142,
            "fpr": 0.13048245614035087,
            "logloss": 1.069377978973291,
            "mae": 0.2675508588173934,
            "precision": 0.75,
            "recall": 0.7611940298507462
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8642876357357182,
            "auditor_fn_violation": 0.01812214967125737,
            "auditor_fp_violation": 0.009745262647969783,
            "ave_precision_score": 0.8644828508178861,
            "fpr": 0.10757409440175632,
            "logloss": 0.8361973950604152,
            "mae": 0.25212307365584496,
            "precision": 0.7941176470588235,
            "recall": 0.7793814432989691
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.7673603424557943,
            "auditor_fn_violation": 0.023000598511203382,
            "auditor_fp_violation": 0.02179864163795493,
            "ave_precision_score": 0.7648534012687789,
            "fpr": 0.23903508771929824,
            "logloss": 1.7455556227738136,
            "mae": 0.337487425957708,
            "precision": 0.6390728476821192,
            "recall": 0.8230277185501066
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7955801486175944,
            "auditor_fn_violation": 0.016592166759084273,
            "auditor_fp_violation": 0.016047989363182386,
            "ave_precision_score": 0.7929777494975696,
            "fpr": 0.22502744237102085,
            "logloss": 1.4185243662470237,
            "mae": 0.31057103384017043,
            "precision": 0.6740858505564388,
            "recall": 0.8742268041237113
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7870566805055015,
            "auditor_fn_violation": 0.017066921033928102,
            "auditor_fp_violation": 0.02685784721397173,
            "ave_precision_score": 0.7869262668314696,
            "fpr": 0.1600877192982456,
            "logloss": 1.2977011184995995,
            "mae": 0.2872844592939986,
            "precision": 0.7125984251968503,
            "recall": 0.7718550106609808
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.810928192618317,
            "auditor_fn_violation": 0.019649869295098855,
            "auditor_fp_violation": 0.014818880351262356,
            "ave_precision_score": 0.810721450199406,
            "fpr": 0.13721185510428102,
            "logloss": 1.0441875435123036,
            "mae": 0.27362195035868475,
            "precision": 0.7529644268774703,
            "recall": 0.7855670103092783
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.7779210127348952,
            "auditor_fn_violation": 0.049538491751767486,
            "auditor_fp_violation": 0.03609015484535267,
            "ave_precision_score": 0.7778447875274995,
            "fpr": 0.19517543859649122,
            "logloss": 1.4063286300810165,
            "mae": 0.32556374166536944,
            "precision": 0.6622390891840607,
            "recall": 0.744136460554371
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.786213852814326,
            "auditor_fn_violation": 0.04979913316056899,
            "auditor_fp_violation": 0.02787526476090352,
            "ave_precision_score": 0.7860871971693965,
            "fpr": 0.16465422612513722,
            "logloss": 1.242881038857465,
            "mae": 0.301321708207526,
            "precision": 0.714828897338403,
            "recall": 0.7752577319587629
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.6621447603070613,
            "auditor_fn_violation": 0.012744080350129054,
            "auditor_fp_violation": 0.009145677398914892,
            "ave_precision_score": 0.6381694022730432,
            "fpr": 0.29385964912280704,
            "logloss": 2.9825015588753443,
            "mae": 0.3829209740402935,
            "precision": 0.594553706505295,
            "recall": 0.837953091684435
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.7071176069110211,
            "auditor_fn_violation": 0.00798940781060803,
            "auditor_fp_violation": 0.0018243379044850926,
            "ave_precision_score": 0.6897921946096799,
            "fpr": 0.25905598243688255,
            "logloss": 2.489384519140937,
            "mae": 0.3518527593546398,
            "precision": 0.6363636363636364,
            "recall": 0.8515463917525773
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.6378751990784541,
            "auditor_fn_violation": 0.019365110537537877,
            "auditor_fp_violation": 0.011393113144033924,
            "ave_precision_score": 0.6159588679932464,
            "fpr": 0.26644736842105265,
            "logloss": 3.2110128313436537,
            "mae": 0.4060802264682933,
            "precision": 0.592964824120603,
            "recall": 0.7547974413646056
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.6895245908657471,
            "auditor_fn_violation": 0.015944866296241807,
            "auditor_fp_violation": 0.00547301371345527,
            "ave_precision_score": 0.6746949838782088,
            "fpr": 0.24039517014270034,
            "logloss": 2.613682116549349,
            "mae": 0.38202968421886085,
            "precision": 0.625,
            "recall": 0.7525773195876289
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6567982456140351,
            "auc_prc": 0.6889445626147286,
            "auditor_fn_violation": 0.013508584895073507,
            "auditor_fp_violation": 0.007692764642984444,
            "ave_precision_score": 0.6727057201389848,
            "fpr": 0.27631578947368424,
            "logloss": 2.661550348749082,
            "mae": 0.3644728826464739,
            "precision": 0.6181818181818182,
            "recall": 0.8699360341151386
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.7201137815909051,
            "auditor_fn_violation": 0.01394185612276076,
            "auditor_fp_violation": 0.017985704199584653,
            "ave_precision_score": 0.7092986752827118,
            "fpr": 0.2645444566410538,
            "logloss": 2.167491445608528,
            "mae": 0.34612050446523834,
            "precision": 0.6408345752608048,
            "recall": 0.8865979381443299
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.73221227854284,
            "auditor_fn_violation": 0.012683294056035618,
            "auditor_fp_violation": 0.011870816997346652,
            "ave_precision_score": 0.7271581322258001,
            "fpr": 0.2412280701754386,
            "logloss": 1.6512651462259826,
            "mae": 0.3521595028889982,
            "precision": 0.6321070234113713,
            "recall": 0.8059701492537313
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7738467958175833,
            "auditor_fn_violation": 0.01282152839861034,
            "auditor_fp_violation": 0.0062151172678220935,
            "ave_precision_score": 0.7689561209606595,
            "fpr": 0.21295279912184412,
            "logloss": 1.444918302798418,
            "mae": 0.320243995684326,
            "precision": 0.6766666666666666,
            "recall": 0.8371134020618557
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 29198,
        "test": {
            "accuracy": 0.4309210526315789,
            "auc_prc": 0.42375711626479257,
            "auditor_fn_violation": 0.022544701305502585,
            "auditor_fp_violation": 0.00420775414835056,
            "ave_precision_score": 0.4233766187122894,
            "fpr": 0.26864035087719296,
            "logloss": 2.4748033264703637,
            "mae": 0.5802279200976515,
            "precision": 0.4431818181818182,
            "recall": 0.4157782515991471
        },
        "train": {
            "accuracy": 0.4621295279912184,
            "auc_prc": 0.4609431358773267,
            "auditor_fn_violation": 0.030242058687066448,
            "auditor_fp_violation": 0.009227336208984615,
            "ave_precision_score": 0.4609273102403116,
            "fpr": 0.23161361141602635,
            "logloss": 2.2627180606276602,
            "mae": 0.5500570174201621,
            "precision": 0.4940047961630695,
            "recall": 0.4247422680412371
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6502192982456141,
            "auc_prc": 0.7209064712787088,
            "auditor_fn_violation": 0.022554053043055397,
            "auditor_fp_violation": 0.016264207358124443,
            "ave_precision_score": 0.7176273705471843,
            "fpr": 0.21600877192982457,
            "logloss": 1.7784001077362974,
            "mae": 0.35778672841233583,
            "precision": 0.6378676470588235,
            "recall": 0.7398720682302772
        },
        "train": {
            "accuracy": 0.6663007683863886,
            "auc_prc": 0.7511917173244995,
            "auditor_fn_violation": 0.021935790510032024,
            "auditor_fp_violation": 0.0018784496219909992,
            "ave_precision_score": 0.7495164100740217,
            "fpr": 0.2030735455543359,
            "logloss": 1.454019700304797,
            "mae": 0.33789933289729995,
            "precision": 0.6642468239564429,
            "recall": 0.7546391752577319
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 29198,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.6439209841368083,
            "auditor_fn_violation": 0.016999120936670036,
            "auditor_fp_violation": 0.0032597718902221753,
            "ave_precision_score": 0.6089617792072883,
            "fpr": 0.3333333333333333,
            "logloss": 4.186099031501691,
            "mae": 0.4031042620224205,
            "precision": 0.5766016713091922,
            "recall": 0.8827292110874201
        },
        "train": {
            "accuracy": 0.6201975850713501,
            "auc_prc": 0.6661774574523851,
            "auditor_fn_violation": 0.010191587357271383,
            "auditor_fp_violation": 0.014811150105904369,
            "ave_precision_score": 0.6351642454315025,
            "fpr": 0.3238199780461032,
            "logloss": 3.7354266049372664,
            "mae": 0.383099320566254,
            "precision": 0.5953360768175583,
            "recall": 0.8948453608247423
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.6475951318048894,
            "auditor_fn_violation": 0.01477808326787117,
            "auditor_fp_violation": 0.005801750425725732,
            "ave_precision_score": 0.619331049515386,
            "fpr": 0.3059210526315789,
            "logloss": 3.6887852100728726,
            "mae": 0.3906658174868762,
            "precision": 0.588495575221239,
            "recall": 0.8507462686567164
        },
        "train": {
            "accuracy": 0.6410537870472008,
            "auc_prc": 0.6738031496084965,
            "auditor_fn_violation": 0.004354566750031121,
            "auditor_fp_violation": 0.007678710388934404,
            "ave_precision_score": 0.648298081724866,
            "fpr": 0.29088913282107576,
            "logloss": 3.2584565375172385,
            "mae": 0.37157701547208233,
            "precision": 0.6148255813953488,
            "recall": 0.8721649484536083
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7946282498439323,
            "auditor_fn_violation": 0.038089627052706396,
            "auditor_fp_violation": 0.0347288226208863,
            "ave_precision_score": 0.7949890002646605,
            "fpr": 0.15570175438596492,
            "logloss": 1.243243041350515,
            "mae": 0.29063552844993884,
            "precision": 0.7119675456389453,
            "recall": 0.7484008528784648
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8087557025945239,
            "auditor_fn_violation": 0.03379542136770514,
            "auditor_fp_violation": 0.027571208443489335,
            "ave_precision_score": 0.8094833708391675,
            "fpr": 0.14270032930845225,
            "logloss": 0.9323622068946847,
            "mae": 0.26549581458449484,
            "precision": 0.7537878787878788,
            "recall": 0.8206185567010309
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6928216537429572,
            "auditor_fn_violation": 0.01158680282796544,
            "auditor_fp_violation": 0.013519266563700455,
            "ave_precision_score": 0.6776477054217244,
            "fpr": 0.2807017543859649,
            "logloss": 2.6640442715533923,
            "mae": 0.36295709889295086,
            "precision": 0.6091603053435114,
            "recall": 0.8507462686567164
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7225992232269505,
            "auditor_fn_violation": 0.010162164608960361,
            "auditor_fp_violation": 0.008943893879191728,
            "ave_precision_score": 0.7109723819011319,
            "fpr": 0.2601536772777168,
            "logloss": 2.2340287720018166,
            "mae": 0.34065201224540503,
            "precision": 0.643609022556391,
            "recall": 0.8824742268041237
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.6634701410994637,
            "auditor_fn_violation": 0.0031141286050948296,
            "auditor_fp_violation": 0.0030939368737871826,
            "ave_precision_score": 0.6309574388453442,
            "fpr": 0.3125,
            "logloss": 3.6362268585755886,
            "mae": 0.3854613818988561,
            "precision": 0.5899280575539568,
            "recall": 0.8742004264392325
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.6817728153310012,
            "auditor_fn_violation": 0.0027770547828940687,
            "auditor_fp_violation": 0.0059935169008930944,
            "ave_precision_score": 0.652967392399786,
            "fpr": 0.2843029637760702,
            "logloss": 3.288521345884977,
            "mae": 0.3642131571150462,
            "precision": 0.630527817403709,
            "recall": 0.911340206185567
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6008771929824561,
            "auc_prc": 0.575004664639376,
            "auditor_fn_violation": 0.02280654995698201,
            "auditor_fp_violation": 0.00601461328264226,
            "ave_precision_score": 0.5697895803002396,
            "fpr": 0.3190789473684211,
            "logloss": 2.3326434469099944,
            "mae": 0.40931954825529787,
            "precision": 0.5764192139737991,
            "recall": 0.8443496801705757
        },
        "train": {
            "accuracy": 0.6399560922063666,
            "auc_prc": 0.6420681400011466,
            "auditor_fn_violation": 0.00815915443547931,
            "auditor_fp_violation": 0.007714784867271689,
            "ave_precision_score": 0.6378070608885182,
            "fpr": 0.283205268935236,
            "logloss": 1.8549245879094682,
            "mae": 0.37953063594580744,
            "precision": 0.6166419019316494,
            "recall": 0.8556701030927835
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.7014442790392257,
            "auditor_fn_violation": 0.014474151797403958,
            "auditor_fp_violation": 0.005012177735535238,
            "ave_precision_score": 0.6906098634077539,
            "fpr": 0.26535087719298245,
            "logloss": 2.1199888942683023,
            "mae": 0.3661118037930143,
            "precision": 0.615262321144674,
            "recall": 0.8251599147121536
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7518727583803927,
            "auditor_fn_violation": 0.01176004617108197,
            "auditor_fp_violation": 0.007467417015816082,
            "ave_precision_score": 0.7442020464205492,
            "fpr": 0.23710208562019758,
            "logloss": 1.7629909868188562,
            "mae": 0.336007550073592,
            "precision": 0.6593059936908517,
            "recall": 0.8618556701030928
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8059497025713683,
            "auditor_fn_violation": 0.04249429544009277,
            "auditor_fp_violation": 0.04394380420577403,
            "ave_precision_score": 0.8055033155553029,
            "fpr": 0.14035087719298245,
            "logloss": 2.4198815075609836,
            "mae": 0.2849004078975599,
            "precision": 0.7293868921775899,
            "recall": 0.7356076759061834
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8183347958903662,
            "auditor_fn_violation": 0.050405694433442345,
            "auditor_fp_violation": 0.03177903866668729,
            "ave_precision_score": 0.8169433309622864,
            "fpr": 0.12403951701427003,
            "logloss": 2.4212207605108,
            "mae": 0.2694889779623187,
            "precision": 0.7650727650727651,
            "recall": 0.7587628865979381
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.7216397178087534,
            "auditor_fn_violation": 0.013211667227770927,
            "auditor_fp_violation": 0.020046235792641875,
            "ave_precision_score": 0.7219725811284237,
            "fpr": 0.10855263157894737,
            "logloss": 5.401144520917699,
            "mae": 0.350354408044529,
            "precision": 0.7171428571428572,
            "recall": 0.535181236673774
        },
        "train": {
            "accuracy": 0.6531284302963776,
            "auc_prc": 0.7215445110198584,
            "auditor_fn_violation": 0.004030916518609892,
            "auditor_fp_violation": 0.00451961678597012,
            "ave_precision_score": 0.7215557529022223,
            "fpr": 0.10318331503841932,
            "logloss": 5.7118165865338915,
            "mae": 0.35898690283123624,
            "precision": 0.7366946778711485,
            "recall": 0.5422680412371134
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.6672488663993419,
            "auditor_fn_violation": 0.013396364044439458,
            "auditor_fp_violation": 0.010526810819373494,
            "ave_precision_score": 0.6418848680972988,
            "fpr": 0.30153508771929827,
            "logloss": 3.373997111491108,
            "mae": 0.37906303673514574,
            "precision": 0.5943952802359882,
            "recall": 0.8592750533049041
        },
        "train": {
            "accuracy": 0.6487376509330406,
            "auc_prc": 0.6973641776456898,
            "auditor_fn_violation": 0.010793622053481505,
            "auditor_fp_violation": 0.00965249970367394,
            "ave_precision_score": 0.6763665680123353,
            "fpr": 0.287596048298573,
            "logloss": 2.8879567071056926,
            "mae": 0.35921589196651765,
            "precision": 0.6197387518142236,
            "recall": 0.8804123711340206
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.6970104119818694,
            "auditor_fn_violation": 0.017515804436464298,
            "auditor_fp_violation": 0.013348481248267404,
            "ave_precision_score": 0.6825401427218192,
            "fpr": 0.26973684210526316,
            "logloss": 2.5115181187081608,
            "mae": 0.36359263977829,
            "precision": 0.615625,
            "recall": 0.8400852878464818
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7286775975687566,
            "auditor_fn_violation": 0.01041338961376985,
            "auditor_fp_violation": 0.010729580556886886,
            "ave_precision_score": 0.7173289341646407,
            "fpr": 0.2535675082327113,
            "logloss": 2.0645920609252415,
            "mae": 0.3391156914775393,
            "precision": 0.65,
            "recall": 0.8845360824742268
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.6531800088348424,
            "auditor_fn_violation": 0.02040081547151461,
            "auditor_fp_violation": 0.006940319195279396,
            "ave_precision_score": 0.6522078721664092,
            "fpr": 0.2730263157894737,
            "logloss": 1.8408065551985637,
            "mae": 0.3677777968734346,
            "precision": 0.6133540372670807,
            "recall": 0.8422174840085288
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.7071832066371264,
            "auditor_fn_violation": 0.0051286113594441384,
            "auditor_fp_violation": 0.008840823941085232,
            "ave_precision_score": 0.7054995653078626,
            "fpr": 0.2491767288693743,
            "logloss": 1.4843221051283153,
            "mae": 0.3436675276752133,
            "precision": 0.6458658346333853,
            "recall": 0.8536082474226804
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8252593283545016,
            "auditor_fn_violation": 0.019432910634795946,
            "auditor_fp_violation": 0.024335669874460418,
            "ave_precision_score": 0.8258269009388581,
            "fpr": 0.11951754385964912,
            "logloss": 0.8718927879633243,
            "mae": 0.2828357090328944,
            "precision": 0.7528344671201814,
            "recall": 0.7078891257995735
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.846976226861722,
            "auditor_fn_violation": 0.018995778967261536,
            "auditor_fp_violation": 0.0168519348804131,
            "ave_precision_score": 0.8471802597814073,
            "fpr": 0.10318331503841932,
            "logloss": 0.809045333821279,
            "mae": 0.2698552733881521,
            "precision": 0.7911111111111111,
            "recall": 0.734020618556701
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6546052631578947,
            "auc_prc": 0.7426508195953709,
            "auditor_fn_violation": 0.024910690906370405,
            "auditor_fp_violation": 0.026286087679695855,
            "ave_precision_score": 0.7373919227952985,
            "fpr": 0.2598684210526316,
            "logloss": 2.039601260148356,
            "mae": 0.34770615746774797,
            "precision": 0.6226114649681529,
            "recall": 0.8336886993603412
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7773246677326129,
            "auditor_fn_violation": 0.01887808797401745,
            "auditor_fp_violation": 0.018014048432563923,
            "ave_precision_score": 0.7713654937895839,
            "fpr": 0.24259055982436883,
            "logloss": 1.6983736696277025,
            "mae": 0.32461787464748676,
            "precision": 0.6584234930448223,
            "recall": 0.8783505154639175
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6359649122807017,
            "auc_prc": 0.6843031761481029,
            "auditor_fn_violation": 0.015596360303744438,
            "auditor_fp_violation": 0.005259692685438211,
            "ave_precision_score": 0.6774339884252842,
            "fpr": 0.27960526315789475,
            "logloss": 2.232944352469862,
            "mae": 0.3682496223857626,
            "precision": 0.6058732612055642,
            "recall": 0.835820895522388
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.7238375694317153,
            "auditor_fn_violation": 0.012300972082338433,
            "auditor_fp_violation": 0.015731049303504892,
            "ave_precision_score": 0.7180096872782594,
            "fpr": 0.2579582875960483,
            "logloss": 1.8200297171917261,
            "mae": 0.3434984629450441,
            "precision": 0.6412213740458015,
            "recall": 0.865979381443299
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 29198,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.6439464818485953,
            "auditor_fn_violation": 0.008192122096285491,
            "auditor_fp_violation": 0.008900637598510952,
            "ave_precision_score": 0.5501690457475482,
            "fpr": 0.3991228070175439,
            "logloss": 7.791762668969538,
            "mae": 0.4287691730344118,
            "precision": 0.5495049504950495,
            "recall": 0.9466950959488273
        },
        "train": {
            "accuracy": 0.5971459934138309,
            "auc_prc": 0.6617255024284678,
            "auditor_fn_violation": 0.008315321330360883,
            "auditor_fp_violation": 0.009608694979978665,
            "ave_precision_score": 0.5713271495935269,
            "fpr": 0.38309549945115257,
            "logloss": 7.260571557890733,
            "mae": 0.4032725996514686,
            "precision": 0.5723039215686274,
            "recall": 0.9628865979381444
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.6913009029702397,
            "auditor_fn_violation": 0.020064152919612468,
            "auditor_fp_violation": 0.015241970615025156,
            "ave_precision_score": 0.6787001790275462,
            "fpr": 0.26644736842105265,
            "logloss": 2.4223956261988984,
            "mae": 0.3671649971267643,
            "precision": 0.6130573248407644,
            "recall": 0.8208955223880597
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.7249528174658253,
            "auditor_fn_violation": 0.010603505833625676,
            "auditor_fp_violation": 0.002097473240467344,
            "ave_precision_score": 0.7171603977357753,
            "fpr": 0.23600439077936333,
            "logloss": 1.9599231378659705,
            "mae": 0.3379184262690082,
            "precision": 0.656,
            "recall": 0.845360824742268
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.6401898077944709,
            "auditor_fn_violation": 0.03643904537463061,
            "auditor_fp_violation": 0.017917607223476302,
            "ave_precision_score": 0.6405171731311751,
            "fpr": 0.22149122807017543,
            "logloss": 1.9262648249763592,
            "mae": 0.36782193133905705,
            "precision": 0.6231343283582089,
            "recall": 0.7121535181236673
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7037446790727261,
            "auditor_fn_violation": 0.032908212341711277,
            "auditor_fp_violation": 0.01640358064964982,
            "ave_precision_score": 0.7034019742024863,
            "fpr": 0.18111964873765093,
            "logloss": 1.4552858591823739,
            "mae": 0.33867668317239386,
            "precision": 0.6875,
            "recall": 0.7484536082474227
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7892190255275668,
            "auditor_fn_violation": 0.03529111959001983,
            "auditor_fp_violation": 0.0332635341174607,
            "ave_precision_score": 0.7882804466924195,
            "fpr": 0.15899122807017543,
            "logloss": 1.321726648458566,
            "mae": 0.28892199135203517,
            "precision": 0.7117296222664016,
            "recall": 0.7633262260127932
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8053576919328348,
            "auditor_fn_violation": 0.027252254801000377,
            "auditor_fp_violation": 0.026025159371891798,
            "ave_precision_score": 0.804319179104587,
            "fpr": 0.14709110867178923,
            "logloss": 1.2021274312169303,
            "mae": 0.2721655560437146,
            "precision": 0.7447619047619047,
            "recall": 0.8061855670103093
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6019736842105263,
            "auc_prc": 0.6518596303794785,
            "auditor_fn_violation": 0.013286481128193618,
            "auditor_fp_violation": 0.012979683972911972,
            "ave_precision_score": 0.550540126141996,
            "fpr": 0.33881578947368424,
            "logloss": 7.943917732345142,
            "mae": 0.4080286183445179,
            "precision": 0.5732044198895028,
            "recall": 0.8848614072494669
        },
        "train": {
            "accuracy": 0.6311745334796927,
            "auc_prc": 0.6669645953218366,
            "auditor_fn_violation": 0.015102923036880284,
            "auditor_fp_violation": 0.009910174548940191,
            "ave_precision_score": 0.5651349670133491,
            "fpr": 0.32711306256860595,
            "logloss": 7.742909617450363,
            "mae": 0.3854942026397466,
            "precision": 0.6,
            "recall": 0.9216494845360824
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 29198,
        "test": {
            "accuracy": 0.5274122807017544,
            "auc_prc": 0.7540656227052279,
            "auditor_fn_violation": 0.001802547413309393,
            "auditor_fp_violation": 0.005603738465803354,
            "ave_precision_score": 0.5870914736728279,
            "fpr": 0.4692982456140351,
            "logloss": 12.283472917950803,
            "mae": 0.4735059086659181,
            "precision": 0.5212527964205816,
            "recall": 0.9936034115138592
        },
        "train": {
            "accuracy": 0.5411635565312843,
            "auc_prc": 0.7750855467708816,
            "auditor_fn_violation": 0.001068272092523227,
            "auditor_fp_violation": 0.004236174456177251,
            "ave_precision_score": 0.615751173848516,
            "fpr": 0.4566410537870472,
            "logloss": 11.724021703207368,
            "mae": 0.46022795764038,
            "precision": 0.5372636262513905,
            "recall": 0.9958762886597938
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8168836264979066,
            "auditor_fn_violation": 0.025747671417349346,
            "auditor_fp_violation": 0.01778394915052869,
            "ave_precision_score": 0.8173661917064132,
            "fpr": 0.12390350877192982,
            "logloss": 1.2416893385288197,
            "mae": 0.281280447888235,
            "precision": 0.7538126361655774,
            "recall": 0.7377398720682303
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8359323556968836,
            "auditor_fn_violation": 0.021833942535109258,
            "auditor_fp_violation": 0.014548321763732787,
            "ave_precision_score": 0.8362793996787747,
            "fpr": 0.1119648737650933,
            "logloss": 0.9740670112128537,
            "mae": 0.2639171074609225,
            "precision": 0.782051282051282,
            "recall": 0.7546391752577319
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6491228070175439,
            "auc_prc": 0.7311090243712635,
            "auditor_fn_violation": 0.032193356525642466,
            "auditor_fp_violation": 0.022682270009108548,
            "ave_precision_score": 0.7247630794118545,
            "fpr": 0.25,
            "logloss": 2.164708082673687,
            "mae": 0.3554233084952484,
            "precision": 0.6231404958677685,
            "recall": 0.8038379530916845
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.764679832784916,
            "auditor_fn_violation": 0.024945963991082645,
            "auditor_fp_violation": 0.03267832387666653,
            "ave_precision_score": 0.7588110688416791,
            "fpr": 0.23600439077936333,
            "logloss": 1.7975089893831955,
            "mae": 0.32946695987623614,
            "precision": 0.6598101265822784,
            "recall": 0.8597938144329897
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6392543859649122,
            "auc_prc": 0.6920393903448522,
            "auditor_fn_violation": 0.017478397486252947,
            "auditor_fp_violation": 0.012680190883529369,
            "ave_precision_score": 0.6813211779732681,
            "fpr": 0.26864035087719296,
            "logloss": 2.3173314828339246,
            "mae": 0.3696463008916482,
            "precision": 0.6111111111111112,
            "recall": 0.8208955223880597
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7169231834039932,
            "auditor_fn_violation": 0.013070490115088216,
            "auditor_fp_violation": 0.0008915549646212486,
            "ave_precision_score": 0.7087741041338251,
            "fpr": 0.2414928649835346,
            "logloss": 1.996573277628285,
            "mae": 0.3463037379362666,
            "precision": 0.6513470681458003,
            "recall": 0.8474226804123711
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6359649122807017,
            "auc_prc": 0.6786158572412889,
            "auditor_fn_violation": 0.013863950922081324,
            "auditor_fp_violation": 0.01139558829353293,
            "ave_precision_score": 0.6603501324730706,
            "fpr": 0.28399122807017546,
            "logloss": 2.865178456779943,
            "mae": 0.37153197739954913,
            "precision": 0.6045801526717557,
            "recall": 0.8443496801705757
        },
        "train": {
            "accuracy": 0.6641053787047201,
            "auc_prc": 0.7027561618728662,
            "auditor_fn_violation": 0.007955458485633778,
            "auditor_fp_violation": 0.00391665764804709,
            "ave_precision_score": 0.6861995333815617,
            "fpr": 0.265642151481888,
            "logloss": 2.5191640176813452,
            "mae": 0.3494293849978808,
            "precision": 0.6349924585218703,
            "recall": 0.8680412371134021
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.798827547663644,
            "auditor_fn_violation": 0.048610331799648375,
            "auditor_fp_violation": 0.04866391430042375,
            "ave_precision_score": 0.7994081190026708,
            "fpr": 0.15460526315789475,
            "logloss": 1.490509539901309,
            "mae": 0.2917486420328354,
            "precision": 0.7116564417177914,
            "recall": 0.7420042643923241
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7941093650973372,
            "auditor_fn_violation": 0.04921067819434857,
            "auditor_fp_violation": 0.042418433027730976,
            "ave_precision_score": 0.7942707190569995,
            "fpr": 0.14709110867178923,
            "logloss": 1.5041203773162157,
            "mae": 0.28655804634090243,
            "precision": 0.7309236947791165,
            "recall": 0.7505154639175258
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.7422514884798832,
            "auditor_fn_violation": 0.025301125949201364,
            "auditor_fp_violation": 0.02620935804522594,
            "ave_precision_score": 0.7370484583069561,
            "fpr": 0.2598684210526316,
            "logloss": 2.042786007297175,
            "mae": 0.34784111760637254,
            "precision": 0.6232114467408585,
            "recall": 0.835820895522388
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.7771556032070384,
            "auditor_fn_violation": 0.01999615240983625,
            "auditor_fp_violation": 0.02079436001298681,
            "ave_precision_score": 0.7712203249805331,
            "fpr": 0.24368825466520308,
            "logloss": 1.7009030167726438,
            "mae": 0.3249322285864161,
            "precision": 0.6568778979907264,
            "recall": 0.8762886597938144
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8029756866642648,
            "auditor_fn_violation": 0.018885833987954962,
            "auditor_fp_violation": 0.025736604490911256,
            "ave_precision_score": 0.8035360720211476,
            "fpr": 0.14802631578947367,
            "logloss": 1.2442431639423432,
            "mae": 0.28447895653362054,
            "precision": 0.7239263803680982,
            "recall": 0.7547974413646056
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8277281078545098,
            "auditor_fn_violation": 0.01949822897688051,
            "auditor_fp_violation": 0.0076838638858397436,
            "ave_precision_score": 0.8280298928481842,
            "fpr": 0.12733260153677278,
            "logloss": 0.9917443695116299,
            "mae": 0.2703229690465051,
            "precision": 0.7613168724279835,
            "recall": 0.7628865979381443
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 29198,
        "test": {
            "accuracy": 0.631578947368421,
            "auc_prc": 0.678413977074662,
            "auditor_fn_violation": 0.011958534395690723,
            "auditor_fp_violation": 0.008073937665835014,
            "ave_precision_score": 0.6572819297937764,
            "fpr": 0.29605263157894735,
            "logloss": 3.1011543127182377,
            "mae": 0.37255592401511617,
            "precision": 0.5988112927191679,
            "recall": 0.8592750533049041
        },
        "train": {
            "accuracy": 0.6553238199780461,
            "auc_prc": 0.7082243595847902,
            "auditor_fn_violation": 0.010198377222266231,
            "auditor_fp_violation": 0.015599635132419102,
            "ave_precision_score": 0.6893824573620788,
            "fpr": 0.2854006586169045,
            "logloss": 2.697084871049568,
            "mae": 0.34927551188718764,
            "precision": 0.6237337192474675,
            "recall": 0.8886597938144329
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 29198,
        "test": {
            "accuracy": 0.506578947368421,
            "auc_prc": 0.46930600253732024,
            "auditor_fn_violation": 0.0228299293008641,
            "auditor_fp_violation": 0.008024434675854423,
            "ave_precision_score": 0.46092071846107846,
            "fpr": 0.2324561403508772,
            "logloss": 6.200292119938901,
            "mae": 0.5091283945120266,
            "precision": 0.5214446952595937,
            "recall": 0.4925373134328358
        },
        "train": {
            "accuracy": 0.5225027442371021,
            "auc_prc": 0.49654513414025103,
            "auditor_fn_violation": 0.005961501465479191,
            "auditor_fp_violation": 0.0021541617064258995,
            "ave_precision_score": 0.4858644629230273,
            "fpr": 0.21624588364434688,
            "logloss": 5.993069705121805,
            "mae": 0.4967007450401892,
            "precision": 0.5563063063063063,
            "recall": 0.5092783505154639
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8083467364636212,
            "auditor_fn_violation": 0.024489862716492727,
            "auditor_fp_violation": 0.031689339036077784,
            "ave_precision_score": 0.8087880687451696,
            "fpr": 0.13596491228070176,
            "logloss": 0.9793523790626092,
            "mae": 0.28723401782271074,
            "precision": 0.7394957983193278,
            "recall": 0.7505330490405118
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8249835505170438,
            "auditor_fn_violation": 0.02378716036529474,
            "auditor_fp_violation": 0.018557742356075713,
            "ave_precision_score": 0.8252937784108942,
            "fpr": 0.12952799121844127,
            "logloss": 0.8739145626412953,
            "mae": 0.2660807723371362,
            "precision": 0.7611336032388664,
            "recall": 0.7752577319587629
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6469298245614035,
            "auc_prc": 0.6940936324350664,
            "auditor_fn_violation": 0.012956832379456102,
            "auditor_fp_violation": 0.013603421646667474,
            "ave_precision_score": 0.680191045814188,
            "fpr": 0.27850877192982454,
            "logloss": 2.5951291443812785,
            "mae": 0.36385411651417204,
            "precision": 0.6122137404580152,
            "recall": 0.8550106609808102
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.7241725296209445,
            "auditor_fn_violation": 0.010999581291658653,
            "auditor_fp_violation": 0.013442896677540563,
            "ave_precision_score": 0.7133458558513354,
            "fpr": 0.2689352360043908,
            "logloss": 2.1870702352690636,
            "mae": 0.34032212275790574,
            "precision": 0.6370370370370371,
            "recall": 0.8865979381443299
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.6645473864732496,
            "auditor_fn_violation": 0.012318576291474957,
            "auditor_fp_violation": 0.010202566235000597,
            "ave_precision_score": 0.6377462140029063,
            "fpr": 0.31469298245614036,
            "logloss": 3.2606302158002616,
            "mae": 0.37669328241289196,
            "precision": 0.5888252148997135,
            "recall": 0.8763326226012793
        },
        "train": {
            "accuracy": 0.6630076838638859,
            "auc_prc": 0.6987300915620196,
            "auditor_fn_violation": 0.013452985843131484,
            "auditor_fp_violation": 0.014362795875141093,
            "ave_precision_score": 0.6780130220807472,
            "fpr": 0.2864983534577388,
            "logloss": 2.6906308638498415,
            "mae": 0.3589173734679858,
            "precision": 0.6271428571428571,
            "recall": 0.9051546391752577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.653178404616522,
            "auditor_fn_violation": 0.02040081547151461,
            "auditor_fp_violation": 0.006940319195279396,
            "ave_precision_score": 0.6522106510678243,
            "fpr": 0.2730263157894737,
            "logloss": 1.8408813305456144,
            "mae": 0.3677794010135819,
            "precision": 0.6133540372670807,
            "recall": 0.8422174840085288
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.7071771424308976,
            "auditor_fn_violation": 0.0051286113594441384,
            "auditor_fp_violation": 0.008840823941085232,
            "ave_precision_score": 0.7054869301224218,
            "fpr": 0.2491767288693743,
            "logloss": 1.4844061981739767,
            "mae": 0.3436686145448503,
            "precision": 0.6458658346333853,
            "recall": 0.8536082474226804
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.6707508858500504,
            "auditor_fn_violation": 0.017455018142370854,
            "auditor_fp_violation": 0.0032572967407231497,
            "ave_precision_score": 0.6533018166056649,
            "fpr": 0.2730263157894737,
            "logloss": 2.7051910862225235,
            "mae": 0.3743280584616875,
            "precision": 0.6109375,
            "recall": 0.8336886993603412
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.7017221341795851,
            "auditor_fn_violation": 0.01248203514886779,
            "auditor_fp_violation": 0.010507980189957905,
            "ave_precision_score": 0.6870976385350568,
            "fpr": 0.2491767288693743,
            "logloss": 2.282538691457935,
            "mae": 0.3483129262344799,
            "precision": 0.6464174454828661,
            "recall": 0.8556701030927835
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6282894736842105,
            "auc_prc": 0.676618560628607,
            "auditor_fn_violation": 0.013861612987693114,
            "auditor_fp_violation": 0.007863549958417494,
            "ave_precision_score": 0.6536590470435011,
            "fpr": 0.29714912280701755,
            "logloss": 3.2120866264504615,
            "mae": 0.37522145875775004,
            "precision": 0.5967261904761905,
            "recall": 0.8550106609808102
        },
        "train": {
            "accuracy": 0.6553238199780461,
            "auc_prc": 0.707155643461608,
            "auditor_fn_violation": 0.009643871581020065,
            "auditor_fp_violation": 0.01617167328891019,
            "ave_precision_score": 0.6870899719711812,
            "fpr": 0.28210757409440174,
            "logloss": 2.7710287133222566,
            "mae": 0.3520680713658352,
            "precision": 0.6248175182481752,
            "recall": 0.8824742268041237
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 29198,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.83401520052844,
            "auditor_fn_violation": 0.016711555006920287,
            "auditor_fp_violation": 0.022115460773830746,
            "ave_precision_score": 0.8342803094964675,
            "fpr": 0.12390350877192982,
            "logloss": 0.9487857692971036,
            "mae": 0.2782700819056201,
            "precision": 0.754880694143167,
            "recall": 0.7420042643923241
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8482195624258088,
            "auditor_fn_violation": 0.00929079860128782,
            "auditor_fp_violation": 0.011469107362801033,
            "ave_precision_score": 0.8484742395549281,
            "fpr": 0.11086717892425905,
            "logloss": 0.6984823764918927,
            "mae": 0.26196401851708706,
            "precision": 0.7887029288702929,
            "recall": 0.777319587628866
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 29198,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8230928662336856,
            "auditor_fn_violation": 0.020005704559907238,
            "auditor_fp_violation": 0.026474199041622116,
            "ave_precision_score": 0.8236500606733685,
            "fpr": 0.14692982456140352,
            "logloss": 0.9046500058565426,
            "mae": 0.27482831852000256,
            "precision": 0.7276422764227642,
            "recall": 0.7633262260127932
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.852169412367112,
            "auditor_fn_violation": 0.004922652121266996,
            "auditor_fp_violation": 0.013713455265070116,
            "ave_precision_score": 0.8523661472070772,
            "fpr": 0.12952799121844127,
            "logloss": 0.8474151710637307,
            "mae": 0.2610337178274491,
            "precision": 0.7686274509803922,
            "recall": 0.8082474226804124
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6359649122807017,
            "auc_prc": 0.6801549915887204,
            "auditor_fn_violation": 0.014747690120824454,
            "auditor_fp_violation": 0.004158251158369968,
            "ave_precision_score": 0.6619710766140008,
            "fpr": 0.28399122807017546,
            "logloss": 2.880944625783592,
            "mae": 0.37109619827881063,
            "precision": 0.6045801526717557,
            "recall": 0.8443496801705757
        },
        "train": {
            "accuracy": 0.6575192096597146,
            "auc_prc": 0.707385376990646,
            "auditor_fn_violation": 0.013772109497889484,
            "auditor_fp_violation": 0.007624598671428516,
            "ave_precision_score": 0.6915794275572698,
            "fpr": 0.2711306256860593,
            "logloss": 2.487821074427058,
            "mae": 0.35072580160132083,
            "precision": 0.6296851574212894,
            "recall": 0.865979381443299
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8064520156506583,
            "auditor_fn_violation": 0.01640294766767666,
            "auditor_fp_violation": 0.0190239990495426,
            "ave_precision_score": 0.8067144945020412,
            "fpr": 0.16885964912280702,
            "logloss": 1.1101786362314288,
            "mae": 0.2904960200667314,
            "precision": 0.7148148148148148,
            "recall": 0.8230277185501066
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8357222650518754,
            "auditor_fn_violation": 0.008073149478877861,
            "auditor_fp_violation": 0.01567951433445165,
            "ave_precision_score": 0.8359724274579232,
            "fpr": 0.150384193194292,
            "logloss": 0.9176110137033573,
            "mae": 0.2734021628658074,
            "precision": 0.7476979742173112,
            "recall": 0.8371134020618557
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.7155754415897818,
            "auditor_fn_violation": 0.028506433995436356,
            "auditor_fp_violation": 0.02707318522038731,
            "ave_precision_score": 0.7022358947647106,
            "fpr": 0.2642543859649123,
            "logloss": 2.7144255962644386,
            "mae": 0.3583446898814857,
            "precision": 0.6162420382165605,
            "recall": 0.8251599147121536
        },
        "train": {
            "accuracy": 0.6695938529088913,
            "auc_prc": 0.7457649120995091,
            "auditor_fn_violation": 0.028164359998642034,
            "auditor_fp_violation": 0.03287415675906887,
            "ave_precision_score": 0.7341486773517046,
            "fpr": 0.26125137211855104,
            "logloss": 2.302780927013265,
            "mae": 0.3406555664175748,
            "precision": 0.6393939393939394,
            "recall": 0.8701030927835052
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6359649122807017,
            "auc_prc": 0.674550865086037,
            "auditor_fn_violation": 0.016201885310290653,
            "auditor_fp_violation": 0.004336461922300118,
            "ave_precision_score": 0.6130260057437498,
            "fpr": 0.31359649122807015,
            "logloss": 5.154758205085217,
            "mae": 0.37456029890185,
            "precision": 0.5966149506346967,
            "recall": 0.9019189765458422
        },
        "train": {
            "accuracy": 0.6465422612513722,
            "auc_prc": 0.6978239363930695,
            "auditor_fn_violation": 0.013222130433306552,
            "auditor_fp_violation": 0.012811593306638237,
            "ave_precision_score": 0.6304989469009664,
            "fpr": 0.31394072447859495,
            "logloss": 5.15051640408095,
            "mae": 0.3562256790467211,
            "precision": 0.6108843537414966,
            "recall": 0.9257731958762887
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.719149652360453,
            "auditor_fn_violation": 0.017751935809673436,
            "auditor_fp_violation": 0.013469763573719866,
            "ave_precision_score": 0.7076464992464611,
            "fpr": 0.26644736842105265,
            "logloss": 2.272134490839329,
            "mae": 0.3632107855534733,
            "precision": 0.6136724960254372,
            "recall": 0.8230277185501066
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.7498852302497776,
            "auditor_fn_violation": 0.015089343306890584,
            "auditor_fp_violation": 0.0050916549424612064,
            "ave_precision_score": 0.7430243862886152,
            "fpr": 0.2414928649835346,
            "logloss": 1.8186004076182545,
            "mae": 0.3373315562597308,
            "precision": 0.6518987341772152,
            "recall": 0.8494845360824742
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7763467691401773,
            "auditor_fn_violation": 0.019164048180151876,
            "auditor_fp_violation": 0.01734089739020238,
            "ave_precision_score": 0.7504856777801409,
            "fpr": 0.19298245614035087,
            "logloss": 2.480637201897095,
            "mae": 0.31827161090101813,
            "precision": 0.6782449725776966,
            "recall": 0.7910447761194029
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7816465909862746,
            "auditor_fn_violation": 0.014270032930845227,
            "auditor_fp_violation": 0.01735955432558763,
            "ave_precision_score": 0.7509986912626695,
            "fpr": 0.17672886937431395,
            "logloss": 2.4422076773076338,
            "mae": 0.30122716820879014,
            "precision": 0.7067395264116576,
            "recall": 0.8
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 29198,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.6625181131516217,
            "auditor_fn_violation": 0.013541315976508438,
            "auditor_fp_violation": 0.011153023642628025,
            "ave_precision_score": 0.6377383695307954,
            "fpr": 0.30372807017543857,
            "logloss": 3.4392691203859567,
            "mae": 0.3815929312023784,
            "precision": 0.593245227606461,
            "recall": 0.8614072494669509
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.6890747895408547,
            "auditor_fn_violation": 0.010739303133522698,
            "auditor_fp_violation": 0.015893384456022636,
            "ave_precision_score": 0.6675271447094603,
            "fpr": 0.29418221734357847,
            "logloss": 3.017077309625134,
            "mae": 0.3628125049669713,
            "precision": 0.6171428571428571,
            "recall": 0.8907216494845361
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.6477245779975065,
            "auditor_fn_violation": 0.013097108442748667,
            "auditor_fp_violation": 0.010662944041820123,
            "ave_precision_score": 0.6235194703466295,
            "fpr": 0.25109649122807015,
            "logloss": 3.3348335903125332,
            "mae": 0.3857244064948214,
            "precision": 0.611864406779661,
            "recall": 0.7697228144989339
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.6758009670566486,
            "auditor_fn_violation": 0.018966356218950514,
            "auditor_fp_violation": 0.012770365331395639,
            "ave_precision_score": 0.6549992703669689,
            "fpr": 0.2502744237102086,
            "logloss": 2.7898411374748946,
            "mae": 0.3598480194183493,
            "precision": 0.632258064516129,
            "recall": 0.8082474226804124
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 29198,
        "test": {
            "accuracy": 0.555921052631579,
            "auc_prc": 0.5829834863106766,
            "auditor_fn_violation": 0.0413978042120226,
            "auditor_fp_violation": 0.01442764642984438,
            "ave_precision_score": 0.5840084121962681,
            "fpr": 0.26644736842105265,
            "logloss": 1.526487327128966,
            "mae": 0.44107068308033587,
            "precision": 0.5581818181818182,
            "recall": 0.6545842217484008
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.6322958889147683,
            "auditor_fn_violation": 0.03169735308429618,
            "auditor_fp_violation": 0.010049318965383969,
            "ave_precision_score": 0.6340207871842406,
            "fpr": 0.24368825466520308,
            "logloss": 1.1571985294855442,
            "mae": 0.411835741223623,
            "precision": 0.6021505376344086,
            "recall": 0.6927835051546392
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 29198,
        "test": {
            "accuracy": 0.569078947368421,
            "auc_prc": 0.517845206410983,
            "auditor_fn_violation": 0.012313900422698533,
            "auditor_fp_violation": 0.010947586234208545,
            "ave_precision_score": 0.5182203005399454,
            "fpr": 0.23026315789473684,
            "logloss": 2.8092921943706455,
            "mae": 0.45335019198556326,
            "precision": 0.5766129032258065,
            "recall": 0.6098081023454158
        },
        "train": {
            "accuracy": 0.5982436882546652,
            "auc_prc": 0.572027481889624,
            "auditor_fn_violation": 0.01426776964251361,
            "auditor_fp_violation": 0.0118427358884371,
            "ave_precision_score": 0.5681485785516566,
            "fpr": 0.21405049396267836,
            "logloss": 2.2760538245924407,
            "mae": 0.4222293190630942,
            "precision": 0.6168958742632613,
            "recall": 0.6474226804123712
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 29198,
        "test": {
            "accuracy": 0.5910087719298246,
            "auc_prc": 0.6492885513212463,
            "auditor_fn_violation": 0.0416993977481016,
            "auditor_fp_violation": 0.029800799968318106,
            "ave_precision_score": 0.6180755801434532,
            "fpr": 0.25548245614035087,
            "logloss": 4.363835372215322,
            "mae": 0.4126385440756338,
            "precision": 0.5854092526690391,
            "recall": 0.7014925373134329
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.6954740479758772,
            "auditor_fn_violation": 0.030758088426675118,
            "auditor_fp_violation": 0.01924058069603129,
            "ave_precision_score": 0.6706020816552094,
            "fpr": 0.23600439077936333,
            "logloss": 3.5109251912228165,
            "mae": 0.36875428576947067,
            "precision": 0.6293103448275862,
            "recall": 0.7525773195876289
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.687833834625714,
            "auditor_fn_violation": 0.013001253132832083,
            "auditor_fp_violation": 0.013147994138845997,
            "ave_precision_score": 0.6720347703180574,
            "fpr": 0.2850877192982456,
            "logloss": 2.7341947078877173,
            "mae": 0.36620236781853627,
            "precision": 0.6072507552870091,
            "recall": 0.8571428571428571
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.7158193013306963,
            "auditor_fn_violation": 0.010420179478764698,
            "auditor_fp_violation": 0.015043057466643995,
            "ave_precision_score": 0.702950424800624,
            "fpr": 0.270032930845225,
            "logloss": 2.3249412430194565,
            "mae": 0.34387719851939846,
            "precision": 0.6350148367952523,
            "recall": 0.8824742268041237
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.7627174262819942,
            "auditor_fn_violation": 0.025577002207010063,
            "auditor_fp_violation": 0.0237762860876797,
            "ave_precision_score": 0.7590975883209354,
            "fpr": 0.25,
            "logloss": 1.861817258341632,
            "mae": 0.33982289277551336,
            "precision": 0.6292682926829268,
            "recall": 0.8251599147121536
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7899700652990866,
            "auditor_fn_violation": 0.020109316826417103,
            "auditor_fp_violation": 0.02227856712172045,
            "ave_precision_score": 0.7863781371188359,
            "fpr": 0.2305159165751921,
            "logloss": 1.5260587089890685,
            "mae": 0.3145296093188631,
            "precision": 0.6698113207547169,
            "recall": 0.8783505154639175
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 29198,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.772526543667161,
            "auditor_fn_violation": 0.01968540754872255,
            "auditor_fp_violation": 0.021457071007088838,
            "ave_precision_score": 0.7690334469197857,
            "fpr": 0.22039473684210525,
            "logloss": 1.7069995404763827,
            "mae": 0.33281091627101794,
            "precision": 0.653448275862069,
            "recall": 0.8081023454157783
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7981765606070291,
            "auditor_fn_violation": 0.014790589247117139,
            "auditor_fp_violation": 0.011255237241230042,
            "ave_precision_score": 0.7955392305641258,
            "fpr": 0.20087815587266739,
            "logloss": 1.370317346714392,
            "mae": 0.30297214342761436,
            "precision": 0.6913996627318718,
            "recall": 0.845360824742268
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.6549322092306381,
            "auditor_fn_violation": 0.0165151685183107,
            "auditor_fp_violation": 0.013714803374123806,
            "ave_precision_score": 0.630057751869215,
            "fpr": 0.3059210526315789,
            "logloss": 3.294816906775882,
            "mae": 0.3817347763669404,
            "precision": 0.5909090909090909,
            "recall": 0.8592750533049041
        },
        "train": {
            "accuracy": 0.6641053787047201,
            "auc_prc": 0.7036784932785158,
            "auditor_fn_violation": 0.01548541876492356,
            "auditor_fp_violation": 0.02026354983173833,
            "ave_precision_score": 0.6818123644575012,
            "fpr": 0.29088913282107576,
            "logloss": 2.6673865712381954,
            "mae": 0.34771267031832764,
            "precision": 0.6262341325811002,
            "recall": 0.9154639175257732
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6392543859649122,
            "auc_prc": 0.6825048334710109,
            "auditor_fn_violation": 0.012531328320802004,
            "auditor_fp_violation": 0.011806463110371881,
            "ave_precision_score": 0.6628311938284325,
            "fpr": 0.28728070175438597,
            "logloss": 2.9867749325462754,
            "mae": 0.36868637633707035,
            "precision": 0.6054216867469879,
            "recall": 0.8571428571428571
        },
        "train": {
            "accuracy": 0.6608122941822173,
            "auc_prc": 0.7105718000752653,
            "auditor_fn_violation": 0.010999581291658653,
            "auditor_fp_violation": 0.018696886772519516,
            "ave_precision_score": 0.6936109476565688,
            "fpr": 0.278814489571899,
            "logloss": 2.5764549820728746,
            "mae": 0.34744859967394287,
            "precision": 0.6286549707602339,
            "recall": 0.8865979381443299
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 29198,
        "test": {
            "accuracy": 0.5975877192982456,
            "auc_prc": 0.6258276467797355,
            "auditor_fn_violation": 0.0038271985934986725,
            "auditor_fp_violation": 0.018142845827888013,
            "ave_precision_score": 0.5813162058418281,
            "fpr": 0.2236842105263158,
            "logloss": 5.83188385850634,
            "mae": 0.4035802698928995,
            "precision": 0.6,
            "recall": 0.652452025586354
        },
        "train": {
            "accuracy": 0.6377607025246982,
            "auc_prc": 0.6809719286759264,
            "auditor_fn_violation": 0.016282096257652756,
            "auditor_fp_violation": 0.004658761202413899,
            "ave_precision_score": 0.6330906369183049,
            "fpr": 0.19978046103183314,
            "logloss": 4.992288379197046,
            "mae": 0.36264736986564355,
            "precision": 0.649325626204239,
            "recall": 0.6948453608247422
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6162280701754386,
            "auc_prc": 0.6577877986395494,
            "auditor_fn_violation": 0.003677570792653275,
            "auditor_fp_violation": 0.006900716803294939,
            "ave_precision_score": 0.627700494123215,
            "fpr": 0.32456140350877194,
            "logloss": 3.584021291506739,
            "mae": 0.38948414620228183,
            "precision": 0.5836849507735584,
            "recall": 0.8848614072494669
        },
        "train": {
            "accuracy": 0.6509330406147091,
            "auc_prc": 0.6781984882749449,
            "auditor_fn_violation": 0.0030214899227087055,
            "auditor_fp_violation": 0.0008348664986626781,
            "ave_precision_score": 0.6525811945490807,
            "fpr": 0.3040614709110867,
            "logloss": 3.165263873243935,
            "mae": 0.37461131087260174,
            "precision": 0.6158113730929264,
            "recall": 0.9154639175257732
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7368405892015006,
            "auditor_fn_violation": 0.01926691729323309,
            "auditor_fp_violation": 0.017840877589006376,
            "ave_precision_score": 0.7317726945424786,
            "fpr": 0.20394736842105263,
            "logloss": 1.8573241225438066,
            "mae": 0.33327404097971114,
            "precision": 0.6612021857923497,
            "recall": 0.7739872068230277
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7571666656633809,
            "auditor_fn_violation": 0.01866533887084545,
            "auditor_fp_violation": 0.00521276211973635,
            "ave_precision_score": 0.7504899319290033,
            "fpr": 0.18990120746432493,
            "logloss": 1.5861264008663847,
            "mae": 0.3078054796698341,
            "precision": 0.6932624113475178,
            "recall": 0.8061855670103093
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6293859649122807,
            "auc_prc": 0.6717921092551142,
            "auditor_fn_violation": 0.0143338757341114,
            "auditor_fp_violation": 0.006472515939962778,
            "ave_precision_score": 0.6434796117163162,
            "fpr": 0.2993421052631579,
            "logloss": 3.3926738140645814,
            "mae": 0.37169605237727676,
            "precision": 0.5967503692762186,
            "recall": 0.8614072494669509
        },
        "train": {
            "accuracy": 0.6608122941822173,
            "auc_prc": 0.7001509009100702,
            "auditor_fn_violation": 0.013636312197992464,
            "auditor_fp_violation": 0.011203702272176793,
            "ave_precision_score": 0.6781399817136033,
            "fpr": 0.2810098792535675,
            "logloss": 2.860521076190982,
            "mae": 0.3546236889991564,
            "precision": 0.627906976744186,
            "recall": 0.8907216494845361
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.6681379404158659,
            "auditor_fn_violation": 0.01348988141996783,
            "auditor_fp_violation": 0.010308997663458887,
            "ave_precision_score": 0.6335493192308219,
            "fpr": 0.30701754385964913,
            "logloss": 3.81697134506081,
            "mae": 0.38084071825741406,
            "precision": 0.5912408759124088,
            "recall": 0.8635394456289979
        },
        "train": {
            "accuracy": 0.6509330406147091,
            "auc_prc": 0.692236159834855,
            "auditor_fn_violation": 0.010198377222266231,
            "auditor_fp_violation": 0.010714120066170918,
            "ave_precision_score": 0.6636357640084685,
            "fpr": 0.2897914379802415,
            "logloss": 3.3048903788186546,
            "mae": 0.3577268273751893,
            "precision": 0.6201438848920864,
            "recall": 0.8886597938144329
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.7751511766824143,
            "auditor_fn_violation": 0.05433125724759661,
            "auditor_fp_violation": 0.08106362124272307,
            "ave_precision_score": 0.774324179824597,
            "fpr": 0.2719298245614035,
            "logloss": 2.4606580520194057,
            "mae": 0.3518084943620053,
            "precision": 0.6118935837245696,
            "recall": 0.8336886993603412
        },
        "train": {
            "accuracy": 0.6641053787047201,
            "auc_prc": 0.7630552146243319,
            "auditor_fn_violation": 0.053900211617459004,
            "auditor_fp_violation": 0.07062352159057529,
            "ave_precision_score": 0.7618162028733616,
            "fpr": 0.2535675082327113,
            "logloss": 2.3031660190327887,
            "mae": 0.3366771000342212,
            "precision": 0.6396255850234009,
            "recall": 0.845360824742268
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.6954389059323594,
            "auditor_fn_violation": 0.014757041858377284,
            "auditor_fp_violation": 0.010504534473882238,
            "ave_precision_score": 0.6789378031350206,
            "fpr": 0.2741228070175439,
            "logloss": 2.701688563501089,
            "mae": 0.3698530668938562,
            "precision": 0.6081504702194357,
            "recall": 0.8272921108742004
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.7216351255378952,
            "auditor_fn_violation": 0.0131089660167257,
            "auditor_fp_violation": 0.005895600459691938,
            "ave_precision_score": 0.7081323015637422,
            "fpr": 0.24807903402854006,
            "logloss": 2.3146373100960242,
            "mae": 0.34227765414956446,
            "precision": 0.6463223787167449,
            "recall": 0.8515463917525773
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7734710307400448,
            "auditor_fn_violation": 0.04358611079938653,
            "auditor_fp_violation": 0.039151914775652455,
            "ave_precision_score": 0.7738176426162167,
            "fpr": 0.1611842105263158,
            "logloss": 1.3900072588958974,
            "mae": 0.29841190699796605,
            "precision": 0.7048192771084337,
            "recall": 0.7484008528784648
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7866659666628759,
            "auditor_fn_violation": 0.04443966639129993,
            "auditor_fp_violation": 0.02580355900496282,
            "ave_precision_score": 0.7865093531414895,
            "fpr": 0.14928649835345773,
            "logloss": 1.267465527780971,
            "mae": 0.2854319516897071,
            "precision": 0.7338551859099804,
            "recall": 0.7731958762886598
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.6800364197103821,
            "auditor_fn_violation": 0.009188082145662666,
            "auditor_fp_violation": 0.008576393014138057,
            "ave_precision_score": 0.6629122514564314,
            "fpr": 0.28399122807017546,
            "logloss": 2.426979929177074,
            "mae": 0.36720913720644466,
            "precision": 0.6093514328808446,
            "recall": 0.8614072494669509
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.730224748978991,
            "auditor_fn_violation": 0.015496735206581645,
            "auditor_fp_violation": 0.010075086449910592,
            "ave_precision_score": 0.7209968980936231,
            "fpr": 0.2491767288693743,
            "logloss": 1.898873397855502,
            "mae": 0.3424465276299452,
            "precision": 0.6507692307692308,
            "recall": 0.8721649484536083
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.6770613592073229,
            "auditor_fn_violation": 0.010546422025212285,
            "auditor_fp_violation": 0.009445170488297506,
            "ave_precision_score": 0.6555852056946702,
            "fpr": 0.2883771929824561,
            "logloss": 2.9733331028237084,
            "mae": 0.3664116379537062,
            "precision": 0.6062874251497006,
            "recall": 0.8635394456289979
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.7071510320569965,
            "auditor_fn_violation": 0.007559383027600801,
            "auditor_fp_violation": 0.011610828527697474,
            "ave_precision_score": 0.6907515692070354,
            "fpr": 0.270032930845225,
            "logloss": 2.4896806512917498,
            "mae": 0.34868772023856104,
            "precision": 0.6387665198237885,
            "recall": 0.8969072164948454
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.6901503509626765,
            "auditor_fn_violation": 0.013616129876931135,
            "auditor_fp_violation": 0.01455387905429488,
            "ave_precision_score": 0.6748520651544291,
            "fpr": 0.27850877192982454,
            "logloss": 2.729199117678175,
            "mae": 0.36642324737723814,
            "precision": 0.6068111455108359,
            "recall": 0.835820895522388
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.717468003339973,
            "auditor_fn_violation": 0.010501657858702912,
            "auditor_fp_violation": 0.011719051962709304,
            "ave_precision_score": 0.7035704796216691,
            "fpr": 0.25686059275521406,
            "logloss": 2.361060804747136,
            "mae": 0.3458597237515708,
            "precision": 0.6405529953917051,
            "recall": 0.8597938144329897
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.7912585671354417,
            "auditor_fn_violation": 0.027293046047955707,
            "auditor_fp_violation": 0.02813254920597205,
            "ave_precision_score": 0.7896700143620446,
            "fpr": 0.25109649122807015,
            "logloss": 1.6034957722307162,
            "mae": 0.3318754570577833,
            "precision": 0.6306451612903226,
            "recall": 0.8336886993603412
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.8150941581580655,
            "auditor_fn_violation": 0.0195231251485283,
            "auditor_fp_violation": 0.02288152625964348,
            "ave_precision_score": 0.814258779903127,
            "fpr": 0.22502744237102085,
            "logloss": 1.279357975875868,
            "mae": 0.3004773472393605,
            "precision": 0.6761453396524486,
            "recall": 0.8824742268041237
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.731806912342001,
            "auditor_fn_violation": 0.029570194142071606,
            "auditor_fp_violation": 0.028095421963486605,
            "ave_precision_score": 0.7251230839385041,
            "fpr": 0.2598684210526316,
            "logloss": 2.0514932250891023,
            "mae": 0.3485186360147854,
            "precision": 0.6261829652996845,
            "recall": 0.8464818763326226
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7679279750685992,
            "auditor_fn_violation": 0.021962949970011428,
            "auditor_fp_violation": 0.024360579871471794,
            "ave_precision_score": 0.7635947419719721,
            "fpr": 0.2491767288693743,
            "logloss": 1.6581822083458388,
            "mae": 0.3298796698381674,
            "precision": 0.6513056835637481,
            "recall": 0.8742268041237113
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 29198,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.6714584769339224,
            "auditor_fn_violation": 0.014696255564283845,
            "auditor_fp_violation": 0.01953635499584175,
            "ave_precision_score": 0.6515666865169916,
            "fpr": 0.2905701754385965,
            "logloss": 3.284067856786595,
            "mae": 0.3820622482133087,
            "precision": 0.5966514459665144,
            "recall": 0.835820895522388
        },
        "train": {
            "accuracy": 0.6564215148188803,
            "auc_prc": 0.7041351133987505,
            "auditor_fn_violation": 0.012038430635870857,
            "auditor_fp_violation": 0.019018980329102337,
            "ave_precision_score": 0.6863359659854283,
            "fpr": 0.27552140504939626,
            "logloss": 2.803518123289871,
            "mae": 0.3557514120161021,
            "precision": 0.6275964391691394,
            "recall": 0.8721649484536083
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.7354125710170991,
            "auditor_fn_violation": 0.03025287098342873,
            "auditor_fp_violation": 0.013977169221020956,
            "ave_precision_score": 0.7364665235531909,
            "fpr": 0.19188596491228072,
            "logloss": 1.0733661565767272,
            "mae": 0.34231319730072685,
            "precision": 0.6608527131782945,
            "recall": 0.7270788912579957
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7587778133945909,
            "auditor_fn_violation": 0.01895503977729243,
            "auditor_fp_violation": 0.013623269069226925,
            "ave_precision_score": 0.7597189273083818,
            "fpr": 0.17014270032930845,
            "logloss": 0.972859197062262,
            "mae": 0.31191491972719554,
            "precision": 0.7102803738317757,
            "recall": 0.7835051546391752
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6348684210526315,
            "auc_prc": 0.6759980695474348,
            "auditor_fn_violation": 0.013351943291063484,
            "auditor_fp_violation": 0.0025766306284899596,
            "ave_precision_score": 0.6567858066366833,
            "fpr": 0.28728070175438597,
            "logloss": 2.918343720505707,
            "mae": 0.372919179149934,
            "precision": 0.603030303030303,
            "recall": 0.8486140724946695
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.6994880842240037,
            "auditor_fn_violation": 0.005884549662204217,
            "auditor_fp_violation": 0.005947135428745177,
            "ave_precision_score": 0.6825992243620702,
            "fpr": 0.2678375411635565,
            "logloss": 2.539826005255048,
            "mae": 0.35117416275700436,
            "precision": 0.6347305389221557,
            "recall": 0.8742268041237113
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.7321007160041603,
            "auditor_fn_violation": 0.06113230838289756,
            "auditor_fp_violation": 0.03286008474911885,
            "ave_precision_score": 0.7316506503862106,
            "fpr": 0.08552631578947369,
            "logloss": 3.046402757584124,
            "mae": 0.3639156953195082,
            "precision": 0.7507987220447284,
            "recall": 0.5010660980810234
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.7622100439549575,
            "auditor_fn_violation": 0.0535788246743694,
            "auditor_fp_violation": 0.02637044366454858,
            "ave_precision_score": 0.7616157959363461,
            "fpr": 0.06476399560922064,
            "logloss": 2.9864000826410537,
            "mae": 0.35405625762996723,
            "precision": 0.8033333333333333,
            "recall": 0.49690721649484537
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7932980069959893,
            "auditor_fn_violation": 0.029827366924774622,
            "auditor_fp_violation": 0.03891677557324463,
            "ave_precision_score": 0.793616460969369,
            "fpr": 0.16228070175438597,
            "logloss": 1.3170536086869133,
            "mae": 0.2837803614772573,
            "precision": 0.7120622568093385,
            "recall": 0.7803837953091685
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8188491000304425,
            "auditor_fn_violation": 0.022490296151278195,
            "auditor_fp_violation": 0.027086779734388774,
            "ave_precision_score": 0.8195289854313975,
            "fpr": 0.14270032930845225,
            "logloss": 1.0531147307309896,
            "mae": 0.2721848451795709,
            "precision": 0.7485493230174082,
            "recall": 0.797938144329897
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7598322821913412,
            "auditor_fn_violation": 0.021069464706542477,
            "auditor_fp_violation": 0.022895132866025108,
            "ave_precision_score": 0.7568104108783184,
            "fpr": 0.23684210526315788,
            "logloss": 1.7651681033896165,
            "mae": 0.3387661095265537,
            "precision": 0.6411960132890365,
            "recall": 0.8230277185501066
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7891509695283047,
            "auditor_fn_violation": 0.01600371179286385,
            "auditor_fp_violation": 0.008518730384502415,
            "ave_precision_score": 0.7865509257553693,
            "fpr": 0.2239297475301866,
            "logloss": 1.421665105849164,
            "mae": 0.3122928105726702,
            "precision": 0.6741214057507987,
            "recall": 0.8701030927835052
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8079945684368023,
            "auditor_fn_violation": 0.03214192196910186,
            "auditor_fp_violation": 0.034503584016474596,
            "ave_precision_score": 0.8081234795921957,
            "fpr": 0.1513157894736842,
            "logloss": 1.367519403361706,
            "mae": 0.2822846639985372,
            "precision": 0.7212121212121212,
            "recall": 0.7611940298507462
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8238548079951622,
            "auditor_fn_violation": 0.025246981339187714,
            "auditor_fp_violation": 0.02510526017429127,
            "ave_precision_score": 0.8239098456112429,
            "fpr": 0.13721185510428102,
            "logloss": 1.0748865192713908,
            "mae": 0.2644980626745235,
            "precision": 0.7563352826510721,
            "recall": 0.8
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 29198,
        "test": {
            "accuracy": 0.5372807017543859,
            "auc_prc": 0.6022564520908483,
            "auditor_fn_violation": 0.021494968765196572,
            "auditor_fp_violation": 0.0078808760049107,
            "ave_precision_score": 0.6034232322536747,
            "fpr": 0.1962719298245614,
            "logloss": 2.285149308725749,
            "mae": 0.46783219163962925,
            "precision": 0.5580246913580247,
            "recall": 0.48187633262260127
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.6194024176579588,
            "auditor_fn_violation": 0.017241730510258346,
            "auditor_fp_violation": 0.0010049318965384023,
            "ave_precision_score": 0.6209829015201226,
            "fpr": 0.20087815587266739,
            "logloss": 2.2189662585192202,
            "mae": 0.45881401823765955,
            "precision": 0.5763888888888888,
            "recall": 0.51340206185567
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6567982456140351,
            "auc_prc": 0.7321128493841438,
            "auditor_fn_violation": 0.015760015710919093,
            "auditor_fp_violation": 0.01772702071205101,
            "ave_precision_score": 0.7286059834430748,
            "fpr": 0.23135964912280702,
            "logloss": 1.453877904020031,
            "mae": 0.35510226170915854,
            "precision": 0.6349480968858131,
            "recall": 0.7825159914712153
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7696363723102191,
            "auditor_fn_violation": 0.023148913055778746,
            "auditor_fp_violation": 0.013986590601052345,
            "ave_precision_score": 0.7654845471043896,
            "fpr": 0.21295279912184412,
            "logloss": 1.29673743633547,
            "mae": 0.32342164019631486,
            "precision": 0.6689419795221843,
            "recall": 0.8082474226804124
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6348684210526315,
            "auc_prc": 0.6781357319739838,
            "auditor_fn_violation": 0.015442056634122622,
            "auditor_fp_violation": 0.0030840362757910697,
            "ave_precision_score": 0.6607734141571275,
            "fpr": 0.27960526315789475,
            "logloss": 2.8538802142012436,
            "mae": 0.3803145783834481,
            "precision": 0.6052631578947368,
            "recall": 0.8336886993603412
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.7103079961792957,
            "auditor_fn_violation": 0.01141149976801295,
            "auditor_fp_violation": 0.0007266430636508511,
            "ave_precision_score": 0.694476303192414,
            "fpr": 0.25686059275521406,
            "logloss": 2.4878041240679383,
            "mae": 0.3523427403705386,
            "precision": 0.64,
            "recall": 0.8577319587628865
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6928216537429572,
            "auditor_fn_violation": 0.01158680282796544,
            "auditor_fp_violation": 0.013519266563700455,
            "ave_precision_score": 0.6776477054217244,
            "fpr": 0.2807017543859649,
            "logloss": 2.664047042987342,
            "mae": 0.3629571143752871,
            "precision": 0.6091603053435114,
            "recall": 0.8507462686567164
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7225992232269505,
            "auditor_fn_violation": 0.010162164608960361,
            "auditor_fp_violation": 0.008943893879191728,
            "ave_precision_score": 0.7109723819011319,
            "fpr": 0.2601536772777168,
            "logloss": 2.2340289733314656,
            "mae": 0.3406520262772654,
            "precision": 0.643609022556391,
            "recall": 0.8824742268041237
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.7246717741769646,
            "auditor_fn_violation": 0.023059046870908616,
            "auditor_fp_violation": 0.026753890935012483,
            "ave_precision_score": 0.7130216169481164,
            "fpr": 0.26973684210526316,
            "logloss": 2.506958225874546,
            "mae": 0.35687896387539636,
            "precision": 0.6144200626959248,
            "recall": 0.835820895522388
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.7592435687805583,
            "auditor_fn_violation": 0.020878834859166886,
            "auditor_fp_violation": 0.03087975345670805,
            "ave_precision_score": 0.7499380318227786,
            "fpr": 0.2623490669593853,
            "logloss": 2.0727180183772576,
            "mae": 0.3338134354416624,
            "precision": 0.6416791604197901,
            "recall": 0.8824742268041237
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6546052631578947,
            "auc_prc": 0.7337660573730932,
            "auditor_fn_violation": 0.024181255377249095,
            "auditor_fp_violation": 0.02683557086848046,
            "ave_precision_score": 0.7251785297849062,
            "fpr": 0.2631578947368421,
            "logloss": 2.2584500017780047,
            "mae": 0.35084882849491494,
            "precision": 0.6214511041009464,
            "recall": 0.8400852878464818
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.7676156435164072,
            "auditor_fn_violation": 0.02242692407799292,
            "auditor_fp_violation": 0.02507691594131199,
            "ave_precision_score": 0.7600410217884617,
            "fpr": 0.2535675082327113,
            "logloss": 1.86812605646987,
            "mae": 0.32979506781767703,
            "precision": 0.648936170212766,
            "recall": 0.8804123711340206
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6546052631578947,
            "auc_prc": 0.7375139316578546,
            "auditor_fn_violation": 0.02291877080761606,
            "auditor_fp_violation": 0.02091748841630035,
            "ave_precision_score": 0.7316138262002614,
            "fpr": 0.2576754385964912,
            "logloss": 2.055650970158338,
            "mae": 0.3481079467275152,
            "precision": 0.6233974358974359,
            "recall": 0.8294243070362474
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7666545335013565,
            "auditor_fn_violation": 0.018239840664501454,
            "auditor_fp_violation": 0.025741717042098926,
            "ave_precision_score": 0.761531830567977,
            "fpr": 0.2502744237102086,
            "logloss": 1.6942732256081434,
            "mae": 0.32810767900072835,
            "precision": 0.6513761467889908,
            "recall": 0.8783505154639175
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6326754385964912,
            "auc_prc": 0.6711513033683615,
            "auditor_fn_violation": 0.009622937941869603,
            "auditor_fp_violation": 0.005937883648172367,
            "ave_precision_score": 0.6434512640486483,
            "fpr": 0.2982456140350877,
            "logloss": 3.3462813466349264,
            "mae": 0.37143338938780596,
            "precision": 0.5988200589970502,
            "recall": 0.8656716417910447
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.7020186342897202,
            "auditor_fn_violation": 0.011769099324408434,
            "auditor_fp_violation": 0.011185665033008154,
            "ave_precision_score": 0.6816937144296933,
            "fpr": 0.2711306256860593,
            "logloss": 2.7722479382889587,
            "mae": 0.3537718287499935,
            "precision": 0.6351550960118169,
            "recall": 0.8865979381443299
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 29198,
        "test": {
            "accuracy": 0.643640350877193,
            "auc_prc": 0.6794593804986103,
            "auditor_fn_violation": 0.01950071073205402,
            "auditor_fp_violation": 0.008417983446200162,
            "ave_precision_score": 0.6809079702115042,
            "fpr": 0.2675438596491228,
            "logloss": 1.2725417221600885,
            "mae": 0.37259279631268233,
            "precision": 0.6139240506329114,
            "recall": 0.8272921108742004
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.7440485394104548,
            "auditor_fn_violation": 0.011230436701483586,
            "auditor_fp_violation": 0.0027931953226862185,
            "ave_precision_score": 0.7445980424054844,
            "fpr": 0.24588364434687157,
            "logloss": 1.142014311188142,
            "mae": 0.34315845279358154,
            "precision": 0.6483516483516484,
            "recall": 0.8515463917525773
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.7271528326826976,
            "auditor_fn_violation": 0.03737889499869076,
            "auditor_fp_violation": 0.035201576175201,
            "ave_precision_score": 0.713546973562121,
            "fpr": 0.20285087719298245,
            "logloss": 3.2690302785050322,
            "mae": 0.3311955897651857,
            "precision": 0.6586715867158671,
            "recall": 0.7611940298507462
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7383180202539517,
            "auditor_fn_violation": 0.04054228388425544,
            "auditor_fp_violation": 0.01966574419072062,
            "ave_precision_score": 0.7230960615504489,
            "fpr": 0.18441273326015367,
            "logloss": 3.114269480842261,
            "mae": 0.3107139913117218,
            "precision": 0.6983842010771992,
            "recall": 0.8020618556701031
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6491228070175439,
            "auc_prc": 0.722133741029864,
            "auditor_fn_violation": 0.02893427598847866,
            "auditor_fp_violation": 0.02928101857352187,
            "ave_precision_score": 0.7103939967285493,
            "fpr": 0.25877192982456143,
            "logloss": 2.567881183832193,
            "mae": 0.3556426065168112,
            "precision": 0.6199677938808373,
            "recall": 0.8208955223880597
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.7556129606141603,
            "auditor_fn_violation": 0.027711702332318625,
            "auditor_fp_violation": 0.029599109475734766,
            "ave_precision_score": 0.7462889302857323,
            "fpr": 0.2557628979143798,
            "logloss": 2.1367149512871486,
            "mae": 0.3352545863789669,
            "precision": 0.6448170731707317,
            "recall": 0.8721649484536083
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6876403503824757,
            "auditor_fn_violation": 0.011792541054127862,
            "auditor_fp_violation": 0.0035840164745950627,
            "ave_precision_score": 0.6727412614639291,
            "fpr": 0.28399122807017546,
            "logloss": 2.547429630572029,
            "mae": 0.36622988402545936,
            "precision": 0.6069802731411229,
            "recall": 0.8528784648187633
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.7222842625683839,
            "auditor_fn_violation": 0.008217999932101351,
            "auditor_fp_violation": 0.008740330751431389,
            "ave_precision_score": 0.7109655401024013,
            "fpr": 0.2678375411635565,
            "logloss": 2.097737214986059,
            "mae": 0.339485911353083,
            "precision": 0.6395864106351551,
            "recall": 0.8927835051546392
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.6686422301386414,
            "auditor_fn_violation": 0.012454176485991099,
            "auditor_fp_violation": 0.004217654746346678,
            "ave_precision_score": 0.6421686956344022,
            "fpr": 0.30043859649122806,
            "logloss": 3.42234797582056,
            "mae": 0.3797855132611216,
            "precision": 0.5952732644017725,
            "recall": 0.8592750533049041
        },
        "train": {
            "accuracy": 0.6487376509330406,
            "auc_prc": 0.6982394138340479,
            "auditor_fn_violation": 0.009666504464336236,
            "auditor_fp_violation": 0.006052782115304343,
            "ave_precision_score": 0.6760488193652103,
            "fpr": 0.2864983534577388,
            "logloss": 2.944186687138599,
            "mae": 0.3603186303105293,
            "precision": 0.6200873362445415,
            "recall": 0.8783505154639175
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 29198,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.78145259885471,
            "auditor_fn_violation": 0.025887947480641906,
            "auditor_fp_violation": 0.02288275711852996,
            "ave_precision_score": 0.7814380534101225,
            "fpr": 0.1600877192982456,
            "logloss": 1.3279924533502372,
            "mae": 0.3049352095026042,
            "precision": 0.7062374245472837,
            "recall": 0.7484008528784648
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8073548517700988,
            "auditor_fn_violation": 0.016288886122647596,
            "auditor_fp_violation": 0.011863349876058405,
            "ave_precision_score": 0.8076268143942759,
            "fpr": 0.1437980241492865,
            "logloss": 1.0615980222217043,
            "mae": 0.2850994252698721,
            "precision": 0.738,
            "recall": 0.7608247422680412
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 29198,
        "test": {
            "accuracy": 0.6348684210526315,
            "auc_prc": 0.6736595470748546,
            "auditor_fn_violation": 0.013964482100774323,
            "auditor_fp_violation": 0.015029107758108593,
            "ave_precision_score": 0.6498214179665613,
            "fpr": 0.29276315789473684,
            "logloss": 3.2712118360853135,
            "mae": 0.37349538452586917,
            "precision": 0.6014925373134329,
            "recall": 0.8592750533049041
        },
        "train": {
            "accuracy": 0.6586169045005489,
            "auc_prc": 0.7033049154707939,
            "auditor_fn_violation": 0.010739303133522698,
            "auditor_fp_violation": 0.01762495941621187,
            "ave_precision_score": 0.683457488118163,
            "fpr": 0.283205268935236,
            "logloss": 2.806065447021566,
            "mae": 0.35183884187958303,
            "precision": 0.6260869565217392,
            "recall": 0.8907216494845361
        }
    }
]
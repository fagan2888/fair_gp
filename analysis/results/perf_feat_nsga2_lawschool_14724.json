[
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(0)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.594889484540733,
            "mae": 0.5383771929824561,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.553735972386,
            "mae": 0.5082327113062569,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(1)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.594889484540733,
            "mae": 0.5383771929824561,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.553735972386,
            "mae": 0.5082327113062569,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(2)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5383771929824561,
            "auc_prc": 0.7691885964912281,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5383771929824561,
            "fpr": 0.4616228070175439,
            "logloss": 0.6930491730634971,
            "mae": 0.49995058173673196,
            "precision": 0.5383771929824561,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5082327113062569,
            "auc_prc": 0.7541163556531285,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5082327113062569,
            "fpr": 0.49176728869374314,
            "logloss": 0.6931268071288975,
            "mae": 0.499989398747977,
            "precision": 0.5082327113062569,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(3)",
        "seed": 14724,
        "test": {
            "accuracy": 0.45614035087719296,
            "auc_prc": 0.49292616190528615,
            "auditor_fn_violation": 0.010111837638903774,
            "auditor_fp_violation": 0.011540505063132893,
            "ave_precision_score": 0.5349248178932497,
            "fpr": 0.023026315789473683,
            "logloss": 0.6960107164800028,
            "mae": 0.5012309462028114,
            "precision": 0.43243243243243246,
            "recall": 0.032586558044806514
        },
        "train": {
            "accuracy": 0.4972557628979144,
            "auc_prc": 0.5478510938072856,
            "auditor_fn_violation": 0.012883096684866762,
            "auditor_fp_violation": 0.010455053316606554,
            "ave_precision_score": 0.5108874343332952,
            "fpr": 0.018660812294182216,
            "logloss": 0.6934743941395678,
            "mae": 0.49996147256520396,
            "precision": 0.5641025641025641,
            "recall": 0.047516198704103674
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(4)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.7324586284398099,
            "auditor_fn_violation": 0.02691428163075714,
            "auditor_fp_violation": 0.043752864941451024,
            "ave_precision_score": 0.735220727369264,
            "fpr": 0.1962719298245614,
            "logloss": 0.6278848277070331,
            "mae": 0.4001934974757636,
            "precision": 0.6703499079189686,
            "recall": 0.7413441955193483
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7259642263593636,
            "auditor_fn_violation": 0.021048239302216967,
            "auditor_fp_violation": 0.03453818409910617,
            "ave_precision_score": 0.7317045521035593,
            "fpr": 0.19319429198682767,
            "logloss": 0.6088781934291961,
            "mae": 0.39345507132723667,
            "precision": 0.6691729323308271,
            "recall": 0.7688984881209503
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(5)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.7245668906437779,
            "auditor_fn_violation": 0.09974497087933683,
            "auditor_fp_violation": 0.10417968912780763,
            "ave_precision_score": 0.5698373299718871,
            "fpr": 0.2741228070175439,
            "logloss": 0.6878556846319258,
            "mae": 0.49689194533908576,
            "precision": 0.5826377295492488,
            "recall": 0.7107942973523421
        },
        "train": {
            "accuracy": 0.5587266739846323,
            "auc_prc": 0.7070645790318465,
            "auditor_fn_violation": 0.09195031686158851,
            "auditor_fp_violation": 0.1028451858240552,
            "ave_precision_score": 0.538512507888318,
            "fpr": 0.300768386388584,
            "logloss": 0.6895254678455869,
            "mae": 0.4977201702958535,
            "precision": 0.5500821018062397,
            "recall": 0.7235421166306696
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(6)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.7625968148932826,
            "auditor_fn_violation": 0.007981384214099411,
            "auditor_fp_violation": 0.0016382256115347756,
            "ave_precision_score": 0.6491020054839489,
            "fpr": 0.005482456140350877,
            "logloss": 0.7622404926269825,
            "mae": 0.4455138722254911,
            "precision": 0.956140350877193,
            "recall": 0.2219959266802444
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.7320296138682668,
            "auditor_fn_violation": 0.006681002292593753,
            "auditor_fp_violation": 0.002509016779049711,
            "ave_precision_score": 0.6096490714109497,
            "fpr": 0.008781558726673985,
            "logloss": 0.7455699150937579,
            "mae": 0.44724112137350636,
            "precision": 0.9215686274509803,
            "recall": 0.20302375809935205
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(7)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4934210526315789,
            "auc_prc": 0.6233438274649696,
            "auditor_fn_violation": 0.0037785400364455054,
            "auditor_fp_violation": 0.0037270283785473196,
            "ave_precision_score": 0.5880643227383303,
            "fpr": 0.03179824561403509,
            "logloss": 0.744711199566286,
            "mae": 0.48181944164712387,
            "precision": 0.6666666666666666,
            "recall": 0.11812627291242363
        },
        "train": {
            "accuracy": 0.5246981339187706,
            "auc_prc": 0.5856610424528526,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.004743609847890858,
            "ave_precision_score": 0.543779819364036,
            "fpr": 0.036223929747530186,
            "logloss": 0.7208530033769224,
            "mae": 0.4819684170167623,
            "precision": 0.65625,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(8)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.784336611865973,
            "auditor_fn_violation": 0.00878309572301427,
            "auditor_fp_violation": 0.0016382256115347756,
            "ave_precision_score": 0.6307660201363219,
            "fpr": 0.005482456140350877,
            "logloss": 0.664861343177439,
            "mae": 0.4513210251175782,
            "precision": 0.9557522123893806,
            "recall": 0.219959266802444
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.7548531014998204,
            "auditor_fn_violation": 0.006993952009635076,
            "auditor_fp_violation": 0.002509016779049711,
            "ave_precision_score": 0.6004442736861738,
            "fpr": 0.008781558726673985,
            "logloss": 0.66667032888337,
            "mae": 0.45073758118860546,
            "precision": 0.9223300970873787,
            "recall": 0.20518358531317496
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(9)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6955022590407612,
            "mae": 0.5010068890753022,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6938086605653749,
            "mae": 0.5001552450735404,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(10)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7898905668218337,
            "auditor_fn_violation": 0.009870654232322152,
            "auditor_fp_violation": 0.01092063591282244,
            "ave_precision_score": 0.779096005362779,
            "fpr": 0.16666666666666666,
            "logloss": 0.6538209877810147,
            "mae": 0.3546886348656563,
            "precision": 0.7179962894248608,
            "recall": 0.7881873727087576
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.7766892569839859,
            "auditor_fn_violation": 0.0009317366575547745,
            "auditor_fp_violation": 0.017337697977105222,
            "ave_precision_score": 0.7691971677692975,
            "fpr": 0.14709110867178923,
            "logloss": 0.5872558806801381,
            "mae": 0.3345267788865683,
            "precision": 0.7398058252427184,
            "recall": 0.8228941684665226
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(11)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4649122807017544,
            "auc_prc": 0.6053424357737522,
            "auditor_fn_violation": 0.0021460856826383827,
            "auditor_fp_violation": 0.0014220527565945744,
            "ave_precision_score": 0.5399448851252368,
            "fpr": 0.003289473684210526,
            "logloss": 0.6937644597419512,
            "mae": 0.5002862766646502,
            "precision": 0.6666666666666666,
            "recall": 0.012219959266802444
        },
        "train": {
            "accuracy": 0.4983534577387486,
            "auc_prc": 0.658364885145083,
            "auditor_fn_violation": 0.0028663349083555127,
            "auditor_fp_violation": 0.0009212795985573156,
            "ave_precision_score": 0.5132740467480494,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6930941820762196,
            "mae": 0.49995095610749446,
            "precision": 0.8,
            "recall": 0.017278617710583154
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(12)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.8005986072133433,
            "auditor_fn_violation": 0.007981384214099411,
            "auditor_fp_violation": 0.0016382256115347756,
            "ave_precision_score": 0.6413089643024978,
            "fpr": 0.005482456140350877,
            "logloss": 0.6273195669087501,
            "mae": 0.4441564064554609,
            "precision": 0.956140350877193,
            "recall": 0.2219959266802444
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.770201731612828,
            "auditor_fn_violation": 0.006993952009635076,
            "auditor_fp_violation": 0.002509016779049711,
            "ave_precision_score": 0.6067469446514413,
            "fpr": 0.008781558726673985,
            "logloss": 0.6336317262568156,
            "mae": 0.44835164718959647,
            "precision": 0.9223300970873787,
            "recall": 0.20518358531317496
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(13)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6513157894736842,
            "auc_prc": 0.7564987637643803,
            "auditor_fn_violation": 0.005332833101082645,
            "auditor_fp_violation": 0.022588761095136908,
            "ave_precision_score": 0.7561804979064142,
            "fpr": 0.3300438596491228,
            "logloss": 1.558853933835226,
            "mae": 0.3514010945841539,
            "precision": 0.6116129032258064,
            "recall": 0.9653767820773931
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.7169965497466486,
            "auditor_fn_violation": 0.0024135061511215217,
            "auditor_fp_violation": 0.027126293711776706,
            "ave_precision_score": 0.7207988666017369,
            "fpr": 0.35236004390779363,
            "logloss": 1.7242670862405558,
            "mae": 0.3746177717689609,
            "precision": 0.5831168831168831,
            "recall": 0.9697624190064795
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(14)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5800438596491229,
            "auc_prc": 0.8038013369628931,
            "auditor_fn_violation": 0.002523493050344823,
            "auditor_fp_violation": 0.0037192149018627327,
            "ave_precision_score": 0.67309782121382,
            "fpr": 0.007675438596491228,
            "logloss": 0.6312767199578794,
            "mae": 0.4219531120789613,
            "precision": 0.9426229508196722,
            "recall": 0.23421588594704684
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.7701579627733776,
            "auditor_fn_violation": 0.008082163525710494,
            "auditor_fp_violation": 0.00012741100831111792,
            "ave_precision_score": 0.6335489083862351,
            "fpr": 0.010976948408342482,
            "logloss": 0.638913697533189,
            "mae": 0.4273467959159358,
            "precision": 0.9107142857142857,
            "recall": 0.2203023758099352
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(15)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5668859649122807,
            "auc_prc": 0.6695081016896836,
            "auditor_fn_violation": 0.00878309572301427,
            "auditor_fp_violation": 0.003682752010668001,
            "ave_precision_score": 0.6699698968024521,
            "fpr": 0.013157894736842105,
            "logloss": 0.6635082811953379,
            "mae": 0.44991555441938863,
            "precision": 0.9,
            "recall": 0.219959266802444
        },
        "train": {
            "accuracy": 0.58397365532382,
            "auc_prc": 0.6283285117365067,
            "auditor_fn_violation": 0.006993952009635076,
            "auditor_fp_violation": 0.0011344480163086093,
            "ave_precision_score": 0.6300070491430249,
            "fpr": 0.012074643249176729,
            "logloss": 0.6659233175746867,
            "mae": 0.44981030777963665,
            "precision": 0.8962264150943396,
            "recall": 0.20518358531317496
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(16)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.784336611865973,
            "auditor_fn_violation": 0.00878309572301427,
            "auditor_fp_violation": 0.0016382256115347756,
            "ave_precision_score": 0.6307660201363219,
            "fpr": 0.005482456140350877,
            "logloss": 0.6648613398073103,
            "mae": 0.45132101783855705,
            "precision": 0.9557522123893806,
            "recall": 0.219959266802444
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.7548531014998204,
            "auditor_fn_violation": 0.006993952009635076,
            "auditor_fp_violation": 0.002509016779049711,
            "ave_precision_score": 0.6004442736861738,
            "fpr": 0.008781558726673985,
            "logloss": 0.6666703187344981,
            "mae": 0.4507375721268675,
            "precision": 0.9223300970873787,
            "recall": 0.20518358531317496
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(17)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7898905668218337,
            "auditor_fn_violation": 0.009870654232322152,
            "auditor_fp_violation": 0.01092063591282244,
            "ave_precision_score": 0.779096005362779,
            "fpr": 0.16666666666666666,
            "logloss": 0.653820502123024,
            "mae": 0.35468863329336336,
            "precision": 0.7179962894248608,
            "recall": 0.7881873727087576
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.7766892569839859,
            "auditor_fn_violation": 0.0009317366575547745,
            "auditor_fp_violation": 0.017337697977105222,
            "ave_precision_score": 0.7691971677692975,
            "fpr": 0.14709110867178923,
            "logloss": 0.5872558024688527,
            "mae": 0.33452679786137124,
            "precision": 0.7398058252427184,
            "recall": 0.8228941684665226
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(18)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.7341835037128647,
            "auditor_fn_violation": 0.025027244792224963,
            "auditor_fp_violation": 0.04206775846980873,
            "ave_precision_score": 0.7363961087505435,
            "fpr": 0.20175438596491227,
            "logloss": 0.629885716360777,
            "mae": 0.4013999872228229,
            "precision": 0.6660617059891107,
            "recall": 0.7474541751527495
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7244184926801621,
            "auditor_fn_violation": 0.017887921326337805,
            "auditor_fp_violation": 0.035430061157284004,
            "ave_precision_score": 0.7302221011001717,
            "fpr": 0.1942919868276619,
            "logloss": 0.6112976202601995,
            "mae": 0.3947389529625406,
            "precision": 0.6703910614525139,
            "recall": 0.7775377969762419
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(19)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.8206687145955192,
            "auditor_fn_violation": 0.00842132061314183,
            "auditor_fp_violation": 0.0022659082385298166,
            "ave_precision_score": 0.7002159011015101,
            "fpr": 0.008771929824561403,
            "logloss": 0.6073199329694757,
            "mae": 0.40558810445700627,
            "precision": 0.9493670886075949,
            "recall": 0.3054989816700611
        },
        "train": {
            "accuracy": 0.6256860592755215,
            "auc_prc": 0.7841805583700686,
            "auditor_fn_violation": 0.0055548574774830555,
            "auditor_fp_violation": 0.0036042614081856677,
            "ave_precision_score": 0.6594937765824431,
            "fpr": 0.014270032930845226,
            "logloss": 0.6208656075061595,
            "mae": 0.41220725885296955,
            "precision": 0.9121621621621622,
            "recall": 0.2915766738660907
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(20)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.7608677321393383,
            "auditor_fn_violation": 0.007981384214099411,
            "auditor_fp_violation": 0.0016382256115347756,
            "ave_precision_score": 0.6431057133325363,
            "fpr": 0.005482456140350877,
            "logloss": 0.7625911093927844,
            "mae": 0.44925065139275894,
            "precision": 0.956140350877193,
            "recall": 0.2219959266802444
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.7304945164661887,
            "auditor_fn_violation": 0.006681002292593753,
            "auditor_fp_violation": 0.002509016779049711,
            "ave_precision_score": 0.6030723479690283,
            "fpr": 0.008781558726673985,
            "logloss": 0.7480147712954042,
            "mae": 0.4503816795728602,
            "precision": 0.9215686274509803,
            "recall": 0.20302375809935205
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(21)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5800438596491229,
            "auc_prc": 0.7612961131310355,
            "auditor_fn_violation": 0.002523493050344823,
            "auditor_fp_violation": 0.0037192149018627327,
            "ave_precision_score": 0.6553507469452878,
            "fpr": 0.007675438596491228,
            "logloss": 0.6304410736431963,
            "mae": 0.4418688218779208,
            "precision": 0.9426229508196722,
            "recall": 0.23421588594704684
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.7366614374732655,
            "auditor_fn_violation": 0.008850312831175492,
            "auditor_fp_violation": 0.00012741100831111792,
            "ave_precision_score": 0.6198933473171757,
            "fpr": 0.010976948408342482,
            "logloss": 0.6374506547595533,
            "mae": 0.44586074947656573,
            "precision": 0.9099099099099099,
            "recall": 0.21814254859611232
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(22)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6008771929824561,
            "auc_prc": 0.7575159053555195,
            "auditor_fn_violation": 0.004575785185979205,
            "auditor_fp_violation": 0.020145747385089813,
            "ave_precision_score": 0.7547549746399295,
            "fpr": 0.3881578947368421,
            "logloss": 1.8025070388066609,
            "mae": 0.38562027864125475,
            "precision": 0.5760479041916168,
            "recall": 0.9796334012219959
        },
        "train": {
            "accuracy": 0.5883644346871569,
            "auc_prc": 0.7243801544837621,
            "auditor_fn_violation": 0.0026719267507995634,
            "auditor_fp_violation": 0.01907734828289165,
            "ave_precision_score": 0.7221295379403463,
            "fpr": 0.40504939626783754,
            "logloss": 1.9244767397076288,
            "mae": 0.4034335387075314,
            "precision": 0.5532687651331719,
            "recall": 0.9870410367170627
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(23)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4934210526315789,
            "auc_prc": 0.6360962172148186,
            "auditor_fn_violation": 0.0037785400364455054,
            "auditor_fp_violation": 0.0037270283785473196,
            "ave_precision_score": 0.5698247872037734,
            "fpr": 0.03179824561403509,
            "logloss": 0.7442164653622492,
            "mae": 0.4817864542955839,
            "precision": 0.6666666666666666,
            "recall": 0.11812627291242363
        },
        "train": {
            "accuracy": 0.5246981339187706,
            "auc_prc": 0.6208940866558585,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.004743609847890858,
            "ave_precision_score": 0.5425632759508282,
            "fpr": 0.036223929747530186,
            "logloss": 0.7199799259515363,
            "mae": 0.48141118118671045,
            "precision": 0.65625,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(24)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.5782347230270022,
            "auditor_fn_violation": 0.03159502626219319,
            "auditor_fp_violation": 0.0192289661207651,
            "ave_precision_score": 0.5809104778789023,
            "fpr": 0.11842105263157894,
            "logloss": 0.7266520814451539,
            "mae": 0.47838899142634383,
            "precision": 0.5862068965517241,
            "recall": 0.31160896130346233
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.5796985126792441,
            "auditor_fn_violation": 0.026946867302207488,
            "auditor_fp_violation": 0.028314646385447704,
            "ave_precision_score": 0.5774966606165848,
            "fpr": 0.11086717892425905,
            "logloss": 0.7024290733560513,
            "mae": 0.47494676874854497,
            "precision": 0.6085271317829457,
            "recall": 0.3390928725701944
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(25)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.7650414156267994,
            "auditor_fn_violation": 0.014287883660270853,
            "auditor_fp_violation": 0.0055814268450222965,
            "ave_precision_score": 0.7655908421611695,
            "fpr": 0.09539473684210527,
            "logloss": 0.7649335348220238,
            "mae": 0.37986031541540716,
            "precision": 0.7563025210084033,
            "recall": 0.5498981670061099
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.7392383534047502,
            "auditor_fn_violation": 0.006107261144684718,
            "auditor_fp_violation": 0.008296416810412419,
            "ave_precision_score": 0.7399445694127187,
            "fpr": 0.08122941822173436,
            "logloss": 0.7033174924650133,
            "mae": 0.3685031126327792,
            "precision": 0.7605177993527508,
            "recall": 0.5075593952483801
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(26)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7820721528247041,
            "auditor_fn_violation": 0.007789330760710329,
            "auditor_fp_violation": 0.005534545984914781,
            "ave_precision_score": 0.7824248877457546,
            "fpr": 0.13925438596491227,
            "logloss": 0.6360408018511927,
            "mae": 0.3584947198049744,
            "precision": 0.7465069860279441,
            "recall": 0.7617107942973523
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.755264157879245,
            "auditor_fn_violation": 0.006318265120568626,
            "auditor_fp_violation": 0.010075270503371488,
            "ave_precision_score": 0.755953209796943,
            "fpr": 0.1207464324917673,
            "logloss": 0.574046546028077,
            "mae": 0.33912706832683165,
            "precision": 0.7698744769874477,
            "recall": 0.7948164146868251
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(27)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4649122807017544,
            "auc_prc": 0.6053424357737522,
            "auditor_fn_violation": 0.0021460856826383827,
            "auditor_fp_violation": 0.0014220527565945744,
            "ave_precision_score": 0.5399448851252368,
            "fpr": 0.003289473684210526,
            "logloss": 0.6937644597419512,
            "mae": 0.5002862766646502,
            "precision": 0.6666666666666666,
            "recall": 0.012219959266802444
        },
        "train": {
            "accuracy": 0.4983534577387486,
            "auc_prc": 0.658364885145083,
            "auditor_fn_violation": 0.0028663349083555127,
            "auditor_fp_violation": 0.0009212795985573156,
            "ave_precision_score": 0.5132740467480494,
            "fpr": 0.0021953896816684962,
            "logloss": 0.6930941820762196,
            "mae": 0.49995095610749446,
            "precision": 0.8,
            "recall": 0.017278617710583154
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(28)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.8201126605673729,
            "auditor_fn_violation": 0.00842132061314183,
            "auditor_fp_violation": 0.0022659082385298166,
            "ave_precision_score": 0.7015027325345113,
            "fpr": 0.008771929824561403,
            "logloss": 0.6075219092584613,
            "mae": 0.4055130136579177,
            "precision": 0.9493670886075949,
            "recall": 0.3054989816700611
        },
        "train": {
            "accuracy": 0.6256860592755215,
            "auc_prc": 0.783117755713088,
            "auditor_fn_violation": 0.0055548574774830555,
            "auditor_fp_violation": 0.0036042614081856677,
            "ave_precision_score": 0.6599779465452678,
            "fpr": 0.014270032930845226,
            "logloss": 0.620862066853975,
            "mae": 0.4121623742654838,
            "precision": 0.9121621621621622,
            "recall": 0.2915766738660907
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(29)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7998930144445454,
            "auditor_fn_violation": 0.006449422946367955,
            "auditor_fp_violation": 0.0029300537567195904,
            "ave_precision_score": 0.7519501006001256,
            "fpr": 0.013157894736842105,
            "logloss": 0.5689315180285625,
            "mae": 0.3848710794884123,
            "precision": 0.9457013574660633,
            "recall": 0.4256619144602851
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.7573209211262151,
            "auditor_fn_violation": 0.002579464334401,
            "auditor_fp_violation": 0.005971165908734515,
            "ave_precision_score": 0.7047961076250082,
            "fpr": 0.029637760702524697,
            "logloss": 0.5984750999529933,
            "mae": 0.3950059393095722,
            "precision": 0.8767123287671232,
            "recall": 0.4146868250539957
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(30)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.8026718369352612,
            "auditor_fn_violation": 0.007981384214099411,
            "auditor_fp_violation": 0.0016382256115347756,
            "ave_precision_score": 0.6586086096232262,
            "fpr": 0.005482456140350877,
            "logloss": 0.6374157075562081,
            "mae": 0.4279813000589217,
            "precision": 0.956140350877193,
            "recall": 0.2219959266802444
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.7693675172724861,
            "auditor_fn_violation": 0.006993952009635076,
            "auditor_fp_violation": 0.002509016779049711,
            "ave_precision_score": 0.6193304821283764,
            "fpr": 0.008781558726673985,
            "logloss": 0.6463895721444131,
            "mae": 0.43189591852259035,
            "precision": 0.9223300970873787,
            "recall": 0.20518358531317496
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(31)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.6781143783258616,
            "auditor_fn_violation": 0.009062243184335586,
            "auditor_fp_violation": 0.02061195149393675,
            "ave_precision_score": 0.6607913634977033,
            "fpr": 0.4100877192982456,
            "logloss": 3.008161700759172,
            "mae": 0.40484051937690874,
            "precision": 0.5605170387779084,
            "recall": 0.9714867617107943
        },
        "train": {
            "accuracy": 0.5587266739846323,
            "auc_prc": 0.6437168662516383,
            "auditor_fn_violation": 0.003997221385845664,
            "auditor_fp_violation": 0.02138299749098322,
            "ave_precision_score": 0.6271828484988525,
            "fpr": 0.433589462129528,
            "logloss": 3.2978639283058553,
            "mae": 0.4230084965732567,
            "precision": 0.535840188014101,
            "recall": 0.9848812095032398
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(32)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5383771929824561,
            "auc_prc": 0.7691885964912281,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5383771929824561,
            "fpr": 0.4616228070175439,
            "logloss": 0.6930491730634971,
            "mae": 0.49995058173673196,
            "precision": 0.5383771929824561,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5082327113062569,
            "auc_prc": 0.7541163556531285,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5082327113062569,
            "fpr": 0.49176728869374314,
            "logloss": 0.6931268071288975,
            "mae": 0.499989398747977,
            "precision": 0.5082327113062569,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(33)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8373470136455127,
            "auditor_fn_violation": 0.04332815235645121,
            "auditor_fp_violation": 0.032910363795474434,
            "ave_precision_score": 0.8260576462349911,
            "fpr": 0.15570175438596492,
            "logloss": 0.5397206647146349,
            "mae": 0.3293999051532234,
            "precision": 0.7325800376647834,
            "recall": 0.7922606924643585
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7833525957641451,
            "auditor_fn_violation": 0.0399579888713184,
            "auditor_fp_violation": 0.051670064293554974,
            "ave_precision_score": 0.7690862425091776,
            "fpr": 0.1756311745334797,
            "logloss": 0.5914273249018995,
            "mae": 0.34960241491664057,
            "precision": 0.6875,
            "recall": 0.7602591792656588
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(34)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.7643770197356132,
            "auditor_fn_violation": 0.01585334262336086,
            "auditor_fp_violation": 0.0055814268450222965,
            "ave_precision_score": 0.7649292996964543,
            "fpr": 0.09539473684210527,
            "logloss": 0.7658313232856866,
            "mae": 0.38005784667284626,
            "precision": 0.7542372881355932,
            "recall": 0.5437881873727087
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.7387539123903349,
            "auditor_fn_violation": 0.006545864914780474,
            "auditor_fp_violation": 0.008296416810412419,
            "ave_precision_score": 0.7394638071005181,
            "fpr": 0.08122941822173436,
            "logloss": 0.7038210306976563,
            "mae": 0.368673813120629,
            "precision": 0.7597402597402597,
            "recall": 0.5053995680345572
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(35)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.8005986072133433,
            "auditor_fn_violation": 0.007981384214099411,
            "auditor_fp_violation": 0.0016382256115347756,
            "ave_precision_score": 0.6413089643024978,
            "fpr": 0.005482456140350877,
            "logloss": 0.6273195539298696,
            "mae": 0.4441563933025719,
            "precision": 0.956140350877193,
            "recall": 0.2219959266802444
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.770201731612828,
            "auditor_fn_violation": 0.006993952009635076,
            "auditor_fp_violation": 0.002509016779049711,
            "ave_precision_score": 0.6067469446514413,
            "fpr": 0.008781558726673985,
            "logloss": 0.6336317028702625,
            "mae": 0.4483516319449398,
            "precision": 0.9223300970873787,
            "recall": 0.20518358531317496
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(36)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7276532686654333,
            "auditor_fn_violation": 0.0015096294708257455,
            "auditor_fp_violation": 0.010819060715922821,
            "ave_precision_score": 0.7250770030177284,
            "fpr": 0.12938596491228072,
            "logloss": 0.6531892619634716,
            "mae": 0.3886740782118068,
            "precision": 0.7591836734693878,
            "recall": 0.7576374745417516
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.7110201402801872,
            "auditor_fn_violation": 0.0008558700594841598,
            "auditor_fp_violation": 0.013098831739062252,
            "ave_precision_score": 0.7124828217238177,
            "fpr": 0.11964873765093303,
            "logloss": 0.5936170138054664,
            "mae": 0.36986070510347446,
            "precision": 0.7714884696016772,
            "recall": 0.7948164146868251
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(37)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7679500933502903,
            "auditor_fn_violation": 0.012751456033158266,
            "auditor_fp_violation": 0.007513960078343128,
            "ave_precision_score": 0.7684395736464713,
            "fpr": 0.09100877192982457,
            "logloss": 0.7933829333152635,
            "mae": 0.37971176436281884,
            "precision": 0.7648725212464589,
            "recall": 0.5498981670061099
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.749450734974995,
            "auditor_fn_violation": 0.0018065733665565806,
            "auditor_fp_violation": 0.00923239767915948,
            "ave_precision_score": 0.7500108129497168,
            "fpr": 0.07903402854006586,
            "logloss": 0.7264222174285561,
            "mae": 0.3673381480673068,
            "precision": 0.7798165137614679,
            "recall": 0.550755939524838
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(38)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.7783506518031554,
            "auditor_fn_violation": 0.005455657984064031,
            "auditor_fp_violation": 0.02140371713130808,
            "ave_precision_score": 0.7728594198870279,
            "fpr": 0.3168859649122807,
            "logloss": 1.8141174511303695,
            "mae": 0.3496359566343017,
            "precision": 0.619235836627141,
            "recall": 0.9572301425661914
        },
        "train": {
            "accuracy": 0.6377607025246982,
            "auc_prc": 0.7417270882777621,
            "auditor_fn_violation": 0.0012802488424416723,
            "auditor_fp_violation": 0.01875637055041557,
            "ave_precision_score": 0.7340956345371272,
            "fpr": 0.3413830954994512,
            "logloss": 2.038036602891127,
            "mae": 0.3710395839764572,
            "precision": 0.5880794701986755,
            "recall": 0.958963282937365
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(39)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7848352304819456,
            "auditor_fn_violation": 0.0015096294708257455,
            "auditor_fp_violation": 0.010819060715922821,
            "ave_precision_score": 0.7737802037038602,
            "fpr": 0.12938596491228072,
            "logloss": 0.6379826903247714,
            "mae": 0.3765218957102158,
            "precision": 0.7591836734693878,
            "recall": 0.7576374745417516
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.7713930828605965,
            "auditor_fn_violation": 0.004196371205781035,
            "auditor_fp_violation": 0.010594715383409132,
            "ave_precision_score": 0.7647753301271307,
            "fpr": 0.11855104281009879,
            "logloss": 0.5824011939109281,
            "mae": 0.3602048669481509,
            "precision": 0.7735849056603774,
            "recall": 0.796976241900648
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(40)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4923245614035088,
            "auc_prc": 0.6248892928536398,
            "auditor_fn_violation": 0.006536516954300226,
            "auditor_fp_violation": 0.002885777388840272,
            "ave_precision_score": 0.5900661736519166,
            "fpr": 0.03070175438596491,
            "logloss": 0.7309634607361569,
            "mae": 0.4785616359573364,
            "precision": 0.6666666666666666,
            "recall": 0.11405295315682282
        },
        "train": {
            "accuracy": 0.5290889132821076,
            "auc_prc": 0.5998838048670202,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.008872216559510744,
            "ave_precision_score": 0.5535603639425444,
            "fpr": 0.031833150384193196,
            "logloss": 0.7057596111979776,
            "mae": 0.47522549113181023,
            "precision": 0.6847826086956522,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(41)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.7650058205391506,
            "auditor_fn_violation": 0.014287883660270853,
            "auditor_fp_violation": 0.0055814268450222965,
            "ave_precision_score": 0.7655555949432062,
            "fpr": 0.09539473684210527,
            "logloss": 0.765004991014537,
            "mae": 0.37987562271833314,
            "precision": 0.7563025210084033,
            "recall": 0.5498981670061099
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.7391362456827715,
            "auditor_fn_violation": 0.006107261144684718,
            "auditor_fp_violation": 0.008296416810412419,
            "ave_precision_score": 0.7398438620952895,
            "fpr": 0.08122941822173436,
            "logloss": 0.7033576546918432,
            "mae": 0.36851634604224903,
            "precision": 0.7605177993527508,
            "recall": 0.5075593952483801
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(42)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6954564607812531,
            "mae": 0.5009898408070991,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6937881572913158,
            "mae": 0.500150860599053,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(43)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6955021532725545,
            "mae": 0.5010068500904661,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.693808603917063,
            "mae": 0.5001552300578808,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(44)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6938077254091857,
            "mae": 0.5003124538547638,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6932700833604745,
            "mae": 0.5000436591591715,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(45)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5668859649122807,
            "auc_prc": 0.6695081016896836,
            "auditor_fn_violation": 0.00878309572301427,
            "auditor_fp_violation": 0.003682752010668001,
            "ave_precision_score": 0.6699698968024521,
            "fpr": 0.013157894736842105,
            "logloss": 0.6635082692774992,
            "mae": 0.44991554111127924,
            "precision": 0.9,
            "recall": 0.219959266802444
        },
        "train": {
            "accuracy": 0.58397365532382,
            "auc_prc": 0.6283285117365067,
            "auditor_fn_violation": 0.006993952009635076,
            "auditor_fp_violation": 0.0011344480163086093,
            "ave_precision_score": 0.6300070491430249,
            "fpr": 0.012074643249176729,
            "logloss": 0.6659232972759936,
            "mae": 0.44981029167215203,
            "precision": 0.8962264150943396,
            "recall": 0.20518358531317496
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(46)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4934210526315789,
            "auc_prc": 0.6233438274649696,
            "auditor_fn_violation": 0.0037785400364455054,
            "auditor_fp_violation": 0.0037270283785473196,
            "ave_precision_score": 0.5880643227383303,
            "fpr": 0.03179824561403509,
            "logloss": 0.744784288710703,
            "mae": 0.48186914374884127,
            "precision": 0.6666666666666666,
            "recall": 0.11812627291242363
        },
        "train": {
            "accuracy": 0.5246981339187706,
            "auc_prc": 0.5867484520157837,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.004743609847890858,
            "ave_precision_score": 0.5451789585282532,
            "fpr": 0.036223929747530186,
            "logloss": 0.7203403991604339,
            "mae": 0.48169038500359834,
            "precision": 0.65625,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(47)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4934210526315789,
            "auc_prc": 0.6233438274649696,
            "auditor_fn_violation": 0.0037785400364455054,
            "auditor_fp_violation": 0.0037270283785473196,
            "ave_precision_score": 0.5880643227383303,
            "fpr": 0.03179824561403509,
            "logloss": 0.7447112008180474,
            "mae": 0.4818194396507192,
            "precision": 0.6666666666666666,
            "recall": 0.11812627291242363
        },
        "train": {
            "accuracy": 0.5246981339187706,
            "auc_prc": 0.5856610424528526,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.004743609847890858,
            "ave_precision_score": 0.543779819364036,
            "fpr": 0.036223929747530186,
            "logloss": 0.7208530063729013,
            "mae": 0.48196841691534614,
            "precision": 0.65625,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(48)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7803005835117467,
            "auditor_fn_violation": 0.006449422946367955,
            "auditor_fp_violation": 0.0029300537567195904,
            "ave_precision_score": 0.7806253284436563,
            "fpr": 0.013157894736842105,
            "logloss": 0.5691519043937161,
            "mae": 0.3874067900551222,
            "precision": 0.9457013574660633,
            "recall": 0.4256619144602851
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.746750920864849,
            "auditor_fn_violation": 0.002579464334401,
            "auditor_fp_violation": 0.005971165908734515,
            "ave_precision_score": 0.7473288456784535,
            "fpr": 0.029637760702524697,
            "logloss": 0.5996655677955344,
            "mae": 0.3969162104612484,
            "precision": 0.8767123287671232,
            "recall": 0.4146868250539957
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(49)",
        "seed": 14724,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.616376754789688,
            "auditor_fn_violation": 0.0037785400364455054,
            "auditor_fp_violation": 0.005412134850189607,
            "ave_precision_score": 0.5824602321248562,
            "fpr": 0.03399122807017544,
            "logloss": 0.688286535868005,
            "mae": 0.49198545815357775,
            "precision": 0.651685393258427,
            "recall": 0.11812627291242363
        },
        "train": {
            "accuracy": 0.5225027442371021,
            "auc_prc": 0.5813853928026108,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.007821075740944018,
            "ave_precision_score": 0.5409304910039753,
            "fpr": 0.038419319429198684,
            "logloss": 0.6847014048814427,
            "mae": 0.4903292892874923,
            "precision": 0.6428571428571429,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(50)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.8000042553484374,
            "auditor_fn_violation": 0.0021728838389252158,
            "auditor_fp_violation": 0.014113743384589747,
            "ave_precision_score": 0.64996645356677,
            "fpr": 0.4067982456140351,
            "logloss": 0.6255397940547878,
            "mae": 0.44414719379621376,
            "precision": 0.5604265402843602,
            "recall": 0.9633401221995926
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.7702091230568264,
            "auditor_fn_violation": 0.0031271263392232693,
            "auditor_fp_violation": 0.011633605143484393,
            "ave_precision_score": 0.6170307950590245,
            "fpr": 0.4281009879253567,
            "logloss": 0.6362287077361323,
            "mae": 0.4501931752751936,
            "precision": 0.5334928229665071,
            "recall": 0.9632829373650108
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(51)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.8033051735880201,
            "auditor_fn_violation": 0.005839764890842181,
            "auditor_fp_violation": 0.010873755052714923,
            "ave_precision_score": 0.7843976841978177,
            "fpr": 0.08991228070175439,
            "logloss": 0.6141536038871657,
            "mae": 0.4108379277913308,
            "precision": 0.7670454545454546,
            "recall": 0.5498981670061099
        },
        "train": {
            "accuracy": 0.6421514818880352,
            "auc_prc": 0.7598129955461139,
            "auditor_fn_violation": 0.008760221245966622,
            "auditor_fp_violation": 0.013858397365532387,
            "ave_precision_score": 0.7401705533168885,
            "fpr": 0.1119648737650933,
            "logloss": 0.6247029900764021,
            "mae": 0.41800353147649705,
            "precision": 0.7008797653958945,
            "recall": 0.5161987041036717
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(52)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.8361233624562361,
            "auditor_fn_violation": 0.007583878229177846,
            "auditor_fp_violation": 0.0029300537567195904,
            "ave_precision_score": 0.7289795793506413,
            "fpr": 0.013157894736842105,
            "logloss": 0.5727532677511618,
            "mae": 0.3860754569422601,
            "precision": 0.9452054794520548,
            "recall": 0.4215885947046843
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.7966613150196048,
            "auditor_fn_violation": 0.002579464334401,
            "auditor_fp_violation": 0.0063950525325388126,
            "ave_precision_score": 0.6828270601273875,
            "fpr": 0.02854006586169045,
            "logloss": 0.5983081122910116,
            "mae": 0.3945753262107364,
            "precision": 0.8807339449541285,
            "recall": 0.4146868250539957
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(53)",
        "seed": 14724,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.6154904963534386,
            "auditor_fn_violation": 0.0037785400364455054,
            "auditor_fp_violation": 0.005412134850189607,
            "ave_precision_score": 0.5810507487691486,
            "fpr": 0.03399122807017544,
            "logloss": 0.6887076326637476,
            "mae": 0.4933842256861298,
            "precision": 0.651685393258427,
            "recall": 0.11812627291242363
        },
        "train": {
            "accuracy": 0.5203073545554336,
            "auc_prc": 0.5738074987702217,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.009693037478438138,
            "ave_precision_score": 0.5349845713605293,
            "fpr": 0.04061470911086718,
            "logloss": 0.6897191726623216,
            "mae": 0.4934356657524402,
            "precision": 0.63,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(54)",
        "seed": 14724,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.6124849601802023,
            "auditor_fn_violation": 0.0037785400364455054,
            "auditor_fp_violation": 0.005412134850189607,
            "ave_precision_score": 0.5774608340988115,
            "fpr": 0.03399122807017544,
            "logloss": 0.697347810195499,
            "mae": 0.49393269283239516,
            "precision": 0.651685393258427,
            "recall": 0.11812627291242363
        },
        "train": {
            "accuracy": 0.5203073545554336,
            "auc_prc": 0.5734927219809316,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.009693037478438138,
            "ave_precision_score": 0.5348614794982253,
            "fpr": 0.04061470911086718,
            "logloss": 0.6871172530407382,
            "mae": 0.490035720889249,
            "precision": 0.63,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(55)",
        "seed": 14724,
        "test": {
            "accuracy": 0.49451754385964913,
            "auc_prc": 0.634970893049623,
            "auditor_fn_violation": 0.002684281988065891,
            "auditor_fp_violation": 0.0036853565028961963,
            "ave_precision_score": 0.5619747250664257,
            "fpr": 0.03179824561403509,
            "logloss": 0.686988786246697,
            "mae": 0.4915793092039071,
            "precision": 0.6704545454545454,
            "recall": 0.12016293279022404
        },
        "train": {
            "accuracy": 0.5214050493962679,
            "auc_prc": 0.6084870808039287,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.007595656264701272,
            "ave_precision_score": 0.5330056390488042,
            "fpr": 0.03951701427003293,
            "logloss": 0.687735483273667,
            "mae": 0.49116728946353944,
            "precision": 0.6363636363636364,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(56)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4934210526315789,
            "auc_prc": 0.6233438274649696,
            "auditor_fn_violation": 0.0037785400364455054,
            "auditor_fp_violation": 0.0037270283785473196,
            "ave_precision_score": 0.5880643227383303,
            "fpr": 0.03179824561403509,
            "logloss": 0.7447843002072349,
            "mae": 0.481869143743113,
            "precision": 0.6666666666666666,
            "recall": 0.11812627291242363
        },
        "train": {
            "accuracy": 0.5246981339187706,
            "auc_prc": 0.5867484520157837,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.004743609847890858,
            "ave_precision_score": 0.5451789585282532,
            "fpr": 0.036223929747530186,
            "logloss": 0.7203404068468352,
            "mae": 0.48169038499744055,
            "precision": 0.65625,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(57)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.746115122025969,
            "auditor_fn_violation": 0.007836227534212327,
            "auditor_fp_violation": 0.005823644622244449,
            "ave_precision_score": 0.7424686952692617,
            "fpr": 0.09210526315789473,
            "logloss": 0.7818578573671915,
            "mae": 0.3828291674758395,
            "precision": 0.7620396600566572,
            "recall": 0.5478615071283096
        },
        "train": {
            "accuracy": 0.6663007683863886,
            "auc_prc": 0.7231704707309836,
            "auditor_fn_violation": 0.0071717643488630745,
            "auditor_fp_violation": 0.006635173278971305,
            "ave_precision_score": 0.7208644616626134,
            "fpr": 0.08342480790340286,
            "logloss": 0.7154361576316354,
            "mae": 0.3712472376531065,
            "precision": 0.7556270096463023,
            "recall": 0.5075593952483801
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(58)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4967105263157895,
            "auc_prc": 0.6469901840993092,
            "auditor_fn_violation": 0.002684281988065891,
            "auditor_fp_violation": 0.00318529399508272,
            "ave_precision_score": 0.5747730782238085,
            "fpr": 0.029605263157894735,
            "logloss": 0.7119063490329084,
            "mae": 0.47959713540943705,
            "precision": 0.686046511627907,
            "recall": 0.12016293279022404
        },
        "train": {
            "accuracy": 0.5257958287596048,
            "auc_prc": 0.6258010573518239,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.004939626783754118,
            "ave_precision_score": 0.5466032318931738,
            "fpr": 0.03512623490669594,
            "logloss": 0.6988014020625974,
            "mae": 0.47988801466890957,
            "precision": 0.6631578947368421,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(59)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0.5220091024404188,
            "auditor_fn_violation": 0.0021460856826383827,
            "auditor_fp_violation": 0.0018570029587031715,
            "ave_precision_score": 0.5379082252474363,
            "fpr": 0.006578947368421052,
            "logloss": 0.6940051297510845,
            "mae": 0.500398429600816,
            "precision": 0.5,
            "recall": 0.012219959266802444
        },
        "train": {
            "accuracy": 0.4961580680570801,
            "auc_prc": 0.5916982184784164,
            "auditor_fn_violation": 0.0028663349083555127,
            "auditor_fp_violation": 0.0024012074643249177,
            "ave_precision_score": 0.5109702310533051,
            "fpr": 0.0043907793633369925,
            "logloss": 0.6931816836834275,
            "mae": 0.49998674500001644,
            "precision": 0.6666666666666666,
            "recall": 0.017278617710583154
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(60)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7553860394864966,
            "auditor_fn_violation": 0.01835003751741881,
            "auditor_fp_violation": 0.005823644622244449,
            "ave_precision_score": 0.7513047830771884,
            "fpr": 0.09210526315789473,
            "logloss": 0.7698940597317203,
            "mae": 0.3790642111883507,
            "precision": 0.7783641160949868,
            "recall": 0.6008146639511202
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.730883953958237,
            "auditor_fn_violation": 0.01357775022345084,
            "auditor_fp_violation": 0.008884467618002196,
            "ave_precision_score": 0.7284775578539475,
            "fpr": 0.0867178924259056,
            "logloss": 0.7032866199269338,
            "mae": 0.3675654506667095,
            "precision": 0.7749287749287749,
            "recall": 0.5874730021598272
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(61)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7998930144445454,
            "auditor_fn_violation": 0.006449422946367955,
            "auditor_fp_violation": 0.0029300537567195904,
            "ave_precision_score": 0.7519501006001256,
            "fpr": 0.013157894736842105,
            "logloss": 0.5689315156220208,
            "mae": 0.3848710806729893,
            "precision": 0.9457013574660633,
            "recall": 0.4256619144602851
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.7573209211262151,
            "auditor_fn_violation": 0.002579464334401,
            "auditor_fp_violation": 0.005971165908734515,
            "ave_precision_score": 0.7047961076250082,
            "fpr": 0.029637760702524697,
            "logloss": 0.5984750988072727,
            "mae": 0.3950059411599496,
            "precision": 0.8767123287671232,
            "recall": 0.4146868250539957
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(62)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.695736421074251,
            "mae": 0.5010855450180539,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6939307952818641,
            "mae": 0.5001831827061629,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(63)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.8353214539362428,
            "auditor_fn_violation": 0.007583878229177846,
            "auditor_fp_violation": 0.0029300537567195904,
            "ave_precision_score": 0.7323956803806685,
            "fpr": 0.013157894736842105,
            "logloss": 0.5736344211969131,
            "mae": 0.3857632109194814,
            "precision": 0.9452054794520548,
            "recall": 0.4215885947046843
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.7958674291783219,
            "auditor_fn_violation": 0.002579464334401,
            "auditor_fp_violation": 0.0063950525325388126,
            "ave_precision_score": 0.6861684630768587,
            "fpr": 0.02854006586169045,
            "logloss": 0.5981392787750844,
            "mae": 0.3938736910221262,
            "precision": 0.8807339449541285,
            "recall": 0.4146868250539957
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(64)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.7802492113628348,
            "auditor_fn_violation": 0.005051452460070749,
            "auditor_fp_violation": 0.0021721465183147906,
            "ave_precision_score": 0.7427812188198903,
            "fpr": 0.020833333333333332,
            "logloss": 0.6222551714953714,
            "mae": 0.410934461272534,
            "precision": 0.9205020920502092,
            "recall": 0.4480651731160896
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.736979902462949,
            "auditor_fn_violation": 0.006095406988736192,
            "auditor_fp_violation": 0.004787713658460092,
            "ave_precision_score": 0.6957034804174242,
            "fpr": 0.03402854006586169,
            "logloss": 0.6391531585491061,
            "mae": 0.42037772224105807,
            "precision": 0.8658008658008658,
            "recall": 0.4319654427645788
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(65)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7679373595476917,
            "auditor_fn_violation": 0.012751456033158266,
            "auditor_fp_violation": 0.007513960078343128,
            "ave_precision_score": 0.7684267980886348,
            "fpr": 0.09100877192982457,
            "logloss": 0.7936295576360798,
            "mae": 0.37974594849385557,
            "precision": 0.7648725212464589,
            "recall": 0.5498981670061099
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7494575979372958,
            "auditor_fn_violation": 0.0018065733665565806,
            "auditor_fp_violation": 0.00923239767915948,
            "ave_precision_score": 0.7500174947083271,
            "fpr": 0.07903402854006586,
            "logloss": 0.7266219018544232,
            "mae": 0.3673639847101027,
            "precision": 0.7798165137614679,
            "recall": 0.550755939524838
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(66)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.7268938320155549,
            "auditor_fn_violation": 0.024678868760495954,
            "auditor_fp_violation": 0.04039046547485102,
            "ave_precision_score": 0.7338440046117519,
            "fpr": 0.19736842105263158,
            "logloss": 0.6279300996890654,
            "mae": 0.4012839649477157,
            "precision": 0.6697247706422018,
            "recall": 0.7433808553971487
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7277370085432614,
            "auditor_fn_violation": 0.020351214932443167,
            "auditor_fp_violation": 0.03276913125294025,
            "ave_precision_score": 0.7306301722781208,
            "fpr": 0.19099890230515917,
            "logloss": 0.6065551736396061,
            "mae": 0.3940509553956794,
            "precision": 0.6729323308270677,
            "recall": 0.7732181425485961
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(67)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.7802492113628348,
            "auditor_fn_violation": 0.005051452460070749,
            "auditor_fp_violation": 0.0021721465183147906,
            "ave_precision_score": 0.7427812188198903,
            "fpr": 0.020833333333333332,
            "logloss": 0.6222551701450415,
            "mae": 0.41093446328223016,
            "precision": 0.9205020920502092,
            "recall": 0.4480651731160896
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7374153749588955,
            "auditor_fn_violation": 0.006095406988736192,
            "auditor_fp_violation": 0.004787713658460092,
            "ave_precision_score": 0.6961490254506093,
            "fpr": 0.03402854006586169,
            "logloss": 0.6387214331167131,
            "mae": 0.42008713052560026,
            "precision": 0.8658008658008658,
            "recall": 0.4319654427645788
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(68)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0.5220091024404188,
            "auditor_fn_violation": 0.0021460856826383827,
            "auditor_fp_violation": 0.0018570029587031715,
            "ave_precision_score": 0.5379082252474363,
            "fpr": 0.006578947368421052,
            "logloss": 0.6940051297510845,
            "mae": 0.500398429600816,
            "precision": 0.5,
            "recall": 0.012219959266802444
        },
        "train": {
            "accuracy": 0.4961580680570801,
            "auc_prc": 0.5916982184784164,
            "auditor_fn_violation": 0.0028663349083555127,
            "auditor_fp_violation": 0.0024012074643249177,
            "ave_precision_score": 0.5109702310533051,
            "fpr": 0.0043907793633369925,
            "logloss": 0.6931816836834275,
            "mae": 0.49998674500001644,
            "precision": 0.6666666666666666,
            "recall": 0.017278617710583154
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(69)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7682793436073441,
            "auditor_fn_violation": 0.013204791510344097,
            "auditor_fp_violation": 0.0073550860524232205,
            "ave_precision_score": 0.7687679724189319,
            "fpr": 0.09210526315789473,
            "logloss": 0.7912658005724759,
            "mae": 0.379305931384505,
            "precision": 0.7633802816901408,
            "recall": 0.5519348268839104
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7494121969745255,
            "auditor_fn_violation": 0.0021361189019258255,
            "auditor_fp_violation": 0.00923239767915948,
            "ave_precision_score": 0.7499716201662833,
            "fpr": 0.07903402854006586,
            "logloss": 0.7251234249043208,
            "mae": 0.36698378300845513,
            "precision": 0.7804878048780488,
            "recall": 0.5529157667386609
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(70)",
        "seed": 14724,
        "test": {
            "accuracy": 0.45614035087719296,
            "auc_prc": 0.5006970456876486,
            "auditor_fn_violation": 0.010614303069282191,
            "auditor_fp_violation": 0.011548318539817477,
            "ave_precision_score": 0.5344773958975441,
            "fpr": 0.029605263157894735,
            "logloss": 0.6984934637204484,
            "mae": 0.5000779103015942,
            "precision": 0.4489795918367347,
            "recall": 0.04480651731160896
        },
        "train": {
            "accuracy": 0.5027442371020856,
            "auc_prc": 0.5427196091465596,
            "auditor_fn_violation": 0.010768315263648288,
            "auditor_fp_violation": 0.011104359416653598,
            "ave_precision_score": 0.5145691955872433,
            "fpr": 0.021953896816684963,
            "logloss": 0.6897796903635286,
            "mae": 0.49755001420198103,
            "precision": 0.6,
            "recall": 0.06479481641468683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(71)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.7646466619115149,
            "auditor_fn_violation": 0.0163357094365241,
            "auditor_fp_violation": 0.0055814268450222965,
            "ave_precision_score": 0.7651980279994004,
            "fpr": 0.09539473684210527,
            "logloss": 0.7655663863535831,
            "mae": 0.3800004574500754,
            "precision": 0.7549295774647887,
            "recall": 0.5458248472505092
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.7389706156273907,
            "auditor_fn_violation": 0.006545864914780474,
            "auditor_fp_violation": 0.008296416810412419,
            "ave_precision_score": 0.73967930249211,
            "fpr": 0.08122941822173436,
            "logloss": 0.7036735448169015,
            "mae": 0.3686246335486297,
            "precision": 0.7597402597402597,
            "recall": 0.5053995680345572
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(72)",
        "seed": 14724,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.6154904963534386,
            "auditor_fn_violation": 0.0037785400364455054,
            "auditor_fp_violation": 0.005412134850189607,
            "ave_precision_score": 0.5810507487691486,
            "fpr": 0.03399122807017544,
            "logloss": 0.6889530105417339,
            "mae": 0.49309041877195503,
            "precision": 0.651685393258427,
            "recall": 0.11812627291242363
        },
        "train": {
            "accuracy": 0.5203073545554336,
            "auc_prc": 0.5748702375743554,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.009693037478438138,
            "ave_precision_score": 0.5363469206949736,
            "fpr": 0.04061470911086718,
            "logloss": 0.6891620264297607,
            "mae": 0.4927792270084113,
            "precision": 0.63,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(73)",
        "seed": 14724,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.6154904963534386,
            "auditor_fn_violation": 0.0037785400364455054,
            "auditor_fp_violation": 0.005412134850189607,
            "ave_precision_score": 0.5810507487691486,
            "fpr": 0.03399122807017544,
            "logloss": 0.688884476897544,
            "mae": 0.49312249067844005,
            "precision": 0.651685393258427,
            "recall": 0.11812627291242363
        },
        "train": {
            "accuracy": 0.5203073545554336,
            "auc_prc": 0.5738074987702217,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.009693037478438138,
            "ave_precision_score": 0.5349845713605293,
            "fpr": 0.04061470911086718,
            "logloss": 0.6896287969547228,
            "mae": 0.49312676549873813,
            "precision": 0.63,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(74)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.7308477798089326,
            "auditor_fn_violation": 0.005143012827384143,
            "auditor_fp_violation": 0.021656352877443014,
            "ave_precision_score": 0.7000917271902432,
            "fpr": 0.3793859649122807,
            "logloss": 3.4831179131221384,
            "mae": 0.3821770995413668,
            "precision": 0.5811138014527845,
            "recall": 0.9775967413441955
        },
        "train": {
            "accuracy": 0.5894621295279913,
            "auc_prc": 0.6944801966216404,
            "auditor_fn_violation": 0.0033310178215380537,
            "auditor_fp_violation": 0.02058912890073703,
            "ave_precision_score": 0.6586800597075029,
            "fpr": 0.40065861690450055,
            "logloss": 3.9363777473467896,
            "mae": 0.40822098471922547,
            "precision": 0.5543345543345544,
            "recall": 0.980561555075594
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(75)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7678451859797961,
            "auditor_fn_violation": 0.012751456033158266,
            "auditor_fp_violation": 0.007513960078343128,
            "ave_precision_score": 0.7683352092954943,
            "fpr": 0.09100877192982457,
            "logloss": 0.7942963402880451,
            "mae": 0.3798667265349572,
            "precision": 0.7648725212464589,
            "recall": 0.5498981670061099
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7494274922008778,
            "auditor_fn_violation": 0.0018065733665565806,
            "auditor_fp_violation": 0.009840050180335583,
            "ave_precision_score": 0.7499873594310621,
            "fpr": 0.07793633369923161,
            "logloss": 0.7270167067007366,
            "mae": 0.36746670454686275,
            "precision": 0.7822085889570553,
            "recall": 0.550755939524838
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(76)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4934210526315789,
            "auc_prc": 0.6248511546672243,
            "auditor_fn_violation": 0.0037785400364455054,
            "auditor_fp_violation": 0.0037270283785473196,
            "ave_precision_score": 0.5903224669366016,
            "fpr": 0.03179824561403509,
            "logloss": 0.7136339855718329,
            "mae": 0.48100399205858385,
            "precision": 0.6666666666666666,
            "recall": 0.11812627291242363
        },
        "train": {
            "accuracy": 0.5246981339187706,
            "auc_prc": 0.5880561659861936,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.004743609847890858,
            "ave_precision_score": 0.547044575584863,
            "fpr": 0.036223929747530186,
            "logloss": 0.7007833055397378,
            "mae": 0.4817679081108057,
            "precision": 0.65625,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(77)",
        "seed": 14724,
        "test": {
            "accuracy": 0.49451754385964913,
            "auc_prc": 0.6345992772227139,
            "auditor_fn_violation": 0.002684281988065891,
            "auditor_fp_violation": 0.0036853565028961963,
            "ave_precision_score": 0.5615098890441291,
            "fpr": 0.03179824561403509,
            "logloss": 0.687836310116149,
            "mae": 0.49195834288471624,
            "precision": 0.6704545454545454,
            "recall": 0.12016293279022404
        },
        "train": {
            "accuracy": 0.5225027442371021,
            "auc_prc": 0.6115466483084419,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.007821075740944018,
            "ave_precision_score": 0.5339132329601955,
            "fpr": 0.038419319429198684,
            "logloss": 0.6879203408057954,
            "mae": 0.4912867953696183,
            "precision": 0.6428571428571429,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(78)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4934210526315789,
            "auc_prc": 0.6244844022055038,
            "auditor_fn_violation": 0.0037785400364455054,
            "auditor_fp_violation": 0.0037270283785473196,
            "ave_precision_score": 0.5879832293424605,
            "fpr": 0.03179824561403509,
            "logloss": 0.7182160054995828,
            "mae": 0.48181921456227683,
            "precision": 0.6666666666666666,
            "recall": 0.11812627291242363
        },
        "train": {
            "accuracy": 0.5246981339187706,
            "auc_prc": 0.5855689151809976,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.004743609847890858,
            "ave_precision_score": 0.5420932139180007,
            "fpr": 0.036223929747530186,
            "logloss": 0.7042852729655,
            "mae": 0.4824815529892395,
            "precision": 0.65625,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(79)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4583333333333333,
            "auc_prc": 0.5104392533401915,
            "auditor_fn_violation": 0.010149801693643486,
            "auditor_fp_violation": 0.010998770679668292,
            "ave_precision_score": 0.534584896568223,
            "fpr": 0.02850877192982456,
            "logloss": 0.6984158923315517,
            "mae": 0.5000299890898372,
            "precision": 0.46938775510204084,
            "recall": 0.04684317718940937
        },
        "train": {
            "accuracy": 0.5038419319429198,
            "auc_prc": 0.5485511426238814,
            "auditor_fn_violation": 0.010768315263648288,
            "auditor_fp_violation": 0.01054081072604673,
            "ave_precision_score": 0.5147807704979851,
            "fpr": 0.020856201975850714,
            "logloss": 0.6896691607702911,
            "mae": 0.497488402625552,
            "precision": 0.6122448979591837,
            "recall": 0.06479481641468683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(80)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6955028627182317,
            "mae": 0.5010071132136019,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6938090130469606,
            "mae": 0.5001553494307396,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(81)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.7640361597480831,
            "auditor_fn_violation": 0.01585334262336086,
            "auditor_fp_violation": 0.007693670042088599,
            "ave_precision_score": 0.7645893474139309,
            "fpr": 0.09429824561403509,
            "logloss": 0.7664506121797151,
            "mae": 0.380188171805593,
            "precision": 0.7563739376770539,
            "recall": 0.5437881873727087
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.7382785909612821,
            "auditor_fn_violation": 0.006545864914780474,
            "auditor_fp_violation": 0.008296416810412419,
            "ave_precision_score": 0.7389910056842599,
            "fpr": 0.08122941822173436,
            "logloss": 0.7041618263327079,
            "mae": 0.3687839191276701,
            "precision": 0.7597402597402597,
            "recall": 0.5053995680345572
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(82)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7683830134775742,
            "auditor_fn_violation": 0.012561635759459766,
            "auditor_fp_violation": 0.007196212026503312,
            "ave_precision_score": 0.7688709398120728,
            "fpr": 0.09320175438596491,
            "logloss": 0.7892653151163013,
            "mae": 0.37889835844321706,
            "precision": 0.7619047619047619,
            "recall": 0.5539714867617108
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7493594827033058,
            "auditor_fn_violation": 0.0021361189019258255,
            "auditor_fp_violation": 0.00923239767915948,
            "ave_precision_score": 0.7499177267967092,
            "fpr": 0.07903402854006586,
            "logloss": 0.7238481308354907,
            "mae": 0.3666172757756903,
            "precision": 0.7804878048780488,
            "recall": 0.5529157667386609
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(83)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.7991623820028766,
            "auditor_fn_violation": 0.007981384214099411,
            "auditor_fp_violation": 0.0016382256115347756,
            "ave_precision_score": 0.6343198240957588,
            "fpr": 0.005482456140350877,
            "logloss": 0.6296797977922939,
            "mae": 0.44573320783347936,
            "precision": 0.956140350877193,
            "recall": 0.2219959266802444
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.7686756533744143,
            "auditor_fn_violation": 0.006993952009635076,
            "auditor_fp_violation": 0.002509016779049711,
            "ave_precision_score": 0.5990846205109122,
            "fpr": 0.008781558726673985,
            "logloss": 0.6363999383710289,
            "mae": 0.4501621037083714,
            "precision": 0.9223300970873787,
            "recall": 0.20518358531317496
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(84)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4923245614035088,
            "auc_prc": 0.6248892928536398,
            "auditor_fn_violation": 0.006536516954300226,
            "auditor_fp_violation": 0.002885777388840272,
            "ave_precision_score": 0.5900661736519166,
            "fpr": 0.03070175438596491,
            "logloss": 0.7265025521812335,
            "mae": 0.478400763973185,
            "precision": 0.6666666666666666,
            "recall": 0.11405295315682282
        },
        "train": {
            "accuracy": 0.5290889132821076,
            "auc_prc": 0.5998838048670202,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.008872216559510744,
            "ave_precision_score": 0.5535603639425444,
            "fpr": 0.031833150384193196,
            "logloss": 0.7027080753146627,
            "mae": 0.47510157247805473,
            "precision": 0.6847826086956522,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(85)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.698054893919797,
            "auditor_fn_violation": 0.007981384214099411,
            "auditor_fp_violation": 0.0016382256115347756,
            "ave_precision_score": 0.6010342679167661,
            "fpr": 0.005482456140350877,
            "logloss": 0.7685458787691218,
            "mae": 0.4821951135037173,
            "precision": 0.956140350877193,
            "recall": 0.2219959266802444
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.6811352287546425,
            "auditor_fn_violation": 0.006993952009635076,
            "auditor_fp_violation": 0.002509016779049711,
            "ave_precision_score": 0.5718541292875043,
            "fpr": 0.008781558726673985,
            "logloss": 0.7466938283948722,
            "mae": 0.47973537800422617,
            "precision": 0.9223300970873787,
            "recall": 0.20518358531317496
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(86)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.7354796930763576,
            "auditor_fn_violation": 0.02402678029084933,
            "auditor_fp_violation": 0.043620035837813056,
            "ave_precision_score": 0.7378047907098451,
            "fpr": 0.19078947368421054,
            "logloss": 0.6268003367413167,
            "mae": 0.4010199721460435,
            "precision": 0.6741573033707865,
            "recall": 0.7331975560081466
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7276502885228703,
            "auditor_fn_violation": 0.01954276149675315,
            "auditor_fp_violation": 0.032428551826877844,
            "ave_precision_score": 0.7339290873959594,
            "fpr": 0.18551042810098792,
            "logloss": 0.6077524407103263,
            "mae": 0.39418277283517067,
            "precision": 0.6768642447418738,
            "recall": 0.7645788336933045
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(87)",
        "seed": 14724,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8353640999683838,
            "auditor_fn_violation": 0.04332815235645121,
            "auditor_fp_violation": 0.032910363795474434,
            "ave_precision_score": 0.8287207884191133,
            "fpr": 0.15570175438596492,
            "logloss": 0.5394484651826031,
            "mae": 0.32937768318592325,
            "precision": 0.7325800376647834,
            "recall": 0.7922606924643585
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.782310389649909,
            "auditor_fn_violation": 0.0399579888713184,
            "auditor_fp_violation": 0.051670064293554974,
            "ave_precision_score": 0.7740255346191938,
            "fpr": 0.1756311745334797,
            "logloss": 0.5901954001577757,
            "mae": 0.3492774707086294,
            "precision": 0.6875,
            "recall": 0.7602591792656588
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(88)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.7793640211195636,
            "auditor_fn_violation": 0.005455657984064031,
            "auditor_fp_violation": 0.019791536442055255,
            "ave_precision_score": 0.7737682320229435,
            "fpr": 0.32127192982456143,
            "logloss": 1.8577730606168936,
            "mae": 0.3521253784460816,
            "precision": 0.6159895150720839,
            "recall": 0.9572301425661914
        },
        "train": {
            "accuracy": 0.6333699231613611,
            "auc_prc": 0.7410003317042457,
            "auditor_fn_violation": 0.0012802488424416723,
            "auditor_fp_violation": 0.026670554335894634,
            "ave_precision_score": 0.7326782509088446,
            "fpr": 0.34577387486278816,
            "logloss": 2.1075608637758165,
            "mae": 0.3742287572504979,
            "precision": 0.5849802371541502,
            "recall": 0.958963282937365
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(89)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.7842678136316067,
            "auditor_fn_violation": 0.006395826633794262,
            "auditor_fp_violation": 0.020111888986123278,
            "ave_precision_score": 0.7780224075800022,
            "fpr": 0.28289473684210525,
            "logloss": 1.8344194253653192,
            "mae": 0.3250545052043826,
            "precision": 0.6416666666666667,
            "recall": 0.9409368635437881
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.7454119178666414,
            "auditor_fn_violation": 0.0040873129710545224,
            "auditor_fp_violation": 0.024090481417594484,
            "ave_precision_score": 0.7336955790545905,
            "fpr": 0.32711306256860595,
            "logloss": 2.1690467536923212,
            "mae": 0.35511697356712585,
            "precision": 0.5962059620596206,
            "recall": 0.9503239740820735
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(90)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.594889484540733,
            "mae": 0.5383771929824561,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.553735972386,
            "mae": 0.5082327113062569,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(91)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6955028261619469,
            "mae": 0.5010070995542041,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6938089937055225,
            "mae": 0.5001553440002395,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(92)",
        "seed": 14724,
        "test": {
            "accuracy": 0.49451754385964913,
            "auc_prc": 0.634970893049623,
            "auditor_fn_violation": 0.002684281988065891,
            "auditor_fp_violation": 0.0036853565028961963,
            "ave_precision_score": 0.5619747250664257,
            "fpr": 0.03179824561403509,
            "logloss": 0.686988738786327,
            "mae": 0.49157927440185295,
            "precision": 0.6704545454545454,
            "recall": 0.12016293279022404
        },
        "train": {
            "accuracy": 0.5214050493962679,
            "auc_prc": 0.6084870808039287,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.007595656264701272,
            "ave_precision_score": 0.5330056390488042,
            "fpr": 0.03951701427003293,
            "logloss": 0.6877354169229736,
            "mae": 0.49116724765523206,
            "precision": 0.6363636363636364,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(93)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6952035016831345,
            "mae": 0.50079939458846,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6935266782251823,
            "mae": 0.4999618316651961,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(94)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4967105263157895,
            "auc_prc": 0.6469901840993092,
            "auditor_fn_violation": 0.002684281988065891,
            "auditor_fp_violation": 0.00318529399508272,
            "ave_precision_score": 0.5747730782238085,
            "fpr": 0.029605263157894735,
            "logloss": 0.7119064151980747,
            "mae": 0.47959716436315303,
            "precision": 0.686046511627907,
            "recall": 0.12016293279022404
        },
        "train": {
            "accuracy": 0.5257958287596048,
            "auc_prc": 0.6258010573518239,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.004939626783754118,
            "ave_precision_score": 0.5466032318931738,
            "fpr": 0.03512623490669594,
            "logloss": 0.6988014901396522,
            "mae": 0.47988805814690144,
            "precision": 0.6631578947368421,
            "recall": 0.13606911447084233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(95)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6954564607812531,
            "mae": 0.5009898408070991,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6937881572913158,
            "mae": 0.500150860599053,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(96)",
        "seed": 14724,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.7245668906437779,
            "auditor_fn_violation": 0.09974497087933683,
            "auditor_fp_violation": 0.10417968912780763,
            "ave_precision_score": 0.5698373299718871,
            "fpr": 0.2741228070175439,
            "logloss": 0.6878556846319258,
            "mae": 0.49689194533908576,
            "precision": 0.5826377295492488,
            "recall": 0.7107942973523421
        },
        "train": {
            "accuracy": 0.5587266739846323,
            "auc_prc": 0.7070645790318465,
            "auditor_fn_violation": 0.09195031686158851,
            "auditor_fp_violation": 0.1028451858240552,
            "ave_precision_score": 0.538512507888318,
            "fpr": 0.300768386388584,
            "logloss": 0.6895254678455869,
            "mae": 0.4977201702958535,
            "precision": 0.5500821018062397,
            "recall": 0.7235421166306696
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(97)",
        "seed": 14724,
        "test": {
            "accuracy": 0.6107456140350878,
            "auc_prc": 0.7742567939878585,
            "auditor_fn_violation": 0.013376746346518026,
            "auditor_fp_violation": 0.0022659082385298166,
            "ave_precision_score": 0.6778352993500963,
            "fpr": 0.008771929824561403,
            "logloss": 0.7331206569648048,
            "mae": 0.4281222345470859,
            "precision": 0.9473684210526315,
            "recall": 0.29327902240325865
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.739715848374866,
            "auditor_fn_violation": 0.0037340591237882124,
            "auditor_fp_violation": 0.0036042614081856677,
            "ave_precision_score": 0.6347792880191201,
            "fpr": 0.014270032930845226,
            "logloss": 0.7176826147006862,
            "mae": 0.43186467309389365,
            "precision": 0.9051094890510949,
            "recall": 0.2678185745140389
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(98)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4616228070175439,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.594889484540733,
            "mae": 0.5383771929824561,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.553735972386,
            "mae": 0.5082327113062569,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(99)",
        "seed": 14724,
        "test": {
            "accuracy": 0.4967105263157895,
            "auc_prc": 0.6469901840993092,
            "auditor_fn_violation": 0.002684281988065891,
            "auditor_fp_violation": 0.00318529399508272,
            "ave_precision_score": 0.5747730782238085,
            "fpr": 0.029605263157894735,
            "logloss": 0.7119063980903286,
            "mae": 0.4795971699707463,
            "precision": 0.686046511627907,
            "recall": 0.12016293279022404
        },
        "train": {
            "accuracy": 0.5257958287596048,
            "auc_prc": 0.6258010573518239,
            "auditor_fn_violation": 0.0045591083778061805,
            "auditor_fp_violation": 0.004939626783754118,
            "ave_precision_score": 0.5466032318931738,
            "fpr": 0.03512623490669594,
            "logloss": 0.6988014869189177,
            "mae": 0.47988806645149257,
            "precision": 0.6631578947368421,
            "recall": 0.13606911447084233
        }
    }
]
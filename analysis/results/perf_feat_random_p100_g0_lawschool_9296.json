[
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(0)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8069855410773961,
            "auditor_fn_violation": 0.015775101071918703,
            "auditor_fp_violation": 0.016629700675378657,
            "ave_precision_score": 0.80756469248066,
            "fpr": 0.13486842105263158,
            "logloss": 0.8960534854705418,
            "mae": 0.27897707409305755,
            "precision": 0.74375,
            "recall": 0.7547568710359408
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8425911759125376,
            "auditor_fn_violation": 0.008099207879668913,
            "auditor_fp_violation": 0.019651290429632654,
            "ave_precision_score": 0.84282052485952,
            "fpr": 0.13062568605927552,
            "logloss": 0.8154252845327473,
            "mae": 0.2682976366700905,
            "precision": 0.7536231884057971,
            "recall": 0.7567567567567568
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(1)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5449561403508771,
            "auc_prc": 0.7080448474518124,
            "auditor_fn_violation": 0.007835391862319647,
            "auditor_fp_violation": 0.010195620029572803,
            "ave_precision_score": 0.5320513461152674,
            "fpr": 0.4155701754385965,
            "logloss": 12.33051103796889,
            "mae": 0.463558460553095,
            "precision": 0.5355392156862745,
            "recall": 0.9238900634249472
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.7039042303711749,
            "auditor_fn_violation": 0.005896971868431804,
            "auditor_fp_violation": 0.0041150792637786445,
            "ave_precision_score": 0.5328688029769041,
            "fpr": 0.4127332601536773,
            "logloss": 12.161054888792243,
            "mae": 0.45858585659075124,
            "precision": 0.5447941888619855,
            "recall": 0.9355509355509356
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(2)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8211828905833929,
            "auditor_fn_violation": 0.024382441304105936,
            "auditor_fp_violation": 0.021492726691443878,
            "ave_precision_score": 0.8126165672038942,
            "fpr": 0.1162280701754386,
            "logloss": 0.5777971813475072,
            "mae": 0.34622943759262365,
            "precision": 0.7596371882086168,
            "recall": 0.7082452431289641
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8352983486664703,
            "auditor_fn_violation": 0.031990615964271295,
            "auditor_fp_violation": 0.018589334490593017,
            "ave_precision_score": 0.8279585306125129,
            "fpr": 0.10757409440175632,
            "logloss": 0.5486399112798742,
            "mae": 0.34149073010316766,
            "precision": 0.772093023255814,
            "recall": 0.6902286902286903
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(3)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5405701754385965,
            "auc_prc": 0.6699135428867332,
            "auditor_fn_violation": 0.008975928192574461,
            "auditor_fp_violation": 0.0013862246733005656,
            "ave_precision_score": 0.6740697789720999,
            "fpr": 0.03728070175438596,
            "logloss": 0.8930732760491289,
            "mae": 0.44698905891722496,
            "precision": 0.7213114754098361,
            "recall": 0.18604651162790697
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.6935360319172159,
            "auditor_fn_violation": 0.008859150461784947,
            "auditor_fp_violation": 0.0032471345059096827,
            "ave_precision_score": 0.704431422705071,
            "fpr": 0.019758507135016465,
            "logloss": 0.9028397740762717,
            "mae": 0.44850040722994133,
            "precision": 0.8163265306122449,
            "recall": 0.16632016632016633
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(4)",
        "seed": 9296,
        "test": {
            "accuracy": 0.518640350877193,
            "auc_prc": 0.4711908216560103,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5790801558605457,
            "fpr": 0.48135964912280704,
            "logloss": 0.6924590752134404,
            "mae": 0.4991235379456428,
            "precision": 0.518640350877193,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5279912184412733,
            "auc_prc": 0.4744620470365373,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5794304339100087,
            "fpr": 0.47200878155872666,
            "logloss": 0.6914579170046,
            "mae": 0.49862356263117785,
            "precision": 0.5279912184412733,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(5)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5855263157894737,
            "auc_prc": 0.8337259319857696,
            "auditor_fn_violation": 0.021199603130447695,
            "auditor_fp_violation": 0.002884845941733605,
            "ave_precision_score": 0.8340831496896082,
            "fpr": 0.005482456140350877,
            "logloss": 0.9125387276897103,
            "mae": 0.4018863301358973,
            "precision": 0.9523809523809523,
            "recall": 0.21141649048625794
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.8392668229598366,
            "auditor_fn_violation": 0.03707972094360681,
            "auditor_fp_violation": 0.0034564623592780743,
            "ave_precision_score": 0.839612246432081,
            "fpr": 0.008781558726673985,
            "logloss": 0.9221036657727182,
            "mae": 0.4003861250130724,
            "precision": 0.9338842975206612,
            "recall": 0.23492723492723494
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(6)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6491228070175439,
            "auc_prc": 0.6307588954002314,
            "auditor_fn_violation": 0.020629334965320278,
            "auditor_fp_violation": 0.016045238380689768,
            "ave_precision_score": 0.6325348016788126,
            "fpr": 0.23026315789473684,
            "logloss": 0.7173413600066132,
            "mae": 0.41492116094664916,
            "precision": 0.6335078534031413,
            "recall": 0.7674418604651163
        },
        "train": {
            "accuracy": 0.6531284302963776,
            "auc_prc": 0.653830071397202,
            "auditor_fn_violation": 0.019966179131931057,
            "auditor_fp_violation": 0.01850764557220536,
            "ave_precision_score": 0.655472097361846,
            "fpr": 0.22283205268935236,
            "logloss": 0.6825342041076569,
            "mae": 0.4153825706106117,
            "precision": 0.6444833625218914,
            "recall": 0.7650727650727651
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(7)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.780095792273937,
            "auditor_fn_violation": 0.013283075553577392,
            "auditor_fp_violation": 0.022257023538344727,
            "ave_precision_score": 0.7711069896073711,
            "fpr": 0.21162280701754385,
            "logloss": 0.646570891497196,
            "mae": 0.350087979171229,
            "precision": 0.6745362563237775,
            "recall": 0.8456659619450317
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7939957260353061,
            "auditor_fn_violation": 0.009756019635273203,
            "auditor_fp_violation": 0.02678375411635566,
            "ave_precision_score": 0.7857326284589903,
            "fpr": 0.18880351262349068,
            "logloss": 0.5982860593811019,
            "mae": 0.3323080879436759,
            "precision": 0.7044673539518901,
            "recall": 0.8523908523908524
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(8)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8145271196299754,
            "auditor_fn_violation": 0.0032848373576647772,
            "auditor_fp_violation": 0.007827798425448588,
            "ave_precision_score": 0.7658265616926603,
            "fpr": 0.09210526315789473,
            "logloss": 0.5468454624986999,
            "mae": 0.3509259198857635,
            "precision": 0.7980769230769231,
            "recall": 0.7019027484143763
        },
        "train": {
            "accuracy": 0.7892425905598244,
            "auc_prc": 0.8542309904862426,
            "auditor_fn_violation": 0.009856432468946198,
            "auditor_fp_violation": 0.008664130906491718,
            "ave_precision_score": 0.8113039581665645,
            "fpr": 0.07025246981339188,
            "logloss": 0.49898506656251584,
            "mae": 0.3280825963249976,
            "precision": 0.8465227817745803,
            "recall": 0.7338877338877339
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(9)",
        "seed": 9296,
        "test": {
            "accuracy": 0.4824561403508772,
            "auc_prc": 0.6664915485129533,
            "auditor_fn_violation": 0.0012263083713512173,
            "auditor_fp_violation": 0.002347839987211765,
            "ave_precision_score": 0.6127372162092548,
            "fpr": 0.0043859649122807015,
            "logloss": 1.1144736222584226,
            "mae": 0.4942588558902539,
            "precision": 0.5555555555555556,
            "recall": 0.010570824524312896
        },
        "train": {
            "accuracy": 0.4774972557628979,
            "auc_prc": 0.7249249012613294,
            "auditor_fn_violation": 0.001624862217617445,
            "auditor_fp_violation": 0.0005999029944094147,
            "ave_precision_score": 0.6507069085112267,
            "fpr": 0.0010976948408342481,
            "logloss": 1.1122484792602925,
            "mae": 0.4953489224952937,
            "precision": 0.8571428571428571,
            "recall": 0.012474012474012475
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(10)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7648645947393581,
            "auditor_fn_violation": 0.00462937205593265,
            "auditor_fp_violation": 0.007775346681053433,
            "ave_precision_score": 0.7548374269070022,
            "fpr": 0.07456140350877193,
            "logloss": 0.5524518752960715,
            "mae": 0.355749878860814,
            "precision": 0.8111111111111111,
            "recall": 0.6173361522198731
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8341845404461739,
            "auditor_fn_violation": 0.02324785310515278,
            "auditor_fp_violation": 0.003157787251423175,
            "ave_precision_score": 0.823062665998603,
            "fpr": 0.05159165751920966,
            "logloss": 0.5058382635028292,
            "mae": 0.33687099940363324,
            "precision": 0.867231638418079,
            "recall": 0.6382536382536382
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(11)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8122627080471033,
            "auditor_fn_violation": 0.01096954860724751,
            "auditor_fp_violation": 0.01059775006993566,
            "ave_precision_score": 0.7572054271647202,
            "fpr": 0.10197368421052631,
            "logloss": 0.5565840763529026,
            "mae": 0.34207477033334344,
            "precision": 0.7816901408450704,
            "recall": 0.7040169133192389
        },
        "train": {
            "accuracy": 0.7837541163556532,
            "auc_prc": 0.8691691122780685,
            "auditor_fn_violation": 0.011328393326197941,
            "auditor_fp_violation": 0.010466392668419576,
            "ave_precision_score": 0.8313631663331446,
            "fpr": 0.07464324917672886,
            "logloss": 0.48753318562176057,
            "mae": 0.31536200984445545,
            "precision": 0.8380952380952381,
            "recall": 0.7318087318087318
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(12)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5043859649122807,
            "auc_prc": 0.5626514743976742,
            "auditor_fn_violation": 0.015749601275917065,
            "auditor_fp_violation": 0.019149882108460217,
            "ave_precision_score": 0.5601458116656929,
            "fpr": 0.0712719298245614,
            "logloss": 0.820993612789757,
            "mae": 0.49941848262147814,
            "precision": 0.5695364238410596,
            "recall": 0.18181818181818182
        },
        "train": {
            "accuracy": 0.49286498353457736,
            "auc_prc": 0.5649170321293197,
            "auditor_fn_violation": 0.014155927438035027,
            "auditor_fp_violation": 0.015214561049702603,
            "ave_precision_score": 0.5636115994397796,
            "fpr": 0.07683863885839737,
            "logloss": 0.841967662559054,
            "mae": 0.5037210383621156,
            "precision": 0.559748427672956,
            "recall": 0.18503118503118504
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(13)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.7172338245576192,
            "auditor_fn_violation": 0.027356644783205375,
            "auditor_fp_violation": 0.014823862046916839,
            "ave_precision_score": 0.7095832326900704,
            "fpr": 0.16666666666666666,
            "logloss": 3.0233846468319965,
            "mae": 0.3562001014972245,
            "precision": 0.6731182795698925,
            "recall": 0.6617336152219874
        },
        "train": {
            "accuracy": 0.6355653128430296,
            "auc_prc": 0.7042121136903441,
            "auditor_fn_violation": 0.031073207802077184,
            "auditor_fp_violation": 0.0169887422459347,
            "ave_precision_score": 0.6951295253022077,
            "fpr": 0.17014270032930845,
            "logloss": 3.5363472144121904,
            "mae": 0.3761360447507271,
            "precision": 0.6623093681917211,
            "recall": 0.632016632016632
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(14)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8385584397366525,
            "auditor_fn_violation": 0.0019148028633952765,
            "auditor_fp_violation": 0.006956100387643368,
            "ave_precision_score": 0.8388652762775638,
            "fpr": 0.07017543859649122,
            "logloss": 0.6043856578037282,
            "mae": 0.3487606488168917,
            "precision": 0.8176638176638177,
            "recall": 0.6067653276955602
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8504172138250798,
            "auditor_fn_violation": 0.015605067196724717,
            "auditor_fp_violation": 0.00206775074668777,
            "ave_precision_score": 0.8507595483104681,
            "fpr": 0.054884742041712405,
            "logloss": 0.5549285795176763,
            "mae": 0.33406446916546906,
            "precision": 0.863013698630137,
            "recall": 0.6548856548856549
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(15)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7967554591939526,
            "auditor_fn_violation": 0.011032139015615157,
            "auditor_fp_violation": 0.011821624105822647,
            "ave_precision_score": 0.7573377456033619,
            "fpr": 0.10087719298245613,
            "logloss": 0.5691819106044379,
            "mae": 0.3637018261902165,
            "precision": 0.7799043062200957,
            "recall": 0.6892177589852009
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8376655863086608,
            "auditor_fn_violation": 0.010871971354957091,
            "auditor_fp_violation": 0.010241748142853497,
            "ave_precision_score": 0.8041345304882479,
            "fpr": 0.07354555433589462,
            "logloss": 0.5056010177466456,
            "mae": 0.3360866272717747,
            "precision": 0.8389423076923077,
            "recall": 0.7255717255717256
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(16)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.6864271099230892,
            "auditor_fn_violation": 0.017879993323689777,
            "auditor_fp_violation": 0.007318267194181355,
            "ave_precision_score": 0.6873856826271777,
            "fpr": 0.1699561403508772,
            "logloss": 0.6367905005404252,
            "mae": 0.44363773623971564,
            "precision": 0.6428571428571429,
            "recall": 0.5898520084566596
        },
        "train": {
            "accuracy": 0.6663007683863886,
            "auc_prc": 0.739526445066704,
            "auditor_fn_violation": 0.01528328970699992,
            "auditor_fp_violation": 0.008860694866362036,
            "ave_precision_score": 0.7401527360275717,
            "fpr": 0.13062568605927552,
            "logloss": 0.6054402505902396,
            "mae": 0.42966779368049357,
            "precision": 0.7132530120481928,
            "recall": 0.6153846153846154
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(17)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.7454943325277666,
            "auditor_fn_violation": 0.0101048737064649,
            "auditor_fp_violation": 0.01163429644726852,
            "ave_precision_score": 0.7460360755889123,
            "fpr": 0.07236842105263158,
            "logloss": 0.9122520651855188,
            "mae": 0.3830179746769238,
            "precision": 0.7836065573770492,
            "recall": 0.5052854122621564
        },
        "train": {
            "accuracy": 0.6695938529088913,
            "auc_prc": 0.777178121391167,
            "auditor_fn_violation": 0.018471397176117278,
            "auditor_fp_violation": 0.010747198325377174,
            "ave_precision_score": 0.776537980267724,
            "fpr": 0.07903402854006586,
            "logloss": 0.9158171008983979,
            "mae": 0.37553870087462904,
            "precision": 0.7777777777777778,
            "recall": 0.5239085239085239
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(18)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8197600014961794,
            "auditor_fn_violation": 0.01096954860724751,
            "auditor_fp_violation": 0.01059775006993566,
            "ave_precision_score": 0.7046860145382559,
            "fpr": 0.10197368421052631,
            "logloss": 0.5705971361792268,
            "mae": 0.3566739738902502,
            "precision": 0.7816901408450704,
            "recall": 0.7040169133192389
        },
        "train": {
            "accuracy": 0.7837541163556532,
            "auc_prc": 0.8562227031840292,
            "auditor_fn_violation": 0.011328393326197941,
            "auditor_fp_violation": 0.010466392668419576,
            "ave_precision_score": 0.7558668497895017,
            "fpr": 0.07464324917672886,
            "logloss": 0.513083394587056,
            "mae": 0.3330069264125091,
            "precision": 0.8380952380952381,
            "recall": 0.7318087318087318
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(19)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5877192982456141,
            "auc_prc": 0.6622129707827682,
            "auditor_fn_violation": 0.025615704165275775,
            "auditor_fp_violation": 0.0366288015026176,
            "ave_precision_score": 0.6631086663208078,
            "fpr": 0.3201754385964912,
            "logloss": 3.199511630747339,
            "mae": 0.419067146397462,
            "precision": 0.5712187958883994,
            "recall": 0.8224101479915433
        },
        "train": {
            "accuracy": 0.5960482985729967,
            "auc_prc": 0.6450851779631,
            "auditor_fn_violation": 0.03317731308949751,
            "auditor_fp_violation": 0.041329487146759256,
            "ave_precision_score": 0.6461235519893154,
            "fpr": 0.3018660812294182,
            "logloss": 3.3364748281968213,
            "mae": 0.42305911673200436,
            "precision": 0.5852187028657617,
            "recall": 0.8066528066528067
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(20)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7691365686349599,
            "auditor_fn_violation": 0.014347112495827305,
            "auditor_fp_violation": 0.023218638852255925,
            "ave_precision_score": 0.6585181706161085,
            "fpr": 0.16885964912280702,
            "logloss": 0.6289213026634152,
            "mae": 0.40331191487871765,
            "precision": 0.6901408450704225,
            "recall": 0.7251585623678647
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7676494029519354,
            "auditor_fn_violation": 0.013085617915475219,
            "auditor_fp_violation": 0.025042759043218548,
            "ave_precision_score": 0.6588626835368484,
            "fpr": 0.17014270032930845,
            "logloss": 0.6393653304095458,
            "mae": 0.40777419391369324,
            "precision": 0.6893787575150301,
            "recall": 0.7151767151767152
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(21)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.66957232283354,
            "auditor_fn_violation": 0.0005053595934868885,
            "auditor_fp_violation": 0.006019462094872719,
            "ave_precision_score": 0.551312294202289,
            "fpr": 0.4440789473684211,
            "logloss": 0.7064169821704008,
            "mae": 0.475487127050496,
            "precision": 0.5381984036488028,
            "recall": 0.9978858350951374
        },
        "train": {
            "accuracy": 0.566410537870472,
            "auc_prc": 0.6843074956480464,
            "auditor_fn_violation": 0.00014377292094086837,
            "auditor_fp_violation": 0.011870420953207566,
            "ave_precision_score": 0.5680875032275978,
            "fpr": 0.4281009879253567,
            "logloss": 0.6983056311478212,
            "mae": 0.4707062312460103,
            "precision": 0.5496535796766744,
            "recall": 0.9896049896049897
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(22)",
        "seed": 9296,
        "test": {
            "accuracy": 0.48026315789473684,
            "auc_prc": 0.5019537200041713,
            "auditor_fn_violation": 0.0036325618485961346,
            "auditor_fp_violation": 0.0009191543779722657,
            "ave_precision_score": 0.5241346701029371,
            "fpr": 0.009868421052631578,
            "logloss": 0.8198108472480545,
            "mae": 0.5079631759997523,
            "precision": 0.47058823529411764,
            "recall": 0.016913319238900635
        },
        "train": {
            "accuracy": 0.47310647639956094,
            "auc_prc": 0.5525124215874192,
            "auditor_fn_violation": 0.0002692889630320994,
            "auditor_fp_violation": 0.0008858142087662419,
            "ave_precision_score": 0.5302050803019787,
            "fpr": 0.003293084522502744,
            "logloss": 0.8289367622517606,
            "mae": 0.5119194577475672,
            "precision": 0.5714285714285714,
            "recall": 0.008316008316008316
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(23)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.7747258999327439,
            "auditor_fn_violation": 0.0076499388004896045,
            "auditor_fp_violation": 0.005437497502297888,
            "ave_precision_score": 0.7751170330981505,
            "fpr": 0.03289473684210526,
            "logloss": 1.4126919120724832,
            "mae": 0.3843021326324559,
            "precision": 0.8543689320388349,
            "recall": 0.37209302325581395
        },
        "train": {
            "accuracy": 0.6421514818880352,
            "auc_prc": 0.7906725256794053,
            "auditor_fn_violation": 0.007613118480297415,
            "auditor_fp_violation": 0.0059735021570980025,
            "ave_precision_score": 0.7910471789206222,
            "fpr": 0.021953896816684963,
            "logloss": 1.3473163036468754,
            "mae": 0.38529954250975645,
            "precision": 0.8974358974358975,
            "recall": 0.36382536382536385
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(24)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5054824561403509,
            "auc_prc": 0.4916952240421999,
            "auditor_fn_violation": 0.010232372686473052,
            "auditor_fp_violation": 0.009438816289014117,
            "ave_precision_score": 0.4932323246543143,
            "fpr": 0.2675438596491228,
            "logloss": 0.695849932417895,
            "mae": 0.5010119653061816,
            "precision": 0.5215686274509804,
            "recall": 0.5623678646934461
        },
        "train": {
            "accuracy": 0.5159165751920965,
            "auc_prc": 0.5043769655338877,
            "auditor_fn_violation": 0.007928049640453592,
            "auditor_fp_violation": 0.011109692900722444,
            "ave_precision_score": 0.5060157354708199,
            "fpr": 0.25905598243688255,
            "logloss": 0.6940727178550238,
            "mae": 0.5001309831370114,
            "precision": 0.5390625,
            "recall": 0.5738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(25)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7569301457208204,
            "auditor_fn_violation": 0.009910147991543342,
            "auditor_fp_violation": 0.06925128881429085,
            "ave_precision_score": 0.6804615632752837,
            "fpr": 0.22478070175438597,
            "logloss": 0.6145416296835515,
            "mae": 0.39523855961664867,
            "precision": 0.6531302876480541,
            "recall": 0.8160676532769556
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7879158098757137,
            "auditor_fn_violation": 0.014112567350767134,
            "auditor_fp_violation": 0.07975135935465755,
            "ave_precision_score": 0.7140610440119926,
            "fpr": 0.21624588364434688,
            "logloss": 0.5932415775364397,
            "mae": 0.3915647302742495,
            "precision": 0.66890756302521,
            "recall": 0.8274428274428275
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(26)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6052631578947368,
            "auc_prc": 0.6627543440057428,
            "auditor_fn_violation": 0.06487379919142466,
            "auditor_fp_violation": 0.0739844343204252,
            "ave_precision_score": 0.6047612348294833,
            "fpr": 0.23903508771929824,
            "logloss": 0.6929998031368313,
            "mae": 0.4522535806652485,
            "precision": 0.6029143897996357,
            "recall": 0.6997885835095138
        },
        "train": {
            "accuracy": 0.610318331503842,
            "auc_prc": 0.7079410967707398,
            "auditor_fn_violation": 0.07027300880209773,
            "auditor_fp_violation": 0.07677737216960663,
            "ave_precision_score": 0.6280444897350451,
            "fpr": 0.2283205268935236,
            "logloss": 0.6791922101451734,
            "mae": 0.44855394898929135,
            "precision": 0.6162361623616236,
            "recall": 0.6943866943866944
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(27)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7349848901124068,
            "auditor_fn_violation": 0.010471143503579255,
            "auditor_fp_violation": 0.011776665467769652,
            "ave_precision_score": 0.7362890121587373,
            "fpr": 0.08333333333333333,
            "logloss": 0.6239533887199633,
            "mae": 0.36130493323594387,
            "precision": 0.7790697674418605,
            "recall": 0.5665961945031712
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.842765232708467,
            "auditor_fn_violation": 0.015114413577640809,
            "auditor_fp_violation": 0.008117836264774208,
            "ave_precision_score": 0.8430557635213776,
            "fpr": 0.06037321624588365,
            "logloss": 0.5287995739093969,
            "mae": 0.33017482804421505,
            "precision": 0.8472222222222222,
            "recall": 0.6340956340956341
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(28)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8498319470620325,
            "auditor_fn_violation": 0.0076244390044879675,
            "auditor_fp_violation": 0.008192462934100635,
            "ave_precision_score": 0.8417250178290802,
            "fpr": 0.08662280701754387,
            "logloss": 0.5223577797712622,
            "mae": 0.3133275256565723,
            "precision": 0.8049382716049382,
            "recall": 0.6892177589852009
        },
        "train": {
            "accuracy": 0.7947310647639956,
            "auc_prc": 0.8867093421094625,
            "auditor_fn_violation": 0.01537000988153568,
            "auditor_fp_violation": 0.008888775432057796,
            "ave_precision_score": 0.8808894605786421,
            "fpr": 0.06147091108671789,
            "logloss": 0.4738420443486835,
            "mae": 0.29151233441874175,
            "precision": 0.8620689655172413,
            "recall": 0.7276507276507277
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(29)",
        "seed": 9296,
        "test": {
            "accuracy": 0.643640350877193,
            "auc_prc": 0.7537662968267591,
            "auditor_fn_violation": 0.01156299840510368,
            "auditor_fp_violation": 0.005449986012868169,
            "ave_precision_score": 0.6827200798308868,
            "fpr": 0.16447368421052633,
            "logloss": 0.6249460864987306,
            "mae": 0.43916806722419305,
            "precision": 0.6651785714285714,
            "recall": 0.6300211416490487
        },
        "train": {
            "accuracy": 0.6553238199780461,
            "auc_prc": 0.7830939555263272,
            "auditor_fn_violation": 0.009607682494619935,
            "auditor_fp_violation": 0.007602174967452076,
            "ave_precision_score": 0.7157097131921553,
            "fpr": 0.16355653128430298,
            "logloss": 0.6142395085905322,
            "mae": 0.433838081739344,
            "precision": 0.6795698924731183,
            "recall": 0.656964656964657
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(30)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7922499381301543,
            "auditor_fn_violation": 0.012024312896405922,
            "auditor_fp_violation": 0.019569496063621475,
            "ave_precision_score": 0.7926812414152655,
            "fpr": 0.15021929824561403,
            "logloss": 0.7131213816451316,
            "mae": 0.3263959800566673,
            "precision": 0.7078891257995735,
            "recall": 0.7019027484143763
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.786565899861868,
            "auditor_fn_violation": 0.014952383777850301,
            "auditor_fp_violation": 0.02349832792995176,
            "ave_precision_score": 0.7870581285926899,
            "fpr": 0.1437980241492865,
            "logloss": 0.7063975554962337,
            "mae": 0.3249864616409827,
            "precision": 0.720682302771855,
            "recall": 0.7027027027027027
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(31)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7418373194369855,
            "auditor_fn_violation": 0.04021549645784653,
            "auditor_fp_violation": 0.026687947088678426,
            "ave_precision_score": 0.743100479392873,
            "fpr": 0.17324561403508773,
            "logloss": 1.3822778686035513,
            "mae": 0.3196786442707972,
            "precision": 0.69140625,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7865992791285079,
            "auditor_fn_violation": 0.027976384727208006,
            "auditor_fp_violation": 0.041702192836902977,
            "ave_precision_score": 0.7876134214652353,
            "fpr": 0.1712403951701427,
            "logloss": 1.0748698169764122,
            "mae": 0.3058311948487312,
            "precision": 0.7073170731707317,
            "recall": 0.7837837837837838
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(32)",
        "seed": 9296,
        "test": {
            "accuracy": 0.4857456140350877,
            "auc_prc": 0.43833601147040463,
            "auditor_fn_violation": 0.0008924928600571347,
            "auditor_fp_violation": 0.0018283179474883112,
            "ave_precision_score": 0.5202220685812211,
            "fpr": 0.007675438596491228,
            "logloss": 17.06837246980077,
            "mae": 0.515978712551786,
            "precision": 0.6111111111111112,
            "recall": 0.023255813953488372
        },
        "train": {
            "accuracy": 0.47091108671789245,
            "auc_prc": 0.4034131159944564,
            "auditor_fn_violation": 0.0024806534136940514,
            "auditor_fp_violation": 0.003436040129681158,
            "ave_precision_score": 0.5270388901856538,
            "fpr": 0.008781558726673985,
            "logloss": 17.650851706287575,
            "mae": 0.5299215025210223,
            "precision": 0.4666666666666667,
            "recall": 0.014553014553014554
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(33)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8683753140561342,
            "auditor_fn_violation": 0.002886113274730169,
            "auditor_fp_violation": 0.012196279422930906,
            "ave_precision_score": 0.8603785116842082,
            "fpr": 0.10197368421052631,
            "logloss": 1.9984485904240648,
            "mae": 0.23234662258849917,
            "precision": 0.7919463087248322,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.7947310647639956,
            "auc_prc": 0.8802439933442175,
            "auditor_fn_violation": 0.006745916734939789,
            "auditor_fp_violation": 0.00988691190360708,
            "ave_precision_score": 0.8677476479758863,
            "fpr": 0.0845225027442371,
            "logloss": 2.075348117600389,
            "mae": 0.2053346328916868,
            "precision": 0.828125,
            "recall": 0.7713097713097713
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(34)",
        "seed": 9296,
        "test": {
            "accuracy": 0.48903508771929827,
            "auc_prc": 0.607427158450309,
            "auditor_fn_violation": 0.0007070397982270682,
            "auditor_fp_violation": 0.005215202014146985,
            "ave_precision_score": 0.6094617717741629,
            "fpr": 0.03070175438596491,
            "logloss": 0.8298637573238038,
            "mae": 0.48535267137432175,
            "precision": 0.5555555555555556,
            "recall": 0.07399577167019028
        },
        "train": {
            "accuracy": 0.47200878155872666,
            "auc_prc": 0.6058016907265172,
            "auditor_fn_violation": 0.0083593684032762,
            "auditor_fp_violation": 0.009366145048885713,
            "ave_precision_score": 0.6073547406808781,
            "fpr": 0.04061470911086718,
            "logloss": 0.8315515924208163,
            "mae": 0.4900013256396865,
            "precision": 0.5,
            "recall": 0.07692307692307693
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(35)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.5781583289777166,
            "auditor_fn_violation": 0.010807277178146214,
            "auditor_fp_violation": 0.02172251528593696,
            "ave_precision_score": 0.615312425892506,
            "fpr": 0.35855263157894735,
            "logloss": 0.6582905222938755,
            "mae": 0.4500381943949482,
            "precision": 0.5753246753246753,
            "recall": 0.9365750528541226
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.670651577462678,
            "auditor_fn_violation": 0.010566168634225714,
            "auditor_fp_violation": 0.0155464222806525,
            "ave_precision_score": 0.6768145412180764,
            "fpr": 0.3556531284302964,
            "logloss": 0.6359843587230083,
            "mae": 0.4409967632606446,
            "precision": 0.583547557840617,
            "recall": 0.9438669438669439
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(36)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8203391447998901,
            "auditor_fn_violation": 0.010566188197767149,
            "auditor_fp_violation": 0.00951374735243576,
            "ave_precision_score": 0.8139572847955704,
            "fpr": 0.09978070175438597,
            "logloss": 0.5144226684923873,
            "mae": 0.3185455657443718,
            "precision": 0.7908045977011494,
            "recall": 0.7272727272727273
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8761074701524123,
            "auditor_fn_violation": 0.016837406519075015,
            "auditor_fp_violation": 0.0030735455543358975,
            "ave_precision_score": 0.8680957461402647,
            "fpr": 0.09440175631174534,
            "logloss": 0.4737092389228843,
            "mae": 0.3035752046996427,
            "precision": 0.8097345132743363,
            "recall": 0.760914760914761
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(37)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.6520373884040847,
            "auditor_fn_violation": 0.022711045584362596,
            "auditor_fp_violation": 0.023051292810614236,
            "ave_precision_score": 0.6528436451985634,
            "fpr": 0.15899122807017543,
            "logloss": 0.6511807236727662,
            "mae": 0.4669460266044265,
            "precision": 0.6643518518518519,
            "recall": 0.6067653276955602
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6400795552985589,
            "auditor_fn_violation": 0.006549655287306225,
            "auditor_fp_violation": 0.02090725754984301,
            "ave_precision_score": 0.6423050713978621,
            "fpr": 0.15587266739846323,
            "logloss": 0.6542312154688668,
            "mae": 0.4688000332249245,
            "precision": 0.6658823529411765,
            "recall": 0.5883575883575883
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(38)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.6569337933156808,
            "auditor_fn_violation": 0.0065835836949668025,
            "auditor_fp_violation": 0.006821224473484395,
            "ave_precision_score": 0.6461925300941295,
            "fpr": 0.039473684210526314,
            "logloss": 0.7486589555253049,
            "mae": 0.45866551846592574,
            "precision": 0.7410071942446043,
            "recall": 0.21775898520084566
        },
        "train": {
            "accuracy": 0.5455543358946213,
            "auc_prc": 0.6642184737940297,
            "auditor_fn_violation": 0.012115721226588405,
            "auditor_fp_violation": 0.003686212442243382,
            "ave_precision_score": 0.6548828982145728,
            "fpr": 0.024149286498353458,
            "logloss": 0.7689002858022189,
            "mae": 0.46890733872758045,
            "precision": 0.8018018018018018,
            "recall": 0.18503118503118504
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(39)",
        "seed": 9296,
        "test": {
            "accuracy": 0.48135964912280704,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 1.0949946611244237,
            "mae": 0.5119731650685712,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.47200878155872666,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 1.085199012227653,
            "mae": 0.5133176657680982,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(40)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.7715649373149784,
            "auditor_fn_violation": 0.0037716516449686683,
            "auditor_fp_violation": 0.010512828198057787,
            "ave_precision_score": 0.7700079107626605,
            "fpr": 0.10635964912280702,
            "logloss": 0.5595498937448878,
            "mae": 0.36401520060015874,
            "precision": 0.7754629629629629,
            "recall": 0.7082452431289641
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8348060686150209,
            "auditor_fn_violation": 0.008158542735930224,
            "auditor_fp_violation": 0.00846246138922217,
            "ave_precision_score": 0.809139899086925,
            "fpr": 0.08232711306256861,
            "logloss": 0.517476031833351,
            "mae": 0.3419984673465777,
            "precision": 0.823943661971831,
            "recall": 0.7297297297297297
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(41)",
        "seed": 9296,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.8490168028707201,
            "auditor_fn_violation": 0.037714198286413717,
            "auditor_fp_violation": 0.016619709866922433,
            "ave_precision_score": 0.8495036249612433,
            "fpr": 0.09868421052631579,
            "logloss": 0.6415569865748086,
            "mae": 0.30690548817283836,
            "precision": 0.7963800904977375,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8755731997093804,
            "auditor_fn_violation": 0.03495051244776822,
            "auditor_fp_violation": 0.014162816225461415,
            "ave_precision_score": 0.8757470193842581,
            "fpr": 0.09659714599341383,
            "logloss": 0.6586608338473758,
            "mae": 0.2930935966221677,
            "precision": 0.8074398249452954,
            "recall": 0.7671517671517671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(42)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8472877725431178,
            "auditor_fn_violation": 0.008519250027817966,
            "auditor_fp_violation": 0.013772329456899657,
            "ave_precision_score": 0.8473157014199459,
            "fpr": 0.1118421052631579,
            "logloss": 0.9480079521356966,
            "mae": 0.25389358405488477,
            "precision": 0.7792207792207793,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.7914379802414928,
            "auc_prc": 0.8770869508825395,
            "auditor_fn_violation": 0.01077612274099651,
            "auditor_fp_violation": 0.008143364051770356,
            "ave_precision_score": 0.8772384194636607,
            "fpr": 0.09549945115257959,
            "logloss": 0.8425128821791567,
            "mae": 0.22724692477551384,
            "precision": 0.8129032258064516,
            "recall": 0.7858627858627859
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(43)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5219298245614035,
            "auc_prc": 0.7558020621693612,
            "auditor_fn_violation": 0.0009759467378806425,
            "auditor_fp_violation": 0.0016085201614514657,
            "ave_precision_score": 0.5207227951910085,
            "fpr": 0.4682017543859649,
            "logloss": 0.6932157205312337,
            "mae": 0.49829240803394403,
            "precision": 0.5207631874298541,
            "recall": 0.9809725158562368
        },
        "train": {
            "accuracy": 0.5411635565312843,
            "auc_prc": 0.7652632673376513,
            "auditor_fn_violation": 0.003103669404437791,
            "auditor_fp_violation": 0.004753273938682254,
            "ave_precision_score": 0.5353557127328271,
            "fpr": 0.45334796926454446,
            "logloss": 0.6873241508113088,
            "mae": 0.4954426516947448,
            "precision": 0.5354330708661418,
            "recall": 0.9896049896049897
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(44)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6491228070175439,
            "auc_prc": 0.6822047265399125,
            "auditor_fn_violation": 0.014602110455843633,
            "auditor_fp_violation": 0.009243995524117815,
            "ave_precision_score": 0.6617569601882203,
            "fpr": 0.08442982456140351,
            "logloss": 0.6437866450139271,
            "mae": 0.4576321728200766,
            "precision": 0.749185667752443,
            "recall": 0.48625792811839325
        },
        "train": {
            "accuracy": 0.6399560922063666,
            "auc_prc": 0.7551610072482388,
            "auditor_fn_violation": 0.02283479122117981,
            "auditor_fp_violation": 0.005197457432415183,
            "ave_precision_score": 0.7005675509628193,
            "fpr": 0.07354555433589462,
            "logloss": 0.6273233742709772,
            "mae": 0.4498553517817404,
            "precision": 0.7665505226480837,
            "recall": 0.4573804573804574
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(45)",
        "seed": 9296,
        "test": {
            "accuracy": 0.48135964912280704,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.913203108325387,
            "mae": 0.518640350877193,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.47200878155872666,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.236170632219583,
            "mae": 0.5279912184412733,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(46)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.7316562452292521,
            "auditor_fn_violation": 0.0049655057304996125,
            "auditor_fp_violation": 0.013812292690724537,
            "ave_precision_score": 0.7323853154330124,
            "fpr": 0.20285087719298245,
            "logloss": 0.6119460317972646,
            "mae": 0.4089112573286943,
            "precision": 0.6678635547576302,
            "recall": 0.7864693446088795
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.8094757184153061,
            "auditor_fn_violation": 0.0035806303643844814,
            "auditor_fp_violation": 0.019094784673116687,
            "ave_precision_score": 0.8098080646688155,
            "fpr": 0.1986827661909989,
            "logloss": 0.5609089150536826,
            "mae": 0.3898224911512449,
            "precision": 0.6779359430604982,
            "recall": 0.7920997920997921
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(47)",
        "seed": 9296,
        "test": {
            "accuracy": 0.518640350877193,
            "auc_prc": 0.6895015237103455,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5695751789087593,
            "fpr": 0.48135964912280704,
            "logloss": 0.7194604446149855,
            "mae": 0.4794440909958722,
            "precision": 0.518640350877193,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5279912184412733,
            "auc_prc": 0.6808215456280327,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5707870423355003,
            "fpr": 0.47200878155872666,
            "logloss": 0.7135282857748693,
            "mae": 0.47910096459540524,
            "precision": 0.5279912184412733,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(48)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.6986652564656687,
            "auditor_fn_violation": 0.03008975928192575,
            "auditor_fp_violation": 0.02057607001558566,
            "ave_precision_score": 0.7005916896165945,
            "fpr": 0.16776315789473684,
            "logloss": 0.6756574127867289,
            "mae": 0.37797698550531483,
            "precision": 0.6772151898734177,
            "recall": 0.678646934460888
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.730570013028111,
            "auditor_fn_violation": 0.022713839398800985,
            "auditor_fp_violation": 0.018374901079825395,
            "ave_precision_score": 0.7420415390494692,
            "fpr": 0.14709110867178923,
            "logloss": 0.6182033079069719,
            "mae": 0.3667971317925954,
            "precision": 0.7130620985010707,
            "recall": 0.6923076923076923
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(49)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8126892879471715,
            "auditor_fn_violation": 0.012061403508771934,
            "auditor_fp_violation": 0.007550553490788473,
            "ave_precision_score": 0.8096839157615002,
            "fpr": 0.07894736842105263,
            "logloss": 0.555542885165182,
            "mae": 0.3258023250341546,
            "precision": 0.8158567774936062,
            "recall": 0.6744186046511628
        },
        "train": {
            "accuracy": 0.7837541163556532,
            "auc_prc": 0.8780842291775689,
            "auditor_fn_violation": 0.009514115990515555,
            "auditor_fp_violation": 0.006037321624588366,
            "ave_precision_score": 0.8743749387394151,
            "fpr": 0.06915477497255763,
            "logloss": 0.478605675431315,
            "mae": 0.2954050633150565,
            "precision": 0.8463414634146341,
            "recall": 0.7214137214137214
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(50)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.7305404671652558,
            "auditor_fn_violation": 0.05041077853195357,
            "auditor_fp_violation": 0.029115713543539944,
            "ave_precision_score": 0.7315348376103121,
            "fpr": 0.11513157894736842,
            "logloss": 0.9298712208401622,
            "mae": 0.338219315790059,
            "precision": 0.733502538071066,
            "recall": 0.6109936575052854
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8103695306349114,
            "auditor_fn_violation": 0.047429089141493096,
            "auditor_fp_violation": 0.03100605008551809,
            "ave_precision_score": 0.8111555944492909,
            "fpr": 0.0845225027442371,
            "logloss": 0.6808249839711266,
            "mae": 0.29930375854375246,
            "precision": 0.8025641025641026,
            "recall": 0.6507276507276507
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(51)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5493421052631579,
            "auc_prc": 0.667819853981356,
            "auditor_fn_violation": 0.01316716738993361,
            "auditor_fp_violation": 0.01028803500779283,
            "ave_precision_score": 0.6361292937333665,
            "fpr": 0.3684210526315789,
            "logloss": 0.8795799045562397,
            "mae": 0.4563333961335031,
            "precision": 0.5422343324250681,
            "recall": 0.8414376321353065
        },
        "train": {
            "accuracy": 0.5817782656421515,
            "auc_prc": 0.7303644256759728,
            "auditor_fn_violation": 0.008389035831406853,
            "auditor_fp_violation": 0.01833405662063157,
            "ave_precision_score": 0.6964181686464358,
            "fpr": 0.3545554335894621,
            "logloss": 0.7848491699678262,
            "mae": 0.4284546999685041,
            "precision": 0.5670241286863271,
            "recall": 0.8794178794178794
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(52)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8451683493644156,
            "auditor_fn_violation": 0.009370015948963322,
            "auditor_fp_violation": 0.020663489589577595,
            "ave_precision_score": 0.8454901247568434,
            "fpr": 0.12609649122807018,
            "logloss": 0.517886261730524,
            "mae": 0.3530893725212337,
            "precision": 0.764344262295082,
            "recall": 0.7885835095137421
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8543859687209608,
            "auditor_fn_violation": 0.01835044535373844,
            "auditor_fp_violation": 0.02264314706558089,
            "ave_precision_score": 0.854669558805495,
            "fpr": 0.1207464324917673,
            "logloss": 0.5098800318187823,
            "mae": 0.3418919209198685,
            "precision": 0.7759674134419552,
            "recall": 0.7920997920997921
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(53)",
        "seed": 9296,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.5897300057172161,
            "auditor_fn_violation": 0.0006189495938578016,
            "auditor_fp_violation": 0.004058765935339488,
            "ave_precision_score": 0.5912752636825961,
            "fpr": 0.008771929824561403,
            "logloss": 2.461003999404132,
            "mae": 0.523504362261701,
            "precision": 0.1111111111111111,
            "recall": 0.0021141649048625794
        },
        "train": {
            "accuracy": 0.4654226125137212,
            "auc_prc": 0.5714558984675686,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0025017231256222396,
            "ave_precision_score": 0.5729982971553872,
            "fpr": 0.006586169045005488,
            "logloss": 2.553756663586724,
            "mae": 0.5326099309605392,
            "precision": 0.0,
            "recall": 0.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(54)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.815900022963495,
            "auditor_fn_violation": 0.003926968584251341,
            "auditor_fp_violation": 0.017628781521000683,
            "ave_precision_score": 0.8147956039991509,
            "fpr": 0.07456140350877193,
            "logloss": 0.5486243087728699,
            "mae": 0.3298356412632162,
            "precision": 0.8196286472148541,
            "recall": 0.653276955602537
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8741677090000758,
            "auditor_fn_violation": 0.013425652284049657,
            "auditor_fp_violation": 0.012904296326551452,
            "ave_precision_score": 0.8722314874593484,
            "fpr": 0.06037321624588365,
            "logloss": 0.5021166462100269,
            "mae": 0.3106862702534731,
            "precision": 0.8582474226804123,
            "recall": 0.6923076923076923
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(55)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8196348697085356,
            "auditor_fn_violation": 0.011013593709432147,
            "auditor_fp_violation": 0.0123911001878272,
            "ave_precision_score": 0.8200088511653633,
            "fpr": 0.10197368421052631,
            "logloss": 0.9028343016880291,
            "mae": 0.3473284863703476,
            "precision": 0.7806603773584906,
            "recall": 0.6997885835095138
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8356009842483116,
            "auditor_fn_violation": 0.009721787987430142,
            "auditor_fp_violation": 0.009036836596635445,
            "ave_precision_score": 0.8359439730186338,
            "fpr": 0.07683863885839737,
            "logloss": 0.5705976539565824,
            "mae": 0.3257177692728626,
            "precision": 0.8341232227488151,
            "recall": 0.7318087318087318
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(56)",
        "seed": 9296,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.778361139805897,
            "auditor_fn_violation": 0.011396090649456628,
            "auditor_fp_violation": 0.01951454661711226,
            "ave_precision_score": 0.7435811148719835,
            "fpr": 0.15350877192982457,
            "logloss": 2.3578573194556314,
            "mae": 0.29946958645310234,
            "precision": 0.7244094488188977,
            "recall": 0.7780126849894292
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8045027745933953,
            "auditor_fn_violation": 0.005835354902314294,
            "auditor_fp_violation": 0.018507645572205352,
            "ave_precision_score": 0.7731878297646745,
            "fpr": 0.14270032930845225,
            "logloss": 2.165313436698185,
            "mae": 0.2862766603671838,
            "precision": 0.7445972495088409,
            "recall": 0.7879417879417879
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(57)",
        "seed": 9296,
        "test": {
            "accuracy": 0.518640350877193,
            "auc_prc": 0.2593201754385965,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.518640350877193,
            "fpr": 0.48135964912280704,
            "logloss": 0.6925145626505494,
            "mae": 0.4995133095796694,
            "precision": 0.518640350877193,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5279912184412733,
            "auc_prc": 0.43028667864013637,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5279330246399401,
            "fpr": 0.47200878155872666,
            "logloss": 0.6920266447624656,
            "mae": 0.4992693995670482,
            "precision": 0.5279912184412733,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(58)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7592450155223696,
            "auditor_fn_violation": 0.00972933125625904,
            "auditor_fp_violation": 0.01751138952164009,
            "ave_precision_score": 0.7595342585597413,
            "fpr": 0.1611842105263158,
            "logloss": 0.8447378162988968,
            "mae": 0.32385729970063265,
            "precision": 0.7071713147410359,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7952712229921989,
            "auditor_fn_violation": 0.011015744275897955,
            "auditor_fp_violation": 0.016108033594567688,
            "ave_precision_score": 0.7964794201625692,
            "fpr": 0.13830954994511527,
            "logloss": 0.8084263096364032,
            "mae": 0.3114681599465425,
            "precision": 0.7347368421052631,
            "recall": 0.7255717255717256
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(59)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5493421052631579,
            "auc_prc": 0.701399440891364,
            "auditor_fn_violation": 0.009365379622417565,
            "auditor_fp_violation": 0.013902209966830522,
            "ave_precision_score": 0.673034351482409,
            "fpr": 0.4243421052631579,
            "logloss": 0.8429888727644809,
            "mae": 0.4219731269731072,
            "precision": 0.5370813397129187,
            "recall": 0.9492600422832981
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.7531162356124039,
            "auditor_fn_violation": 0.004815251796590985,
            "auditor_fp_violation": 0.015291144410691048,
            "ave_precision_score": 0.7248670660738229,
            "fpr": 0.4039517014270033,
            "logloss": 0.7769905567264471,
            "mae": 0.3967992642119478,
            "precision": 0.5603345280764636,
            "recall": 0.975051975051975
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(60)",
        "seed": 9296,
        "test": {
            "accuracy": 0.4901315789473684,
            "auc_prc": 0.5913987680423538,
            "auditor_fn_violation": 0.019936204146730457,
            "auditor_fp_violation": 0.021330376054030296,
            "ave_precision_score": 0.5186305288217297,
            "fpr": 0.23684210526315788,
            "logloss": 0.6835062888738747,
            "mae": 0.48871459607688483,
            "precision": 0.509090909090909,
            "recall": 0.47357293868921774
        },
        "train": {
            "accuracy": 0.47639956092206365,
            "auc_prc": 0.5998421452718488,
            "auditor_fn_violation": 0.030153517530026864,
            "auditor_fp_violation": 0.030117683098052236,
            "ave_precision_score": 0.5271382079145362,
            "fpr": 0.2327113062568606,
            "logloss": 0.6837519251417123,
            "mae": 0.4896941262365827,
            "precision": 0.5046728971962616,
            "recall": 0.4490644490644491
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(61)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8493260141870427,
            "auditor_fn_violation": 0.006680946552427589,
            "auditor_fp_violation": 0.0027025136874075854,
            "ave_precision_score": 0.8308270794920332,
            "fpr": 0.08442982456140351,
            "logloss": 0.522154132901554,
            "mae": 0.3141360713879725,
            "precision": 0.8108108108108109,
            "recall": 0.6976744186046512
        },
        "train": {
            "accuracy": 0.7969264544456641,
            "auc_prc": 0.8848697514505263,
            "auditor_fn_violation": 0.006508577309894549,
            "auditor_fp_violation": 0.003921068082607921,
            "ave_precision_score": 0.8687686154085608,
            "fpr": 0.06366630076838639,
            "logloss": 0.47182898062120154,
            "mae": 0.29060449013660294,
            "precision": 0.8592233009708737,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(62)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5844298245614035,
            "auc_prc": 0.6112099045934323,
            "auditor_fn_violation": 0.05512592262898262,
            "auditor_fp_violation": 0.030691763577508697,
            "ave_precision_score": 0.6083162846790419,
            "fpr": 0.09210526315789473,
            "logloss": 3.8326365033196494,
            "mae": 0.4411358242451476,
            "precision": 0.6793893129770993,
            "recall": 0.3763213530655391
        },
        "train": {
            "accuracy": 0.5982436882546652,
            "auc_prc": 0.6459829303538143,
            "auditor_fn_violation": 0.046486577770880735,
            "auditor_fp_violation": 0.030806933346948152,
            "ave_precision_score": 0.6421158201534489,
            "fpr": 0.0889132821075741,
            "logloss": 3.4603816322770617,
            "mae": 0.426480668497948,
            "precision": 0.7075812274368231,
            "recall": 0.4074844074844075
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(63)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.802186031031495,
            "auditor_fn_violation": 0.030817662549608705,
            "auditor_fp_violation": 0.011936518403069177,
            "ave_precision_score": 0.6882286432264357,
            "fpr": 0.07894736842105263,
            "logloss": 0.5928441123131594,
            "mae": 0.3940930018822352,
            "precision": 0.797752808988764,
            "recall": 0.6004228329809725
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8303691425439369,
            "auditor_fn_violation": 0.01774112202213191,
            "auditor_fp_violation": 0.006575957930207036,
            "ave_precision_score": 0.7229355772020633,
            "fpr": 0.06366630076838639,
            "logloss": 0.5652629530796315,
            "mae": 0.38132962299885004,
            "precision": 0.8384401114206128,
            "recall": 0.6257796257796258
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(64)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5581140350877193,
            "auc_prc": 0.6977864303592122,
            "auditor_fn_violation": 0.004511145729015986,
            "auditor_fp_violation": 0.012426068017423969,
            "ave_precision_score": 0.6979467584152137,
            "fpr": 0.4342105263157895,
            "logloss": 0.9948499568355009,
            "mae": 0.4174236137059408,
            "precision": 0.5406032482598608,
            "recall": 0.985200845665962
        },
        "train": {
            "accuracy": 0.579582875960483,
            "auc_prc": 0.762601033038333,
            "auditor_fn_violation": 0.004173978926997588,
            "auditor_fp_violation": 0.015531105608454806,
            "ave_precision_score": 0.7637011709634294,
            "fpr": 0.4127332601536773,
            "logloss": 0.8930063622676124,
            "mae": 0.39743557118684764,
            "precision": 0.5576470588235294,
            "recall": 0.9854469854469855
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(65)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7550142472129351,
            "auditor_fn_violation": 0.006952171655354037,
            "auditor_fp_violation": 0.009286456460056748,
            "ave_precision_score": 0.7464322648530473,
            "fpr": 0.09429824561403509,
            "logloss": 0.5746284692283137,
            "mae": 0.35014304119517636,
            "precision": 0.7881773399014779,
            "recall": 0.6765327695560254
        },
        "train": {
            "accuracy": 0.7848518111964874,
            "auc_prc": 0.8223637200914965,
            "auditor_fn_violation": 0.014372727874374418,
            "auditor_fp_violation": 0.007096724784928394,
            "ave_precision_score": 0.8107595452398966,
            "fpr": 0.06586169045005488,
            "logloss": 0.506840162805709,
            "mae": 0.3207477517962726,
            "precision": 0.8518518518518519,
            "recall": 0.7172557172557172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(66)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.7483523207179988,
            "auditor_fn_violation": 0.014254385964912285,
            "auditor_fp_violation": 0.01964942253127123,
            "ave_precision_score": 0.7102734966308462,
            "fpr": 0.13596491228070176,
            "logloss": 2.833627873901906,
            "mae": 0.3524277343246957,
            "precision": 0.7082352941176471,
            "recall": 0.6363636363636364
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7841642642131168,
            "auditor_fn_violation": 0.014327085677250329,
            "auditor_fp_violation": 0.023393664003267565,
            "ave_precision_score": 0.7525143694274145,
            "fpr": 0.13391877058177826,
            "logloss": 2.460209821574135,
            "mae": 0.33442757245777255,
            "precision": 0.7169373549883991,
            "recall": 0.6424116424116424
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(67)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8215332183829338,
            "auditor_fn_violation": 0.005507955936352515,
            "auditor_fp_violation": 0.008889321823921992,
            "ave_precision_score": 0.8155074176976387,
            "fpr": 0.12609649122807018,
            "logloss": 0.5377247735477609,
            "mae": 0.32764816940406566,
            "precision": 0.7537473233404711,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8750596586499897,
            "auditor_fn_violation": 0.015107567248072192,
            "auditor_fp_violation": 0.0057692798611288405,
            "ave_precision_score": 0.8685848330389361,
            "fpr": 0.10757409440175632,
            "logloss": 0.4778128974147741,
            "mae": 0.3112181328542404,
            "precision": 0.7874186550976139,
            "recall": 0.7546777546777547
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(68)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.6920754936544542,
            "auditor_fn_violation": 0.025281888653981677,
            "auditor_fp_violation": 0.015997782040522716,
            "ave_precision_score": 0.6924543398076364,
            "fpr": 0.20723684210526316,
            "logloss": 1.8641340128007466,
            "mae": 0.40648229456130874,
            "precision": 0.6365384615384615,
            "recall": 0.6997885835095138
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.7514983988289228,
            "auditor_fn_violation": 0.017928255030340658,
            "auditor_fp_violation": 0.022467005335307495,
            "ave_precision_score": 0.7518906973805843,
            "fpr": 0.17892425905598244,
            "logloss": 1.4621320243805371,
            "mae": 0.3752647274706419,
            "precision": 0.681640625,
            "recall": 0.7255717255717256
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(69)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8208444862744129,
            "auditor_fn_violation": 0.00828511553725752,
            "auditor_fp_violation": 0.01163179874515446,
            "ave_precision_score": 0.7049181636849935,
            "fpr": 0.10416666666666667,
            "logloss": 0.5663427242486171,
            "mae": 0.36232198578746694,
            "precision": 0.7800925925925926,
            "recall": 0.7124735729386892
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8522068966995278,
            "auditor_fn_violation": 0.011467602027426404,
            "auditor_fp_violation": 0.008914303219053942,
            "ave_precision_score": 0.7495529696857728,
            "fpr": 0.0801317233809001,
            "logloss": 0.5203730188097169,
            "mae": 0.3428271854486476,
            "precision": 0.8290398126463701,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(70)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8419888244286867,
            "auditor_fn_violation": 0.004079967360261116,
            "auditor_fp_violation": 0.011456959597170607,
            "ave_precision_score": 0.8191213395143255,
            "fpr": 0.09429824561403509,
            "logloss": 0.5135957839420371,
            "mae": 0.32957614365180854,
            "precision": 0.7985948477751756,
            "recall": 0.7209302325581395
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8766799934551437,
            "auditor_fn_violation": 0.009477602232816285,
            "auditor_fp_violation": 0.0050417379317386985,
            "ave_precision_score": 0.8592009619118481,
            "fpr": 0.08232711306256861,
            "logloss": 0.48261765206171564,
            "mae": 0.3168928119078949,
            "precision": 0.8243559718969555,
            "recall": 0.7318087318087318
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(71)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5515350877192983,
            "auc_prc": 0.5463897891103847,
            "auditor_fn_violation": 0.03913986869923223,
            "auditor_fp_violation": 0.03136114774407546,
            "ave_precision_score": 0.5422075594590325,
            "fpr": 0.27631578947368424,
            "logloss": 2.7314411543147967,
            "mae": 0.46632575080150995,
            "precision": 0.5563380281690141,
            "recall": 0.6680761099365751
        },
        "train": {
            "accuracy": 0.5334796926454446,
            "auc_prc": 0.5632870669629109,
            "auditor_fn_violation": 0.02876371262759847,
            "auditor_fp_violation": 0.02380210859520589,
            "ave_precision_score": 0.5588234962781766,
            "fpr": 0.28210757409440174,
            "logloss": 2.586963479538783,
            "mae": 0.46466274424896625,
            "precision": 0.5491228070175439,
            "recall": 0.6507276507276507
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(72)",
        "seed": 9296,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.6558121397437235,
            "auditor_fn_violation": 0.023520084566596204,
            "auditor_fp_violation": 0.022229548815090126,
            "ave_precision_score": 0.6545115311994109,
            "fpr": 0.2412280701754386,
            "logloss": 0.945010543623661,
            "mae": 0.40756216552517,
            "precision": 0.6147110332749562,
            "recall": 0.7420718816067653
        },
        "train": {
            "accuracy": 0.6465422612513722,
            "auc_prc": 0.7415211369694534,
            "auditor_fn_violation": 0.015118977797353217,
            "auditor_fp_violation": 0.012253337758149746,
            "ave_precision_score": 0.7393766771844705,
            "fpr": 0.21514818880351264,
            "logloss": 0.7953668801975281,
            "mae": 0.36911761217795847,
            "precision": 0.6442831215970962,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(73)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7468228206258685,
            "auditor_fn_violation": 0.017903174956418536,
            "auditor_fp_violation": 0.03327438756344163,
            "ave_precision_score": 0.6895874574746048,
            "fpr": 0.14473684210526316,
            "logloss": 0.6080264543325301,
            "mae": 0.39815004346402066,
            "precision": 0.7105263157894737,
            "recall": 0.6849894291754757
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.7539032740176423,
            "auditor_fn_violation": 0.01641065197596482,
            "auditor_fp_violation": 0.026541240139892276,
            "ave_precision_score": 0.6955399237275341,
            "fpr": 0.15697036223929747,
            "logloss": 0.6081852712290814,
            "mae": 0.4000600000309761,
            "precision": 0.6970338983050848,
            "recall": 0.683991683991684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(74)",
        "seed": 9296,
        "test": {
            "accuracy": 0.4857456140350877,
            "auc_prc": 0.5097511946266426,
            "auditor_fn_violation": 0.01090927636215274,
            "auditor_fp_violation": 0.01610268552931303,
            "ave_precision_score": 0.5173780684218604,
            "fpr": 0.11074561403508772,
            "logloss": 0.6986248167792307,
            "mae": 0.5009628808158532,
            "precision": 0.5097087378640777,
            "recall": 0.2219873150105708
        },
        "train": {
            "accuracy": 0.5093304061470911,
            "auc_prc": 0.5658994165375791,
            "auditor_fn_violation": 0.015771661216227653,
            "auditor_fp_violation": 0.0005565057565159676,
            "ave_precision_score": 0.5693290768485892,
            "fpr": 0.10647639956092206,
            "logloss": 0.6933746821733543,
            "mae": 0.4983480798753242,
            "precision": 0.5745614035087719,
            "recall": 0.27234927234927236
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(75)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8355233792020769,
            "auditor_fn_violation": 0.01310689514483884,
            "auditor_fp_violation": 0.014067058306358156,
            "ave_precision_score": 0.7949983287008214,
            "fpr": 0.11403508771929824,
            "logloss": 0.535918229254455,
            "mae": 0.3361184520548896,
            "precision": 0.7662921348314606,
            "recall": 0.7209302325581395
        },
        "train": {
            "accuracy": 0.7848518111964874,
            "auc_prc": 0.8735459212470886,
            "auditor_fn_violation": 0.01359681052326497,
            "auditor_fp_violation": 0.007285630408699872,
            "ave_precision_score": 0.8341648003693914,
            "fpr": 0.08562019758507135,
            "logloss": 0.4992602122772246,
            "mae": 0.3172581908666211,
            "precision": 0.8231292517006803,
            "recall": 0.7546777546777547
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(76)",
        "seed": 9296,
        "test": {
            "accuracy": 0.48135964912280704,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 1.0878690736712693,
            "mae": 0.49030522600151016,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.47200878155872666,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 1.1248068568321576,
            "mae": 0.49996197704272677,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(77)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5383771929824561,
            "auc_prc": 0.4008097147396766,
            "auditor_fn_violation": 0.0013306257186306146,
            "auditor_fp_violation": 0.013887223754146196,
            "ave_precision_score": 0.5078495590865547,
            "fpr": 0.4594298245614035,
            "logloss": 0.7120354036415748,
            "mae": 0.5033636584126374,
            "precision": 0.5292134831460674,
            "recall": 0.9957716701902748
        },
        "train": {
            "accuracy": 0.5521405049396267,
            "auc_prc": 0.41267989630744367,
            "auditor_fn_violation": 0.003058027207313706,
            "auditor_fp_violation": 0.017920506471294013,
            "ave_precision_score": 0.5328814269725888,
            "fpr": 0.4434687156970362,
            "logloss": 0.7142095192985457,
            "mae": 0.5038531672283107,
            "precision": 0.5414301929625426,
            "recall": 0.9916839916839917
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(78)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8029381184924573,
            "auditor_fn_violation": 0.00827352472089315,
            "auditor_fp_violation": 0.006966091196099589,
            "ave_precision_score": 0.7922681115969512,
            "fpr": 0.04824561403508772,
            "logloss": 0.7013674077831639,
            "mae": 0.34007471147720425,
            "precision": 0.8616352201257862,
            "recall": 0.5792811839323467
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8341858528508359,
            "auditor_fn_violation": 0.008030744583982799,
            "auditor_fp_violation": 0.009835856329614787,
            "ave_precision_score": 0.8240785575832609,
            "fpr": 0.036223929747530186,
            "logloss": 0.6975309132452685,
            "mae": 0.33448121380612955,
            "precision": 0.8955696202531646,
            "recall": 0.5883575883575883
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(79)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7713911329483216,
            "auditor_fn_violation": 0.007394940840473278,
            "auditor_fp_violation": 0.02398293569915679,
            "ave_precision_score": 0.742677168863897,
            "fpr": 0.21710526315789475,
            "logloss": 2.646545927991368,
            "mae": 0.29448250234303186,
            "precision": 0.6727272727272727,
            "recall": 0.8604651162790697
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7992047130443054,
            "auditor_fn_violation": 0.011362624974041,
            "auditor_fp_violation": 0.022597197048987827,
            "ave_precision_score": 0.7761512007155288,
            "fpr": 0.21295279912184412,
            "logloss": 2.2468864458388715,
            "mae": 0.28526966628103356,
            "precision": 0.6819672131147541,
            "recall": 0.8648648648648649
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(80)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.8301810852920697,
            "auditor_fn_violation": 0.012856533511368276,
            "auditor_fp_violation": 0.035459876913239825,
            "ave_precision_score": 0.831458772929991,
            "fpr": 0.3333333333333333,
            "logloss": 0.9651419052543089,
            "mae": 0.35710196570583147,
            "precision": 0.5946666666666667,
            "recall": 0.9429175475687104
        },
        "train": {
            "accuracy": 0.6717892425905598,
            "auc_prc": 0.8520278019367697,
            "auditor_fn_violation": 0.007316444198990852,
            "auditor_fp_violation": 0.038074694304750724,
            "ave_precision_score": 0.8522923783005806,
            "fpr": 0.3018660812294182,
            "logloss": 0.9087421944076959,
            "mae": 0.3372508977027661,
            "precision": 0.6243169398907104,
            "recall": 0.9501039501039501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(81)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5087719298245614,
            "auc_prc": 0.563207640634617,
            "auditor_fn_violation": 0.022351730277066884,
            "auditor_fp_violation": 0.02023888023018823,
            "ave_precision_score": 0.5628804813809913,
            "fpr": 0.04824561403508772,
            "logloss": 2.4079807403927536,
            "mae": 0.4967672250005557,
            "precision": 0.6106194690265486,
            "recall": 0.14587737843551796
        },
        "train": {
            "accuracy": 0.47200878155872666,
            "auc_prc": 0.5460295351774563,
            "auditor_fn_violation": 0.0178483811853735,
            "auditor_fp_violation": 0.01718020064840579,
            "ave_precision_score": 0.5482760164483569,
            "fpr": 0.059275521405049394,
            "logloss": 2.422339684297816,
            "mae": 0.5091349346768457,
            "precision": 0.5,
            "recall": 0.11226611226611227
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(82)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.73515576664054,
            "auditor_fn_violation": 0.003143429398019369,
            "auditor_fp_violation": 0.027946788954162177,
            "ave_precision_score": 0.735697646607767,
            "fpr": 0.12828947368421054,
            "logloss": 0.7910246001601636,
            "mae": 0.33033039585288954,
            "precision": 0.7450980392156863,
            "recall": 0.7230443974630021
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.7390517047372747,
            "auditor_fn_violation": 0.013961948100257655,
            "auditor_fp_violation": 0.03405917341025707,
            "ave_precision_score": 0.7395607216715122,
            "fpr": 0.11306256860592755,
            "logloss": 0.8102301338674212,
            "mae": 0.3133874688961421,
            "precision": 0.7808510638297872,
            "recall": 0.762993762993763
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(83)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5877192982456141,
            "auc_prc": 0.5985939132088576,
            "auditor_fn_violation": 0.03252383071844517,
            "auditor_fp_violation": 0.02664798385485354,
            "ave_precision_score": 0.5879768981559512,
            "fpr": 0.13157894736842105,
            "logloss": 7.587723978134154,
            "mae": 0.4577996824917039,
            "precision": 0.6439169139465876,
            "recall": 0.4587737843551797
        },
        "train": {
            "accuracy": 0.5806805708013172,
            "auc_prc": 0.60830985688227,
            "auditor_fn_violation": 0.037828252976441795,
            "auditor_fp_violation": 0.032366681132412645,
            "ave_precision_score": 0.6003579953376477,
            "fpr": 0.1350164654226125,
            "logloss": 7.3806611949233565,
            "mae": 0.4616883950211866,
            "precision": 0.6434782608695652,
            "recall": 0.46153846153846156
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(84)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6030701754385965,
            "auc_prc": 0.6114320600252726,
            "auditor_fn_violation": 0.002519843477615825,
            "auditor_fp_violation": 0.018430543899612354,
            "ave_precision_score": 0.6062060460406499,
            "fpr": 0.1337719298245614,
            "logloss": 2.292029337243123,
            "mae": 0.42179300896660016,
            "precision": 0.6563380281690141,
            "recall": 0.492600422832981
        },
        "train": {
            "accuracy": 0.6114160263446762,
            "auc_prc": 0.6362056179817578,
            "auditor_fn_violation": 0.0011159517196838843,
            "auditor_fp_violation": 0.016873867204452053,
            "ave_precision_score": 0.6344725093762176,
            "fpr": 0.10976948408342481,
            "logloss": 1.9803681073354966,
            "mae": 0.4214997930773232,
            "precision": 0.6941896024464832,
            "recall": 0.47193347193347196
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(85)",
        "seed": 9296,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.7351899231486405,
            "auditor_fn_violation": 0.035685805422647536,
            "auditor_fp_violation": 0.08046347360428406,
            "ave_precision_score": 0.6676270642892614,
            "fpr": 0.27850877192982454,
            "logloss": 0.6420323550527308,
            "mae": 0.414858028243639,
            "precision": 0.6151515151515151,
            "recall": 0.8583509513742071
        },
        "train": {
            "accuracy": 0.6487376509330406,
            "auc_prc": 0.7370126492650859,
            "auditor_fn_violation": 0.04162796588702187,
            "auditor_fp_violation": 0.07592984964133459,
            "ave_precision_score": 0.6711281884647115,
            "fpr": 0.270032930845225,
            "logloss": 0.6485086597855335,
            "mae": 0.4169751592675627,
            "precision": 0.6232771822358346,
            "recall": 0.8461538461538461
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(86)",
        "seed": 9296,
        "test": {
            "accuracy": 0.48135964912280704,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.913203108325387,
            "mae": 0.518640350877193,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.47200878155872666,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.236170632219583,
            "mae": 0.5279912184412733,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(87)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5252192982456141,
            "auc_prc": 0.5908043261128331,
            "auditor_fn_violation": 0.003442472460220319,
            "auditor_fp_violation": 7.742876553571657e-05,
            "ave_precision_score": 0.5099483896094027,
            "fpr": 0.42653508771929827,
            "logloss": 0.693105626173387,
            "mae": 0.4994595743911831,
            "precision": 0.5244498777506112,
            "recall": 0.9069767441860465
        },
        "train": {
            "accuracy": 0.5301866081229418,
            "auc_prc": 0.6224160211005371,
            "auditor_fn_violation": 0.004621272458813624,
            "auditor_fp_violation": 0.0031450233579251126,
            "ave_precision_score": 0.5326008408990026,
            "fpr": 0.4313940724478595,
            "logloss": 0.6915516052275711,
            "mae": 0.49865792117579444,
            "precision": 0.531585220500596,
            "recall": 0.9272349272349273
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(88)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7794022162151237,
            "auditor_fn_violation": 0.011155001669077562,
            "auditor_fp_violation": 0.017411481437077894,
            "ave_precision_score": 0.7506790843515103,
            "fpr": 0.14692982456140352,
            "logloss": 1.9915355297820119,
            "mae": 0.2956283704527602,
            "precision": 0.7341269841269841,
            "recall": 0.7822410147991543
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.805598967719301,
            "auditor_fn_violation": 0.006663760780116435,
            "auditor_fp_violation": 0.015441758353968297,
            "ave_precision_score": 0.7825986064716428,
            "fpr": 0.1350164654226125,
            "logloss": 1.7315519305903353,
            "mae": 0.2829137289590937,
            "precision": 0.757396449704142,
            "recall": 0.7983367983367984
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(89)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5581140350877193,
            "auc_prc": 0.7403028749802998,
            "auditor_fn_violation": 0.009715422276621795,
            "auditor_fp_violation": 0.0032320265355872606,
            "ave_precision_score": 0.7057524802338468,
            "fpr": 0.043859649122807015,
            "logloss": 0.9560288782403187,
            "mae": 0.41225501061949454,
            "precision": 0.7333333333333333,
            "recall": 0.23255813953488372
        },
        "train": {
            "accuracy": 0.5817782656421515,
            "auc_prc": 0.8111279448792643,
            "auditor_fn_violation": 0.0057554810573471615,
            "auditor_fp_violation": 0.002570648150511832,
            "ave_precision_score": 0.7762536796613784,
            "fpr": 0.018660812294182216,
            "logloss": 0.8948557909307556,
            "mae": 0.39411503855465774,
            "precision": 0.8731343283582089,
            "recall": 0.24324324324324326
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(90)",
        "seed": 9296,
        "test": {
            "accuracy": 0.48135964912280704,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.913203108325387,
            "mae": 0.518640350877193,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.47200878155872666,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.236170632219583,
            "mae": 0.5279912184412733,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(91)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5460526315789473,
            "auc_prc": 0.6936837232104255,
            "auditor_fn_violation": 0.005329457364341086,
            "auditor_fp_violation": 0.006818726771370336,
            "ave_precision_score": 0.5292544636431288,
            "fpr": 0.41776315789473684,
            "logloss": 11.742301473742211,
            "mae": 0.4533849136454574,
            "precision": 0.535931790499391,
            "recall": 0.9302325581395349
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7264588150589287,
            "auditor_fn_violation": 0.004297212859232619,
            "auditor_fp_violation": 0.011538559722257693,
            "ave_precision_score": 0.5736309704845897,
            "fpr": 0.40065861690450055,
            "logloss": 10.13895231452692,
            "mae": 0.4182588786850638,
            "precision": 0.5554202192448234,
            "recall": 0.9480249480249481
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(92)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8158222800184188,
            "auditor_fn_violation": 0.013556618819776713,
            "auditor_fp_violation": 0.013327738480597854,
            "ave_precision_score": 0.727871492683339,
            "fpr": 0.08552631578947369,
            "logloss": 0.6023654141786259,
            "mae": 0.3570603847895798,
            "precision": 0.7941952506596306,
            "recall": 0.6363636363636364
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8508978635825553,
            "auditor_fn_violation": 0.0043907793633369985,
            "auditor_fp_violation": 0.008855589308962809,
            "ave_precision_score": 0.7726693542448722,
            "fpr": 0.06695938529088913,
            "logloss": 0.5452402185550628,
            "mae": 0.33675274602929794,
            "precision": 0.8419689119170984,
            "recall": 0.6756756756756757
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(93)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7929283042678881,
            "auditor_fn_violation": 0.009877693705723091,
            "auditor_fp_violation": 0.020034068656835716,
            "ave_precision_score": 0.7938316673223578,
            "fpr": 0.13596491228070176,
            "logloss": 0.9862615248525446,
            "mae": 0.30196277082898654,
            "precision": 0.7321814254859611,
            "recall": 0.7167019027484144
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.8296262048150485,
            "auditor_fn_violation": 0.015290136036568534,
            "auditor_fp_violation": 0.016358205907129914,
            "ave_precision_score": 0.8298541263421906,
            "fpr": 0.13062568605927552,
            "logloss": 0.8881001199499449,
            "mae": 0.2925461354621557,
            "precision": 0.741304347826087,
            "recall": 0.7089397089397089
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(94)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5219298245614035,
            "auc_prc": 0.6161140231657367,
            "auditor_fn_violation": 0.01250417269389118,
            "auditor_fp_violation": 0.010517823602285899,
            "ave_precision_score": 0.5381537485362455,
            "fpr": 0.0756578947368421,
            "logloss": 0.6891861837745396,
            "mae": 0.4950301038395417,
            "precision": 0.6057142857142858,
            "recall": 0.22410147991543342
        },
        "train": {
            "accuracy": 0.5246981339187706,
            "auc_prc": 0.6344327832253189,
            "auditor_fn_violation": 0.0013989333418532263,
            "auditor_fp_violation": 0.01508947489342149,
            "ave_precision_score": 0.5511346487197201,
            "fpr": 0.0889132821075741,
            "logloss": 0.6875372494154225,
            "mae": 0.49365839924822785,
            "precision": 0.6142857142857143,
            "recall": 0.2681912681912682
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(95)",
        "seed": 9296,
        "test": {
            "accuracy": 0.4791666666666667,
            "auc_prc": 0.6747314793041963,
            "auditor_fn_violation": 0.0006189495938578016,
            "auditor_fp_violation": 0.0018957559045677977,
            "ave_precision_score": 0.6752789424201404,
            "fpr": 0.003289473684210526,
            "logloss": 2.9104114478925607,
            "mae": 0.5189540825035932,
            "precision": 0.25,
            "recall": 0.0021141649048625794
        },
        "train": {
            "accuracy": 0.4698133918770582,
            "auc_prc": 0.7188896741088042,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0008679447578689405,
            "ave_precision_score": 0.720408326391625,
            "fpr": 0.0021953896816684962,
            "logloss": 2.9266907756417164,
            "mae": 0.5278940567619468,
            "precision": 0.0,
            "recall": 0.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(96)",
        "seed": 9296,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8206988065484782,
            "auditor_fn_violation": 0.00828511553725752,
            "auditor_fp_violation": 0.01163179874515446,
            "ave_precision_score": 0.7119408146915387,
            "fpr": 0.10416666666666667,
            "logloss": 0.5661472640253127,
            "mae": 0.35597047038180263,
            "precision": 0.7800925925925926,
            "recall": 0.7124735729386892
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8516171142470008,
            "auditor_fn_violation": 0.011467602027426404,
            "auditor_fp_violation": 0.008914303219053942,
            "ave_precision_score": 0.7533574521614066,
            "fpr": 0.0801317233809001,
            "logloss": 0.5194199806838653,
            "mae": 0.33653717672065897,
            "precision": 0.8290398126463701,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(97)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5307017543859649,
            "auc_prc": 0.5240164693728817,
            "auditor_fn_violation": 0.007867846148139906,
            "auditor_fp_violation": 0.006526495624025924,
            "ave_precision_score": 0.5161012776807343,
            "fpr": 0.45394736842105265,
            "logloss": 0.6919439252238683,
            "mae": 0.4973076669531956,
            "precision": 0.5257731958762887,
            "recall": 0.9704016913319239
        },
        "train": {
            "accuracy": 0.5598243688254665,
            "auc_prc": 0.5180607361011781,
            "auditor_fn_violation": 0.005556937499857369,
            "auditor_fp_violation": 0.01169172644423455,
            "ave_precision_score": 0.5087715069968194,
            "fpr": 0.43029637760702527,
            "logloss": 0.6893777070974688,
            "mae": 0.495995017523038,
            "precision": 0.5462962962962963,
            "recall": 0.9812889812889813
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(98)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5175438596491229,
            "auc_prc": 0.5633735038595145,
            "auditor_fn_violation": 0.01343839249286007,
            "auditor_fp_violation": 0.009805978499780207,
            "ave_precision_score": 0.5506319714017499,
            "fpr": 0.04824561403508772,
            "logloss": 0.8400603303093428,
            "mae": 0.4857869696485036,
            "precision": 0.6363636363636364,
            "recall": 0.16279069767441862
        },
        "train": {
            "accuracy": 0.47639956092206365,
            "auc_prc": 0.5351271560117538,
            "auditor_fn_violation": 0.0030557450974574947,
            "auditor_fp_violation": 0.0047941183978760914,
            "ave_precision_score": 0.5398260643617698,
            "fpr": 0.059275521405049394,
            "logloss": 0.9167923852942724,
            "mae": 0.5055483191299153,
            "precision": 0.5178571428571429,
            "recall": 0.12058212058212059
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(99)",
        "seed": 9296,
        "test": {
            "accuracy": 0.5833333333333334,
            "auc_prc": 0.6118043433612743,
            "auditor_fn_violation": 0.01880494046956715,
            "auditor_fp_violation": 0.016155137273708185,
            "ave_precision_score": 0.6065179718636798,
            "fpr": 0.3541666666666667,
            "logloss": 0.6749598622315276,
            "mae": 0.4839572027456342,
            "precision": 0.5629228687415426,
            "recall": 0.879492600422833
        },
        "train": {
            "accuracy": 0.610318331503842,
            "auc_prc": 0.6604095007122895,
            "auditor_fn_violation": 0.012357624871346059,
            "auditor_fp_violation": 0.010323437061241161,
            "ave_precision_score": 0.6549309532209626,
            "fpr": 0.33150384193194293,
            "logloss": 0.6620468033443347,
            "mae": 0.47779402450855685,
            "precision": 0.5863013698630137,
            "recall": 0.8898128898128899
        }
    }
]
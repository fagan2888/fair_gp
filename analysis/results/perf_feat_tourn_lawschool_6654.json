[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7743162500549441,
            "auditor_fn_violation": 0.0072674418604651144,
            "auditor_fp_violation": 0.025291731606921643,
            "ave_precision_score": 0.774858765646357,
            "fpr": 0.14473684210526316,
            "logloss": 1.0545160539294638,
            "mae": 0.28403817168275086,
            "precision": 0.7333333333333333,
            "recall": 0.7674418604651163
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8071927064574977,
            "auditor_fn_violation": 0.010979230518198687,
            "auditor_fp_violation": 0.027914635080284893,
            "ave_precision_score": 0.8075370925021461,
            "fpr": 0.14818880351262348,
            "logloss": 0.9533366183717363,
            "mae": 0.27714650614140723,
            "precision": 0.7337278106508875,
            "recall": 0.7733887733887734
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7770431615458491,
            "auditor_fn_violation": 0.010707596157412562,
            "auditor_fp_violation": 0.02449746233465212,
            "ave_precision_score": 0.7564324826227673,
            "fpr": 0.14692982456140352,
            "logloss": 1.8693592811362767,
            "mae": 0.2946043131489181,
            "precision": 0.7270875763747454,
            "recall": 0.7547568710359408
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7847254606907077,
            "auditor_fn_violation": 0.01220700562083658,
            "auditor_fp_violation": 0.02730962652847625,
            "ave_precision_score": 0.7651479054711899,
            "fpr": 0.15587266739846323,
            "logloss": 1.8683485370011297,
            "mae": 0.2920618049431239,
            "precision": 0.7188118811881188,
            "recall": 0.7546777546777547
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.7923475987441437,
            "auditor_fn_violation": 0.01819758169207374,
            "auditor_fp_violation": 0.02404537825200816,
            "ave_precision_score": 0.7343602355104695,
            "fpr": 0.1425438596491228,
            "logloss": 3.2830243405731854,
            "mae": 0.2727407156336811,
            "precision": 0.7415506958250497,
            "recall": 0.7885835095137421
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.809956545422022,
            "auditor_fn_violation": 0.01940706221716101,
            "auditor_fp_violation": 0.016909606106246654,
            "ave_precision_score": 0.7512382573118657,
            "fpr": 0.14050493962678376,
            "logloss": 3.2449303553259243,
            "mae": 0.25876264354499723,
            "precision": 0.7504873294346979,
            "recall": 0.8004158004158004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7782799309831271,
            "auditor_fn_violation": 0.005501001446533882,
            "auditor_fp_violation": 0.03050443591895456,
            "ave_precision_score": 0.764034373179852,
            "fpr": 0.1600877192982456,
            "logloss": 1.6170035929144786,
            "mae": 0.28998101645892455,
            "precision": 0.7234848484848485,
            "recall": 0.8076109936575053
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7840416727161901,
            "auditor_fn_violation": 0.015349470892829845,
            "auditor_fp_violation": 0.03133791131646799,
            "ave_precision_score": 0.7677277212482922,
            "fpr": 0.1690450054884742,
            "logloss": 1.6917849616930691,
            "mae": 0.28847807792729246,
            "precision": 0.7158671586715867,
            "recall": 0.8066528066528067
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7751489487832945,
            "auditor_fn_violation": 0.009182244723860394,
            "auditor_fp_violation": 0.02707758861847101,
            "ave_precision_score": 0.7560828311723249,
            "fpr": 0.16337719298245615,
            "logloss": 1.8234085105836735,
            "mae": 0.2995180688587088,
            "precision": 0.7129094412331407,
            "recall": 0.7822410147991543
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7788958957078216,
            "auditor_fn_violation": 0.017344034907152365,
            "auditor_fp_violation": 0.025497153651749933,
            "ave_precision_score": 0.7600496949533682,
            "fpr": 0.1712403951701427,
            "logloss": 1.8514915883508338,
            "mae": 0.3050463665405623,
            "precision": 0.7045454545454546,
            "recall": 0.7733887733887734
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8068411635901291,
            "auditor_fn_violation": 0.012525036163347061,
            "auditor_fp_violation": 0.012288694401150947,
            "ave_precision_score": 0.8082562806591149,
            "fpr": 0.10197368421052631,
            "logloss": 0.57141465947983,
            "mae": 0.3162914387885753,
            "precision": 0.7862068965517242,
            "recall": 0.7230443974630021
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8179459265050604,
            "auditor_fn_violation": 0.018021821534445027,
            "auditor_fp_violation": 0.021361652158374392,
            "ave_precision_score": 0.8185550384489516,
            "fpr": 0.10318331503841932,
            "logloss": 0.550257210312755,
            "mae": 0.3095601912238184,
            "precision": 0.7906458797327395,
            "recall": 0.738045738045738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.747717691031154,
            "auditor_fn_violation": 0.014847835762768444,
            "auditor_fp_violation": 0.036713723374495466,
            "ave_precision_score": 0.749520356295667,
            "fpr": 0.1337719298245614,
            "logloss": 0.7594560937155155,
            "mae": 0.3067212797388602,
            "precision": 0.7463617463617463,
            "recall": 0.758985200845666
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.7460794984518957,
            "auditor_fn_violation": 0.016339906570422492,
            "auditor_fp_violation": 0.046991550302504285,
            "ave_precision_score": 0.7479702442439495,
            "fpr": 0.1350164654226125,
            "logloss": 0.730338926304825,
            "mae": 0.2977500018955847,
            "precision": 0.7520161290322581,
            "recall": 0.7754677754677755
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7788330509828124,
            "auditor_fn_violation": 0.008292070027076144,
            "auditor_fp_violation": 0.028683611077808417,
            "ave_precision_score": 0.7654227894372299,
            "fpr": 0.15570175438596492,
            "logloss": 1.5802673262552105,
            "mae": 0.2900072352497374,
            "precision": 0.7269230769230769,
            "recall": 0.7991543340380549
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.785638804043399,
            "auditor_fn_violation": 0.014548450333302149,
            "auditor_fp_violation": 0.031986317106170066,
            "ave_precision_score": 0.7700442888109993,
            "fpr": 0.16465422612513722,
            "logloss": 1.6537983811898702,
            "mae": 0.288276111307324,
            "precision": 0.7201492537313433,
            "recall": 0.8024948024948025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7788282143398545,
            "auditor_fn_violation": 0.008292070027076144,
            "auditor_fp_violation": 0.028683611077808417,
            "ave_precision_score": 0.7654179549716087,
            "fpr": 0.15570175438596492,
            "logloss": 1.5803117117763903,
            "mae": 0.29000665201574743,
            "precision": 0.7269230769230769,
            "recall": 0.7991543340380549
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.785643479082949,
            "auditor_fn_violation": 0.014548450333302149,
            "auditor_fp_violation": 0.031986317106170066,
            "ave_precision_score": 0.7700621817923046,
            "fpr": 0.16465422612513722,
            "logloss": 1.6538792894067975,
            "mae": 0.2882780553977547,
            "precision": 0.7201492537313433,
            "recall": 0.8024948024948025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7764240396469274,
            "auditor_fn_violation": 0.012318719632061126,
            "auditor_fp_violation": 0.02503696599128802,
            "ave_precision_score": 0.7589546502901099,
            "fpr": 0.15021929824561403,
            "logloss": 1.7378161937952703,
            "mae": 0.2949358958192527,
            "precision": 0.7221095334685599,
            "recall": 0.7526427061310782
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7832440697037275,
            "auditor_fn_violation": 0.013055950487344566,
            "auditor_fp_violation": 0.02914252163479948,
            "ave_precision_score": 0.7653507062190863,
            "fpr": 0.15587266739846323,
            "logloss": 1.778115105579309,
            "mae": 0.29349974477385377,
            "precision": 0.7176938369781312,
            "recall": 0.7505197505197505
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7788310534260611,
            "auditor_fn_violation": 0.008064890026334337,
            "auditor_fp_violation": 0.028683611077808417,
            "ave_precision_score": 0.7654170991606216,
            "fpr": 0.15570175438596492,
            "logloss": 1.5803724534312191,
            "mae": 0.29001683215838997,
            "precision": 0.727447216890595,
            "recall": 0.8012684989429175
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7856382326296498,
            "auditor_fn_violation": 0.014548450333302149,
            "auditor_fp_violation": 0.031986317106170066,
            "ave_precision_score": 0.7700569578606689,
            "fpr": 0.16465422612513722,
            "logloss": 1.6539471194371111,
            "mae": 0.28827889029120746,
            "precision": 0.7201492537313433,
            "recall": 0.8024948024948025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.7797129133129697,
            "auditor_fn_violation": 0.00953460554133749,
            "auditor_fp_violation": 0.026490628621668066,
            "ave_precision_score": 0.766366683206817,
            "fpr": 0.15570175438596492,
            "logloss": 1.5638495490585407,
            "mae": 0.2893681372074811,
            "precision": 0.7263969171483622,
            "recall": 0.7970401691331924
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7857723647750989,
            "auditor_fn_violation": 0.01124395526151838,
            "auditor_fp_violation": 0.03133791131646799,
            "ave_precision_score": 0.7702083896616436,
            "fpr": 0.1690450054884742,
            "logloss": 1.6405503253541713,
            "mae": 0.2889190600668515,
            "precision": 0.7169117647058824,
            "recall": 0.8108108108108109
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.7805955331436054,
            "auditor_fn_violation": 0.009643559215162642,
            "auditor_fp_violation": 0.030224693282180397,
            "ave_precision_score": 0.7679391379792475,
            "fpr": 0.15899122807017543,
            "logloss": 1.549247547176695,
            "mae": 0.28977965180727855,
            "precision": 0.7222222222222222,
            "recall": 0.7970401691331924
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7868287326203994,
            "auditor_fn_violation": 0.015712326359966323,
            "auditor_fp_violation": 0.03126132795547954,
            "ave_precision_score": 0.7712490245072297,
            "fpr": 0.16355653128430298,
            "logloss": 1.6427864731669783,
            "mae": 0.2878816326396057,
            "precision": 0.7209737827715356,
            "recall": 0.8004158004158004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8124846391584948,
            "auditor_fn_violation": 0.014847835762768446,
            "auditor_fp_violation": 0.025236782160412433,
            "ave_precision_score": 0.8072142282549911,
            "fpr": 0.14583333333333334,
            "logloss": 0.9653574223373894,
            "mae": 0.26695133003092225,
            "precision": 0.7461832061068703,
            "recall": 0.8266384778012685
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.826818735842419,
            "auditor_fn_violation": 0.016581810215180143,
            "auditor_fp_violation": 0.015038419319429204,
            "ave_precision_score": 0.821466999086805,
            "fpr": 0.141602634467618,
            "logloss": 0.9447660604312353,
            "mae": 0.25593075355444106,
            "precision": 0.7579737335834896,
            "recall": 0.83991683991684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8010827270391819,
            "auditor_fn_violation": 0.011338136567634731,
            "auditor_fp_violation": 0.026048535347480323,
            "ave_precision_score": 0.7868126552250545,
            "fpr": 0.1513157894736842,
            "logloss": 1.4473881848775128,
            "mae": 0.2682249583721219,
            "precision": 0.7376425855513308,
            "recall": 0.8202959830866807
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8103307736330508,
            "auditor_fn_violation": 0.014546168223445944,
            "auditor_fp_violation": 0.015163505475710319,
            "ave_precision_score": 0.795444444992471,
            "fpr": 0.14270032930845225,
            "logloss": 1.4759441883075994,
            "mae": 0.2561296425186018,
            "precision": 0.752851711026616,
            "recall": 0.8232848232848233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.7935137165678579,
            "auditor_fn_violation": 0.007886391454322917,
            "auditor_fp_violation": 0.018173280581864692,
            "ave_precision_score": 0.78016915956568,
            "fpr": 0.14802631578947367,
            "logloss": 1.4674757554591777,
            "mae": 0.27506636283702524,
            "precision": 0.7423664122137404,
            "recall": 0.8224101479915433
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8012865299913252,
            "auditor_fn_violation": 0.011259930030511815,
            "auditor_fp_violation": 0.021024685370025276,
            "ave_precision_score": 0.7850396327147178,
            "fpr": 0.15697036223929747,
            "logloss": 1.5399980139511107,
            "mae": 0.270304947151345,
            "precision": 0.7366482504604052,
            "recall": 0.8316008316008316
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7740817456806144,
            "auditor_fn_violation": 0.01155140758873929,
            "auditor_fp_violation": 0.01989419733844863,
            "ave_precision_score": 0.7745219263310119,
            "fpr": 0.1425438596491228,
            "logloss": 1.1317679947693746,
            "mae": 0.2905891354663321,
            "precision": 0.7302904564315352,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.803557403145388,
            "auditor_fn_violation": 0.010228416375507489,
            "auditor_fp_violation": 0.02579582875960483,
            "ave_precision_score": 0.8039669836524503,
            "fpr": 0.141602634467618,
            "logloss": 1.0464356345537658,
            "mae": 0.28369043039807873,
            "precision": 0.7361963190184049,
            "recall": 0.7484407484407485
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.7798117540978654,
            "auditor_fn_violation": 0.010116464522829273,
            "auditor_fp_violation": 0.02851126963193862,
            "ave_precision_score": 0.767135692968366,
            "fpr": 0.15350877192982457,
            "logloss": 1.5556294149700944,
            "mae": 0.2900171347192116,
            "precision": 0.7270955165692008,
            "recall": 0.7885835095137421
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7856944187210355,
            "auditor_fn_violation": 0.014461730158766388,
            "auditor_fp_violation": 0.03213693104944733,
            "ave_precision_score": 0.7701145156193958,
            "fpr": 0.16136114160263446,
            "logloss": 1.6479375730932575,
            "mae": 0.28861132665988565,
            "precision": 0.7226415094339622,
            "recall": 0.7962577962577962
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7715908274018662,
            "auditor_fn_violation": 0.010301917584659325,
            "auditor_fp_violation": 0.019344702873356514,
            "ave_precision_score": 0.7727898038858309,
            "fpr": 0.1337719298245614,
            "logloss": 0.9215050822435091,
            "mae": 0.29749883000999744,
            "precision": 0.7365010799136069,
            "recall": 0.7209302325581395
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.8000227221471993,
            "auditor_fn_violation": 0.01738054866485163,
            "auditor_fp_violation": 0.025027442371020853,
            "ave_precision_score": 0.800652756448792,
            "fpr": 0.141602634467618,
            "logloss": 0.8573991321948026,
            "mae": 0.29429819622118264,
            "precision": 0.728421052631579,
            "recall": 0.7193347193347194
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7760085633419895,
            "auditor_fn_violation": 0.00715385186009421,
            "auditor_fp_violation": 0.026095991687647372,
            "ave_precision_score": 0.7505626771849766,
            "fpr": 0.15460526315789475,
            "logloss": 2.0994495254985037,
            "mae": 0.29367399222643775,
            "precision": 0.7213438735177866,
            "recall": 0.7716701902748414
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7855750639908692,
            "auditor_fn_violation": 0.011209723613675318,
            "auditor_fp_violation": 0.028356265795318213,
            "ave_precision_score": 0.7594400373101836,
            "fpr": 0.16136114160263446,
            "logloss": 2.1457553091991537,
            "mae": 0.29066345729487775,
            "precision": 0.7173076923076923,
            "recall": 0.7754677754677755
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7524624825024893,
            "auditor_fn_violation": 0.024266533140462152,
            "auditor_fp_violation": 0.03135615233984735,
            "ave_precision_score": 0.7537750939789669,
            "fpr": 0.11403508771929824,
            "logloss": 0.697618807823601,
            "mae": 0.3171873728293066,
            "precision": 0.7575757575757576,
            "recall": 0.6871035940803383
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7577009397985679,
            "auditor_fn_violation": 0.02092694738139305,
            "auditor_fp_violation": 0.0402675312077196,
            "ave_precision_score": 0.7586979162050134,
            "fpr": 0.11745334796926454,
            "logloss": 0.678644140676181,
            "mae": 0.31515019876503797,
            "precision": 0.7579185520361991,
            "recall": 0.6964656964656964
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8047148545802058,
            "auditor_fn_violation": 0.01737927005674864,
            "auditor_fp_violation": 0.023121228469807777,
            "ave_precision_score": 0.8047263286126674,
            "fpr": 0.13706140350877194,
            "logloss": 1.1295027529081305,
            "mae": 0.27521034094779884,
            "precision": 0.7395833333333334,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8363401600979885,
            "auditor_fn_violation": 0.01656583544618671,
            "auditor_fp_violation": 0.018247262144844675,
            "ave_precision_score": 0.8365766062853297,
            "fpr": 0.13391877058177826,
            "logloss": 1.0101936796854267,
            "mae": 0.26277214385828285,
            "precision": 0.7525354969574036,
            "recall": 0.7713097713097713
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7916488306892184,
            "auditor_fn_violation": 0.013185712696116614,
            "auditor_fp_violation": 0.016347460336490434,
            "ave_precision_score": 0.7922233030387915,
            "fpr": 0.13925438596491227,
            "logloss": 1.0907962664302528,
            "mae": 0.28840855841047214,
            "precision": 0.7343096234309623,
            "recall": 0.7420718816067653
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8198849679729878,
            "auditor_fn_violation": 0.0054040361394916865,
            "auditor_fp_violation": 0.015740433461823196,
            "ave_precision_score": 0.8201926889376282,
            "fpr": 0.1394072447859495,
            "logloss": 0.9839186949839933,
            "mae": 0.27715112345932724,
            "precision": 0.744466800804829,
            "recall": 0.7692307692307693
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 6654,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.7909280677134127,
            "auditor_fn_violation": 0.024493713141203962,
            "auditor_fp_violation": 0.01785357471126564,
            "ave_precision_score": 0.7910241202639589,
            "fpr": 0.12171052631578948,
            "logloss": 1.3849285562403815,
            "mae": 0.2878580316977708,
            "precision": 0.7533333333333333,
            "recall": 0.7167019027484144
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8275237308425781,
            "auditor_fn_violation": 0.0220246422222273,
            "auditor_fp_violation": 0.017757128634518674,
            "ave_precision_score": 0.827776161705161,
            "fpr": 0.12184412733260154,
            "logloss": 1.159854694261422,
            "mae": 0.2731216901924904,
            "precision": 0.756578947368421,
            "recall": 0.7172557172557172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7790155171373554,
            "auditor_fn_violation": 0.008064890026334337,
            "auditor_fp_violation": 0.028683611077808417,
            "ave_precision_score": 0.7656346423632916,
            "fpr": 0.15570175438596492,
            "logloss": 1.5747080052274904,
            "mae": 0.28981291686064553,
            "precision": 0.727447216890595,
            "recall": 0.8012684989429175
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7859500700960156,
            "auditor_fn_violation": 0.014548450333302149,
            "auditor_fp_violation": 0.031986317106170066,
            "ave_precision_score": 0.770364571325007,
            "fpr": 0.16465422612513722,
            "logloss": 1.647710753798961,
            "mae": 0.2880472625601655,
            "precision": 0.7201492537313433,
            "recall": 0.8024948024948025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.7679602943951267,
            "auditor_fn_violation": 0.015999962909387638,
            "auditor_fp_violation": 0.03481796746992767,
            "ave_precision_score": 0.769654643651811,
            "fpr": 0.11513157894736842,
            "logloss": 0.6082712703666989,
            "mae": 0.3189346823842163,
            "precision": 0.7666666666666667,
            "recall": 0.7293868921775899
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.7693375089327636,
            "auditor_fn_violation": 0.01898715400361943,
            "auditor_fp_violation": 0.04296326551451255,
            "ave_precision_score": 0.7702490042513633,
            "fpr": 0.1207464324917673,
            "logloss": 0.600382849916424,
            "mae": 0.3165572664476052,
            "precision": 0.7639484978540773,
            "recall": 0.7401247401247402
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7578075232560834,
            "auditor_fn_violation": 0.01758790475130745,
            "auditor_fp_violation": 0.01927476721416297,
            "ave_precision_score": 0.7561962176947916,
            "fpr": 0.13815789473684212,
            "logloss": 1.4006912266233271,
            "mae": 0.295074320552742,
            "precision": 0.7307692307692307,
            "recall": 0.7230443974630021
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7681844211576725,
            "auditor_fn_violation": 0.018409780209999754,
            "auditor_fp_violation": 0.013213182549204807,
            "ave_precision_score": 0.7654367609470256,
            "fpr": 0.14709110867178923,
            "logloss": 1.3772233668916374,
            "mae": 0.30774533881978366,
            "precision": 0.7242798353909465,
            "recall": 0.7318087318087318
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.7851866963846279,
            "auditor_fn_violation": 0.005178776751604177,
            "auditor_fp_violation": 0.024410042760660194,
            "ave_precision_score": 0.7718605621289909,
            "fpr": 0.15350877192982457,
            "logloss": 1.52322424741521,
            "mae": 0.28337122868671105,
            "precision": 0.7323135755258127,
            "recall": 0.8097251585623678
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.7924477043171643,
            "auditor_fn_violation": 0.01493184478914446,
            "auditor_fp_violation": 0.025410359175963036,
            "ave_precision_score": 0.7769085176549031,
            "fpr": 0.15697036223929747,
            "logloss": 1.5870562447130314,
            "mae": 0.2812820989509521,
            "precision": 0.730188679245283,
            "recall": 0.8045738045738046
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7707076847483469,
            "auditor_fn_violation": 0.011637179629835688,
            "auditor_fp_violation": 0.023960456380130283,
            "ave_precision_score": 0.7715568552377763,
            "fpr": 0.14035087719298245,
            "logloss": 1.1206279312289886,
            "mae": 0.2873753696096634,
            "precision": 0.732776617954071,
            "recall": 0.7420718816067653
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.8026680065844731,
            "auditor_fn_violation": 0.011976512525359948,
            "auditor_fp_violation": 0.028320526893523604,
            "ave_precision_score": 0.8030195415285251,
            "fpr": 0.141602634467618,
            "logloss": 1.0148449077508368,
            "mae": 0.28083496001181607,
            "precision": 0.7351129363449692,
            "recall": 0.7442827442827443
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7766464447608763,
            "auditor_fn_violation": 0.011950131671673905,
            "auditor_fp_violation": 0.02503696599128802,
            "ave_precision_score": 0.7591974689339679,
            "fpr": 0.15021929824561403,
            "logloss": 1.7321134484028538,
            "mae": 0.29464661089933114,
            "precision": 0.7215447154471545,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7835013360726417,
            "auditor_fn_violation": 0.013055950487344566,
            "auditor_fp_violation": 0.029954305261276907,
            "ave_precision_score": 0.7656296325945545,
            "fpr": 0.15477497255762898,
            "logloss": 1.771901770885,
            "mae": 0.29312034107240376,
            "precision": 0.7191235059760956,
            "recall": 0.7505197505197505
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.7833051043420617,
            "auditor_fn_violation": 0.005299321241793706,
            "auditor_fp_violation": 0.026485633217439956,
            "ave_precision_score": 0.769905302770736,
            "fpr": 0.1600877192982456,
            "logloss": 1.5496146416669245,
            "mae": 0.28513285033137226,
            "precision": 0.7255639097744361,
            "recall": 0.8160676532769556
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.7899807670832345,
            "auditor_fn_violation": 0.01659322076446116,
            "auditor_fp_violation": 0.02890766599443494,
            "ave_precision_score": 0.7744030166450686,
            "fpr": 0.1602634467618002,
            "logloss": 1.6205003647226774,
            "mae": 0.28322026317421345,
            "precision": 0.7286245353159851,
            "recall": 0.814968814968815
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.7716976011138552,
            "auditor_fn_violation": 0.006685582878973332,
            "auditor_fp_violation": 0.02299134795987692,
            "ave_precision_score": 0.7723429159762215,
            "fpr": 0.14473684210526316,
            "logloss": 1.0503984074755983,
            "mae": 0.2839044944388793,
            "precision": 0.7327935222672065,
            "recall": 0.7653276955602537
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8048906308685678,
            "auditor_fn_violation": 0.012248083598248258,
            "auditor_fp_violation": 0.02746279325045312,
            "ave_precision_score": 0.8053280409030915,
            "fpr": 0.14928649835345773,
            "logloss": 0.9486733951469242,
            "mae": 0.27788455463649897,
            "precision": 0.7333333333333333,
            "recall": 0.7775467775467776
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7978988981849671,
            "auditor_fn_violation": 0.01396925188234858,
            "auditor_fp_violation": 0.017104264077049115,
            "ave_precision_score": 0.7982468239572705,
            "fpr": 0.13267543859649122,
            "logloss": 1.1455847333680287,
            "mae": 0.28770628810332194,
            "precision": 0.7392241379310345,
            "recall": 0.7251585623678647
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8213841295518582,
            "auditor_fn_violation": 0.017640709188458916,
            "auditor_fp_violation": 0.023087330559313814,
            "ave_precision_score": 0.8215698227767901,
            "fpr": 0.132821075740944,
            "logloss": 1.0278096948728779,
            "mae": 0.27902145571618997,
            "precision": 0.7473903966597077,
            "recall": 0.7442827442827443
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 6654,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.7733098512873641,
            "auditor_fn_violation": 0.006989262267720039,
            "auditor_fp_violation": 0.02700765295927747,
            "ave_precision_score": 0.7526293736936184,
            "fpr": 0.16228070175438597,
            "logloss": 1.8975942843729365,
            "mae": 0.2948696119268255,
            "precision": 0.7175572519083969,
            "recall": 0.7949260042283298
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7816560329750836,
            "auditor_fn_violation": 0.011433370379583332,
            "auditor_fp_violation": 0.032101192147652724,
            "ave_precision_score": 0.762743953028893,
            "fpr": 0.17014270032930845,
            "logloss": 1.8806178718073305,
            "mae": 0.2922245836698419,
            "precision": 0.7118959107806692,
            "recall": 0.7962577962577962
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8075559950085075,
            "auditor_fn_violation": 0.019579207002707617,
            "auditor_fp_violation": 0.020608540143068378,
            "ave_precision_score": 0.8084638523404736,
            "fpr": 0.13486842105263158,
            "logloss": 1.0754971212841078,
            "mae": 0.27943367284078396,
            "precision": 0.7442827442827443,
            "recall": 0.7568710359408034
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8434153260393392,
            "auditor_fn_violation": 0.014267750820989022,
            "auditor_fp_violation": 0.021826257881704236,
            "ave_precision_score": 0.8436249544151069,
            "fpr": 0.13172338090010977,
            "logloss": 0.9298528378548641,
            "mae": 0.2655793182270477,
            "precision": 0.7535934291581109,
            "recall": 0.762993762993763
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.773444514078437,
            "auditor_fn_violation": 0.01069368717777531,
            "auditor_fp_violation": 0.030277145026575556,
            "ave_precision_score": 0.753482754268922,
            "fpr": 0.16447368421052633,
            "logloss": 1.8973871117961552,
            "mae": 0.3012685821622968,
            "precision": 0.7087378640776699,
            "recall": 0.7716701902748414
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7797368670565292,
            "auditor_fn_violation": 0.00944337058497322,
            "auditor_fp_violation": 0.03477395144614913,
            "ave_precision_score": 0.7602240021403912,
            "fpr": 0.1734357848518112,
            "logloss": 1.9194525001153024,
            "mae": 0.29727789372327496,
            "precision": 0.7018867924528301,
            "recall": 0.7733887733887734
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7793523699513918,
            "auditor_fn_violation": 0.008064890026334337,
            "auditor_fp_violation": 0.028683611077808417,
            "ave_precision_score": 0.7659406072757173,
            "fpr": 0.15570175438596492,
            "logloss": 1.5772220629637754,
            "mae": 0.28982541018281105,
            "precision": 0.727447216890595,
            "recall": 0.8012684989429175
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7859478824088048,
            "auditor_fn_violation": 0.014548450333302149,
            "auditor_fp_violation": 0.031986317106170066,
            "ave_precision_score": 0.7703662378000682,
            "fpr": 0.16465422612513722,
            "logloss": 1.650752285426572,
            "mae": 0.28803443853503247,
            "precision": 0.7201492537313433,
            "recall": 0.8024948024948025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 6654,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.7744356422035484,
            "auditor_fn_violation": 0.009448833500241092,
            "auditor_fp_violation": 0.022639171961795147,
            "ave_precision_score": 0.7755229209800143,
            "fpr": 0.14692982456140352,
            "logloss": 0.9666384642566255,
            "mae": 0.283435692269668,
            "precision": 0.7298387096774194,
            "recall": 0.7653276955602537
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8063405134829154,
            "auditor_fn_violation": 0.01258127163725408,
            "auditor_fp_violation": 0.026038342736068212,
            "ave_precision_score": 0.8067768562071685,
            "fpr": 0.14818880351262348,
            "logloss": 0.890482647392088,
            "mae": 0.27732299481988903,
            "precision": 0.735812133072407,
            "recall": 0.7817047817047817
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.7734481610830899,
            "auditor_fn_violation": 0.007622120841215092,
            "auditor_fp_violation": 0.028366402909323428,
            "ave_precision_score": 0.7521409448157992,
            "fpr": 0.16337719298245615,
            "logloss": 1.9113055049438976,
            "mae": 0.2932177655451853,
            "precision": 0.7178030303030303,
            "recall": 0.8012684989429175
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7826117656507199,
            "auditor_fn_violation": 0.011638760266641715,
            "auditor_fp_violation": 0.03284405074924055,
            "ave_precision_score": 0.7636948259768508,
            "fpr": 0.17453347969264543,
            "logloss": 1.8762919396901754,
            "mae": 0.29052166621047737,
            "precision": 0.7087912087912088,
            "recall": 0.8045738045738046
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 6654,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.7438778191712785,
            "auditor_fn_violation": 0.01947488965542821,
            "auditor_fp_violation": 0.04770111497422372,
            "ave_precision_score": 0.7135763219078802,
            "fpr": 0.18311403508771928,
            "logloss": 2.863526175465642,
            "mae": 0.3173798974064272,
            "precision": 0.6812977099236641,
            "recall": 0.7547568710359408
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7606165163893787,
            "auditor_fn_violation": 0.016321649691572855,
            "auditor_fp_violation": 0.040627473004365255,
            "ave_precision_score": 0.7284226663217224,
            "fpr": 0.17014270032930845,
            "logloss": 2.9323511332996426,
            "mae": 0.30291631419027903,
            "precision": 0.7041984732824428,
            "recall": 0.7671517671517671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.7933354514321186,
            "auditor_fn_violation": 0.007886391454322917,
            "auditor_fp_violation": 0.018173280581864692,
            "ave_precision_score": 0.7799630365242664,
            "fpr": 0.14802631578947367,
            "logloss": 1.4707642531324912,
            "mae": 0.2751122063055514,
            "precision": 0.7423664122137404,
            "recall": 0.8224101479915433
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8013799982909067,
            "auditor_fn_violation": 0.011259930030511815,
            "auditor_fp_violation": 0.021024685370025276,
            "ave_precision_score": 0.7851123551540765,
            "fpr": 0.15697036223929747,
            "logloss": 1.54301405934744,
            "mae": 0.2703750288097814,
            "precision": 0.7366482504604052,
            "recall": 0.8316008316008316
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7240155846233136,
            "auditor_fn_violation": 0.015165424131152405,
            "auditor_fp_violation": 0.04172161611317589,
            "ave_precision_score": 0.681385108306606,
            "fpr": 0.17434210526315788,
            "logloss": 3.1442423128413717,
            "mae": 0.3142288735281526,
            "precision": 0.6942307692307692,
            "recall": 0.7632135306553911
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7224874944355455,
            "auditor_fn_violation": 0.014952383777850298,
            "auditor_fp_violation": 0.040257320092921156,
            "ave_precision_score": 0.6766187455893028,
            "fpr": 0.18111964873765093,
            "logloss": 3.4260309190430775,
            "mae": 0.31196921076917744,
            "precision": 0.6921641791044776,
            "recall": 0.7713097713097713
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7851118232966925,
            "auditor_fn_violation": 0.012786988613182003,
            "auditor_fp_violation": 0.019606961595332303,
            "ave_precision_score": 0.7685083322135198,
            "fpr": 0.14802631578947367,
            "logloss": 1.7283621881388695,
            "mae": 0.2812962511917241,
            "precision": 0.7305389221556886,
            "recall": 0.773784355179704
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.7945645945312887,
            "auditor_fn_violation": 0.012884792248129245,
            "auditor_fp_violation": 0.022964797181732322,
            "ave_precision_score": 0.7759753535188987,
            "fpr": 0.14709110867178923,
            "logloss": 1.7470763065288095,
            "mae": 0.2752439269525609,
            "precision": 0.7398058252427184,
            "recall": 0.7920997920997921
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8018600785072081,
            "auditor_fn_violation": 0.01077714105559883,
            "auditor_fp_violation": 0.026503117132238345,
            "ave_precision_score": 0.787528409870941,
            "fpr": 0.15679824561403508,
            "logloss": 1.4592037072420785,
            "mae": 0.2689736967356247,
            "precision": 0.7351851851851852,
            "recall": 0.8393234672304439
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8079169187987342,
            "auditor_fn_violation": 0.014833714065327679,
            "auditor_fp_violation": 0.017700967503127166,
            "ave_precision_score": 0.7916208416676818,
            "fpr": 0.15148188803512624,
            "logloss": 1.533745862552576,
            "mae": 0.2594738960369686,
            "precision": 0.744916820702403,
            "recall": 0.8378378378378378
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7816654095204928,
            "auditor_fn_violation": 0.01517933311078966,
            "auditor_fp_violation": 0.024070355273148714,
            "ave_precision_score": 0.7657429851266444,
            "fpr": 0.14583333333333334,
            "logloss": 1.6558095653407912,
            "mae": 0.2900830220077689,
            "precision": 0.7313131313131314,
            "recall": 0.7653276955602537
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.7876270912590082,
            "auditor_fn_violation": 0.01169353090319062,
            "auditor_fp_violation": 0.027363234881168153,
            "ave_precision_score": 0.7704527293582569,
            "fpr": 0.15477497255762898,
            "logloss": 1.7065344460204692,
            "mae": 0.2877882055174546,
            "precision": 0.724609375,
            "recall": 0.7713097713097713
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7788324410181814,
            "auditor_fn_violation": 0.008292070027076144,
            "auditor_fp_violation": 0.028683611077808417,
            "ave_precision_score": 0.7654280029379299,
            "fpr": 0.15570175438596492,
            "logloss": 1.5802292710465866,
            "mae": 0.29000761614467185,
            "precision": 0.7269230769230769,
            "recall": 0.7991543340380549
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7856397367282462,
            "auditor_fn_violation": 0.014548450333302149,
            "auditor_fp_violation": 0.031986317106170066,
            "ave_precision_score": 0.7700407573479144,
            "fpr": 0.16465422612513722,
            "logloss": 1.653709732492336,
            "mae": 0.28827407510038466,
            "precision": 0.7201492537313433,
            "recall": 0.8024948024948025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.795656487732159,
            "auditor_fn_violation": 0.012664125959719599,
            "auditor_fp_violation": 0.023523358510170653,
            "ave_precision_score": 0.7763163520053727,
            "fpr": 0.17214912280701755,
            "logloss": 1.7242925966227605,
            "mae": 0.27956461953155004,
            "precision": 0.7196428571428571,
            "recall": 0.8520084566596194
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8042143781904114,
            "auditor_fn_violation": 0.015714608469822523,
            "auditor_fp_violation": 0.02153013555254895,
            "ave_precision_score": 0.7843142213567778,
            "fpr": 0.16575192096597147,
            "logloss": 1.756937526991916,
            "mae": 0.2656269186509175,
            "precision": 0.7298747763864043,
            "recall": 0.8482328482328483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7883904742978989,
            "auditor_fn_violation": 0.011354363710544869,
            "auditor_fp_violation": 0.014501658474203738,
            "ave_precision_score": 0.7887883038254467,
            "fpr": 0.13596491228070176,
            "logloss": 1.0690323840686864,
            "mae": 0.281008709138431,
            "precision": 0.7443298969072165,
            "recall": 0.7632135306553911
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8159853477058039,
            "auditor_fn_violation": 0.0101280035418345,
            "auditor_fp_violation": 0.020269062874939375,
            "ave_precision_score": 0.8163052479366322,
            "fpr": 0.14270032930845225,
            "logloss": 0.9788949568288474,
            "mae": 0.2774735726110123,
            "precision": 0.7394789579158316,
            "recall": 0.7671517671517671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 6654,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.779265662785275,
            "auditor_fn_violation": 0.008711657579466637,
            "auditor_fp_violation": 0.0290682572033729,
            "ave_precision_score": 0.765876259301711,
            "fpr": 0.15350877192982457,
            "logloss": 1.5771447356935537,
            "mae": 0.2893138444967852,
            "precision": 0.7302504816955684,
            "recall": 0.8012684989429175
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7856107516951825,
            "auditor_fn_violation": 0.014995743865118177,
            "auditor_fp_violation": 0.031986317106170066,
            "ave_precision_score": 0.7700321647622344,
            "fpr": 0.16465422612513722,
            "logloss": 1.6537680517236468,
            "mae": 0.2881947755930086,
            "precision": 0.7201492537313433,
            "recall": 0.8024948024948025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7789160335289156,
            "auditor_fn_violation": 0.008292070027076144,
            "auditor_fp_violation": 0.02761709227510691,
            "ave_precision_score": 0.7655308503861459,
            "fpr": 0.15460526315789475,
            "logloss": 1.5781005056176043,
            "mae": 0.2895445858906151,
            "precision": 0.7283236994219653,
            "recall": 0.7991543340380549
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7854176513534797,
            "auditor_fn_violation": 0.014614631519132072,
            "auditor_fp_violation": 0.030965205626324262,
            "ave_precision_score": 0.7698463101012221,
            "fpr": 0.16245883644346873,
            "logloss": 1.6538596875672928,
            "mae": 0.2888298228566768,
            "precision": 0.7223264540337712,
            "recall": 0.8004158004158004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8435898145370171,
            "auditor_fn_violation": 0.023005452320017807,
            "auditor_fp_violation": 0.022459337409583183,
            "ave_precision_score": 0.8438840927081066,
            "fpr": 0.125,
            "logloss": 0.557792385927343,
            "mae": 0.28382068900062724,
            "precision": 0.7644628099173554,
            "recall": 0.7822410147991543
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8681676524104688,
            "auditor_fn_violation": 0.02510549052810305,
            "auditor_fp_violation": 0.01864549562198453,
            "ave_precision_score": 0.8684172033006831,
            "fpr": 0.10757409440175632,
            "logloss": 0.5255294335213112,
            "mae": 0.2707318245248054,
            "precision": 0.7919320594479831,
            "recall": 0.7754677754677755
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7795881035503345,
            "auditor_fn_violation": 0.00927960758132117,
            "auditor_fp_violation": 0.029397953882428166,
            "ave_precision_score": 0.7660446704711565,
            "fpr": 0.1600877192982456,
            "logloss": 1.583475928728471,
            "mae": 0.2905553619496684,
            "precision": 0.7208413001912046,
            "recall": 0.7970401691331924
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7847809229309558,
            "auditor_fn_violation": 0.012476294583868683,
            "auditor_fp_violation": 0.030799275010849315,
            "ave_precision_score": 0.7684432215843879,
            "fpr": 0.16575192096597147,
            "logloss": 1.6824196821656383,
            "mae": 0.2891808046160794,
            "precision": 0.7177570093457943,
            "recall": 0.7983367983367984
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7808440846759689,
            "auditor_fn_violation": 0.011651088609472946,
            "auditor_fp_violation": 0.022067198177676536,
            "ave_precision_score": 0.7641226918459968,
            "fpr": 0.14364035087719298,
            "logloss": 1.6823437552926266,
            "mae": 0.2917454870607001,
            "precision": 0.7298969072164948,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7880561482089756,
            "auditor_fn_violation": 0.00945249902439804,
            "auditor_fp_violation": 0.02515508130600158,
            "ave_precision_score": 0.7701608529121736,
            "fpr": 0.14928649835345773,
            "logloss": 1.7272195692958296,
            "mae": 0.28779589239104164,
            "precision": 0.7274549098196392,
            "recall": 0.7546777546777547
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7790037604353601,
            "auditor_fn_violation": 0.008292070027076144,
            "auditor_fp_violation": 0.02761709227510691,
            "ave_precision_score": 0.7656193146267439,
            "fpr": 0.15460526315789475,
            "logloss": 1.57623545592617,
            "mae": 0.2893149288188734,
            "precision": 0.7283236994219653,
            "recall": 0.7991543340380549
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7857004929969755,
            "auditor_fn_violation": 0.014614631519132072,
            "auditor_fp_violation": 0.030965205626324262,
            "ave_precision_score": 0.7701148555098917,
            "fpr": 0.16245883644346873,
            "logloss": 1.651427367617998,
            "mae": 0.2886024045405581,
            "precision": 0.7223264540337712,
            "recall": 0.8004158004158004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7821749466266561,
            "auditor_fn_violation": 0.005698045324728311,
            "auditor_fp_violation": 0.029270571074611354,
            "ave_precision_score": 0.7687072419767214,
            "fpr": 0.1611842105263158,
            "logloss": 1.5786278936821159,
            "mae": 0.2871226460141663,
            "precision": 0.7231638418079096,
            "recall": 0.8118393234672304
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.787990973861751,
            "auditor_fn_violation": 0.0146853769246744,
            "auditor_fp_violation": 0.03008704975365686,
            "ave_precision_score": 0.7716411130356609,
            "fpr": 0.16575192096597147,
            "logloss": 1.667214295284898,
            "mae": 0.2856221031160803,
            "precision": 0.7208872458410351,
            "recall": 0.8108108108108109
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7884750535481466,
            "auditor_fn_violation": 0.00548245614035088,
            "auditor_fp_violation": 0.023118730767693725,
            "ave_precision_score": 0.7760285303090252,
            "fpr": 0.15789473684210525,
            "logloss": 1.4291309210972682,
            "mae": 0.2876202768665392,
            "precision": 0.7277882797731569,
            "recall": 0.813953488372093
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.796683113426486,
            "auditor_fn_violation": 0.010972384188630079,
            "auditor_fp_violation": 0.0267735430015572,
            "ave_precision_score": 0.7841824734295915,
            "fpr": 0.15806805708013172,
            "logloss": 1.4443519433105934,
            "mae": 0.2828772906311757,
            "precision": 0.7272727272727273,
            "recall": 0.7983367983367984
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 6654,
        "test": {
            "accuracy": 0.6535087719298246,
            "auc_prc": 0.728387520703554,
            "auditor_fn_violation": 0.02906976744186049,
            "auditor_fp_violation": 0.027627083083563122,
            "ave_precision_score": 0.7300818057079959,
            "fpr": 0.06907894736842106,
            "logloss": 0.821364726479123,
            "mae": 0.3728266679349473,
            "precision": 0.7773851590106007,
            "recall": 0.46511627906976744
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.7188149795710802,
            "auditor_fn_violation": 0.019943358033369028,
            "auditor_fp_violation": 0.03384984555688868,
            "ave_precision_score": 0.7198903553658897,
            "fpr": 0.08232711306256861,
            "logloss": 0.8208908761275331,
            "mae": 0.3739532278658353,
            "precision": 0.755700325732899,
            "recall": 0.48232848232848236
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7199308493982083,
            "auditor_fn_violation": 0.012270038203330738,
            "auditor_fp_violation": 0.0420987691323982,
            "ave_precision_score": 0.6876712710261794,
            "fpr": 0.19078947368421054,
            "logloss": 2.7338366905154063,
            "mae": 0.30482644513202667,
            "precision": 0.6909413854351687,
            "recall": 0.8224101479915433
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7262047176309495,
            "auditor_fn_violation": 0.01699943631886552,
            "auditor_fp_violation": 0.04011181170704312,
            "ave_precision_score": 0.6903669159879965,
            "fpr": 0.1877058177826564,
            "logloss": 2.9007569016661225,
            "mae": 0.30516567469852374,
            "precision": 0.6962699822380106,
            "recall": 0.814968814968815
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.777354620807943,
            "auditor_fn_violation": 0.010494325136308002,
            "auditor_fp_violation": 0.02449746233465212,
            "ave_precision_score": 0.7566959882568305,
            "fpr": 0.14692982456140352,
            "logloss": 1.869666978691446,
            "mae": 0.29462459106615535,
            "precision": 0.7276422764227642,
            "recall": 0.7568710359408034
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7847537994450674,
            "auditor_fn_violation": 0.01220700562083658,
            "auditor_fp_violation": 0.02730962652847625,
            "ave_precision_score": 0.7651805992250592,
            "fpr": 0.15587266739846323,
            "logloss": 1.8688839716807415,
            "mae": 0.2920817608851778,
            "precision": 0.7188118811881188,
            "recall": 0.7546777546777547
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7683719446443877,
            "auditor_fn_violation": 0.01535551351952821,
            "auditor_fp_violation": 0.03053191064220917,
            "ave_precision_score": 0.7696060267377018,
            "fpr": 0.13925438596491227,
            "logloss": 0.7451348460472849,
            "mae": 0.30279328714041504,
            "precision": 0.7309322033898306,
            "recall": 0.7293868921775899
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7813497457037046,
            "auditor_fn_violation": 0.01381132884974817,
            "auditor_fp_violation": 0.03782196921348888,
            "ave_precision_score": 0.7822402175994836,
            "fpr": 0.13391877058177826,
            "logloss": 0.7180302579026605,
            "mae": 0.29933629504916565,
            "precision": 0.7436974789915967,
            "recall": 0.735966735966736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8162580448137247,
            "auditor_fn_violation": 0.012571399428804574,
            "auditor_fp_violation": 0.024210226591535794,
            "ave_precision_score": 0.8130974317429585,
            "fpr": 0.17543859649122806,
            "logloss": 0.9108410165516788,
            "mae": 0.2816178284607212,
            "precision": 0.7202797202797203,
            "recall": 0.8710359408033826
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8361640841795963,
            "auditor_fn_violation": 0.015251340169013061,
            "auditor_fp_violation": 0.013764582748321546,
            "ave_precision_score": 0.8344921523129756,
            "fpr": 0.1690450054884742,
            "logloss": 0.8373119700619568,
            "mae": 0.26481437880735376,
            "precision": 0.7349397590361446,
            "recall": 0.8877338877338877
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7735088329893722,
            "auditor_fn_violation": 0.012485627387708175,
            "auditor_fp_violation": 0.024887103864444714,
            "ave_precision_score": 0.7740833583735169,
            "fpr": 0.1425438596491228,
            "logloss": 0.9771554772821317,
            "mae": 0.28295452018050327,
            "precision": 0.7379032258064516,
            "recall": 0.773784355179704
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8036797045172631,
            "auditor_fn_violation": 0.0154886795940583,
            "auditor_fp_violation": 0.029693921833916226,
            "ave_precision_score": 0.8041409155085028,
            "fpr": 0.14489571899012074,
            "logloss": 0.889866792800891,
            "mae": 0.27654992523935934,
            "precision": 0.7396449704142012,
            "recall": 0.7796257796257796
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.7801918014271628,
            "auditor_fn_violation": 0.008064890026334337,
            "auditor_fp_violation": 0.030044858729968434,
            "ave_precision_score": 0.767565235122375,
            "fpr": 0.1611842105263158,
            "logloss": 1.5491648336523467,
            "mae": 0.29058040903697097,
            "precision": 0.720532319391635,
            "recall": 0.8012684989429175
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7858242232333933,
            "auditor_fn_violation": 0.015646145174136396,
            "auditor_fp_violation": 0.031935261532177774,
            "ave_precision_score": 0.7703197030582625,
            "fpr": 0.16465422612513722,
            "logloss": 1.6445700151222185,
            "mae": 0.28824679571931516,
            "precision": 0.7201492537313433,
            "recall": 0.8024948024948025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.7793441608580498,
            "auditor_fn_violation": 0.008292070027076144,
            "auditor_fp_violation": 0.02867861567358031,
            "ave_precision_score": 0.7667049897701935,
            "fpr": 0.1600877192982456,
            "logloss": 1.5610135404599046,
            "mae": 0.290753416864357,
            "precision": 0.7213740458015268,
            "recall": 0.7991543340380549
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7853537093551053,
            "auditor_fn_violation": 0.013950537550976635,
            "auditor_fp_violation": 0.03308401194700432,
            "ave_precision_score": 0.7697641882897096,
            "fpr": 0.16465422612513722,
            "logloss": 1.6543111469370908,
            "mae": 0.2885980888489496,
            "precision": 0.719626168224299,
            "recall": 0.8004158004158004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7318640560318863,
            "auditor_fn_violation": 0.006720355328066468,
            "auditor_fp_violation": 0.019944151380729733,
            "ave_precision_score": 0.6591166619321951,
            "fpr": 0.20285087719298245,
            "logloss": 4.641393424070463,
            "mae": 0.30766317716830127,
            "precision": 0.6826758147512865,
            "recall": 0.8414376321353065
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7378814788969479,
            "auditor_fn_violation": 0.005068565990629657,
            "auditor_fp_violation": 0.025313353585377688,
            "ave_precision_score": 0.6622846036153645,
            "fpr": 0.1964873765093304,
            "logloss": 4.844120658424394,
            "mae": 0.30359553675876394,
            "precision": 0.6897746967071057,
            "recall": 0.8274428274428275
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.7830159807974615,
            "auditor_fn_violation": 0.013997069841623095,
            "auditor_fp_violation": 0.02131289213923191,
            "ave_precision_score": 0.7842898106313039,
            "fpr": 0.12719298245614036,
            "logloss": 0.9577940037693817,
            "mae": 0.28659832233928123,
            "precision": 0.7489177489177489,
            "recall": 0.7315010570824524
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8128587161183206,
            "auditor_fn_violation": 0.013279597253252578,
            "auditor_fp_violation": 0.027013504199320967,
            "ave_precision_score": 0.8132659886676066,
            "fpr": 0.1394072447859495,
            "logloss": 0.8824999237721581,
            "mae": 0.28079553140852304,
            "precision": 0.741869918699187,
            "recall": 0.7588357588357588
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 6654,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.7493844182697835,
            "auditor_fn_violation": 0.02242359333852603,
            "auditor_fp_violation": 0.037145825840227,
            "ave_precision_score": 0.7512401171548648,
            "fpr": 0.1425438596491228,
            "logloss": 0.7349796257003322,
            "mae": 0.30921297271614856,
            "precision": 0.7394789579158316,
            "recall": 0.7801268498942917
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.7433748945498532,
            "auditor_fn_violation": 0.014240365502714573,
            "auditor_fp_violation": 0.04233528195440738,
            "ave_precision_score": 0.7445874055236558,
            "fpr": 0.14489571899012074,
            "logloss": 0.7197525711667003,
            "mae": 0.3014330666664567,
            "precision": 0.7411764705882353,
            "recall": 0.7858627858627859
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7798948453873399,
            "auditor_fn_violation": 0.008292070027076144,
            "auditor_fp_violation": 0.02761709227510691,
            "ave_precision_score": 0.7664798474093648,
            "fpr": 0.15460526315789475,
            "logloss": 1.5702495508321612,
            "mae": 0.28893366638399,
            "precision": 0.7283236994219653,
            "recall": 0.7991543340380549
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7861677175206039,
            "auditor_fn_violation": 0.014548450333302149,
            "auditor_fp_violation": 0.030965205626324262,
            "ave_precision_score": 0.7705917206051647,
            "fpr": 0.16245883644346873,
            "logloss": 1.6458201790497167,
            "mae": 0.28816367692230077,
            "precision": 0.7228464419475655,
            "recall": 0.8024948024948025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8191871755110437,
            "auditor_fn_violation": 0.018450261488817177,
            "auditor_fp_violation": 0.026615513727370817,
            "ave_precision_score": 0.8196847463885528,
            "fpr": 0.13815789473684212,
            "logloss": 0.9211350539645334,
            "mae": 0.2653208865733892,
            "precision": 0.7449392712550608,
            "recall": 0.7780126849894292
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8570903566181063,
            "auditor_fn_violation": 0.015500090143339322,
            "auditor_fp_violation": 0.015275827738493354,
            "ave_precision_score": 0.8572820349851585,
            "fpr": 0.13830954994511527,
            "logloss": 0.8185135991866064,
            "mae": 0.25509851989850596,
            "precision": 0.7529411764705882,
            "recall": 0.7983367983367984
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7894771277836787,
            "auditor_fn_violation": 0.008410296353992807,
            "auditor_fp_violation": 0.018675318706789755,
            "ave_precision_score": 0.790152208844474,
            "fpr": 0.13815789473684212,
            "logloss": 1.0607718184534412,
            "mae": 0.29129565398024887,
            "precision": 0.733615221987315,
            "recall": 0.733615221987315
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8144169490156551,
            "auditor_fn_violation": 0.01526731493800649,
            "auditor_fp_violation": 0.01840553442422077,
            "ave_precision_score": 0.8147993789529189,
            "fpr": 0.13721185510428102,
            "logloss": 0.9953389855608935,
            "mae": 0.28320101173273643,
            "precision": 0.7422680412371134,
            "recall": 0.7484407484407485
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7452063677185349,
            "auditor_fn_violation": 0.017673676792403843,
            "auditor_fp_violation": 0.03706340167046318,
            "ave_precision_score": 0.7470522943779696,
            "fpr": 0.12609649122807018,
            "logloss": 0.7478476403693081,
            "mae": 0.30908537801668073,
            "precision": 0.7558386411889597,
            "recall": 0.7526427061310782
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.7416603601550527,
            "auditor_fn_violation": 0.020351855697629578,
            "auditor_fp_violation": 0.046011283281852305,
            "ave_precision_score": 0.7435657462838771,
            "fpr": 0.132821075740944,
            "logloss": 0.7200406389072139,
            "mae": 0.30164153681378114,
            "precision": 0.7505154639175258,
            "recall": 0.7567567567567568
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8157991380047607,
            "auditor_fn_violation": 0.014998516375505361,
            "auditor_fp_violation": 0.023700695360268566,
            "ave_precision_score": 0.8136046306693733,
            "fpr": 0.16885964912280702,
            "logloss": 0.870957704937386,
            "mae": 0.28287903595415836,
            "precision": 0.7240143369175627,
            "recall": 0.854122621564482
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8388782666103051,
            "auditor_fn_violation": 0.013850124717303647,
            "auditor_fp_violation": 0.015092027672121108,
            "ave_precision_score": 0.8381261634833848,
            "fpr": 0.1668496158068057,
            "logloss": 0.793136803603866,
            "mae": 0.2647766132615067,
            "precision": 0.7361111111111112,
            "recall": 0.8814968814968815
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7407508306678353,
            "auditor_fn_violation": 0.016002281072660513,
            "auditor_fp_violation": 0.038651940215002206,
            "ave_precision_score": 0.7426183704085187,
            "fpr": 0.12390350877192982,
            "logloss": 0.7592446826864653,
            "mae": 0.3103655847332725,
            "precision": 0.7580299785867237,
            "recall": 0.7484143763213531
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.7367318094584855,
            "auditor_fn_violation": 0.02062570888037409,
            "auditor_fp_violation": 0.047588900518214086,
            "ave_precision_score": 0.7386471108996647,
            "fpr": 0.132821075740944,
            "logloss": 0.734888495447131,
            "mae": 0.3033100272906546,
            "precision": 0.7484407484407485,
            "recall": 0.7484407484407485
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7537697343573415,
            "auditor_fn_violation": 0.02499907273469085,
            "auditor_fp_violation": 0.026922731087399593,
            "ave_precision_score": 0.7554568784286966,
            "fpr": 0.0800438596491228,
            "logloss": 0.731518255511628,
            "mae": 0.33300249886006467,
            "precision": 0.7960893854748603,
            "recall": 0.6025369978858351
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7466051135123385,
            "auditor_fn_violation": 0.023937050281726463,
            "auditor_fp_violation": 0.036657902126464664,
            "ave_precision_score": 0.7475771010633825,
            "fpr": 0.09001097694840834,
            "logloss": 0.7222442985308793,
            "mae": 0.3320467415000003,
            "precision": 0.7759562841530054,
            "recall": 0.5904365904365905
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8480712779061479,
            "auditor_fn_violation": 0.022416638848707395,
            "auditor_fp_violation": 0.013707389201934221,
            "ave_precision_score": 0.8484892560084136,
            "fpr": 0.0712719298245614,
            "logloss": 0.542005756658137,
            "mae": 0.2921355723521724,
            "precision": 0.8320413436692506,
            "recall": 0.6807610993657506
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8799553233346444,
            "auditor_fn_violation": 0.01867678706317565,
            "auditor_fp_violation": 0.002248998034360403,
            "ave_precision_score": 0.8802355829253596,
            "fpr": 0.05817782656421515,
            "logloss": 0.5038984186231257,
            "mae": 0.2803881653555404,
            "precision": 0.8612565445026178,
            "recall": 0.683991683991684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7801769890844255,
            "auditor_fn_violation": 0.008292070027076144,
            "auditor_fp_violation": 0.0283289373776126,
            "ave_precision_score": 0.7667057514952054,
            "fpr": 0.15570175438596492,
            "logloss": 1.5829567272165102,
            "mae": 0.28928831465735205,
            "precision": 0.7269230769230769,
            "recall": 0.7991543340380549
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.785773675138478,
            "auditor_fn_violation": 0.013215698177278858,
            "auditor_fp_violation": 0.03151150026804176,
            "ave_precision_score": 0.7694513122444309,
            "fpr": 0.16575192096597147,
            "logloss": 1.6771888050746298,
            "mae": 0.28771385695569757,
            "precision": 0.7193308550185874,
            "recall": 0.8045738045738046
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7913461194431861,
            "auditor_fn_violation": 0.004984051036682614,
            "auditor_fp_violation": 0.019611956999560403,
            "ave_precision_score": 0.7919380503565228,
            "fpr": 0.1425438596491228,
            "logloss": 1.0194022011579538,
            "mae": 0.28324158534579047,
            "precision": 0.7368421052631579,
            "recall": 0.7695560253699789
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8216841310082148,
            "auditor_fn_violation": 0.008970973844738944,
            "auditor_fp_violation": 0.02075919638526537,
            "ave_precision_score": 0.8220103459403548,
            "fpr": 0.15148188803512624,
            "logloss": 0.9339342533867139,
            "mae": 0.27880331543841574,
            "precision": 0.73046875,
            "recall": 0.7775467775467776
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 6654,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.7902703821733053,
            "auditor_fn_violation": 0.007673120433218354,
            "auditor_fp_violation": 0.018857650961115777,
            "ave_precision_score": 0.7910944202027295,
            "fpr": 0.13706140350877194,
            "logloss": 0.9116569517776084,
            "mae": 0.2784803051688089,
            "precision": 0.7443762781186094,
            "recall": 0.7695560253699789
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8256736123752224,
            "auditor_fn_violation": 0.011004333726616935,
            "auditor_fp_violation": 0.020404360146018947,
            "ave_precision_score": 0.8260163897662625,
            "fpr": 0.1394072447859495,
            "logloss": 0.8281651799576395,
            "mae": 0.2709763060930055,
            "precision": 0.7475149105367793,
            "recall": 0.7817047817047817
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7771264111026925,
            "auditor_fn_violation": 0.011989540447312786,
            "auditor_fp_violation": 0.022559245494145397,
            "ave_precision_score": 0.7572919172434849,
            "fpr": 0.14802631578947367,
            "logloss": 1.8187116835465147,
            "mae": 0.2964244993176372,
            "precision": 0.725609756097561,
            "recall": 0.7547568710359408
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7845872964318443,
            "auditor_fn_violation": 0.008733634419693697,
            "auditor_fp_violation": 0.026702065197967986,
            "ave_precision_score": 0.7650283792885585,
            "fpr": 0.15916575192096596,
            "logloss": 1.8304269184130992,
            "mae": 0.2943538941682123,
            "precision": 0.7156862745098039,
            "recall": 0.7588357588357588
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8134409915927616,
            "auditor_fn_violation": 0.016591094543970918,
            "auditor_fp_violation": 0.01900001998161692,
            "ave_precision_score": 0.8144008436592662,
            "fpr": 0.11403508771929824,
            "logloss": 0.9761534328279965,
            "mae": 0.2741136796368536,
            "precision": 0.7699115044247787,
            "recall": 0.7357293868921776
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8534771621610614,
            "auditor_fn_violation": 0.01508018192979774,
            "auditor_fp_violation": 0.012626043448293472,
            "ave_precision_score": 0.8536597237362287,
            "fpr": 0.1141602634467618,
            "logloss": 0.8543374956744894,
            "mae": 0.264128870710077,
            "precision": 0.7758620689655172,
            "recall": 0.7484407484407485
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.7776275587449631,
            "auditor_fn_violation": 0.009643559215162642,
            "auditor_fp_violation": 0.028683611077808417,
            "ave_precision_score": 0.7635379530147997,
            "fpr": 0.15570175438596492,
            "logloss": 1.6028105655291953,
            "mae": 0.2904086622271385,
            "precision": 0.7263969171483622,
            "recall": 0.7970401691331924
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7853450400521904,
            "auditor_fn_violation": 0.014548450333302149,
            "auditor_fp_violation": 0.031986317106170066,
            "ave_precision_score": 0.7697644795864063,
            "fpr": 0.16465422612513722,
            "logloss": 1.657422723988378,
            "mae": 0.2887030080491758,
            "precision": 0.7201492537313433,
            "recall": 0.8024948024948025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8172834839141772,
            "auditor_fn_violation": 0.012777715960090502,
            "auditor_fp_violation": 0.02512188786316589,
            "ave_precision_score": 0.814155498314405,
            "fpr": 0.1699561403508772,
            "logloss": 0.9124827648332486,
            "mae": 0.2801374424031521,
            "precision": 0.726148409893993,
            "recall": 0.86892177589852
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8377943446824527,
            "auditor_fn_violation": 0.01489304892158899,
            "auditor_fp_violation": 0.013197865877007128,
            "ave_precision_score": 0.8361660057416407,
            "fpr": 0.16465422612513722,
            "logloss": 0.8345888218607962,
            "mae": 0.26275043570656,
            "precision": 0.7400346620450606,
            "recall": 0.8877338877338877
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.7947267656107316,
            "auditor_fn_violation": 0.01542737658098735,
            "auditor_fp_violation": 0.026003576709427333,
            "ave_precision_score": 0.7945687506461809,
            "fpr": 0.125,
            "logloss": 1.2238749375874918,
            "mae": 0.2816991660985659,
            "precision": 0.7494505494505495,
            "recall": 0.7209302325581395
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8213042476563532,
            "auditor_fn_violation": 0.01399846185795692,
            "auditor_fp_violation": 0.017619278584739488,
            "ave_precision_score": 0.8215798895838862,
            "fpr": 0.12294182217343579,
            "logloss": 1.1118892118431767,
            "mae": 0.27533842511117934,
            "precision": 0.7586206896551724,
            "recall": 0.7318087318087318
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 6654,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.7733829933045835,
            "auditor_fn_violation": 0.006989262267720039,
            "auditor_fp_violation": 0.02700765295927747,
            "ave_precision_score": 0.7527196399006085,
            "fpr": 0.16228070175438597,
            "logloss": 1.8964220425941067,
            "mae": 0.2948256064301502,
            "precision": 0.7175572519083969,
            "recall": 0.7949260042283298
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7816848512629979,
            "auditor_fn_violation": 0.011314700667060713,
            "auditor_fp_violation": 0.032101192147652724,
            "ave_precision_score": 0.7627501506169921,
            "fpr": 0.17014270032930845,
            "logloss": 1.8801693528312147,
            "mae": 0.29219771815127654,
            "precision": 0.712430426716141,
            "recall": 0.7983367983367984
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 6654,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.6857363634444642,
            "auditor_fn_violation": 0.007979117985237942,
            "auditor_fp_violation": 0.02093324141789554,
            "ave_precision_score": 0.6701084558543855,
            "fpr": 0.19188596491228072,
            "logloss": 2.3630574260005544,
            "mae": 0.3009160500796751,
            "precision": 0.6806569343065694,
            "recall": 0.7885835095137421
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7065318645317644,
            "auditor_fn_violation": 0.012430652386744599,
            "auditor_fp_violation": 0.019615551527838053,
            "ave_precision_score": 0.6857766601979732,
            "fpr": 0.19319429198682767,
            "logloss": 2.4249127169767046,
            "mae": 0.2957216989547424,
            "precision": 0.6890459363957597,
            "recall": 0.8108108108108109
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 6654,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.7917001610750964,
            "auditor_fn_violation": 0.013881161677979306,
            "auditor_fp_violation": 0.017603804499860133,
            "ave_precision_score": 0.776690942858653,
            "fpr": 0.14144736842105263,
            "logloss": 1.5503681474147468,
            "mae": 0.27601371449942436,
            "precision": 0.7465618860510805,
            "recall": 0.8033826638477801
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.7978616435198668,
            "auditor_fn_violation": 0.010534219096238856,
            "auditor_fp_violation": 0.020851096418451485,
            "ave_precision_score": 0.7808456658330303,
            "fpr": 0.1525795828759605,
            "logloss": 1.6110331680652994,
            "mae": 0.27288855592948275,
            "precision": 0.7362428842504743,
            "recall": 0.8066528066528067
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8099704260934669,
            "auditor_fn_violation": 0.018540669856459333,
            "auditor_fp_violation": 0.02378311953003237,
            "ave_precision_score": 0.8106047235290343,
            "fpr": 0.13815789473684212,
            "logloss": 1.0422442004033494,
            "mae": 0.2761937799724302,
            "precision": 0.7407407407407407,
            "recall": 0.7610993657505285
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8395091217660144,
            "auditor_fn_violation": 0.011990205184497178,
            "auditor_fp_violation": 0.01957981262604346,
            "ave_precision_score": 0.8397437851274254,
            "fpr": 0.12623490669593854,
            "logloss": 0.9398395270042003,
            "mae": 0.264488144637085,
            "precision": 0.762396694214876,
            "recall": 0.7671517671517671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7780780121839852,
            "auditor_fn_violation": 0.008083435332517343,
            "auditor_fp_violation": 0.020853314950245776,
            "ave_precision_score": 0.7787706110444352,
            "fpr": 0.14035087719298245,
            "logloss": 1.0827319141617973,
            "mae": 0.29048281862859854,
            "precision": 0.7333333333333333,
            "recall": 0.7441860465116279
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.8082551449315747,
            "auditor_fn_violation": 0.009582579286201681,
            "auditor_fp_violation": 0.025558420340540677,
            "ave_precision_score": 0.8085588278311963,
            "fpr": 0.14709110867178923,
            "logloss": 0.9834913277722374,
            "mae": 0.2826950765610776,
            "precision": 0.7309236947791165,
            "recall": 0.7567567567567568
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7815908001950955,
            "auditor_fn_violation": 0.00836161492526242,
            "auditor_fp_violation": 0.017918514966231074,
            "ave_precision_score": 0.7592833801302735,
            "fpr": 0.14692982456140352,
            "logloss": 1.8918189937920984,
            "mae": 0.2897358685910516,
            "precision": 0.7276422764227642,
            "recall": 0.7568710359408034
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7931689172334689,
            "auditor_fn_violation": 0.009032590810856455,
            "auditor_fp_violation": 0.02361320297143441,
            "ave_precision_score": 0.7715537398605425,
            "fpr": 0.15367727771679474,
            "logloss": 1.8858632694954403,
            "mae": 0.2825627243382929,
            "precision": 0.7281553398058253,
            "recall": 0.7796257796257796
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.7952612937265532,
            "auditor_fn_violation": 0.01091391268869849,
            "auditor_fp_violation": 0.020745913759341414,
            "ave_precision_score": 0.7615715027345493,
            "fpr": 0.18201754385964913,
            "logloss": 2.261815280908535,
            "mae": 0.28327268445031895,
            "precision": 0.7097902097902098,
            "recall": 0.8583509513742071
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.803514937562745,
            "auditor_fn_violation": 0.013245365605409518,
            "auditor_fp_violation": 0.015906364077298146,
            "ave_precision_score": 0.7681865918761487,
            "fpr": 0.16355653128430298,
            "logloss": 2.3326065240254876,
            "mae": 0.2689066097499933,
            "precision": 0.7353463587921847,
            "recall": 0.8607068607068608
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.7917339602619389,
            "auditor_fn_violation": 0.008440432476540185,
            "auditor_fp_violation": 0.01421692043320146,
            "ave_precision_score": 0.7804847797156858,
            "fpr": 0.13596491228070176,
            "logloss": 1.3443644070430565,
            "mae": 0.28000113889331996,
            "precision": 0.7459016393442623,
            "recall": 0.7695560253699789
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8027225840047347,
            "auditor_fn_violation": 0.014021282956518966,
            "auditor_fp_violation": 0.018405534424220767,
            "ave_precision_score": 0.7903518886183589,
            "fpr": 0.13611416026344675,
            "logloss": 1.3788960428373036,
            "mae": 0.2744649253258099,
            "precision": 0.7484787018255578,
            "recall": 0.7671517671517671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.7716893465999668,
            "auditor_fn_violation": 0.006685582878973332,
            "auditor_fp_violation": 0.02299134795987692,
            "ave_precision_score": 0.7723379334077892,
            "fpr": 0.14473684210526316,
            "logloss": 1.050523309586004,
            "mae": 0.2839331020527025,
            "precision": 0.7327935222672065,
            "recall": 0.7653276955602537
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8048266418088608,
            "auditor_fn_violation": 0.012248083598248258,
            "auditor_fp_violation": 0.02746279325045312,
            "ave_precision_score": 0.8052642249182431,
            "fpr": 0.14928649835345773,
            "logloss": 0.9488067342577219,
            "mae": 0.2779074680734149,
            "precision": 0.7333333333333333,
            "recall": 0.7775467775467776
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7509693651825141,
            "auditor_fn_violation": 0.007601257371759213,
            "auditor_fp_violation": 0.029310534308436244,
            "ave_precision_score": 0.7517523154702059,
            "fpr": 0.1513157894736842,
            "logloss": 1.0031003932158107,
            "mae": 0.2842976087900727,
            "precision": 0.727810650887574,
            "recall": 0.7801268498942917
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.7701832141710576,
            "auditor_fn_violation": 0.01465570949654375,
            "auditor_fp_violation": 0.03043422765680444,
            "ave_precision_score": 0.771109359809079,
            "fpr": 0.15148188803512624,
            "logloss": 0.9486402024798184,
            "mae": 0.2821896667122122,
            "precision": 0.7320388349514563,
            "recall": 0.7837837837837838
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.781282184609148,
            "auditor_fn_violation": 0.014993880048959608,
            "auditor_fp_violation": 0.023982935699156785,
            "ave_precision_score": 0.7614105933524942,
            "fpr": 0.1513157894736842,
            "logloss": 1.8198047452693957,
            "mae": 0.2932658112822101,
            "precision": 0.7223340040241448,
            "recall": 0.758985200845666
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.7920272097815093,
            "auditor_fn_violation": 0.013099310574612444,
            "auditor_fp_violation": 0.020764301942664595,
            "ave_precision_score": 0.7724393853077164,
            "fpr": 0.15587266739846323,
            "logloss": 1.7881735131693586,
            "mae": 0.2865856324399227,
            "precision": 0.7237354085603113,
            "recall": 0.7733887733887734
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7844472842085786,
            "auditor_fn_violation": 0.012545899632802943,
            "auditor_fp_violation": 0.019961635295528114,
            "ave_precision_score": 0.7847838801116059,
            "fpr": 0.13486842105263158,
            "logloss": 1.1692247221809111,
            "mae": 0.29160001559534576,
            "precision": 0.7337662337662337,
            "recall": 0.7167019027484144
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.8102871005208684,
            "auditor_fn_violation": 0.014274597150557636,
            "auditor_fp_violation": 0.021624588364434684,
            "ave_precision_score": 0.8106713135409123,
            "fpr": 0.141602634467618,
            "logloss": 1.0678588174058843,
            "mae": 0.2858089534349318,
            "precision": 0.7323651452282157,
            "recall": 0.7338877338877339
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.7945815595337036,
            "auditor_fn_violation": 0.014354066985645939,
            "auditor_fp_violation": 0.021095592055309118,
            "ave_precision_score": 0.7942390568912956,
            "fpr": 0.12828947368421054,
            "logloss": 1.293135535335659,
            "mae": 0.2783892855066268,
            "precision": 0.7521186440677966,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8220045307092865,
            "auditor_fn_violation": 0.009502705441234533,
            "auditor_fp_violation": 0.01937559033007429,
            "ave_precision_score": 0.8221274201186677,
            "fpr": 0.12733260153677278,
            "logloss": 1.1280514206937582,
            "mae": 0.26911452607584624,
            "precision": 0.7598343685300207,
            "recall": 0.762993762993763
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7718935404983827,
            "auditor_fn_violation": 0.007200215125551723,
            "auditor_fp_violation": 0.02299134795987692,
            "ave_precision_score": 0.7727026142347676,
            "fpr": 0.14473684210526316,
            "logloss": 1.0542370682897342,
            "mae": 0.2840333464311887,
            "precision": 0.7322515212981744,
            "recall": 0.7632135306553911
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8050038879209945,
            "auditor_fn_violation": 0.012076925359032932,
            "auditor_fp_violation": 0.027914635080284893,
            "ave_precision_score": 0.80543949991507,
            "fpr": 0.14818880351262348,
            "logloss": 0.9519552684981183,
            "mae": 0.2779682947077131,
            "precision": 0.7337278106508875,
            "recall": 0.7733887733887734
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.7424216606500289,
            "auditor_fn_violation": 0.015499239642446497,
            "auditor_fp_violation": 0.038651940215002206,
            "ave_precision_score": 0.7442847240078325,
            "fpr": 0.12390350877192982,
            "logloss": 0.754423639922516,
            "mae": 0.3097854119178143,
            "precision": 0.7585470085470085,
            "recall": 0.7505285412262156
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.7385438737192989,
            "auditor_fn_violation": 0.02062570888037409,
            "auditor_fp_violation": 0.047588900518214086,
            "ave_precision_score": 0.7404555491321054,
            "fpr": 0.132821075740944,
            "logloss": 0.7289278038166406,
            "mae": 0.30271903703572983,
            "precision": 0.7484407484407485,
            "recall": 0.7484407484407485
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7715674169581033,
            "auditor_fn_violation": 0.011143410852713179,
            "auditor_fp_violation": 0.023208648043799712,
            "ave_precision_score": 0.7717402792493406,
            "fpr": 0.14912280701754385,
            "logloss": 1.160035229209829,
            "mae": 0.2892239740193945,
            "precision": 0.7246963562753036,
            "recall": 0.7568710359408034
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8031217803308448,
            "auditor_fn_violation": 0.011301008007923491,
            "auditor_fp_violation": 0.02261251372118551,
            "ave_precision_score": 0.803473501966337,
            "fpr": 0.14928649835345773,
            "logloss": 1.0613933754897582,
            "mae": 0.2811942914249053,
            "precision": 0.7306930693069307,
            "recall": 0.7671517671517671
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 6654,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.7849352573144988,
            "auditor_fn_violation": 0.009050109417306487,
            "auditor_fp_violation": 0.019574491467849585,
            "ave_precision_score": 0.7692707793078126,
            "fpr": 0.15679824561403508,
            "logloss": 1.621503254065327,
            "mae": 0.2819640251950964,
            "precision": 0.7317073170731707,
            "recall": 0.8245243128964059
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.7934822718299992,
            "auditor_fn_violation": 0.01590174147803127,
            "auditor_fp_violation": 0.023784239144308585,
            "ave_precision_score": 0.7771110352328623,
            "fpr": 0.16355653128430298,
            "logloss": 1.6417011869925684,
            "mae": 0.2792133203655296,
            "precision": 0.7245841035120147,
            "recall": 0.814968814968815
        }
    }
]
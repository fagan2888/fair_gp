[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.84804082092017,
            "auditor_fn_violation": 0.009283025325119784,
            "auditor_fp_violation": 0.019775541795665638,
            "ave_precision_score": 0.8483083364678922,
            "fpr": 0.125,
            "logloss": 0.7005369947843103,
            "mae": 0.26609303757305125,
            "precision": 0.7692307692307693,
            "recall": 0.7802874743326489
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8603224485964012,
            "auditor_fn_violation": 0.006938747687671738,
            "auditor_fp_violation": 0.006066989052719021,
            "ave_precision_score": 0.8608397410448787,
            "fpr": 0.141602634467618,
            "logloss": 0.635343277932905,
            "mae": 0.2490895526963755,
            "precision": 0.7547528517110266,
            "recall": 0.8501070663811563
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8448892557606598,
            "auditor_fn_violation": 0.015164000864584458,
            "auditor_fp_violation": 0.01855263157894737,
            "ave_precision_score": 0.8452064355518323,
            "fpr": 0.16776315789473684,
            "logloss": 0.7336463213049358,
            "mae": 0.2687264432495619,
            "precision": 0.7258064516129032,
            "recall": 0.8316221765913757
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8524426906306091,
            "auditor_fn_violation": 0.006489797549343382,
            "auditor_fp_violation": 0.01395358036411824,
            "ave_precision_score": 0.8536900098177792,
            "fpr": 0.18221734357848518,
            "logloss": 0.7078302958505652,
            "mae": 0.26331948104125685,
            "precision": 0.711304347826087,
            "recall": 0.8758029978586723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.855961049670998,
            "auditor_fn_violation": 0.01326371627220001,
            "auditor_fp_violation": 0.016253869969040255,
            "ave_precision_score": 0.8561741928363273,
            "fpr": 0.16447368421052633,
            "logloss": 0.708559061305788,
            "mae": 0.26669444232702905,
            "precision": 0.7311827956989247,
            "recall": 0.837782340862423
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8558408541126569,
            "auditor_fn_violation": 0.0040969638277817905,
            "auditor_fp_violation": 0.011681549826445552,
            "ave_precision_score": 0.8570809585200304,
            "fpr": 0.17672886937431395,
            "logloss": 0.6996611189835387,
            "mae": 0.2624399297792774,
            "precision": 0.7170474516695958,
            "recall": 0.8736616702355461
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8463710830691955,
            "auditor_fn_violation": 0.011752944990813792,
            "auditor_fp_violation": 0.01924406604747162,
            "ave_precision_score": 0.8468143980056233,
            "fpr": 0.17214912280701755,
            "logloss": 0.7279446695427152,
            "mae": 0.2704102133471647,
            "precision": 0.7221238938053097,
            "recall": 0.837782340862423
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8498607831036717,
            "auditor_fn_violation": 0.00754518295305768,
            "auditor_fp_violation": 0.016999436318865524,
            "ave_precision_score": 0.8511184292496483,
            "fpr": 0.18441273326015367,
            "logloss": 0.7196410290883464,
            "mae": 0.2650233395614247,
            "precision": 0.7108433734939759,
            "recall": 0.8843683083511777
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8433368119070959,
            "auditor_fn_violation": 0.011151788609099754,
            "auditor_fp_violation": 0.018482972136222915,
            "ave_precision_score": 0.843628022781799,
            "fpr": 0.1787280701754386,
            "logloss": 0.7474477560379535,
            "mae": 0.2701812832185898,
            "precision": 0.7175043327556326,
            "recall": 0.8501026694045175
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8499188908477173,
            "auditor_fn_violation": 0.004564718160385674,
            "auditor_fp_violation": 0.01824052372899794,
            "ave_precision_score": 0.8511744770718858,
            "fpr": 0.18990120746432493,
            "logloss": 0.7346149873734619,
            "mae": 0.2658425767449043,
            "precision": 0.7062818336162988,
            "recall": 0.8907922912205567
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8398267648266837,
            "auditor_fn_violation": 0.012869699917143994,
            "auditor_fp_violation": 0.014576883384932925,
            "ave_precision_score": 0.8401396707820546,
            "fpr": 0.13706140350877194,
            "logloss": 0.6736553758516494,
            "mae": 0.27723438876348966,
            "precision": 0.7539370078740157,
            "recall": 0.7864476386036962
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8580615714202442,
            "auditor_fn_violation": 0.004969008337309637,
            "auditor_fp_violation": 0.008521968730530754,
            "ave_precision_score": 0.8593037972699089,
            "fpr": 0.141602634467618,
            "logloss": 0.5914110397397617,
            "mae": 0.2546091715421176,
            "precision": 0.7528735632183908,
            "recall": 0.841541755888651
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8429382075357166,
            "auditor_fn_violation": 0.010467325912316729,
            "auditor_fp_violation": 0.01888286893704851,
            "ave_precision_score": 0.8434790639080362,
            "fpr": 0.13486842105263158,
            "logloss": 0.714049062758204,
            "mae": 0.2677912241404024,
            "precision": 0.757396449704142,
            "recall": 0.7885010266940452
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8532068488086861,
            "auditor_fn_violation": 0.011625693110848379,
            "auditor_fp_violation": 0.009829807853957142,
            "ave_precision_score": 0.8534933504630261,
            "fpr": 0.14270032930845225,
            "logloss": 0.6668204736170934,
            "mae": 0.25677388200825907,
            "precision": 0.7495183044315993,
            "recall": 0.8329764453961456
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8417183107006029,
            "auditor_fn_violation": 0.012000612413991862,
            "auditor_fp_violation": 0.019143446852425187,
            "ave_precision_score": 0.8422558469895214,
            "fpr": 0.1787280701754386,
            "logloss": 0.7366266316196868,
            "mae": 0.2738649088668211,
            "precision": 0.7160278745644599,
            "recall": 0.8439425051334702
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.847715993827938,
            "auditor_fn_violation": 0.007655657594426437,
            "auditor_fp_violation": 0.022695582519951355,
            "ave_precision_score": 0.8484507963516401,
            "fpr": 0.19758507135016465,
            "logloss": 0.7374103510866016,
            "mae": 0.27232919286620566,
            "precision": 0.6959459459459459,
            "recall": 0.8822269807280514
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8428464961928153,
            "auditor_fn_violation": 0.01559404157210275,
            "auditor_fp_violation": 0.018908668730650155,
            "ave_precision_score": 0.8432089457989209,
            "fpr": 0.12938596491228072,
            "logloss": 0.6976472559831017,
            "mae": 0.2669199192037162,
            "precision": 0.7620967741935484,
            "recall": 0.7761806981519507
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8573339797454627,
            "auditor_fn_violation": 0.007063325474747146,
            "auditor_fp_violation": 0.013202005518141637,
            "ave_precision_score": 0.8575941125366733,
            "fpr": 0.14489571899012074,
            "logloss": 0.6386235663137423,
            "mae": 0.2560170568613496,
            "precision": 0.746641074856046,
            "recall": 0.8329764453961456
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8531934590390046,
            "auditor_fn_violation": 0.012401383335134556,
            "auditor_fp_violation": 0.018570691434468524,
            "ave_precision_score": 0.8536257847394835,
            "fpr": 0.15460526315789475,
            "logloss": 0.6409272864475527,
            "mae": 0.2703020980015297,
            "precision": 0.7427007299270073,
            "recall": 0.8357289527720739
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8515239507575009,
            "auditor_fn_violation": 0.005361545892811394,
            "auditor_fp_violation": 0.020974871688373337,
            "ave_precision_score": 0.8520727530880639,
            "fpr": 0.17233809001097694,
            "logloss": 0.6478145501086621,
            "mae": 0.26670089129512803,
            "precision": 0.7191413237924866,
            "recall": 0.860813704496788
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8440632148384102,
            "auditor_fn_violation": 0.012486941172232435,
            "auditor_fp_violation": 0.01776057791537668,
            "ave_precision_score": 0.844378043733794,
            "fpr": 0.14035087719298245,
            "logloss": 0.7248829946502071,
            "mae": 0.2664185909020929,
            "precision": 0.7504873294346979,
            "recall": 0.7905544147843943
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8584037430979596,
            "auditor_fn_violation": 0.0031755583082806625,
            "auditor_fp_violation": 0.015427062627940791,
            "ave_precision_score": 0.8586632118211616,
            "fpr": 0.15806805708013172,
            "logloss": 0.6780396943359396,
            "mae": 0.25518379370883887,
            "precision": 0.7343173431734318,
            "recall": 0.8522483940042827
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8465454698299266,
            "auditor_fn_violation": 0.011658381065600349,
            "auditor_fp_violation": 0.019943240454076375,
            "ave_precision_score": 0.8468851647804987,
            "fpr": 0.18092105263157895,
            "logloss": 0.7331292094008502,
            "mae": 0.2699510268415734,
            "precision": 0.7135416666666666,
            "recall": 0.8439425051334702
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.851157218349925,
            "auditor_fn_violation": 0.004564718160385674,
            "auditor_fp_violation": 0.0132910078025336,
            "ave_precision_score": 0.8524017098060561,
            "fpr": 0.18880351262349068,
            "logloss": 0.7212325143087517,
            "mae": 0.2657775926802042,
            "precision": 0.7074829931972789,
            "recall": 0.8907922912205567
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8408169192979582,
            "auditor_fn_violation": 0.011701159984149286,
            "auditor_fp_violation": 0.020190918472652225,
            "ave_precision_score": 0.8411433042750048,
            "fpr": 0.18311403508771928,
            "logloss": 0.729367409765282,
            "mae": 0.27440649421936414,
            "precision": 0.7125645438898451,
            "recall": 0.8501026694045175
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.844542858230328,
            "auditor_fn_violation": 0.0051899576200471545,
            "auditor_fp_violation": 0.017919126590915838,
            "ave_precision_score": 0.8458074207327422,
            "fpr": 0.18880351262349068,
            "logloss": 0.7222390376954245,
            "mae": 0.27091441129177185,
            "precision": 0.706984667802385,
            "recall": 0.8886509635974305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8503937211296893,
            "auditor_fn_violation": 0.01410128246694766,
            "auditor_fp_violation": 0.01869969040247678,
            "ave_precision_score": 0.8507503841972426,
            "fpr": 0.12719298245614036,
            "logloss": 0.7027290254091908,
            "mae": 0.26311518675535844,
            "precision": 0.7661290322580645,
            "recall": 0.7802874743326489
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8612596586410352,
            "auditor_fn_violation": 0.006228889353770359,
            "auditor_fp_violation": 0.010255040990496539,
            "ave_precision_score": 0.8614904313570853,
            "fpr": 0.14050493962678376,
            "logloss": 0.6507168629254243,
            "mae": 0.24985616325477186,
            "precision": 0.7543186180422264,
            "recall": 0.841541755888651
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8319541459399595,
            "auditor_fn_violation": 0.01207266111891639,
            "auditor_fp_violation": 0.02156346749226007,
            "ave_precision_score": 0.8333401609114309,
            "fpr": 0.17982456140350878,
            "logloss": 0.7525220051956836,
            "mae": 0.27367534047158815,
            "precision": 0.7142857142857143,
            "recall": 0.8418891170431212
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8460641001154567,
            "auditor_fn_violation": 0.0064192818208101314,
            "auditor_fp_violation": 0.023679552219618084,
            "ave_precision_score": 0.8467961003116553,
            "fpr": 0.19978046103183314,
            "logloss": 0.7488024002697189,
            "mae": 0.2733316236186037,
            "precision": 0.6946308724832215,
            "recall": 0.8865096359743041
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8513533155098773,
            "auditor_fn_violation": 0.013819842213336219,
            "auditor_fp_violation": 0.017180082559339525,
            "ave_precision_score": 0.8515958288587235,
            "fpr": 0.14144736842105263,
            "logloss": 0.7024194907401655,
            "mae": 0.26456116429101445,
            "precision": 0.7519230769230769,
            "recall": 0.8028747433264887
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.858370198089622,
            "auditor_fn_violation": 0.0056201035640999755,
            "auditor_fp_violation": 0.012593823241463202,
            "ave_precision_score": 0.8586616839870993,
            "fpr": 0.15587266739846323,
            "logloss": 0.6766445469488228,
            "mae": 0.2549890319554323,
            "precision": 0.7365491651205937,
            "recall": 0.8501070663811563
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8513060577196523,
            "auditor_fn_violation": 0.01117880687344645,
            "auditor_fp_violation": 0.02042311661506708,
            "ave_precision_score": 0.8516979373800633,
            "fpr": 0.1337719298245614,
            "logloss": 0.6930779116629947,
            "mae": 0.26281154126384537,
            "precision": 0.7579365079365079,
            "recall": 0.784394250513347
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8633646027117551,
            "auditor_fn_violation": 0.005989135876757311,
            "auditor_fp_violation": 0.015056219776307595,
            "ave_precision_score": 0.8635705861189203,
            "fpr": 0.14818880351262348,
            "logloss": 0.6401415586599446,
            "mae": 0.25028907305400416,
            "precision": 0.7448015122873346,
            "recall": 0.8436830835117773
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8077833144547002,
            "auditor_fn_violation": 0.013527144349580324,
            "auditor_fp_violation": 0.02525283797729619,
            "ave_precision_score": 0.8081352589498194,
            "fpr": 0.13267543859649122,
            "logloss": 1.2942067731391416,
            "mae": 0.3133031328646349,
            "precision": 0.7436440677966102,
            "recall": 0.7207392197125256
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8289385673856161,
            "auditor_fn_violation": 0.013449699955575095,
            "auditor_fp_violation": 0.01208947696324206,
            "ave_precision_score": 0.8291678333264891,
            "fpr": 0.12952799121844127,
            "logloss": 1.348688512602686,
            "mae": 0.29307344649620554,
            "precision": 0.7467811158798283,
            "recall": 0.7451820128479657
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8476049777616363,
            "auditor_fn_violation": 0.014513310998234805,
            "auditor_fp_violation": 0.018103715170278634,
            "ave_precision_score": 0.8479708531763842,
            "fpr": 0.12171052631578948,
            "logloss": 0.6859077658868171,
            "mae": 0.26721108177254294,
            "precision": 0.7730061349693251,
            "recall": 0.7761806981519507
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8623658804682159,
            "auditor_fn_violation": 0.006814169900596329,
            "auditor_fp_violation": 0.011622214970184237,
            "ave_precision_score": 0.8626653894331873,
            "fpr": 0.13721185510428102,
            "logloss": 0.6195672770617956,
            "mae": 0.2517746590471595,
            "precision": 0.7553816046966731,
            "recall": 0.8265524625267666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8445609235519063,
            "auditor_fn_violation": 0.012200997874563206,
            "auditor_fp_violation": 0.01858875128998969,
            "ave_precision_score": 0.8450083946618829,
            "fpr": 0.17763157894736842,
            "logloss": 0.7343033960839951,
            "mae": 0.27218366384428855,
            "precision": 0.7162872154115587,
            "recall": 0.839835728952772
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8494206809110719,
            "auditor_fn_violation": 0.005662413001219925,
            "auditor_fp_violation": 0.01933821856983219,
            "ave_precision_score": 0.8506787055663085,
            "fpr": 0.18660812294182216,
            "logloss": 0.7236041639801949,
            "mae": 0.26672037669558374,
            "precision": 0.7098976109215017,
            "recall": 0.8907922912205567
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8468298097253831,
            "auditor_fn_violation": 0.011752944990813792,
            "auditor_fp_violation": 0.019264705882352944,
            "ave_precision_score": 0.847271937579278,
            "fpr": 0.17324561403508773,
            "logloss": 0.7255297772252047,
            "mae": 0.2702786217318481,
            "precision": 0.7208480565371025,
            "recall": 0.837782340862423
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8514987287022048,
            "auditor_fn_violation": 0.00754518295305768,
            "auditor_fp_violation": 0.017941377162013827,
            "ave_precision_score": 0.8516839360423736,
            "fpr": 0.18551042810098792,
            "logloss": 0.717359202672465,
            "mae": 0.2650051869706811,
            "precision": 0.7096219931271478,
            "recall": 0.8843683083511777
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8447266992974626,
            "auditor_fn_violation": 0.010194891746820853,
            "auditor_fp_violation": 0.01843911248710011,
            "ave_precision_score": 0.8451901504747075,
            "fpr": 0.13596491228070176,
            "logloss": 0.7001468892516475,
            "mae": 0.26807058978132164,
            "precision": 0.7529880478087649,
            "recall": 0.7761806981519507
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8620810494312472,
            "auditor_fn_violation": 0.006938747687671738,
            "auditor_fp_violation": 0.014141474075612389,
            "ave_precision_score": 0.8622957303525055,
            "fpr": 0.150384193194292,
            "logloss": 0.6384849637209981,
            "mae": 0.25510655828510514,
            "precision": 0.7390476190476191,
            "recall": 0.8308351177730193
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8463783651875486,
            "auditor_fn_violation": 0.011752944990813792,
            "auditor_fp_violation": 0.01924406604747162,
            "ave_precision_score": 0.8468900882365709,
            "fpr": 0.17214912280701755,
            "logloss": 0.7278543985911282,
            "mae": 0.27040612343504966,
            "precision": 0.7221238938053097,
            "recall": 0.837782340862423
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8498652639543969,
            "auditor_fn_violation": 0.00754518295305768,
            "auditor_fp_violation": 0.016999436318865524,
            "ave_precision_score": 0.8511220581325035,
            "fpr": 0.18441273326015367,
            "logloss": 0.7195754599528553,
            "mae": 0.2650376371954658,
            "precision": 0.7108433734939759,
            "recall": 0.8843683083511777
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8460545172637315,
            "auditor_fn_violation": 0.013709517633920528,
            "auditor_fp_violation": 0.02065789473684211,
            "ave_precision_score": 0.8463282746295852,
            "fpr": 0.13048245614035087,
            "logloss": 0.7164676757526984,
            "mae": 0.26574285102697726,
            "precision": 0.7629482071713147,
            "recall": 0.7864476386036962
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8587451448241044,
            "auditor_fn_violation": 0.005972682206766224,
            "auditor_fp_violation": 0.012868246951671764,
            "ave_precision_score": 0.8591818213601993,
            "fpr": 0.150384193194292,
            "logloss": 0.6543261939372644,
            "mae": 0.2522052352910884,
            "precision": 0.7429643527204502,
            "recall": 0.8479657387580299
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8467619276535734,
            "auditor_fn_violation": 0.015711120717605106,
            "auditor_fp_violation": 0.017742518059855526,
            "ave_precision_score": 0.8470893795677292,
            "fpr": 0.125,
            "logloss": 0.6604245409910375,
            "mae": 0.2711811053547272,
            "precision": 0.7663934426229508,
            "recall": 0.7679671457905544
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8569556171973545,
            "auditor_fn_violation": 0.005918620148224063,
            "auditor_fp_violation": 0.013602515797905484,
            "ave_precision_score": 0.8574065076218436,
            "fpr": 0.1350164654226125,
            "logloss": 0.6053704487502609,
            "mae": 0.255127577105698,
            "precision": 0.7588235294117647,
            "recall": 0.828693790149893
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8438343386884343,
            "auditor_fn_violation": 0.012998036672790808,
            "auditor_fp_violation": 0.0201109391124871,
            "ave_precision_score": 0.844160509125996,
            "fpr": 0.15350877192982457,
            "logloss": 0.7267853922864681,
            "mae": 0.26828652118112134,
            "precision": 0.7392923649906891,
            "recall": 0.8151950718685832
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8482358431013867,
            "auditor_fn_violation": 0.006431034442232344,
            "auditor_fp_violation": 0.014616152925702883,
            "ave_precision_score": 0.8495854769153334,
            "fpr": 0.1602634467618002,
            "logloss": 0.6963667981484547,
            "mae": 0.258096075781202,
            "precision": 0.7364620938628159,
            "recall": 0.8736616702355461
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8384880261645875,
            "auditor_fn_violation": 0.02067572679131093,
            "auditor_fp_violation": 0.012680598555211557,
            "ave_precision_score": 0.8387257481340368,
            "fpr": 0.10416666666666667,
            "logloss": 0.5503716285087084,
            "mae": 0.31083043904723845,
            "precision": 0.7907488986784141,
            "recall": 0.7371663244353183
        },
        "train": {
            "accuracy": 0.7936333699231614,
            "auc_prc": 0.8898415311830703,
            "auditor_fn_violation": 0.008177473985572486,
            "auditor_fp_violation": 0.01336023180150513,
            "ave_precision_score": 0.8903203239057962,
            "fpr": 0.12294182217343579,
            "logloss": 0.4439532216125041,
            "mae": 0.2721779433966683,
            "precision": 0.7773359840954275,
            "recall": 0.8372591006423983
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8432336862186968,
            "auditor_fn_violation": 0.010289455672034294,
            "auditor_fp_violation": 0.021104231166150674,
            "ave_precision_score": 0.8435034167969108,
            "fpr": 0.17543859649122806,
            "logloss": 0.7477098994426811,
            "mae": 0.2709811898423017,
            "precision": 0.7222222222222222,
            "recall": 0.8542094455852156
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8483043160004364,
            "auditor_fn_violation": 0.004444841421879153,
            "auditor_fp_violation": 0.01584240662176996,
            "ave_precision_score": 0.849585194953809,
            "fpr": 0.19099890230515917,
            "logloss": 0.7429820727317438,
            "mae": 0.2687586572739439,
            "precision": 0.7050847457627119,
            "recall": 0.8907922912205567
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.8039306090997722,
            "auditor_fn_violation": 0.008992578983392776,
            "auditor_fp_violation": 0.01615067079463365,
            "ave_precision_score": 0.8044722802799236,
            "fpr": 0.19736842105263158,
            "logloss": 2.324112784402171,
            "mae": 0.31159331732887907,
            "precision": 0.6885813148788927,
            "recall": 0.8172484599589322
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.7895246487904022,
            "auditor_fn_violation": 0.006442787063654551,
            "auditor_fp_violation": 0.027479455306019526,
            "ave_precision_score": 0.7907337180697427,
            "fpr": 0.22063666300768386,
            "logloss": 2.7817125400185954,
            "mae": 0.3230099016626421,
            "precision": 0.6552315608919382,
            "recall": 0.8179871520342612
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 31658,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8535803997829294,
            "auditor_fn_violation": 0.012590511185561442,
            "auditor_fp_violation": 0.020910732714138295,
            "ave_precision_score": 0.8538030348545405,
            "fpr": 0.17543859649122806,
            "logloss": 0.7192942967574343,
            "mae": 0.2702584492517138,
            "precision": 0.7207678883071553,
            "recall": 0.8480492813141683
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8527740971493183,
            "auditor_fn_violation": 0.005439113194197966,
            "auditor_fp_violation": 0.01613166404604386,
            "ave_precision_score": 0.8540601648593705,
            "fpr": 0.18990120746432493,
            "logloss": 0.719846595269609,
            "mae": 0.26546376699684443,
            "precision": 0.70578231292517,
            "recall": 0.8886509635974305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8567590259146682,
            "auditor_fn_violation": 0.010874851399546094,
            "auditor_fp_violation": 0.01847781217750259,
            "ave_precision_score": 0.8570515177619573,
            "fpr": 0.14364035087719298,
            "logloss": 0.6493115468165004,
            "mae": 0.26583859990593117,
            "precision": 0.7480769230769231,
            "recall": 0.7987679671457906
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8634706059433933,
            "auditor_fn_violation": 0.0030415784240674904,
            "auditor_fp_violation": 0.01646047804115862,
            "ave_precision_score": 0.8636847658727037,
            "fpr": 0.1525795828759605,
            "logloss": 0.6154636979037815,
            "mae": 0.2571592401354143,
            "precision": 0.7392120075046904,
            "recall": 0.8436830835117773
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8418063105774811,
            "auditor_fn_violation": 0.013596941532475955,
            "auditor_fp_violation": 0.018077915376676988,
            "ave_precision_score": 0.8421259867534241,
            "fpr": 0.17105263157894737,
            "logloss": 0.7679211886598464,
            "mae": 0.2699645658109495,
            "precision": 0.7224199288256228,
            "recall": 0.8336755646817249
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8482436203502075,
            "auditor_fn_violation": 0.006292353509450285,
            "auditor_fp_violation": 0.013914023793277363,
            "ave_precision_score": 0.8494986055418426,
            "fpr": 0.17453347969264543,
            "logloss": 0.7422241233355613,
            "mae": 0.26315600295099184,
            "precision": 0.7195767195767195,
            "recall": 0.8736616702355461
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8438109845575326,
            "auditor_fn_violation": 0.011800226953420515,
            "auditor_fp_violation": 0.018482972136222915,
            "ave_precision_score": 0.8440958384782891,
            "fpr": 0.1787280701754386,
            "logloss": 0.7451887506544896,
            "mae": 0.27004168146950946,
            "precision": 0.7170138888888888,
            "recall": 0.8480492813141683
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.850480031795551,
            "auditor_fn_violation": 0.004564718160385674,
            "auditor_fp_violation": 0.016101996617913204,
            "ave_precision_score": 0.8517335133487969,
            "fpr": 0.18990120746432493,
            "logloss": 0.7325071741129158,
            "mae": 0.2658382780998596,
            "precision": 0.7062818336162988,
            "recall": 0.8907922912205567
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8429126016956333,
            "auditor_fn_violation": 0.008711138729781336,
            "auditor_fp_violation": 0.020838493292053662,
            "ave_precision_score": 0.8432000824092327,
            "fpr": 0.1337719298245614,
            "logloss": 0.6854930182295788,
            "mae": 0.27672474577205647,
            "precision": 0.7550200803212851,
            "recall": 0.7720739219712526
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8514541419835784,
            "auditor_fn_violation": 0.003920674506448667,
            "auditor_fp_violation": 0.011243955261518378,
            "ave_precision_score": 0.8517887769282783,
            "fpr": 0.14270032930845225,
            "logloss": 0.6311546907006063,
            "mae": 0.2590437903346504,
            "precision": 0.7485493230174082,
            "recall": 0.828693790149893
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8390483429805905,
            "auditor_fn_violation": 0.012381119636874532,
            "auditor_fp_violation": 0.017711558307533546,
            "ave_precision_score": 0.8397610545674378,
            "fpr": 0.13815789473684212,
            "logloss": 0.7188073658955547,
            "mae": 0.2716767594204675,
            "precision": 0.7514792899408284,
            "recall": 0.7823408624229979
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.849320897411247,
            "auditor_fn_violation": 0.00528397859142482,
            "auditor_fp_violation": 0.015071053490372922,
            "ave_precision_score": 0.8495700341481534,
            "fpr": 0.14489571899012074,
            "logloss": 0.6750032873103269,
            "mae": 0.2616725725027799,
            "precision": 0.7446808510638298,
            "recall": 0.8244111349036403
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8415540169468467,
            "auditor_fn_violation": 0.011633614323282542,
            "auditor_fp_violation": 0.021991744066047477,
            "ave_precision_score": 0.841871566971967,
            "fpr": 0.17324561403508773,
            "logloss": 0.7669447244667206,
            "mae": 0.270863790605598,
            "precision": 0.720353982300885,
            "recall": 0.8357289527720739
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8456456486185835,
            "auditor_fn_violation": 0.004379026741914785,
            "auditor_fp_violation": 0.020055181416323023,
            "ave_precision_score": 0.8469135825598764,
            "fpr": 0.18221734357848518,
            "logloss": 0.7538552589679061,
            "mae": 0.2641619067951704,
            "precision": 0.7132987910189983,
            "recall": 0.8843683083511777
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8382911815365506,
            "auditor_fn_violation": 0.0144097409849058,
            "auditor_fp_violation": 0.019649122807017545,
            "ave_precision_score": 0.8385941439563728,
            "fpr": 0.11732456140350878,
            "logloss": 0.7696154911432305,
            "mae": 0.27138991887247493,
            "precision": 0.7733050847457628,
            "recall": 0.7494866529774127
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8483682788063828,
            "auditor_fn_violation": 0.005063029308687302,
            "auditor_fp_violation": 0.008131347593477126,
            "ave_precision_score": 0.8488285327747802,
            "fpr": 0.13062568605927552,
            "logloss": 0.6912444330099119,
            "mae": 0.25521915055651995,
            "precision": 0.7600806451612904,
            "recall": 0.8072805139186295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8457091313161349,
            "auditor_fn_violation": 0.011800226953420515,
            "auditor_fp_violation": 0.016292569659442733,
            "ave_precision_score": 0.8460083698096552,
            "fpr": 0.18092105263157895,
            "logloss": 0.7314282302139785,
            "mae": 0.27126448949476367,
            "precision": 0.7145328719723183,
            "recall": 0.8480492813141683
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8532108641007772,
            "auditor_fn_violation": 0.007021016037627192,
            "auditor_fp_violation": 0.016101996617913204,
            "ave_precision_score": 0.8544619168067222,
            "fpr": 0.18990120746432493,
            "logloss": 0.7145707880869548,
            "mae": 0.26544475186081384,
            "precision": 0.70578231292517,
            "recall": 0.8886509635974305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 31658,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.855251616132761,
            "auditor_fn_violation": 0.011678644763860373,
            "auditor_fp_violation": 0.016689886480908155,
            "ave_precision_score": 0.8554701216065461,
            "fpr": 0.13486842105263158,
            "logloss": 0.6719905646982453,
            "mae": 0.26196440856730624,
            "precision": 0.7592954990215264,
            "recall": 0.7967145790554415
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8549179630348352,
            "auditor_fn_violation": 0.006494498597912269,
            "auditor_fp_violation": 0.01890804086193769,
            "ave_precision_score": 0.8561652420071275,
            "fpr": 0.14928649835345773,
            "logloss": 0.6402119617859267,
            "mae": 0.25313762226610287,
            "precision": 0.7457943925233644,
            "recall": 0.854389721627409
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 31658,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7866505069276335,
            "auditor_fn_violation": 0.012862945351057333,
            "auditor_fp_violation": 0.020291537667698662,
            "ave_precision_score": 0.7871333704097203,
            "fpr": 0.11513157894736842,
            "logloss": 1.5178127572056592,
            "mae": 0.33098876412925665,
            "precision": 0.7475961538461539,
            "recall": 0.6386036960985626
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.8113311045539854,
            "auditor_fn_violation": 0.00800823623709268,
            "auditor_fp_violation": 0.014354090643882086,
            "ave_precision_score": 0.8116234400511096,
            "fpr": 0.1163556531284303,
            "logloss": 1.5601896261625818,
            "mae": 0.30870635392771933,
            "precision": 0.7488151658767772,
            "recall": 0.6766595289079229
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.830277177267454,
            "auditor_fn_violation": 0.014168828127814411,
            "auditor_fp_violation": 0.017324561403508767,
            "ave_precision_score": 0.8303449211047427,
            "fpr": 0.13157894736842105,
            "logloss": 0.8952623728846498,
            "mae": 0.26922121179299013,
            "precision": 0.7585513078470825,
            "recall": 0.7741273100616016
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8252063873287321,
            "auditor_fn_violation": 0.00447304771329245,
            "auditor_fp_violation": 0.011248899832873492,
            "ave_precision_score": 0.8254958909315998,
            "fpr": 0.14270032930845225,
            "logloss": 0.8774974571046388,
            "mae": 0.2642029316778592,
            "precision": 0.7440944881889764,
            "recall": 0.8094218415417559
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 31658,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8364961737396369,
            "auditor_fn_violation": 0.013666738715371593,
            "auditor_fp_violation": 0.021989164086687312,
            "ave_precision_score": 0.8367635787378542,
            "fpr": 0.12719298245614036,
            "logloss": 0.7623564367476752,
            "mae": 0.27294750831683484,
            "precision": 0.7637474541751528,
            "recall": 0.7700205338809035
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8482561759058892,
            "auditor_fn_violation": 0.00716204749469369,
            "auditor_fp_violation": 0.011847192966841712,
            "ave_precision_score": 0.8485613134153807,
            "fpr": 0.14050493962678376,
            "logloss": 0.6889317735750651,
            "mae": 0.25648723121220257,
            "precision": 0.7509727626459144,
            "recall": 0.8265524625267666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8387147298291129,
            "auditor_fn_violation": 0.015503980690947081,
            "auditor_fp_violation": 0.021039731682146543,
            "ave_precision_score": 0.839015283896849,
            "fpr": 0.12609649122807018,
            "logloss": 0.7723630653997733,
            "mae": 0.26957190160225486,
            "precision": 0.764344262295082,
            "recall": 0.7659137577002053
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8516685198580688,
            "auditor_fn_violation": 0.0036691684080134112,
            "auditor_fp_violation": 0.008267323305742633,
            "ave_precision_score": 0.8519489783306792,
            "fpr": 0.13611416026344675,
            "logloss": 0.7003484079193493,
            "mae": 0.25499633072293654,
            "precision": 0.7559055118110236,
            "recall": 0.8222698072805139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8441985019591008,
            "auditor_fn_violation": 0.01256349292121474,
            "auditor_fp_violation": 0.020990712074303412,
            "ave_precision_score": 0.8446023038264582,
            "fpr": 0.17653508771929824,
            "logloss": 0.7261135256022113,
            "mae": 0.27162723447345405,
            "precision": 0.7195121951219512,
            "recall": 0.8480492813141683
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8498601488839751,
            "auditor_fn_violation": 0.004564718160385674,
            "auditor_fp_violation": 0.022339573382383487,
            "ave_precision_score": 0.851117719547785,
            "fpr": 0.19319429198682767,
            "logloss": 0.7180045829333412,
            "mae": 0.26925775212045766,
            "precision": 0.7027027027027027,
            "recall": 0.8907922912205567
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 31658,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8385058358050206,
            "auditor_fn_violation": 0.01110900969055081,
            "auditor_fp_violation": 0.02035087719298247,
            "ave_precision_score": 0.8387796306340596,
            "fpr": 0.13048245614035087,
            "logloss": 0.7356991563604219,
            "mae": 0.2723775567613724,
            "precision": 0.7605633802816901,
            "recall": 0.7761806981519507
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8465115279991989,
            "auditor_fn_violation": 0.006520354365041121,
            "auditor_fp_violation": 0.007767921598876595,
            "ave_precision_score": 0.8468646425281906,
            "fpr": 0.14270032930845225,
            "logloss": 0.6763725494841497,
            "mae": 0.2577580301740651,
            "precision": 0.7504798464491362,
            "recall": 0.8372591006423983
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 31658,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8356672355628708,
            "auditor_fn_violation": 0.012925987967866275,
            "auditor_fp_violation": 0.02238132094943241,
            "ave_precision_score": 0.8360152088687793,
            "fpr": 0.13706140350877194,
            "logloss": 0.7165670930902857,
            "mae": 0.27569332701586485,
            "precision": 0.7514910536779325,
            "recall": 0.7761806981519507
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8412468310830123,
            "auditor_fn_violation": 0.0047269043360121474,
            "auditor_fp_violation": 0.014574124069184447,
            "ave_precision_score": 0.8415556554976755,
            "fpr": 0.150384193194292,
            "logloss": 0.6613927482638166,
            "mae": 0.2640662368128271,
            "precision": 0.7390476190476191,
            "recall": 0.8308351177730193
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.855577391039614,
            "auditor_fn_violation": 0.013097103642062038,
            "auditor_fp_violation": 0.017079463364293095,
            "ave_precision_score": 0.8559528873346962,
            "fpr": 0.17214912280701755,
            "logloss": 0.667308057609011,
            "mae": 0.2699623885866564,
            "precision": 0.7235915492957746,
            "recall": 0.8439425051334702
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.858698815659781,
            "auditor_fn_violation": 0.006818870949165213,
            "auditor_fp_violation": 0.01710327231732282,
            "ave_precision_score": 0.859229644847934,
            "fpr": 0.18221734357848518,
            "logloss": 0.6642050547270336,
            "mae": 0.26443238487173293,
            "precision": 0.71280276816609,
            "recall": 0.8822269807280514
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8456893337435508,
            "auditor_fn_violation": 0.013063330811628662,
            "auditor_fp_violation": 0.01737100103199175,
            "ave_precision_score": 0.8460074056587815,
            "fpr": 0.1787280701754386,
            "logloss": 0.7395581511207427,
            "mae": 0.2705376814813765,
            "precision": 0.7170138888888888,
            "recall": 0.8480492813141683
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8516468755203401,
            "auditor_fn_violation": 0.005070080881540629,
            "auditor_fp_violation": 0.015760821194410666,
            "ave_precision_score": 0.8528978500349271,
            "fpr": 0.18990120746432493,
            "logloss": 0.726764359760575,
            "mae": 0.2653953559681276,
            "precision": 0.70578231292517,
            "recall": 0.8886509635974305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8468010684497639,
            "auditor_fn_violation": 0.010595662667963544,
            "auditor_fp_violation": 0.02156346749226007,
            "ave_precision_score": 0.8470629671136173,
            "fpr": 0.17982456140350878,
            "logloss": 0.7274828466582113,
            "mae": 0.26973609910616286,
            "precision": 0.7152777777777778,
            "recall": 0.8459958932238193
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8498186750409134,
            "auditor_fn_violation": 0.004564718160385674,
            "auditor_fp_violation": 0.016517340611742367,
            "ave_precision_score": 0.8510801654084801,
            "fpr": 0.1942919868276619,
            "logloss": 0.7193833349296548,
            "mae": 0.2665960743696087,
            "precision": 0.7015177065767285,
            "recall": 0.8907922912205567
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8430910673099976,
            "auditor_fn_violation": 0.014758726899383984,
            "auditor_fp_violation": 0.009347265221878238,
            "ave_precision_score": 0.8433089523816835,
            "fpr": 0.14035087719298245,
            "logloss": 0.5869210284557013,
            "mae": 0.3072381420162807,
            "precision": 0.7470355731225297,
            "recall": 0.7761806981519507
        },
        "train": {
            "accuracy": 0.7925356750823271,
            "auc_prc": 0.8743422760114333,
            "auditor_fn_violation": 0.007324233670320167,
            "auditor_fp_violation": 0.005654117344567397,
            "ave_precision_score": 0.8756009134293008,
            "fpr": 0.13721185510428102,
            "logloss": 0.47995963403621794,
            "mae": 0.271553085381835,
            "precision": 0.7632575757575758,
            "recall": 0.8629550321199143
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 31658,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8450692740028366,
            "auditor_fn_violation": 0.01268507511077489,
            "auditor_fp_violation": 0.018196594427244576,
            "ave_precision_score": 0.8454351441513042,
            "fpr": 0.16337719298245615,
            "logloss": 0.7294546749714972,
            "mae": 0.2688842765860021,
            "precision": 0.7295825771324864,
            "recall": 0.8254620123203286
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8527928930204318,
            "auditor_fn_violation": 0.005446164767051292,
            "auditor_fp_violation": 0.01717249631629435,
            "ave_precision_score": 0.8535071374738534,
            "fpr": 0.1734357848518112,
            "logloss": 0.7088448714856806,
            "mae": 0.2625973731198647,
            "precision": 0.7198581560283688,
            "recall": 0.8693790149892934
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8410199555869957,
            "auditor_fn_violation": 0.01329073453654671,
            "auditor_fp_violation": 0.017479360165118685,
            "ave_precision_score": 0.841574007855901,
            "fpr": 0.12609649122807018,
            "logloss": 0.7201074951941968,
            "mae": 0.2673967378930777,
            "precision": 0.7681451612903226,
            "recall": 0.7823408624229979
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8519953835885571,
            "auditor_fn_violation": 0.0057305782054687315,
            "auditor_fp_violation": 0.01601546661919879,
            "ave_precision_score": 0.8522629486553488,
            "fpr": 0.132821075740944,
            "logloss": 0.6713753267653959,
            "mae": 0.25831283676277844,
            "precision": 0.7575150300601202,
            "recall": 0.8094218415417559
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8461927493557173,
            "auditor_fn_violation": 0.011390449944162257,
            "auditor_fp_violation": 0.01942982456140352,
            "ave_precision_score": 0.8465656316185384,
            "fpr": 0.13048245614035087,
            "logloss": 0.7174566441207244,
            "mae": 0.26463583728069245,
            "precision": 0.7638888888888888,
            "recall": 0.7905544147843943
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8588723969370489,
            "auditor_fn_violation": 0.008179824509856928,
            "auditor_fp_violation": 0.015308392915418164,
            "ave_precision_score": 0.8591284146916918,
            "fpr": 0.14489571899012074,
            "logloss": 0.662650095753736,
            "mae": 0.2534120605558297,
            "precision": 0.7476099426386233,
            "recall": 0.8372591006423983
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8490662584052844,
            "auditor_fn_violation": 0.01362846284088044,
            "auditor_fp_violation": 0.019143446852425184,
            "ave_precision_score": 0.8494283215469671,
            "fpr": 0.12609649122807018,
            "logloss": 0.6950576216173213,
            "mae": 0.2649220740775375,
            "precision": 0.7672064777327935,
            "recall": 0.7782340862422998
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8645871854928608,
            "auditor_fn_violation": 0.006367570286552416,
            "auditor_fp_violation": 0.013676684368232118,
            "ave_precision_score": 0.8647995993498585,
            "fpr": 0.14050493962678376,
            "logloss": 0.6317351650346403,
            "mae": 0.249896695139491,
            "precision": 0.7533718689788054,
            "recall": 0.8372591006423983
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8457624893934748,
            "auditor_fn_violation": 0.013063330811628662,
            "auditor_fp_violation": 0.01737100103199175,
            "ave_precision_score": 0.8460831865602347,
            "fpr": 0.1787280701754386,
            "logloss": 0.7388546155076774,
            "mae": 0.2706435377135088,
            "precision": 0.7170138888888888,
            "recall": 0.8480492813141683
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8516604379599979,
            "auditor_fn_violation": 0.005070080881540629,
            "auditor_fp_violation": 0.015760821194410666,
            "ave_precision_score": 0.8529150474810632,
            "fpr": 0.18990120746432493,
            "logloss": 0.7261681705997852,
            "mae": 0.26546607741590944,
            "precision": 0.70578231292517,
            "recall": 0.8886509635974305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8346249276637439,
            "auditor_fn_violation": 0.011248604056342088,
            "auditor_fp_violation": 0.013588751289989692,
            "ave_precision_score": 0.834868437335572,
            "fpr": 0.1962719298245614,
            "logloss": 0.8752050658025918,
            "mae": 0.2790903718076466,
            "precision": 0.7006688963210702,
            "recall": 0.8603696098562629
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8383475213586804,
            "auditor_fn_violation": 0.00625709564518366,
            "auditor_fp_violation": 0.024564630492182635,
            "ave_precision_score": 0.8386498067677792,
            "fpr": 0.21953896816684962,
            "logloss": 0.9013142814713241,
            "mae": 0.28067259186452137,
            "precision": 0.6763754045307443,
            "recall": 0.8950749464668094
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 31658,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8365057163524001,
            "auditor_fn_violation": 0.017764508807954174,
            "auditor_fp_violation": 0.018359133126934987,
            "ave_precision_score": 0.8369747402222236,
            "fpr": 0.10635964912280702,
            "logloss": 0.6998421924154092,
            "mae": 0.28465101317497704,
            "precision": 0.7858719646799117,
            "recall": 0.731006160164271
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8435991719802134,
            "auditor_fn_violation": 0.008518300006816523,
            "auditor_fp_violation": 0.01981784199127778,
            "ave_precision_score": 0.8438433479704378,
            "fpr": 0.1207464324917673,
            "logloss": 0.6255511208871803,
            "mae": 0.2698659705879813,
            "precision": 0.7654584221748401,
            "recall": 0.7687366167023555
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8454337310055026,
            "auditor_fn_violation": 0.012761626859757198,
            "auditor_fp_violation": 0.015451496388028908,
            "ave_precision_score": 0.845799079777942,
            "fpr": 0.1524122807017544,
            "logloss": 0.8376806611455181,
            "mae": 0.27027870119382963,
            "precision": 0.7337164750957854,
            "recall": 0.7864476386036962
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8499781817819928,
            "auditor_fn_violation": 0.010694885494209484,
            "auditor_fp_violation": 0.020411190553890884,
            "ave_precision_score": 0.8505286589183652,
            "fpr": 0.17014270032930845,
            "logloss": 0.802460806149644,
            "mae": 0.26839601343587094,
            "precision": 0.712430426716141,
            "recall": 0.8222698072805139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8476623735544114,
            "auditor_fn_violation": 0.012545480744983609,
            "auditor_fp_violation": 0.017636738906088755,
            "ave_precision_score": 0.8479248424075188,
            "fpr": 0.11074561403508772,
            "logloss": 0.72054789132904,
            "mae": 0.2665027748405549,
            "precision": 0.7846481876332623,
            "recall": 0.75564681724846
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.860950656411762,
            "auditor_fn_violation": 0.0063393639951391184,
            "auditor_fp_violation": 0.009790251283116268,
            "ave_precision_score": 0.8612370379069141,
            "fpr": 0.12403951701427003,
            "logloss": 0.6373683827772112,
            "mae": 0.2489990941087819,
            "precision": 0.7674897119341564,
            "recall": 0.7987152034261242
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.840011827103744,
            "auditor_fn_violation": 0.011960085017471813,
            "auditor_fp_violation": 0.019711042311661507,
            "ave_precision_score": 0.840420210375405,
            "fpr": 0.1699561403508772,
            "logloss": 0.7605204144406001,
            "mae": 0.26854147010664675,
            "precision": 0.7256637168141593,
            "recall": 0.8418891170431212
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8443599673450058,
            "auditor_fn_violation": 0.006793015182036356,
            "auditor_fp_violation": 0.015975910048357914,
            "ave_precision_score": 0.845625567974769,
            "fpr": 0.18660812294182216,
            "logloss": 0.7564696856034021,
            "mae": 0.26732593414928174,
            "precision": 0.708904109589041,
            "recall": 0.8865096359743041
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8338258807394668,
            "auditor_fn_violation": 0.012200997874563206,
            "auditor_fp_violation": 0.020892672858617137,
            "ave_precision_score": 0.8340950035066019,
            "fpr": 0.17434210526315788,
            "logloss": 0.8001156106901581,
            "mae": 0.2793786359418603,
            "precision": 0.7200704225352113,
            "recall": 0.839835728952772
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8341442273137329,
            "auditor_fn_violation": 0.0040946133034973475,
            "auditor_fp_violation": 0.018008128875307807,
            "ave_precision_score": 0.8354395687555973,
            "fpr": 0.19209659714599342,
            "logloss": 0.7890810476475096,
            "mae": 0.2718413802864367,
            "precision": 0.7033898305084746,
            "recall": 0.8886509635974305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8471167212210455,
            "auditor_fn_violation": 0.012448665297741275,
            "auditor_fp_violation": 0.017352941176470585,
            "ave_precision_score": 0.8473666689120383,
            "fpr": 0.17653508771929824,
            "logloss": 0.7419627135361051,
            "mae": 0.26988449268575704,
            "precision": 0.7190226876090751,
            "recall": 0.8459958932238193
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8511984258617697,
            "auditor_fn_violation": 0.007021016037627192,
            "auditor_fp_violation": 0.016101996617913204,
            "ave_precision_score": 0.8524542028051156,
            "fpr": 0.18990120746432493,
            "logloss": 0.7286238158685706,
            "mae": 0.2648259285663255,
            "precision": 0.70578231292517,
            "recall": 0.8886509635974305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8486834692653793,
            "auditor_fn_violation": 0.01048984113260564,
            "auditor_fp_violation": 0.01932662538699691,
            "ave_precision_score": 0.8489764376926656,
            "fpr": 0.1337719298245614,
            "logloss": 0.7067432984819491,
            "mae": 0.26445357150483584,
            "precision": 0.758893280632411,
            "recall": 0.7885010266940452
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8582541043275687,
            "auditor_fn_violation": 0.005841052846837488,
            "auditor_fp_violation": 0.00864805530008604,
            "ave_precision_score": 0.8585628345554116,
            "fpr": 0.14270032930845225,
            "logloss": 0.6528520273629191,
            "mae": 0.25122047295695427,
            "precision": 0.7533206831119544,
            "recall": 0.8501070663811563
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8264318573340046,
            "auditor_fn_violation": 0.009323552721639825,
            "auditor_fp_violation": 0.020768833849329208,
            "ave_precision_score": 0.8269081775369103,
            "fpr": 0.13706140350877194,
            "logloss": 0.7902092728507008,
            "mae": 0.2804645303724504,
            "precision": 0.750996015936255,
            "recall": 0.7741273100616016
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.827250478639763,
            "auditor_fn_violation": 0.008706341949571855,
            "auditor_fp_violation": 0.019953817703543285,
            "ave_precision_score": 0.8276036064946257,
            "fpr": 0.15697036223929747,
            "logloss": 0.7677883333915081,
            "mae": 0.27361612978987665,
            "precision": 0.72552783109405,
            "recall": 0.8094218415417559
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8438122896792168,
            "auditor_fn_violation": 0.011800226953420515,
            "auditor_fp_violation": 0.018482972136222915,
            "ave_precision_score": 0.8440971435148712,
            "fpr": 0.1787280701754386,
            "logloss": 0.7451842975840283,
            "mae": 0.2700409489654255,
            "precision": 0.7170138888888888,
            "recall": 0.8480492813141683
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8504740719696053,
            "auditor_fn_violation": 0.004564718160385674,
            "auditor_fp_violation": 0.016101996617913204,
            "ave_precision_score": 0.8517275574063143,
            "fpr": 0.18990120746432493,
            "logloss": 0.7325088738284629,
            "mae": 0.26584162474055995,
            "precision": 0.7062818336162988,
            "recall": 0.8907922912205567
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.846398474665454,
            "auditor_fn_violation": 0.012448665297741275,
            "auditor_fp_violation": 0.016754385964912293,
            "ave_precision_score": 0.8466502916741406,
            "fpr": 0.17653508771929824,
            "logloss": 0.7469005286620866,
            "mae": 0.27071469012158156,
            "precision": 0.7190226876090751,
            "recall": 0.8459958932238193
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8514756733411694,
            "auditor_fn_violation": 0.005920970672508505,
            "auditor_fp_violation": 0.014171141503743047,
            "ave_precision_score": 0.8527569062179174,
            "fpr": 0.18880351262349068,
            "logloss": 0.7308535057081537,
            "mae": 0.2653282815951126,
            "precision": 0.7064846416382252,
            "recall": 0.8865096359743041
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8463710830691955,
            "auditor_fn_violation": 0.011752944990813792,
            "auditor_fp_violation": 0.01924406604747162,
            "ave_precision_score": 0.8468143980056233,
            "fpr": 0.17214912280701755,
            "logloss": 0.7279464932048078,
            "mae": 0.27041055578543566,
            "precision": 0.7221238938053097,
            "recall": 0.837782340862423
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8498607867996306,
            "auditor_fn_violation": 0.00754518295305768,
            "auditor_fp_violation": 0.016999436318865524,
            "ave_precision_score": 0.8511175828750939,
            "fpr": 0.18441273326015367,
            "logloss": 0.7196425491652543,
            "mae": 0.26502349107183265,
            "precision": 0.7108433734939759,
            "recall": 0.8843683083511777
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 31658,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8547467733147487,
            "auditor_fn_violation": 0.01089511509780612,
            "auditor_fp_violation": 0.018286893704850368,
            "ave_precision_score": 0.8549674110528638,
            "fpr": 0.13267543859649122,
            "logloss": 0.7076848335433885,
            "mae": 0.263193600266695,
            "precision": 0.7613412228796844,
            "recall": 0.7926078028747433
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8587055255938764,
            "auditor_fn_violation": 0.009507870730566454,
            "auditor_fp_violation": 0.013380010086925567,
            "ave_precision_score": 0.8589322945027231,
            "fpr": 0.14050493962678376,
            "logloss": 0.6683522785767863,
            "mae": 0.2521808362595023,
            "precision": 0.752895752895753,
            "recall": 0.8351177730192719
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8413383274609496,
            "auditor_fn_violation": 0.015294589142260165,
            "auditor_fp_violation": 0.017608359133126937,
            "ave_precision_score": 0.8417291157382498,
            "fpr": 0.13706140350877194,
            "logloss": 0.702283409244824,
            "mae": 0.26738299606993654,
            "precision": 0.7549019607843137,
            "recall": 0.7905544147843943
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8517407328972801,
            "auditor_fn_violation": 0.005624804612668858,
            "auditor_fp_violation": 0.015664402052986033,
            "ave_precision_score": 0.8522855201455402,
            "fpr": 0.15367727771679474,
            "logloss": 0.6543362450586231,
            "mae": 0.25849454407092964,
            "precision": 0.7358490566037735,
            "recall": 0.8351177730192719
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 31658,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.7539745310719689,
            "auditor_fn_violation": 0.00957797471090457,
            "auditor_fp_violation": 0.020931372549019604,
            "ave_precision_score": 0.7477644111686071,
            "fpr": 0.28728070175438597,
            "logloss": 1.8577908733804342,
            "mae": 0.3231921136317983,
            "precision": 0.6376210235131397,
            "recall": 0.946611909650924
        },
        "train": {
            "accuracy": 0.6520307354555434,
            "auc_prc": 0.7185111710308966,
            "auditor_fn_violation": 0.0025174115086370017,
            "auditor_fp_violation": 0.02540273533687366,
            "ave_precision_score": 0.7159314127869799,
            "fpr": 0.3260153677277717,
            "logloss": 2.2969169580254687,
            "mae": 0.34615727812848873,
            "precision": 0.6008064516129032,
            "recall": 0.9571734475374732
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8295155110483057,
            "auditor_fn_violation": 0.011527792787924639,
            "auditor_fp_violation": 0.021101651186790516,
            "ave_precision_score": 0.8298459254150656,
            "fpr": 0.18421052631578946,
            "logloss": 0.8342082565638166,
            "mae": 0.28311049384549125,
            "precision": 0.7068062827225131,
            "recall": 0.8316221765913757
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8226323657480707,
            "auditor_fn_violation": 0.006292353509450285,
            "auditor_fp_violation": 0.024245705639778094,
            "ave_precision_score": 0.8230966240903088,
            "fpr": 0.20087815587266739,
            "logloss": 0.8533262733226171,
            "mae": 0.28040420569991714,
            "precision": 0.6903553299492385,
            "recall": 0.8736616702355461
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8457510196844915,
            "auditor_fn_violation": 0.013063330811628662,
            "auditor_fp_violation": 0.01737100103199175,
            "ave_precision_score": 0.8460603257067244,
            "fpr": 0.1787280701754386,
            "logloss": 0.7388784991398397,
            "mae": 0.27063506037337537,
            "precision": 0.7170138888888888,
            "recall": 0.8480492813141683
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8516053953769307,
            "auditor_fn_violation": 0.005070080881540629,
            "auditor_fp_violation": 0.015760821194410666,
            "ave_precision_score": 0.8528406997059983,
            "fpr": 0.18990120746432493,
            "logloss": 0.726279306767123,
            "mae": 0.265488777108536,
            "precision": 0.70578231292517,
            "recall": 0.8886509635974305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8470981807340439,
            "auditor_fn_violation": 0.009559962534673443,
            "auditor_fp_violation": 0.015046439628482977,
            "ave_precision_score": 0.8473410817391402,
            "fpr": 0.15460526315789475,
            "logloss": 0.7095369223737891,
            "mae": 0.2656282153020053,
            "precision": 0.738404452690167,
            "recall": 0.8172484599589322
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8502904052298805,
            "auditor_fn_violation": 0.007314831573182399,
            "auditor_fp_violation": 0.018660812294182216,
            "ave_precision_score": 0.8515517014062031,
            "fpr": 0.1712403951701427,
            "logloss": 0.6936823013422869,
            "mae": 0.25999301095443406,
            "precision": 0.7238938053097345,
            "recall": 0.8758029978586723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8417318497731936,
            "auditor_fn_violation": 0.01139720451024893,
            "auditor_fp_violation": 0.020939112487100106,
            "ave_precision_score": 0.8420491128000637,
            "fpr": 0.17763157894736842,
            "logloss": 0.7631564792368944,
            "mae": 0.27114231323601123,
            "precision": 0.7157894736842105,
            "recall": 0.837782340862423
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8460800943262963,
            "auditor_fn_violation": 0.004379026741914785,
            "auditor_fp_violation": 0.0192665222851831,
            "ave_precision_score": 0.847345469495302,
            "fpr": 0.18551042810098792,
            "logloss": 0.7488738455109177,
            "mae": 0.2642362353740978,
            "precision": 0.7096219931271478,
            "recall": 0.8843683083511777
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8443321963480029,
            "auditor_fn_violation": 0.009870672574660474,
            "auditor_fp_violation": 0.01737100103199175,
            "ave_precision_score": 0.8446332892337542,
            "fpr": 0.1787280701754386,
            "logloss": 0.7460421918452752,
            "mae": 0.27069420854692744,
            "precision": 0.7170138888888888,
            "recall": 0.8480492813141683
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8502402716787753,
            "auditor_fn_violation": 0.005920970672508505,
            "auditor_fp_violation": 0.01582015605067198,
            "ave_precision_score": 0.8514978124278332,
            "fpr": 0.19209659714599342,
            "logloss": 0.733530271742153,
            "mae": 0.265765898454079,
            "precision": 0.7028862478777589,
            "recall": 0.8865096359743041
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8388512409021184,
            "auditor_fn_violation": 0.010093573255520736,
            "auditor_fp_violation": 0.019453044375644997,
            "ave_precision_score": 0.8390920655134746,
            "fpr": 0.17543859649122806,
            "logloss": 0.778090718551745,
            "mae": 0.27850228714339686,
            "precision": 0.7183098591549296,
            "recall": 0.837782340862423
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8331090384314993,
            "auditor_fn_violation": 0.0022635548859173053,
            "auditor_fp_violation": 0.01619099890230517,
            "ave_precision_score": 0.8344099449365003,
            "fpr": 0.2030735455543359,
            "logloss": 0.7866464257548782,
            "mae": 0.2736569702976501,
            "precision": 0.6916666666666667,
            "recall": 0.8886509635974305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.8086426442683871,
            "auditor_fn_violation": 0.007592132281422243,
            "auditor_fp_violation": 0.016558307533539735,
            "ave_precision_score": 0.8089821854657165,
            "fpr": 0.09978070175438597,
            "logloss": 1.4930810382651771,
            "mae": 0.31786384149253,
            "precision": 0.7775061124694377,
            "recall": 0.6529774127310062
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8273275429835824,
            "auditor_fn_violation": 0.0072137590289514135,
            "auditor_fp_violation": 0.009567745572136353,
            "ave_precision_score": 0.8275765605565263,
            "fpr": 0.10757409440175632,
            "logloss": 1.546949346994537,
            "mae": 0.29782011769791106,
            "precision": 0.7683215130023641,
            "recall": 0.69593147751606
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8415537217665908,
            "auditor_fn_violation": 0.011104506646493033,
            "auditor_fp_violation": 0.019143446852425187,
            "ave_precision_score": 0.8421055991315938,
            "fpr": 0.1787280701754386,
            "logloss": 0.738010970157044,
            "mae": 0.273782446604346,
            "precision": 0.715034965034965,
            "recall": 0.839835728952772
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8473791869920468,
            "auditor_fn_violation": 0.007655657594426437,
            "auditor_fp_violation": 0.02326668051146646,
            "ave_precision_score": 0.8481111995266095,
            "fpr": 0.1964873765093304,
            "logloss": 0.7387714203995114,
            "mae": 0.2720447205602671,
            "precision": 0.6971235194585449,
            "recall": 0.8822269807280514
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8456802912081353,
            "auditor_fn_violation": 0.013063330811628662,
            "auditor_fp_violation": 0.01737100103199175,
            "ave_precision_score": 0.846003119274124,
            "fpr": 0.1787280701754386,
            "logloss": 0.7395561647046938,
            "mae": 0.27053728965226925,
            "precision": 0.7170138888888888,
            "recall": 0.8480492813141683
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8516499443007333,
            "auditor_fn_violation": 0.005070080881540629,
            "auditor_fp_violation": 0.015760821194410666,
            "ave_precision_score": 0.8529009168582042,
            "fpr": 0.18990120746432493,
            "logloss": 0.726763403951861,
            "mae": 0.2653956331807951,
            "precision": 0.70578231292517,
            "recall": 0.8886509635974305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8596440869755514,
            "auditor_fn_violation": 0.010721747901581473,
            "auditor_fp_violation": 0.016715686274509808,
            "ave_precision_score": 0.8598744759273339,
            "fpr": 0.12938596491228072,
            "logloss": 0.6555774753639809,
            "mae": 0.26115227410734176,
            "precision": 0.7649402390438247,
            "recall": 0.7885010266940452
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8652329355652837,
            "auditor_fn_violation": 0.00605730108100612,
            "auditor_fp_violation": 0.013565431512742167,
            "ave_precision_score": 0.8654486882074146,
            "fpr": 0.1394072447859495,
            "logloss": 0.6150034535173664,
            "mae": 0.25134188108086764,
            "precision": 0.7543520309477756,
            "recall": 0.8351177730192719
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8418038856366445,
            "auditor_fn_violation": 0.01139720451024893,
            "auditor_fp_violation": 0.019458204334365327,
            "ave_precision_score": 0.8421208710290441,
            "fpr": 0.1787280701754386,
            "logloss": 0.7622667441682638,
            "mae": 0.271155083364375,
            "precision": 0.7145359019264448,
            "recall": 0.837782340862423
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8460352813837237,
            "auditor_fn_violation": 0.004379026741914785,
            "auditor_fp_violation": 0.018868484291096808,
            "ave_precision_score": 0.847300799053778,
            "fpr": 0.18441273326015367,
            "logloss": 0.7484543761224158,
            "mae": 0.2642739523447876,
            "precision": 0.7108433734939759,
            "recall": 0.8843683083511777
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8457589092214834,
            "auditor_fn_violation": 0.013063330811628662,
            "auditor_fp_violation": 0.01737100103199175,
            "ave_precision_score": 0.8460887799212882,
            "fpr": 0.1787280701754386,
            "logloss": 0.738856194716401,
            "mae": 0.27064010296371366,
            "precision": 0.7170138888888888,
            "recall": 0.8480492813141683
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8516556828508224,
            "auditor_fn_violation": 0.005070080881540629,
            "auditor_fp_violation": 0.015760821194410666,
            "ave_precision_score": 0.8529068698952634,
            "fpr": 0.18990120746432493,
            "logloss": 0.7261906802204496,
            "mae": 0.2654653889112564,
            "precision": 0.70578231292517,
            "recall": 0.8886509635974305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8454311272758535,
            "auditor_fn_violation": 0.010026027594653987,
            "auditor_fp_violation": 0.019953560371517035,
            "ave_precision_score": 0.8459657635423143,
            "fpr": 0.14035087719298245,
            "logloss": 0.697710215874013,
            "mae": 0.26698498031438245,
            "precision": 0.7509727626459144,
            "recall": 0.7926078028747433
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.855597278401709,
            "auditor_fn_violation": 0.013209946478562046,
            "auditor_fp_violation": 0.010272346990239422,
            "ave_precision_score": 0.8558611385907064,
            "fpr": 0.14818880351262348,
            "logloss": 0.6545174771891135,
            "mae": 0.2567442585405068,
            "precision": 0.7428571428571429,
            "recall": 0.8351177730192719
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8457941424239811,
            "auditor_fn_violation": 0.013063330811628662,
            "auditor_fp_violation": 0.01737100103199175,
            "ave_precision_score": 0.8461034842131372,
            "fpr": 0.1787280701754386,
            "logloss": 0.7388663370655089,
            "mae": 0.27063981952494837,
            "precision": 0.7170138888888888,
            "recall": 0.8480492813141683
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8517079966870846,
            "auditor_fn_violation": 0.005070080881540629,
            "auditor_fp_violation": 0.015760821194410666,
            "ave_precision_score": 0.8529427441616236,
            "fpr": 0.18990120746432493,
            "logloss": 0.7261709384350641,
            "mae": 0.26545964157492047,
            "precision": 0.70578231292517,
            "recall": 0.8886509635974305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8566091395808462,
            "auditor_fn_violation": 0.011948827407327355,
            "auditor_fp_violation": 0.018857069143446856,
            "ave_precision_score": 0.8568215480041049,
            "fpr": 0.14035087719298245,
            "logloss": 0.6730249733353738,
            "mae": 0.2615814403178672,
            "precision": 0.7547892720306514,
            "recall": 0.8090349075975359
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8550262168556741,
            "auditor_fn_violation": 0.00331188871677828,
            "auditor_fp_violation": 0.016860988320922463,
            "ave_precision_score": 0.8562721115560363,
            "fpr": 0.15587266739846323,
            "logloss": 0.6497547016976726,
            "mae": 0.2548217348482056,
            "precision": 0.7384898710865562,
            "recall": 0.8586723768736617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8378897056619841,
            "auditor_fn_violation": 0.012059151986743042,
            "auditor_fp_violation": 0.018335913312693502,
            "ave_precision_score": 0.8383021945487024,
            "fpr": 0.15789473684210525,
            "logloss": 0.8182251549547288,
            "mae": 0.26971918827092245,
            "precision": 0.7323420074349443,
            "recall": 0.8090349075975359
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8408648055926236,
            "auditor_fn_violation": 0.006609674287849905,
            "auditor_fp_violation": 0.01611930261765608,
            "ave_precision_score": 0.8410733935083915,
            "fpr": 0.1668496158068057,
            "logloss": 0.7988378495762989,
            "mae": 0.2641436990629452,
            "precision": 0.7231329690346083,
            "recall": 0.8501070663811563
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8422842043433393,
            "auditor_fn_violation": 0.013299740624662276,
            "auditor_fp_violation": 0.022652218782249753,
            "ave_precision_score": 0.8428473599957523,
            "fpr": 0.17214912280701755,
            "logloss": 0.6077622529084012,
            "mae": 0.2881697401105971,
            "precision": 0.7235915492957746,
            "recall": 0.8439425051334702
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8835377536711018,
            "auditor_fn_violation": 0.0041298711677639705,
            "auditor_fp_violation": 0.019276411427893315,
            "ave_precision_score": 0.8838037147046344,
            "fpr": 0.17453347969264543,
            "logloss": 0.5280656486175103,
            "mae": 0.26524107647198686,
            "precision": 0.7258620689655172,
            "recall": 0.9014989293361885
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.844551284455739,
            "auditor_fn_violation": 0.012108685471378652,
            "auditor_fp_violation": 0.018482972136222915,
            "ave_precision_score": 0.8450819778853538,
            "fpr": 0.1787280701754386,
            "logloss": 0.7115450169785685,
            "mae": 0.2735371716880075,
            "precision": 0.7145359019264448,
            "recall": 0.837782340862423
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8540265455082788,
            "auditor_fn_violation": 0.004564718160385674,
            "auditor_fp_violation": 0.018275135728483702,
            "ave_precision_score": 0.8552896493104181,
            "fpr": 0.18551042810098792,
            "logloss": 0.6851721064069869,
            "mae": 0.2641110064594553,
            "precision": 0.7111111111111111,
            "recall": 0.8907922912205567
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8314500067544626,
            "auditor_fn_violation": 0.017338971144493683,
            "auditor_fp_violation": 0.015474716202270382,
            "ave_precision_score": 0.8319366495354974,
            "fpr": 0.08662280701754387,
            "logloss": 0.8548304454582771,
            "mae": 0.2821833276325083,
            "precision": 0.8068459657701712,
            "recall": 0.6776180698151951
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8400114447291085,
            "auditor_fn_violation": 0.010076697607401335,
            "auditor_fp_violation": 0.01305366837748836,
            "ave_precision_score": 0.840314730717982,
            "fpr": 0.09989023051591657,
            "logloss": 0.7490578363823659,
            "mae": 0.26649318251353343,
            "precision": 0.7838479809976246,
            "recall": 0.7066381156316917
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8404838229019074,
            "auditor_fn_violation": 0.011430977340682304,
            "auditor_fp_violation": 0.02076883384932921,
            "ave_precision_score": 0.8407837046497371,
            "fpr": 0.16447368421052633,
            "logloss": 0.7633118486336684,
            "mae": 0.2666267864499448,
            "precision": 0.7292418772563177,
            "recall": 0.8295687885010267
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8427119482636902,
            "auditor_fn_violation": 0.008753352435260688,
            "auditor_fp_violation": 0.017503782597086664,
            "ave_precision_score": 0.8439825873342948,
            "fpr": 0.1778265642151482,
            "logloss": 0.7499173407041101,
            "mae": 0.2649374727120511,
            "precision": 0.7157894736842105,
            "recall": 0.8736616702355461
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 31658,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8509410160459012,
            "auditor_fn_violation": 0.009942721279584999,
            "auditor_fp_violation": 0.020892672858617133,
            "ave_precision_score": 0.8511898927604258,
            "fpr": 0.12719298245614036,
            "logloss": 0.6800036996838829,
            "mae": 0.264933144428881,
            "precision": 0.7665995975855131,
            "recall": 0.7823408624229979
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8687637342851757,
            "auditor_fn_violation": 0.006597921666427698,
            "auditor_fp_violation": 0.008806281583449534,
            "ave_precision_score": 0.869001835887068,
            "fpr": 0.14270032930845225,
            "logloss": 0.6065202626072377,
            "mae": 0.24731640457964463,
            "precision": 0.7523809523809524,
            "recall": 0.8458244111349036
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8470906471794415,
            "auditor_fn_violation": 0.011658381065600349,
            "auditor_fp_violation": 0.019138286893704858,
            "ave_precision_score": 0.8474380803789758,
            "fpr": 0.17982456140350878,
            "logloss": 0.7315887655498782,
            "mae": 0.2696297644620599,
            "precision": 0.7147826086956521,
            "recall": 0.8439425051334702
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8516754032716769,
            "auditor_fn_violation": 0.005070080881540629,
            "auditor_fp_violation": 0.013936274364375362,
            "ave_precision_score": 0.8529269152524119,
            "fpr": 0.18990120746432493,
            "logloss": 0.7197355871330491,
            "mae": 0.2655198056604681,
            "precision": 0.70578231292517,
            "recall": 0.8886509635974305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8451913580341376,
            "auditor_fn_violation": 0.01275487229367052,
            "auditor_fp_violation": 0.018178534571723436,
            "ave_precision_score": 0.8454838231946242,
            "fpr": 0.17982456140350878,
            "logloss": 0.7459252609394745,
            "mae": 0.2707310340026016,
            "precision": 0.7147826086956521,
            "recall": 0.8439425051334702
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8473932371443293,
            "auditor_fn_violation": 0.005819898128277516,
            "auditor_fp_violation": 0.021118264257671514,
            "ave_precision_score": 0.8486559535602202,
            "fpr": 0.18660812294182216,
            "logloss": 0.7387986557506442,
            "mae": 0.2652833752457013,
            "precision": 0.708904109589041,
            "recall": 0.8865096359743041
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8431284772917905,
            "auditor_fn_violation": 0.012696332720919343,
            "auditor_fp_violation": 0.02213622291021672,
            "ave_precision_score": 0.8435844148080771,
            "fpr": 0.18092105263157895,
            "logloss": 0.7322045872769783,
            "mae": 0.2718632125977699,
            "precision": 0.7155172413793104,
            "recall": 0.8521560574948666
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8491222704881117,
            "auditor_fn_violation": 0.004564718160385674,
            "auditor_fp_violation": 0.022772223375955542,
            "ave_precision_score": 0.8503817382650067,
            "fpr": 0.1964873765093304,
            "logloss": 0.7259257610222154,
            "mae": 0.26944485017918235,
            "precision": 0.6991596638655462,
            "recall": 0.8907922912205567
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8329686457324766,
            "auditor_fn_violation": 0.01339655607190461,
            "auditor_fp_violation": 0.021687306501547987,
            "ave_precision_score": 0.8333568934864258,
            "fpr": 0.18421052631578946,
            "logloss": 0.7894890124993865,
            "mae": 0.28160090794594317,
            "precision": 0.7108433734939759,
            "recall": 0.8480492813141683
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8349833680415899,
            "auditor_fn_violation": 0.008250340238390172,
            "auditor_fp_violation": 0.023983643357957298,
            "ave_precision_score": 0.8362853257092322,
            "fpr": 0.21185510428100987,
            "logloss": 0.7959962219243197,
            "mae": 0.2795803664241716,
            "precision": 0.6836065573770492,
            "recall": 0.892933618843683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8511560062636816,
            "auditor_fn_violation": 0.014186840304045534,
            "auditor_fp_violation": 0.01977038183694531,
            "ave_precision_score": 0.8515452679711186,
            "fpr": 0.13267543859649122,
            "logloss": 0.683814075820092,
            "mae": 0.2635547998143248,
            "precision": 0.7594433399602386,
            "recall": 0.784394250513347
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8653995390170381,
            "auditor_fn_violation": 0.008149267694159187,
            "auditor_fp_violation": 0.015016663205466727,
            "ave_precision_score": 0.8656082959619401,
            "fpr": 0.1437980241492865,
            "logloss": 0.6271049369428331,
            "mae": 0.24886020198089714,
            "precision": 0.7504761904761905,
            "recall": 0.8436830835117773
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 31658,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8329943683192791,
            "auditor_fn_violation": 0.011590835404733602,
            "auditor_fp_violation": 0.02517543859649123,
            "ave_precision_score": 0.8334247160249757,
            "fpr": 0.14912280701754385,
            "logloss": 0.8118254729574398,
            "mae": 0.2700356539323488,
            "precision": 0.7409523809523809,
            "recall": 0.7987679671457906
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.835514463312254,
            "auditor_fn_violation": 0.006019692692455054,
            "auditor_fp_violation": 0.016999436318865524,
            "ave_precision_score": 0.8365402609703252,
            "fpr": 0.16136114160263446,
            "logloss": 0.7773405704147147,
            "mae": 0.26437652843318654,
            "precision": 0.724202626641651,
            "recall": 0.8265524625267666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8292592865951656,
            "auditor_fn_violation": 0.009460895565402215,
            "auditor_fp_violation": 0.021339009287925707,
            "ave_precision_score": 0.8299997989749159,
            "fpr": 0.17214912280701755,
            "logloss": 0.7747134487097891,
            "mae": 0.2770829224677234,
            "precision": 0.718132854578097,
            "recall": 0.8213552361396304
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8312536299298159,
            "auditor_fn_violation": 0.0067859636091830286,
            "auditor_fp_violation": 0.021543497394210902,
            "ave_precision_score": 0.832817221533389,
            "fpr": 0.18221734357848518,
            "logloss": 0.7670557999762946,
            "mae": 0.2731379228896814,
            "precision": 0.7092819614711033,
            "recall": 0.867237687366167
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.830203780764772,
            "auditor_fn_violation": 0.015209031305162287,
            "auditor_fp_violation": 0.020866873065015484,
            "ave_precision_score": 0.8304941802981003,
            "fpr": 0.13267543859649122,
            "logloss": 0.8077865186152452,
            "mae": 0.2761233512531516,
            "precision": 0.7565392354124748,
            "recall": 0.7720739219712526
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8384626136666663,
            "auditor_fn_violation": 0.004181582702021689,
            "auditor_fp_violation": 0.016465422612513728,
            "ave_precision_score": 0.8389132392574141,
            "fpr": 0.14928649835345773,
            "logloss": 0.7407745986499312,
            "mae": 0.26127280850783935,
            "precision": 0.7384615384615385,
            "recall": 0.8222698072805139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 31658,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8118781281012177,
            "auditor_fn_violation": 0.016129903814978926,
            "auditor_fp_violation": 0.02026831785345717,
            "ave_precision_score": 0.8133043233880499,
            "fpr": 0.16666666666666666,
            "logloss": 0.8679975998653273,
            "mae": 0.27470612664725275,
            "precision": 0.7256317689530686,
            "recall": 0.8254620123203286
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8216530715661136,
            "auditor_fn_violation": 0.011910106549265815,
            "auditor_fp_violation": 0.022005814815913613,
            "ave_precision_score": 0.8224600679066933,
            "fpr": 0.1942919868276619,
            "logloss": 0.8629411882988921,
            "mae": 0.273442052676302,
            "precision": 0.6948275862068966,
            "recall": 0.8629550321199143
        }
    }
]
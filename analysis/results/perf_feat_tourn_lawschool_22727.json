[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.8066006510088732,
            "auditor_fn_violation": 0.015112014995996223,
            "auditor_fp_violation": 0.022873317013463904,
            "ave_precision_score": 0.8070663494691123,
            "fpr": 0.17324561403508773,
            "logloss": 1.078293597721092,
            "mae": 0.28443026401965654,
            "precision": 0.7090239410681399,
            "recall": 0.7987551867219918
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8308280718829313,
            "auditor_fn_violation": 0.006274535340192376,
            "auditor_fp_violation": 0.023019085887745074,
            "ave_precision_score": 0.8311386462013761,
            "fpr": 0.16136114160263446,
            "logloss": 0.8686076945609792,
            "mae": 0.25153782738711694,
            "precision": 0.7327272727272728,
            "recall": 0.8538135593220338
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8023754541379293,
            "auditor_fn_violation": 0.01764850404018345,
            "auditor_fp_violation": 0.015669624643002863,
            "ave_precision_score": 0.802826382044362,
            "fpr": 0.15899122807017543,
            "logloss": 1.1501675248536474,
            "mae": 0.2813221927393506,
            "precision": 0.7222222222222222,
            "recall": 0.7821576763485477
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8334243081314939,
            "auditor_fn_violation": 0.007181528958678302,
            "auditor_fp_violation": 0.02192639193456839,
            "ave_precision_score": 0.8336486902074358,
            "fpr": 0.14928649835345773,
            "logloss": 0.90188332461342,
            "mae": 0.24777875317337675,
            "precision": 0.7443609022556391,
            "recall": 0.8389830508474576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.802893495377552,
            "auditor_fn_violation": 0.017061585499017255,
            "auditor_fp_violation": 0.019237046103631174,
            "ave_precision_score": 0.803293707682603,
            "fpr": 0.1600877192982456,
            "logloss": 1.1384897906254963,
            "mae": 0.283327868708993,
            "precision": 0.7208413001912046,
            "recall": 0.7821576763485477
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8319938180169186,
            "auditor_fn_violation": 0.005809410407635494,
            "auditor_fp_violation": 0.021441305831785145,
            "ave_precision_score": 0.8322996731897623,
            "fpr": 0.15367727771679474,
            "logloss": 0.8989155556652615,
            "mae": 0.2501759077256716,
            "precision": 0.7397769516728625,
            "recall": 0.8432203389830508
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8041985198500181,
            "auditor_fn_violation": 0.02183427968260902,
            "auditor_fp_violation": 0.01701856385148919,
            "ave_precision_score": 0.8044876632758245,
            "fpr": 0.13815789473684212,
            "logloss": 1.1535080554630106,
            "mae": 0.2868570645524687,
            "precision": 0.7391304347826086,
            "recall": 0.7406639004149378
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.835147160193799,
            "auditor_fn_violation": 0.01744916184487153,
            "auditor_fp_violation": 0.026984789800189538,
            "ave_precision_score": 0.8354189562995517,
            "fpr": 0.12733260153677278,
            "logloss": 0.8743469540963139,
            "mae": 0.2554854893421586,
            "precision": 0.7578288100208769,
            "recall": 0.7690677966101694
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7898867894310562,
            "auditor_fn_violation": 0.015187085972191896,
            "auditor_fp_violation": 0.0202468380252958,
            "ave_precision_score": 0.7900552113055512,
            "fpr": 0.1611842105263158,
            "logloss": 1.8355663964258369,
            "mae": 0.2937888038694098,
            "precision": 0.7156673114119922,
            "recall": 0.7676348547717843
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8117899739682474,
            "auditor_fn_violation": 0.00840945878062848,
            "auditor_fp_violation": 0.02245898647009845,
            "ave_precision_score": 0.8118934859098368,
            "fpr": 0.1525795828759605,
            "logloss": 1.3794864643647318,
            "mae": 0.2613607579806477,
            "precision": 0.734225621414914,
            "recall": 0.8135593220338984
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.794268843749157,
            "auditor_fn_violation": 0.01564661134163209,
            "auditor_fp_violation": 0.01510862913096696,
            "ave_precision_score": 0.7949546643958981,
            "fpr": 0.14583333333333334,
            "logloss": 1.1898408092769936,
            "mae": 0.2895019670912288,
            "precision": 0.7307692307692307,
            "recall": 0.7489626556016598
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8268152444218856,
            "auditor_fn_violation": 0.003890770060838344,
            "auditor_fp_violation": 0.026607222782043815,
            "ave_precision_score": 0.8271373325137121,
            "fpr": 0.13721185510428102,
            "logloss": 0.9223802818513206,
            "mae": 0.25757938888232695,
            "precision": 0.750996015936255,
            "recall": 0.798728813559322
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8039446067665859,
            "auditor_fn_violation": 0.018531156729999276,
            "auditor_fp_violation": 0.019787841697266426,
            "ave_precision_score": 0.8042867080896848,
            "fpr": 0.16228070175438597,
            "logloss": 1.1564262292455454,
            "mae": 0.28178598170344377,
            "precision": 0.720754716981132,
            "recall": 0.7925311203319502
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8323618942609163,
            "auditor_fn_violation": 0.006809429012632799,
            "auditor_fp_violation": 0.021043735263009188,
            "ave_precision_score": 0.8326709591075996,
            "fpr": 0.15367727771679474,
            "logloss": 0.9095142648986344,
            "mae": 0.24873892720307092,
            "precision": 0.7407407407407407,
            "recall": 0.847457627118644
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8061683721060082,
            "auditor_fn_violation": 0.01550329402344035,
            "auditor_fp_violation": 0.016485618115055084,
            "ave_precision_score": 0.8064581987114048,
            "fpr": 0.16557017543859648,
            "logloss": 1.1171320168943224,
            "mae": 0.2838385826876797,
            "precision": 0.7161654135338346,
            "recall": 0.7904564315352697
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8316508768587701,
            "auditor_fn_violation": 0.006381514074680465,
            "auditor_fp_violation": 0.022539000672619394,
            "ave_precision_score": 0.8319643480359041,
            "fpr": 0.15367727771679474,
            "logloss": 0.8949860946081181,
            "mae": 0.2500523132039373,
            "precision": 0.7407407407407407,
            "recall": 0.847457627118644
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8045440984587104,
            "auditor_fn_violation": 0.018071631360559077,
            "auditor_fp_violation": 0.015470726234190138,
            "ave_precision_score": 0.8048226657146558,
            "fpr": 0.14364035087719298,
            "logloss": 1.1576846027246905,
            "mae": 0.2856661420160325,
            "precision": 0.7358870967741935,
            "recall": 0.7572614107883817
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8321570051780615,
            "auditor_fn_violation": 0.005181491748683697,
            "auditor_fp_violation": 0.021341288078633964,
            "ave_precision_score": 0.8324231449963888,
            "fpr": 0.14050493962678376,
            "logloss": 0.907713891357046,
            "mae": 0.25344550780017977,
            "precision": 0.7465346534653465,
            "recall": 0.798728813559322
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 22727,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7416373392603453,
            "auditor_fn_violation": 0.02094252748052705,
            "auditor_fp_violation": 0.025767543859649127,
            "ave_precision_score": 0.742127107555992,
            "fpr": 0.18311403508771928,
            "logloss": 1.25399274026528,
            "mae": 0.3223880130201718,
            "precision": 0.6872659176029963,
            "recall": 0.7614107883817427
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7628247266064531,
            "auditor_fn_violation": 0.009162961171370635,
            "auditor_fp_violation": 0.026287165971960024,
            "ave_precision_score": 0.7636746648970576,
            "fpr": 0.17672886937431395,
            "logloss": 1.0615559869998046,
            "mae": 0.29510207086546636,
            "precision": 0.696798493408663,
            "recall": 0.7838983050847458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7966319497560312,
            "auditor_fn_violation": 0.01302595180898304,
            "auditor_fp_violation": 0.016217870257037945,
            "ave_precision_score": 0.7961016471211029,
            "fpr": 0.14802631578947367,
            "logloss": 1.5023764863155653,
            "mae": 0.28521253776776784,
            "precision": 0.731610337972167,
            "recall": 0.7634854771784232
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8232644934239786,
            "auditor_fn_violation": 0.0018884072261809539,
            "auditor_fp_violation": 0.022649020201085692,
            "ave_precision_score": 0.8234303295867169,
            "fpr": 0.14270032930845225,
            "logloss": 1.1698388487249614,
            "mae": 0.2583594992075113,
            "precision": 0.7420634920634921,
            "recall": 0.7923728813559322
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8028397547633693,
            "auditor_fn_violation": 0.015523767926039166,
            "auditor_fp_violation": 0.020705834353325174,
            "ave_precision_score": 0.8030288974116335,
            "fpr": 0.1600877192982456,
            "logloss": 1.1659809886575239,
            "mae": 0.28247797731438146,
            "precision": 0.7224334600760456,
            "recall": 0.7883817427385892
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8313687896132758,
            "auditor_fn_violation": 0.004960557405719177,
            "auditor_fp_violation": 0.021043735263009188,
            "ave_precision_score": 0.8316809654113021,
            "fpr": 0.15367727771679474,
            "logloss": 0.917167463386338,
            "mae": 0.2494805015829621,
            "precision": 0.7392923649906891,
            "recall": 0.8411016949152542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8098145824876407,
            "auditor_fn_violation": 0.014632015723957198,
            "auditor_fp_violation": 0.017599959200326403,
            "ave_precision_score": 0.8101004017034283,
            "fpr": 0.15789473684210525,
            "logloss": 1.102184307758944,
            "mae": 0.27940600580139774,
            "precision": 0.7283018867924528,
            "recall": 0.8008298755186722
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8376477146022557,
            "auditor_fn_violation": 0.009586224859997395,
            "auditor_fp_violation": 0.025331996429366215,
            "ave_precision_score": 0.8379384163782841,
            "fpr": 0.145993413830955,
            "logloss": 0.8562161311754635,
            "mae": 0.249212888566528,
            "precision": 0.745697896749522,
            "recall": 0.826271186440678
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 22727,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.7302240009067053,
            "auditor_fn_violation": 0.011037708378830896,
            "auditor_fp_violation": 0.023954508363933095,
            "ave_precision_score": 0.731256910637274,
            "fpr": 0.2598684210526316,
            "logloss": 0.6522283701074598,
            "mae": 0.4036373297179428,
            "precision": 0.625,
            "recall": 0.8195020746887967
        },
        "train": {
            "accuracy": 0.6300768386388584,
            "auc_prc": 0.716900984520624,
            "auditor_fn_violation": 0.006297791586820222,
            "auditor_fp_violation": 0.019223412155657635,
            "ave_precision_score": 0.7174625251021185,
            "fpr": 0.2601536772777168,
            "logloss": 0.6679481018134079,
            "mae": 0.41351346122704113,
            "precision": 0.6108374384236454,
            "recall": 0.788135593220339
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 22727,
        "test": {
            "accuracy": 0.5986842105263158,
            "auc_prc": 0.6734673672010822,
            "auditor_fn_violation": 0.018194474776151996,
            "auditor_fp_violation": 0.010327417380660965,
            "ave_precision_score": 0.6733088101396448,
            "fpr": 0.16885964912280702,
            "logloss": 2.091053563980495,
            "mae": 0.4062548041711962,
            "precision": 0.6367924528301887,
            "recall": 0.5601659751037344
        },
        "train": {
            "accuracy": 0.6520307354555434,
            "auc_prc": 0.7189101346390576,
            "auditor_fn_violation": 0.006195464101657715,
            "auditor_fp_violation": 0.014932650545471821,
            "ave_precision_score": 0.7188216721489042,
            "fpr": 0.16136114160263446,
            "logloss": 1.7150976630062984,
            "mae": 0.35504300379530157,
            "precision": 0.6726057906458798,
            "recall": 0.6398305084745762
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8055856534703341,
            "auditor_fn_violation": 0.01594234549028172,
            "auditor_fp_violation": 0.016888514891880865,
            "ave_precision_score": 0.8058854462333834,
            "fpr": 0.15460526315789475,
            "logloss": 1.1512798112292588,
            "mae": 0.2809255602058527,
            "precision": 0.7298850574712644,
            "recall": 0.7904564315352697
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8346656219572692,
            "auditor_fn_violation": 0.009990883551321883,
            "auditor_fp_violation": 0.021973900367315204,
            "ave_precision_score": 0.8349673446863599,
            "fpr": 0.145993413830955,
            "logloss": 0.894906883463269,
            "mae": 0.24990878137431746,
            "precision": 0.7442307692307693,
            "recall": 0.8199152542372882
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8143297037702398,
            "auditor_fn_violation": 0.031161279755405114,
            "auditor_fp_violation": 0.028990718074255406,
            "ave_precision_score": 0.8147792205361992,
            "fpr": 0.16776315789473684,
            "logloss": 0.6432355435772242,
            "mae": 0.30439853444817194,
            "precision": 0.7150837988826816,
            "recall": 0.7966804979253111
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.849309420465947,
            "auditor_fn_violation": 0.018391039833299226,
            "auditor_fp_violation": 0.026389684168939987,
            "ave_precision_score": 0.8495321821393358,
            "fpr": 0.15367727771679474,
            "logloss": 0.5476045398414562,
            "mae": 0.28663868760155403,
            "precision": 0.7353497164461248,
            "recall": 0.8241525423728814
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7912020186771633,
            "auditor_fn_violation": 0.017279973793404676,
            "auditor_fp_violation": 0.018334353325173396,
            "ave_precision_score": 0.7907663276825042,
            "fpr": 0.15789473684210525,
            "logloss": 1.383493876121615,
            "mae": 0.2945335324755657,
            "precision": 0.7181996086105675,
            "recall": 0.7614107883817427
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8176906128160623,
            "auditor_fn_violation": 0.01002809354592644,
            "auditor_fp_violation": 0.023389151574404465,
            "ave_precision_score": 0.8170206716149038,
            "fpr": 0.1525795828759605,
            "logloss": 1.116678533681166,
            "mae": 0.26640964023222247,
            "precision": 0.7269155206286837,
            "recall": 0.7838983050847458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7880783894996308,
            "auditor_fn_violation": 0.01613798500400379,
            "auditor_fp_violation": 0.018092105263157906,
            "ave_precision_score": 0.7876508246110844,
            "fpr": 0.1524122807017544,
            "logloss": 1.3037705729138112,
            "mae": 0.29835963729901843,
            "precision": 0.7191919191919192,
            "recall": 0.7385892116182573
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8143186232053807,
            "auditor_fn_violation": 0.010032744795252006,
            "auditor_fp_violation": 0.024964431186535622,
            "ave_precision_score": 0.8139432030609828,
            "fpr": 0.145993413830955,
            "logloss": 1.0463987590769688,
            "mae": 0.270595065873291,
            "precision": 0.7329317269076305,
            "recall": 0.7733050847457628
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 22727,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7418504830871406,
            "auditor_fn_violation": 0.02094252748052705,
            "auditor_fp_violation": 0.025767543859649127,
            "ave_precision_score": 0.7423278572383687,
            "fpr": 0.18311403508771928,
            "logloss": 1.250361282909346,
            "mae": 0.3224046071788428,
            "precision": 0.6872659176029963,
            "recall": 0.7614107883817427
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7632510053874051,
            "auditor_fn_violation": 0.009162961171370635,
            "auditor_fp_violation": 0.025519529716524686,
            "ave_precision_score": 0.7640990216402941,
            "fpr": 0.1756311745334797,
            "logloss": 1.0584878568173366,
            "mae": 0.2950687353162296,
            "precision": 0.6981132075471698,
            "recall": 0.7838983050847458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8059218207840784,
            "auditor_fn_violation": 0.014308983038509134,
            "auditor_fp_violation": 0.02085883312933497,
            "ave_precision_score": 0.8063646618431671,
            "fpr": 0.17543859649122806,
            "logloss": 1.1358048071285305,
            "mae": 0.2815476532119616,
            "precision": 0.7111913357400722,
            "recall": 0.8174273858921162
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8323703756445273,
            "auditor_fn_violation": 0.006802452138644443,
            "auditor_fp_violation": 0.022716532184462745,
            "ave_precision_score": 0.832688010203894,
            "fpr": 0.16245883644346873,
            "logloss": 0.8956482066951958,
            "mae": 0.24903648020135355,
            "precision": 0.7323688969258589,
            "recall": 0.8580508474576272
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7985443444427004,
            "auditor_fn_violation": 0.01609703719880615,
            "auditor_fp_violation": 0.01534577723378214,
            "ave_precision_score": 0.7989585193893579,
            "fpr": 0.1611842105263158,
            "logloss": 1.1844615207377425,
            "mae": 0.2826580133450954,
            "precision": 0.720532319391635,
            "recall": 0.7863070539419087
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8304630418968981,
            "auditor_fn_violation": 0.006223371597611122,
            "auditor_fp_violation": 0.02651470636037897,
            "ave_precision_score": 0.8307645892422031,
            "fpr": 0.15148188803512624,
            "logloss": 0.9300865203223895,
            "mae": 0.2490584577372135,
            "precision": 0.7415730337078652,
            "recall": 0.8389830508474576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8042705989018807,
            "auditor_fn_violation": 0.017357319647666892,
            "auditor_fp_violation": 0.018691350469196247,
            "ave_precision_score": 0.8045748315784068,
            "fpr": 0.16228070175438597,
            "logloss": 1.1262479728951835,
            "mae": 0.28292953532126763,
            "precision": 0.7196969696969697,
            "recall": 0.7883817427385892
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8332056083120621,
            "auditor_fn_violation": 0.006614076540958902,
            "auditor_fp_violation": 0.022028910131548358,
            "ave_precision_score": 0.8335034563666266,
            "fpr": 0.15148188803512624,
            "logloss": 0.8896474999899457,
            "mae": 0.24942817289125752,
            "precision": 0.7415730337078652,
            "recall": 0.8389830508474576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8019690838830106,
            "auditor_fn_violation": 0.015501019145373815,
            "auditor_fp_violation": 0.0221593227254182,
            "ave_precision_score": 0.8019388480166296,
            "fpr": 0.16447368421052633,
            "logloss": 1.1674530555185096,
            "mae": 0.28584082735931166,
            "precision": 0.714828897338403,
            "recall": 0.7800829875518672
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8298576516773857,
            "auditor_fn_violation": 0.004455896853894959,
            "auditor_fp_violation": 0.02187138217033524,
            "ave_precision_score": 0.8301519348366322,
            "fpr": 0.150384193194292,
            "logloss": 0.9269964786495635,
            "mae": 0.25022085846997105,
            "precision": 0.7429643527204502,
            "recall": 0.8389830508474576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 22727,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7336163697184859,
            "auditor_fn_violation": 0.021533995777826314,
            "auditor_fp_violation": 0.017166462668298663,
            "ave_precision_score": 0.7343056047160216,
            "fpr": 0.1513157894736842,
            "logloss": 1.375218254641811,
            "mae": 0.3289482489229068,
            "precision": 0.703862660944206,
            "recall": 0.6804979253112033
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.7512805849348604,
            "auditor_fn_violation": 0.014883997841820314,
            "auditor_fp_violation": 0.03011534547382161,
            "ave_precision_score": 0.7520639334876782,
            "fpr": 0.15916575192096596,
            "logloss": 1.1734615808118298,
            "mae": 0.31160844180498065,
            "precision": 0.6985446985446986,
            "recall": 0.711864406779661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 22727,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.7492428566082766,
            "auditor_fn_violation": 0.07525296644099877,
            "auditor_fp_violation": 0.05506425948592412,
            "ave_precision_score": 0.7500519495810487,
            "fpr": 0.17324561403508773,
            "logloss": 1.8447655889015886,
            "mae": 0.34500330439487353,
            "precision": 0.6775510204081633,
            "recall": 0.6887966804979253
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7696112148767102,
            "auditor_fn_violation": 0.053628904723808817,
            "auditor_fp_violation": 0.05488974292936997,
            "ave_precision_score": 0.7702964274575196,
            "fpr": 0.17453347969264543,
            "logloss": 1.3229067575373838,
            "mae": 0.29879971473292477,
            "precision": 0.6994328922495274,
            "recall": 0.7838983050847458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7963445861963591,
            "auditor_fn_violation": 0.018767744048918983,
            "auditor_fp_violation": 0.015519175846593227,
            "ave_precision_score": 0.7965410304326562,
            "fpr": 0.14912280701754385,
            "logloss": 1.2330048122263062,
            "mae": 0.2856485007485193,
            "precision": 0.7274549098196392,
            "recall": 0.7531120331950207
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8296852431641604,
            "auditor_fn_violation": 0.004130309401105137,
            "auditor_fp_violation": 0.022666523307887154,
            "ave_precision_score": 0.8299950172690185,
            "fpr": 0.14050493962678376,
            "logloss": 0.9633233501071315,
            "mae": 0.25548742273214065,
            "precision": 0.7470355731225297,
            "recall": 0.8008474576271186
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.8044101521036707,
            "auditor_fn_violation": 0.014697987187886731,
            "auditor_fp_violation": 0.015014279885760913,
            "ave_precision_score": 0.8046310421855897,
            "fpr": 0.1513157894736842,
            "logloss": 1.1233903353572403,
            "mae": 0.2897338130997981,
            "precision": 0.7256461232604374,
            "recall": 0.7572614107883817
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8308480885495152,
            "auditor_fn_violation": 0.008065266330536383,
            "auditor_fp_violation": 0.021568828467052912,
            "ave_precision_score": 0.831176011171158,
            "fpr": 0.14050493962678376,
            "logloss": 0.8969976029800738,
            "mae": 0.25993961159926326,
            "precision": 0.7434869739478958,
            "recall": 0.7860169491525424
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8057939371726621,
            "auditor_fn_violation": 0.01662025915410934,
            "auditor_fp_violation": 0.01746736026111792,
            "ave_precision_score": 0.806166565001919,
            "fpr": 0.16447368421052633,
            "logloss": 1.139216935597935,
            "mae": 0.28320023047801457,
            "precision": 0.7169811320754716,
            "recall": 0.7883817427385892
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8319039086391502,
            "auditor_fn_violation": 0.007479208915514708,
            "auditor_fp_violation": 0.02190888882776693,
            "ave_precision_score": 0.8322210284309282,
            "fpr": 0.15587266739846323,
            "logloss": 0.9104033248767877,
            "mae": 0.24922953638009304,
            "precision": 0.7380073800738007,
            "recall": 0.847457627118644
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8024141487431824,
            "auditor_fn_violation": 0.01642916939652035,
            "auditor_fp_violation": 0.018775499796001637,
            "ave_precision_score": 0.8028340653900988,
            "fpr": 0.1611842105263158,
            "logloss": 1.1401808260731257,
            "mae": 0.28320128665703026,
            "precision": 0.72,
            "recall": 0.7842323651452282
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8320424153153643,
            "auditor_fn_violation": 0.006007088503972167,
            "auditor_fp_violation": 0.022028910131548358,
            "ave_precision_score": 0.8323451409607144,
            "fpr": 0.15148188803512624,
            "logloss": 0.8987544197572299,
            "mae": 0.24980187852631103,
            "precision": 0.7420560747663552,
            "recall": 0.8411016949152542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.7955944044596934,
            "auditor_fn_violation": 0.01414746669578511,
            "auditor_fp_violation": 0.014861281109751122,
            "ave_precision_score": 0.795502670654801,
            "fpr": 0.15021929824561403,
            "logloss": 1.2983751590453259,
            "mae": 0.283541703428075,
            "precision": 0.7313725490196078,
            "recall": 0.7738589211618258
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8266168319207867,
            "auditor_fn_violation": 0.004472176226534448,
            "auditor_fp_violation": 0.02384423235124235,
            "ave_precision_score": 0.8269289292935995,
            "fpr": 0.14709110867178923,
            "logloss": 1.0151453859965887,
            "mae": 0.2523096288286052,
            "precision": 0.7447619047619047,
            "recall": 0.8283898305084746
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8286071875458567,
            "auditor_fn_violation": 0.02096755113925893,
            "auditor_fp_violation": 0.01600367197062424,
            "ave_precision_score": 0.8293529861605546,
            "fpr": 0.12171052631578948,
            "logloss": 0.5707737147741065,
            "mae": 0.31336050045752417,
            "precision": 0.756578947368421,
            "recall": 0.7157676348547718
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8455361189106934,
            "auditor_fn_violation": 0.004623341829615441,
            "auditor_fp_violation": 0.010779413345868892,
            "ave_precision_score": 0.8458706197632007,
            "fpr": 0.09330406147091108,
            "logloss": 0.5094687901156147,
            "mae": 0.29849700998221235,
            "precision": 0.8,
            "recall": 0.7203389830508474
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 22727,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.7332660080049522,
            "auditor_fn_violation": 0.03554269491155274,
            "auditor_fp_violation": 0.03222154222766218,
            "ave_precision_score": 0.7338637054075514,
            "fpr": 0.18530701754385964,
            "logloss": 1.2670563778122852,
            "mae": 0.32678355863010283,
            "precision": 0.6817325800376648,
            "recall": 0.7510373443983402
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7514808969387106,
            "auditor_fn_violation": 0.018204989860276473,
            "auditor_fp_violation": 0.0385893496095557,
            "ave_precision_score": 0.7523401623585557,
            "fpr": 0.1756311745334797,
            "logloss": 1.048800410284661,
            "mae": 0.29171355636167506,
            "precision": 0.7053406998158379,
            "recall": 0.8114406779661016
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8008318459190249,
            "auditor_fn_violation": 0.01704793623061804,
            "auditor_fp_violation": 0.017480110159118736,
            "ave_precision_score": 0.80082550450518,
            "fpr": 0.15899122807017543,
            "logloss": 1.1965955723364448,
            "mae": 0.28499932408403883,
            "precision": 0.720616570327553,
            "recall": 0.7759336099585062
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8304503221819377,
            "auditor_fn_violation": 0.004402407486650917,
            "auditor_fp_violation": 0.023629194181967305,
            "ave_precision_score": 0.8307586995932714,
            "fpr": 0.1525795828759605,
            "logloss": 0.9337300390533257,
            "mae": 0.2514638835615354,
            "precision": 0.7387218045112782,
            "recall": 0.8326271186440678
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8024917094124482,
            "auditor_fn_violation": 0.017357319647666892,
            "auditor_fp_violation": 0.019410444716442272,
            "ave_precision_score": 0.8029293725060866,
            "fpr": 0.16228070175438597,
            "logloss": 1.138567546695228,
            "mae": 0.2831494018718424,
            "precision": 0.7196969696969697,
            "recall": 0.7883817427385892
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8322844586104876,
            "auditor_fn_violation": 0.006614076540958902,
            "auditor_fp_violation": 0.022028910131548358,
            "ave_precision_score": 0.8325912152750545,
            "fpr": 0.15148188803512624,
            "logloss": 0.8975378717588544,
            "mae": 0.24969652192314232,
            "precision": 0.7415730337078652,
            "recall": 0.8389830508474576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7993889208187782,
            "auditor_fn_violation": 0.012698369367401903,
            "auditor_fp_violation": 0.02069308445532436,
            "ave_precision_score": 0.7997280817740519,
            "fpr": 0.15899122807017543,
            "logloss": 1.1578875587068511,
            "mae": 0.28579530481725024,
            "precision": 0.7200772200772201,
            "recall": 0.7738589211618258
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.824868515299443,
            "auditor_fn_violation": 0.004632644328266575,
            "auditor_fp_violation": 0.024479345083752373,
            "ave_precision_score": 0.8251910118126444,
            "fpr": 0.150384193194292,
            "logloss": 0.9197619429208642,
            "mae": 0.2553323168387737,
            "precision": 0.7400379506641366,
            "recall": 0.826271186440678
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8019943545197076,
            "auditor_fn_violation": 0.015059692800465898,
            "auditor_fp_violation": 0.016638616891064877,
            "ave_precision_score": 0.8022393926059616,
            "fpr": 0.15899122807017543,
            "logloss": 1.1712918617482644,
            "mae": 0.28285343034061544,
            "precision": 0.7238095238095238,
            "recall": 0.7883817427385892
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8315912935645877,
            "auditor_fn_violation": 0.007181528958678302,
            "auditor_fp_violation": 0.021441305831785145,
            "ave_precision_score": 0.8318978244092116,
            "fpr": 0.15367727771679474,
            "logloss": 0.9209442392423295,
            "mae": 0.24941446586557187,
            "precision": 0.7388059701492538,
            "recall": 0.8389830508474576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7963478027444235,
            "auditor_fn_violation": 0.01817627575161972,
            "auditor_fp_violation": 0.01839810281517748,
            "ave_precision_score": 0.7969653320536314,
            "fpr": 0.14583333333333334,
            "logloss": 1.1815800178914426,
            "mae": 0.2919389216974635,
            "precision": 0.7285714285714285,
            "recall": 0.7406639004149378
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8244456155101874,
            "auditor_fn_violation": 0.012286275093490109,
            "auditor_fp_violation": 0.02458186328073234,
            "ave_precision_score": 0.8246989930103277,
            "fpr": 0.1350164654226125,
            "logloss": 0.9322934010668998,
            "mae": 0.2653728326617137,
            "precision": 0.7469135802469136,
            "recall": 0.7690677966101694
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.803749229608269,
            "auditor_fn_violation": 0.01550329402344035,
            "auditor_fp_violation": 0.0202468380252958,
            "ave_precision_score": 0.8040805820270416,
            "fpr": 0.1611842105263158,
            "logloss": 1.1608026741835975,
            "mae": 0.2820417740702687,
            "precision": 0.7215909090909091,
            "recall": 0.7904564315352697
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8319206173295473,
            "auditor_fn_violation": 0.006139649109750881,
            "auditor_fp_violation": 0.021043735263009188,
            "ave_precision_score": 0.8322271338645317,
            "fpr": 0.15367727771679474,
            "logloss": 0.912956981941447,
            "mae": 0.2490264238113947,
            "precision": 0.7402597402597403,
            "recall": 0.8453389830508474
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.8004703131150288,
            "auditor_fn_violation": 0.017302722574070034,
            "auditor_fp_violation": 0.017314361485108128,
            "ave_precision_score": 0.8006098843835879,
            "fpr": 0.16557017543859648,
            "logloss": 1.188207907771331,
            "mae": 0.28637468454861226,
            "precision": 0.7112810707456979,
            "recall": 0.7717842323651453
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8291048710723447,
            "auditor_fn_violation": 0.006325699082773636,
            "auditor_fp_violation": 0.023941749660564754,
            "ave_precision_score": 0.8294254505234947,
            "fpr": 0.15587266739846323,
            "logloss": 0.9313320233340937,
            "mae": 0.25227050085896313,
            "precision": 0.7355679702048417,
            "recall": 0.836864406779661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8105657651451708,
            "auditor_fn_violation": 0.017471063550993666,
            "auditor_fp_violation": 0.020221338229294172,
            "ave_precision_score": 0.8107614277051017,
            "fpr": 0.17434210526315788,
            "logloss": 1.1099956529581956,
            "mae": 0.28306308752894194,
            "precision": 0.7103825136612022,
            "recall": 0.8091286307053942
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8330669890980817,
            "auditor_fn_violation": 0.007621072019944562,
            "auditor_fp_violation": 0.02240897759352285,
            "ave_precision_score": 0.8333347535792446,
            "fpr": 0.16245883644346873,
            "logloss": 0.8869622936811132,
            "mae": 0.24957662889266685,
            "precision": 0.7333333333333333,
            "recall": 0.8622881355932204
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7992232570588185,
            "auditor_fn_violation": 0.014668413773021768,
            "auditor_fp_violation": 0.021152080783353733,
            "ave_precision_score": 0.7995754425715237,
            "fpr": 0.1787280701754386,
            "logloss": 1.1630508555046268,
            "mae": 0.2847388838151249,
            "precision": 0.7063063063063063,
            "recall": 0.8132780082987552
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8232390543223125,
            "auditor_fn_violation": 0.007853634486223,
            "auditor_fp_violation": 0.023336642254000086,
            "ave_precision_score": 0.8235761711704026,
            "fpr": 0.16136114160263446,
            "logloss": 0.9255232740105184,
            "mae": 0.2533234756996154,
            "precision": 0.7327272727272728,
            "recall": 0.8538135593220338
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 22727,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8036032657641443,
            "auditor_fn_violation": 0.015705758171362015,
            "auditor_fp_violation": 0.018984598123215023,
            "ave_precision_score": 0.8040192676401798,
            "fpr": 0.1699561403508772,
            "logloss": 1.1223815623067284,
            "mae": 0.2814430309267175,
            "precision": 0.716636197440585,
            "recall": 0.8132780082987552
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.831495334076739,
            "auditor_fn_violation": 0.007790842620327823,
            "auditor_fp_violation": 0.022506494902845252,
            "ave_precision_score": 0.8318079378927157,
            "fpr": 0.16465422612513722,
            "logloss": 0.8833005856050747,
            "mae": 0.247959599355814,
            "precision": 0.7302158273381295,
            "recall": 0.8601694915254238
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8035381166829422,
            "auditor_fn_violation": 0.01756205867365509,
            "auditor_fp_violation": 0.016136270909832728,
            "ave_precision_score": 0.8039013998040253,
            "fpr": 0.15789473684210525,
            "logloss": 1.1677147151525695,
            "mae": 0.283488855151994,
            "precision": 0.7230769230769231,
            "recall": 0.7800829875518672
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8323797476160205,
            "auditor_fn_violation": 0.007181528958678302,
            "auditor_fp_violation": 0.022073918120466385,
            "ave_precision_score": 0.8326819812798898,
            "fpr": 0.150384193194292,
            "logloss": 0.9182659634089274,
            "mae": 0.24980506804354066,
            "precision": 0.7429643527204502,
            "recall": 0.8389830508474576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.8734828344501311,
            "auditor_fn_violation": 0.00986387129649851,
            "auditor_fp_violation": 0.003947368421052632,
            "ave_precision_score": 0.8696073660739709,
            "fpr": 0.09429824561403509,
            "logloss": 1.6317033812034085,
            "mae": 0.22272230842059945,
            "precision": 0.8093126385809313,
            "recall": 0.7572614107883817
        },
        "train": {
            "accuracy": 0.7925356750823271,
            "auc_prc": 0.8843907230726664,
            "auditor_fn_violation": 0.010030419170589224,
            "auditor_fp_violation": 0.01481763012934796,
            "ave_precision_score": 0.8814701533505352,
            "fpr": 0.09440175631174534,
            "logloss": 1.255241463025994,
            "mae": 0.20666275110502014,
            "precision": 0.810989010989011,
            "recall": 0.7817796610169492
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8087809401429256,
            "auditor_fn_violation": 0.022409823833442533,
            "auditor_fp_violation": 0.01952264381884945,
            "ave_precision_score": 0.8090723754729061,
            "fpr": 0.14912280701754385,
            "logloss": 1.1322250932814624,
            "mae": 0.28474352270272,
            "precision": 0.7306930693069307,
            "recall": 0.7655601659751037
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8372000955740126,
            "auditor_fn_violation": 0.01028158663416994,
            "auditor_fp_violation": 0.02859757606975239,
            "ave_precision_score": 0.8374794404739733,
            "fpr": 0.1437980241492865,
            "logloss": 0.9029897725548747,
            "mae": 0.25407477646847604,
            "precision": 0.7431372549019608,
            "recall": 0.8029661016949152
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7932390387478823,
            "auditor_fn_violation": 0.014616091577491448,
            "auditor_fp_violation": 0.016962464300285595,
            "ave_precision_score": 0.7932646740558316,
            "fpr": 0.16557017543859648,
            "logloss": 1.2294400527309595,
            "mae": 0.28725383134469323,
            "precision": 0.7150943396226415,
            "recall": 0.7863070539419087
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.824145335044928,
            "auditor_fn_violation": 0.008202478185640667,
            "auditor_fp_violation": 0.024591865056047452,
            "ave_precision_score": 0.8244422031380818,
            "fpr": 0.15806805708013172,
            "logloss": 0.9738581604795497,
            "mae": 0.25537563607852776,
            "precision": 0.7338262476894639,
            "recall": 0.8411016949152542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8401113735604905,
            "auditor_fn_violation": 0.023676930916502876,
            "auditor_fp_violation": 0.01528457772337822,
            "ave_precision_score": 0.8404024694570646,
            "fpr": 0.14035087719298245,
            "logloss": 0.5415602393675674,
            "mae": 0.30566542893699844,
            "precision": 0.7470355731225297,
            "recall": 0.7842323651452282
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8535026951499887,
            "auditor_fn_violation": 0.004441943105918251,
            "auditor_fp_violation": 0.013072320336859795,
            "ave_precision_score": 0.853735776898013,
            "fpr": 0.12952799121844127,
            "logloss": 0.49742393883422703,
            "mae": 0.2969012568571501,
            "precision": 0.7596741344195519,
            "recall": 0.7902542372881356
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.790611450351491,
            "auditor_fn_violation": 0.01611523622333843,
            "auditor_fp_violation": 0.01409628722970216,
            "ave_precision_score": 0.791450156137117,
            "fpr": 0.14912280701754385,
            "logloss": 1.2134883214799153,
            "mae": 0.2906149458250746,
            "precision": 0.7274549098196392,
            "recall": 0.7531120331950207
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8269208490368072,
            "auditor_fn_violation": 0.00529777298182292,
            "auditor_fp_violation": 0.026837263614291534,
            "ave_precision_score": 0.8272242174931079,
            "fpr": 0.14270032930845225,
            "logloss": 0.9387175584087234,
            "mae": 0.2582337365265565,
            "precision": 0.7455968688845401,
            "recall": 0.8072033898305084
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8273782195664098,
            "auditor_fn_violation": 0.006679042003348626,
            "auditor_fp_violation": 0.009345675234598123,
            "ave_precision_score": 0.8277261204802344,
            "fpr": 0.14802631578947367,
            "logloss": 0.5421848272079729,
            "mae": 0.31554920382767815,
            "precision": 0.7403846153846154,
            "recall": 0.7987551867219918
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8249199486648351,
            "auditor_fn_violation": 0.008758302480046143,
            "auditor_fp_violation": 0.013874962805898049,
            "ave_precision_score": 0.8254326026719303,
            "fpr": 0.1350164654226125,
            "logloss": 0.5170672867867048,
            "mae": 0.312914890518447,
            "precision": 0.7484662576687117,
            "recall": 0.7754237288135594
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 22727,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7758537689197309,
            "auditor_fn_violation": 0.015314479143917887,
            "auditor_fp_violation": 0.021669726642186873,
            "ave_precision_score": 0.7751215010714575,
            "fpr": 0.2324561403508772,
            "logloss": 1.5167049296633734,
            "mae": 0.30651240605531793,
            "precision": 0.6634920634920635,
            "recall": 0.8672199170124482
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7773553083099951,
            "auditor_fn_violation": 0.007439673296247373,
            "auditor_fp_violation": 0.025256983114502835,
            "ave_precision_score": 0.7767486308281502,
            "fpr": 0.2239297475301866,
            "logloss": 1.3949057520901746,
            "mae": 0.2782710478222399,
            "precision": 0.6777251184834123,
            "recall": 0.9088983050847458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.796882784924723,
            "auditor_fn_violation": 0.019950680643517515,
            "auditor_fp_violation": 0.008833129334965327,
            "ave_precision_score": 0.7971481155346182,
            "fpr": 0.1337719298245614,
            "logloss": 1.2672079088853305,
            "mae": 0.29760529834640953,
            "precision": 0.7347826086956522,
            "recall": 0.7012448132780082
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8221112499912038,
            "auditor_fn_violation": 0.009376918640346803,
            "auditor_fp_violation": 0.02280404771847003,
            "ave_precision_score": 0.8225517243670846,
            "fpr": 0.12623490669593854,
            "logloss": 1.0079345308904397,
            "mae": 0.26838722558801714,
            "precision": 0.7521551724137931,
            "recall": 0.739406779661017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 22727,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.6401018070931455,
            "auditor_fn_violation": 0.022102715294460222,
            "auditor_fp_violation": 0.027886576907384757,
            "ave_precision_score": 0.6421613212959781,
            "fpr": 0.2543859649122807,
            "logloss": 0.7350302302178591,
            "mae": 0.43640357286801046,
            "precision": 0.6094276094276094,
            "recall": 0.7510373443983402
        },
        "train": {
            "accuracy": 0.6158068057080132,
            "auc_prc": 0.6525949507808593,
            "auditor_fn_violation": 0.02496790637965358,
            "auditor_fp_violation": 0.02537950486211303,
            "ave_precision_score": 0.6540849041366659,
            "fpr": 0.2502744237102086,
            "logloss": 0.7188697639240795,
            "mae": 0.43041687527350453,
            "precision": 0.6055363321799307,
            "recall": 0.7415254237288136
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8033237440622906,
            "auditor_fn_violation": 0.01566253548809784,
            "auditor_fp_violation": 0.018043655650754796,
            "ave_precision_score": 0.8034502407105848,
            "fpr": 0.1513157894736842,
            "logloss": 1.1556428447073193,
            "mae": 0.2863017428666533,
            "precision": 0.727810650887574,
            "recall": 0.7655601659751037
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8303701681798693,
            "auditor_fn_violation": 0.004395430612662565,
            "auditor_fp_violation": 0.023439160450980056,
            "ave_precision_score": 0.8306696314549301,
            "fpr": 0.14818880351262348,
            "logloss": 0.9166430697523728,
            "mae": 0.2550457061103469,
            "precision": 0.7388781431334622,
            "recall": 0.809322033898305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7775390746122777,
            "auditor_fn_violation": 0.017082059401616074,
            "auditor_fp_violation": 0.016748266013871888,
            "ave_precision_score": 0.7780811235418451,
            "fpr": 0.19956140350877194,
            "logloss": 0.6746552988796992,
            "mae": 0.32761890961308127,
            "precision": 0.6930860033726813,
            "recall": 0.8526970954356846
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7855056936240208,
            "auditor_fn_violation": 0.007786191371002256,
            "auditor_fp_violation": 0.005398458226335174,
            "ave_precision_score": 0.7860813890660829,
            "fpr": 0.19099890230515917,
            "logloss": 0.6513987511243131,
            "mae": 0.31806097694792257,
            "precision": 0.697391304347826,
            "recall": 0.8495762711864406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8048684678487495,
            "auditor_fn_violation": 0.015785378903690767,
            "auditor_fp_violation": 0.016574867401060792,
            "ave_precision_score": 0.8051670393356446,
            "fpr": 0.15350877192982457,
            "logloss": 1.1683766217388434,
            "mae": 0.2863904926295692,
            "precision": 0.726027397260274,
            "recall": 0.7697095435684648
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8341225289928726,
            "auditor_fn_violation": 0.005916389142123579,
            "auditor_fp_violation": 0.021463809826244157,
            "ave_precision_score": 0.8344134367511364,
            "fpr": 0.14709110867178923,
            "logloss": 0.9188335787061319,
            "mae": 0.2528391445658086,
            "precision": 0.7413127413127413,
            "recall": 0.8135593220338984
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 22727,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7422175841627212,
            "auditor_fn_violation": 0.022550866273567737,
            "auditor_fp_violation": 0.025923092615259078,
            "ave_precision_score": 0.7426925932829076,
            "fpr": 0.18421052631578946,
            "logloss": 1.2395170887391573,
            "mae": 0.3223961717969513,
            "precision": 0.6871508379888268,
            "recall": 0.7655601659751037
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7638558671871648,
            "auditor_fn_violation": 0.009004818694301291,
            "auditor_fp_violation": 0.026287165971960024,
            "ave_precision_score": 0.7646769357496586,
            "fpr": 0.17672886937431395,
            "logloss": 1.0493327124646938,
            "mae": 0.29469717887682795,
            "precision": 0.6979362101313321,
            "recall": 0.788135593220339
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7932980397371944,
            "auditor_fn_violation": 0.016495140860449887,
            "auditor_fp_violation": 0.01860210118319054,
            "ave_precision_score": 0.7932703899779442,
            "fpr": 0.15899122807017543,
            "logloss": 1.7165699287889955,
            "mae": 0.29224192860275183,
            "precision": 0.716796875,
            "recall": 0.7614107883817427
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8168220643114492,
            "auditor_fn_violation": 0.007195482706655013,
            "auditor_fp_violation": 0.022714031740633966,
            "ave_precision_score": 0.8169769473186028,
            "fpr": 0.14928649835345773,
            "logloss": 1.2852323810990895,
            "mae": 0.25883819029000166,
            "precision": 0.7394636015325671,
            "recall": 0.8177966101694916
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8062076047745753,
            "auditor_fn_violation": 0.014850404018344626,
            "auditor_fp_violation": 0.018410852713178303,
            "ave_precision_score": 0.806604310366196,
            "fpr": 0.16557017543859648,
            "logloss": 1.131532138298475,
            "mae": 0.2823255093795595,
            "precision": 0.7166979362101313,
            "recall": 0.7925311203319502
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8327735169533059,
            "auditor_fn_violation": 0.007969915719362222,
            "auditor_fp_violation": 0.02190888882776693,
            "ave_precision_score": 0.8330896987414261,
            "fpr": 0.15587266739846323,
            "logloss": 0.9039420980979556,
            "mae": 0.24818717692865316,
            "precision": 0.7384898710865562,
            "recall": 0.8495762711864406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7925240395714093,
            "auditor_fn_violation": 0.016495140860449887,
            "auditor_fp_violation": 0.016707466340269275,
            "ave_precision_score": 0.7925152245750067,
            "fpr": 0.15570175438596492,
            "logloss": 1.77145650914047,
            "mae": 0.2927423396690902,
            "precision": 0.7210216110019646,
            "recall": 0.7614107883817427
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8155847671895085,
            "auditor_fn_violation": 0.007516418910119262,
            "auditor_fp_violation": 0.022714031740633966,
            "ave_precision_score": 0.8157054385946733,
            "fpr": 0.14928649835345773,
            "logloss": 1.3277703542648895,
            "mae": 0.2589650341708148,
            "precision": 0.7384615384615385,
            "recall": 0.8135593220338984
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8002668451163433,
            "auditor_fn_violation": 0.014663864016888703,
            "auditor_fp_violation": 0.019481844145246837,
            "ave_precision_score": 0.8006755311475716,
            "fpr": 0.16447368421052633,
            "logloss": 1.154393423340236,
            "mae": 0.28862223471358883,
            "precision": 0.7142857142857143,
            "recall": 0.7780082987551867
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8223729174488049,
            "auditor_fn_violation": 0.004655900574894419,
            "auditor_fp_violation": 0.0245493575109582,
            "ave_precision_score": 0.8226658785899836,
            "fpr": 0.150384193194292,
            "logloss": 0.9514029364737905,
            "mae": 0.2618251058571154,
            "precision": 0.7375478927203065,
            "recall": 0.815677966101695
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8101534319284094,
            "auditor_fn_violation": 0.014632015723957198,
            "auditor_fp_violation": 0.017579559363525097,
            "ave_precision_score": 0.8104419124609189,
            "fpr": 0.15789473684210525,
            "logloss": 1.0912362187250597,
            "mae": 0.2787315297513709,
            "precision": 0.7283018867924528,
            "recall": 0.8008298755186722
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8387443478533659,
            "auditor_fn_violation": 0.009586224859997395,
            "auditor_fp_violation": 0.026339675292364393,
            "ave_precision_score": 0.8390311034220075,
            "fpr": 0.1437980241492865,
            "logloss": 0.8469713443505676,
            "mae": 0.2481713417403932,
            "precision": 0.7485604606525912,
            "recall": 0.826271186440678
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.800509316704033,
            "auditor_fn_violation": 0.014090594744121719,
            "auditor_fp_violation": 0.01892084863321094,
            "ave_precision_score": 0.80086122348037,
            "fpr": 0.16557017543859648,
            "logloss": 1.1517161863301624,
            "mae": 0.28406616196700724,
            "precision": 0.7156308851224106,
            "recall": 0.7883817427385892
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8258018209928342,
            "auditor_fn_violation": 0.005586150440008188,
            "auditor_fp_violation": 0.02058615404234252,
            "ave_precision_score": 0.826123669586371,
            "fpr": 0.15148188803512624,
            "logloss": 0.9160261277430176,
            "mae": 0.2528044037556235,
            "precision": 0.7410881801125704,
            "recall": 0.836864406779661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.800390764400005,
            "auditor_fn_violation": 0.01574215622042659,
            "auditor_fp_violation": 0.0208843329253366,
            "ave_precision_score": 0.8005707184965789,
            "fpr": 0.16228070175438597,
            "logloss": 1.1607103269346852,
            "mae": 0.2846823789265279,
            "precision": 0.7186311787072244,
            "recall": 0.7842323651452282
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8294965220126402,
            "auditor_fn_violation": 0.005134979255428009,
            "auditor_fp_violation": 0.0234941702152132,
            "ave_precision_score": 0.8298071398050552,
            "fpr": 0.15367727771679474,
            "logloss": 0.92319339873358,
            "mae": 0.25137144016248214,
            "precision": 0.7392923649906891,
            "recall": 0.8411016949152542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8098171269831969,
            "auditor_fn_violation": 0.014632015723957198,
            "auditor_fp_violation": 0.017579559363525097,
            "ave_precision_score": 0.8102506489374722,
            "fpr": 0.15789473684210525,
            "logloss": 1.0963924977277157,
            "mae": 0.2790756798470314,
            "precision": 0.7283018867924528,
            "recall": 0.8008298755186722
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8383714559448011,
            "auditor_fn_violation": 0.008402481906640126,
            "auditor_fp_violation": 0.026272163308987358,
            "ave_precision_score": 0.8386590905777673,
            "fpr": 0.14489571899012074,
            "logloss": 0.8512610904844629,
            "mae": 0.2486560729000067,
            "precision": 0.746641074856046,
            "recall": 0.8241525423728814
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7993185355613318,
            "auditor_fn_violation": 0.01691371842469244,
            "auditor_fp_violation": 0.02014483884128927,
            "ave_precision_score": 0.8000437735461705,
            "fpr": 0.16666666666666666,
            "logloss": 1.1571581402223243,
            "mae": 0.2865209198195155,
            "precision": 0.7104761904761905,
            "recall": 0.7738589211618258
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8292917549806651,
            "auditor_fn_violation": 0.0054070773409737915,
            "auditor_fp_violation": 0.025274486221304283,
            "ave_precision_score": 0.8296016446629634,
            "fpr": 0.15477497255762898,
            "logloss": 0.9172525796346056,
            "mae": 0.25307106530768203,
            "precision": 0.7369402985074627,
            "recall": 0.836864406779661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 22727,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.807389628328226,
            "auditor_fn_violation": 0.017475613307126738,
            "auditor_fp_violation": 0.01821960424316606,
            "ave_precision_score": 0.8078154668369532,
            "fpr": 0.15899122807017543,
            "logloss": 1.1315345041967497,
            "mae": 0.280312022460631,
            "precision": 0.7248576850094877,
            "recall": 0.7925311203319502
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8362642772820469,
            "auditor_fn_violation": 0.00942808238292806,
            "auditor_fp_violation": 0.025864590964896277,
            "ave_precision_score": 0.8365611450825321,
            "fpr": 0.145993413830955,
            "logloss": 0.8773824063611406,
            "mae": 0.24793891522369776,
            "precision": 0.7461832061068703,
            "recall": 0.8283898305084746
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7859572187507041,
            "auditor_fn_violation": 0.013769836936740195,
            "auditor_fp_violation": 0.018359853121175034,
            "ave_precision_score": 0.7862557097615805,
            "fpr": 0.14912280701754385,
            "logloss": 1.2716503732343416,
            "mae": 0.2949910667257659,
            "precision": 0.7263581488933601,
            "recall": 0.7489626556016598
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8220990405138535,
            "auditor_fn_violation": 0.006297791586820221,
            "auditor_fp_violation": 0.023016585443916295,
            "ave_precision_score": 0.8223722559788267,
            "fpr": 0.14818880351262348,
            "logloss": 1.0042710044259688,
            "mae": 0.2653213212226693,
            "precision": 0.7321428571428571,
            "recall": 0.7817796610169492
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8208157719404645,
            "auditor_fn_violation": 0.02335617310912135,
            "auditor_fp_violation": 0.01739596083231335,
            "ave_precision_score": 0.8213336891510011,
            "fpr": 0.1513157894736842,
            "logloss": 0.5646066877143862,
            "mae": 0.31171505711736563,
            "precision": 0.7341040462427746,
            "recall": 0.7904564315352697
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8391071245090305,
            "auditor_fn_violation": 0.007404788926305608,
            "auditor_fp_violation": 0.014970157202903515,
            "ave_precision_score": 0.8395325211021623,
            "fpr": 0.13391877058177826,
            "logloss": 0.5110265375688056,
            "mae": 0.30035069614065335,
            "precision": 0.7545271629778671,
            "recall": 0.7944915254237288
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.7959334746453389,
            "auditor_fn_violation": 0.01414746669578511,
            "auditor_fp_violation": 0.014861281109751122,
            "ave_precision_score": 0.7958366444342461,
            "fpr": 0.15021929824561403,
            "logloss": 1.3001508918059277,
            "mae": 0.28325822932573363,
            "precision": 0.7313725490196078,
            "recall": 0.7738589211618258
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8268872505935049,
            "auditor_fn_violation": 0.004472176226534448,
            "auditor_fp_violation": 0.02384423235124235,
            "ave_precision_score": 0.827205833111615,
            "fpr": 0.14709110867178923,
            "logloss": 1.0160692346303282,
            "mae": 0.25198241632779544,
            "precision": 0.7447619047619047,
            "recall": 0.8283898305084746
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 22727,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8160681994667232,
            "auditor_fn_violation": 0.04372088156074835,
            "auditor_fp_violation": 0.031283149734802125,
            "ave_precision_score": 0.8166949937004111,
            "fpr": 0.1337719298245614,
            "logloss": 0.6250083287303378,
            "mae": 0.31263256303751613,
            "precision": 0.7463617463617463,
            "recall": 0.7448132780082988
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8459889788206545,
            "auditor_fn_violation": 0.0304447524605109,
            "auditor_fp_violation": 0.03089798439222963,
            "ave_precision_score": 0.8462607823471612,
            "fpr": 0.13611416026344675,
            "logloss": 0.5228574974802889,
            "mae": 0.29338410404582815,
            "precision": 0.751503006012024,
            "recall": 0.7944915254237288
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7935293858307346,
            "auditor_fn_violation": 0.015355426949115538,
            "auditor_fp_violation": 0.015988372093023263,
            "ave_precision_score": 0.7942389246444712,
            "fpr": 0.1425438596491228,
            "logloss": 1.1768161100176495,
            "mae": 0.2908362883207285,
            "precision": 0.7336065573770492,
            "recall": 0.7427385892116183
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8277370667183976,
            "auditor_fn_violation": 0.011193231501981437,
            "auditor_fp_violation": 0.02770491762287806,
            "ave_precision_score": 0.8280450875176174,
            "fpr": 0.13721185510428102,
            "logloss": 0.914301843539497,
            "mae": 0.2586355525964665,
            "precision": 0.748995983935743,
            "recall": 0.7902542372881356
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 22727,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7389463930431186,
            "auditor_fn_violation": 0.01232301448642353,
            "auditor_fp_violation": 0.021521827825377402,
            "ave_precision_score": 0.7394141403243706,
            "fpr": 0.23026315789473684,
            "logloss": 1.4609960364287247,
            "mae": 0.32792968645763104,
            "precision": 0.6517412935323383,
            "recall": 0.8153526970954357
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7606072784374811,
            "auditor_fn_violation": 0.007437347671584593,
            "auditor_fp_violation": 0.023239124944677697,
            "ave_precision_score": 0.7607774145773283,
            "fpr": 0.22283205268935236,
            "logloss": 1.316351973970726,
            "mae": 0.3019718341819806,
            "precision": 0.6666666666666666,
            "recall": 0.8601694915254238
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8289754888260152,
            "auditor_fn_violation": 0.02108811967678533,
            "auditor_fp_violation": 0.010123419012647902,
            "ave_precision_score": 0.829752147775185,
            "fpr": 0.12609649122807018,
            "logloss": 1.107434350092205,
            "mae": 0.2885337798352805,
            "precision": 0.7516198704103672,
            "recall": 0.7219917012448133
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8669872405255288,
            "auditor_fn_violation": 0.010446705985227633,
            "auditor_fp_violation": 0.022351467385460927,
            "ave_precision_score": 0.8671490475815355,
            "fpr": 0.1207464324917673,
            "logloss": 0.8205941869512107,
            "mae": 0.25720782719789287,
            "precision": 0.7649572649572649,
            "recall": 0.7584745762711864
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7966653579230281,
            "auditor_fn_violation": 0.01594007061221519,
            "auditor_fp_violation": 0.016845165238678093,
            "ave_precision_score": 0.7967988696598884,
            "fpr": 0.15789473684210525,
            "logloss": 1.2376445600867558,
            "mae": 0.28161716520244956,
            "precision": 0.7288135593220338,
            "recall": 0.8029045643153527
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8250794680901756,
            "auditor_fn_violation": 0.00850480939180264,
            "auditor_fp_violation": 0.024729389466630326,
            "ave_precision_score": 0.8253795669556123,
            "fpr": 0.1602634467618002,
            "logloss": 0.9858153132593166,
            "mae": 0.2531532880324801,
            "precision": 0.7321100917431193,
            "recall": 0.8453389830508474
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.7967734956150407,
            "auditor_fn_violation": 0.017723575016379126,
            "auditor_fp_violation": 0.0196858425132599,
            "ave_precision_score": 0.7977433254071643,
            "fpr": 0.16776315789473684,
            "logloss": 1.2006155039600732,
            "mae": 0.2875414400248101,
            "precision": 0.7091254752851711,
            "recall": 0.7738589211618258
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8265221700917276,
            "auditor_fn_violation": 0.006302442836145787,
            "auditor_fp_violation": 0.02472688902280155,
            "ave_precision_score": 0.8268341039889193,
            "fpr": 0.15148188803512624,
            "logloss": 0.9600595919298754,
            "mae": 0.25391104179758345,
            "precision": 0.7410881801125704,
            "recall": 0.836864406779661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 22727,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.7475578130310618,
            "auditor_fn_violation": 0.02975085535415302,
            "auditor_fp_violation": 0.02289626682986536,
            "ave_precision_score": 0.7480445367469193,
            "fpr": 0.16557017543859648,
            "logloss": 1.3586545576735556,
            "mae": 0.32315280597341844,
            "precision": 0.6961770623742455,
            "recall": 0.7178423236514523
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7689982056119954,
            "auditor_fn_violation": 0.016967757539675166,
            "auditor_fp_violation": 0.030562924919173156,
            "ave_precision_score": 0.769810504434737,
            "fpr": 0.15477497255762898,
            "logloss": 1.1317385888880973,
            "mae": 0.29947020303288174,
            "precision": 0.7122448979591837,
            "recall": 0.739406779661017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.803310593421511,
            "auditor_fn_violation": 0.01805570721409333,
            "auditor_fp_violation": 0.018775499796001637,
            "ave_precision_score": 0.8034397338217455,
            "fpr": 0.1611842105263158,
            "logloss": 1.1411514404136203,
            "mae": 0.2832192744158797,
            "precision": 0.720532319391635,
            "recall": 0.7863070539419087
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8315041389107323,
            "auditor_fn_violation": 0.005809410407635494,
            "auditor_fp_violation": 0.021441305831785145,
            "ave_precision_score": 0.8318194618771868,
            "fpr": 0.15367727771679474,
            "logloss": 0.9003342371734522,
            "mae": 0.24997488372701326,
            "precision": 0.7397769516728625,
            "recall": 0.8432203389830508
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 22727,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.738490822215736,
            "auditor_fn_violation": 0.07162226104680788,
            "auditor_fp_violation": 0.07030803753569972,
            "ave_precision_score": 0.7336995565423003,
            "fpr": 0.21820175438596492,
            "logloss": 3.316111355656011,
            "mae": 0.3537380787088266,
            "precision": 0.6394927536231884,
            "recall": 0.7323651452282157
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7505568685391797,
            "auditor_fn_violation": 0.06002902379579156,
            "auditor_fp_violation": 0.0680445779125795,
            "ave_precision_score": 0.7425978184791613,
            "fpr": 0.2052689352360044,
            "logloss": 2.676447369707053,
            "mae": 0.31016658058746677,
            "precision": 0.6666666666666666,
            "recall": 0.7923728813559322
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.816369856309596,
            "auditor_fn_violation": 0.035046771493047975,
            "auditor_fp_violation": 0.034858221134230934,
            "ave_precision_score": 0.8167840620755781,
            "fpr": 0.18092105263157895,
            "logloss": 0.6531570147129171,
            "mae": 0.30479454540668566,
            "precision": 0.7048300536672629,
            "recall": 0.8174273858921162
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8498444386242405,
            "auditor_fn_violation": 0.020009674598597184,
            "auditor_fp_violation": 0.0303728911881859,
            "ave_precision_score": 0.8500650524715008,
            "fpr": 0.1668496158068057,
            "logloss": 0.5575196038924827,
            "mae": 0.2881303340789844,
            "precision": 0.7236363636363636,
            "recall": 0.8432203389830508
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 22727,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7403794921363813,
            "auditor_fn_violation": 0.0114653854553396,
            "auditor_fp_violation": 0.004105467156262755,
            "ave_precision_score": 0.7413234322728746,
            "fpr": 0.22478070175438597,
            "logloss": 0.7347743799003523,
            "mae": 0.33642644836766666,
            "precision": 0.6677471636952999,
            "recall": 0.8547717842323651
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.6840179746297416,
            "auditor_fn_violation": 0.009135053675417221,
            "auditor_fp_violation": 0.008924084024914437,
            "ave_precision_score": 0.685252224225652,
            "fpr": 0.22283205268935236,
            "logloss": 0.820217761020094,
            "mae": 0.3381371162408484,
            "precision": 0.6688417618270799,
            "recall": 0.8686440677966102
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7864186888586324,
            "auditor_fn_violation": 0.023877120186358014,
            "auditor_fp_violation": 0.01790085679314566,
            "ave_precision_score": 0.7859838641396348,
            "fpr": 0.15350877192982457,
            "logloss": 1.536871104018328,
            "mae": 0.30146895785175626,
            "precision": 0.717741935483871,
            "recall": 0.7385892116182573
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.7810921887507909,
            "auditor_fn_violation": 0.008000148839978419,
            "auditor_fp_violation": 0.02518947113112578,
            "ave_precision_score": 0.7809866152683393,
            "fpr": 0.13721185510428102,
            "logloss": 1.2085311323468266,
            "mae": 0.27039502042061625,
            "precision": 0.7474747474747475,
            "recall": 0.7838983050847458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8064321957633812,
            "auditor_fn_violation": 0.0174346655019291,
            "auditor_fp_violation": 0.018410852713178303,
            "ave_precision_score": 0.8067451190577789,
            "fpr": 0.16557017543859648,
            "logloss": 1.114982749206649,
            "mae": 0.2835526388029241,
            "precision": 0.7166979362101313,
            "recall": 0.7925311203319502
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8318843495062468,
            "auditor_fn_violation": 0.006381514074680465,
            "auditor_fp_violation": 0.022539000672619394,
            "ave_precision_score": 0.8321526342463441,
            "fpr": 0.15367727771679474,
            "logloss": 0.8933259078896904,
            "mae": 0.24971712067873444,
            "precision": 0.7407407407407407,
            "recall": 0.847457627118644
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8289788480464346,
            "auditor_fn_violation": 0.05176712528208489,
            "auditor_fp_violation": 0.03696450428396573,
            "ave_precision_score": 0.8294988791921023,
            "fpr": 0.13925438596491227,
            "logloss": 0.5875781382549081,
            "mae": 0.3208348823256305,
            "precision": 0.7392197125256673,
            "recall": 0.7468879668049793
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8544732585268768,
            "auditor_fn_violation": 0.03387970008744349,
            "auditor_fp_violation": 0.03937698941562128,
            "ave_precision_score": 0.8546812869306368,
            "fpr": 0.1350164654226125,
            "logloss": 0.5031744631899198,
            "mae": 0.3034708136860206,
            "precision": 0.7535070140280561,
            "recall": 0.7966101694915254
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7967969956062086,
            "auditor_fn_violation": 0.013758462546407514,
            "auditor_fp_violation": 0.01759485924112607,
            "ave_precision_score": 0.7972404750959441,
            "fpr": 0.16228070175438597,
            "logloss": 1.309345443380907,
            "mae": 0.2909255627471836,
            "precision": 0.7164750957854407,
            "recall": 0.7759336099585062
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8182119751109627,
            "auditor_fn_violation": 0.003118662672793914,
            "auditor_fp_violation": 0.021778865748670394,
            "ave_precision_score": 0.8185933060347741,
            "fpr": 0.15148188803512624,
            "logloss": 1.0262903406693826,
            "mae": 0.25746418916058744,
            "precision": 0.7381404174573055,
            "recall": 0.8241525423728814
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8011653837323206,
            "auditor_fn_violation": 0.016383671835189635,
            "auditor_fp_violation": 0.015937372501019998,
            "ave_precision_score": 0.8013146639629171,
            "fpr": 0.15350877192982457,
            "logloss": 1.6341283475284516,
            "mae": 0.2874457784677521,
            "precision": 0.7254901960784313,
            "recall": 0.7676348547717843
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8247695310961157,
            "auditor_fn_violation": 0.00655361029972651,
            "auditor_fp_violation": 0.023671701727056557,
            "ave_precision_score": 0.8248970283085273,
            "fpr": 0.14050493962678376,
            "logloss": 1.2104047296637532,
            "mae": 0.25681561661906654,
            "precision": 0.7470355731225297,
            "recall": 0.8008474576271186
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7971277865806858,
            "auditor_fn_violation": 0.017468788672927137,
            "auditor_fp_violation": 0.0196858425132599,
            "ave_precision_score": 0.797861594536693,
            "fpr": 0.16776315789473684,
            "logloss": 1.1959316662479755,
            "mae": 0.2873029434106285,
            "precision": 0.7102272727272727,
            "recall": 0.7780082987551867
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8269021345241397,
            "auditor_fn_violation": 0.006302442836145787,
            "auditor_fp_violation": 0.024031765638400822,
            "ave_precision_score": 0.8272118967313262,
            "fpr": 0.1525795828759605,
            "logloss": 0.9558814628395155,
            "mae": 0.2537217101812895,
            "precision": 0.7397003745318352,
            "recall": 0.836864406779661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7931327952474275,
            "auditor_fn_violation": 0.015114289874062749,
            "auditor_fp_violation": 0.01489188086495308,
            "ave_precision_score": 0.7931317037523904,
            "fpr": 0.15350877192982457,
            "logloss": 1.26239876491831,
            "mae": 0.2867366187461644,
            "precision": 0.726027397260274,
            "recall": 0.7697095435684648
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8209838211796655,
            "auditor_fn_violation": 0.002176784684366223,
            "auditor_fp_violation": 0.025369503086797925,
            "ave_precision_score": 0.8213003820052586,
            "fpr": 0.14709110867178923,
            "logloss": 1.003010213356907,
            "mae": 0.2567106713893481,
            "precision": 0.7428023032629558,
            "recall": 0.8199152542372882
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8042582580705537,
            "auditor_fn_violation": 0.01662025915410934,
            "auditor_fp_violation": 0.018053855569155456,
            "ave_precision_score": 0.8046039763798774,
            "fpr": 0.1611842105263158,
            "logloss": 1.1112367044038232,
            "mae": 0.28227979637971495,
            "precision": 0.7210626185958254,
            "recall": 0.7883817427385892
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8345252300414123,
            "auditor_fn_violation": 0.006232674096262255,
            "auditor_fp_violation": 0.022791545499326132,
            "ave_precision_score": 0.8348227817902811,
            "fpr": 0.14928649835345773,
            "logloss": 0.8750464106071325,
            "mae": 0.24820350051395584,
            "precision": 0.7448405253283302,
            "recall": 0.8411016949152542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8040333886629043,
            "auditor_fn_violation": 0.015309929387784816,
            "auditor_fp_violation": 0.0209608323133415,
            "ave_precision_score": 0.8044551664329231,
            "fpr": 0.1699561403508772,
            "logloss": 1.136335504174302,
            "mae": 0.28210860993736664,
            "precision": 0.7176684881602914,
            "recall": 0.8174273858921162
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8311136724861465,
            "auditor_fn_violation": 0.006802452138644443,
            "auditor_fp_violation": 0.023026587219231415,
            "ave_precision_score": 0.8314316368454866,
            "fpr": 0.16575192096597147,
            "logloss": 0.8965021844751121,
            "mae": 0.250086938499704,
            "precision": 0.7284172661870504,
            "recall": 0.8580508474576272
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7613680375413882,
            "auditor_fn_violation": 0.017093433791948748,
            "auditor_fp_violation": 0.015605875152998779,
            "ave_precision_score": 0.7629911775611061,
            "fpr": 0.16666666666666666,
            "logloss": 1.1072851847710097,
            "mae": 0.3022638456203147,
            "precision": 0.7076923076923077,
            "recall": 0.7634854771784232
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7694051340412099,
            "auditor_fn_violation": 0.006325699082773634,
            "auditor_fp_violation": 0.022496493127530146,
            "ave_precision_score": 0.7699931823439994,
            "fpr": 0.1778265642151482,
            "logloss": 0.9852031521367043,
            "mae": 0.2817938250959163,
            "precision": 0.7054545454545454,
            "recall": 0.8220338983050848
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7871330410770463,
            "auditor_fn_violation": 0.015742156220426587,
            "auditor_fp_violation": 0.019099347205222366,
            "ave_precision_score": 0.7871678356184353,
            "fpr": 0.1875,
            "logloss": 1.2467656761768053,
            "mae": 0.2972909195373195,
            "precision": 0.6929982046678635,
            "recall": 0.8008298755186722
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8128444583531915,
            "auditor_fn_violation": 0.007246646449236266,
            "auditor_fp_violation": 0.022819050381442704,
            "ave_precision_score": 0.8131808039552989,
            "fpr": 0.1877058177826564,
            "logloss": 1.0499403632237765,
            "mae": 0.26729471589001275,
            "precision": 0.7026086956521739,
            "recall": 0.8559322033898306
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.7983568889363104,
            "auditor_fn_violation": 0.015027844507534397,
            "auditor_fp_violation": 0.01847715218278254,
            "ave_precision_score": 0.7986678904471318,
            "fpr": 0.16228070175438597,
            "logloss": 1.2380275584976124,
            "mae": 0.2804317760907058,
            "precision": 0.7233644859813084,
            "recall": 0.8029045643153527
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8279855246697194,
            "auditor_fn_violation": 0.010028093545926436,
            "auditor_fp_violation": 0.022378972267577495,
            "ave_precision_score": 0.8282841339884179,
            "fpr": 0.15806805708013172,
            "logloss": 0.9700536080640639,
            "mae": 0.2518512454983741,
            "precision": 0.7343173431734318,
            "recall": 0.8432203389830508
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8212813201668414,
            "auditor_fn_violation": 0.021902526024605087,
            "auditor_fp_violation": 0.016906364749082012,
            "ave_precision_score": 0.8217659512321782,
            "fpr": 0.15350877192982457,
            "logloss": 0.5634459489743872,
            "mae": 0.3116492494972542,
            "precision": 0.7323135755258127,
            "recall": 0.7946058091286307
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8393461379763081,
            "auditor_fn_violation": 0.007404788926305608,
            "auditor_fp_violation": 0.015095179394342502,
            "ave_precision_score": 0.8397707561792263,
            "fpr": 0.13611416026344675,
            "logloss": 0.5111132728103855,
            "mae": 0.30056666302579427,
            "precision": 0.751503006012024,
            "recall": 0.7944915254237288
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8069268008405569,
            "auditor_fn_violation": 0.01637912207905657,
            "auditor_fp_violation": 0.015621175030599763,
            "ave_precision_score": 0.8073391568304951,
            "fpr": 0.15570175438596492,
            "logloss": 1.1422916155169351,
            "mae": 0.2805333769213618,
            "precision": 0.7290076335877863,
            "recall": 0.7925311203319502
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8356894578237829,
            "auditor_fn_violation": 0.00911412305345216,
            "auditor_fp_violation": 0.021973900367315204,
            "ave_precision_score": 0.8359876052892224,
            "fpr": 0.145993413830955,
            "logloss": 0.8879551656624268,
            "mae": 0.2492059461971973,
            "precision": 0.7452107279693486,
            "recall": 0.8241525423728814
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7776490085587933,
            "auditor_fn_violation": 0.021920725049137373,
            "auditor_fp_violation": 0.018716850265197885,
            "ave_precision_score": 0.7778956896245965,
            "fpr": 0.15350877192982457,
            "logloss": 1.3285066509187806,
            "mae": 0.3035902979445226,
            "precision": 0.7142857142857143,
            "recall": 0.7261410788381742
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8026658403631672,
            "auditor_fn_violation": 0.01649333010846714,
            "auditor_fp_violation": 0.02551202838503835,
            "ave_precision_score": 0.8026107871794699,
            "fpr": 0.14489571899012074,
            "logloss": 1.0535794081480694,
            "mae": 0.27412025352716113,
            "precision": 0.7322515212981744,
            "recall": 0.7648305084745762
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8019900081724955,
            "auditor_fn_violation": 0.015059692800465898,
            "auditor_fp_violation": 0.016638616891064877,
            "ave_precision_score": 0.8022467975037875,
            "fpr": 0.15899122807017543,
            "logloss": 1.1720635216881752,
            "mae": 0.2829045508293974,
            "precision": 0.7238095238095238,
            "recall": 0.7883817427385892
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8315652436481697,
            "auditor_fn_violation": 0.007181528958678302,
            "auditor_fp_violation": 0.021441305831785145,
            "ave_precision_score": 0.8318687887257408,
            "fpr": 0.15367727771679474,
            "logloss": 0.9215798178185601,
            "mae": 0.2494768554374873,
            "precision": 0.7388059701492538,
            "recall": 0.8389830508474576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8013981794525836,
            "auditor_fn_violation": 0.013758462546407514,
            "auditor_fp_violation": 0.01823235414116687,
            "ave_precision_score": 0.8018977213115475,
            "fpr": 0.16337719298245615,
            "logloss": 1.2144828654236897,
            "mae": 0.28974265537000254,
            "precision": 0.7151051625239006,
            "recall": 0.7759336099585062
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8238536103109924,
            "auditor_fn_violation": 0.003118662672793914,
            "auditor_fp_violation": 0.021778865748670394,
            "ave_precision_score": 0.8241786809235636,
            "fpr": 0.15148188803512624,
            "logloss": 0.9626369976717806,
            "mae": 0.25656304569386856,
            "precision": 0.7381404174573055,
            "recall": 0.8241525423728814
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7886819712800026,
            "auditor_fn_violation": 0.01642234476232074,
            "auditor_fp_violation": 0.018892798857609146,
            "ave_precision_score": 0.7903548446216921,
            "fpr": 0.17653508771929824,
            "logloss": 1.1214718616914745,
            "mae": 0.28957796131698593,
            "precision": 0.7051282051282052,
            "recall": 0.7987551867219918
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8262796090021161,
            "auditor_fn_violation": 0.0072512976985618345,
            "auditor_fp_violation": 0.021786367080156733,
            "ave_precision_score": 0.8266372419381294,
            "fpr": 0.1602634467618002,
            "logloss": 0.8573538741470914,
            "mae": 0.25374567542405013,
            "precision": 0.7330895795246801,
            "recall": 0.8495762711864406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 22727,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7902281132930771,
            "auditor_fn_violation": 0.01611523622333843,
            "auditor_fp_violation": 0.01409628722970216,
            "ave_precision_score": 0.7910953464237918,
            "fpr": 0.14912280701754385,
            "logloss": 1.223261683309109,
            "mae": 0.2906890871964303,
            "precision": 0.7274549098196392,
            "recall": 0.7531120331950207
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8264139371114736,
            "auditor_fn_violation": 0.00529777298182292,
            "auditor_fp_violation": 0.026837263614291534,
            "ave_precision_score": 0.8267221673518654,
            "fpr": 0.14270032930845225,
            "logloss": 0.9461979346587689,
            "mae": 0.258415064608051,
            "precision": 0.7455968688845401,
            "recall": 0.8072033898305084
        }
    }
]
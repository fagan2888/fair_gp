[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8286871196298531,
            "auditor_fn_violation": 0.041020313942751616,
            "auditor_fp_violation": 0.02417549881568911,
            "ave_precision_score": 0.8289609996529239,
            "fpr": 0.09100877192982457,
            "logloss": 0.7185390438942362,
            "mae": 0.29363606260368746,
            "precision": 0.7914572864321608,
            "recall": 0.6631578947368421
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.7997770514198241,
            "auditor_fn_violation": 0.054421831064993174,
            "auditor_fp_violation": 0.025185998292474695,
            "ave_precision_score": 0.8001682092073007,
            "fpr": 0.10318331503841932,
            "logloss": 0.8481648814080291,
            "mae": 0.31252179221988885,
            "precision": 0.7712895377128953,
            "recall": 0.6617954070981211
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.830658675889778,
            "auditor_fn_violation": 0.030953370267774706,
            "auditor_fp_violation": 0.03105554217351158,
            "ave_precision_score": 0.8309588762165023,
            "fpr": 0.16447368421052633,
            "logloss": 0.7574323668965541,
            "mae": 0.2835223040869035,
            "precision": 0.7252747252747253,
            "recall": 0.8336842105263158
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.801526459525163,
            "auditor_fn_violation": 0.04015867304964377,
            "auditor_fp_violation": 0.03231339187705818,
            "ave_precision_score": 0.8019164263746368,
            "fpr": 0.18111964873765093,
            "logloss": 0.875033534766717,
            "mae": 0.31048233578504125,
            "precision": 0.6933085501858736,
            "recall": 0.778705636743215
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8400370503691017,
            "auditor_fn_violation": 0.032089104339796864,
            "auditor_fp_violation": 0.02609749488136818,
            "ave_precision_score": 0.8402994345613396,
            "fpr": 0.12280701754385964,
            "logloss": 0.6771289147236108,
            "mae": 0.2796928614907763,
            "precision": 0.7642105263157895,
            "recall": 0.7642105263157895
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.8105182884918753,
            "auditor_fn_violation": 0.04428591398564059,
            "auditor_fp_violation": 0.030552506403219905,
            "ave_precision_score": 0.8108770974158962,
            "fpr": 0.13172338090010977,
            "logloss": 0.807736601425948,
            "mae": 0.3059464357786937,
            "precision": 0.7396963123644251,
            "recall": 0.7118997912317327
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 27690,
        "test": {
            "accuracy": 0.5614035087719298,
            "auc_prc": 0.6875263424484638,
            "auditor_fn_violation": 0.01287626962142198,
            "auditor_fp_violation": 0.012691195953269903,
            "ave_precision_score": 0.6882479101524608,
            "fpr": 0.03618421052631579,
            "logloss": 2.688203519386821,
            "mae": 0.43307182547011935,
            "precision": 0.7659574468085106,
            "recall": 0.22736842105263158
        },
        "train": {
            "accuracy": 0.58397365532382,
            "auc_prc": 0.6858174371309527,
            "auditor_fn_violation": 0.029635469064026095,
            "auditor_fp_violation": 0.015342318168882384,
            "ave_precision_score": 0.6875186688572232,
            "fpr": 0.03732162458836443,
            "logloss": 2.9288401640909805,
            "mae": 0.42300721213164894,
            "precision": 0.7976190476190477,
            "recall": 0.2797494780793319
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8356929649478363,
            "auditor_fn_violation": 0.03758541089566021,
            "auditor_fp_violation": 0.025452647637400138,
            "ave_precision_score": 0.8359555506058385,
            "fpr": 0.08991228070175439,
            "logloss": 0.699716726638237,
            "mae": 0.28605116237296035,
            "precision": 0.7995110024449877,
            "recall": 0.6884210526315789
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.8062212647085254,
            "auditor_fn_violation": 0.05325997034619783,
            "auditor_fp_violation": 0.02577550107736716,
            "ave_precision_score": 0.8066096688580371,
            "fpr": 0.10757409440175632,
            "logloss": 0.827847067512522,
            "mae": 0.30668717659525363,
            "precision": 0.7672209026128266,
            "recall": 0.6743215031315241
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.8344691241487345,
            "auditor_fn_violation": 0.03330563250230841,
            "auditor_fp_violation": 0.011629832590629894,
            "ave_precision_score": 0.8347311868543564,
            "fpr": 0.05592105263157895,
            "logloss": 0.8217080287539604,
            "mae": 0.30607100098098206,
            "precision": 0.8375796178343949,
            "recall": 0.5536842105263158
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.8021902088859914,
            "auditor_fn_violation": 0.04572277132426914,
            "auditor_fp_violation": 0.01793663861446518,
            "ave_precision_score": 0.8025968670125778,
            "fpr": 0.06256860592755215,
            "logloss": 0.9526549791665266,
            "mae": 0.321193164492405,
            "precision": 0.821875,
            "recall": 0.5490605427974948
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8152080404718646,
            "auditor_fn_violation": 0.029847645429362888,
            "auditor_fp_violation": 0.02355574290417119,
            "ave_precision_score": 0.8157943746607803,
            "fpr": 0.09978070175438597,
            "logloss": 0.789018320343455,
            "mae": 0.2938508805575642,
            "precision": 0.7785888077858881,
            "recall": 0.6736842105263158
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.8099747416578602,
            "auditor_fn_violation": 0.03915035211025531,
            "auditor_fp_violation": 0.020149815018091644,
            "ave_precision_score": 0.8104075821762537,
            "fpr": 0.09110867178924259,
            "logloss": 0.901292859298284,
            "mae": 0.302540580986733,
            "precision": 0.7877237851662404,
            "recall": 0.6430062630480167
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.850820462871288,
            "auditor_fn_violation": 0.03530009233610342,
            "auditor_fp_violation": 0.022923441326428207,
            "ave_precision_score": 0.8511665372464472,
            "fpr": 0.10526315789473684,
            "logloss": 0.6378442475340974,
            "mae": 0.2704960515463882,
            "precision": 0.789010989010989,
            "recall": 0.7557894736842106
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8361767923287934,
            "auditor_fn_violation": 0.0446021600984488,
            "auditor_fp_violation": 0.027620238240435826,
            "ave_precision_score": 0.8365048459492096,
            "fpr": 0.10757409440175632,
            "logloss": 0.7433094866126736,
            "mae": 0.28552626168429657,
            "precision": 0.7767653758542141,
            "recall": 0.7118997912317327
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8043525114919092,
            "auditor_fn_violation": 0.008070175438596493,
            "auditor_fp_violation": 0.014721084748484492,
            "ave_precision_score": 0.8047365484951237,
            "fpr": 0.13486842105263158,
            "logloss": 0.6249824405410365,
            "mae": 0.31377840145379005,
            "precision": 0.7432150313152401,
            "recall": 0.7494736842105263
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.8023226690985514,
            "auditor_fn_violation": 0.0009533216154218159,
            "auditor_fp_violation": 0.009493027604992483,
            "ave_precision_score": 0.8026381933241218,
            "fpr": 0.1602634467618002,
            "logloss": 0.6491574097865628,
            "mae": 0.325937401269194,
            "precision": 0.7153996101364523,
            "recall": 0.7661795407098121
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.8106641817837386,
            "auditor_fn_violation": 0.013882733148661135,
            "auditor_fp_violation": 0.029203801838692842,
            "ave_precision_score": 0.8112212375508856,
            "fpr": 0.14692982456140352,
            "logloss": 1.5157421173790337,
            "mae": 0.2957079771284723,
            "precision": 0.7161016949152542,
            "recall": 0.7115789473684211
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8124828197386018,
            "auditor_fn_violation": 0.02851714947670435,
            "auditor_fp_violation": 0.029393828515672644,
            "ave_precision_score": 0.8127785532344063,
            "fpr": 0.12733260153677278,
            "logloss": 1.6054188640289466,
            "mae": 0.285467528953031,
            "precision": 0.7494600431965442,
            "recall": 0.7244258872651357
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8372292267252317,
            "auditor_fn_violation": 0.038621883656509697,
            "auditor_fp_violation": 0.02551286683528043,
            "ave_precision_score": 0.8374922043498289,
            "fpr": 0.12719298245614036,
            "logloss": 0.6911808967109874,
            "mae": 0.28016632190182716,
            "precision": 0.7573221757322176,
            "recall": 0.7621052631578947
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.8094219993562294,
            "auditor_fn_violation": 0.0453744422724804,
            "auditor_fp_violation": 0.03476033662641786,
            "ave_precision_score": 0.8097842052969508,
            "fpr": 0.1437980241492865,
            "logloss": 0.821076694974058,
            "mae": 0.3037592144341374,
            "precision": 0.7265135699373695,
            "recall": 0.7265135699373695
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8359074494457972,
            "auditor_fn_violation": 0.0358471837488458,
            "auditor_fp_violation": 0.02290838652695813,
            "ave_precision_score": 0.8361637759852201,
            "fpr": 0.08223684210526316,
            "logloss": 0.7255795389340066,
            "mae": 0.29071647061393485,
            "precision": 0.804177545691906,
            "recall": 0.6484210526315789
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.804486541358461,
            "auditor_fn_violation": 0.05193540329400118,
            "auditor_fp_violation": 0.02284577387486279,
            "ave_precision_score": 0.8048757117856318,
            "fpr": 0.0889132821075741,
            "logloss": 0.8589970289106812,
            "mae": 0.3102271155286802,
            "precision": 0.7928388746803069,
            "recall": 0.6471816283924844
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8476184130339788,
            "auditor_fn_violation": 0.033134810710988,
            "auditor_fp_violation": 0.02308402585410896,
            "ave_precision_score": 0.8478545237284979,
            "fpr": 0.10087719298245613,
            "logloss": 0.6562034716762684,
            "mae": 0.2759285069225984,
            "precision": 0.7889908256880734,
            "recall": 0.7242105263157895
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8207994533410257,
            "auditor_fn_violation": 0.045626522507327516,
            "auditor_fp_violation": 0.024738789283245924,
            "ave_precision_score": 0.8211316659176584,
            "fpr": 0.11086717892425905,
            "logloss": 0.7611990543032372,
            "mae": 0.29619894666047725,
            "precision": 0.767816091954023,
            "recall": 0.697286012526096
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8483498496084299,
            "auditor_fn_violation": 0.003351800554016631,
            "auditor_fp_violation": 0.01140902886506885,
            "ave_precision_score": 0.8485962166638432,
            "fpr": 0.09539473684210527,
            "logloss": 0.5734068096689149,
            "mae": 0.2865932340518827,
            "precision": 0.7918660287081339,
            "recall": 0.6968421052631579
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.842383689374,
            "auditor_fn_violation": 0.011524650009510312,
            "auditor_fp_violation": 0.01481888035126235,
            "ave_precision_score": 0.8426439327471533,
            "fpr": 0.09879253567508232,
            "logloss": 0.5751320691196069,
            "mae": 0.2867033411494089,
            "precision": 0.7911832946635731,
            "recall": 0.7118997912317327
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8391625999519557,
            "auditor_fn_violation": 0.034392890120036934,
            "auditor_fp_violation": 0.026784997390501424,
            "ave_precision_score": 0.8394187081176395,
            "fpr": 0.09320175438596491,
            "logloss": 0.6931375819659739,
            "mae": 0.283869517621995,
            "precision": 0.7946859903381642,
            "recall": 0.6926315789473684
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8107743728250512,
            "auditor_fn_violation": 0.04954751597844944,
            "auditor_fp_violation": 0.024230597227304147,
            "ave_precision_score": 0.8111537021040902,
            "fpr": 0.10318331503841932,
            "logloss": 0.8189337092027443,
            "mae": 0.3035166314485794,
            "precision": 0.7751196172248804,
            "recall": 0.6764091858037579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7763157894736842,
            "auc_prc": 0.8460895264329583,
            "auditor_fn_violation": 0.008042474607571563,
            "auditor_fp_violation": 0.013348588863463007,
            "ave_precision_score": 0.7622159251969565,
            "fpr": 0.08442982456140351,
            "logloss": 7.49021363701589,
            "mae": 0.22566051521279062,
            "precision": 0.8188235294117647,
            "recall": 0.7326315789473684
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8366247231351438,
            "auditor_fn_violation": 0.0008868640989621208,
            "auditor_fp_violation": 0.004299304793267476,
            "ave_precision_score": 0.7485297324773432,
            "fpr": 0.09549945115257959,
            "logloss": 7.962760483090292,
            "mae": 0.2435013602032396,
            "precision": 0.7990762124711316,
            "recall": 0.7223382045929019
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8429759689901525,
            "auditor_fn_violation": 0.036193444136657434,
            "auditor_fp_violation": 0.021929824561403508,
            "ave_precision_score": 0.8432285082867743,
            "fpr": 0.10087719298245613,
            "logloss": 0.713412286458371,
            "mae": 0.27824145637303305,
            "precision": 0.7850467289719626,
            "recall": 0.7073684210526315
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8148225864272662,
            "auditor_fn_violation": 0.04847732079959852,
            "auditor_fp_violation": 0.02463206895149815,
            "ave_precision_score": 0.8152058381504997,
            "fpr": 0.10757409440175632,
            "logloss": 0.8356689968948031,
            "mae": 0.2994816767779156,
            "precision": 0.7710280373831776,
            "recall": 0.6889352818371608
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8404289419834607,
            "auditor_fn_violation": 0.035773314866112654,
            "auditor_fp_violation": 0.024903147456742544,
            "ave_precision_score": 0.8407449575840138,
            "fpr": 0.12828947368421054,
            "logloss": 0.6626348778787862,
            "mae": 0.2780948838337796,
            "precision": 0.7592592592592593,
            "recall": 0.7768421052631579
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.8133990544345124,
            "auditor_fn_violation": 0.041962192548049934,
            "auditor_fp_violation": 0.031238565678741316,
            "ave_precision_score": 0.8137483219825583,
            "fpr": 0.13830954994511527,
            "logloss": 0.7826565140538374,
            "mae": 0.301054211058218,
            "precision": 0.7369519832985386,
            "recall": 0.7369519832985386
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 27690,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8420945436293694,
            "auditor_fn_violation": 0.03027931671283473,
            "auditor_fp_violation": 0.014374824360672852,
            "ave_precision_score": 0.8423384118175311,
            "fpr": 0.05701754385964912,
            "logloss": 0.7760306012522014,
            "mae": 0.29771836128466184,
            "precision": 0.844311377245509,
            "recall": 0.5936842105263158
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.8114908362564296,
            "auditor_fn_violation": 0.04724441928734626,
            "auditor_fp_violation": 0.016800829369435298,
            "ave_precision_score": 0.8118464910981209,
            "fpr": 0.06586169045005488,
            "logloss": 0.8913957425769713,
            "mae": 0.3136024093609151,
            "precision": 0.8198198198198198,
            "recall": 0.569937369519833
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.872640211510674,
            "auditor_fn_violation": 0.012892428439519859,
            "auditor_fp_violation": 0.007055682684973302,
            "ave_precision_score": 0.8728057850368767,
            "fpr": 0.041666666666666664,
            "logloss": 0.7533946563488327,
            "mae": 0.24399525900797572,
            "precision": 0.8908045977011494,
            "recall": 0.6526315789473685
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8682413464653786,
            "auditor_fn_violation": 0.010216124426803917,
            "auditor_fp_violation": 0.003437919258446152,
            "ave_precision_score": 0.8684681382824707,
            "fpr": 0.05598243688254665,
            "logloss": 0.7856419169026315,
            "mae": 0.25454477412711146,
            "precision": 0.8583333333333333,
            "recall": 0.6450939457202505
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8430685275603735,
            "auditor_fn_violation": 0.012726223453370275,
            "auditor_fp_violation": 0.00964761732707054,
            "ave_precision_score": 0.8433284755263478,
            "fpr": 0.046052631578947366,
            "logloss": 0.7178397417868029,
            "mae": 0.30891886071056335,
            "precision": 0.8622950819672132,
            "recall": 0.5536842105263158
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.8175138453270065,
            "auditor_fn_violation": 0.026617381161356566,
            "auditor_fp_violation": 0.008369923161361141,
            "ave_precision_score": 0.8178452165332959,
            "fpr": 0.059275521405049394,
            "logloss": 0.7669161801152915,
            "mae": 0.3305376750211051,
            "precision": 0.8217821782178217,
            "recall": 0.5198329853862212
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8134118830780348,
            "auditor_fn_violation": 0.03746537396121884,
            "auditor_fp_violation": 0.024516740937010725,
            "ave_precision_score": 0.8137270174424156,
            "fpr": 0.09978070175438597,
            "logloss": 0.7219510710727713,
            "mae": 0.30173605868637016,
            "precision": 0.7780487804878049,
            "recall": 0.671578947368421
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7882131131761941,
            "auditor_fn_violation": 0.050418338607921276,
            "auditor_fp_violation": 0.030577916006016996,
            "ave_precision_score": 0.7890178657450002,
            "fpr": 0.11964873765093303,
            "logloss": 0.8478414633740536,
            "mae": 0.3235956528887176,
            "precision": 0.7429245283018868,
            "recall": 0.6576200417536534
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 27690,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.655830806377408,
            "auditor_fn_violation": 0.011045706371191141,
            "auditor_fp_violation": 0.023415231442450526,
            "ave_precision_score": 0.6310511744907996,
            "fpr": 0.13706140350877194,
            "logloss": 3.197099375053389,
            "mae": 0.33667801681135806,
            "precision": 0.7093023255813954,
            "recall": 0.6421052631578947
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.6933463644643104,
            "auditor_fn_violation": 0.009993835492438752,
            "auditor_fp_violation": 0.023148148148148154,
            "ave_precision_score": 0.6642859297546346,
            "fpr": 0.12952799121844127,
            "logloss": 2.783613273556742,
            "mae": 0.3331975994998342,
            "precision": 0.7183770883054893,
            "recall": 0.6283924843423799
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 27690,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.8096278821692745,
            "auditor_fn_violation": 0.003732686980609419,
            "auditor_fp_violation": 0.017079669998795614,
            "ave_precision_score": 0.8099444075062983,
            "fpr": 0.36293859649122806,
            "logloss": 1.028861567523579,
            "mae": 0.3713191802709939,
            "precision": 0.5825977301387137,
            "recall": 0.9726315789473684
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.7828967715644717,
            "auditor_fn_violation": 0.0006577002491011046,
            "auditor_fp_violation": 0.015306744724966456,
            "ave_precision_score": 0.783313949247641,
            "fpr": 0.37760702524698136,
            "logloss": 1.112601160089761,
            "mae": 0.3761773278860582,
            "precision": 0.5747836835599506,
            "recall": 0.9707724425887265
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.848091280142846,
            "auditor_fn_violation": 0.04078024007386889,
            "auditor_fp_violation": 0.030531133325304112,
            "ave_precision_score": 0.8484177813236585,
            "fpr": 0.09320175438596491,
            "logloss": 0.581688033941822,
            "mae": 0.302886156696505,
            "precision": 0.7985781990521327,
            "recall": 0.7094736842105264
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8400211206673036,
            "auditor_fn_violation": 0.05394975353427948,
            "auditor_fp_violation": 0.019788998658372976,
            "ave_precision_score": 0.8402918860172695,
            "fpr": 0.08342480790340286,
            "logloss": 0.6512996966604055,
            "mae": 0.31344176715807154,
            "precision": 0.8095238095238095,
            "recall": 0.6743215031315241
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8405328964802333,
            "auditor_fn_violation": 0.01723684210526316,
            "auditor_fp_violation": 0.03769972700630295,
            "ave_precision_score": 0.8409560357540582,
            "fpr": 0.16447368421052633,
            "logloss": 0.7447624810612506,
            "mae": 0.27183008787226653,
            "precision": 0.726775956284153,
            "recall": 0.84
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8238850785716356,
            "auditor_fn_violation": 0.033996457126881145,
            "auditor_fp_violation": 0.028064906289384888,
            "ave_precision_score": 0.8242406758923025,
            "fpr": 0.145993413830955,
            "logloss": 0.836702543295488,
            "mae": 0.2879982316162779,
            "precision": 0.740234375,
            "recall": 0.791231732776618
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.809767737948514,
            "auditor_fn_violation": 0.03887811634349031,
            "auditor_fp_violation": 0.024930747922437678,
            "ave_precision_score": 0.8100900739984208,
            "fpr": 0.10087719298245613,
            "logloss": 0.7336788671806688,
            "mae": 0.3026918364940764,
            "precision": 0.7750611246943765,
            "recall": 0.6673684210526316
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7826773211719671,
            "auditor_fn_violation": 0.04968501428836604,
            "auditor_fp_violation": 0.029165142090498843,
            "ave_precision_score": 0.7834945383750737,
            "fpr": 0.12184412733260154,
            "logloss": 0.8626471465333059,
            "mae": 0.3260262663344918,
            "precision": 0.7375886524822695,
            "recall": 0.651356993736952
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8509518203186883,
            "auditor_fn_violation": 0.03530009233610342,
            "auditor_fp_violation": 0.022923441326428207,
            "ave_precision_score": 0.851297711495678,
            "fpr": 0.10526315789473684,
            "logloss": 0.6376960364241586,
            "mae": 0.27039700061391064,
            "precision": 0.789010989010989,
            "recall": 0.7557894736842106
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8361746221528354,
            "auditor_fn_violation": 0.0446021600984488,
            "auditor_fp_violation": 0.027620238240435826,
            "ave_precision_score": 0.8365027041830215,
            "fpr": 0.10757409440175632,
            "logloss": 0.7432931460770235,
            "mae": 0.28550858769074805,
            "precision": 0.7767653758542141,
            "recall": 0.7118997912317327
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8439533526564695,
            "auditor_fn_violation": 0.028859649122807026,
            "auditor_fp_violation": 0.01890631900116424,
            "ave_precision_score": 0.844408038893383,
            "fpr": 0.09429824561403509,
            "logloss": 0.6763771986770858,
            "mae": 0.2707095280876965,
            "precision": 0.7990654205607477,
            "recall": 0.72
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8268042964361886,
            "auditor_fn_violation": 0.04480153264782788,
            "auditor_fp_violation": 0.02003293084522503,
            "ave_precision_score": 0.8271854132497787,
            "fpr": 0.09440175631174534,
            "logloss": 0.7915151061041541,
            "mae": 0.29069722701405093,
            "precision": 0.7917675544794189,
            "recall": 0.6826722338204593
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.839776897768421,
            "auditor_fn_violation": 0.03456832871652816,
            "auditor_fp_violation": 0.025480248103095262,
            "ave_precision_score": 0.8400378524029135,
            "fpr": 0.09539473684210527,
            "logloss": 0.7027340245970592,
            "mae": 0.2821462689572561,
            "precision": 0.7888349514563107,
            "recall": 0.6842105263157895
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8120027917382608,
            "auditor_fn_violation": 0.04954064106295361,
            "auditor_fp_violation": 0.021892913769971954,
            "ave_precision_score": 0.8123531651969665,
            "fpr": 0.10098792535675083,
            "logloss": 0.8286113221835799,
            "mae": 0.30262755438875566,
            "precision": 0.7793764988009593,
            "recall": 0.6784968684759917
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8410523995383186,
            "auditor_fn_violation": 0.029533702677747006,
            "auditor_fp_violation": 0.03933568188205067,
            "ave_precision_score": 0.8413096244402247,
            "fpr": 0.19517543859649122,
            "logloss": 0.7327688135658638,
            "mae": 0.28958692349023385,
            "precision": 0.696763202725724,
            "recall": 0.8610526315789474
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.8157793732506164,
            "auditor_fn_violation": 0.040839289683730975,
            "auditor_fp_violation": 0.03736227995283979,
            "ave_precision_score": 0.8161651010756672,
            "fpr": 0.20197585071350166,
            "logloss": 0.8385314899991017,
            "mae": 0.3130405770432356,
            "precision": 0.6816608996539792,
            "recall": 0.8225469728601252
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8436103876887657,
            "auditor_fn_violation": 0.03728070175438597,
            "auditor_fp_violation": 0.02085591553253844,
            "ave_precision_score": 0.8440481678031028,
            "fpr": 0.1074561403508772,
            "logloss": 0.657531103682962,
            "mae": 0.2732220466783812,
            "precision": 0.7864923747276689,
            "recall": 0.76
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8300112931402901,
            "auditor_fn_violation": 0.04339446660968126,
            "auditor_fp_violation": 0.02597877789974387,
            "ave_precision_score": 0.8303666590286178,
            "fpr": 0.10537870472008781,
            "logloss": 0.7662956349500382,
            "mae": 0.28960470537036365,
            "precision": 0.7782909930715936,
            "recall": 0.7035490605427975
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8315992190268056,
            "auditor_fn_violation": 0.014741458910433984,
            "auditor_fp_violation": 0.032121923802641617,
            "ave_precision_score": 0.8323003335492865,
            "fpr": 0.15021929824561403,
            "logloss": 0.7782284171674854,
            "mae": 0.26891520166989935,
            "precision": 0.732943469785575,
            "recall": 0.791578947368421
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8195658220330619,
            "auditor_fn_violation": 0.035194984061654236,
            "auditor_fp_violation": 0.02677155750701305,
            "ave_precision_score": 0.8200294945133325,
            "fpr": 0.13172338090010977,
            "logloss": 0.8714701653764307,
            "mae": 0.2787577155499767,
            "precision": 0.7505197505197505,
            "recall": 0.7536534446764092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 27690,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.8120416181091872,
            "auditor_fn_violation": 0.028536472760849495,
            "auditor_fp_violation": 0.010149443976072906,
            "ave_precision_score": 0.8124492386534214,
            "fpr": 0.05592105263157895,
            "logloss": 0.8607619434843712,
            "mae": 0.32509081040873467,
            "precision": 0.8271186440677966,
            "recall": 0.5136842105263157
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7960425407234212,
            "auditor_fn_violation": 0.04616964083149812,
            "auditor_fp_violation": 0.015385514493637436,
            "ave_precision_score": 0.7964865210612696,
            "fpr": 0.05817782656421515,
            "logloss": 0.9927238066739491,
            "mae": 0.3380471405741444,
            "precision": 0.8197278911564626,
            "recall": 0.5031315240083507
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8015484064775236,
            "auditor_fn_violation": 0.00866112650046168,
            "auditor_fp_violation": 0.012997310209161353,
            "ave_precision_score": 0.7689358072572935,
            "fpr": 0.1206140350877193,
            "logloss": 2.301132881596498,
            "mae": 0.28237118114144677,
            "precision": 0.755011135857461,
            "recall": 0.7136842105263158
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.779542502146735,
            "auditor_fn_violation": 0.03126024075954067,
            "auditor_fp_violation": 0.014264951010285808,
            "ave_precision_score": 0.7415122935588845,
            "fpr": 0.12623490669593854,
            "logloss": 2.6223284146803088,
            "mae": 0.2962868700722393,
            "precision": 0.7433035714285714,
            "recall": 0.6951983298538622
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8284187767032944,
            "auditor_fn_violation": 0.03280701754385965,
            "auditor_fp_violation": 0.03575514874141876,
            "ave_precision_score": 0.828705151805722,
            "fpr": 0.17543859649122806,
            "logloss": 0.7088486661876191,
            "mae": 0.29269238127768454,
            "precision": 0.7137745974955277,
            "recall": 0.84
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.8025146585969706,
            "auditor_fn_violation": 0.04616047427750368,
            "auditor_fp_violation": 0.037108183924868886,
            "ave_precision_score": 0.8038147775749322,
            "fpr": 0.19099890230515917,
            "logloss": 0.8246490104785195,
            "mae": 0.3167950391029672,
            "precision": 0.6881720430107527,
            "recall": 0.8016701461377871
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 27690,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.8493364831789485,
            "auditor_fn_violation": 0.006867497691597434,
            "auditor_fp_violation": 1.7563932715082945e-05,
            "ave_precision_score": 0.8495585567991518,
            "fpr": 0.021929824561403508,
            "logloss": 0.7456045605772653,
            "mae": 0.3569258633208791,
            "precision": 0.900497512437811,
            "recall": 0.38105263157894737
        },
        "train": {
            "accuracy": 0.6509330406147091,
            "auc_prc": 0.8364325795937693,
            "auditor_fn_violation": 0.0046199432131980135,
            "auditor_fp_violation": 0.002835711672155141,
            "ave_precision_score": 0.836731382949184,
            "fpr": 0.021953896816684963,
            "logloss": 0.7357979836738932,
            "mae": 0.3628577837509239,
            "precision": 0.900497512437811,
            "recall": 0.3778705636743215
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8376970789297186,
            "auditor_fn_violation": 0.03649353647276085,
            "auditor_fp_violation": 0.02753271909751496,
            "ave_precision_score": 0.8379987102343773,
            "fpr": 0.13048245614035087,
            "logloss": 0.6717620871594732,
            "mae": 0.2794880961526448,
            "precision": 0.75564681724846,
            "recall": 0.7747368421052632
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8089706611449963,
            "auditor_fn_violation": 0.041962192548049934,
            "auditor_fp_violation": 0.032702158799853644,
            "ave_precision_score": 0.8093363252466451,
            "fpr": 0.141602634467618,
            "logloss": 0.7942122223114795,
            "mae": 0.3032923186882123,
            "precision": 0.7323651452282157,
            "recall": 0.7369519832985386
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 27690,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.8518486580959832,
            "auditor_fn_violation": 0.009122807017543874,
            "auditor_fp_violation": 0.003964430527118712,
            "ave_precision_score": 0.8520641134652309,
            "fpr": 0.027412280701754384,
            "logloss": 0.6652987349008177,
            "mae": 0.34198412968132225,
            "precision": 0.8931623931623932,
            "recall": 0.44
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.8387355262875942,
            "auditor_fn_violation": 0.003859119231659461,
            "auditor_fp_violation": 0.0055443753303248385,
            "ave_precision_score": 0.8390319650966631,
            "fpr": 0.031833150384193196,
            "logloss": 0.6619762836700511,
            "mae": 0.34891417507181194,
            "precision": 0.8796680497925311,
            "recall": 0.44258872651356995
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.8076863227968253,
            "auditor_fn_violation": 0.03831948291782088,
            "auditor_fp_violation": 0.029028162511542017,
            "ave_precision_score": 0.8080245177035684,
            "fpr": 0.12609649122807018,
            "logloss": 0.7321294041499301,
            "mae": 0.3092611930042782,
            "precision": 0.742152466367713,
            "recall": 0.6968421052631579
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7796855864851597,
            "auditor_fn_violation": 0.04848648735359294,
            "auditor_fp_violation": 0.03046103183315038,
            "ave_precision_score": 0.7801073547317465,
            "fpr": 0.13611416026344675,
            "logloss": 0.8577812695118634,
            "mae": 0.3275531259482353,
            "precision": 0.7244444444444444,
            "recall": 0.6805845511482255
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8508573457020046,
            "auditor_fn_violation": 0.03530009233610342,
            "auditor_fp_violation": 0.022923441326428207,
            "ave_precision_score": 0.8512034481539439,
            "fpr": 0.10526315789473684,
            "logloss": 0.6380500634434839,
            "mae": 0.27032552809409444,
            "precision": 0.789010989010989,
            "recall": 0.7557894736842106
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8362075384943275,
            "auditor_fn_violation": 0.0446021600984488,
            "auditor_fp_violation": 0.027620238240435826,
            "ave_precision_score": 0.8365349777799926,
            "fpr": 0.10757409440175632,
            "logloss": 0.7435727130991783,
            "mae": 0.28547675853846344,
            "precision": 0.7767653758542141,
            "recall": 0.7118997912317327
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8379108468712537,
            "auditor_fn_violation": 0.033709602954755315,
            "auditor_fp_violation": 0.02913856437432254,
            "ave_precision_score": 0.8381722606363023,
            "fpr": 0.13815789473684212,
            "logloss": 0.6925212134970844,
            "mae": 0.27997949315890147,
            "precision": 0.7514792899408284,
            "recall": 0.8021052631578948
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.8101815396531604,
            "auditor_fn_violation": 0.04162532168875424,
            "auditor_fp_violation": 0.03479082814977436,
            "ave_precision_score": 0.8105443838814494,
            "fpr": 0.15477497255762898,
            "logloss": 0.8202808840816459,
            "mae": 0.30414397811785226,
            "precision": 0.718,
            "recall": 0.7494780793319415
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.840313813315604,
            "auditor_fn_violation": 0.035773314866112654,
            "auditor_fp_violation": 0.024903147456742544,
            "ave_precision_score": 0.8406302669812407,
            "fpr": 0.12828947368421054,
            "logloss": 0.6623097711606541,
            "mae": 0.2781619214419592,
            "precision": 0.7592592592592593,
            "recall": 0.7768421052631579
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8135207473656565,
            "auditor_fn_violation": 0.04270468342159961,
            "auditor_fp_violation": 0.030781192828393714,
            "ave_precision_score": 0.8138654648291875,
            "fpr": 0.13830954994511527,
            "logloss": 0.7823783861871392,
            "mae": 0.30118320447369873,
            "precision": 0.7364016736401674,
            "recall": 0.7348643006263048
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 27690,
        "test": {
            "accuracy": 0.6162280701754386,
            "auc_prc": 0.8528425942540425,
            "auditor_fn_violation": 0.0005078485687903975,
            "auditor_fp_violation": 0.010287446304548574,
            "ave_precision_score": 0.8531636876813629,
            "fpr": 0.37280701754385964,
            "logloss": 1.2386301356037581,
            "mae": 0.3732522823495521,
            "precision": 0.577639751552795,
            "recall": 0.9789473684210527
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.8450502973593395,
            "auditor_fn_violation": 0.0017095623199631506,
            "auditor_fp_violation": 0.012186445501483944,
            "ave_precision_score": 0.8452588458461703,
            "fpr": 0.36443468715697036,
            "logloss": 1.2431013031428255,
            "mae": 0.3703152821861866,
            "precision": 0.585,
            "recall": 0.9770354906054279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8317586809222546,
            "auditor_fn_violation": 0.03807248384118191,
            "auditor_fp_violation": 0.021691456903127383,
            "ave_precision_score": 0.8320229376117458,
            "fpr": 0.08333333333333333,
            "logloss": 0.7361826230405007,
            "mae": 0.29252473004802765,
            "precision": 0.8041237113402062,
            "recall": 0.6568421052631579
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.798910179441063,
            "auditor_fn_violation": 0.05412620969867246,
            "auditor_fp_violation": 0.024385595804366385,
            "ave_precision_score": 0.799340816980681,
            "fpr": 0.09549945115257959,
            "logloss": 0.8716130929541626,
            "mae": 0.31308791796057384,
            "precision": 0.779746835443038,
            "recall": 0.6430062630480167
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8601828026800182,
            "auditor_fn_violation": 0.011881348107109887,
            "auditor_fp_violation": 0.011097896342687381,
            "ave_precision_score": 0.860503434268682,
            "fpr": 0.07456140350877193,
            "logloss": 0.6363677812795127,
            "mae": 0.2610657110792972,
            "precision": 0.8278481012658228,
            "recall": 0.6884210526315789
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8433867042334062,
            "auditor_fn_violation": 0.029142766786824915,
            "auditor_fp_violation": 0.01844737163068667,
            "ave_precision_score": 0.8436579550137024,
            "fpr": 0.08232711306256861,
            "logloss": 0.7255593439231307,
            "mae": 0.2834499020725178,
            "precision": 0.8096446700507615,
            "recall": 0.6659707724425887
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.84989090950838,
            "auditor_fn_violation": 0.03237534626038782,
            "auditor_fp_violation": 0.01955367537837729,
            "ave_precision_score": 0.8502375406053277,
            "fpr": 0.08223684210526316,
            "logloss": 0.6698068886490866,
            "mae": 0.2757743319638599,
            "precision": 0.8152709359605911,
            "recall": 0.6968421052631579
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8348504322172576,
            "auditor_fn_violation": 0.04631859733390777,
            "auditor_fp_violation": 0.020957840387039073,
            "ave_precision_score": 0.8351797346519374,
            "fpr": 0.08342480790340286,
            "logloss": 0.7850114219845244,
            "mae": 0.2902044250280046,
            "precision": 0.8090452261306532,
            "recall": 0.6722338204592901
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8434155865882365,
            "auditor_fn_violation": 0.03398891966759003,
            "auditor_fp_violation": 0.02763057529407042,
            "ave_precision_score": 0.8436642091167312,
            "fpr": 0.13048245614035087,
            "logloss": 0.6691468667415069,
            "mae": 0.2763862473003353,
            "precision": 0.758130081300813,
            "recall": 0.7852631578947369
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8154480516219166,
            "auditor_fn_violation": 0.041962192548049934,
            "auditor_fp_violation": 0.032702158799853644,
            "ave_precision_score": 0.815827797335549,
            "fpr": 0.141602634467618,
            "logloss": 0.7889902423132281,
            "mae": 0.29939782527610165,
            "precision": 0.7323651452282157,
            "recall": 0.7369519832985386
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8303699901201445,
            "auditor_fn_violation": 0.032670821791320415,
            "auditor_fp_violation": 0.019809606969368503,
            "ave_precision_score": 0.8306455142753317,
            "fpr": 0.09320175438596491,
            "logloss": 0.7249706845969621,
            "mae": 0.2901695099513804,
            "precision": 0.7864321608040201,
            "recall": 0.6589473684210526
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7941548819619115,
            "auditor_fn_violation": 0.04916022907218432,
            "auditor_fp_violation": 0.020032930845225026,
            "ave_precision_score": 0.794529962189511,
            "fpr": 0.10976948408342481,
            "logloss": 0.8634491310497509,
            "mae": 0.31538052319372717,
            "precision": 0.7560975609756098,
            "recall": 0.6471816283924844
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8401506298967812,
            "auditor_fn_violation": 0.03848337950138504,
            "auditor_fp_violation": 0.02708358424665784,
            "ave_precision_score": 0.8404085220974246,
            "fpr": 0.12390350877192982,
            "logloss": 0.6712009318736866,
            "mae": 0.27967635241340405,
            "precision": 0.7600849256900213,
            "recall": 0.7536842105263157
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8130767661215469,
            "auditor_fn_violation": 0.046061933822063444,
            "auditor_fp_violation": 0.03144184250111802,
            "ave_precision_score": 0.8134524648363048,
            "fpr": 0.13391877058177826,
            "logloss": 0.7924614905278556,
            "mae": 0.3015281594558586,
            "precision": 0.7393162393162394,
            "recall": 0.7223382045929019
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8295531851468956,
            "auditor_fn_violation": 0.029235918744228998,
            "auditor_fp_violation": 0.02139537918021599,
            "ave_precision_score": 0.8299164773491601,
            "fpr": 0.10635964912280702,
            "logloss": 0.7273263482629857,
            "mae": 0.2841434478672178,
            "precision": 0.7728337236533958,
            "recall": 0.6947368421052632
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7897287096909489,
            "auditor_fn_violation": 0.04954751597844943,
            "auditor_fp_violation": 0.026893523600439083,
            "ave_precision_score": 0.7902178168831406,
            "fpr": 0.11855104281009879,
            "logloss": 0.857272364589032,
            "mae": 0.3135283420458965,
            "precision": 0.7522935779816514,
            "recall": 0.6847599164926931
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8301716482009633,
            "auditor_fn_violation": 0.04140581717451525,
            "auditor_fp_violation": 0.029386968565578705,
            "ave_precision_score": 0.830442614077016,
            "fpr": 0.12280701754385964,
            "logloss": 0.7176470590161856,
            "mae": 0.2856565803897114,
            "precision": 0.7570498915401301,
            "recall": 0.7347368421052631
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7989605696236108,
            "auditor_fn_violation": 0.04844065458362075,
            "auditor_fp_violation": 0.032666585355937715,
            "ave_precision_score": 0.7993649638589053,
            "fpr": 0.13611416026344675,
            "logloss": 0.8610900746580372,
            "mae": 0.3091392880910523,
            "precision": 0.7304347826086957,
            "recall": 0.7014613778705637
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8186860450224384,
            "auditor_fn_violation": 0.032366112650046165,
            "auditor_fp_violation": 0.03769721787305793,
            "ave_precision_score": 0.8189915772363049,
            "fpr": 0.17324561403508773,
            "logloss": 0.7208756747938506,
            "mae": 0.2977619815513341,
            "precision": 0.7127272727272728,
            "recall": 0.8252631578947368
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7891574008091748,
            "auditor_fn_violation": 0.04698775577550193,
            "auditor_fp_violation": 0.03275805992600724,
            "ave_precision_score": 0.790473845381892,
            "fpr": 0.17892425905598244,
            "logloss": 0.8501914889981594,
            "mae": 0.32514501953404984,
            "precision": 0.6958955223880597,
            "recall": 0.778705636743215
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7719298245614035,
            "auc_prc": 0.8445097468315087,
            "auditor_fn_violation": 0.015034626038781163,
            "auditor_fp_violation": 0.01665562648038862,
            "ave_precision_score": 0.8451331805340629,
            "fpr": 0.10964912280701754,
            "logloss": 1.1973249894728537,
            "mae": 0.2625174633649904,
            "precision": 0.7858672376873662,
            "recall": 0.7726315789473684
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8206490094542593,
            "auditor_fn_violation": 0.03308438500443432,
            "auditor_fp_violation": 0.019717851770541122,
            "ave_precision_score": 0.820779687362637,
            "fpr": 0.1141602634467618,
            "logloss": 1.8592333690046923,
            "mae": 0.2938647790417645,
            "precision": 0.767337807606264,
            "recall": 0.7160751565762005
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8436000920271456,
            "auditor_fn_violation": 0.03728070175438597,
            "auditor_fp_violation": 0.02085591553253844,
            "ave_precision_score": 0.8440377237919293,
            "fpr": 0.1074561403508772,
            "logloss": 0.6577033022896585,
            "mae": 0.273270139400033,
            "precision": 0.7864923747276689,
            "recall": 0.76
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8300745020819764,
            "auditor_fn_violation": 0.042897181055482854,
            "auditor_fp_violation": 0.02597877789974387,
            "ave_precision_score": 0.8304236045237836,
            "fpr": 0.10537870472008781,
            "logloss": 0.7664031884464002,
            "mae": 0.28958606531014075,
            "precision": 0.7788018433179723,
            "recall": 0.7056367432150313
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8449696160223881,
            "auditor_fn_violation": 0.0358056325023084,
            "auditor_fp_violation": 0.025113914649323544,
            "ave_precision_score": 0.84521716668474,
            "fpr": 0.12938596491228072,
            "logloss": 0.6635328195111639,
            "mae": 0.2755694501620212,
            "precision": 0.7591836734693878,
            "recall": 0.783157894736842
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.818050112270764,
            "auditor_fn_violation": 0.042250938998874804,
            "auditor_fp_violation": 0.03187888766922796,
            "ave_precision_score": 0.8184139837191176,
            "fpr": 0.13830954994511527,
            "logloss": 0.7820219031154019,
            "mae": 0.2981574713636192,
            "precision": 0.7369519832985386,
            "recall": 0.7369519832985386
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8548155586272359,
            "auditor_fn_violation": 0.03878347183748846,
            "auditor_fp_violation": 0.04031173471436028,
            "ave_precision_score": 0.8551223242601637,
            "fpr": 0.1337719298245614,
            "logloss": 0.7346726473090512,
            "mae": 0.2707832986176461,
            "precision": 0.7555110220440882,
            "recall": 0.7936842105263158
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8412508074861064,
            "auditor_fn_violation": 0.04444862031904192,
            "auditor_fp_violation": 0.031040370776924016,
            "ave_precision_score": 0.8416138712508544,
            "fpr": 0.13172338090010977,
            "logloss": 1.0829431600040094,
            "mae": 0.2875577880399549,
            "precision": 0.7560975609756098,
            "recall": 0.7766179540709812
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8492257395577388,
            "auditor_fn_violation": 0.025680978762696222,
            "auditor_fp_violation": 0.024193062748404204,
            "ave_precision_score": 0.8494607348100913,
            "fpr": 0.12719298245614036,
            "logloss": 0.6436712840573572,
            "mae": 0.2733384645359784,
            "precision": 0.7627811860940695,
            "recall": 0.7852631578947369
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.824293014916852,
            "auditor_fn_violation": 0.038565984293109735,
            "auditor_fp_violation": 0.029345550270358182,
            "ave_precision_score": 0.8246127028268948,
            "fpr": 0.132821075740944,
            "logloss": 0.7424962827855728,
            "mae": 0.29427178432717266,
            "precision": 0.7457983193277311,
            "recall": 0.7411273486430062
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8423945478890581,
            "auditor_fn_violation": 0.027393813481071106,
            "auditor_fp_violation": 0.03977227106668273,
            "ave_precision_score": 0.8426505156163105,
            "fpr": 0.1962719298245614,
            "logloss": 0.7529742353216422,
            "mae": 0.2865397993544342,
            "precision": 0.6971235194585449,
            "recall": 0.8673684210526316
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.8151145572887891,
            "auditor_fn_violation": 0.04077970708276711,
            "auditor_fp_violation": 0.04229174289547506,
            "ave_precision_score": 0.8154668278454721,
            "fpr": 0.2052689352360044,
            "logloss": 0.8762161069909347,
            "mae": 0.31398764322915695,
            "precision": 0.6797945205479452,
            "recall": 0.8288100208768268
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8422700961587508,
            "auditor_fn_violation": 0.03360572483841182,
            "auditor_fp_violation": 0.024714962463366658,
            "ave_precision_score": 0.8425247082018871,
            "fpr": 0.1206140350877193,
            "logloss": 0.6666554956466765,
            "mae": 0.2796805557038913,
            "precision": 0.7649572649572649,
            "recall": 0.7536842105263157
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.815098452876292,
            "auditor_fn_violation": 0.04389862707937549,
            "auditor_fp_violation": 0.028115725494979066,
            "ave_precision_score": 0.8154497791625469,
            "fpr": 0.132821075740944,
            "logloss": 0.7884862114258571,
            "mae": 0.30327440844246945,
            "precision": 0.7392241379310345,
            "recall": 0.7160751565762005
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.8162013234909238,
            "auditor_fn_violation": 0.011754385964912281,
            "auditor_fp_violation": 0.008829639889196682,
            "ave_precision_score": 0.8165166849184187,
            "fpr": 0.20065789473684212,
            "logloss": 0.7810830175747807,
            "mae": 0.3157147136318871,
            "precision": 0.6855670103092784,
            "recall": 0.84
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.8111788304641222,
            "auditor_fn_violation": 0.01451065497319929,
            "auditor_fp_violation": 0.009315160385412857,
            "ave_precision_score": 0.8115093402764177,
            "fpr": 0.20856201975850713,
            "logloss": 0.7984891131239003,
            "mae": 0.3163315109194838,
            "precision": 0.6828046744574291,
            "recall": 0.8538622129436325
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8110407277896436,
            "auditor_fn_violation": 0.016239612188365653,
            "auditor_fp_violation": 0.016063471034565818,
            "ave_precision_score": 0.7927373291044513,
            "fpr": 0.16447368421052633,
            "logloss": 1.479946475749857,
            "mae": 0.2805100253320377,
            "precision": 0.7321428571428571,
            "recall": 0.8631578947368421
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7868863881760781,
            "auditor_fn_violation": 0.01674500250934416,
            "auditor_fp_violation": 0.017217546855307554,
            "ave_precision_score": 0.7687803802630757,
            "fpr": 0.18880351262349068,
            "logloss": 1.6503056031810752,
            "mae": 0.30036126512435546,
            "precision": 0.6987740805604203,
            "recall": 0.8329853862212944
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8439870067524926,
            "auditor_fn_violation": 0.031934441366574334,
            "auditor_fp_violation": 0.022163173953189615,
            "ave_precision_score": 0.8444244406300798,
            "fpr": 0.10307017543859649,
            "logloss": 0.6580363722471382,
            "mae": 0.2726890293939752,
            "precision": 0.7924944812362031,
            "recall": 0.7557894736842106
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.830684448606106,
            "auditor_fn_violation": 0.04339446660968126,
            "auditor_fp_violation": 0.02542484855876733,
            "ave_precision_score": 0.8310234818809517,
            "fpr": 0.10318331503841932,
            "logloss": 0.7683122278910721,
            "mae": 0.2891472316364263,
            "precision": 0.7819025522041764,
            "recall": 0.7035490605427975
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8261462899831737,
            "auditor_fn_violation": 0.0278162511542013,
            "auditor_fp_violation": 0.024160444016219043,
            "ave_precision_score": 0.8264445161073388,
            "fpr": 0.13815789473684212,
            "logloss": 0.7466777526558522,
            "mae": 0.28590157808840305,
            "precision": 0.7418032786885246,
            "recall": 0.7621052631578947
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7945676511064648,
            "auditor_fn_violation": 0.041909484862581906,
            "auditor_fp_violation": 0.032232081148107494,
            "ave_precision_score": 0.794964955735318,
            "fpr": 0.1437980241492865,
            "logloss": 0.8760627418694347,
            "mae": 0.31102985400761063,
            "precision": 0.7276507276507277,
            "recall": 0.7306889352818372
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8413147307491676,
            "auditor_fn_violation": 0.026133425669436747,
            "auditor_fp_violation": 0.021844514031073105,
            "ave_precision_score": 0.841802684116413,
            "fpr": 0.10307017543859649,
            "logloss": 0.6645369229020966,
            "mae": 0.27568768114888237,
            "precision": 0.7924944812362031,
            "recall": 0.7557894736842106
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8280503040610983,
            "auditor_fn_violation": 0.04244801990975528,
            "auditor_fp_violation": 0.027564337114282238,
            "ave_precision_score": 0.828371117749325,
            "fpr": 0.10537870472008781,
            "logloss": 0.7727003635502966,
            "mae": 0.29319724629811617,
            "precision": 0.7762237762237763,
            "recall": 0.6951983298538622
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8335596534000279,
            "auditor_fn_violation": 0.03958910433979686,
            "auditor_fp_violation": 0.02989632261431612,
            "ave_precision_score": 0.8338264634921647,
            "fpr": 0.1162280701754386,
            "logloss": 0.7066596387822697,
            "mae": 0.28425249487780757,
            "precision": 0.7675438596491229,
            "recall": 0.7368421052631579
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.8036129453633636,
            "auditor_fn_violation": 0.04915106251818988,
            "auditor_fp_violation": 0.03309092572264911,
            "ave_precision_score": 0.8039982094104806,
            "fpr": 0.13062568605927552,
            "logloss": 0.8477216755395627,
            "mae": 0.30668470022859257,
            "precision": 0.7373068432671082,
            "recall": 0.697286012526096
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7763157894736842,
            "auc_prc": 0.8626885347826656,
            "auditor_fn_violation": 0.00023084025854109162,
            "auditor_fp_violation": 0.018005540166204984,
            "ave_precision_score": 0.8629635655569498,
            "fpr": 0.08662280701754387,
            "logloss": 0.47850308273359393,
            "mae": 0.291798636750747,
            "precision": 0.8158508158508159,
            "recall": 0.7368421052631579
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8685314834697688,
            "auditor_fn_violation": 0.013836913254607919,
            "auditor_fp_violation": 0.012013660202463715,
            "ave_precision_score": 0.8687298223240941,
            "fpr": 0.09220636663007684,
            "logloss": 0.4720566495238333,
            "mae": 0.2914730945240929,
            "precision": 0.8108108108108109,
            "recall": 0.7515657620041754
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 27690,
        "test": {
            "accuracy": 0.48026315789473684,
            "auc_prc": 0.4212222222222222,
            "auditor_fn_violation": 0.0006048014773776502,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5244035087719298,
            "fpr": 0.0,
            "logloss": 17.808046507739355,
            "mae": 0.5194883701851563,
            "precision": 1.0,
            "recall": 0.002105263157894737
        },
        "train": {
            "accuracy": 0.47859495060373214,
            "auc_prc": 0.4362588580897844,
            "auditor_fn_violation": 0.0007814487280260604,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5308526628015169,
            "fpr": 0.0,
            "logloss": 17.836200157951712,
            "mae": 0.522897376957463,
            "precision": 1.0,
            "recall": 0.008350730688935281
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 27690,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.8506425880284856,
            "auditor_fn_violation": 0.008236380424746084,
            "auditor_fp_violation": 0.013574410855514077,
            "ave_precision_score": 0.8510397453985472,
            "fpr": 0.09758771929824561,
            "logloss": 0.565398977918019,
            "mae": 0.2703751989529337,
            "precision": 0.7986425339366516,
            "recall": 0.7431578947368421
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8531497438252854,
            "auditor_fn_violation": 0.01741645258943693,
            "auditor_fp_violation": 0.014259869089726391,
            "ave_precision_score": 0.8534000739694036,
            "fpr": 0.10647639956092206,
            "logloss": 0.565068959995958,
            "mae": 0.2762356464173205,
            "precision": 0.7829977628635347,
            "recall": 0.7306889352818372
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.8342953043977182,
            "auditor_fn_violation": 0.034268236380424755,
            "auditor_fp_violation": 0.011629832590629894,
            "ave_precision_score": 0.8345579278030002,
            "fpr": 0.05592105263157895,
            "logloss": 0.8234409614608342,
            "mae": 0.3063668673510089,
            "precision": 0.8370607028753994,
            "recall": 0.5515789473684211
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.8020037718832791,
            "auditor_fn_violation": 0.04572277132426914,
            "auditor_fp_violation": 0.01793663861446518,
            "ave_precision_score": 0.8024106647862632,
            "fpr": 0.06256860592755215,
            "logloss": 0.9544087593543482,
            "mae": 0.32140530696927705,
            "precision": 0.821875,
            "recall": 0.5490605427974948
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8410055054104729,
            "auditor_fn_violation": 0.03401200369344414,
            "auditor_fp_violation": 0.02735457063711912,
            "ave_precision_score": 0.8412615511816295,
            "fpr": 0.12609649122807018,
            "logloss": 0.6695985655790592,
            "mae": 0.2787459009628846,
            "precision": 0.7573839662447257,
            "recall": 0.7557894736842106
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.8141354366276812,
            "auditor_fn_violation": 0.04457695207496409,
            "auditor_fp_violation": 0.031200451274545682,
            "ave_precision_score": 0.8144980313136079,
            "fpr": 0.1350164654226125,
            "logloss": 0.7908068293656425,
            "mae": 0.3008321669591235,
            "precision": 0.7388535031847133,
            "recall": 0.7265135699373695
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 27690,
        "test": {
            "accuracy": 0.5964912280701754,
            "auc_prc": 0.836211274395248,
            "auditor_fn_violation": 0.01069713758079409,
            "auditor_fp_violation": 0.039102332490264576,
            "ave_precision_score": 0.8364707116554533,
            "fpr": 0.3881578947368421,
            "logloss": 1.6457066834483394,
            "mae": 0.391410579223606,
            "precision": 0.5656441717791411,
            "recall": 0.9705263157894737
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.8127139825848491,
            "auditor_fn_violation": 0.01760436694632295,
            "auditor_fp_violation": 0.04452524698133919,
            "ave_precision_score": 0.8129542611410896,
            "fpr": 0.3918770581778266,
            "logloss": 1.7055331014086557,
            "mae": 0.4048953841695613,
            "precision": 0.5608856088560885,
            "recall": 0.9519832985386222
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8367843526780856,
            "auditor_fn_violation": 0.037243767313019406,
            "auditor_fp_violation": 0.022692601067887112,
            "ave_precision_score": 0.8370466633466938,
            "fpr": 0.11403508771929824,
            "logloss": 0.6980750457337209,
            "mae": 0.2814029819447238,
            "precision": 0.7704194260485652,
            "recall": 0.7347368421052631
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.8082123803953564,
            "auditor_fn_violation": 0.048188574348773626,
            "auditor_fp_violation": 0.03021709964629833,
            "ave_precision_score": 0.8085764849648278,
            "fpr": 0.1251372118551043,
            "logloss": 0.8314095525693254,
            "mae": 0.3037630749105434,
            "precision": 0.7461024498886414,
            "recall": 0.6993736951983298
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 27690,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8283972985224042,
            "auditor_fn_violation": 0.029602954755309333,
            "auditor_fp_violation": 0.03073437311815007,
            "ave_precision_score": 0.8287447632187499,
            "fpr": 0.15789473684210525,
            "logloss": 0.7363462656103676,
            "mae": 0.28418306850471553,
            "precision": 0.7308411214953271,
            "recall": 0.8231578947368421
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.8010819153599702,
            "auditor_fn_violation": 0.0425648934731844,
            "auditor_fp_violation": 0.03554549335284791,
            "ave_precision_score": 0.8015040289780108,
            "fpr": 0.16575192096597147,
            "logloss": 0.8507534966568828,
            "mae": 0.31090178289714054,
            "precision": 0.7073643410852714,
            "recall": 0.7620041753653445
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8466240644721901,
            "auditor_fn_violation": 0.032777008310249316,
            "auditor_fp_violation": 0.02641113653699467,
            "ave_precision_score": 0.8468667071974081,
            "fpr": 0.125,
            "logloss": 0.6629366412745192,
            "mae": 0.27621086434020486,
            "precision": 0.7605042016806722,
            "recall": 0.7621052631578947
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.8175625014464969,
            "auditor_fn_violation": 0.04468465908439876,
            "auditor_fp_violation": 0.03171626621132659,
            "ave_precision_score": 0.8179052535002305,
            "fpr": 0.132821075740944,
            "logloss": 0.7953223590357025,
            "mae": 0.2998625538697336,
            "precision": 0.7392241379310345,
            "recall": 0.7160751565762005
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8401532749828928,
            "auditor_fn_violation": 0.03848337950138504,
            "auditor_fp_violation": 0.02708358424665784,
            "ave_precision_score": 0.8404111657960573,
            "fpr": 0.12390350877192982,
            "logloss": 0.671202426734725,
            "mae": 0.27967643123565505,
            "precision": 0.7600849256900213,
            "recall": 0.7536842105263157
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8130767661215469,
            "auditor_fn_violation": 0.046061933822063444,
            "auditor_fp_violation": 0.03144184250111802,
            "ave_precision_score": 0.8134524648363048,
            "fpr": 0.13391877058177826,
            "logloss": 0.7924640012782671,
            "mae": 0.3015281296890559,
            "precision": 0.7393162393162394,
            "recall": 0.7223382045929019
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 27690,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7991591092131827,
            "auditor_fn_violation": 0.036927516158818105,
            "auditor_fp_violation": 0.025962001686137548,
            "ave_precision_score": 0.7995117852894912,
            "fpr": 0.09210526315789473,
            "logloss": 0.8277365183330577,
            "mae": 0.32266657190554143,
            "precision": 0.768595041322314,
            "recall": 0.5873684210526315
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7660445535714033,
            "auditor_fn_violation": 0.05297580717237018,
            "auditor_fp_violation": 0.02766343456519088,
            "ave_precision_score": 0.7664988831404888,
            "fpr": 0.09549945115257959,
            "logloss": 0.9769671972415572,
            "mae": 0.33842245228197954,
            "precision": 0.7622950819672131,
            "recall": 0.5824634655532359
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 27690,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8251875276788223,
            "auditor_fn_violation": 0.0227539242843952,
            "auditor_fp_violation": 0.01520785659801678,
            "ave_precision_score": 0.8256048355497011,
            "fpr": 0.09210526315789473,
            "logloss": 0.6926786905827953,
            "mae": 0.29339709883266357,
            "precision": 0.7889447236180904,
            "recall": 0.6610526315789473
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8084724106332505,
            "auditor_fn_violation": 0.04085074787622403,
            "auditor_fp_violation": 0.01847786315404318,
            "ave_precision_score": 0.808896183883421,
            "fpr": 0.07903402854006586,
            "logloss": 0.789988337494166,
            "mae": 0.3086749304458597,
            "precision": 0.8110236220472441,
            "recall": 0.6450939457202505
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8489143047051321,
            "auditor_fn_violation": 0.02715604801477378,
            "auditor_fp_violation": 0.022409069011200773,
            "ave_precision_score": 0.8491663961228878,
            "fpr": 0.09758771929824561,
            "logloss": 0.660569598433822,
            "mae": 0.2737319732657523,
            "precision": 0.7920560747663551,
            "recall": 0.7136842105263158
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8234960008446583,
            "auditor_fn_violation": 0.04601610105209123,
            "auditor_fp_violation": 0.02409846729275928,
            "ave_precision_score": 0.823808376644497,
            "fpr": 0.10976948408342481,
            "logloss": 0.7621965476531903,
            "mae": 0.2945366699875534,
            "precision": 0.7706422018348624,
            "recall": 0.7014613778705637
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 27690,
        "test": {
            "accuracy": 0.5274122807017544,
            "auc_prc": 0.5518931570713402,
            "auditor_fn_violation": 0.00540166204986151,
            "auditor_fp_violation": 0.002453932313621583,
            "ave_precision_score": 0.5809306071173501,
            "fpr": 0.03070175438596491,
            "logloss": 14.508754822411527,
            "mae": 0.4785977312672956,
            "precision": 0.72,
            "recall": 0.15157894736842106
        },
        "train": {
            "accuracy": 0.5367727771679474,
            "auc_prc": 0.5994057150444058,
            "auditor_fn_violation": 0.004704733837646599,
            "auditor_fp_violation": 0.00244440378907997,
            "ave_precision_score": 0.6001401720199896,
            "fpr": 0.020856201975850714,
            "logloss": 13.876059235905226,
            "mae": 0.4706769496759913,
            "precision": 0.8,
            "recall": 0.15866388308977036
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 27690,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.84860830797919,
            "auditor_fn_violation": 0.0091528162511542,
            "auditor_fp_violation": 0.010440503432494284,
            "ave_precision_score": 0.8489275913195011,
            "fpr": 0.08333333333333333,
            "logloss": 0.5740809854558925,
            "mae": 0.271987223291673,
            "precision": 0.8173076923076923,
            "recall": 0.7157894736842105
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8428232397730848,
            "auditor_fn_violation": 0.01862414607820446,
            "auditor_fp_violation": 0.01396511769728016,
            "ave_precision_score": 0.8431519268024946,
            "fpr": 0.09220636663007684,
            "logloss": 0.5942864588958034,
            "mae": 0.2822656348025413,
            "precision": 0.7956204379562044,
            "recall": 0.6826722338204593
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.826889198148617,
            "auditor_fn_violation": 0.04253462603878117,
            "auditor_fp_violation": 0.02806465534545747,
            "ave_precision_score": 0.8271806266554987,
            "fpr": 0.12280701754385964,
            "logloss": 0.6797719892989894,
            "mae": 0.28851955138195473,
            "precision": 0.7586206896551724,
            "recall": 0.7410526315789474
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.8019349985657056,
            "auditor_fn_violation": 0.046545469545270175,
            "auditor_fp_violation": 0.028700146359312118,
            "ave_precision_score": 0.8023626298367762,
            "fpr": 0.1350164654226125,
            "logloss": 0.8066428041539806,
            "mae": 0.3117730985821968,
            "precision": 0.7314410480349345,
            "recall": 0.6993736951983298
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8471297051775093,
            "auditor_fn_violation": 0.026297322253000924,
            "auditor_fp_violation": 0.027158858244008197,
            "ave_precision_score": 0.8473744076035931,
            "fpr": 0.12390350877192982,
            "logloss": 0.6517943039641556,
            "mae": 0.27364820090051367,
            "precision": 0.7645833333333333,
            "recall": 0.7726315789473684
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8190788505690225,
            "auditor_fn_violation": 0.04021825565060763,
            "auditor_fp_violation": 0.028844981095255528,
            "ave_precision_score": 0.8194221564935118,
            "fpr": 0.12733260153677278,
            "logloss": 0.7625780001580724,
            "mae": 0.29741266935137856,
            "precision": 0.7521367521367521,
            "recall": 0.7348643006263048
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8435932385048857,
            "auditor_fn_violation": 0.03728070175438597,
            "auditor_fp_violation": 0.02085591553253844,
            "ave_precision_score": 0.8440311335682017,
            "fpr": 0.1074561403508772,
            "logloss": 0.6575369438596472,
            "mae": 0.2732192568944461,
            "precision": 0.7864923747276689,
            "recall": 0.76
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8299968610087758,
            "auditor_fn_violation": 0.042897181055482854,
            "auditor_fp_violation": 0.02597877789974387,
            "ave_precision_score": 0.8303460658335472,
            "fpr": 0.10537870472008781,
            "logloss": 0.7662633032158019,
            "mae": 0.28960705848974266,
            "precision": 0.7788018433179723,
            "recall": 0.7056367432150313
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 27690,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.8026446058572435,
            "auditor_fn_violation": 0.034552169898430296,
            "auditor_fp_violation": 0.014552972821068691,
            "ave_precision_score": 0.8029881028265073,
            "fpr": 0.06140350877192982,
            "logloss": 0.885722473740406,
            "mae": 0.33340817049209287,
            "precision": 0.8101694915254237,
            "recall": 0.5031578947368421
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.7693424220412114,
            "auditor_fn_violation": 0.04468236744590015,
            "auditor_fp_violation": 0.0181424563971216,
            "ave_precision_score": 0.7697887324002683,
            "fpr": 0.06586169045005488,
            "logloss": 1.0186484393758723,
            "mae": 0.34585386950810953,
            "precision": 0.801980198019802,
            "recall": 0.5073068893528184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8016777794141166,
            "auditor_fn_violation": 0.03780470914127424,
            "auditor_fp_violation": 0.02694056365169216,
            "ave_precision_score": 0.8020394344050092,
            "fpr": 0.13048245614035087,
            "logloss": 0.7231777012993206,
            "mae": 0.3034290687218414,
            "precision": 0.7484143763213531,
            "recall": 0.7452631578947368
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7695578412309738,
            "auditor_fn_violation": 0.04568839674678999,
            "auditor_fp_violation": 0.02959710533804936,
            "ave_precision_score": 0.770911560506036,
            "fpr": 0.14928649835345773,
            "logloss": 0.8719712677413515,
            "mae": 0.33496536194519577,
            "precision": 0.711864406779661,
            "recall": 0.7014613778705637
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8517803844793592,
            "auditor_fn_violation": 0.011606648199445985,
            "auditor_fp_violation": 0.0119484925127464,
            "ave_precision_score": 0.852121475892037,
            "fpr": 0.09978070175438597,
            "logloss": 0.5499991147041381,
            "mae": 0.2725576419619997,
            "precision": 0.7945823927765236,
            "recall": 0.7410526315789474
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8519018841862818,
            "auditor_fn_violation": 0.018997683153477905,
            "auditor_fp_violation": 0.010966784567223645,
            "ave_precision_score": 0.852189764287552,
            "fpr": 0.10318331503841932,
            "logloss": 0.5550723171362334,
            "mae": 0.27949088757168133,
            "precision": 0.7906458797327395,
            "recall": 0.7411273486430062
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8112522173579486,
            "auditor_fn_violation": 0.03782548476454295,
            "auditor_fp_violation": 0.022672528001927015,
            "ave_precision_score": 0.8115849911889437,
            "fpr": 0.09210526315789473,
            "logloss": 0.7409651475253904,
            "mae": 0.30218259874288184,
            "precision": 0.7910447761194029,
            "recall": 0.6694736842105263
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.780612486727142,
            "auditor_fn_violation": 0.05080104223718917,
            "auditor_fp_violation": 0.028936455665325045,
            "ave_precision_score": 0.7819559337802477,
            "fpr": 0.11964873765093303,
            "logloss": 0.87654074561251,
            "mae": 0.3275085610747742,
            "precision": 0.7392344497607656,
            "recall": 0.6450939457202505
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8379101197511264,
            "auditor_fn_violation": 0.034021237303785784,
            "auditor_fp_violation": 0.028579027660684896,
            "ave_precision_score": 0.8381714941219853,
            "fpr": 0.1337719298245614,
            "logloss": 0.6912623667910064,
            "mae": 0.2798648870445687,
            "precision": 0.7550200803212851,
            "recall": 0.791578947368421
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.8099420663050881,
            "auditor_fn_violation": 0.04121970167450026,
            "auditor_fp_violation": 0.03192208399398301,
            "ave_precision_score": 0.8103037180879733,
            "fpr": 0.150384193194292,
            "logloss": 0.8197319005075607,
            "mae": 0.3038237289153664,
            "precision": 0.7209775967413442,
            "recall": 0.7390396659707724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 27690,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.8811968536722011,
            "auditor_fn_violation": 0.011765927977839345,
            "auditor_fp_violation": 0.006546328636235898,
            "ave_precision_score": 0.881418992278411,
            "fpr": 0.017543859649122806,
            "logloss": 0.6998522964751822,
            "mae": 0.32306899477106,
            "precision": 0.9330543933054394,
            "recall": 0.4694736842105263
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.8657459977299562,
            "auditor_fn_violation": 0.017560825814849367,
            "auditor_fp_violation": 0.005907732650323212,
            "ave_precision_score": 0.8660147766033988,
            "fpr": 0.02305159165751921,
            "logloss": 0.7093737629322574,
            "mae": 0.33720349570839864,
            "precision": 0.9090909090909091,
            "recall": 0.4384133611691023
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8365321321891253,
            "auditor_fn_violation": 0.038277931671283474,
            "auditor_fp_violation": 0.026258079409048944,
            "ave_precision_score": 0.8367957377344936,
            "fpr": 0.12609649122807018,
            "logloss": 0.6942862352842415,
            "mae": 0.280337416865161,
            "precision": 0.7599164926931107,
            "recall": 0.7663157894736842
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.8077890404304056,
            "auditor_fn_violation": 0.04433633003261002,
            "auditor_fp_violation": 0.03259543846810587,
            "ave_precision_score": 0.8081639857074967,
            "fpr": 0.14489571899012074,
            "logloss": 0.8250746267771387,
            "mae": 0.30432337760182926,
            "precision": 0.7255717255717256,
            "recall": 0.7286012526096033
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 27690,
        "test": {
            "accuracy": 0.6392543859649122,
            "auc_prc": 0.8331297816418557,
            "auditor_fn_violation": 0.011327331486611266,
            "auditor_fp_violation": 0.04385713998956201,
            "ave_precision_score": 0.833426527518875,
            "fpr": 0.3366228070175439,
            "logloss": 1.2476167152603836,
            "mae": 0.35817766857819683,
            "precision": 0.5960526315789474,
            "recall": 0.9536842105263158
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.8157823638092037,
            "auditor_fn_violation": 0.023828457108548043,
            "auditor_fp_violation": 0.04376295889742654,
            "ave_precision_score": 0.816128530874878,
            "fpr": 0.34796926454445665,
            "logloss": 1.2907241740248498,
            "mae": 0.374806460362911,
            "precision": 0.583989501312336,
            "recall": 0.9290187891440501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8483310697147892,
            "auditor_fn_violation": 0.003351800554016631,
            "auditor_fp_violation": 0.01140902886506885,
            "ave_precision_score": 0.848577462727506,
            "fpr": 0.09539473684210527,
            "logloss": 0.5734404183291867,
            "mae": 0.2865682938243146,
            "precision": 0.7918660287081339,
            "recall": 0.6968421052631579
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8423916452294382,
            "auditor_fn_violation": 0.011524650009510312,
            "auditor_fp_violation": 0.01481888035126235,
            "ave_precision_score": 0.8426519217123258,
            "fpr": 0.09879253567508232,
            "logloss": 0.575184691508087,
            "mae": 0.2866794182676248,
            "precision": 0.7911832946635731,
            "recall": 0.7118997912317327
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8479974407711826,
            "auditor_fn_violation": 0.034439058171745154,
            "auditor_fp_violation": 0.019649022441687744,
            "ave_precision_score": 0.8482274141361901,
            "fpr": 0.07456140350877193,
            "logloss": 0.6972435692348227,
            "mae": 0.28053817563191297,
            "precision": 0.821522309711286,
            "recall": 0.6589473684210526
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8169661674323874,
            "auditor_fn_violation": 0.04341509135616875,
            "auditor_fp_violation": 0.018643025572224255,
            "ave_precision_score": 0.8173196162670291,
            "fpr": 0.0845225027442371,
            "logloss": 0.813650236887786,
            "mae": 0.3006443605900333,
            "precision": 0.8025641025641026,
            "recall": 0.6534446764091858
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8574068190115833,
            "auditor_fn_violation": 0.02499769159741459,
            "auditor_fp_violation": 0.028167529808502956,
            "ave_precision_score": 0.8576401220403604,
            "fpr": 0.15789473684210525,
            "logloss": 0.6524465382423683,
            "mae": 0.26997684875133293,
            "precision": 0.7362637362637363,
            "recall": 0.8463157894736842
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.8304623119126435,
            "auditor_fn_violation": 0.036746423325213294,
            "auditor_fp_violation": 0.03454689596292231,
            "ave_precision_score": 0.8307698001731347,
            "fpr": 0.1712403951701427,
            "logloss": 0.7479065957807925,
            "mae": 0.29468359728686055,
            "precision": 0.708411214953271,
            "recall": 0.791231732776618
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8362657723061162,
            "auditor_fn_violation": 0.04093028624192059,
            "auditor_fp_violation": 0.01738327512144205,
            "ave_precision_score": 0.8366587485259416,
            "fpr": 0.051535087719298246,
            "logloss": 0.7583086220584687,
            "mae": 0.3115843817732774,
            "precision": 0.8567073170731707,
            "recall": 0.5915789473684211
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8258704138156595,
            "auditor_fn_violation": 0.0563788903428062,
            "auditor_fp_violation": 0.01549985770622434,
            "ave_precision_score": 0.8262201118842567,
            "fpr": 0.04720087815587267,
            "logloss": 0.8840856888117121,
            "mae": 0.32376381271858135,
            "precision": 0.8621794871794872,
            "recall": 0.5615866388308977
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8497441662484689,
            "auditor_fn_violation": 0.02493074792243768,
            "auditor_fp_violation": 0.026689650327190978,
            "ave_precision_score": 0.8499797525946191,
            "fpr": 0.13706140350877194,
            "logloss": 0.6419806852617356,
            "mae": 0.27357382802559266,
            "precision": 0.7534516765285996,
            "recall": 0.8042105263157895
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8250467624784154,
            "auditor_fn_violation": 0.03831619569676123,
            "auditor_fp_violation": 0.0332306785380331,
            "ave_precision_score": 0.8253666179552656,
            "fpr": 0.1437980241492865,
            "logloss": 0.7399277154003979,
            "mae": 0.2950161335465565,
            "precision": 0.7364185110663984,
            "recall": 0.7640918580375783
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8502682866295408,
            "auditor_fn_violation": 0.03304478301015698,
            "auditor_fp_violation": 0.020123248624994983,
            "ave_precision_score": 0.8506138124388077,
            "fpr": 0.07675438596491228,
            "logloss": 0.6746516961874431,
            "mae": 0.277182257437361,
            "precision": 0.8205128205128205,
            "recall": 0.6736842105263158
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8355844579182177,
            "auditor_fn_violation": 0.046346096995891095,
            "auditor_fp_violation": 0.020673252835711677,
            "ave_precision_score": 0.8359024106929682,
            "fpr": 0.07903402854006586,
            "logloss": 0.787146297247791,
            "mae": 0.29125664563716086,
            "precision": 0.8153846153846154,
            "recall": 0.6638830897703549
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8417776045300458,
            "auditor_fn_violation": 0.029374422899353648,
            "auditor_fp_violation": 0.028400879200289053,
            "ave_precision_score": 0.8420395266412394,
            "fpr": 0.1337719298245614,
            "logloss": 0.6656260056539907,
            "mae": 0.27696286968574424,
            "precision": 0.7540322580645161,
            "recall": 0.7873684210526316
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.8112655450401134,
            "auditor_fn_violation": 0.04380237826243386,
            "auditor_fp_violation": 0.03546926454445665,
            "ave_precision_score": 0.8116176214346887,
            "fpr": 0.14818880351262348,
            "logloss": 0.7929118065961099,
            "mae": 0.30275077487211915,
            "precision": 0.7227926078028748,
            "recall": 0.7348643006263048
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 27690,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8435980473926938,
            "auditor_fn_violation": 0.03728070175438597,
            "auditor_fp_violation": 0.02085591553253844,
            "ave_precision_score": 0.8440360371480814,
            "fpr": 0.1074561403508772,
            "logloss": 0.6574332823236204,
            "mae": 0.2731977794876255,
            "precision": 0.7864923747276689,
            "recall": 0.76
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8300069336804934,
            "auditor_fn_violation": 0.042897181055482854,
            "auditor_fp_violation": 0.02597877789974387,
            "ave_precision_score": 0.8303561165570829,
            "fpr": 0.10537870472008781,
            "logloss": 0.7661306987722154,
            "mae": 0.2895904105741856,
            "precision": 0.7788018433179723,
            "recall": 0.7056367432150313
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.7655475868962444,
            "auditor_fn_violation": 0.030096636820266438,
            "auditor_fp_violation": 0.008970828233374136,
            "ave_precision_score": 0.7671506196844834,
            "fpr": 0.07894736842105263,
            "logloss": 1.1393751876279432,
            "mae": 0.3341401403664747,
            "precision": 0.7869822485207101,
            "recall": 0.5518672199170125
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7715864399551133,
            "auditor_fn_violation": 0.03193547786935571,
            "auditor_fp_violation": 0.00998927309597454,
            "ave_precision_score": 0.7725099879765445,
            "fpr": 0.08781558726673985,
            "logloss": 1.003180944817941,
            "mae": 0.3093587496496148,
            "precision": 0.7814207650273224,
            "recall": 0.6059322033898306
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7731410137728604,
            "auditor_fn_violation": 0.007693637621023514,
            "auditor_fp_violation": 0.02457415340677275,
            "ave_precision_score": 0.7736654570308328,
            "fpr": 0.13048245614035087,
            "logloss": 0.9452014721047466,
            "mae": 0.3158614287320297,
            "precision": 0.7313769751693002,
            "recall": 0.6721991701244814
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.7281809465428508,
            "auditor_fn_violation": 0.013414203054940557,
            "auditor_fp_violation": 0.025744569661114856,
            "ave_precision_score": 0.7282391707253966,
            "fpr": 0.16465422612513722,
            "logloss": 1.068205844047337,
            "mae": 0.3376780569087954,
            "precision": 0.6808510638297872,
            "recall": 0.6779661016949152
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7727554199531965,
            "auditor_fn_violation": 0.029728106573487675,
            "auditor_fp_violation": 0.009128926968584253,
            "ave_precision_score": 0.7738099604312831,
            "fpr": 0.07675438596491228,
            "logloss": 1.123128020503946,
            "mae": 0.3322464055961443,
            "precision": 0.7928994082840237,
            "recall": 0.5560165975103735
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7788263355441216,
            "auditor_fn_violation": 0.03304712645816666,
            "auditor_fp_violation": 0.012327188075883476,
            "ave_precision_score": 0.7796381425495439,
            "fpr": 0.0867178924259056,
            "logloss": 0.9788446896008005,
            "mae": 0.3068314802826556,
            "precision": 0.7817679558011049,
            "recall": 0.5995762711864406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8396922271414089,
            "auditor_fn_violation": 0.0085671907985732,
            "auditor_fp_violation": 0.02325326397388821,
            "ave_precision_score": 0.839963216423782,
            "fpr": 0.12828947368421054,
            "logloss": 0.6843213104497035,
            "mae": 0.2615671021767691,
            "precision": 0.7607361963190185,
            "recall": 0.7717842323651453
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8164601198093919,
            "auditor_fn_violation": 0.013656068019870139,
            "auditor_fp_violation": 0.017063028687592043,
            "ave_precision_score": 0.8168318385531083,
            "fpr": 0.13721185510428102,
            "logloss": 0.7433063666778216,
            "mae": 0.2704520521943542,
            "precision": 0.7484909456740443,
            "recall": 0.788135593220339
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7563105984485102,
            "auditor_fn_violation": 0.02579939215258063,
            "auditor_fp_violation": 0.019155446756425953,
            "ave_precision_score": 0.7568801125630769,
            "fpr": 0.12938596491228072,
            "logloss": 0.9785753831125321,
            "mae": 0.32368663339288894,
            "precision": 0.7299771167048055,
            "recall": 0.6618257261410788
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7347748124260738,
            "auditor_fn_violation": 0.02332136411840221,
            "auditor_fp_violation": 0.011982126827511892,
            "ave_precision_score": 0.7357344389717284,
            "fpr": 0.14928649835345773,
            "logloss": 0.9716205323844348,
            "mae": 0.32620775078443137,
            "precision": 0.7081545064377682,
            "recall": 0.6991525423728814
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.838979580204719,
            "auditor_fn_violation": 0.012853061075926332,
            "auditor_fp_violation": 0.016829865361077112,
            "ave_precision_score": 0.8392427488831257,
            "fpr": 0.10855263157894737,
            "logloss": 0.7393251782257839,
            "mae": 0.26220416619081005,
            "precision": 0.7833698030634574,
            "recall": 0.7427385892116183
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8188963535897558,
            "auditor_fn_violation": 0.013804907998288337,
            "auditor_fp_violation": 0.019473456538535595,
            "ave_precision_score": 0.8196611373172097,
            "fpr": 0.1163556531284303,
            "logloss": 0.7687688860319577,
            "mae": 0.2642036072998304,
            "precision": 0.7685589519650655,
            "recall": 0.7457627118644068
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7698326795268711,
            "auditor_fn_violation": 0.03125000000000001,
            "auditor_fp_violation": 0.009373725010199922,
            "ave_precision_score": 0.7708998589301277,
            "fpr": 0.0756578947368421,
            "logloss": 1.1218809817233555,
            "mae": 0.3349232901724775,
            "precision": 0.7946428571428571,
            "recall": 0.553941908713693
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7765579983420785,
            "auditor_fn_violation": 0.03405877318647789,
            "auditor_fp_violation": 0.012327188075883476,
            "ave_precision_score": 0.7774671171138571,
            "fpr": 0.0867178924259056,
            "logloss": 0.9768106754627209,
            "mae": 0.3089730297878237,
            "precision": 0.782967032967033,
            "recall": 0.6038135593220338
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8194397073950519,
            "auditor_fn_violation": 0.016142534760136856,
            "auditor_fp_violation": 0.016396368829049365,
            "ave_precision_score": 0.8159953342326081,
            "fpr": 0.10964912280701754,
            "logloss": 1.5403168084471326,
            "mae": 0.26555822394257367,
            "precision": 0.7816593886462883,
            "recall": 0.7427385892116183
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.7940392793021982,
            "auditor_fn_violation": 0.01387002548884631,
            "auditor_fp_violation": 0.01748060280699824,
            "ave_precision_score": 0.7881866697088404,
            "fpr": 0.11855104281009879,
            "logloss": 1.708088913312335,
            "mae": 0.27437666541206224,
            "precision": 0.7657266811279827,
            "recall": 0.7478813559322034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 25349,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8226541798601703,
            "auditor_fn_violation": 0.013010027662517295,
            "auditor_fp_violation": 0.016523867809057527,
            "ave_precision_score": 0.8229813314501846,
            "fpr": 0.10087719298245613,
            "logloss": 0.6842672883295977,
            "mae": 0.2945505476438727,
            "precision": 0.7814726840855107,
            "recall": 0.6825726141078838
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8150187692920989,
            "auditor_fn_violation": 0.024339987720701783,
            "auditor_fp_violation": 0.012322187188225909,
            "ave_precision_score": 0.8155122804063304,
            "fpr": 0.09989023051591657,
            "logloss": 0.6617370823942695,
            "mae": 0.2824853863089744,
            "precision": 0.78125,
            "recall": 0.6885593220338984
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.8175585824325814,
            "auditor_fn_violation": 0.016643007934774693,
            "auditor_fp_violation": 0.0065075479396164825,
            "ave_precision_score": 0.8179533620731244,
            "fpr": 0.05043859649122807,
            "logloss": 0.907584848088761,
            "mae": 0.3267244771174687,
            "precision": 0.8481848184818482,
            "recall": 0.533195020746888
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.8152171077887165,
            "auditor_fn_violation": 0.02446557145249214,
            "auditor_fp_violation": 0.0037906728444298886,
            "ave_precision_score": 0.81568996400797,
            "fpr": 0.048298572996706916,
            "logloss": 0.8482503481145249,
            "mae": 0.31276319685106374,
            "precision": 0.8472222222222222,
            "recall": 0.5169491525423728
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7943160514097305,
            "auditor_fn_violation": 0.020892480163063273,
            "auditor_fp_violation": 0.01873980008159935,
            "ave_precision_score": 0.7949167825734196,
            "fpr": 0.11074561403508772,
            "logloss": 0.9969643631518564,
            "mae": 0.30329237562228817,
            "precision": 0.7554479418886199,
            "recall": 0.6473029045643154
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7901142263473607,
            "auditor_fn_violation": 0.02749120913877468,
            "auditor_fp_violation": 0.01019430948993446,
            "ave_precision_score": 0.7905755388644422,
            "fpr": 0.11086717892425905,
            "logloss": 0.8988822498753987,
            "mae": 0.2843298668077553,
            "precision": 0.7651162790697674,
            "recall": 0.6970338983050848
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.7045795649540436,
            "auditor_fn_violation": 0.031925638785761094,
            "auditor_fp_violation": 0.006357099143206852,
            "ave_precision_score": 0.7063830075557813,
            "fpr": 0.07346491228070176,
            "logloss": 1.3239241304547011,
            "mae": 0.39339535915071133,
            "precision": 0.7490636704119851,
            "recall": 0.4149377593360996
        },
        "train": {
            "accuracy": 0.6553238199780461,
            "auc_prc": 0.734870663013734,
            "auditor_fn_violation": 0.026028391225883282,
            "auditor_fp_violation": 0.010529368962990933,
            "ave_precision_score": 0.7366243004031343,
            "fpr": 0.07244785949506037,
            "logloss": 1.1401595544220262,
            "mae": 0.35754381592867923,
            "precision": 0.7724137931034483,
            "recall": 0.4745762711864407
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8380388627694497,
            "auditor_fn_violation": 0.01720717769527554,
            "auditor_fp_violation": 0.020287637698898412,
            "ave_precision_score": 0.8383184832726106,
            "fpr": 0.10197368421052631,
            "logloss": 0.6922347351636197,
            "mae": 0.265379189015817,
            "precision": 0.7937915742793792,
            "recall": 0.7427385892116183
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8252486054709102,
            "auditor_fn_violation": 0.020212003944259434,
            "auditor_fp_violation": 0.009874252679850674,
            "ave_precision_score": 0.8256798457681682,
            "fpr": 0.10867178924259056,
            "logloss": 0.6887212313961237,
            "mae": 0.2669746361181329,
            "precision": 0.7780269058295964,
            "recall": 0.7351694915254238
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7936328542041822,
            "auditor_fn_violation": 0.0531275023658732,
            "auditor_fp_violation": 0.028360873113015096,
            "ave_precision_score": 0.7940041027314273,
            "fpr": 0.125,
            "logloss": 0.8639239228851995,
            "mae": 0.32511615201628835,
            "precision": 0.740909090909091,
            "recall": 0.6763485477178424
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8223877452385233,
            "auditor_fn_violation": 0.043777558652254,
            "auditor_fp_violation": 0.029445226527708666,
            "ave_precision_score": 0.8226342410319347,
            "fpr": 0.10976948408342481,
            "logloss": 0.7384880392662654,
            "mae": 0.3005506550443197,
            "precision": 0.7663551401869159,
            "recall": 0.6949152542372882
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8168639191864707,
            "auditor_fn_violation": 0.009413445439324453,
            "auditor_fp_violation": 0.012642798857609144,
            "ave_precision_score": 0.8153877644438354,
            "fpr": 0.08991228070175439,
            "logloss": 1.2791003133675944,
            "mae": 0.28554849539875815,
            "precision": 0.7960199004975125,
            "recall": 0.6639004149377593
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8038541043739332,
            "auditor_fn_violation": 0.022009711808591795,
            "auditor_fp_violation": 0.008208957089883454,
            "ave_precision_score": 0.8013638018078928,
            "fpr": 0.07793633369923161,
            "logloss": 1.3958197272701967,
            "mae": 0.284065089079367,
            "precision": 0.8096514745308311,
            "recall": 0.6398305084745762
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7749993926388423,
            "auditor_fn_violation": 0.027890005095726866,
            "auditor_fp_violation": 0.009128926968584253,
            "ave_precision_score": 0.7760296526328616,
            "fpr": 0.07675438596491228,
            "logloss": 1.1155560288956703,
            "mae": 0.32985552301733007,
            "precision": 0.7953216374269005,
            "recall": 0.5643153526970954
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7818910095981944,
            "auditor_fn_violation": 0.033916910082048043,
            "auditor_fp_violation": 0.013069819893031017,
            "ave_precision_score": 0.78260276160774,
            "fpr": 0.09001097694840834,
            "logloss": 0.9716005924722589,
            "mae": 0.3046093399130254,
            "precision": 0.7783783783783784,
            "recall": 0.6101694915254238
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8013963872679563,
            "auditor_fn_violation": 0.01664073305670817,
            "auditor_fp_violation": 0.025464096287229707,
            "ave_precision_score": 0.8017832399927995,
            "fpr": 0.13815789473684212,
            "logloss": 1.003168520409922,
            "mae": 0.28737557626605553,
            "precision": 0.7402061855670103,
            "recall": 0.7448132780082988
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7908762851779298,
            "auditor_fn_violation": 0.01434445292005433,
            "auditor_fp_violation": 0.020711176233781502,
            "ave_precision_score": 0.7914241648956657,
            "fpr": 0.15806805708013172,
            "logloss": 0.9652169514369117,
            "mae": 0.2809935276865548,
            "precision": 0.7203883495145631,
            "recall": 0.7860169491525424
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.666833348788179,
            "auditor_fn_violation": 0.053341340904127545,
            "auditor_fp_violation": 0.0500560995512036,
            "ave_precision_score": 0.6590527848380352,
            "fpr": 0.1206140350877193,
            "logloss": 2.512946456479811,
            "mae": 0.35143775457592835,
            "precision": 0.7215189873417721,
            "recall": 0.5912863070539419
        },
        "train": {
            "accuracy": 0.6564215148188803,
            "auc_prc": 0.6442862716493332,
            "auditor_fn_violation": 0.05535684384825766,
            "auditor_fp_violation": 0.04758594650550473,
            "ave_precision_score": 0.6331918365726656,
            "fpr": 0.1251372118551043,
            "logloss": 2.6770442002976136,
            "mae": 0.36091333568487005,
            "precision": 0.7054263565891473,
            "recall": 0.5783898305084746
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6173245614035088,
            "auc_prc": 0.7045446119030698,
            "auditor_fn_violation": 0.031925638785761094,
            "auditor_fp_violation": 0.006357099143206852,
            "ave_precision_score": 0.7063480517762268,
            "fpr": 0.07346491228070176,
            "logloss": 1.324571047394538,
            "mae": 0.39345010484792786,
            "precision": 0.7490636704119851,
            "recall": 0.4149377593360996
        },
        "train": {
            "accuracy": 0.6553238199780461,
            "auc_prc": 0.7349184955115873,
            "auditor_fn_violation": 0.026028391225883282,
            "auditor_fp_violation": 0.010529368962990933,
            "ave_precision_score": 0.7366719999571604,
            "fpr": 0.07244785949506037,
            "logloss": 1.1407274519321347,
            "mae": 0.357559670768029,
            "precision": 0.7724137931034483,
            "recall": 0.4745762711864407
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7717381294798856,
            "auditor_fn_violation": 0.029427822668704962,
            "auditor_fp_violation": 0.008098735210118321,
            "ave_precision_score": 0.7722550899448425,
            "fpr": 0.0800438596491228,
            "logloss": 1.121589523088991,
            "mae": 0.3319401806298487,
            "precision": 0.7871720116618076,
            "recall": 0.5601659751037344
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7757189705183328,
            "auditor_fn_violation": 0.030181956873616266,
            "auditor_fp_violation": 0.009004098227435375,
            "ave_precision_score": 0.7766644878494382,
            "fpr": 0.09001097694840834,
            "logloss": 0.9849580369087766,
            "mae": 0.3089192470829161,
            "precision": 0.7728531855955678,
            "recall": 0.5911016949152542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8494182929408004,
            "auditor_fn_violation": 0.016124335735604574,
            "auditor_fp_violation": 0.01715116279069767,
            "ave_precision_score": 0.8498559585652522,
            "fpr": 0.10526315789473684,
            "logloss": 0.6847295351564183,
            "mae": 0.27318657748324127,
            "precision": 0.7917570498915402,
            "recall": 0.7572614107883817
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8583302800454351,
            "auditor_fn_violation": 0.01827940984948557,
            "auditor_fp_violation": 0.011342013207344303,
            "ave_precision_score": 0.8586146501490697,
            "fpr": 0.10428100987925357,
            "logloss": 0.6927716557751893,
            "mae": 0.27380274732653104,
            "precision": 0.7879464285714286,
            "recall": 0.7478813559322034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.8122987988759576,
            "auditor_fn_violation": 0.024291147994467504,
            "auditor_fp_violation": 0.016809465524275806,
            "ave_precision_score": 0.8126874539710394,
            "fpr": 0.07894736842105263,
            "logloss": 1.0584931366473613,
            "mae": 0.3043404556726038,
            "precision": 0.7942857142857143,
            "recall": 0.5767634854771784
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.8060238049640605,
            "auditor_fn_violation": 0.02828889839810973,
            "auditor_fp_violation": 0.009411670571526446,
            "ave_precision_score": 0.8064766411846513,
            "fpr": 0.0801317233809001,
            "logloss": 0.9443346723020383,
            "mae": 0.2927078572828065,
            "precision": 0.7972222222222223,
            "recall": 0.6080508474576272
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.8235885679464551,
            "auditor_fn_violation": 0.025976832641770416,
            "auditor_fp_violation": 0.009776621787025704,
            "ave_precision_score": 0.8241423904932451,
            "fpr": 0.03508771929824561,
            "logloss": 0.7648308008615475,
            "mae": 0.3504270574754835,
            "precision": 0.8773946360153256,
            "recall": 0.475103734439834
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.8553585664844774,
            "auditor_fn_violation": 0.020774805112653293,
            "auditor_fp_violation": 0.003980706575417136,
            "ave_precision_score": 0.8557260163294584,
            "fpr": 0.019758507135016465,
            "logloss": 0.7301619139850725,
            "mae": 0.3344135438581962,
            "precision": 0.9192825112107623,
            "recall": 0.4343220338983051
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7504891441275849,
            "auditor_fn_violation": 0.02849057290529228,
            "auditor_fp_violation": 0.017342411260709917,
            "ave_precision_score": 0.7514881703346457,
            "fpr": 0.10855263157894737,
            "logloss": 1.2042673047122237,
            "mae": 0.334077316224012,
            "precision": 0.7367021276595744,
            "recall": 0.5746887966804979
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7372284885262459,
            "auditor_fn_violation": 0.031744776647007394,
            "auditor_fp_violation": 0.014332544026564713,
            "ave_precision_score": 0.738321248772355,
            "fpr": 0.11086717892425905,
            "logloss": 1.1151322528649732,
            "mae": 0.31251826939227884,
            "precision": 0.7487562189054726,
            "recall": 0.6377118644067796
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8292668533882945,
            "auditor_fn_violation": 0.018694947950789846,
            "auditor_fp_violation": 0.007700938392492864,
            "ave_precision_score": 0.8295758208133563,
            "fpr": 0.05921052631578947,
            "logloss": 0.7690868557654423,
            "mae": 0.3120925872330429,
            "precision": 0.8353658536585366,
            "recall": 0.5684647302904564
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.8297756439588684,
            "auditor_fn_violation": 0.017672421812498836,
            "auditor_fp_violation": 0.004883366797606577,
            "ave_precision_score": 0.830191810057851,
            "fpr": 0.06037321624588365,
            "logloss": 0.7412697251299408,
            "mae": 0.2992317057346759,
            "precision": 0.8323170731707317,
            "recall": 0.5783898305084746
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7728634814252291,
            "auditor_fn_violation": 0.021079020164519187,
            "auditor_fp_violation": 0.013101795185638518,
            "ave_precision_score": 0.7738881909319083,
            "fpr": 0.09758771929824561,
            "logloss": 1.045984472362089,
            "mae": 0.32220603140324344,
            "precision": 0.7670157068062827,
            "recall": 0.6078838174273858
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7754551721061169,
            "auditor_fn_violation": 0.030454054959162032,
            "auditor_fp_violation": 0.011144478144870719,
            "ave_precision_score": 0.7762063855264352,
            "fpr": 0.10428100987925357,
            "logloss": 0.9326983792368542,
            "mae": 0.3000140399000665,
            "precision": 0.763681592039801,
            "recall": 0.6504237288135594
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 25349,
        "test": {
            "accuracy": 0.5932017543859649,
            "auc_prc": 0.6837054082322467,
            "auditor_fn_violation": 0.012957705466986984,
            "auditor_fp_violation": 0.016758465932272544,
            "ave_precision_score": 0.6831250841758731,
            "fpr": 0.06469298245614036,
            "logloss": 1.4636813423040664,
            "mae": 0.4034431699075642,
            "precision": 0.74235807860262,
            "recall": 0.35269709543568467
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.6784556983746457,
            "auditor_fn_violation": 0.014642132876890743,
            "auditor_fp_violation": 0.02665223077096185,
            "ave_precision_score": 0.678251398166426,
            "fpr": 0.06915477497255763,
            "logloss": 1.4353561348175876,
            "mae": 0.3830587794866601,
            "precision": 0.7567567567567568,
            "recall": 0.4152542372881356
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8311671011828314,
            "auditor_fn_violation": 0.021806981145810585,
            "auditor_fp_violation": 0.012803447572419423,
            "ave_precision_score": 0.8314557604022008,
            "fpr": 0.09100877192982457,
            "logloss": 0.6989495190243846,
            "mae": 0.2905393646161776,
            "precision": 0.7950617283950617,
            "recall": 0.6680497925311203
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8290526317880392,
            "auditor_fn_violation": 0.02475162328601463,
            "auditor_fp_violation": 0.005295940029355211,
            "ave_precision_score": 0.8295193246503411,
            "fpr": 0.08562019758507135,
            "logloss": 0.6633558111061244,
            "mae": 0.27583294871276426,
            "precision": 0.8064516129032258,
            "recall": 0.6885593220338984
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7558893249780679,
            "auditor_fn_violation": 0.027553323141879606,
            "auditor_fp_violation": 0.015368727050183597,
            "ave_precision_score": 0.7569538925470896,
            "fpr": 0.09758771929824561,
            "logloss": 1.1960799505466322,
            "mae": 0.340126836800182,
            "precision": 0.7534626038781164,
            "recall": 0.5643153526970954
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7542998099452565,
            "auditor_fn_violation": 0.03615416100764666,
            "auditor_fp_violation": 0.012329688519712253,
            "ave_precision_score": 0.7553351750584263,
            "fpr": 0.10208562019758508,
            "logloss": 1.0759646522765811,
            "mae": 0.3157556386926757,
            "precision": 0.7578125,
            "recall": 0.6165254237288136
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7727554199531965,
            "auditor_fn_violation": 0.029728106573487675,
            "auditor_fp_violation": 0.009128926968584253,
            "ave_precision_score": 0.7738099604312831,
            "fpr": 0.07675438596491228,
            "logloss": 1.1231258416325012,
            "mae": 0.33224610660723003,
            "precision": 0.7928994082840237,
            "recall": 0.5560165975103735
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7788263355441216,
            "auditor_fn_violation": 0.03304712645816666,
            "auditor_fp_violation": 0.012327188075883476,
            "ave_precision_score": 0.7796381425495439,
            "fpr": 0.0867178924259056,
            "logloss": 0.9788408411467033,
            "mae": 0.3068315199874602,
            "precision": 0.7817679558011049,
            "recall": 0.5995762711864406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.835384604044127,
            "auditor_fn_violation": 0.014008699133726432,
            "auditor_fp_violation": 0.018997348021215832,
            "ave_precision_score": 0.835676504805565,
            "fpr": 0.09868421052631579,
            "logloss": 0.7013296020442497,
            "mae": 0.2704481014391144,
            "precision": 0.7935779816513762,
            "recall": 0.7178423236514523
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8248628378588203,
            "auditor_fn_violation": 0.017072410649500463,
            "auditor_fp_violation": 0.00780888607727872,
            "ave_precision_score": 0.825307974322183,
            "fpr": 0.10208562019758508,
            "logloss": 0.6853885947728641,
            "mae": 0.266061626121747,
            "precision": 0.7857142857142857,
            "recall": 0.722457627118644
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.756824540968599,
            "auditor_fn_violation": 0.02437759336099586,
            "auditor_fp_violation": 0.01652386780905753,
            "ave_precision_score": 0.7573291049193415,
            "fpr": 0.1206140350877193,
            "logloss": 1.0816279157703774,
            "mae": 0.3288358794980192,
            "precision": 0.7362110311750599,
            "recall": 0.6369294605809128
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7292532910310823,
            "auditor_fn_violation": 0.030456380583824817,
            "auditor_fp_violation": 0.013619917535362527,
            "ave_precision_score": 0.7298023545122405,
            "fpr": 0.150384193194292,
            "logloss": 1.0849693248084133,
            "mae": 0.32915474592579474,
            "precision": 0.6989010989010989,
            "recall": 0.673728813559322
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.757247137366851,
            "auditor_fn_violation": 0.03126137439033269,
            "auditor_fp_violation": 0.01707976336189311,
            "ave_precision_score": 0.7581820221984553,
            "fpr": 0.1118421052631579,
            "logloss": 0.9809069304332052,
            "mae": 0.3301582587993399,
            "precision": 0.7424242424242424,
            "recall": 0.6099585062240664
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7603046922891428,
            "auditor_fn_violation": 0.033749465106327564,
            "auditor_fp_violation": 0.010876930655191297,
            "ave_precision_score": 0.7611137861403887,
            "fpr": 0.1141602634467618,
            "logloss": 0.8984156395206295,
            "mae": 0.3098099384631491,
            "precision": 0.75,
            "recall": 0.6610169491525424
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.824687800389595,
            "auditor_fn_violation": 0.017866892334570866,
            "auditor_fp_violation": 0.013476642186862507,
            "ave_precision_score": 0.8250547741576698,
            "fpr": 0.0800438596491228,
            "logloss": 0.7690338729012717,
            "mae": 0.30164708186976685,
            "precision": 0.8063660477453581,
            "recall": 0.6307053941908713
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8258639551142711,
            "auditor_fn_violation": 0.01947478092615677,
            "auditor_fp_violation": 0.006438642859107497,
            "ave_precision_score": 0.8262992400512637,
            "fpr": 0.07244785949506037,
            "logloss": 0.7153155552334485,
            "mae": 0.28596431154758567,
            "precision": 0.8166666666666667,
            "recall": 0.6228813559322034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 25349,
        "test": {
            "accuracy": 0.5888157894736842,
            "auc_prc": 0.6715407229772877,
            "auditor_fn_violation": 0.019618548445803317,
            "auditor_fp_violation": 0.010929212566299472,
            "ave_precision_score": 0.6733811854830248,
            "fpr": 0.07346491228070176,
            "logloss": 1.9161872001257498,
            "mae": 0.41885600537998274,
            "precision": 0.7219917012448133,
            "recall": 0.36099585062240663
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.6838346566125475,
            "auditor_fn_violation": 0.030830806154533114,
            "auditor_fp_violation": 0.006543661499916235,
            "ave_precision_score": 0.6851320799182143,
            "fpr": 0.08122941822173436,
            "logloss": 1.7087196896416093,
            "mae": 0.39477627149491085,
            "precision": 0.7186311787072244,
            "recall": 0.4004237288135593
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8065685705025399,
            "auditor_fn_violation": 0.007152216641188043,
            "auditor_fp_violation": 0.013769889840881273,
            "ave_precision_score": 0.7815249197879521,
            "fpr": 0.10087719298245613,
            "logloss": 3.061465410044475,
            "mae": 0.26953329847487567,
            "precision": 0.7880184331797235,
            "recall": 0.7095435684647303
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7794168474855767,
            "auditor_fn_violation": 0.012125806991757993,
            "auditor_fp_violation": 0.010471858754929004,
            "ave_precision_score": 0.7495865680834092,
            "fpr": 0.1141602634467618,
            "logloss": 3.356844249726771,
            "mae": 0.283451932338847,
            "precision": 0.7609195402298851,
            "recall": 0.701271186440678
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6359649122807017,
            "auc_prc": 0.702974668239396,
            "auditor_fn_violation": 0.030715403654364128,
            "auditor_fp_violation": 0.011760505915952678,
            "ave_precision_score": 0.7048041357344568,
            "fpr": 0.08552631578947369,
            "logloss": 1.2163053993858386,
            "mae": 0.38357941401319035,
            "precision": 0.7450980392156863,
            "recall": 0.4730290456431535
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.7331795178921299,
            "auditor_fn_violation": 0.025888853746116217,
            "auditor_fp_violation": 0.013442386023519177,
            "ave_precision_score": 0.7349612627976565,
            "fpr": 0.09001097694840834,
            "logloss": 1.051713968111404,
            "mae": 0.34934790088425616,
            "precision": 0.7544910179640718,
            "recall": 0.5338983050847458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.7673631105158765,
            "auditor_fn_violation": 0.030428769017980646,
            "auditor_fp_violation": 0.00804263565891473,
            "ave_precision_score": 0.7684350251738216,
            "fpr": 0.07894736842105263,
            "logloss": 1.1397867498903402,
            "mae": 0.33672450455606145,
            "precision": 0.7863501483679525,
            "recall": 0.549792531120332
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7726209331765447,
            "auditor_fn_violation": 0.03303317271018996,
            "auditor_fp_violation": 0.01287978616204377,
            "ave_precision_score": 0.7735395640009766,
            "fpr": 0.08562019758507135,
            "logloss": 0.996499699625865,
            "mae": 0.311362714402645,
            "precision": 0.7857142857142857,
            "recall": 0.6059322033898306
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8577669600161575,
            "auditor_fn_violation": 0.01557836499963603,
            "auditor_fp_violation": 0.013178294573643414,
            "ave_precision_score": 0.8579857046614267,
            "fpr": 0.09758771929824561,
            "logloss": 0.6487744506761915,
            "mae": 0.2630245901359106,
            "precision": 0.8,
            "recall": 0.7385892116182573
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8475496616003118,
            "auditor_fn_violation": 0.013897932984799717,
            "auditor_fp_violation": 0.004563309987522789,
            "ave_precision_score": 0.8480453532588879,
            "fpr": 0.10208562019758508,
            "logloss": 0.6347322648249192,
            "mae": 0.2595886559401147,
            "precision": 0.7914798206278026,
            "recall": 0.7478813559322034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8026149252532289,
            "auditor_fn_violation": 0.02491218970663173,
            "auditor_fp_violation": 0.016990514075887395,
            "ave_precision_score": 0.8030188478266979,
            "fpr": 0.11732456140350878,
            "logloss": 1.0360196318099824,
            "mae": 0.289783784607319,
            "precision": 0.759009009009009,
            "recall": 0.6991701244813278
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7938046009895061,
            "auditor_fn_violation": 0.02100039070494335,
            "auditor_fp_violation": 0.015727791683023736,
            "ave_precision_score": 0.7942094116069458,
            "fpr": 0.12952799121844127,
            "logloss": 0.9691451070592073,
            "mae": 0.2819579351815434,
            "precision": 0.7434782608695653,
            "recall": 0.7245762711864406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8109713488195157,
            "auditor_fn_violation": 0.022862524568683124,
            "auditor_fp_violation": 0.020471236230110164,
            "ave_precision_score": 0.8113268511802924,
            "fpr": 0.1074561403508772,
            "logloss": 1.0102136008535045,
            "mae": 0.30129033559645113,
            "precision": 0.7621359223300971,
            "recall": 0.6514522821576764
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8081382116326872,
            "auditor_fn_violation": 0.037726283279688924,
            "auditor_fp_violation": 0.013764943277431746,
            "ave_precision_score": 0.8086207273185005,
            "fpr": 0.10757409440175632,
            "logloss": 0.8935346869460866,
            "mae": 0.28633334902855834,
            "precision": 0.7683215130023641,
            "recall": 0.6885593220338984
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.7717781607783628,
            "auditor_fn_violation": 0.030588010482638144,
            "auditor_fp_violation": 0.00803243574051408,
            "ave_precision_score": 0.7728344149583186,
            "fpr": 0.07675438596491228,
            "logloss": 1.1219887064502132,
            "mae": 0.33450678366789527,
            "precision": 0.7916666666666666,
            "recall": 0.5518672199170125
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7791573429428484,
            "auditor_fn_violation": 0.0332657351764684,
            "auditor_fp_violation": 0.012327188075883476,
            "ave_precision_score": 0.7800485084254514,
            "fpr": 0.0867178924259056,
            "logloss": 0.9714669655544815,
            "mae": 0.3078613858794233,
            "precision": 0.7817679558011049,
            "recall": 0.5995762711864406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.8274541391537756,
            "auditor_fn_violation": 0.02502365873189198,
            "auditor_fp_violation": 0.005880252957976338,
            "ave_precision_score": 0.8278360729104781,
            "fpr": 0.04824561403508772,
            "logloss": 0.8453698386444606,
            "mae": 0.3246280036432709,
            "precision": 0.8498293515358362,
            "recall": 0.516597510373444
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.842850270389936,
            "auditor_fn_violation": 0.022486464864462593,
            "auditor_fp_violation": 0.006336124662127529,
            "ave_precision_score": 0.8431373169794604,
            "fpr": 0.04500548847420417,
            "logloss": 0.7846642794949563,
            "mae": 0.30892458390822,
            "precision": 0.8576388888888888,
            "recall": 0.5233050847457628
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 25349,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.689339460757189,
            "auditor_fn_violation": 0.027391806799155562,
            "auditor_fp_violation": 0.0075275397796817625,
            "ave_precision_score": 0.6907100005170844,
            "fpr": 0.07894736842105263,
            "logloss": 1.3887500999427345,
            "mae": 0.4000540780768598,
            "precision": 0.7303370786516854,
            "recall": 0.4045643153526971
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.7114497883369701,
            "auditor_fn_violation": 0.02770284098308807,
            "auditor_fp_violation": 0.011284502999282372,
            "ave_precision_score": 0.7127473595834839,
            "fpr": 0.08342480790340286,
            "logloss": 1.2175620473141557,
            "mae": 0.36611516643310665,
            "precision": 0.7466666666666667,
            "recall": 0.4745762711864407
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7857567901801537,
            "auditor_fn_violation": 0.02437304360486278,
            "auditor_fp_violation": 0.009026927784577725,
            "ave_precision_score": 0.7871971608082959,
            "fpr": 0.11513157894736842,
            "logloss": 0.945418803470417,
            "mae": 0.3145414257134752,
            "precision": 0.75177304964539,
            "recall": 0.6597510373443983
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7946510738406068,
            "auditor_fn_violation": 0.026972594838973754,
            "auditor_fp_violation": 0.011204488796761428,
            "ave_precision_score": 0.7952736587908344,
            "fpr": 0.1119648737650933,
            "logloss": 0.8287125377203096,
            "mae": 0.28981373199104976,
            "precision": 0.7622377622377622,
            "recall": 0.6927966101694916
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8291991929241715,
            "auditor_fn_violation": 0.015009645483002114,
            "auditor_fp_violation": 0.01193390452876377,
            "ave_precision_score": 0.8295058103492103,
            "fpr": 0.08114035087719298,
            "logloss": 0.72695408909369,
            "mae": 0.2941705543329921,
            "precision": 0.8052631578947368,
            "recall": 0.6348547717842323
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8267292801805497,
            "auditor_fn_violation": 0.02223064615155631,
            "auditor_fp_violation": 0.004243253177439002,
            "ave_precision_score": 0.8271961076584706,
            "fpr": 0.07244785949506037,
            "logloss": 0.6991475978926108,
            "mae": 0.28208086951792577,
            "precision": 0.8221024258760108,
            "recall": 0.6461864406779662
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7689264152738757,
            "auditor_fn_violation": 0.029184410715585665,
            "auditor_fp_violation": 0.007496940024479807,
            "ave_precision_score": 0.7699883386532995,
            "fpr": 0.07894736842105263,
            "logloss": 1.1395439303610706,
            "mae": 0.3343953983778535,
            "precision": 0.7888563049853372,
            "recall": 0.558091286307054
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7753703356581585,
            "auditor_fn_violation": 0.03303317271018996,
            "auditor_fp_violation": 0.013472391349464536,
            "ave_precision_score": 0.7761071800948729,
            "fpr": 0.08781558726673985,
            "logloss": 0.9966871440485682,
            "mae": 0.3092521256719132,
            "precision": 0.7814207650273224,
            "recall": 0.6059322033898306
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7744891318093028,
            "auditor_fn_violation": 0.026427258498944457,
            "auditor_fp_violation": 0.0161796205630355,
            "ave_precision_score": 0.7755236406952631,
            "fpr": 0.08223684210526316,
            "logloss": 1.0927141553372037,
            "mae": 0.3304546444120705,
            "precision": 0.7844827586206896,
            "recall": 0.5663900414937759
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7723743661618112,
            "auditor_fn_violation": 0.03130058233641557,
            "auditor_fp_violation": 0.007856394510025533,
            "ave_precision_score": 0.7732914402771508,
            "fpr": 0.09549945115257959,
            "logloss": 0.977056465396906,
            "mae": 0.30741150229612263,
            "precision": 0.767379679144385,
            "recall": 0.6080508474576272
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6282894736842105,
            "auc_prc": 0.7582921782263276,
            "auditor_fn_violation": 0.022780628958287835,
            "auditor_fp_violation": 0.0054722562219502256,
            "ave_precision_score": 0.7599060402774562,
            "fpr": 0.05592105263157895,
            "logloss": 0.9945011680112189,
            "mae": 0.38157526645393675,
            "precision": 0.7918367346938775,
            "recall": 0.4024896265560166
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.821352660884006,
            "auditor_fn_violation": 0.011797893914305389,
            "auditor_fp_violation": 0.007626353677777807,
            "ave_precision_score": 0.8216023834716195,
            "fpr": 0.031833150384193196,
            "logloss": 0.8936505499446763,
            "mae": 0.35136921825648915,
            "precision": 0.875,
            "recall": 0.4300847457627119
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7696010027668194,
            "auditor_fn_violation": 0.028640714857683634,
            "auditor_fp_violation": 0.010064769481844149,
            "ave_precision_score": 0.7706666351352713,
            "fpr": 0.0800438596491228,
            "logloss": 1.1112132743607588,
            "mae": 0.33367913070286453,
            "precision": 0.7871720116618076,
            "recall": 0.5601659751037344
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7735966867887087,
            "auditor_fn_violation": 0.0341308675510242,
            "auditor_fp_violation": 0.008594025439515514,
            "ave_precision_score": 0.7744242042007936,
            "fpr": 0.09001097694840834,
            "logloss": 0.9782539214521453,
            "mae": 0.30929952787181836,
            "precision": 0.7771739130434783,
            "recall": 0.6059322033898306
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7993858422293116,
            "auditor_fn_violation": 0.01983921161825726,
            "auditor_fp_violation": 0.019476744186046516,
            "ave_precision_score": 0.7995046437413866,
            "fpr": 0.15460526315789475,
            "logloss": 1.1013269892466189,
            "mae": 0.29064146810060437,
            "precision": 0.72568093385214,
            "recall": 0.7738589211618258
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7865862668255268,
            "auditor_fn_violation": 0.021944594318033826,
            "auditor_fp_violation": 0.019268420144575662,
            "ave_precision_score": 0.7866467431890545,
            "fpr": 0.1778265642151482,
            "logloss": 1.0775145878147843,
            "mae": 0.2867239546962457,
            "precision": 0.7011070110701108,
            "recall": 0.8050847457627118
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8006052792364442,
            "auditor_fn_violation": 0.02337892188978671,
            "auditor_fp_violation": 0.018724500203998375,
            "ave_precision_score": 0.8009727694573027,
            "fpr": 0.12390350877192982,
            "logloss": 1.0416867805229926,
            "mae": 0.2902725054126227,
            "precision": 0.7532751091703057,
            "recall": 0.7157676348547718
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7920598663368799,
            "auditor_fn_violation": 0.02195854806601053,
            "auditor_fp_violation": 0.019240915262459083,
            "ave_precision_score": 0.79257676211576,
            "fpr": 0.13391877058177826,
            "logloss": 0.9761903242183844,
            "mae": 0.28190538793295994,
            "precision": 0.7393162393162394,
            "recall": 0.7330508474576272
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8293443593138382,
            "auditor_fn_violation": 0.021422526752566076,
            "auditor_fp_violation": 0.006323949408404735,
            "ave_precision_score": 0.8296607777736236,
            "fpr": 0.05482456140350877,
            "logloss": 0.7985012968684051,
            "mae": 0.31369987263408916,
            "precision": 0.8422712933753943,
            "recall": 0.553941908713693
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.8339389965683628,
            "auditor_fn_violation": 0.020860853225176286,
            "auditor_fp_violation": 0.005721015480247745,
            "ave_precision_score": 0.8343687206302137,
            "fpr": 0.05378704720087816,
            "logloss": 0.7490101202417588,
            "mae": 0.2980206314721719,
            "precision": 0.8449367088607594,
            "recall": 0.565677966101695
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7719298245614035,
            "auc_prc": 0.851452607073423,
            "auditor_fn_violation": 0.013624244740481913,
            "auditor_fp_violation": 0.0191936964504284,
            "ave_precision_score": 0.8515540140669374,
            "fpr": 0.11732456140350878,
            "logloss": 0.8583351686813067,
            "mae": 0.25828645402472966,
            "precision": 0.7807377049180327,
            "recall": 0.7904564315352697
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8346168795821156,
            "auditor_fn_violation": 0.014846787847215763,
            "auditor_fp_violation": 0.012459711598808793,
            "ave_precision_score": 0.8350842202307024,
            "fpr": 0.1207464324917673,
            "logloss": 0.7391131160794127,
            "mae": 0.2556758467537161,
            "precision": 0.7736625514403292,
            "recall": 0.7966101694915254
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.826183970674369,
            "auditor_fn_violation": 0.019532103079274957,
            "auditor_fp_violation": 0.01193390452876377,
            "ave_precision_score": 0.8265202776821019,
            "fpr": 0.08114035087719298,
            "logloss": 0.7809680416206273,
            "mae": 0.2977693949029801,
            "precision": 0.8010752688172043,
            "recall": 0.6182572614107884
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8216881844820012,
            "auditor_fn_violation": 0.020307354555433598,
            "auditor_fp_violation": 0.005000887657559218,
            "ave_precision_score": 0.8221706525325929,
            "fpr": 0.07354555433589462,
            "logloss": 0.7395844203414954,
            "mae": 0.285378480620923,
            "precision": 0.8149171270718232,
            "recall": 0.625
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7709030669660858,
            "auditor_fn_violation": 0.028037872170051693,
            "auditor_fp_violation": 0.008787229702162386,
            "ave_precision_score": 0.7714353814292383,
            "fpr": 0.07894736842105263,
            "logloss": 1.124208210560484,
            "mae": 0.3324966368040139,
            "precision": 0.7888563049853372,
            "recall": 0.558091286307054
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7751331097420682,
            "auditor_fn_violation": 0.030330796852034464,
            "auditor_fp_violation": 0.009756731819898033,
            "ave_precision_score": 0.7760263134855981,
            "fpr": 0.09110867178924259,
            "logloss": 0.9874547084075648,
            "mae": 0.309430672466563,
            "precision": 0.7700831024930748,
            "recall": 0.5889830508474576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.7483281441623862,
            "auditor_fn_violation": 0.03274004513358084,
            "auditor_fp_violation": 0.015049979600163202,
            "ave_precision_score": 0.748894841198803,
            "fpr": 0.1162280701754386,
            "logloss": 1.1068053787611987,
            "mae": 0.34161868578837373,
            "precision": 0.7302798982188295,
            "recall": 0.5954356846473029
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.7235002045139247,
            "auditor_fn_violation": 0.02733074103704256,
            "auditor_fp_violation": 0.014482570656291497,
            "ave_precision_score": 0.7238264025852716,
            "fpr": 0.13391877058177826,
            "logloss": 1.0640136059358816,
            "mae": 0.33797085401475585,
            "precision": 0.7095238095238096,
            "recall": 0.6313559322033898
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.771448390216374,
            "auditor_fn_violation": 0.031031611705612586,
            "auditor_fp_violation": 0.009373725010199922,
            "ave_precision_score": 0.7725116394331051,
            "fpr": 0.0756578947368421,
            "logloss": 1.1305327302976549,
            "mae": 0.33410021658498007,
            "precision": 0.7934131736526946,
            "recall": 0.549792531120332
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7778690655813215,
            "auditor_fn_violation": 0.03494948743232433,
            "auditor_fp_violation": 0.012327188075883476,
            "ave_precision_score": 0.7784342423777821,
            "fpr": 0.0867178924259056,
            "logloss": 0.9837447571516753,
            "mae": 0.30850796379798107,
            "precision": 0.782967032967033,
            "recall": 0.6038135593220338
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 25349,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7671546060204089,
            "auditor_fn_violation": 0.015043768654000146,
            "auditor_fp_violation": 0.017844757241942073,
            "ave_precision_score": 0.7674506597909216,
            "fpr": 0.13596491228070176,
            "logloss": 0.9880355163451333,
            "mae": 0.3177673249924418,
            "precision": 0.7250554323725056,
            "recall": 0.6784232365145229
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7359641990745776,
            "auditor_fn_violation": 0.01585610895086421,
            "auditor_fp_violation": 0.02354917997944635,
            "ave_precision_score": 0.7355671795024015,
            "fpr": 0.15916575192096596,
            "logloss": 1.0600061271248051,
            "mae": 0.3275508513826149,
            "precision": 0.692144373673036,
            "recall": 0.690677966101695
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7711631526230219,
            "auditor_fn_violation": 0.029175311203319513,
            "auditor_fp_violation": 0.008389432884536929,
            "ave_precision_score": 0.7722261480487507,
            "fpr": 0.07675438596491228,
            "logloss": 1.142426499882604,
            "mae": 0.3342869417386667,
            "precision": 0.7922848664688428,
            "recall": 0.553941908713693
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7761020614268122,
            "auditor_fn_violation": 0.034365755641965444,
            "auditor_fp_violation": 0.008584023664200401,
            "ave_precision_score": 0.7770109480254699,
            "fpr": 0.08562019758507135,
            "logloss": 0.9972790438026617,
            "mae": 0.309170781222042,
            "precision": 0.7839335180055401,
            "recall": 0.5995762711864406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.772652183599181,
            "auditor_fn_violation": 0.030290001455921978,
            "auditor_fp_violation": 0.009128926968584255,
            "ave_precision_score": 0.7735344164691919,
            "fpr": 0.07785087719298246,
            "logloss": 1.1241454781958058,
            "mae": 0.3311300585557068,
            "precision": 0.7923976608187134,
            "recall": 0.5622406639004149
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7779815196838378,
            "auditor_fn_violation": 0.03370295261307188,
            "auditor_fp_violation": 0.013069819893031017,
            "ave_precision_score": 0.7787953785057622,
            "fpr": 0.09001097694840834,
            "logloss": 0.9815696876280529,
            "mae": 0.30602588927205254,
            "precision": 0.7789757412398922,
            "recall": 0.6122881355932204
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.773683904660849,
            "auditor_fn_violation": 0.03070175438596492,
            "auditor_fp_violation": 0.0137265401876785,
            "ave_precision_score": 0.7747203329694505,
            "fpr": 0.07785087719298246,
            "logloss": 1.1286946074275217,
            "mae": 0.33543872247350015,
            "precision": 0.7893175074183977,
            "recall": 0.5518672199170125
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7711828613991903,
            "auditor_fn_violation": 0.028154012167668243,
            "auditor_fp_violation": 0.007096259586076529,
            "ave_precision_score": 0.7721173205384593,
            "fpr": 0.08562019758507135,
            "logloss": 1.0053338081430048,
            "mae": 0.31230725763271927,
            "precision": 0.7784090909090909,
            "recall": 0.5805084745762712
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7693954937424796,
            "auditor_fn_violation": 0.02205721773312951,
            "auditor_fp_violation": 0.02212362301101592,
            "ave_precision_score": 0.7705068595123864,
            "fpr": 0.12280701754385964,
            "logloss": 1.0239138597586337,
            "mae": 0.31818645398187834,
            "precision": 0.7358490566037735,
            "recall": 0.6473029045643154
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7721826302563297,
            "auditor_fn_violation": 0.03222385532754098,
            "auditor_fp_violation": 0.008456501028932636,
            "ave_precision_score": 0.7729782862136585,
            "fpr": 0.12623490669593854,
            "logloss": 0.9220539345547211,
            "mae": 0.2941515573974587,
            "precision": 0.7404063205417607,
            "recall": 0.6949152542372882
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7964241593115149,
            "auditor_fn_violation": 0.025715221664118805,
            "auditor_fp_violation": 0.02282996736026112,
            "ave_precision_score": 0.7974211902254768,
            "fpr": 0.13048245614035087,
            "logloss": 0.9239892210892223,
            "mae": 0.28591059835454247,
            "precision": 0.7510460251046025,
            "recall": 0.7448132780082988
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.7911187059573599,
            "auditor_fn_violation": 0.021991106811289515,
            "auditor_fp_violation": 0.02640968771957023,
            "ave_precision_score": 0.7917551909839917,
            "fpr": 0.141602634467618,
            "logloss": 0.8517033821495572,
            "mae": 0.27286747952665447,
            "precision": 0.7404426559356136,
            "recall": 0.7796610169491526
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7816330682451417,
            "auditor_fn_violation": 0.02933682754604354,
            "auditor_fp_violation": 0.014856181150550799,
            "ave_precision_score": 0.7821286748477043,
            "fpr": 0.08552631578947369,
            "logloss": 1.045541682075951,
            "mae": 0.32105797319533386,
            "precision": 0.7833333333333333,
            "recall": 0.5850622406639004
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7868911351384101,
            "auditor_fn_violation": 0.028026102811215103,
            "auditor_fp_violation": 0.008519012124652127,
            "ave_precision_score": 0.7875408383282902,
            "fpr": 0.09549945115257959,
            "logloss": 0.9195874617081581,
            "mae": 0.2977223939738956,
            "precision": 0.7746113989637305,
            "recall": 0.6334745762711864
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8375679258491315,
            "auditor_fn_violation": 0.015098365727597002,
            "auditor_fp_violation": 0.017860057119543048,
            "ave_precision_score": 0.8378387259199596,
            "fpr": 0.11403508771929824,
            "logloss": 0.7304213009998872,
            "mae": 0.2633836031059111,
            "precision": 0.7791932059447984,
            "recall": 0.7614107883817427
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8193667103761845,
            "auditor_fn_violation": 0.011088578392156136,
            "auditor_fp_violation": 0.015400233541453612,
            "ave_precision_score": 0.8196240059771811,
            "fpr": 0.12184412733260154,
            "logloss": 0.7564937981929892,
            "mae": 0.26708156771541247,
            "precision": 0.7643312101910829,
            "recall": 0.7627118644067796
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7768778564240815,
            "auditor_fn_violation": 0.03001701608793769,
            "auditor_fp_violation": 0.010044369645042843,
            "ave_precision_score": 0.7779043013095863,
            "fpr": 0.07785087719298246,
            "logloss": 1.0861672720837172,
            "mae": 0.3294164578052408,
            "precision": 0.7923976608187134,
            "recall": 0.5622406639004149
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7809477380036143,
            "auditor_fn_violation": 0.03385179259149008,
            "auditor_fp_violation": 0.009124119531216793,
            "ave_precision_score": 0.7816617416880798,
            "fpr": 0.0889132821075741,
            "logloss": 0.9649401998919145,
            "mae": 0.3077527459835115,
            "precision": 0.7786885245901639,
            "recall": 0.6038135593220338
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7636642272503068,
            "auditor_fn_violation": 0.03218952464147922,
            "auditor_fp_violation": 0.01095216238270094,
            "ave_precision_score": 0.7646515584715998,
            "fpr": 0.08223684210526316,
            "logloss": 1.1558691740706344,
            "mae": 0.33758025710573475,
            "precision": 0.7787610619469026,
            "recall": 0.5477178423236515
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7651088246363958,
            "auditor_fn_violation": 0.028656347094829684,
            "auditor_fp_violation": 0.014557583971154883,
            "ave_precision_score": 0.7658469445082151,
            "fpr": 0.09220636663007684,
            "logloss": 1.0272909548671305,
            "mae": 0.3131365028825813,
            "precision": 0.771117166212534,
            "recall": 0.5995762711864406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7699348332965522,
            "auditor_fn_violation": 0.02213001383125865,
            "auditor_fp_violation": 0.029064667482660147,
            "ave_precision_score": 0.7702383696552286,
            "fpr": 0.1513157894736842,
            "logloss": 1.246199100474699,
            "mae": 0.3106970306685629,
            "precision": 0.7136929460580913,
            "recall": 0.7136929460580913
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.7364021564935002,
            "auditor_fn_violation": 0.02086550447450186,
            "auditor_fp_violation": 0.029042655071275156,
            "ave_precision_score": 0.7352151998549815,
            "fpr": 0.17892425905598244,
            "logloss": 1.2569390659549893,
            "mae": 0.3284836955904456,
            "precision": 0.681640625,
            "recall": 0.739406779661017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7830792998143546,
            "auditor_fn_violation": 0.02545588556453374,
            "auditor_fp_violation": 0.014677682578539375,
            "ave_precision_score": 0.783524135544486,
            "fpr": 0.09649122807017543,
            "logloss": 1.0461801498602108,
            "mae": 0.3158531757297942,
            "precision": 0.7702349869451697,
            "recall": 0.6120331950207469
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7873631069183703,
            "auditor_fn_violation": 0.02974939068633835,
            "auditor_fp_violation": 0.009604204746342479,
            "ave_precision_score": 0.7880160927274426,
            "fpr": 0.10208562019758508,
            "logloss": 0.9257875351151348,
            "mae": 0.2913574677498517,
            "precision": 0.7703703703703704,
            "recall": 0.6610169491525424
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7714761636758996,
            "auditor_fn_violation": 0.029184410715585665,
            "auditor_fp_violation": 0.009128926968584255,
            "ave_precision_score": 0.7725359307413056,
            "fpr": 0.07785087719298246,
            "logloss": 1.1136893309807145,
            "mae": 0.33351655963609067,
            "precision": 0.7911764705882353,
            "recall": 0.558091286307054
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7753711838000372,
            "auditor_fn_violation": 0.03421459003888444,
            "auditor_fp_violation": 0.009891755786652138,
            "ave_precision_score": 0.7761936898145263,
            "fpr": 0.08781558726673985,
            "logloss": 0.9794128439296922,
            "mae": 0.30897360844694166,
            "precision": 0.7802197802197802,
            "recall": 0.6016949152542372
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8362404579987723,
            "auditor_fn_violation": 0.01723447623207397,
            "auditor_fp_violation": 0.01778610771113831,
            "ave_precision_score": 0.8365277644775215,
            "fpr": 0.09100877192982457,
            "logloss": 0.7048772916471746,
            "mae": 0.27174083020608414,
            "precision": 0.8047058823529412,
            "recall": 0.7095435684647303
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8247717193461241,
            "auditor_fn_violation": 0.018742209157379675,
            "auditor_fp_violation": 0.006046073177989096,
            "ave_precision_score": 0.8252161635994881,
            "fpr": 0.09769484083424808,
            "logloss": 0.6891661963817586,
            "mae": 0.26731666419808886,
            "precision": 0.7910798122065728,
            "recall": 0.7139830508474576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8296418336840105,
            "auditor_fn_violation": 0.014488698405765449,
            "auditor_fp_violation": 0.007708588331293351,
            "ave_precision_score": 0.8299536419768337,
            "fpr": 0.06469298245614036,
            "logloss": 0.738047242710512,
            "mae": 0.30695264187514676,
            "precision": 0.8284883720930233,
            "recall": 0.5912863070539419
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.828713711165882,
            "auditor_fn_violation": 0.019932928984725297,
            "auditor_fp_violation": 0.00767136166669584,
            "ave_precision_score": 0.8291606935913132,
            "fpr": 0.06695938529088913,
            "logloss": 0.714265627985421,
            "mae": 0.2943875812728444,
            "precision": 0.8236994219653179,
            "recall": 0.6038135593220338
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.7707159820296536,
            "auditor_fn_violation": 0.028920524859867524,
            "auditor_fp_violation": 0.015412076703386375,
            "ave_precision_score": 0.7717578398643882,
            "fpr": 0.08333333333333333,
            "logloss": 1.1003721024110897,
            "mae": 0.3328973593162704,
            "precision": 0.7809798270893372,
            "recall": 0.5622406639004149
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7676495081862099,
            "auditor_fn_violation": 0.031968036614634696,
            "auditor_fp_violation": 0.007856394510025533,
            "ave_precision_score": 0.7687481229985943,
            "fpr": 0.09549945115257959,
            "logloss": 0.9883724551074925,
            "mae": 0.3104094989288046,
            "precision": 0.767379679144385,
            "recall": 0.6080508474576272
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 25349,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8396715707499074,
            "auditor_fn_violation": 0.010041311785688293,
            "auditor_fp_violation": 0.025576295389636884,
            "ave_precision_score": 0.8399433547214802,
            "fpr": 0.12609649122807018,
            "logloss": 0.6895379532141137,
            "mae": 0.2618969978408857,
            "precision": 0.7653061224489796,
            "recall": 0.7780082987551867
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8179544858587768,
            "auditor_fn_violation": 0.012572326927012595,
            "auditor_fp_violation": 0.017498105913799696,
            "ave_precision_score": 0.8182935560125463,
            "fpr": 0.13830954994511527,
            "logloss": 0.744596383025426,
            "mae": 0.2695541073891454,
            "precision": 0.7459677419354839,
            "recall": 0.7838983050847458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8258084399441585,
            "auditor_fn_violation": 0.013094198150979106,
            "auditor_fp_violation": 0.013718890248878013,
            "ave_precision_score": 0.826115723424085,
            "fpr": 0.07675438596491228,
            "logloss": 0.8776476216093922,
            "mae": 0.2882057185674398,
            "precision": 0.8087431693989071,
            "recall": 0.6141078838174274
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7981850049164951,
            "auditor_fn_violation": 0.017004967534279717,
            "auditor_fp_violation": 0.011969624608367986,
            "ave_precision_score": 0.7986030120450954,
            "fpr": 0.08781558726673985,
            "logloss": 0.9145273131710132,
            "mae": 0.296168477487176,
            "precision": 0.7849462365591398,
            "recall": 0.6186440677966102
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7872285104409116,
            "auditor_fn_violation": 0.01276206595326491,
            "auditor_fp_violation": 0.020257037943696456,
            "ave_precision_score": 0.7876655976291682,
            "fpr": 0.14035087719298245,
            "logloss": 0.9787289135219119,
            "mae": 0.2984526398291914,
            "precision": 0.7344398340248963,
            "recall": 0.7344398340248963
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.7483434406410382,
            "auditor_fn_violation": 0.016865430054512645,
            "auditor_fp_violation": 0.023121604084725047,
            "ave_precision_score": 0.7467396104376011,
            "fpr": 0.1734357848518112,
            "logloss": 1.1392557322855912,
            "mae": 0.3233626560384725,
            "precision": 0.6877470355731226,
            "recall": 0.7372881355932204
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6041666666666666,
            "auc_prc": 0.7558595356384685,
            "auditor_fn_violation": 0.015874099148285654,
            "auditor_fp_violation": 0.0038402692778457784,
            "ave_precision_score": 0.7573423544986171,
            "fpr": 0.039473684210526314,
            "logloss": 1.1539992365828569,
            "mae": 0.396046629054764,
            "precision": 0.8134715025906736,
            "recall": 0.3257261410788382
        },
        "train": {
            "accuracy": 0.6498353457738749,
            "auc_prc": 0.81312354736853,
            "auditor_fn_violation": 0.015358425273028333,
            "auditor_fp_violation": 0.004903370348236812,
            "ave_precision_score": 0.814412906340279,
            "fpr": 0.02305159165751921,
            "logloss": 1.0201208607280068,
            "mae": 0.3622664491120588,
            "precision": 0.8923076923076924,
            "recall": 0.3686440677966102
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.8320513816345205,
            "auditor_fn_violation": 0.02577436849384874,
            "auditor_fp_violation": 0.009185026519787845,
            "ave_precision_score": 0.83252699733801,
            "fpr": 0.07236842105263158,
            "logloss": 0.7217542750504051,
            "mae": 0.30491108173664866,
            "precision": 0.8130311614730878,
            "recall": 0.5954356846473029
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.8343414615800641,
            "auditor_fn_violation": 0.026823754860555552,
            "auditor_fp_violation": 0.009449177228958145,
            "ave_precision_score": 0.8346454098500851,
            "fpr": 0.07574094401756312,
            "logloss": 0.6966061739227364,
            "mae": 0.29494729349833804,
            "precision": 0.807799442896936,
            "recall": 0.614406779661017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.8073986269820688,
            "auditor_fn_violation": 0.028149341195311938,
            "auditor_fp_violation": 0.009251325989392083,
            "ave_precision_score": 0.8087057450137338,
            "fpr": 0.05701754385964912,
            "logloss": 0.8846453602567956,
            "mae": 0.3313356344526695,
            "precision": 0.8266666666666667,
            "recall": 0.5145228215767634
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.8355974567038514,
            "auditor_fn_violation": 0.021498074382779217,
            "auditor_fp_violation": 0.005343448462102023,
            "ave_precision_score": 0.8358923064181388,
            "fpr": 0.05378704720087816,
            "logloss": 0.8075651859368959,
            "mae": 0.3137976799964905,
            "precision": 0.8361204013377926,
            "recall": 0.5296610169491526
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7656721617245659,
            "auditor_fn_violation": 0.018476559656402426,
            "auditor_fp_violation": 0.025275397796817627,
            "ave_precision_score": 0.7660440736527601,
            "fpr": 0.12828947368421054,
            "logloss": 0.9985775629025421,
            "mae": 0.32117636659498267,
            "precision": 0.728538283062645,
            "recall": 0.6514522821576764
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.7342714319864496,
            "auditor_fn_violation": 0.018407319205938712,
            "auditor_fp_violation": 0.022248949188480957,
            "ave_precision_score": 0.7341360192578565,
            "fpr": 0.15148188803512624,
            "logloss": 1.0585210006478016,
            "mae": 0.32912772938461243,
            "precision": 0.6967032967032967,
            "recall": 0.6716101694915254
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8490922909508256,
            "auditor_fn_violation": 0.018035233311494512,
            "auditor_fp_violation": 0.0193390452876377,
            "ave_precision_score": 0.8493359392489113,
            "fpr": 0.10087719298245613,
            "logloss": 0.7314823089076326,
            "mae": 0.2649965739861532,
            "precision": 0.7923250564334086,
            "recall": 0.7282157676348547
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8397470741484123,
            "auditor_fn_violation": 0.016670077582838755,
            "auditor_fp_violation": 0.018170725303741416,
            "ave_precision_score": 0.8400417630241002,
            "fpr": 0.11525795828759605,
            "logloss": 0.7144010135052106,
            "mae": 0.26562220087234045,
            "precision": 0.7682119205298014,
            "recall": 0.7372881355932204
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.753757293049121,
            "auditor_fn_violation": 0.02686403508771931,
            "auditor_fp_violation": 0.024196756425948596,
            "ave_precision_score": 0.7542924535455644,
            "fpr": 0.14583333333333334,
            "logloss": 1.0298508189549531,
            "mae": 0.32512921403281836,
            "precision": 0.7121212121212122,
            "recall": 0.6825726141078838
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.7274078482286246,
            "auditor_fn_violation": 0.018981748497646465,
            "auditor_fp_violation": 0.01679048031025508,
            "ave_precision_score": 0.7279250628191238,
            "fpr": 0.16575192096597147,
            "logloss": 1.0555941602106416,
            "mae": 0.32634551513325605,
            "precision": 0.6937119675456389,
            "recall": 0.7245762711864406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.7585029347180684,
            "auditor_fn_violation": 0.02858611778408678,
            "auditor_fp_violation": 0.007853937168502655,
            "ave_precision_score": 0.7596019501928699,
            "fpr": 0.07456140350877193,
            "logloss": 1.2266689020727237,
            "mae": 0.3439875635437966,
            "precision": 0.7861635220125787,
            "recall": 0.5186721991701245
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7594234535647002,
            "auditor_fn_violation": 0.031033135500195368,
            "auditor_fp_violation": 0.01169207534337345,
            "ave_precision_score": 0.7610986096530394,
            "fpr": 0.0889132821075741,
            "logloss": 1.0799183262638758,
            "mae": 0.3195863480727571,
            "precision": 0.7692307692307693,
            "recall": 0.5720338983050848
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7749220406440646,
            "auditor_fn_violation": 0.026427258498944457,
            "auditor_fp_violation": 0.016702366381068954,
            "ave_precision_score": 0.7759552455810756,
            "fpr": 0.08333333333333333,
            "logloss": 1.0899132256554591,
            "mae": 0.33026899157656686,
            "precision": 0.7822349570200573,
            "recall": 0.5663900414937759
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7728512629338271,
            "auditor_fn_violation": 0.03130058233641557,
            "auditor_fp_violation": 0.007856394510025533,
            "ave_precision_score": 0.7737650791348261,
            "fpr": 0.09549945115257959,
            "logloss": 0.974472832049929,
            "mae": 0.3072217118513149,
            "precision": 0.767379679144385,
            "recall": 0.6080508474576272
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.8266194821218453,
            "auditor_fn_violation": 0.014777607920215482,
            "auditor_fp_violation": 0.010046919624643007,
            "ave_precision_score": 0.826940842555739,
            "fpr": 0.07236842105263158,
            "logloss": 0.729176114474345,
            "mae": 0.3010382809029081,
            "precision": 0.8161559888579387,
            "recall": 0.6078838174273858
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8276302689195298,
            "auditor_fn_violation": 0.02523302759121101,
            "auditor_fp_violation": 0.004380777588021877,
            "ave_precision_score": 0.8280873929279915,
            "fpr": 0.07025246981339188,
            "logloss": 0.6945268981426973,
            "mae": 0.2882009898095439,
            "precision": 0.8176638176638177,
            "recall": 0.6080508474576272
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8297610093003127,
            "auditor_fn_violation": 0.009904819101696149,
            "auditor_fp_violation": 0.011729906160750715,
            "ave_precision_score": 0.8300785233498312,
            "fpr": 0.07236842105263158,
            "logloss": 0.7073699375552038,
            "mae": 0.3017399891552163,
            "precision": 0.8181818181818182,
            "recall": 0.6161825726141079
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8237905947569022,
            "auditor_fn_violation": 0.019032912240227736,
            "auditor_fp_violation": 0.005728516811734086,
            "ave_precision_score": 0.824288433977233,
            "fpr": 0.07244785949506037,
            "logloss": 0.7005982141355614,
            "mae": 0.2934798112631339,
            "precision": 0.8156424581005587,
            "recall": 0.6186440677966102
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 25349,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8264983210388434,
            "auditor_fn_violation": 0.014941399141006046,
            "auditor_fp_violation": 0.016809465524275802,
            "ave_precision_score": 0.8268074450099989,
            "fpr": 0.12280701754385964,
            "logloss": 0.801319482603593,
            "mae": 0.26764913329941514,
            "precision": 0.7656903765690377,
            "recall": 0.7593360995850622
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8054431558758368,
            "auditor_fn_violation": 0.01447003665184469,
            "auditor_fp_violation": 0.016457921281027386,
            "ave_precision_score": 0.8058353170405176,
            "fpr": 0.1350164654226125,
            "logloss": 0.8297237917731214,
            "mae": 0.27592042090638663,
            "precision": 0.7442827442827443,
            "recall": 0.7584745762711864
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.7500299520808947,
            "auditor_fn_violation": 0.03165265341777681,
            "auditor_fp_violation": 0.013968788249694,
            "ave_precision_score": 0.7505858175154634,
            "fpr": 0.11732456140350878,
            "logloss": 1.0994106878325787,
            "mae": 0.3410393743309866,
            "precision": 0.7291139240506329,
            "recall": 0.5975103734439834
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.7257125257349437,
            "auditor_fn_violation": 0.02744004539619343,
            "auditor_fp_violation": 0.010394344996236836,
            "ave_precision_score": 0.7259149798177007,
            "fpr": 0.13611416026344675,
            "logloss": 1.0555190173848927,
            "mae": 0.3363726835784831,
            "precision": 0.706855791962175,
            "recall": 0.6334745762711864
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.823572455073511,
            "auditor_fn_violation": 0.01830821867947879,
            "auditor_fp_violation": 0.016421868625051003,
            "ave_precision_score": 0.8239682396360061,
            "fpr": 0.07675438596491228,
            "logloss": 0.7413247603635733,
            "mae": 0.3075267654285408,
            "precision": 0.803921568627451,
            "recall": 0.5954356846473029
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8314667317352784,
            "auditor_fn_violation": 0.017953822396695754,
            "auditor_fp_violation": 0.007656359003723161,
            "ave_precision_score": 0.831870060669356,
            "fpr": 0.06915477497255763,
            "logloss": 0.6771756331715557,
            "mae": 0.28843614002238716,
            "precision": 0.8245125348189415,
            "recall": 0.6271186440677966
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8550604348016841,
            "auditor_fn_violation": 0.024100058236878514,
            "auditor_fp_violation": 0.013999388004895965,
            "ave_precision_score": 0.8553027720651766,
            "fpr": 0.06798245614035088,
            "logloss": 0.6189534441525196,
            "mae": 0.29011060556694357,
            "precision": 0.8310626702997275,
            "recall": 0.6327800829875518
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8495698438721764,
            "auditor_fn_violation": 0.023451599099518133,
            "auditor_fp_violation": 0.0014377552015482763,
            "ave_precision_score": 0.8499961554221332,
            "fpr": 0.06366630076838639,
            "logloss": 0.5956301191137646,
            "mae": 0.27909132739826903,
            "precision": 0.8406593406593407,
            "recall": 0.6483050847457628
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.7599359982488925,
            "auditor_fn_violation": 0.030428769017980646,
            "auditor_fp_violation": 0.008606181150550793,
            "ave_precision_score": 0.760778578473192,
            "fpr": 0.08223684210526316,
            "logloss": 1.1676013157749054,
            "mae": 0.34100677506572713,
            "precision": 0.7761194029850746,
            "recall": 0.5394190871369294
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7607506901197792,
            "auditor_fn_violation": 0.03123081359653203,
            "auditor_fp_violation": 0.013619917535362529,
            "ave_precision_score": 0.7614498759473135,
            "fpr": 0.09440175631174534,
            "logloss": 1.0450844602958027,
            "mae": 0.31747340050563433,
            "precision": 0.7643835616438356,
            "recall": 0.5911016949152542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7995243927245781,
            "auditor_fn_violation": 0.022757880177622477,
            "auditor_fp_violation": 0.006196450428396574,
            "ave_precision_score": 0.8010072102325844,
            "fpr": 0.05263157894736842,
            "logloss": 0.9598450451971174,
            "mae": 0.3417906576939067,
            "precision": 0.8315789473684211,
            "recall": 0.491701244813278
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.8293981182259624,
            "auditor_fn_violation": 0.017986381141974737,
            "auditor_fp_violation": 0.0065786677135191525,
            "ave_precision_score": 0.8296374306621551,
            "fpr": 0.052689352360043906,
            "logloss": 0.9135968624971561,
            "mae": 0.32996056394589685,
            "precision": 0.8315789473684211,
            "recall": 0.5021186440677966
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7968877360485759,
            "auditor_fn_violation": 0.018390114289874065,
            "auditor_fp_violation": 0.018130354957160345,
            "ave_precision_score": 0.7973552735845507,
            "fpr": 0.10964912280701754,
            "logloss": 1.004058355760042,
            "mae": 0.3020510945356095,
            "precision": 0.7578692493946732,
            "recall": 0.6493775933609959
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7907713914187882,
            "auditor_fn_violation": 0.027647025991181233,
            "auditor_fp_violation": 0.008776557839016427,
            "ave_precision_score": 0.7912976628860745,
            "fpr": 0.1119648737650933,
            "logloss": 0.9068748822747265,
            "mae": 0.2837779787066964,
            "precision": 0.7627906976744186,
            "recall": 0.6949152542372882
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7796171177097883,
            "auditor_fn_violation": 0.024186503603406864,
            "auditor_fp_violation": 0.024556303549571612,
            "ave_precision_score": 0.7801386114093517,
            "fpr": 0.1425438596491228,
            "logloss": 0.9602254468787047,
            "mae": 0.3093901553961956,
            "precision": 0.7251585623678647,
            "recall": 0.7116182572614108
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7651722218613248,
            "auditor_fn_violation": 0.030961041135649042,
            "auditor_fp_violation": 0.019030877980841604,
            "ave_precision_score": 0.7658516144074953,
            "fpr": 0.15697036223929747,
            "logloss": 0.9174327720702439,
            "mae": 0.29878659041584826,
            "precision": 0.7162698412698413,
            "recall": 0.7648305084745762
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.8267417687769425,
            "auditor_fn_violation": 0.02173191016961492,
            "auditor_fp_violation": 0.007966136270909835,
            "ave_precision_score": 0.827065835677671,
            "fpr": 0.05701754385964912,
            "logloss": 0.7921037395729129,
            "mae": 0.3165150122151742,
            "precision": 0.8359621451104101,
            "recall": 0.549792531120332
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.830076895143496,
            "auditor_fn_violation": 0.02337717911030903,
            "auditor_fp_violation": 0.0019128395290164018,
            "ave_precision_score": 0.8305140211713872,
            "fpr": 0.0570801317233809,
            "logloss": 0.748844717717345,
            "mae": 0.3020500647847798,
            "precision": 0.834920634920635,
            "recall": 0.5572033898305084
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 25349,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.778439771522033,
            "auditor_fn_violation": 0.028991046079930128,
            "auditor_fp_violation": 0.014575683394532843,
            "ave_precision_score": 0.779431948055429,
            "fpr": 0.0800438596491228,
            "logloss": 1.076394348189135,
            "mae": 0.32920366106041793,
            "precision": 0.7871720116618076,
            "recall": 0.5601659751037344
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7862616603257142,
            "auditor_fn_violation": 0.027874937208134113,
            "auditor_fp_violation": 0.009186630626936282,
            "ave_precision_score": 0.7869019220545308,
            "fpr": 0.0867178924259056,
            "logloss": 0.9338735649328352,
            "mae": 0.3019667503066729,
            "precision": 0.7882037533512064,
            "recall": 0.6228813559322034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8496565105837282,
            "auditor_fn_violation": 0.013353534250564179,
            "auditor_fp_violation": 0.01756425948592412,
            "ave_precision_score": 0.8499146853292323,
            "fpr": 0.10526315789473684,
            "logloss": 0.714233092542945,
            "mae": 0.26414165425829805,
            "precision": 0.788546255506608,
            "recall": 0.7427385892116183
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8386351629019884,
            "auditor_fn_violation": 0.014977022828331691,
            "auditor_fp_violation": 0.009564197645082005,
            "ave_precision_score": 0.8390656864305733,
            "fpr": 0.1119648737650933,
            "logloss": 0.6756284124476584,
            "mae": 0.2602777391258078,
            "precision": 0.7829787234042553,
            "recall": 0.7796610169491526
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 25349,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7784137008865336,
            "auditor_fn_violation": 0.03195748707869258,
            "auditor_fp_violation": 0.014947980416156671,
            "ave_precision_score": 0.7795622336399473,
            "fpr": 0.0800438596491228,
            "logloss": 1.0613007914776587,
            "mae": 0.32561214255685555,
            "precision": 0.7908309455587392,
            "recall": 0.5726141078838174
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.78523189486881,
            "auditor_fn_violation": 0.03423552066084952,
            "auditor_fp_violation": 0.013069819893031017,
            "ave_precision_score": 0.7860941282864105,
            "fpr": 0.09001097694840834,
            "logloss": 0.924337320654198,
            "mae": 0.3004667398394649,
            "precision": 0.7813333333333333,
            "recall": 0.6207627118644068
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 25349,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8307222688258971,
            "auditor_fn_violation": 0.01116055179442382,
            "auditor_fp_violation": 0.013604141166870668,
            "ave_precision_score": 0.8310312347777582,
            "fpr": 0.09320175438596491,
            "logloss": 0.6957385786847601,
            "mae": 0.29321686377555745,
            "precision": 0.7875,
            "recall": 0.6535269709543569
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8270835412583916,
            "auditor_fn_violation": 0.023098104150774908,
            "auditor_fp_violation": 0.0068712196414863645,
            "ave_precision_score": 0.8275158303080745,
            "fpr": 0.0801317233809001,
            "logloss": 0.670896783098451,
            "mae": 0.28409948128017726,
            "precision": 0.8053333333333333,
            "recall": 0.6398305084745762
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 20404,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.807806677075769,
            "auditor_fn_violation": 0.012602273621272914,
            "auditor_fp_violation": 0.011364919810255263,
            "ave_precision_score": 0.8083604462466001,
            "fpr": 0.19188596491228072,
            "logloss": 1.0994990993121492,
            "mae": 0.30240145890370773,
            "precision": 0.6823956442831216,
            "recall": 0.8430493273542601
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8688855032606596,
            "auditor_fn_violation": 0.02061202969826357,
            "auditor_fp_violation": 0.020197040309642558,
            "ave_precision_score": 0.8690703950495766,
            "fpr": 0.13062568605927552,
            "logloss": 1.810409816190911,
            "mae": 0.2782999506296478,
            "precision": 0.7746212121212122,
            "recall": 0.8051181102362205
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.7798612962719862,
            "auditor_fn_violation": 0.014141294941389349,
            "auditor_fp_violation": 0.016390896769821556,
            "ave_precision_score": 0.7659801631274032,
            "fpr": 0.17763157894736842,
            "logloss": 1.8034628503952859,
            "mae": 0.2767263388103375,
            "precision": 0.6943396226415094,
            "recall": 0.8251121076233184
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8157138776428975,
            "auditor_fn_violation": 0.017044521465552267,
            "auditor_fp_violation": 0.023193229701497824,
            "ave_precision_score": 0.804921122652365,
            "fpr": 0.12733260153677278,
            "logloss": 2.369488435939342,
            "mae": 0.2616750391181903,
            "precision": 0.77431906614786,
            "recall": 0.7834645669291339
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7436118674319108,
            "auditor_fn_violation": 0.006569113366375585,
            "auditor_fp_violation": 0.009722535953617951,
            "ave_precision_score": 0.7445114484106861,
            "fpr": 0.0756578947368421,
            "logloss": 1.031308788306255,
            "mae": 0.3383978121814594,
            "precision": 0.790273556231003,
            "recall": 0.5829596412556054
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8627870768978653,
            "auditor_fn_violation": 0.007528285089501019,
            "auditor_fp_violation": 0.0067005690036036,
            "ave_precision_score": 0.8629709092676261,
            "fpr": 0.031833150384193196,
            "logloss": 1.9020183419208747,
            "mae": 0.33624533351923297,
            "precision": 0.9085173501577287,
            "recall": 0.5669291338582677
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8137575190381479,
            "auditor_fn_violation": 0.01385119188104791,
            "auditor_fp_violation": 0.019438012950832008,
            "ave_precision_score": 0.8141335602595254,
            "fpr": 0.23135964912280702,
            "logloss": 1.1236185172065025,
            "mae": 0.3134403479663767,
            "precision": 0.6500829187396352,
            "recall": 0.8789237668161435
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8783629008639355,
            "auditor_fn_violation": 0.012869823763796813,
            "auditor_fp_violation": 0.012654814467781434,
            "ave_precision_score": 0.8784847412289895,
            "fpr": 0.17672886937431395,
            "logloss": 1.8510184600508583,
            "mae": 0.29046310725532676,
            "precision": 0.7228915662650602,
            "recall": 0.8267716535433071
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7903199754501857,
            "auditor_fn_violation": 0.008191723703878533,
            "auditor_fp_violation": 0.016230893758000152,
            "ave_precision_score": 0.7917679404743085,
            "fpr": 0.1699561403508772,
            "logloss": 1.0708139811615727,
            "mae": 0.2999646429505005,
            "precision": 0.7013487475915221,
            "recall": 0.8161434977578476
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8523227839704502,
            "auditor_fn_violation": 0.01175484239003604,
            "auditor_fp_violation": 0.022057401541130874,
            "ave_precision_score": 0.8525516202574113,
            "fpr": 0.11086717892425905,
            "logloss": 1.809950668069729,
            "mae": 0.27857758583712366,
            "precision": 0.7934560327198364,
            "recall": 0.7637795275590551
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8186845879792226,
            "auditor_fn_violation": 0.013816772873888758,
            "auditor_fp_violation": 0.017736804457495675,
            "ave_precision_score": 0.8190800915147551,
            "fpr": 0.19078947368421054,
            "logloss": 1.0632259292745725,
            "mae": 0.2971886290872218,
            "precision": 0.6864864864864865,
            "recall": 0.8542600896860987
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8766056777572929,
            "auditor_fn_violation": 0.015912253558864973,
            "auditor_fp_violation": 0.01935538347138503,
            "ave_precision_score": 0.8767485724175978,
            "fpr": 0.12952799121844127,
            "logloss": 1.747738457789124,
            "mae": 0.27296964950713815,
            "precision": 0.7790262172284644,
            "recall": 0.8188976377952756
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.7990524610150394,
            "auditor_fn_violation": 0.010652682715758,
            "auditor_fp_violation": 0.012997891725020715,
            "ave_precision_score": 0.7995166384342924,
            "fpr": 0.15679824561403508,
            "logloss": 0.9268388413111406,
            "mae": 0.32001407531464554,
            "precision": 0.709349593495935,
            "recall": 0.7825112107623319
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8719229391809166,
            "auditor_fn_violation": 0.00799502147851716,
            "auditor_fp_violation": 7.899044760345743e-05,
            "ave_precision_score": 0.8720525135983282,
            "fpr": 0.09879253567508232,
            "logloss": 1.7809640431831655,
            "mae": 0.3095174197363604,
            "precision": 0.8085106382978723,
            "recall": 0.7480314960629921
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8102320327721431,
            "auditor_fn_violation": 0.014411729997639842,
            "auditor_fp_violation": 0.01707326255553046,
            "ave_precision_score": 0.8106506929309598,
            "fpr": 0.19517543859649122,
            "logloss": 1.1844084844123197,
            "mae": 0.2858062642719542,
            "precision": 0.6810035842293907,
            "recall": 0.852017937219731
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8568626209645173,
            "auditor_fn_violation": 0.019389007493712026,
            "auditor_fp_violation": 0.025236086105035513,
            "ave_precision_score": 0.8570527477233556,
            "fpr": 0.1350164654226125,
            "logloss": 1.8625797546526481,
            "mae": 0.2665589273571757,
            "precision": 0.7705223880597015,
            "recall": 0.812992125984252
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8259713286384234,
            "auditor_fn_violation": 0.01445106600582173,
            "auditor_fp_violation": 0.015021459227467815,
            "ave_precision_score": 0.826314296566689,
            "fpr": 0.17653508771929824,
            "logloss": 0.9458749803632863,
            "mae": 0.291966487162793,
            "precision": 0.7024029574861368,
            "recall": 0.852017937219731
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8924634834922744,
            "auditor_fn_violation": 0.01981036673379604,
            "auditor_fp_violation": 0.01929818349208598,
            "ave_precision_score": 0.8925708719143098,
            "fpr": 0.1141602634467618,
            "logloss": 1.6670995271564992,
            "mae": 0.27052623327131703,
            "precision": 0.7976653696498055,
            "recall": 0.8070866141732284
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7741228070175439,
            "auc_prc": 0.8023374131147286,
            "auditor_fn_violation": 0.012469514593659038,
            "auditor_fp_violation": 0.02018861531511182,
            "ave_precision_score": 0.8028439828718437,
            "fpr": 0.11951754385964912,
            "logloss": 0.7812386407892054,
            "mae": 0.302464621765654,
            "precision": 0.7620087336244541,
            "recall": 0.7825112107623319
        },
        "train": {
            "accuracy": 0.7947310647639956,
            "auc_prc": 0.8869784095385603,
            "auditor_fn_violation": 0.01041729690484628,
            "auditor_fp_violation": 0.014621404232253707,
            "ave_precision_score": 0.8871136846856155,
            "fpr": 0.07025246981339188,
            "logloss": 1.456721379897317,
            "mae": 0.2886688026944526,
            "precision": 0.8574610244988864,
            "recall": 0.7578740157480315
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8185956261082711,
            "auditor_fn_violation": 0.014278970970025963,
            "auditor_fp_violation": 0.020146261576688508,
            "ave_precision_score": 0.8189787316820356,
            "fpr": 0.19298245614035087,
            "logloss": 1.0580330536222473,
            "mae": 0.2968194607115267,
            "precision": 0.6845878136200717,
            "recall": 0.8565022421524664
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8746564631113521,
            "auditor_fn_violation": 0.013323595253118057,
            "auditor_fp_violation": 0.022193591968033385,
            "ave_precision_score": 0.8748048730416657,
            "fpr": 0.13062568605927552,
            "logloss": 1.7404008244997615,
            "mae": 0.27309386523162504,
            "precision": 0.776735459662289,
            "recall": 0.8149606299212598
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8185953671820418,
            "auditor_fn_violation": 0.012892376681614352,
            "auditor_fp_violation": 0.0203250884722536,
            "ave_precision_score": 0.8189901172936012,
            "fpr": 0.18859649122807018,
            "logloss": 1.0663334797901436,
            "mae": 0.294875423442877,
            "precision": 0.6878402903811253,
            "recall": 0.8497757847533632
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8753470612978685,
            "auditor_fn_violation": 0.01478862891864093,
            "auditor_fp_violation": 0.019292735875009875,
            "ave_precision_score": 0.8754919644928784,
            "fpr": 0.13172338090010977,
            "logloss": 1.763336459615959,
            "mae": 0.27421487333834604,
            "precision": 0.7752808988764045,
            "recall": 0.8149606299212598
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8081630624762873,
            "auditor_fn_violation": 0.012769451656045944,
            "auditor_fp_violation": 0.011524922822076653,
            "ave_precision_score": 0.8086637765076334,
            "fpr": 0.18530701754385964,
            "logloss": 1.0916538158257103,
            "mae": 0.29878381318716707,
            "precision": 0.6876155268022182,
            "recall": 0.8340807174887892
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.868013567088469,
            "auditor_fn_violation": 0.016731203056259025,
            "auditor_fp_violation": 0.020063573691278095,
            "ave_precision_score": 0.8681987011634571,
            "fpr": 0.13172338090010977,
            "logloss": 1.8150677886547177,
            "mae": 0.2782601047373527,
            "precision": 0.7722960151802657,
            "recall": 0.8011811023622047
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8170795466376022,
            "auditor_fn_violation": 0.015719652269687672,
            "auditor_fp_violation": 0.01523322791958437,
            "ave_precision_score": 0.8174648894207166,
            "fpr": 0.19078947368421054,
            "logloss": 1.0755774369153868,
            "mae": 0.2965649981780338,
            "precision": 0.6847826086956522,
            "recall": 0.8475336322869955
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8746921995040925,
            "auditor_fn_violation": 0.0158603939600854,
            "auditor_fp_violation": 0.019418031067760192,
            "ave_precision_score": 0.8748378413296043,
            "fpr": 0.12733260153677278,
            "logloss": 1.770730507239135,
            "mae": 0.2729305933913849,
            "precision": 0.7803030303030303,
            "recall": 0.8110236220472441
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 20404,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.8102650038427983,
            "auditor_fn_violation": 0.013703681850365825,
            "auditor_fp_violation": 0.011524922822076655,
            "ave_precision_score": 0.8106926448567693,
            "fpr": 0.23684210526315788,
            "logloss": 1.2285273169938078,
            "mae": 0.3190230734932981,
            "precision": 0.6459016393442623,
            "recall": 0.8834080717488789
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8597120218539118,
            "auditor_fn_violation": 0.010155838094332615,
            "auditor_fp_violation": 0.01723353662024389,
            "ave_precision_score": 0.859893207022214,
            "fpr": 0.1778265642151482,
            "logloss": 1.9066715184862788,
            "mae": 0.2906176929546419,
            "precision": 0.7258883248730964,
            "recall": 0.844488188976378
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8191411763893615,
            "auditor_fn_violation": 0.014141294941389349,
            "auditor_fp_violation": 0.014988517430916342,
            "ave_precision_score": 0.8195637215709688,
            "fpr": 0.17982456140350878,
            "logloss": 1.0576932403797121,
            "mae": 0.2723727118365468,
            "precision": 0.6917293233082706,
            "recall": 0.8251121076233184
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8662032781661851,
            "auditor_fn_violation": 0.021089570170358788,
            "auditor_fp_violation": 0.023874181836010387,
            "ave_precision_score": 0.866366408190471,
            "fpr": 0.12294182217343579,
            "logloss": 1.703786527070111,
            "mae": 0.25976159773214114,
            "precision": 0.7795275590551181,
            "recall": 0.7795275590551181
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7722877988281781,
            "auditor_fn_violation": 0.011055876799622376,
            "auditor_fp_violation": 0.016870905805285746,
            "ave_precision_score": 0.7461764916315179,
            "fpr": 0.18201754385964913,
            "logloss": 2.381528292712433,
            "mae": 0.2810311331685915,
            "precision": 0.6920222634508348,
            "recall": 0.8363228699551569
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.809022467083146,
            "auditor_fn_violation": 0.014192243532675878,
            "auditor_fp_violation": 0.022517725184061366,
            "ave_precision_score": 0.7864909816902234,
            "fpr": 0.13062568605927552,
            "logloss": 2.9878230879919747,
            "mae": 0.26292013974049244,
            "precision": 0.7715930902111324,
            "recall": 0.7913385826771654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7843857265818799,
            "auditor_fn_violation": 0.016012213830540478,
            "auditor_fp_violation": 0.016169716135833147,
            "ave_precision_score": 0.779457182385687,
            "fpr": 0.20065789473684212,
            "logloss": 1.6282590409611255,
            "mae": 0.2991722735953723,
            "precision": 0.6778169014084507,
            "recall": 0.8632286995515696
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8179025190044191,
            "auditor_fn_violation": 0.015825820894232352,
            "auditor_fp_violation": 0.026145838156744292,
            "ave_precision_score": 0.8144922037960696,
            "fpr": 0.141602634467618,
            "logloss": 2.2702178540836373,
            "mae": 0.2758731144664198,
            "precision": 0.763302752293578,
            "recall": 0.8188976377952756
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8041995980694274,
            "auditor_fn_violation": 0.011407442372748017,
            "auditor_fp_violation": 0.017317973044198482,
            "ave_precision_score": 0.8038562565408179,
            "fpr": 0.17543859649122806,
            "logloss": 1.1881333551702036,
            "mae": 0.27338994146852047,
            "precision": 0.6952380952380952,
            "recall": 0.8183856502242153
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8247895887169514,
            "auditor_fn_violation": 0.01616506910291538,
            "auditor_fp_violation": 0.022430563310843758,
            "ave_precision_score": 0.8252220168072902,
            "fpr": 0.12403951701427003,
            "logloss": 1.991513824751631,
            "mae": 0.26036303490298474,
            "precision": 0.779296875,
            "recall": 0.7854330708661418
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8266433686606385,
            "auditor_fn_violation": 0.011053418299111007,
            "auditor_fp_violation": 0.017701509675476245,
            "ave_precision_score": 0.8269984960610035,
            "fpr": 0.2050438596491228,
            "logloss": 0.7360469863042767,
            "mae": 0.2942796690299075,
            "precision": 0.6781411359724613,
            "recall": 0.8834080717488789
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8909990142804357,
            "auditor_fn_violation": 0.011629515026318746,
            "auditor_fp_violation": 0.016814070105384157,
            "ave_precision_score": 0.8911395981800999,
            "fpr": 0.15806805708013172,
            "logloss": 0.5460089746717068,
            "mae": 0.25934613744345414,
            "precision": 0.7587939698492462,
            "recall": 0.8917322834645669
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7733027852020881,
            "auditor_fn_violation": 0.011193552828258992,
            "auditor_fp_violation": 0.017527388750847077,
            "ave_precision_score": 0.7470841106327271,
            "fpr": 0.18311403508771928,
            "logloss": 2.3909489176028105,
            "mae": 0.2801499878097331,
            "precision": 0.6895910780669146,
            "recall": 0.8318385650224215
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8086961705003028,
            "auditor_fn_violation": 0.015076017528544388,
            "auditor_fp_violation": 0.019622316708113954,
            "ave_precision_score": 0.7854669688087537,
            "fpr": 0.12952799121844127,
            "logloss": 3.0167822508225206,
            "mae": 0.2633537274844801,
            "precision": 0.7735124760076776,
            "recall": 0.7933070866141733
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 20404,
        "test": {
            "accuracy": 0.5098684210526315,
            "auc_prc": 0.7719613492947865,
            "auditor_fn_violation": 0.002846943592164267,
            "auditor_fp_violation": 0.005004800090354633,
            "ave_precision_score": 0.6596759389643808,
            "fpr": 0.4769736842105263,
            "logloss": 7.894577917086249,
            "mae": 0.46741008001730067,
            "precision": 0.4994246260069045,
            "recall": 0.9730941704035875
        },
        "train": {
            "accuracy": 0.5510428100987925,
            "auc_prc": 0.8209439251818177,
            "auditor_fn_violation": 0.005609479934656906,
            "auditor_fp_violation": 0.007662073417535332,
            "ave_precision_score": 0.7437266149548186,
            "fpr": 0.4061470911086718,
            "logloss": 6.8758289437594895,
            "mae": 0.4296495483425971,
            "precision": 0.5589988081048868,
            "recall": 0.9232283464566929
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8156083332000925,
            "auditor_fn_violation": 0.011943395484226262,
            "auditor_fp_violation": 0.011122562307055198,
            "ave_precision_score": 0.816048250521106,
            "fpr": 0.19407894736842105,
            "logloss": 0.7358717140498503,
            "mae": 0.30350593639535584,
            "precision": 0.6827956989247311,
            "recall": 0.8542600896860987
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8805922387524214,
            "auditor_fn_violation": 0.014648175838612928,
            "auditor_fp_violation": 0.01759307934726653,
            "ave_precision_score": 0.8807593938208661,
            "fpr": 0.14489571899012074,
            "logloss": 0.5571099977707364,
            "mae": 0.27373383020497255,
            "precision": 0.7663716814159292,
            "recall": 0.8523622047244095
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 20404,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.817159965919145,
            "auditor_fn_violation": 0.011309102352293291,
            "auditor_fp_violation": 0.018400346359460883,
            "ave_precision_score": 0.817673154736184,
            "fpr": 0.18640350877192982,
            "logloss": 1.0360999180048382,
            "mae": 0.29356994574092504,
            "precision": 0.6920289855072463,
            "recall": 0.8565022421524664
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8726744395674005,
            "auditor_fn_violation": 0.01072197204767626,
            "auditor_fp_violation": 0.01831216480131179,
            "ave_precision_score": 0.8728549663240815,
            "fpr": 0.13172338090010977,
            "logloss": 1.7344754618229243,
            "mae": 0.2711821387305817,
            "precision": 0.7752808988764045,
            "recall": 0.8149606299212598
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8206527350767214,
            "auditor_fn_violation": 0.012838289670364253,
            "auditor_fp_violation": 0.01640972065356525,
            "ave_precision_score": 0.8210189210261818,
            "fpr": 0.20394736842105263,
            "logloss": 1.0718218559955095,
            "mae": 0.3010773663601597,
            "precision": 0.6748251748251748,
            "recall": 0.8654708520179372
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8740138666731551,
            "auditor_fn_violation": 0.017012109216315033,
            "auditor_fp_violation": 0.02185039209223906,
            "ave_precision_score": 0.8741623081549006,
            "fpr": 0.1350164654226125,
            "logloss": 1.746539025089144,
            "mae": 0.2767345893917608,
            "precision": 0.7713754646840149,
            "recall": 0.8169291338582677
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.7690557697833802,
            "auditor_fn_violation": 0.007953249154275824,
            "auditor_fp_violation": 0.021951001430615166,
            "ave_precision_score": 0.7517303241947747,
            "fpr": 0.16557017543859648,
            "logloss": 2.233144379297476,
            "mae": 0.2818680715052523,
            "precision": 0.703921568627451,
            "recall": 0.804932735426009
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8117571881351975,
            "auditor_fn_violation": 0.007065870333716526,
            "auditor_fp_violation": 0.0217877444958639,
            "ave_precision_score": 0.7983726658934807,
            "fpr": 0.11306256860592755,
            "logloss": 3.011971883871295,
            "mae": 0.2684894240649179,
            "precision": 0.7889344262295082,
            "recall": 0.7578740157480315
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8099806022829449,
            "auditor_fn_violation": 0.012946463692864448,
            "auditor_fp_violation": 0.017967397033355925,
            "ave_precision_score": 0.8104094472476544,
            "fpr": 0.18201754385964913,
            "logloss": 1.159364433637464,
            "mae": 0.28115751769108605,
            "precision": 0.6937269372693727,
            "recall": 0.8430493273542601
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8583118971101171,
            "auditor_fn_violation": 0.02019931372464282,
            "auditor_fp_violation": 0.02176323021902145,
            "ave_precision_score": 0.8584814049957302,
            "fpr": 0.12733260153677278,
            "logloss": 1.8635235149837928,
            "mae": 0.2654205609230849,
            "precision": 0.7786259541984732,
            "recall": 0.8031496062992126
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7979003520025462,
            "auditor_fn_violation": 0.01678664149162143,
            "auditor_fp_violation": 0.01829681499887057,
            "ave_precision_score": 0.7976635273761359,
            "fpr": 0.21929824561403508,
            "logloss": 1.107856327792702,
            "mae": 0.3210204605419935,
            "precision": 0.6563573883161512,
            "recall": 0.8565022421524664
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8127038665379511,
            "auditor_fn_violation": 0.018280508569798702,
            "auditor_fp_violation": 0.02688671407909396,
            "ave_precision_score": 0.811667239627097,
            "fpr": 0.16245883644346873,
            "logloss": 2.0354978889708026,
            "mae": 0.30051263742229406,
            "precision": 0.7385159010600707,
            "recall": 0.8228346456692913
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 20404,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8048857572391149,
            "auditor_fn_violation": 0.014362559987412479,
            "auditor_fp_violation": 0.010595493562231764,
            "ave_precision_score": 0.805491131238049,
            "fpr": 0.19407894736842105,
            "logloss": 1.1246205470574364,
            "mae": 0.3032804195074889,
            "precision": 0.6810810810810811,
            "recall": 0.8475336322869955
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8637079616812814,
            "auditor_fn_violation": 0.021608166158154484,
            "auditor_fp_violation": 0.02104414476497619,
            "ave_precision_score": 0.8639026171086817,
            "fpr": 0.13172338090010977,
            "logloss": 1.8286814869186352,
            "mae": 0.2785315382232528,
            "precision": 0.7744360902255639,
            "recall": 0.8110236220472441
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 20404,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.6265935924093387,
            "auditor_fn_violation": 0.015021438124459134,
            "auditor_fp_violation": 0.013802612755063623,
            "ave_precision_score": 0.6284014487397126,
            "fpr": 0.10526315789473684,
            "logloss": 4.314095150612477,
            "mae": 0.4041360027757018,
            "precision": 0.6778523489932886,
            "recall": 0.452914798206278
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.7310679548222645,
            "auditor_fn_violation": 0.01574370986283137,
            "auditor_fp_violation": 0.011061386473022041,
            "ave_precision_score": 0.7314396776289764,
            "fpr": 0.06805708013172337,
            "logloss": 5.216565230289622,
            "mae": 0.40501037684343405,
            "precision": 0.7912457912457912,
            "recall": 0.4625984251968504
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8048265628635275,
            "auditor_fn_violation": 0.013819231374400127,
            "auditor_fp_violation": 0.017026202846171224,
            "ave_precision_score": 0.8052271217863591,
            "fpr": 0.17324561403508773,
            "logloss": 1.0632993017690338,
            "mae": 0.2900988234423059,
            "precision": 0.699047619047619,
            "recall": 0.8228699551569507
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8639364612833575,
            "auditor_fn_violation": 0.012143789380882827,
            "auditor_fp_violation": 0.016127670353795497,
            "ave_precision_score": 0.8641145316340416,
            "fpr": 0.12294182217343579,
            "logloss": 1.7689824170676198,
            "mae": 0.27518578622007517,
            "precision": 0.78125,
            "recall": 0.7874015748031497
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 20404,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.8004832895669403,
            "auditor_fn_violation": 0.009804500039336007,
            "auditor_fp_violation": 0.02184511708455689,
            "ave_precision_score": 0.8010513541702944,
            "fpr": 0.12280701754385964,
            "logloss": 0.8721274292212395,
            "mae": 0.29575310330035265,
            "precision": 0.7559912854030502,
            "recall": 0.7780269058295964
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.887514141836899,
            "auditor_fn_violation": 0.005566263602340605,
            "auditor_fp_violation": 0.012216281293155343,
            "ave_precision_score": 0.8876292438985172,
            "fpr": 0.07683863885839737,
            "logloss": 1.6669966306379078,
            "mae": 0.2857526430913779,
            "precision": 0.8416289592760181,
            "recall": 0.7322834645669292
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 20404,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.7438473355468516,
            "auditor_fn_violation": 0.015601644245142004,
            "auditor_fp_violation": 0.033939462389880284,
            "ave_precision_score": 0.7439642869673958,
            "fpr": 0.27521929824561403,
            "logloss": 1.2827468606284294,
            "mae": 0.3501764192654102,
            "precision": 0.6078125,
            "recall": 0.8721973094170403
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.8048794594513736,
            "auditor_fn_violation": 0.019581320172519602,
            "auditor_fp_violation": 0.026104981028673546,
            "ave_precision_score": 0.8051194219037976,
            "fpr": 0.2074643249176729,
            "logloss": 1.9922553824398221,
            "mae": 0.3195206848794125,
            "precision": 0.6951612903225807,
            "recall": 0.8484251968503937
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 20404,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8306457590372327,
            "auditor_fn_violation": 0.011739339941782709,
            "auditor_fp_violation": 0.01258376628265945,
            "ave_precision_score": 0.8309174798541176,
            "fpr": 0.18311403508771928,
            "logloss": 0.5705040372015965,
            "mae": 0.30426303866695154,
            "precision": 0.6941391941391941,
            "recall": 0.8497757847533632
        },
        "train": {
            "accuracy": 0.7958287596048299,
            "auc_prc": 0.9068116573231344,
            "auditor_fn_violation": 0.011754842390036048,
            "auditor_fp_violation": 0.010331405784824572,
            "ave_precision_score": 0.9069166423668953,
            "fpr": 0.12952799121844127,
            "logloss": 0.4478602798032235,
            "mae": 0.2709801991182762,
            "precision": 0.7885304659498208,
            "recall": 0.8661417322834646
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 20404,
        "test": {
            "accuracy": 0.6107456140350878,
            "auc_prc": 0.4839936573750212,
            "auditor_fn_violation": 0.0047178624813154,
            "auditor_fp_violation": 0.013868496348166554,
            "ave_precision_score": 0.4853761097512991,
            "fpr": 0.33114035087719296,
            "logloss": 2.3536821186966987,
            "mae": 0.4142937147105643,
            "precision": 0.5654676258992806,
            "recall": 0.8811659192825112
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.5619470113682241,
            "auditor_fn_violation": 0.005272392542589697,
            "auditor_fp_violation": 0.004306341298657442,
            "ave_precision_score": 0.5632058250805547,
            "fpr": 0.2689352360043908,
            "logloss": 1.9982154135651973,
            "mae": 0.36997707357354576,
            "precision": 0.6464646464646465,
            "recall": 0.8818897637795275
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8229563544393701,
            "auditor_fn_violation": 0.012031901502635514,
            "auditor_fp_violation": 0.012630825992018674,
            "ave_precision_score": 0.8233560496162222,
            "fpr": 0.19736842105263158,
            "logloss": 0.9930891265381546,
            "mae": 0.29654533010473516,
            "precision": 0.6836555360281195,
            "recall": 0.8721973094170403
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.875471442249421,
            "auditor_fn_violation": 0.012610525769898968,
            "auditor_fp_violation": 0.022122772946044082,
            "ave_precision_score": 0.8756154559710286,
            "fpr": 0.13611416026344675,
            "logloss": 1.554318464816698,
            "mae": 0.2748205117513929,
            "precision": 0.7703703703703704,
            "recall": 0.8188976377952756
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.8091356893318061,
            "auditor_fn_violation": 0.013359491778774287,
            "auditor_fp_violation": 0.01193904826443792,
            "ave_precision_score": 0.8095564173991754,
            "fpr": 0.23026315789473684,
            "logloss": 1.2086816268322913,
            "mae": 0.31525363927345657,
            "precision": 0.6517412935323383,
            "recall": 0.8811659192825112
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8609614929466252,
            "auditor_fn_violation": 0.012325297976611322,
            "auditor_fp_violation": 0.017140927129950183,
            "ave_precision_score": 0.8611384247482653,
            "fpr": 0.17014270032930845,
            "logloss": 1.8834786565823003,
            "mae": 0.2877746417588292,
            "precision": 0.7341337907375644,
            "recall": 0.84251968503937
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8176344074386703,
            "auditor_fn_violation": 0.014549406026276454,
            "auditor_fp_violation": 0.018136811987049178,
            "ave_precision_score": 0.8180428820673967,
            "fpr": 0.18640350877192982,
            "logloss": 1.064634077477846,
            "mae": 0.29478752911148726,
            "precision": 0.6897810218978102,
            "recall": 0.8475336322869955
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8755494630820811,
            "auditor_fn_violation": 0.01719361781204353,
            "auditor_fp_violation": 0.01832033622692594,
            "ave_precision_score": 0.8756973764215963,
            "fpr": 0.12733260153677278,
            "logloss": 1.758647795740643,
            "mae": 0.27133089630789137,
            "precision": 0.780718336483932,
            "recall": 0.812992125984252
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8179729076272861,
            "auditor_fn_violation": 0.011884391471953427,
            "auditor_fp_violation": 0.019936845870039913,
            "ave_precision_score": 0.8184750572612677,
            "fpr": 0.18530701754385964,
            "logloss": 1.0374213808137718,
            "mae": 0.2937128312903172,
            "precision": 0.6943942133815552,
            "recall": 0.8609865470852018
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8724673813141064,
            "auditor_fn_violation": 0.01072197204767626,
            "auditor_fp_violation": 0.01877521225278033,
            "ave_precision_score": 0.8726486603432506,
            "fpr": 0.13062568605927552,
            "logloss": 1.7349859078057075,
            "mae": 0.27156845297619575,
            "precision": 0.776735459662289,
            "recall": 0.8149606299212598
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8010185962773796,
            "auditor_fn_violation": 0.005851231217056093,
            "auditor_fp_violation": 0.01631795422031474,
            "ave_precision_score": 0.80167459343975,
            "fpr": 0.12171052631578948,
            "logloss": 0.9089171581743559,
            "mae": 0.3152427354111842,
            "precision": 0.7527839643652561,
            "recall": 0.757847533632287
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8846964217322713,
            "auditor_fn_violation": 0.009084073052888152,
            "auditor_fp_violation": 0.00991466307850289,
            "ave_precision_score": 0.8848143382784359,
            "fpr": 0.06586169045005488,
            "logloss": 1.7665989960127246,
            "mae": 0.3021196361032446,
            "precision": 0.8584905660377359,
            "recall": 0.7165354330708661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8048499156425113,
            "auditor_fn_violation": 0.009039906380300531,
            "auditor_fp_violation": 0.017590919358482043,
            "ave_precision_score": 0.8053931586393432,
            "fpr": 0.17105263157894737,
            "logloss": 1.0053209161611498,
            "mae": 0.29142295136285873,
            "precision": 0.7039848197343453,
            "recall": 0.8318385650224215
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8638892478780624,
            "auditor_fn_violation": 0.006214508587085233,
            "auditor_fp_violation": 0.022288925266865146,
            "ave_precision_score": 0.8641034220073512,
            "fpr": 0.10867178924259056,
            "logloss": 1.7381869699028647,
            "mae": 0.2714874752846896,
            "precision": 0.8,
            "recall": 0.7795275590551181
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8208696761902461,
            "auditor_fn_violation": 0.01208598851388561,
            "auditor_fp_violation": 0.013656727656050008,
            "ave_precision_score": 0.8212500339716017,
            "fpr": 0.19736842105263158,
            "logloss": 1.0703417352914901,
            "mae": 0.29797836151226437,
            "precision": 0.6797153024911032,
            "recall": 0.8565022421524664
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8753015585127251,
            "auditor_fn_violation": 0.01067443408212832,
            "auditor_fp_violation": 0.021237535171177754,
            "ave_precision_score": 0.8754509455365377,
            "fpr": 0.132821075740944,
            "logloss": 1.7628212020164802,
            "mae": 0.2742850721215424,
            "precision": 0.7763401109057301,
            "recall": 0.8267716535433071
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.813713647609767,
            "auditor_fn_violation": 0.00937672095035796,
            "auditor_fp_violation": 0.01486616218658234,
            "ave_precision_score": 0.8141961349352504,
            "fpr": 0.20614035087719298,
            "logloss": 0.7578338017050772,
            "mae": 0.3100165026894262,
            "precision": 0.6736111111111112,
            "recall": 0.8699551569506726
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8768198220817651,
            "auditor_fn_violation": 0.004308668331936005,
            "auditor_fp_violation": 0.021735992133640945,
            "ave_precision_score": 0.8770197088340753,
            "fpr": 0.1690450054884742,
            "logloss": 0.5754769091237374,
            "mae": 0.2730621662908786,
            "precision": 0.7454545454545455,
            "recall": 0.8877952755905512
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 20404,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.8202830435588865,
            "auditor_fn_violation": 0.01556968373849422,
            "auditor_fp_violation": 0.016875611776221665,
            "ave_precision_score": 0.8206324923239592,
            "fpr": 0.24013157894736842,
            "logloss": 1.0951257838414123,
            "mae": 0.3158561508377854,
            "precision": 0.6409836065573771,
            "recall": 0.8766816143497758
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8768323540778418,
            "auditor_fn_violation": 0.014520687658279819,
            "auditor_fp_violation": 0.020175249841338155,
            "ave_precision_score": 0.8769657146882628,
            "fpr": 0.1800219538968167,
            "logloss": 1.8040603081193696,
            "mae": 0.29085737998926897,
            "precision": 0.722972972972973,
            "recall": 0.84251968503937
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8272617610390192,
            "auditor_fn_violation": 0.01066989221933758,
            "auditor_fp_violation": 0.019035652435810558,
            "ave_precision_score": 0.827622744489758,
            "fpr": 0.20394736842105263,
            "logloss": 0.7365472860811219,
            "mae": 0.292484542231653,
            "precision": 0.6798623063683304,
            "recall": 0.8856502242152466
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8897562218095603,
            "auditor_fn_violation": 0.009552970258520102,
            "auditor_fp_violation": 0.01909934546880831,
            "ave_precision_score": 0.8898987376232776,
            "fpr": 0.1602634467618002,
            "logloss": 0.5519568571037917,
            "mae": 0.26047356964953416,
            "precision": 0.7562604340567612,
            "recall": 0.8917322834645669
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8190375347887202,
            "auditor_fn_violation": 0.013816772873888758,
            "auditor_fp_violation": 0.017296796174986827,
            "ave_precision_score": 0.8194149380843285,
            "fpr": 0.19188596491228072,
            "logloss": 1.0634787310783178,
            "mae": 0.29674006637962713,
            "precision": 0.685251798561151,
            "recall": 0.8542600896860987
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8763339936912986,
            "auditor_fn_violation": 0.016718238156564133,
            "auditor_fp_violation": 0.01935538347138503,
            "ave_precision_score": 0.8764794826248075,
            "fpr": 0.12952799121844127,
            "logloss": 1.748659758768846,
            "mae": 0.272726397715462,
            "precision": 0.7786116322701688,
            "recall": 0.8169291338582677
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8122209326072088,
            "auditor_fn_violation": 0.01260719062229565,
            "auditor_fp_violation": 0.012094345305323401,
            "ave_precision_score": 0.8126539339918517,
            "fpr": 0.22039473684210525,
            "logloss": 0.8651187398415199,
            "mae": 0.3023642838212082,
            "precision": 0.6621848739495798,
            "recall": 0.8834080717488789
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8658194703922335,
            "auditor_fn_violation": 0.014386717028099263,
            "auditor_fp_violation": 0.017470507963054265,
            "ave_precision_score": 0.8660258809588717,
            "fpr": 0.17453347969264543,
            "logloss": 0.6651602750865302,
            "mae": 0.2665052545882324,
            "precision": 0.7376237623762376,
            "recall": 0.8799212598425197
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.7719829666701133,
            "auditor_fn_violation": 0.010232279128314065,
            "auditor_fp_violation": 0.016419132595437095,
            "ave_precision_score": 0.7458874108401785,
            "fpr": 0.18201754385964913,
            "logloss": 2.396981254841755,
            "mae": 0.27997649038723993,
            "precision": 0.6908752327746741,
            "recall": 0.8318385650224215
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8079726745804193,
            "auditor_fn_violation": 0.015222953058419844,
            "auditor_fp_violation": 0.023212296361264175,
            "ave_precision_score": 0.7854467245168923,
            "fpr": 0.132821075740944,
            "logloss": 2.993449798065531,
            "mae": 0.26458825470510805,
            "precision": 0.7673076923076924,
            "recall": 0.7854330708661418
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.7800497785397941,
            "auditor_fn_violation": 0.01444123200377626,
            "auditor_fp_violation": 0.0065648294556132905,
            "ave_precision_score": 0.7663259534303805,
            "fpr": 0.17105263157894737,
            "logloss": 1.4890449636471326,
            "mae": 0.27859934705117395,
            "precision": 0.6982591876208898,
            "recall": 0.8094170403587444
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.824759004984682,
            "auditor_fn_violation": 0.014291641097003382,
            "auditor_fp_violation": 0.02167879215434189,
            "ave_precision_score": 0.8140676056764151,
            "fpr": 0.12294182217343579,
            "logloss": 1.2985100412469293,
            "mae": 0.2607845958140557,
            "precision": 0.7786561264822134,
            "recall": 0.7755905511811023
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8190334803585544,
            "auditor_fn_violation": 0.013816772873888758,
            "auditor_fp_violation": 0.017296796174986827,
            "ave_precision_score": 0.8194108869594863,
            "fpr": 0.19188596491228072,
            "logloss": 1.0635147635992859,
            "mae": 0.29673957654949734,
            "precision": 0.685251798561151,
            "recall": 0.8542600896860987
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8763412013604552,
            "auditor_fn_violation": 0.016718238156564133,
            "auditor_fp_violation": 0.01935538347138503,
            "ave_precision_score": 0.8764866836496817,
            "fpr": 0.12952799121844127,
            "logloss": 1.748739537020506,
            "mae": 0.272726940812993,
            "precision": 0.7786116322701688,
            "recall": 0.8169291338582677
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8155873847692005,
            "auditor_fn_violation": 0.009470143969789952,
            "auditor_fp_violation": 0.0186027031097056,
            "ave_precision_score": 0.8160686270510802,
            "fpr": 0.16447368421052633,
            "logloss": 0.8468828259355192,
            "mae": 0.2820586446123681,
            "precision": 0.7098646034816247,
            "recall": 0.8228699551569507
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8642276812895666,
            "auditor_fn_violation": 0.004341080581173236,
            "auditor_fp_violation": 0.018826964615003286,
            "ave_precision_score": 0.864492725335372,
            "fpr": 0.10757409440175632,
            "logloss": 1.263483353985826,
            "mae": 0.2699986595283523,
            "precision": 0.8012170385395537,
            "recall": 0.7775590551181102
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7866124310085851,
            "auditor_fn_violation": 0.010365038155927937,
            "auditor_fp_violation": 0.01800033882990739,
            "ave_precision_score": 0.7867260425904634,
            "fpr": 0.18640350877192982,
            "logloss": 1.3000546082454845,
            "mae": 0.29989581777163665,
            "precision": 0.6875,
            "recall": 0.8385650224215246
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8332853704147185,
            "auditor_fn_violation": 0.0080425594440651,
            "auditor_fp_violation": 0.01805885060727312,
            "ave_precision_score": 0.833521095791462,
            "fpr": 0.12843029637760703,
            "logloss": 1.995942693152652,
            "mae": 0.28293968012555754,
            "precision": 0.775,
            "recall": 0.7933070866141733
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8048593313865721,
            "auditor_fn_violation": 0.013819231374400127,
            "auditor_fp_violation": 0.01762856712596944,
            "ave_precision_score": 0.8052628360597034,
            "fpr": 0.17434210526315788,
            "logloss": 1.062679744771432,
            "mae": 0.2898901906079035,
            "precision": 0.6977186311787072,
            "recall": 0.8228699551569507
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8640809372463129,
            "auditor_fn_violation": 0.012143789380882827,
            "auditor_fp_violation": 0.016127670353795497,
            "ave_precision_score": 0.8642581987247386,
            "fpr": 0.12294182217343579,
            "logloss": 1.7676530579503356,
            "mae": 0.2750197658272689,
            "precision": 0.78125,
            "recall": 0.7874015748031497
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.8083760369401813,
            "auditor_fn_violation": 0.013590590826842892,
            "auditor_fp_violation": 0.014357917325502602,
            "ave_precision_score": 0.8088150163463281,
            "fpr": 0.2225877192982456,
            "logloss": 1.2443253790758977,
            "mae": 0.3141308402394572,
            "precision": 0.657672849915683,
            "recall": 0.874439461883408
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8573392481309614,
            "auditor_fn_violation": 0.011210316602850549,
            "auditor_fp_violation": 0.01990014517899508,
            "ave_precision_score": 0.8575393910713631,
            "fpr": 0.15697036223929747,
            "logloss": 1.9124255444176228,
            "mae": 0.2842755819602855,
            "precision": 0.7491228070175439,
            "recall": 0.8405511811023622
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7997831038950273,
            "auditor_fn_violation": 0.00955619148768783,
            "auditor_fp_violation": 0.01067784805361043,
            "ave_precision_score": 0.8001838769754199,
            "fpr": 0.21271929824561403,
            "logloss": 1.190384848520544,
            "mae": 0.3118568798973348,
            "precision": 0.6637781629116117,
            "recall": 0.8587443946188341
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8539136010726044,
            "auditor_fn_violation": 0.007243057296213389,
            "auditor_fp_violation": 0.015661899093788907,
            "ave_precision_score": 0.8541196689894905,
            "fpr": 0.14928649835345773,
            "logloss": 1.8921618922782133,
            "mae": 0.28938114781752816,
            "precision": 0.7531760435571688,
            "recall": 0.8169291338582677
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8163152686649219,
            "auditor_fn_violation": 0.014819841082526946,
            "auditor_fp_violation": 0.01581206234470296,
            "ave_precision_score": 0.816466530714612,
            "fpr": 0.21929824561403508,
            "logloss": 0.9217337073054853,
            "mae": 0.3014087305476479,
            "precision": 0.6610169491525424,
            "recall": 0.874439461883408
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8538024520756688,
            "auditor_fn_violation": 0.016953767167688014,
            "auditor_fp_violation": 0.025647381194281095,
            "ave_precision_score": 0.853893759157819,
            "fpr": 0.1668496158068057,
            "logloss": 0.7312326165145078,
            "mae": 0.26172155433258903,
            "precision": 0.745819397993311,
            "recall": 0.8779527559055118
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.810465055377004,
            "auditor_fn_violation": 0.009902840059790736,
            "auditor_fp_violation": 0.01936977637226113,
            "ave_precision_score": 0.8109547357491003,
            "fpr": 0.18530701754385964,
            "logloss": 0.7055599995367932,
            "mae": 0.2802866912854306,
            "precision": 0.692167577413479,
            "recall": 0.852017937219731
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8766610905572754,
            "auditor_fn_violation": 0.007951805146200856,
            "auditor_fp_violation": 0.01777285071077784,
            "ave_precision_score": 0.8768575844310523,
            "fpr": 0.1350164654226125,
            "logloss": 0.5487574700442187,
            "mae": 0.25797800850071095,
            "precision": 0.7767695099818511,
            "recall": 0.84251968503937
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8204906741731385,
            "auditor_fn_violation": 0.01208598851388561,
            "auditor_fp_violation": 0.013656727656050008,
            "ave_precision_score": 0.820886199211242,
            "fpr": 0.19736842105263158,
            "logloss": 1.0696451735586083,
            "mae": 0.2979322880835888,
            "precision": 0.6797153024911032,
            "recall": 0.8565022421524664
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.875206414751814,
            "auditor_fn_violation": 0.01067443408212832,
            "auditor_fp_violation": 0.021237535171177754,
            "ave_precision_score": 0.8753556003616394,
            "fpr": 0.132821075740944,
            "logloss": 1.762207483661891,
            "mae": 0.27427503234622574,
            "precision": 0.7763401109057301,
            "recall": 0.8267716535433071
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8098588562418965,
            "auditor_fn_violation": 0.013447997797183545,
            "auditor_fp_violation": 0.016666196069573074,
            "ave_precision_score": 0.8101790361560689,
            "fpr": 0.20723684210526316,
            "logloss": 0.8962880789439376,
            "mae": 0.3022496987104157,
            "precision": 0.6758147512864494,
            "recall": 0.8834080717488789
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8472054161166351,
            "auditor_fn_violation": 0.009516236376051238,
            "auditor_fp_violation": 0.02138462083223246,
            "ave_precision_score": 0.8474527250046932,
            "fpr": 0.16136114160263446,
            "logloss": 0.7246445768501222,
            "mae": 0.2681315962586952,
            "precision": 0.7516891891891891,
            "recall": 0.8759842519685039
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 20404,
        "test": {
            "accuracy": 0.5482456140350878,
            "auc_prc": 0.824991278935906,
            "auditor_fn_violation": 0.002315907481708756,
            "auditor_fp_violation": 0.009280174685641148,
            "ave_precision_score": 0.8251776972932121,
            "fpr": 0.44298245614035087,
            "logloss": 1.6266306939509652,
            "mae": 0.4259296205325507,
            "precision": 0.5201900237529691,
            "recall": 0.9820627802690582
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.8737468657835636,
            "auditor_fn_violation": 0.002800418334096822,
            "auditor_fp_violation": 0.010157082038389375,
            "ave_precision_score": 0.8739235071047464,
            "fpr": 0.3787047200878156,
            "logloss": 1.302069174112335,
            "mae": 0.3611756909763748,
            "precision": 0.5936395759717314,
            "recall": 0.9921259842519685
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.8188415543257064,
            "auditor_fn_violation": 0.014293721973094168,
            "auditor_fp_violation": 0.015040283111211503,
            "ave_precision_score": 0.8192053441879453,
            "fpr": 0.2324561403508772,
            "logloss": 1.0942788123526583,
            "mae": 0.31389055682962413,
            "precision": 0.6454849498327759,
            "recall": 0.8654708520179372
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8787556547767794,
            "auditor_fn_violation": 0.015246722041193813,
            "auditor_fp_violation": 0.015452165836359038,
            "ave_precision_score": 0.8788861450783365,
            "fpr": 0.17014270032930845,
            "logloss": 1.8255448396436522,
            "mae": 0.28940776501900867,
            "precision": 0.7322970639032815,
            "recall": 0.8346456692913385
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.807580303595567,
            "auditor_fn_violation": 0.014441232003776255,
            "auditor_fp_violation": 0.01003548302085687,
            "ave_precision_score": 0.8080095671070268,
            "fpr": 0.2138157894736842,
            "logloss": 1.224952053960862,
            "mae": 0.30865076706671546,
            "precision": 0.6643717728055077,
            "recall": 0.8654708520179372
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8579558737369741,
            "auditor_fn_violation": 0.012867662947180997,
            "auditor_fp_violation": 0.023057039274595313,
            "ave_precision_score": 0.8581471616714574,
            "fpr": 0.15587266739846323,
            "logloss": 1.8959875408813658,
            "mae": 0.2801113028710273,
            "precision": 0.7486725663716814,
            "recall": 0.8326771653543307
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7813360635932116,
            "auditor_fn_violation": 0.01382660687593423,
            "auditor_fp_violation": 0.013167306678713957,
            "ave_precision_score": 0.7816848196758915,
            "fpr": 0.16557017543859648,
            "logloss": 1.6017139872634003,
            "mae": 0.3170551369649988,
            "precision": 0.6912065439672802,
            "recall": 0.757847533632287
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8464241012726927,
            "auditor_fn_violation": 0.02178751393726718,
            "auditor_fp_violation": 0.00956056796855636,
            "ave_precision_score": 0.846544561519203,
            "fpr": 0.1163556531284303,
            "logloss": 1.7733573969816836,
            "mae": 0.30130750286163666,
            "precision": 0.778705636743215,
            "recall": 0.734251968503937
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8075378359869695,
            "auditor_fn_violation": 0.012666194634568487,
            "auditor_fp_violation": 0.01357202017920338,
            "ave_precision_score": 0.807983667341095,
            "fpr": 0.21710526315789475,
            "logloss": 1.2483083333515286,
            "mae": 0.3086874271457597,
            "precision": 0.6621160409556314,
            "recall": 0.8699551569506726
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8546572051772741,
            "auditor_fn_violation": 0.015413104920611601,
            "auditor_fp_violation": 0.01673235584924265,
            "ave_precision_score": 0.854855894901621,
            "fpr": 0.15806805708013172,
            "logloss": 1.930322769913207,
            "mae": 0.2842785181979656,
            "precision": 0.746031746031746,
            "recall": 0.8326771653543307
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.7905653533398054,
            "auditor_fn_violation": 0.011382857367634332,
            "auditor_fp_violation": 0.01902859347940667,
            "ave_precision_score": 0.7916881771646487,
            "fpr": 0.14583333333333334,
            "logloss": 0.8537105562674449,
            "mae": 0.304772722368074,
            "precision": 0.7263374485596708,
            "recall": 0.7914798206278026
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8754082025968254,
            "auditor_fn_violation": 0.0048661590188163965,
            "auditor_fp_violation": 0.00638460721318977,
            "ave_precision_score": 0.8755451794229454,
            "fpr": 0.09330406147091108,
            "logloss": 1.4954292188634324,
            "mae": 0.29215524703521945,
            "precision": 0.8210526315789474,
            "recall": 0.7677165354330708
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8187310285962677,
            "auditor_fn_violation": 0.015203367162300373,
            "auditor_fp_violation": 0.020146261576688508,
            "ave_precision_score": 0.8191139989883545,
            "fpr": 0.19298245614035087,
            "logloss": 1.0633508652889911,
            "mae": 0.2979516804103882,
            "precision": 0.6857142857142857,
            "recall": 0.8609865470852018
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8747088699897816,
            "auditor_fn_violation": 0.014120936584353965,
            "auditor_fp_violation": 0.02267842988780633,
            "ave_precision_score": 0.8748572412711355,
            "fpr": 0.13172338090010977,
            "logloss": 1.7427266309578793,
            "mae": 0.2736599384722364,
            "precision": 0.7757009345794392,
            "recall": 0.8169291338582677
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8173284577812391,
            "auditor_fn_violation": 0.017403725119974824,
            "auditor_fp_violation": 0.016407367668097278,
            "ave_precision_score": 0.8178864295902485,
            "fpr": 0.1962719298245614,
            "logloss": 1.0741588152568942,
            "mae": 0.293628034464474,
            "precision": 0.6803571428571429,
            "recall": 0.8542600896860987
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.86497808144896,
            "auditor_fn_violation": 0.021806961286809513,
            "auditor_fp_violation": 0.023724372366417626,
            "ave_precision_score": 0.8651524333149099,
            "fpr": 0.14270032930845225,
            "logloss": 1.7849257643169256,
            "mae": 0.276585567794272,
            "precision": 0.7597042513863216,
            "recall": 0.8090551181102362
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8273290101720863,
            "auditor_fn_violation": 0.01151561639524821,
            "auditor_fp_violation": 0.016513252014155565,
            "ave_precision_score": 0.827687280858971,
            "fpr": 0.20723684210526316,
            "logloss": 0.7394020851039688,
            "mae": 0.2947199808578389,
            "precision": 0.6763698630136986,
            "recall": 0.8856502242152466
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8908862043097292,
            "auditor_fn_violation": 0.010894837376941495,
            "auditor_fp_violation": 0.016748698700470945,
            "ave_precision_score": 0.8910284721690154,
            "fpr": 0.15697036223929747,
            "logloss": 0.5469707279840323,
            "mae": 0.25895908632698245,
            "precision": 0.7592592592592593,
            "recall": 0.8877952755905512
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.7767420579319597,
            "auditor_fn_violation": 0.011773758948941865,
            "auditor_fp_violation": 0.015117931631654244,
            "ave_precision_score": 0.756206602189515,
            "fpr": 0.18092105263157895,
            "logloss": 2.13136189092176,
            "mae": 0.27771526353359943,
            "precision": 0.6933085501858736,
            "recall": 0.8363228699551569
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8126210936886116,
            "auditor_fn_violation": 0.018500911864611877,
            "auditor_fp_violation": 0.02444345782046289,
            "ave_precision_score": 0.7952120114045373,
            "fpr": 0.12952799121844127,
            "logloss": 2.7263931217558772,
            "mae": 0.26134181595926015,
            "precision": 0.7748091603053435,
            "recall": 0.7992125984251969
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.7724318000233312,
            "auditor_fn_violation": 0.012658819133034384,
            "auditor_fp_violation": 0.015506174233867938,
            "ave_precision_score": 0.7471984449721476,
            "fpr": 0.18092105263157895,
            "logloss": 2.3189435339195685,
            "mae": 0.27927791254436185,
            "precision": 0.6921641791044776,
            "recall": 0.8318385650224215
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8100867466698487,
            "auditor_fn_violation": 0.018466338798758832,
            "auditor_fp_violation": 0.021253878022406057,
            "ave_precision_score": 0.7889939877702017,
            "fpr": 0.13172338090010977,
            "logloss": 2.905549912563293,
            "mae": 0.26277529415741163,
            "precision": 0.7696737044145874,
            "recall": 0.7893700787401575
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.819462684119543,
            "auditor_fn_violation": 0.01295629769490992,
            "auditor_fp_violation": 0.015390877945937806,
            "ave_precision_score": 0.8199376820170612,
            "fpr": 0.19846491228070176,
            "logloss": 1.0424631062880878,
            "mae": 0.29557925662036466,
            "precision": 0.6835664335664335,
            "recall": 0.8766816143497758
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8742985958539858,
            "auditor_fn_violation": 0.01181534525527888,
            "auditor_fp_violation": 0.022460525204762306,
            "ave_precision_score": 0.874446207765315,
            "fpr": 0.13611416026344675,
            "logloss": 1.7240537111622303,
            "mae": 0.27317487358945486,
            "precision": 0.7716390423572744,
            "recall": 0.8248031496062992
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.7764510463490917,
            "auditor_fn_violation": 0.013728266855479508,
            "auditor_fp_violation": 0.01599088924026805,
            "ave_precision_score": 0.7534972432630713,
            "fpr": 0.18421052631578946,
            "logloss": 2.195585319650304,
            "mae": 0.2793869919755757,
            "precision": 0.6900369003690037,
            "recall": 0.8385650224215246
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8122157455840938,
            "auditor_fn_violation": 0.015402300837532525,
            "auditor_fp_violation": 0.025554771703987386,
            "ave_precision_score": 0.7932523600167464,
            "fpr": 0.12733260153677278,
            "logloss": 2.719133439183624,
            "mae": 0.26276476125665915,
            "precision": 0.7756286266924565,
            "recall": 0.7893700787401575
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 20404,
        "test": {
            "accuracy": 0.6326754385964912,
            "auc_prc": 0.6359405249939586,
            "auditor_fn_violation": 0.009103827393596097,
            "auditor_fp_violation": 0.011623748211731056,
            "ave_precision_score": 0.5629793004231894,
            "fpr": 0.29385964912280704,
            "logloss": 5.847326232542536,
            "mae": 0.37724255295002534,
            "precision": 0.5857805255023184,
            "recall": 0.8497757847533632
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.6774549294331141,
            "auditor_fn_violation": 0.011849918321131922,
            "auditor_fp_violation": 0.0007027426028169662,
            "ave_precision_score": 0.6122455775612774,
            "fpr": 0.23600439077936333,
            "logloss": 6.065328995397129,
            "mae": 0.35321582571074805,
            "precision": 0.660347551342812,
            "recall": 0.8228346456692913
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8034523293896227,
            "auditor_fn_violation": 0.010512548186610026,
            "auditor_fp_violation": 0.014635569610722089,
            "ave_precision_score": 0.8038385197854514,
            "fpr": 0.2149122807017544,
            "logloss": 1.161556258838948,
            "mae": 0.3122251803630871,
            "precision": 0.6620689655172414,
            "recall": 0.8609865470852018
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8575999255673395,
            "auditor_fn_violation": 0.010432422621156992,
            "auditor_fp_violation": 0.012774662043455644,
            "ave_precision_score": 0.8577923022112381,
            "fpr": 0.15367727771679474,
            "logloss": 1.8527220056743972,
            "mae": 0.2893431771326824,
            "precision": 0.7491039426523297,
            "recall": 0.8228346456692913
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8273810031930326,
            "auditor_fn_violation": 0.01151561639524821,
            "auditor_fp_violation": 0.017289737218582946,
            "ave_precision_score": 0.8277390751328707,
            "fpr": 0.20833333333333334,
            "logloss": 0.740146109382546,
            "mae": 0.2950884141557349,
            "precision": 0.6752136752136753,
            "recall": 0.8856502242152466
        },
        "train": {
            "accuracy": 0.7837541163556532,
            "auc_prc": 0.8910787688883564,
            "auditor_fn_violation": 0.007804869616325403,
            "auditor_fp_violation": 0.01873980274178567,
            "ave_precision_score": 0.8912207051303922,
            "fpr": 0.15587266739846323,
            "logloss": 0.5468913101055637,
            "mae": 0.2588029175797758,
            "precision": 0.761344537815126,
            "recall": 0.8917322834645669
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8186352100206256,
            "auditor_fn_violation": 0.013816772873888758,
            "auditor_fp_violation": 0.017296796174986827,
            "ave_precision_score": 0.819030336227955,
            "fpr": 0.19188596491228072,
            "logloss": 1.0635392876850989,
            "mae": 0.2969228968679715,
            "precision": 0.685251798561151,
            "recall": 0.8542600896860987
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8765328884594834,
            "auditor_fn_violation": 0.016718238156564133,
            "auditor_fp_violation": 0.01935538347138503,
            "ave_precision_score": 0.8766765837415507,
            "fpr": 0.12952799121844127,
            "logloss": 1.7490037076800964,
            "mae": 0.27286984807796744,
            "precision": 0.7786116322701688,
            "recall": 0.8169291338582677
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8180053153501794,
            "auditor_fn_violation": 0.01225808354968138,
            "auditor_fp_violation": 0.017560330547398546,
            "ave_precision_score": 0.8183963937406379,
            "fpr": 0.18969298245614036,
            "logloss": 1.0745490934236066,
            "mae": 0.29514935348536836,
            "precision": 0.6871609403254972,
            "recall": 0.852017937219731
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8740055854808876,
            "auditor_fn_violation": 0.01478862891864093,
            "auditor_fp_violation": 0.019292735875009875,
            "ave_precision_score": 0.8741533437392399,
            "fpr": 0.13172338090010977,
            "logloss": 1.769199722183968,
            "mae": 0.2743200981705675,
            "precision": 0.7752808988764045,
            "recall": 0.8149606299212598
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8204887619667446,
            "auditor_fn_violation": 0.01208598851388561,
            "auditor_fp_violation": 0.012362585648671036,
            "ave_precision_score": 0.8208831852130064,
            "fpr": 0.1962719298245614,
            "logloss": 1.0709546020064469,
            "mae": 0.29798804411912994,
            "precision": 0.6809269162210339,
            "recall": 0.8565022421524664
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8752770503520525,
            "auditor_fn_violation": 0.01067443408212832,
            "auditor_fp_violation": 0.021237535171177754,
            "ave_precision_score": 0.875425781699178,
            "fpr": 0.132821075740944,
            "logloss": 1.7634538421577517,
            "mae": 0.274272017473885,
            "precision": 0.7763401109057301,
            "recall": 0.8267716535433071
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8170630686504564,
            "auditor_fn_violation": 0.015719652269687672,
            "auditor_fp_violation": 0.01523322791958437,
            "ave_precision_score": 0.817448403934453,
            "fpr": 0.19078947368421054,
            "logloss": 1.0756962242189194,
            "mae": 0.2965831944182943,
            "precision": 0.6847826086956522,
            "recall": 0.8475336322869955
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.874553857236315,
            "auditor_fn_violation": 0.0158603939600854,
            "auditor_fp_violation": 0.019418031067760192,
            "ave_precision_score": 0.8747009757594724,
            "fpr": 0.12733260153677278,
            "logloss": 1.7709703323983537,
            "mae": 0.2729419715983834,
            "precision": 0.7803030303030303,
            "recall": 0.8110236220472441
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7722798568313771,
            "auditor_fn_violation": 0.011055876799622376,
            "auditor_fp_violation": 0.016870905805285746,
            "ave_precision_score": 0.7462027270773386,
            "fpr": 0.18201754385964913,
            "logloss": 2.3746259440928603,
            "mae": 0.2804526040709983,
            "precision": 0.6920222634508348,
            "recall": 0.8363228699551569
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.809307967928856,
            "auditor_fn_violation": 0.015112751411013245,
            "auditor_fp_violation": 0.022517725184061366,
            "ave_precision_score": 0.7867732864101133,
            "fpr": 0.13062568605927552,
            "logloss": 2.9793493901446917,
            "mae": 0.2623963143327028,
            "precision": 0.7720306513409961,
            "recall": 0.7933070866141733
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 20404,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8220661178169473,
            "auditor_fn_violation": 0.012808787664227834,
            "auditor_fp_violation": 0.015948535501844743,
            "ave_precision_score": 0.8225449341807335,
            "fpr": 0.2050438596491228,
            "logloss": 1.0459957440542835,
            "mae": 0.29788838084351615,
            "precision": 0.6747826086956522,
            "recall": 0.8699551569506726
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.872588457710664,
            "auditor_fn_violation": 0.011962280785154324,
            "auditor_fp_violation": 0.02391503896408114,
            "ave_precision_score": 0.8727499945190491,
            "fpr": 0.1437980241492865,
            "logloss": 1.7068924992498984,
            "mae": 0.2750374845271933,
            "precision": 0.7622504537205081,
            "recall": 0.8267716535433071
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8250299137458205,
            "auditor_fn_violation": 0.014505153017071828,
            "auditor_fp_violation": 0.013884967246442285,
            "ave_precision_score": 0.8253379281713076,
            "fpr": 0.18092105263157895,
            "logloss": 0.8136206688167533,
            "mae": 0.27089478046146354,
            "precision": 0.6933085501858736,
            "recall": 0.8363228699551569
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8760885882230816,
            "auditor_fn_violation": 0.01576531802898952,
            "auditor_fp_violation": 0.024467972097305342,
            "ave_precision_score": 0.8762628841480253,
            "fpr": 0.12843029637760703,
            "logloss": 0.7013881718514392,
            "mae": 0.25302137192551555,
            "precision": 0.7745664739884393,
            "recall": 0.7913385826771654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.808153664590187,
            "auditor_fn_violation": 0.007990126661946347,
            "auditor_fp_violation": 0.018522701603794902,
            "ave_precision_score": 0.8086110554019897,
            "fpr": 0.20942982456140352,
            "logloss": 1.192168189496513,
            "mae": 0.3070617645525573,
            "precision": 0.6684027777777778,
            "recall": 0.8632286995515696
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8510651258168673,
            "auditor_fn_violation": 0.010227145042654522,
            "auditor_fp_violation": 0.02287999171962205,
            "ave_precision_score": 0.8513068603660641,
            "fpr": 0.14709110867178923,
            "logloss": 1.905512030131704,
            "mae": 0.28169103035884563,
            "precision": 0.7585585585585586,
            "recall": 0.8287401574803149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8074667181406938,
            "auditor_fn_violation": 0.014362559987412479,
            "auditor_fp_violation": 0.010174309163466615,
            "ave_precision_score": 0.8081137376459759,
            "fpr": 0.19078947368421054,
            "logloss": 1.1018114333069424,
            "mae": 0.3026013720412112,
            "precision": 0.6847826086956522,
            "recall": 0.8475336322869955
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8688165306178967,
            "auditor_fn_violation": 0.02098369015618383,
            "auditor_fp_violation": 0.02288543933669815,
            "ave_precision_score": 0.8690025475445138,
            "fpr": 0.132821075740944,
            "logloss": 1.8115133730350768,
            "mae": 0.2781806681000722,
            "precision": 0.7716981132075472,
            "recall": 0.8051181102362205
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 20404,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.6080140254021402,
            "auditor_fn_violation": 0.010780524742349149,
            "auditor_fp_violation": 0.009244879903621715,
            "ave_precision_score": 0.5923908695788167,
            "fpr": 0.2532894736842105,
            "logloss": 3.4802951508262336,
            "mae": 0.3391913418100133,
            "precision": 0.6200657894736842,
            "recall": 0.8452914798206278
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.6809690125630812,
            "auditor_fn_violation": 0.017614977052127542,
            "auditor_fp_violation": 0.017195403300711194,
            "ave_precision_score": 0.6639674069482103,
            "fpr": 0.1942919868276619,
            "logloss": 3.6579981813368008,
            "mae": 0.3054887254457049,
            "precision": 0.7025210084033613,
            "recall": 0.8228346456692913
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.781361485112104,
            "auditor_fn_violation": 0.012031901502635512,
            "auditor_fp_violation": 0.01840740531586477,
            "ave_precision_score": 0.7662601116113622,
            "fpr": 0.18092105263157895,
            "logloss": 1.8465906305337223,
            "mae": 0.27612159041180323,
            "precision": 0.6938775510204082,
            "recall": 0.8385650224215246
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8160680563710054,
            "auditor_fn_violation": 0.013881085939998448,
            "auditor_fp_violation": 0.020848030550236565,
            "ave_precision_score": 0.8049225663672838,
            "fpr": 0.12733260153677278,
            "logloss": 2.3160159367598125,
            "mae": 0.26150990807100155,
            "precision": 0.7769230769230769,
            "recall": 0.7952755905511811
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.817994142596637,
            "auditor_fn_violation": 0.01225808354968138,
            "auditor_fp_violation": 0.017560330547398546,
            "ave_precision_score": 0.8183967263091583,
            "fpr": 0.18969298245614036,
            "logloss": 1.0745238402641537,
            "mae": 0.295149435166948,
            "precision": 0.6871609403254972,
            "recall": 0.852017937219731
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8740088177703387,
            "auditor_fn_violation": 0.01478862891864093,
            "auditor_fp_violation": 0.019292735875009875,
            "ave_precision_score": 0.8741570660432338,
            "fpr": 0.13172338090010977,
            "logloss": 1.7691456592342925,
            "mae": 0.27431952217994404,
            "precision": 0.7752808988764045,
            "recall": 0.8149606299212598
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7732952082035449,
            "auditor_fn_violation": 0.010918200770985764,
            "auditor_fp_violation": 0.016214422859724423,
            "ave_precision_score": 0.7480721846321516,
            "fpr": 0.18092105263157895,
            "logloss": 2.3315420096584147,
            "mae": 0.27917516263073155,
            "precision": 0.6944444444444444,
            "recall": 0.8408071748878924
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.812609916912061,
            "auditor_fn_violation": 0.0195813201725196,
            "auditor_fp_violation": 0.025887076345629517,
            "ave_precision_score": 0.7928375024168448,
            "fpr": 0.12733260153677278,
            "logloss": 2.8878602150693156,
            "mae": 0.26044650087207694,
            "precision": 0.7777777777777778,
            "recall": 0.7992125984251969
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.77870648643347,
            "auditor_fn_violation": 0.013266068759342301,
            "auditor_fp_violation": 0.015550880957759203,
            "ave_precision_score": 0.7557460810331538,
            "fpr": 0.18530701754385964,
            "logloss": 1.9588594159205395,
            "mae": 0.2785065371763166,
            "precision": 0.6910420475319927,
            "recall": 0.8475336322869955
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8244698743150848,
            "auditor_fn_violation": 0.012783391099164198,
            "auditor_fp_violation": 0.02412204841297296,
            "ave_precision_score": 0.8055196520747538,
            "fpr": 0.12952799121844127,
            "logloss": 1.6743240365535423,
            "mae": 0.2526265422529937,
            "precision": 0.7765151515151515,
            "recall": 0.8070866141732284
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8186183258923636,
            "auditor_fn_violation": 0.013816772873888758,
            "auditor_fp_violation": 0.017296796174986827,
            "ave_precision_score": 0.819013452099693,
            "fpr": 0.19188596491228072,
            "logloss": 1.0635394685939028,
            "mae": 0.2969231113433094,
            "precision": 0.685251798561151,
            "recall": 0.8542600896860987
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8765328884594834,
            "auditor_fn_violation": 0.016718238156564133,
            "auditor_fp_violation": 0.01935538347138503,
            "ave_precision_score": 0.8766765837415507,
            "fpr": 0.12952799121844127,
            "logloss": 1.7490036826838735,
            "mae": 0.2728700060100191,
            "precision": 0.7786116322701688,
            "recall": 0.8169291338582677
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7783246574434517,
            "auditor_fn_violation": 0.009229210919675873,
            "auditor_fp_violation": 0.01361672690309465,
            "ave_precision_score": 0.7632493437257646,
            "fpr": 0.15460526315789475,
            "logloss": 1.8732505146138627,
            "mae": 0.27306838324907534,
            "precision": 0.7157258064516129,
            "recall": 0.7959641255605381
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8157989863241786,
            "auditor_fn_violation": 0.00755421488889081,
            "auditor_fp_violation": 0.016854927233454907,
            "ave_precision_score": 0.8046377797074367,
            "fpr": 0.1119648737650933,
            "logloss": 2.39402307950157,
            "mae": 0.2687547493188277,
            "precision": 0.7909836065573771,
            "recall": 0.7598425196850394
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.82048307232374,
            "auditor_fn_violation": 0.012125324522067502,
            "auditor_fp_violation": 0.01388261426097432,
            "ave_precision_score": 0.8208413326240287,
            "fpr": 0.25,
            "logloss": 0.833609980264363,
            "mae": 0.31921854135383376,
            "precision": 0.638095238095238,
            "recall": 0.9013452914798207
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8868468963071109,
            "auditor_fn_violation": 0.007281951995298064,
            "auditor_fp_violation": 0.014610508998101507,
            "ave_precision_score": 0.8869879046281812,
            "fpr": 0.20636663007683864,
            "logloss": 0.6051814733024585,
            "mae": 0.2749910026914073,
            "precision": 0.7129770992366412,
            "recall": 0.9192913385826772
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7480476476044736,
            "auditor_fn_violation": 0.012371174573204321,
            "auditor_fp_violation": 0.014106147880430693,
            "ave_precision_score": 0.7489297795479193,
            "fpr": 0.12609649122807018,
            "logloss": 1.0814902555678936,
            "mae": 0.3258270330167901,
            "precision": 0.7201946472019465,
            "recall": 0.6636771300448431
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.8517090286738707,
            "auditor_fn_violation": 0.026240956982462812,
            "auditor_fp_violation": 0.017219917577553638,
            "ave_precision_score": 0.851874722321665,
            "fpr": 0.08232711306256861,
            "logloss": 1.1308234025628805,
            "mae": 0.31239751531715443,
            "precision": 0.8081841432225064,
            "recall": 0.6220472440944882
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8224451415346354,
            "auditor_fn_violation": 0.008555581779561016,
            "auditor_fp_violation": 0.017308561102326637,
            "ave_precision_score": 0.8228224205398346,
            "fpr": 0.16666666666666666,
            "logloss": 0.831102835876375,
            "mae": 0.2823888928728435,
            "precision": 0.7099236641221374,
            "recall": 0.8340807174887892
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8768678064053406,
            "auditor_fn_violation": 0.012333941243074586,
            "auditor_fp_violation": 0.018290374333007387,
            "ave_precision_score": 0.8770289317874791,
            "fpr": 0.1163556531284303,
            "logloss": 0.9309471269436543,
            "mae": 0.2664132213862238,
            "precision": 0.7933723196881092,
            "recall": 0.8011811023622047
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8083607767306433,
            "auditor_fn_violation": 0.00854574777751554,
            "auditor_fp_violation": 0.016974437165876072,
            "ave_precision_score": 0.8087606731848718,
            "fpr": 0.1699561403508772,
            "logloss": 0.9978014070173227,
            "mae": 0.2877218932114841,
            "precision": 0.7058823529411765,
            "recall": 0.8340807174887892
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8637646029661237,
            "auditor_fn_violation": 0.012236704495362885,
            "auditor_fp_violation": 0.01875886940155203,
            "ave_precision_score": 0.8639466481688398,
            "fpr": 0.1251372118551043,
            "logloss": 1.6384863526581828,
            "mae": 0.2753449275263395,
            "precision": 0.776908023483366,
            "recall": 0.781496062992126
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.7865848995124007,
            "auditor_fn_violation": 0.00849166076626544,
            "auditor_fp_violation": 0.015214404035840678,
            "ave_precision_score": 0.7871461473939101,
            "fpr": 0.1875,
            "logloss": 1.239315514507723,
            "mae": 0.3056078115682361,
            "precision": 0.6862385321100918,
            "recall": 0.8385650224215246
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8368433580189389,
            "auditor_fn_violation": 0.0080425594440651,
            "auditor_fp_violation": 0.018080641075577525,
            "ave_precision_score": 0.837124891197571,
            "fpr": 0.12403951701427003,
            "logloss": 1.9549955017087783,
            "mae": 0.28394563164765485,
            "precision": 0.7810077519379846,
            "recall": 0.7933070866141733
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8108394526479049,
            "auditor_fn_violation": 0.011741798442294078,
            "auditor_fp_violation": 0.011477863112717425,
            "ave_precision_score": 0.8113417213661139,
            "fpr": 0.18201754385964913,
            "logloss": 1.034730015267615,
            "mae": 0.2972848575845757,
            "precision": 0.6937269372693727,
            "recall": 0.8430493273542601
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8619014537975329,
            "auditor_fn_violation": 0.008980353855329007,
            "auditor_fp_violation": 0.024920124314621685,
            "ave_precision_score": 0.8620920261783188,
            "fpr": 0.13391877058177826,
            "logloss": 1.7633723222520763,
            "mae": 0.2797936980646211,
            "precision": 0.7676190476190476,
            "recall": 0.7933070866141733
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8046955287081787,
            "auditor_fn_violation": 0.013433246794115335,
            "auditor_fp_violation": 0.019510955500338836,
            "ave_precision_score": 0.805107909718349,
            "fpr": 0.17543859649122806,
            "logloss": 1.0756655736628655,
            "mae": 0.2912395647893313,
            "precision": 0.6981132075471698,
            "recall": 0.8295964125560538
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8634953067322673,
            "auditor_fn_violation": 0.012143789380882827,
            "auditor_fp_violation": 0.016830412956612452,
            "ave_precision_score": 0.8636781385501356,
            "fpr": 0.1251372118551043,
            "logloss": 1.7833338400147896,
            "mae": 0.27582009499039234,
            "precision": 0.7782101167315175,
            "recall": 0.7874015748031497
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7726129680064058,
            "auditor_fn_violation": 0.007277161513649601,
            "auditor_fp_violation": 0.015997948196671948,
            "ave_precision_score": 0.7473462104796768,
            "fpr": 0.1787280701754386,
            "logloss": 2.3515308735548377,
            "mae": 0.278322924469419,
            "precision": 0.6924528301886792,
            "recall": 0.8228699551569507
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8092030491092987,
            "auditor_fn_violation": 0.01421385169883403,
            "auditor_fp_violation": 0.021932106348380564,
            "ave_precision_score": 0.7873491651768024,
            "fpr": 0.13172338090010977,
            "logloss": 2.9667971475704675,
            "mae": 0.2633986612495094,
            "precision": 0.7683397683397684,
            "recall": 0.7834645669291339
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 20404,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.8133944474777325,
            "auditor_fn_violation": 0.013381618283376605,
            "auditor_fp_violation": 0.0122543483171448,
            "ave_precision_score": 0.8137587307855189,
            "fpr": 0.2324561403508772,
            "logloss": 1.1274932420034045,
            "mae": 0.31252698974513704,
            "precision": 0.6460767946577629,
            "recall": 0.8677130044843049
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8630131605249618,
            "auditor_fn_violation": 0.010179607077106584,
            "auditor_fp_violation": 0.017524984133815272,
            "ave_precision_score": 0.8631809720844524,
            "fpr": 0.1756311745334797,
            "logloss": 1.7189996502181306,
            "mae": 0.2870156494246749,
            "precision": 0.7283531409168081,
            "recall": 0.844488188976378
        }
    }
]
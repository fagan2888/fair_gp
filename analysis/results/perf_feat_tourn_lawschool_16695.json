[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7842924360735625,
            "auditor_fn_violation": 0.0061403508771929885,
            "auditor_fp_violation": 0.026592573667177665,
            "ave_precision_score": 0.7376921627628907,
            "fpr": 0.19736842105263158,
            "logloss": 3.317162098941226,
            "mae": 0.3007487793527725,
            "precision": 0.6974789915966386,
            "recall": 0.83
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.782870290360545,
            "auditor_fn_violation": 0.009115219273006864,
            "auditor_fp_violation": 0.021055564496177293,
            "ave_precision_score": 0.7341840251227484,
            "fpr": 0.19978046103183314,
            "logloss": 3.139269285952421,
            "mae": 0.281916148186155,
            "precision": 0.6840277777777778,
            "recall": 0.8678414096916299
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.791145637330426,
            "auditor_fn_violation": 0.012684210526315794,
            "auditor_fp_violation": 0.025687702265372165,
            "ave_precision_score": 0.7514062597943978,
            "fpr": 0.17543859649122806,
            "logloss": 3.026268293560102,
            "mae": 0.29542981524467343,
            "precision": 0.714795008912656,
            "recall": 0.802
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.7900443635980308,
            "auditor_fn_violation": 0.011419411306740434,
            "auditor_fp_violation": 0.015946599668049396,
            "ave_precision_score": 0.7464369538601799,
            "fpr": 0.18221734357848518,
            "logloss": 2.9226036579197765,
            "mae": 0.27486463750484547,
            "precision": 0.6954128440366972,
            "recall": 0.8348017621145375
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7900094669274649,
            "auditor_fn_violation": 0.014635964912280709,
            "auditor_fp_violation": 0.018800034065746894,
            "ave_precision_score": 0.7400177213882922,
            "fpr": 0.15021929824561403,
            "logloss": 3.0843250504491877,
            "mae": 0.2907877844504822,
            "precision": 0.7380497131931166,
            "recall": 0.772
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8064943216747354,
            "auditor_fn_violation": 0.010106529591821931,
            "auditor_fp_violation": 0.014010621458613067,
            "ave_precision_score": 0.7558289908944338,
            "fpr": 0.12843029637760703,
            "logloss": 2.688287743267934,
            "mae": 0.25076201464610254,
            "precision": 0.7587628865979381,
            "recall": 0.8105726872246696
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7857354591139059,
            "auditor_fn_violation": 0.007701754385964913,
            "auditor_fp_violation": 0.023803440640436054,
            "ave_precision_score": 0.7398313221928554,
            "fpr": 0.19298245614035087,
            "logloss": 3.257542207479124,
            "mae": 0.2995305055150532,
            "precision": 0.7011884550084889,
            "recall": 0.826
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.7868204266998008,
            "auditor_fn_violation": 0.012463913886565089,
            "auditor_fp_violation": 0.021427867997991963,
            "ave_precision_score": 0.73708821099506,
            "fpr": 0.19209659714599342,
            "logloss": 3.1664165087696445,
            "mae": 0.28039356600549936,
            "precision": 0.6886120996441281,
            "recall": 0.8524229074889867
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.670760055196855,
            "auditor_fn_violation": 0.007824561403508772,
            "auditor_fp_violation": 0.011731391585760522,
            "ave_precision_score": 0.5756145426624635,
            "fpr": 0.32456140350877194,
            "logloss": 7.298246856563774,
            "mae": 0.38997155137488,
            "precision": 0.6042780748663101,
            "recall": 0.904
        },
        "train": {
            "accuracy": 0.6004390779363337,
            "auc_prc": 0.6370231430154687,
            "auditor_fn_violation": 0.004482656905080828,
            "auditor_fp_violation": 0.006862394223771223,
            "ave_precision_score": 0.5290964750465302,
            "fpr": 0.36663007683863885,
            "logloss": 8.234052714411014,
            "mae": 0.40910364978659985,
            "precision": 0.5593667546174143,
            "recall": 0.933920704845815
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7736462640471421,
            "auditor_fn_violation": 0.01008771929824562,
            "auditor_fp_violation": 0.02390457332652019,
            "ave_precision_score": 0.715841679047702,
            "fpr": 0.19188596491228072,
            "logloss": 3.8615193140358555,
            "mae": 0.3101503205776249,
            "precision": 0.6956521739130435,
            "recall": 0.8
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7651652438693026,
            "auditor_fn_violation": 0.013786466921667144,
            "auditor_fp_violation": 0.005084945247365656,
            "ave_precision_score": 0.7025182549690241,
            "fpr": 0.19978046103183314,
            "logloss": 3.784679573093464,
            "mae": 0.2947035142125862,
            "precision": 0.6738351254480287,
            "recall": 0.8281938325991189
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.783679316808885,
            "auditor_fn_violation": 0.0019429824561403538,
            "auditor_fp_violation": 0.020564533299267593,
            "ave_precision_score": 0.7322284380909148,
            "fpr": 0.2050438596491228,
            "logloss": 3.393416545980903,
            "mae": 0.29952507872466966,
            "precision": 0.6949429037520392,
            "recall": 0.852
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7837446304062513,
            "auditor_fn_violation": 0.009236110775301384,
            "auditor_fp_violation": 0.01796664641015356,
            "ave_precision_score": 0.7292913834564744,
            "fpr": 0.21405049396267836,
            "logloss": 3.3224240318379015,
            "mae": 0.28654897229205584,
            "precision": 0.674457429048414,
            "recall": 0.8898678414096917
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7076464117214496,
            "auditor_fn_violation": 0.009103070175438597,
            "auditor_fp_violation": 0.026374339976153984,
            "ave_precision_score": 0.693373208922852,
            "fpr": 0.16885964912280702,
            "logloss": 2.263516829560608,
            "mae": 0.30456739970350694,
            "precision": 0.7153419593345656,
            "recall": 0.774
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.6917590970728051,
            "auditor_fn_violation": 0.002292102883504113,
            "auditor_fp_violation": 0.03126388632012817,
            "ave_precision_score": 0.6772902395599067,
            "fpr": 0.16575192096597147,
            "logloss": 2.1831018390682333,
            "mae": 0.28256112699268116,
            "precision": 0.7062256809338522,
            "recall": 0.7995594713656388
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7781756890936726,
            "auditor_fn_violation": 0.0029342105263157952,
            "auditor_fp_violation": 0.019106093510475215,
            "ave_precision_score": 0.7262134339911402,
            "fpr": 0.1875,
            "logloss": 3.5352848003126005,
            "mae": 0.30812294104612264,
            "precision": 0.6994727592267135,
            "recall": 0.796
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.7791905527648716,
            "auditor_fn_violation": 0.00745175220143426,
            "auditor_fp_violation": 0.017651989902168246,
            "ave_precision_score": 0.7239743959356333,
            "fpr": 0.18880351262349068,
            "logloss": 3.3495165886436253,
            "mae": 0.2884705290529441,
            "precision": 0.6917562724014337,
            "recall": 0.8502202643171806
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.6579794995484269,
            "auditor_fn_violation": 0.017359649122807016,
            "auditor_fp_violation": 0.007220341509112598,
            "ave_precision_score": 0.6053380074630131,
            "fpr": 0.29714912280701755,
            "logloss": 5.030237596652774,
            "mae": 0.4007649878522551,
            "precision": 0.6100719424460431,
            "recall": 0.848
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.6155554049823837,
            "auditor_fn_violation": 0.007969167831254806,
            "auditor_fp_violation": 0.012713564097452274,
            "ave_precision_score": 0.5615447076856005,
            "fpr": 0.3227222832052689,
            "logloss": 5.39403152066048,
            "mae": 0.3943216424176329,
            "precision": 0.5787965616045845,
            "recall": 0.8898678414096917
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 16695,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7827755336893036,
            "auditor_fn_violation": 0.02190789473684211,
            "auditor_fp_violation": 0.02457524271844661,
            "ave_precision_score": 0.7602899468494009,
            "fpr": 0.16776315789473684,
            "logloss": 2.5113566099797744,
            "mae": 0.2972908815152311,
            "precision": 0.7156133828996283,
            "recall": 0.77
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.7986852374252087,
            "auditor_fn_violation": 0.012630744159731526,
            "auditor_fp_violation": 0.02433663922829891,
            "ave_precision_score": 0.781227078713308,
            "fpr": 0.15806805708013172,
            "logloss": 1.988084024101975,
            "mae": 0.2606327344537137,
            "precision": 0.722007722007722,
            "recall": 0.8237885462555066
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7882911441047367,
            "auditor_fn_violation": 0.014745614035087725,
            "auditor_fp_violation": 0.02170626809742804,
            "ave_precision_score": 0.7516238564575481,
            "fpr": 0.16337719298245615,
            "logloss": 2.6980423609941404,
            "mae": 0.28809922553331546,
            "precision": 0.7271062271062271,
            "recall": 0.794
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.7996496717268669,
            "auditor_fn_violation": 0.011915066466147962,
            "auditor_fp_violation": 0.024098845378752766,
            "ave_precision_score": 0.7647502575610274,
            "fpr": 0.150384193194292,
            "logloss": 2.323925857425846,
            "mae": 0.25147548284484417,
            "precision": 0.732421875,
            "recall": 0.8259911894273128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7857280455449961,
            "auditor_fn_violation": 0.018149122807017547,
            "auditor_fp_violation": 0.028508771929824563,
            "ave_precision_score": 0.751732031089462,
            "fpr": 0.22149122807017543,
            "logloss": 2.9792430407744317,
            "mae": 0.30347465186077816,
            "precision": 0.6788553259141494,
            "recall": 0.854
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7882666526253864,
            "auditor_fn_violation": 0.011155867831738372,
            "auditor_fp_violation": 0.019715271889644448,
            "ave_precision_score": 0.7527005097019377,
            "fpr": 0.2217343578485181,
            "logloss": 2.7077814572601073,
            "mae": 0.2868414401880137,
            "precision": 0.6655629139072847,
            "recall": 0.8854625550660793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7624328673370449,
            "auditor_fn_violation": 0.008587719298245615,
            "auditor_fp_violation": 0.021078180889116,
            "ave_precision_score": 0.695713888549916,
            "fpr": 0.19298245614035087,
            "logloss": 4.246163348793154,
            "mae": 0.310067173661098,
            "precision": 0.696551724137931,
            "recall": 0.808
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.762876520972241,
            "auditor_fn_violation": 0.007345367679415082,
            "auditor_fp_violation": 0.0307810927468072,
            "ave_precision_score": 0.6931783060025499,
            "fpr": 0.19758507135016465,
            "logloss": 4.002348685666742,
            "mae": 0.2876871894167071,
            "precision": 0.6814159292035398,
            "recall": 0.8480176211453745
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.768067865788765,
            "auditor_fn_violation": 0.009763157894736845,
            "auditor_fp_violation": 0.02694121529552036,
            "ave_precision_score": 0.7081250652271744,
            "fpr": 0.2050438596491228,
            "logloss": 3.8694568803308,
            "mae": 0.30573390399928957,
            "precision": 0.6909090909090909,
            "recall": 0.836
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.7721272191795432,
            "auditor_fn_violation": 0.006136452656469873,
            "auditor_fp_violation": 0.022086004510877267,
            "ave_precision_score": 0.7077462204595638,
            "fpr": 0.20087815587266739,
            "logloss": 3.680545663297292,
            "mae": 0.28098839592107866,
            "precision": 0.685025817555938,
            "recall": 0.8766519823788547
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7982255418921613,
            "auditor_fn_violation": 0.04900877192982457,
            "auditor_fp_violation": 0.03647430165218873,
            "ave_precision_score": 0.7987316968379247,
            "fpr": 0.14583333333333334,
            "logloss": 0.6929835905107936,
            "mae": 0.31147050271235793,
            "precision": 0.73767258382643,
            "recall": 0.748
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.83045690222567,
            "auditor_fn_violation": 0.03623118323766786,
            "auditor_fp_violation": 0.03575554792266657,
            "ave_precision_score": 0.830771747146141,
            "fpr": 0.14050493962678376,
            "logloss": 0.5759193735141954,
            "mae": 0.2779042628217981,
            "precision": 0.7403651115618661,
            "recall": 0.8039647577092511
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 16695,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.8191749572508433,
            "auditor_fn_violation": 0.029300438596491233,
            "auditor_fp_violation": 0.014179867143587122,
            "ave_precision_score": 0.8193697002235731,
            "fpr": 0.07894736842105263,
            "logloss": 1.7239946600443714,
            "mae": 0.316351931379144,
            "precision": 0.8027397260273973,
            "recall": 0.586
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.835240915190074,
            "auditor_fn_violation": 0.011136525191371259,
            "auditor_fp_violation": 0.015970619248811633,
            "ave_precision_score": 0.8359507416960542,
            "fpr": 0.07025246981339188,
            "logloss": 1.3585288362517522,
            "mae": 0.26291308395561286,
            "precision": 0.819718309859155,
            "recall": 0.6409691629955947
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7855919785071805,
            "auditor_fn_violation": 0.007482456140350877,
            "auditor_fp_violation": 0.022957119741100332,
            "ave_precision_score": 0.7397242630162104,
            "fpr": 0.19407894736842105,
            "logloss": 3.261014030260906,
            "mae": 0.2998932014985834,
            "precision": 0.6989795918367347,
            "recall": 0.822
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7860517403298496,
            "auditor_fn_violation": 0.009170829364062345,
            "auditor_fp_violation": 0.022479925635377967,
            "ave_precision_score": 0.7363698060237743,
            "fpr": 0.19538968166849616,
            "logloss": 3.1735357737776617,
            "mae": 0.28092158797740924,
            "precision": 0.6849557522123894,
            "recall": 0.8524229074889867
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7820771303216927,
            "auditor_fn_violation": 0.005087719298245617,
            "auditor_fp_violation": 0.022573880088570947,
            "ave_precision_score": 0.7362770655934133,
            "fpr": 0.23026315789473684,
            "logloss": 3.3259409098571155,
            "mae": 0.3106311521833847,
            "precision": 0.671875,
            "recall": 0.86
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7789745237835952,
            "auditor_fn_violation": 0.006939172231705489,
            "auditor_fp_violation": 0.014577483564601885,
            "ave_precision_score": 0.7303005529991845,
            "fpr": 0.24039517014270034,
            "logloss": 3.288505353575775,
            "mae": 0.3011740744892334,
            "precision": 0.6473429951690821,
            "recall": 0.8854625550660793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7567419568665913,
            "auditor_fn_violation": 0.00806140350877193,
            "auditor_fp_violation": 0.02083333333333333,
            "ave_precision_score": 0.7053492456424206,
            "fpr": 0.22587719298245615,
            "logloss": 3.8397527487027627,
            "mae": 0.3381723182873272,
            "precision": 0.6611842105263158,
            "recall": 0.804
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7544280033841406,
            "auditor_fn_violation": 0.009076533992272616,
            "auditor_fp_violation": 0.01295375990507462,
            "ave_precision_score": 0.7014917564358879,
            "fpr": 0.23380900109769484,
            "logloss": 3.7217406966445514,
            "mae": 0.3250538307706847,
            "precision": 0.6408094435075885,
            "recall": 0.8370044052863436
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7794956084567471,
            "auditor_fn_violation": 0.030517543859649127,
            "auditor_fp_violation": 0.026837421222960314,
            "ave_precision_score": 0.763584851093673,
            "fpr": 0.17982456140350878,
            "logloss": 2.707039379318326,
            "mae": 0.3059973610825371,
            "precision": 0.7028985507246377,
            "recall": 0.776
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.7871621724574722,
            "auditor_fn_violation": 0.015923828682234274,
            "auditor_fp_violation": 0.030334328544629585,
            "ave_precision_score": 0.77410179026427,
            "fpr": 0.17892425905598244,
            "logloss": 2.1913889359115624,
            "mae": 0.27341275236848017,
            "precision": 0.6998158379373849,
            "recall": 0.8370044052863436
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7830153041122253,
            "auditor_fn_violation": 0.011370614035087724,
            "auditor_fp_violation": 0.02332172968829842,
            "ave_precision_score": 0.7353994360122864,
            "fpr": 0.18530701754385964,
            "logloss": 3.3248923222075626,
            "mae": 0.3033951521663455,
            "precision": 0.700354609929078,
            "recall": 0.79
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7835878086763721,
            "auditor_fn_violation": 0.008165012064971932,
            "auditor_fp_violation": 0.018276499001986423,
            "ave_precision_score": 0.7313735436109409,
            "fpr": 0.18660812294182216,
            "logloss": 3.2030583922358487,
            "mae": 0.28158447182825946,
            "precision": 0.6903460837887068,
            "recall": 0.8348017621145375
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7653331566150866,
            "auditor_fn_violation": 0.004769736842105265,
            "auditor_fp_violation": 0.029317833418497714,
            "ave_precision_score": 0.703322798258587,
            "fpr": 0.19736842105263158,
            "logloss": 3.9519832028346977,
            "mae": 0.3054559458909933,
            "precision": 0.6984924623115578,
            "recall": 0.834
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.7766688360243083,
            "auditor_fn_violation": 0.007625835964738366,
            "auditor_fp_violation": 0.019460664333564727,
            "ave_precision_score": 0.7115478930729937,
            "fpr": 0.1986827661909989,
            "logloss": 3.621885431023348,
            "mae": 0.27796940667911013,
            "precision": 0.687392055267703,
            "recall": 0.8766519823788547
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.780316499043759,
            "auditor_fn_violation": 0.006052631578947366,
            "auditor_fp_violation": 0.023925864418327374,
            "ave_precision_score": 0.7314271675771066,
            "fpr": 0.21271929824561403,
            "logloss": 3.2931207908902427,
            "mae": 0.3196526817570024,
            "precision": 0.686084142394822,
            "recall": 0.848
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7848628906734565,
            "auditor_fn_violation": 0.011337205085180152,
            "auditor_fp_violation": 0.01937899775897312,
            "ave_precision_score": 0.7333685776652666,
            "fpr": 0.2327113062568606,
            "logloss": 3.1112446740450737,
            "mae": 0.3058789762084475,
            "precision": 0.6564019448946515,
            "recall": 0.8920704845814978
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7866069619426518,
            "auditor_fn_violation": 0.007083333333333335,
            "auditor_fp_violation": 0.023803440640436054,
            "ave_precision_score": 0.7405534865086165,
            "fpr": 0.19298245614035087,
            "logloss": 3.2659751725450614,
            "mae": 0.29997614022422286,
            "precision": 0.7011884550084889,
            "recall": 0.826
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7856379705853108,
            "auditor_fn_violation": 0.011076079440223988,
            "auditor_fp_violation": 0.022479925635377967,
            "ave_precision_score": 0.7352373469859207,
            "fpr": 0.19538968166849616,
            "logloss": 3.19994032224947,
            "mae": 0.28081477160800683,
            "precision": 0.6860670194003528,
            "recall": 0.8568281938325991
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.7934572096639517,
            "auditor_fn_violation": 0.02405701754385965,
            "auditor_fp_violation": 0.02549608243910748,
            "ave_precision_score": 0.755680781636384,
            "fpr": 0.14912280701754385,
            "logloss": 3.0232524663453377,
            "mae": 0.29395592054890946,
            "precision": 0.7333333333333333,
            "recall": 0.748
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8091545617171637,
            "auditor_fn_violation": 0.019335386876985647,
            "auditor_fp_violation": 0.018307724456977332,
            "ave_precision_score": 0.7708711497554468,
            "fpr": 0.13172338090010977,
            "logloss": 2.6661443242646934,
            "mae": 0.24701231168285737,
            "precision": 0.7525773195876289,
            "recall": 0.8039647577092511
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7666751920922519,
            "auditor_fn_violation": 0.01692982456140351,
            "auditor_fp_violation": 0.022472747402486806,
            "ave_precision_score": 0.76509615182755,
            "fpr": 0.16228070175438597,
            "logloss": 1.554520152825787,
            "mae": 0.2966694960913214,
            "precision": 0.7238805970149254,
            "recall": 0.776
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7531118835688272,
            "auditor_fn_violation": 0.00938601623814659,
            "auditor_fp_violation": 0.018163606972403913,
            "ave_precision_score": 0.7495537907082719,
            "fpr": 0.17453347969264543,
            "logloss": 1.3805621022434744,
            "mae": 0.2722169535781567,
            "precision": 0.6988636363636364,
            "recall": 0.8127753303964758
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7653482465907228,
            "auditor_fn_violation": 0.006052631578947369,
            "auditor_fp_violation": 0.024431527848748087,
            "ave_precision_score": 0.7051648793584826,
            "fpr": 0.18640350877192982,
            "logloss": 4.007251370127354,
            "mae": 0.3155931612331289,
            "precision": 0.6991150442477876,
            "recall": 0.79
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7551889215075365,
            "auditor_fn_violation": 0.004083714947508914,
            "auditor_fp_violation": 0.03678358597929032,
            "ave_precision_score": 0.6894170384751414,
            "fpr": 0.18990120746432493,
            "logloss": 3.9713781795307375,
            "mae": 0.290210542480248,
            "precision": 0.6831501831501832,
            "recall": 0.8215859030837004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7862148690751107,
            "auditor_fn_violation": 0.010030701754385966,
            "auditor_fp_violation": 0.02252065235905298,
            "ave_precision_score": 0.7429567023597896,
            "fpr": 0.18969298245614036,
            "logloss": 3.236464701448985,
            "mae": 0.30153425969456893,
            "precision": 0.7027491408934707,
            "recall": 0.818
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7874488449297732,
            "auditor_fn_violation": 0.012014197498029469,
            "auditor_fp_violation": 0.021483113033745112,
            "ave_precision_score": 0.7399066259571911,
            "fpr": 0.1964873765093304,
            "logloss": 3.130003428848529,
            "mae": 0.2815814240269947,
            "precision": 0.6848591549295775,
            "recall": 0.8568281938325991
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 16695,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7638364216429911,
            "auditor_fn_violation": 0.004978070175438598,
            "auditor_fp_violation": 0.02972236416283427,
            "ave_precision_score": 0.7032921827211007,
            "fpr": 0.20723684210526316,
            "logloss": 4.012566123727008,
            "mae": 0.31300571623965673,
            "precision": 0.6844741235392321,
            "recall": 0.82
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7602565157590343,
            "auditor_fn_violation": 0.005154813657838369,
            "auditor_fp_violation": 0.03434319657384701,
            "ave_precision_score": 0.6941682677293239,
            "fpr": 0.21295279912184412,
            "logloss": 3.9271340654769626,
            "mae": 0.2970928823131309,
            "precision": 0.6666666666666666,
            "recall": 0.8546255506607929
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7799185273133681,
            "auditor_fn_violation": 0.006447368421052634,
            "auditor_fp_violation": 0.02238758303525805,
            "ave_precision_score": 0.7272732522221026,
            "fpr": 0.20394736842105263,
            "logloss": 3.484674831417737,
            "mae": 0.303516407580071,
            "precision": 0.6945812807881774,
            "recall": 0.846
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7815480010851296,
            "auditor_fn_violation": 0.008194026025522615,
            "auditor_fp_violation": 0.01641978540906547,
            "ave_precision_score": 0.726393412420977,
            "fpr": 0.20636663007683864,
            "logloss": 3.3667785260054397,
            "mae": 0.28796450392376044,
            "precision": 0.6797274275979557,
            "recall": 0.8788546255506607
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 16695,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7646016162394511,
            "auditor_fn_violation": 0.009140350877192987,
            "auditor_fp_violation": 0.020460739226707554,
            "ave_precision_score": 0.7060166752080685,
            "fpr": 0.2138157894736842,
            "logloss": 3.985991472459466,
            "mae": 0.3206190290794525,
            "precision": 0.674457429048414,
            "recall": 0.808
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7741047267431416,
            "auditor_fn_violation": 0.007272832778038368,
            "auditor_fp_violation": 0.011906506183841072,
            "ave_precision_score": 0.7181989169441323,
            "fpr": 0.21405049396267836,
            "logloss": 3.4522603279244524,
            "mae": 0.3021193958229136,
            "precision": 0.6614583333333334,
            "recall": 0.8392070484581498
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7818149658716246,
            "auditor_fn_violation": 0.007368421052631582,
            "auditor_fp_violation": 0.020604454096406072,
            "ave_precision_score": 0.7429925476914444,
            "fpr": 0.18640350877192982,
            "logloss": 2.9254252539399146,
            "mae": 0.29719126266810786,
            "precision": 0.7043478260869566,
            "recall": 0.81
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.778284060606657,
            "auditor_fn_violation": 0.009182918514291791,
            "auditor_fp_violation": 0.016804098701261268,
            "ave_precision_score": 0.7395803817527427,
            "fpr": 0.17892425905598244,
            "logloss": 2.7521073601953674,
            "mae": 0.2761255865970378,
            "precision": 0.6987060998151571,
            "recall": 0.8325991189427313
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5942982456140351,
            "auc_prc": 0.6982229457348728,
            "auditor_fn_violation": 0.01514035087719301,
            "auditor_fp_violation": 0.01648462783171521,
            "ave_precision_score": 0.7009154549242298,
            "fpr": 0.10307017543859649,
            "logloss": 5.159538102580521,
            "mae": 0.4086858758468806,
            "precision": 0.7044025157232704,
            "recall": 0.448
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.7112659752556088,
            "auditor_fn_violation": 0.018220767225830162,
            "auditor_fp_violation": 0.009932096645185157,
            "ave_precision_score": 0.7138122749660949,
            "fpr": 0.09220636663007684,
            "logloss": 4.4344044526457855,
            "mae": 0.3599522110885208,
            "precision": 0.732484076433121,
            "recall": 0.5066079295154186
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7395398981294431,
            "auditor_fn_violation": 0.02475438596491229,
            "auditor_fp_violation": 0.030504811786748433,
            "ave_precision_score": 0.6925644015775474,
            "fpr": 0.16447368421052633,
            "logloss": 4.556275378331964,
            "mae": 0.3242064496976309,
            "precision": 0.7093023255813954,
            "recall": 0.732
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.6931961786986781,
            "auditor_fn_violation": 0.03073787337340484,
            "auditor_fp_violation": 0.019549536782385003,
            "ave_precision_score": 0.6428956606644177,
            "fpr": 0.17453347969264543,
            "logloss": 4.889392801808263,
            "mae": 0.32512695655094,
            "precision": 0.6701244813278008,
            "recall": 0.711453744493392
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7864168820279563,
            "auditor_fn_violation": 0.00978070175438597,
            "auditor_fp_violation": 0.02252065235905298,
            "ave_precision_score": 0.7431586279844054,
            "fpr": 0.18969298245614036,
            "logloss": 3.23406522805091,
            "mae": 0.3008310898290615,
            "precision": 0.7032590051457976,
            "recall": 0.82
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.7875918600949473,
            "auditor_fn_violation": 0.012014197498029469,
            "auditor_fp_violation": 0.01913880195135074,
            "ave_precision_score": 0.740049536930049,
            "fpr": 0.19209659714599342,
            "logloss": 3.1268174706025316,
            "mae": 0.281356057091163,
            "precision": 0.6897163120567376,
            "recall": 0.8568281938325991
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7920014605237483,
            "auditor_fn_violation": 0.016710526315789474,
            "auditor_fp_violation": 0.015957673309487313,
            "ave_precision_score": 0.7522772634052283,
            "fpr": 0.15350877192982457,
            "logloss": 3.022036603538017,
            "mae": 0.294592414919613,
            "precision": 0.7307692307692307,
            "recall": 0.76
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.7974017050428717,
            "auditor_fn_violation": 0.011088168590453439,
            "auditor_fp_violation": 0.010643076235747383,
            "ave_precision_score": 0.7552620846478655,
            "fpr": 0.15148188803512624,
            "logloss": 2.7985939433312783,
            "mae": 0.2643945497072191,
            "precision": 0.7250996015936255,
            "recall": 0.801762114537445
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7785891732301967,
            "auditor_fn_violation": 0.018184210526315792,
            "auditor_fp_violation": 0.029522760177141883,
            "ave_precision_score": 0.7318244772501343,
            "fpr": 0.24671052631578946,
            "logloss": 3.339925982899204,
            "mae": 0.33920282649409045,
            "precision": 0.6631736526946108,
            "recall": 0.886
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7848426413578655,
            "auditor_fn_violation": 0.017473657741650026,
            "auditor_fp_violation": 0.036862850595805706,
            "ave_precision_score": 0.7368427275458416,
            "fpr": 0.25686059275521406,
            "logloss": 3.1518132265095824,
            "mae": 0.3339301304967217,
            "precision": 0.6338028169014085,
            "recall": 0.8920704845814978
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7832629239054293,
            "auditor_fn_violation": 0.013664473684210528,
            "auditor_fp_violation": 0.021695622551524445,
            "ave_precision_score": 0.7318626809388133,
            "fpr": 0.17653508771929824,
            "logloss": 3.5333722299445345,
            "mae": 0.2993660438198741,
            "precision": 0.7083333333333334,
            "recall": 0.782
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7857361756884577,
            "auditor_fn_violation": 0.00804170273263152,
            "auditor_fp_violation": 0.013530229843368324,
            "ave_precision_score": 0.732762858043851,
            "fpr": 0.18551042810098792,
            "logloss": 3.231039098991398,
            "mae": 0.2795655247546653,
            "precision": 0.686456400742115,
            "recall": 0.8149779735682819
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7817502762232741,
            "auditor_fn_violation": 0.007368421052631582,
            "auditor_fp_violation": 0.02104890563788112,
            "ave_precision_score": 0.7429352267627989,
            "fpr": 0.1875,
            "logloss": 2.930423807787407,
            "mae": 0.29728599778860415,
            "precision": 0.703125,
            "recall": 0.81
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.7786555116836922,
            "auditor_fn_violation": 0.009182918514291791,
            "auditor_fp_violation": 0.018348557744273145,
            "ave_precision_score": 0.7399054790714248,
            "fpr": 0.1800219538968167,
            "logloss": 2.756201677739895,
            "mae": 0.276244375833995,
            "precision": 0.6974169741697417,
            "recall": 0.8325991189427313
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7766574664825362,
            "auditor_fn_violation": 0.011061403508771928,
            "auditor_fp_violation": 0.01757579628683359,
            "ave_precision_score": 0.7266755324100487,
            "fpr": 0.15679824561403508,
            "logloss": 3.281957379089636,
            "mae": 0.3111646242751592,
            "precision": 0.7228682170542635,
            "recall": 0.746
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8026110052176524,
            "auditor_fn_violation": 0.007408231260608226,
            "auditor_fp_violation": 0.010921703372589341,
            "ave_precision_score": 0.7519485962848396,
            "fpr": 0.12623490669593854,
            "logloss": 2.737054959489999,
            "mae": 0.25510904470650436,
            "precision": 0.7553191489361702,
            "recall": 0.7819383259911894
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7870307434120787,
            "auditor_fn_violation": 0.009515350877192987,
            "auditor_fp_violation": 0.024000383239652527,
            "ave_precision_score": 0.7409302579825058,
            "fpr": 0.19517543859649122,
            "logloss": 3.26505379794499,
            "mae": 0.30046054683976253,
            "precision": 0.6988155668358714,
            "recall": 0.826
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.7850187741055046,
            "auditor_fn_violation": 0.011883634675551386,
            "auditor_fp_violation": 0.022818601724125517,
            "ave_precision_score": 0.7339720984657618,
            "fpr": 0.1964873765093304,
            "logloss": 3.212842428669204,
            "mae": 0.281095507830311,
            "precision": 0.6859649122807018,
            "recall": 0.8612334801762115
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.789321193892448,
            "auditor_fn_violation": 0.016206140350877193,
            "auditor_fp_violation": 0.025892629024016352,
            "ave_precision_score": 0.7386950866688573,
            "fpr": 0.17653508771929824,
            "logloss": 3.1301896969311453,
            "mae": 0.2892992063347763,
            "precision": 0.7175438596491228,
            "recall": 0.818
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8061107081095932,
            "auditor_fn_violation": 0.011392815176235633,
            "auditor_fp_violation": 0.016566304851715116,
            "ave_precision_score": 0.7547962960624658,
            "fpr": 0.15148188803512624,
            "logloss": 2.7818415217999615,
            "mae": 0.2508937792627807,
            "precision": 0.7381404174573055,
            "recall": 0.8568281938325991
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.781906431030275,
            "auditor_fn_violation": 0.01044078947368421,
            "auditor_fp_violation": 0.019577158916709247,
            "ave_precision_score": 0.7442969842358444,
            "fpr": 0.19736842105263158,
            "logloss": 2.98144330774593,
            "mae": 0.2973457974097269,
            "precision": 0.6994991652754591,
            "recall": 0.838
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7816965932605291,
            "auditor_fn_violation": 0.009661648863378098,
            "auditor_fp_violation": 0.01637655016369344,
            "ave_precision_score": 0.742162155635816,
            "fpr": 0.2030735455543359,
            "logloss": 2.839261429941139,
            "mae": 0.27820288192609466,
            "precision": 0.6788194444444444,
            "recall": 0.8612334801762115
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7986009293062009,
            "auditor_fn_violation": 0.0039013157894736876,
            "auditor_fp_violation": 0.017187233861352413,
            "ave_precision_score": 0.798393948976994,
            "fpr": 0.06359649122807018,
            "logloss": 0.7861329484626552,
            "mae": 0.3352856835018955,
            "precision": 0.8347578347578347,
            "recall": 0.586
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8090493888175739,
            "auditor_fn_violation": 0.004903359333065761,
            "auditor_fp_violation": 0.01310268130580049,
            "ave_precision_score": 0.8086096731390862,
            "fpr": 0.06366630076838639,
            "logloss": 0.6532567033641777,
            "mae": 0.30176949126547015,
            "precision": 0.826865671641791,
            "recall": 0.6101321585903083
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7694609083593227,
            "auditor_fn_violation": 0.007714912280701758,
            "auditor_fp_violation": 0.024553951626639432,
            "ave_precision_score": 0.7146742295985371,
            "fpr": 0.19517543859649122,
            "logloss": 3.720569337557103,
            "mae": 0.3087157079714212,
            "precision": 0.693631669535284,
            "recall": 0.806
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.7736844325215341,
            "auditor_fn_violation": 0.006745745828034257,
            "auditor_fp_violation": 0.022631248994180057,
            "ave_precision_score": 0.7149566766938192,
            "fpr": 0.18990120746432493,
            "logloss": 3.513607110210204,
            "mae": 0.28055742730131505,
            "precision": 0.6905187835420393,
            "recall": 0.8502202643171806
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7836695336838898,
            "auditor_fn_violation": 0.00954824561403509,
            "auditor_fp_violation": 0.020178632260262314,
            "ave_precision_score": 0.746887350156952,
            "fpr": 0.1962719298245614,
            "logloss": 2.8681272342374857,
            "mae": 0.29493167535902737,
            "precision": 0.7001675041876047,
            "recall": 0.836
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7801915531545528,
            "auditor_fn_violation": 0.011402486496419196,
            "auditor_fp_violation": 0.01637655016369344,
            "ave_precision_score": 0.7410016196667039,
            "fpr": 0.2030735455543359,
            "logloss": 2.7729747524252506,
            "mae": 0.27834169296379047,
            "precision": 0.6810344827586207,
            "recall": 0.8700440528634361
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5932017543859649,
            "auc_prc": 0.6330851885632482,
            "auditor_fn_violation": 0.019506578947368423,
            "auditor_fp_violation": 0.019012944983818777,
            "ave_precision_score": 0.6328716542940451,
            "fpr": 0.14912280701754385,
            "logloss": 5.98527379689662,
            "mae": 0.4135113888430151,
            "precision": 0.6608478802992519,
            "recall": 0.53
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.608808346022852,
            "auditor_fn_violation": 0.013428628074875357,
            "auditor_fp_violation": 0.018850566982203894,
            "ave_precision_score": 0.6076284826712586,
            "fpr": 0.1525795828759605,
            "logloss": 5.359477695271821,
            "mae": 0.3819244140474828,
            "precision": 0.6454081632653061,
            "recall": 0.5572687224669604
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7692254350234786,
            "auditor_fn_violation": 0.01075438596491228,
            "auditor_fp_violation": 0.030153508771929825,
            "ave_precision_score": 0.7137610399642826,
            "fpr": 0.2050438596491228,
            "logloss": 3.7739722304981784,
            "mae": 0.30825180482477726,
            "precision": 0.6883333333333334,
            "recall": 0.826
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7682588047615816,
            "auditor_fn_violation": 0.007688699545931519,
            "auditor_fp_violation": 0.027716194241545714,
            "ave_precision_score": 0.7086097872503148,
            "fpr": 0.20636663007683864,
            "logloss": 3.621719035777923,
            "mae": 0.2846401053387712,
            "precision": 0.6758620689655173,
            "recall": 0.8634361233480177
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7759467553231245,
            "auditor_fn_violation": 0.0068245614035087766,
            "auditor_fp_violation": 0.024122807017543862,
            "ave_precision_score": 0.7214684984587333,
            "fpr": 0.2149122807017544,
            "logloss": 3.6178727789715746,
            "mae": 0.3089238366849241,
            "precision": 0.684887459807074,
            "recall": 0.852
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.7782328322095594,
            "auditor_fn_violation": 0.010217749773932892,
            "auditor_fp_violation": 0.023911492648807313,
            "ave_precision_score": 0.7218311030358467,
            "fpr": 0.21953896816684962,
            "logloss": 3.425794490814718,
            "mae": 0.2931233638891552,
            "precision": 0.6683250414593698,
            "recall": 0.8876651982378855
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7797042504975317,
            "auditor_fn_violation": 0.008662280701754385,
            "auditor_fp_violation": 0.01974748765116676,
            "ave_precision_score": 0.728366111707125,
            "fpr": 0.21271929824561403,
            "logloss": 3.4639497799740666,
            "mae": 0.3057160340576206,
            "precision": 0.6865912762520194,
            "recall": 0.85
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7839460237038989,
            "auditor_fn_violation": 0.00961329226246029,
            "auditor_fp_violation": 0.019674438602348642,
            "ave_precision_score": 0.7301325984407308,
            "fpr": 0.21624588364434688,
            "logloss": 3.302122196054354,
            "mae": 0.28934710095689137,
            "precision": 0.6716666666666666,
            "recall": 0.8876651982378855
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7863917059077288,
            "auditor_fn_violation": 0.007083333333333335,
            "auditor_fp_violation": 0.024833397206608755,
            "ave_precision_score": 0.7396834371073291,
            "fpr": 0.19407894736842105,
            "logloss": 3.2910934964388168,
            "mae": 0.2997673853191125,
            "precision": 0.7,
            "recall": 0.826
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7845118853795318,
            "auditor_fn_violation": 0.011769996663394535,
            "auditor_fp_violation": 0.0220691908043437,
            "ave_precision_score": 0.7320537726056049,
            "fpr": 0.19758507135016465,
            "logloss": 3.2632414630333777,
            "mae": 0.28093869232905316,
            "precision": 0.6830985915492958,
            "recall": 0.8546255506607929
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7765630832297953,
            "auditor_fn_violation": 0.004252192982456142,
            "auditor_fp_violation": 0.02399506046670074,
            "ave_precision_score": 0.7217823693774836,
            "fpr": 0.21929824561403508,
            "logloss": 3.637543265585931,
            "mae": 0.31488667684524235,
            "precision": 0.6830427892234548,
            "recall": 0.862
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.774900111867454,
            "auditor_fn_violation": 0.008360856298689054,
            "auditor_fp_violation": 0.020990711628119253,
            "ave_precision_score": 0.7184129549054271,
            "fpr": 0.2305159165751921,
            "logloss": 3.529701629879032,
            "mae": 0.3046359450153786,
            "precision": 0.6579804560260586,
            "recall": 0.8898678414096917
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7794998475070555,
            "auditor_fn_violation": 0.03494078947368422,
            "auditor_fp_violation": 0.02956534236075626,
            "ave_precision_score": 0.7606501167634214,
            "fpr": 0.15679824561403508,
            "logloss": 3.1125078354042754,
            "mae": 0.3048486547897461,
            "precision": 0.720703125,
            "recall": 0.738
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.7892091164096328,
            "auditor_fn_violation": 0.019961604858871267,
            "auditor_fp_violation": 0.03394447153319386,
            "ave_precision_score": 0.7743810646311036,
            "fpr": 0.1525795828759605,
            "logloss": 2.354516856723615,
            "mae": 0.26462607571919483,
            "precision": 0.7203219315895373,
            "recall": 0.788546255506608
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 16695,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7598598107234944,
            "auditor_fn_violation": 0.005885964912280705,
            "auditor_fp_violation": 0.021360287855561235,
            "ave_precision_score": 0.7118444979634698,
            "fpr": 0.20285087719298245,
            "logloss": 3.408930468301219,
            "mae": 0.3107504253078187,
            "precision": 0.6869712351945855,
            "recall": 0.812
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7564275581689665,
            "auditor_fn_violation": 0.008046538392723302,
            "auditor_fp_violation": 0.018670420126487116,
            "ave_precision_score": 0.7094717243546261,
            "fpr": 0.19209659714599342,
            "logloss": 3.1024665589430667,
            "mae": 0.2862278173752826,
            "precision": 0.6858168761220825,
            "recall": 0.8414096916299559
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.785653356530376,
            "auditor_fn_violation": 0.007710526315789477,
            "auditor_fp_violation": 0.022957119741100332,
            "ave_precision_score": 0.7397696803612663,
            "fpr": 0.19407894736842105,
            "logloss": 3.258756132266438,
            "mae": 0.299663207016494,
            "precision": 0.6994906621392191,
            "recall": 0.824
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7865274180109506,
            "auditor_fn_violation": 0.011769996663394535,
            "auditor_fp_violation": 0.022818601724125517,
            "ave_precision_score": 0.7368109212728555,
            "fpr": 0.1964873765093304,
            "logloss": 3.172527936192261,
            "mae": 0.28093211531310275,
            "precision": 0.6843033509700176,
            "recall": 0.8546255506607929
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.793367715750525,
            "auditor_fn_violation": 0.02405701754385965,
            "auditor_fp_violation": 0.02549608243910748,
            "ave_precision_score": 0.7555868508527107,
            "fpr": 0.14912280701754385,
            "logloss": 3.0256444741246646,
            "mae": 0.29394581488334137,
            "precision": 0.7333333333333333,
            "recall": 0.748
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8098611184556708,
            "auditor_fn_violation": 0.019335386876985647,
            "auditor_fp_violation": 0.018307724456977332,
            "ave_precision_score": 0.7722441756389258,
            "fpr": 0.13172338090010977,
            "logloss": 2.6485860843948354,
            "mae": 0.24710184760012766,
            "precision": 0.7525773195876289,
            "recall": 0.8039647577092511
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.788175604089576,
            "auditor_fn_violation": 0.007934210526315792,
            "auditor_fp_violation": 0.024833397206608755,
            "ave_precision_score": 0.7440658421303665,
            "fpr": 0.19407894736842105,
            "logloss": 3.190741002173599,
            "mae": 0.2991090837019883,
            "precision": 0.700507614213198,
            "recall": 0.828
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7842979670114907,
            "auditor_fn_violation": 0.011076079440223988,
            "auditor_fp_violation": 0.021994730103980762,
            "ave_precision_score": 0.7347760449665551,
            "fpr": 0.19538968166849616,
            "logloss": 3.1889866498367194,
            "mae": 0.2818469881835476,
            "precision": 0.6860670194003528,
            "recall": 0.8568281938325991
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.750640365756841,
            "auditor_fn_violation": 0.005824561403508776,
            "auditor_fp_violation": 0.020476707545562946,
            "ave_precision_score": 0.6898374343312997,
            "fpr": 0.20833333333333334,
            "logloss": 4.156006333840201,
            "mae": 0.3215838082010214,
            "precision": 0.6801346801346801,
            "recall": 0.808
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7270458414298253,
            "auditor_fn_violation": 0.009714841124387686,
            "auditor_fp_violation": 0.022722523401076552,
            "ave_precision_score": 0.6618098464033515,
            "fpr": 0.21624588364434688,
            "logloss": 4.2648746817514125,
            "mae": 0.3031749251981837,
            "precision": 0.6573913043478261,
            "recall": 0.8325991189427313
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7513981585234855,
            "auditor_fn_violation": 0.022828947368421056,
            "auditor_fp_violation": 0.01673479816044967,
            "ave_precision_score": 0.7491301690764194,
            "fpr": 0.13267543859649122,
            "logloss": 1.2507036985378002,
            "mae": 0.3122815934587842,
            "precision": 0.7535641547861507,
            "recall": 0.74
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8252593228191272,
            "auditor_fn_violation": 0.019395832628132905,
            "auditor_fp_violation": 0.011245967712879543,
            "ave_precision_score": 0.8257548117804252,
            "fpr": 0.1163556531284303,
            "logloss": 0.6350409757605302,
            "mae": 0.26204229917880206,
            "precision": 0.7710583153347732,
            "recall": 0.7863436123348018
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7818429509054869,
            "auditor_fn_violation": 0.007368421052631582,
            "auditor_fp_violation": 0.020604454096406072,
            "ave_precision_score": 0.7430193871920965,
            "fpr": 0.18640350877192982,
            "logloss": 2.92513568361408,
            "mae": 0.2972115911526766,
            "precision": 0.7043478260869566,
            "recall": 0.81
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.7782927711134653,
            "auditor_fn_violation": 0.009182918514291791,
            "auditor_fp_violation": 0.016804098701261268,
            "ave_precision_score": 0.7395904730907688,
            "fpr": 0.17892425905598244,
            "logloss": 2.751071150925566,
            "mae": 0.27604819598199054,
            "precision": 0.6987060998151571,
            "recall": 0.8325991189427313
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7882026788371773,
            "auditor_fn_violation": 0.019070175438596494,
            "auditor_fp_violation": 0.028077627320729017,
            "ave_precision_score": 0.7432254347607159,
            "fpr": 0.16885964912280702,
            "logloss": 3.3094672635895557,
            "mae": 0.2941125169756919,
            "precision": 0.72,
            "recall": 0.792
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8006154226240824,
            "auditor_fn_violation": 0.012272905312939744,
            "auditor_fp_violation": 0.020471888683654915,
            "ave_precision_score": 0.7527097111618086,
            "fpr": 0.15697036223929747,
            "logloss": 3.0630558791838793,
            "mae": 0.2549757937484591,
            "precision": 0.7265774378585086,
            "recall": 0.8370044052863436
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7792487420338714,
            "auditor_fn_violation": 0.005324561403508772,
            "auditor_fp_violation": 0.020450093680803947,
            "ave_precision_score": 0.7390890449486169,
            "fpr": 0.24780701754385964,
            "logloss": 3.161635852991719,
            "mae": 0.30927813809893484,
            "precision": 0.6666666666666666,
            "recall": 0.904
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7760251286214953,
            "auditor_fn_violation": 0.010135543552372618,
            "auditor_fp_violation": 0.022126837798173083,
            "ave_precision_score": 0.7363731077097926,
            "fpr": 0.2678375411635565,
            "logloss": 3.1044194725590097,
            "mae": 0.3137535274198282,
            "precision": 0.6314199395770392,
            "recall": 0.920704845814978
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7648282919491951,
            "auditor_fn_violation": 0.005129385964912281,
            "auditor_fp_violation": 0.020295733265201843,
            "ave_precision_score": 0.7055772312710049,
            "fpr": 0.22149122807017543,
            "logloss": 3.916386161099871,
            "mae": 0.3241031911083262,
            "precision": 0.6736672051696284,
            "recall": 0.834
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7711299182499727,
            "auditor_fn_violation": 0.004584205767008227,
            "auditor_fp_violation": 0.009086607402354405,
            "ave_precision_score": 0.7113574371354288,
            "fpr": 0.22283205268935236,
            "logloss": 3.550661810834379,
            "mae": 0.30378236295950173,
            "precision": 0.66110183639399,
            "recall": 0.8722466960352423
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7923223864888622,
            "auditor_fn_violation": 0.023793859649122805,
            "auditor_fp_violation": 0.010076009197751661,
            "ave_precision_score": 0.7568414761016848,
            "fpr": 0.12171052631578948,
            "logloss": 3.0417265728782747,
            "mae": 0.30237970127225067,
            "precision": 0.7592190889370932,
            "recall": 0.7
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.802919226959332,
            "auditor_fn_violation": 0.015046156375576047,
            "auditor_fp_violation": 0.013729592363694887,
            "ave_precision_score": 0.7648518855933323,
            "fpr": 0.1141602634467618,
            "logloss": 2.8431411737078807,
            "mae": 0.25479900280952555,
            "precision": 0.7683741648106904,
            "recall": 0.7599118942731278
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7873185933659576,
            "auditor_fn_violation": 0.007482456140350877,
            "auditor_fp_violation": 0.026592573667177665,
            "ave_precision_score": 0.7411338381172242,
            "fpr": 0.19736842105263158,
            "logloss": 3.2884848462320915,
            "mae": 0.30122944646386357,
            "precision": 0.6954314720812182,
            "recall": 0.822
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7838808473166092,
            "auditor_fn_violation": 0.009925192338380153,
            "auditor_fp_violation": 0.022823405640277957,
            "ave_precision_score": 0.7307819845577179,
            "fpr": 0.1942919868276619,
            "logloss": 3.275727713037585,
            "mae": 0.2811047270122906,
            "precision": 0.6861702127659575,
            "recall": 0.8524229074889867
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7685147544890056,
            "auditor_fn_violation": 0.006309210526315792,
            "auditor_fp_violation": 0.024569919945494807,
            "ave_precision_score": 0.7106165964268863,
            "fpr": 0.19846491228070176,
            "logloss": 3.8409902175097086,
            "mae": 0.3105695750828888,
            "precision": 0.6921768707482994,
            "recall": 0.814
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7689974737927449,
            "auditor_fn_violation": 0.0072631614578548085,
            "auditor_fp_violation": 0.02488428566967793,
            "ave_precision_score": 0.705183852387509,
            "fpr": 0.19538968166849616,
            "logloss": 3.711863933019622,
            "mae": 0.28625902153288524,
            "precision": 0.6855123674911661,
            "recall": 0.8546255506607929
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.743818144113793,
            "auditor_fn_violation": 0.013504385964912284,
            "auditor_fp_violation": 0.026310466700732418,
            "ave_precision_score": 0.667266907822353,
            "fpr": 0.19407894736842105,
            "logloss": 5.323266356089456,
            "mae": 0.35117435461057006,
            "precision": 0.6810810810810811,
            "recall": 0.756
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.7160287836801519,
            "auditor_fn_violation": 0.006629689985831519,
            "auditor_fp_violation": 0.0271084988482611,
            "ave_precision_score": 0.6242730580110623,
            "fpr": 0.21185510428100987,
            "logloss": 5.8878548250839104,
            "mae": 0.35239708904223305,
            "precision": 0.6445672191528545,
            "recall": 0.7709251101321586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7717481388119765,
            "auditor_fn_violation": 0.004605263157894739,
            "auditor_fp_violation": 0.020524612502129117,
            "ave_precision_score": 0.7144062515873371,
            "fpr": 0.2149122807017544,
            "logloss": 3.706506851184389,
            "mae": 0.31669830916821995,
            "precision": 0.6869009584664537,
            "recall": 0.86
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7739305362534629,
            "auditor_fn_violation": 0.004337587102327403,
            "auditor_fp_violation": 0.014661552097269695,
            "ave_precision_score": 0.7155978112439572,
            "fpr": 0.22941822173435786,
            "logloss": 3.5005442259782424,
            "mae": 0.3044547567472101,
            "precision": 0.6612641815235009,
            "recall": 0.8986784140969163
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7852911354500837,
            "auditor_fn_violation": 0.005201754385964914,
            "auditor_fp_violation": 0.018853261795264867,
            "ave_precision_score": 0.7360353030132533,
            "fpr": 0.23026315789473684,
            "logloss": 3.4493750888074604,
            "mae": 0.3044778045078294,
            "precision": 0.6754250386398764,
            "recall": 0.874
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7852152506685364,
            "auditor_fn_violation": 0.007364710319782205,
            "auditor_fp_violation": 0.019357380136287114,
            "ave_precision_score": 0.7320467479123623,
            "fpr": 0.23710208562019758,
            "logloss": 3.3546812541688844,
            "mae": 0.29468790315966664,
            "precision": 0.6521739130434783,
            "recall": 0.8920704845814978
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7780443275167988,
            "auditor_fn_violation": 0.004434210526315788,
            "auditor_fp_violation": 0.023196644523931192,
            "ave_precision_score": 0.7267039048395438,
            "fpr": 0.20833333333333334,
            "logloss": 3.4966155693294847,
            "mae": 0.3101339228809944,
            "precision": 0.6900489396411092,
            "recall": 0.846
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7801419034334344,
            "auditor_fn_violation": 0.007727384826665767,
            "auditor_fp_violation": 0.023438306907791247,
            "ave_precision_score": 0.725813484852283,
            "fpr": 0.21075740944017562,
            "logloss": 3.367147074704992,
            "mae": 0.2916710631520134,
            "precision": 0.6756756756756757,
            "recall": 0.8810572687224669
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.789446303864244,
            "auditor_fn_violation": 0.0118421052631579,
            "auditor_fp_violation": 0.014499233520694942,
            "ave_precision_score": 0.7475151399251909,
            "fpr": 0.1524122807017544,
            "logloss": 2.9111674022772,
            "mae": 0.2937376629544751,
            "precision": 0.7321772639691715,
            "recall": 0.76
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.7907355065199255,
            "auditor_fn_violation": 0.011545138469126732,
            "auditor_fp_violation": 0.014430964121952222,
            "ave_precision_score": 0.743760170613305,
            "fpr": 0.150384193194292,
            "logloss": 2.798930215106693,
            "mae": 0.26136871136694967,
            "precision": 0.7270916334661355,
            "recall": 0.8039647577092511
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7715984471728156,
            "auditor_fn_violation": 0.012638157894736848,
            "auditor_fp_violation": 0.01862970533128939,
            "ave_precision_score": 0.7176044466905475,
            "fpr": 0.1513157894736842,
            "logloss": 3.5238820423733577,
            "mae": 0.31311601672745093,
            "precision": 0.727810650887574,
            "recall": 0.738
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8000936515452495,
            "auditor_fn_violation": 0.006953679211980832,
            "auditor_fp_violation": 0.010840036797997731,
            "ave_precision_score": 0.7494331693821595,
            "fpr": 0.12843029637760703,
            "logloss": 2.786692883346854,
            "mae": 0.2588204054086703,
            "precision": 0.7505330490405118,
            "recall": 0.775330396475771
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5877192982456141,
            "auc_prc": 0.6468724685152404,
            "auditor_fn_violation": 0.014243421052631591,
            "auditor_fp_violation": 0.00593755322772952,
            "ave_precision_score": 0.6348483575406629,
            "fpr": 0.08881578947368421,
            "logloss": 9.472648700619958,
            "mae": 0.43042835421801623,
            "precision": 0.7167832167832168,
            "recall": 0.41
        },
        "train": {
            "accuracy": 0.6388583973655324,
            "auc_prc": 0.6784493945681616,
            "auditor_fn_violation": 0.009550428681267138,
            "auditor_fp_violation": 0.00977596937023061,
            "ave_precision_score": 0.6669886936511342,
            "fpr": 0.0801317233809001,
            "logloss": 8.094114044155763,
            "mae": 0.3754404524660458,
            "precision": 0.7306273062730627,
            "recall": 0.43612334801762115
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8113695142536492,
            "auditor_fn_violation": 0.012903508771929828,
            "auditor_fp_violation": 0.022185317663089774,
            "ave_precision_score": 0.8110148558811556,
            "fpr": 0.13596491228070176,
            "logloss": 1.0144401757370736,
            "mae": 0.28318143707569204,
            "precision": 0.7524950099800399,
            "recall": 0.754
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.839705648910154,
            "auditor_fn_violation": 0.00931831699686166,
            "auditor_fp_violation": 0.01486091461759626,
            "ave_precision_score": 0.8401234111580578,
            "fpr": 0.14050493962678376,
            "logloss": 0.7462838962171081,
            "mae": 0.25202994007558815,
            "precision": 0.7398373983739838,
            "recall": 0.801762114537445
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.781198080170774,
            "auditor_fn_violation": 0.01126535087719298,
            "auditor_fp_violation": 0.025506727985011076,
            "ave_precision_score": 0.7313098797787565,
            "fpr": 0.17543859649122806,
            "logloss": 3.3591927852044856,
            "mae": 0.3023153213248367,
            "precision": 0.7106690777576854,
            "recall": 0.786
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7820622471317538,
            "auditor_fn_violation": 0.008887943248693162,
            "auditor_fp_violation": 0.010967340576037592,
            "ave_precision_score": 0.7299087738891372,
            "fpr": 0.18441273326015367,
            "logloss": 3.198712453190774,
            "mae": 0.28589184962685343,
            "precision": 0.6900369003690037,
            "recall": 0.8237885462555066
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7884888982703557,
            "auditor_fn_violation": 0.01082456140350877,
            "auditor_fp_violation": 0.023803440640436054,
            "ave_precision_score": 0.7457710209678259,
            "fpr": 0.19298245614035087,
            "logloss": 3.171193643019109,
            "mae": 0.30058471979211326,
            "precision": 0.6991452991452991,
            "recall": 0.818
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.7839019936109709,
            "auditor_fn_violation": 0.007799919728042477,
            "auditor_fp_violation": 0.02073850603011575,
            "ave_precision_score": 0.7367343576798848,
            "fpr": 0.19758507135016465,
            "logloss": 3.120348230090616,
            "mae": 0.2810505686786709,
            "precision": 0.6853146853146853,
            "recall": 0.8634361233480177
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7438757070132046,
            "auditor_fn_violation": 0.005603070175438601,
            "auditor_fp_violation": 0.026757579628683362,
            "ave_precision_score": 0.7092926422602301,
            "fpr": 0.21710526315789475,
            "logloss": 2.9752978485406794,
            "mae": 0.31096506781077865,
            "precision": 0.6759410801963993,
            "recall": 0.826
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7308419031717496,
            "auditor_fn_violation": 0.007768487937445903,
            "auditor_fp_violation": 0.018226057882385734,
            "ave_precision_score": 0.6972973310173654,
            "fpr": 0.21844127332601537,
            "logloss": 2.725628379695882,
            "mae": 0.2950975028195906,
            "precision": 0.6615646258503401,
            "recall": 0.8568281938325991
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 16695,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7794152875810579,
            "auditor_fn_violation": 0.005493421052631585,
            "auditor_fp_violation": 0.02659523505365356,
            "ave_precision_score": 0.7271437727596608,
            "fpr": 0.20285087719298245,
            "logloss": 3.5428004017964834,
            "mae": 0.30366205378862565,
            "precision": 0.6926910299003323,
            "recall": 0.834
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.7806599727964845,
            "auditor_fn_violation": 0.004884016692698639,
            "auditor_fp_violation": 0.02250154325806398,
            "ave_precision_score": 0.7242308203204738,
            "fpr": 0.20087815587266739,
            "logloss": 3.350417585731889,
            "mae": 0.2820900345327463,
            "precision": 0.6833910034602076,
            "recall": 0.8700440528634361
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7847093746505817,
            "auditor_fn_violation": 0.0061403508771929885,
            "auditor_fp_violation": 0.026592573667177665,
            "ave_precision_score": 0.7380612812566927,
            "fpr": 0.19736842105263158,
            "logloss": 3.3206174405327196,
            "mae": 0.3006858810100941,
            "precision": 0.6974789915966386,
            "recall": 0.83
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.7815327661893756,
            "auditor_fn_violation": 0.008363274128734947,
            "auditor_fp_violation": 0.021055564496177293,
            "ave_precision_score": 0.7315748779910316,
            "fpr": 0.19978046103183314,
            "logloss": 3.169546557429903,
            "mae": 0.28144827006027806,
            "precision": 0.6845753899480069,
            "recall": 0.8700440528634361
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.7918054725773166,
            "auditor_fn_violation": 0.011776315789473682,
            "auditor_fp_violation": 0.0170435189916539,
            "ave_precision_score": 0.7521874698178034,
            "fpr": 0.16666666666666666,
            "logloss": 2.99241803498992,
            "mae": 0.2936058391092401,
            "precision": 0.7195571955719557,
            "recall": 0.78
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.7972950503767015,
            "auditor_fn_violation": 0.011387979516143851,
            "auditor_fp_violation": 0.018677626000715784,
            "ave_precision_score": 0.75659968304872,
            "fpr": 0.1690450054884742,
            "logloss": 2.7210618113578753,
            "mae": 0.2654349206782439,
            "precision": 0.7088846880907372,
            "recall": 0.8259911894273128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 16695,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7832841592494404,
            "auditor_fn_violation": 0.008037280701754386,
            "auditor_fp_violation": 0.026781532106966457,
            "ave_precision_score": 0.7359567623517631,
            "fpr": 0.19846491228070176,
            "logloss": 3.362240788573761,
            "mae": 0.3022101796969633,
            "precision": 0.6952861952861953,
            "recall": 0.826
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7829012130205865,
            "auditor_fn_violation": 0.008578461002819194,
            "auditor_fp_violation": 0.02183620087095,
            "ave_precision_score": 0.7319553469441202,
            "fpr": 0.2030735455543359,
            "logloss": 3.20741201487725,
            "mae": 0.28270261712552325,
            "precision": 0.6799307958477508,
            "recall": 0.8656387665198237
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7860651487146996,
            "auditor_fn_violation": 0.004096491228070178,
            "auditor_fp_violation": 0.020907852154658506,
            "ave_precision_score": 0.7376423172818032,
            "fpr": 0.19956140350877194,
            "logloss": 3.3218696282318367,
            "mae": 0.2966840097676269,
            "precision": 0.6991735537190082,
            "recall": 0.846
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7868998952477647,
            "auditor_fn_violation": 0.012101239379681524,
            "auditor_fp_violation": 0.022331004234652094,
            "ave_precision_score": 0.7343858380134061,
            "fpr": 0.20417124039517015,
            "logloss": 3.2350536708065682,
            "mae": 0.2806686209413387,
            "precision": 0.6809605488850772,
            "recall": 0.8744493392070485
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.777736622198938,
            "auditor_fn_violation": 0.0047127192982456155,
            "auditor_fp_violation": 0.024404913983989102,
            "ave_precision_score": 0.7269772475901066,
            "fpr": 0.18969298245614036,
            "logloss": 3.455763093002295,
            "mae": 0.31171014556173726,
            "precision": 0.6964912280701754,
            "recall": 0.794
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7758957817088384,
            "auditor_fn_violation": 0.007758816617262343,
            "auditor_fp_violation": 0.015932187919592056,
            "ave_precision_score": 0.7222701320319138,
            "fpr": 0.20087815587266739,
            "logloss": 3.33609041476368,
            "mae": 0.29791641083129905,
            "precision": 0.676678445229682,
            "recall": 0.8436123348017621
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 16695,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7845629815756071,
            "auditor_fn_violation": 0.017166666666666667,
            "auditor_fp_violation": 0.018871891500596157,
            "ave_precision_score": 0.7532883429352684,
            "fpr": 0.1524122807017544,
            "logloss": 2.8463247095295,
            "mae": 0.3028609302187045,
            "precision": 0.7274509803921568,
            "recall": 0.742
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.791854235169546,
            "auditor_fn_violation": 0.013182009410194541,
            "auditor_fp_violation": 0.014721601049175291,
            "ave_precision_score": 0.7600736696683836,
            "fpr": 0.1394072447859495,
            "logloss": 2.4058794204364546,
            "mae": 0.25919887859759794,
            "precision": 0.7386831275720165,
            "recall": 0.7907488986784141
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7806803473872418,
            "auditor_fn_violation": 0.007182017543859653,
            "auditor_fp_violation": 0.02238758303525805,
            "ave_precision_score": 0.7286006615794669,
            "fpr": 0.20394736842105263,
            "logloss": 3.457989890899452,
            "mae": 0.3039648406599251,
            "precision": 0.6955810147299509,
            "recall": 0.85
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7814329912169979,
            "auditor_fn_violation": 0.007780577087675356,
            "auditor_fp_violation": 0.014558267899992079,
            "ave_precision_score": 0.7262815043136478,
            "fpr": 0.20965971459934138,
            "logloss": 3.370285276313014,
            "mae": 0.28977387477560795,
            "precision": 0.6773648648648649,
            "recall": 0.8832599118942731
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6337719298245614,
            "auc_prc": 0.738943517116659,
            "auditor_fn_violation": 0.011565789473684215,
            "auditor_fp_violation": 0.037546840401975826,
            "ave_precision_score": 0.6793885086895444,
            "fpr": 0.29385964912280704,
            "logloss": 4.40709146979157,
            "mae": 0.3729214932997866,
            "precision": 0.6182336182336182,
            "recall": 0.868
        },
        "train": {
            "accuracy": 0.6487376509330406,
            "auc_prc": 0.7364733922898491,
            "auditor_fn_violation": 0.010416011837695907,
            "auditor_fp_violation": 0.03145844492430229,
            "ave_precision_score": 0.6757915490621285,
            "fpr": 0.3029637760702525,
            "logloss": 4.22666225765544,
            "mae": 0.3662769689679136,
            "precision": 0.597667638483965,
            "recall": 0.9030837004405287
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7853893229701057,
            "auditor_fn_violation": 0.008557017543859655,
            "auditor_fp_violation": 0.021589167092488502,
            "ave_precision_score": 0.7365484837191948,
            "fpr": 0.19517543859649122,
            "logloss": 3.326799054128304,
            "mae": 0.29619349908901643,
            "precision": 0.7008403361344537,
            "recall": 0.834
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7871554665457892,
            "auditor_fn_violation": 0.012969240366156185,
            "auditor_fp_violation": 0.01987860503882765,
            "ave_precision_score": 0.7342450653781002,
            "fpr": 0.20087815587266739,
            "logloss": 3.2121303793522284,
            "mae": 0.27787873771326455,
            "precision": 0.6822916666666666,
            "recall": 0.8656387665198237
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7552612680487882,
            "auditor_fn_violation": 0.015155701754385966,
            "auditor_fp_violation": 0.01968361437574519,
            "ave_precision_score": 0.751671213920226,
            "fpr": 0.1787280701754386,
            "logloss": 1.5653884149835684,
            "mae": 0.2961200625723984,
            "precision": 0.7099644128113879,
            "recall": 0.798
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7527464825476634,
            "auditor_fn_violation": 0.011337205085180158,
            "auditor_fp_violation": 0.0173733627653263,
            "ave_precision_score": 0.7476446522922713,
            "fpr": 0.1756311745334797,
            "logloss": 1.4601096002492318,
            "mae": 0.2713349227166227,
            "precision": 0.6986817325800376,
            "recall": 0.8171806167400881
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8084919088040854,
            "auditor_fn_violation": 0.01324561403508772,
            "auditor_fp_violation": 0.019742164878214962,
            "ave_precision_score": 0.8081312741279425,
            "fpr": 0.14144736842105263,
            "logloss": 1.1345789017804149,
            "mae": 0.28309428514319485,
            "precision": 0.7465618860510805,
            "recall": 0.76
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8378210367099966,
            "auditor_fn_violation": 0.013767124281300022,
            "auditor_fp_violation": 0.019256497897085702,
            "ave_precision_score": 0.8378680960650644,
            "fpr": 0.132821075740944,
            "logloss": 0.8214452968415142,
            "mae": 0.2503410113022114,
            "precision": 0.7468619246861925,
            "recall": 0.7863436123348018
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7756269803030885,
            "auditor_fn_violation": 0.0025087719298245662,
            "auditor_fp_violation": 0.027066300459887584,
            "ave_precision_score": 0.7205515851452906,
            "fpr": 0.2050438596491228,
            "logloss": 3.624041569270522,
            "mae": 0.30852620771816885,
            "precision": 0.6939443535188216,
            "recall": 0.848
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7776790811165861,
            "auditor_fn_violation": 0.009760779895259602,
            "auditor_fp_violation": 0.019655222937738855,
            "ave_precision_score": 0.7212726195850473,
            "fpr": 0.21624588364434688,
            "logloss": 3.425329287943208,
            "mae": 0.2948046458937004,
            "precision": 0.6694630872483222,
            "recall": 0.8788546255506607
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7752982673862614,
            "auditor_fn_violation": 0.007864035087719302,
            "auditor_fp_violation": 0.028295861011752684,
            "ave_precision_score": 0.7211997197791057,
            "fpr": 0.21052631578947367,
            "logloss": 3.6961381688919706,
            "mae": 0.3079567960777029,
            "precision": 0.6847290640394089,
            "recall": 0.834
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7738115833512528,
            "auditor_fn_violation": 0.0057302572087602815,
            "auditor_fp_violation": 0.019525517201622756,
            "ave_precision_score": 0.7140880367964143,
            "fpr": 0.21295279912184412,
            "logloss": 3.584379870925603,
            "mae": 0.29045372256564383,
            "precision": 0.6722972972972973,
            "recall": 0.8766519823788547
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7775416721711742,
            "auditor_fn_violation": 0.030208333333333337,
            "auditor_fp_violation": 0.025453500255493103,
            "ave_precision_score": 0.7540192682162873,
            "fpr": 0.18859649122807018,
            "logloss": 3.006684116334346,
            "mae": 0.30561516297890773,
            "precision": 0.6966490299823633,
            "recall": 0.79
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7819189219683567,
            "auditor_fn_violation": 0.018996890670560988,
            "auditor_fp_violation": 0.033785942300163095,
            "ave_precision_score": 0.7610550043677496,
            "fpr": 0.1942919868276619,
            "logloss": 2.5144134731073113,
            "mae": 0.2795690924997018,
            "precision": 0.6861702127659575,
            "recall": 0.8524229074889867
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7800354699488737,
            "auditor_fn_violation": 0.006447368421052634,
            "auditor_fp_violation": 0.02238758303525805,
            "ave_precision_score": 0.727397060014964,
            "fpr": 0.20394736842105263,
            "logloss": 3.4791414925482873,
            "mae": 0.3035327126768256,
            "precision": 0.6945812807881774,
            "recall": 0.846
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7816170243455315,
            "auditor_fn_violation": 0.008194026025522615,
            "auditor_fp_violation": 0.01641978540906547,
            "ave_precision_score": 0.7264684127974025,
            "fpr": 0.20636663007683864,
            "logloss": 3.365514285753815,
            "mae": 0.28811266088252907,
            "precision": 0.6797274275979557,
            "recall": 0.8788546255506607
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7735392705387454,
            "auditor_fn_violation": 0.008991228070175439,
            "auditor_fp_violation": 0.008984840742633303,
            "ave_precision_score": 0.7177089087550179,
            "fpr": 0.2532894736842105,
            "logloss": 3.6920552885857045,
            "mae": 0.3295511880296127,
            "precision": 0.6607929515418502,
            "recall": 0.9
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7766281047371515,
            "auditor_fn_violation": 0.006518469803720557,
            "auditor_fp_violation": 0.0015973021206887985,
            "ave_precision_score": 0.7201612872771737,
            "fpr": 0.25686059275521406,
            "logloss": 3.455928392770236,
            "mae": 0.3245370884181185,
            "precision": 0.6383307573415765,
            "recall": 0.9096916299559471
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.751906902746591,
            "auditor_fn_violation": 0.005394736842105263,
            "auditor_fp_violation": 0.020516628342701425,
            "ave_precision_score": 0.6886038876402294,
            "fpr": 0.20065789473684212,
            "logloss": 4.250528692459858,
            "mae": 0.3188583655019398,
            "precision": 0.6871794871794872,
            "recall": 0.804
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7199647631693044,
            "auditor_fn_violation": 0.006856966010145213,
            "auditor_fp_violation": 0.026347078138098192,
            "ave_precision_score": 0.6482106785030817,
            "fpr": 0.2074643249176729,
            "logloss": 4.5988581369688815,
            "mae": 0.3042655995182896,
            "precision": 0.6637010676156584,
            "recall": 0.8215859030837004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7574028288793196,
            "auditor_fn_violation": 0.010182017543859657,
            "auditor_fp_violation": 0.027835441151422245,
            "ave_precision_score": 0.7195576367207196,
            "fpr": 0.1875,
            "logloss": 3.1347190117259744,
            "mae": 0.30828427051463236,
            "precision": 0.7,
            "recall": 0.798
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.7528716444321357,
            "auditor_fn_violation": 0.006634525645923299,
            "auditor_fp_violation": 0.023020366202528297,
            "ave_precision_score": 0.7167589149566209,
            "fpr": 0.17672886937431395,
            "logloss": 2.7684929067321438,
            "mae": 0.2783162850827808,
            "precision": 0.7012987012987013,
            "recall": 0.8325991189427313
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 16695,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7595775800618105,
            "auditor_fn_violation": 0.011885964912280703,
            "auditor_fp_violation": 0.02963187702265373,
            "ave_precision_score": 0.7014972843036453,
            "fpr": 0.15899122807017543,
            "logloss": 4.051367849435387,
            "mae": 0.3255729643756275,
            "precision": 0.7094188376753507,
            "recall": 0.708
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7496538827608064,
            "auditor_fn_violation": 0.006296029439498642,
            "auditor_fp_violation": 0.03198927765914773,
            "ave_precision_score": 0.6863889307155902,
            "fpr": 0.16136114160263446,
            "logloss": 3.958381488365928,
            "mae": 0.2977104163724476,
            "precision": 0.6981519507186859,
            "recall": 0.748898678414097
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5767543859649122,
            "auc_prc": 0.6890422773786968,
            "auditor_fn_violation": 0.05745833333333335,
            "auditor_fp_violation": 0.07503779168795775,
            "ave_precision_score": 0.6057679623979628,
            "fpr": 0.18969298245614036,
            "logloss": 9.049199052243138,
            "mae": 0.42504847666125195,
            "precision": 0.6239130434782608,
            "recall": 0.574
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.6639230064879718,
            "auditor_fn_violation": 0.05424885273964323,
            "auditor_fp_violation": 0.08188275081846721,
            "ave_precision_score": 0.574343187547882,
            "fpr": 0.18990120746432493,
            "logloss": 8.84565535885097,
            "mae": 0.4015096099275062,
            "precision": 0.6085972850678733,
            "recall": 0.5925110132158591
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.786268227057338,
            "auditor_fn_violation": 0.020903508771929823,
            "auditor_fp_violation": 0.02219064043604156,
            "ave_precision_score": 0.7406920731308089,
            "fpr": 0.16557017543859648,
            "logloss": 3.4494037452282833,
            "mae": 0.3016050916288756,
            "precision": 0.713472485768501,
            "recall": 0.752
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.7982751157600645,
            "auditor_fn_violation": 0.013791302581758924,
            "auditor_fp_violation": 0.024766589723942967,
            "ave_precision_score": 0.7503188462922373,
            "fpr": 0.15587266739846323,
            "logloss": 3.0779315895096113,
            "mae": 0.26161376129965075,
            "precision": 0.7171314741035857,
            "recall": 0.7929515418502202
        }
    }
]
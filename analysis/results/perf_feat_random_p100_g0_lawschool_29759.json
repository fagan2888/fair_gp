[
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(0)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8285778488156916,
            "auditor_fn_violation": 0.018168741746840224,
            "auditor_fp_violation": 0.022508732681816402,
            "ave_precision_score": 0.8288456032944664,
            "fpr": 0.1425438596491228,
            "logloss": 0.802717097140069,
            "mae": 0.27398684962845427,
            "precision": 0.7373737373737373,
            "recall": 0.7849462365591398
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8434814714594137,
            "auditor_fn_violation": 0.015046724985914039,
            "auditor_fp_violation": 0.021717190109301274,
            "ave_precision_score": 0.8439836976330635,
            "fpr": 0.11964873765093303,
            "logloss": 0.7426491488935815,
            "mae": 0.26220611760943496,
            "precision": 0.7766393442622951,
            "recall": 0.7750511247443763
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(1)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5964912280701754,
            "auc_prc": 0.6774599961019243,
            "auditor_fn_violation": 0.06932654216185626,
            "auditor_fp_violation": 0.061248969739785705,
            "ave_precision_score": 0.6303718465273461,
            "fpr": 0.21820175438596492,
            "logloss": 0.6715904855249266,
            "mae": 0.4610256720048359,
            "precision": 0.597979797979798,
            "recall": 0.6365591397849463
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.6852335668688017,
            "auditor_fn_violation": 0.06611310521932572,
            "auditor_fp_violation": 0.06472757919270007,
            "ave_precision_score": 0.64610931192126,
            "fpr": 0.2030735455543359,
            "logloss": 0.6648287139059683,
            "mae": 0.45902471961586455,
            "precision": 0.6372549019607843,
            "recall": 0.6646216768916156
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(2)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.8193227173064457,
            "auditor_fn_violation": 0.022580645161290328,
            "auditor_fp_violation": 0.050291416460614635,
            "ave_precision_score": 0.8196687037139749,
            "fpr": 0.23026315789473684,
            "logloss": 0.6262959676478267,
            "mae": 0.35547182240246816,
            "precision": 0.6568627450980392,
            "recall": 0.864516129032258
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8427076176885209,
            "auditor_fn_violation": 0.03272208117554363,
            "auditor_fp_violation": 0.028038039548228345,
            "ave_precision_score": 0.8430389528465414,
            "fpr": 0.18990120746432493,
            "logloss": 0.5364604304461328,
            "mae": 0.33878734896754875,
            "precision": 0.709731543624161,
            "recall": 0.8650306748466258
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(3)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.7617066380152794,
            "auditor_fn_violation": 0.016779852857951333,
            "auditor_fp_violation": 0.022920836767534057,
            "ave_precision_score": 0.7050854895428091,
            "fpr": 0.17982456140350878,
            "logloss": 5.127442563606654,
            "mae": 0.32026972661336683,
            "precision": 0.6739562624254473,
            "recall": 0.7290322580645161
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.7730917344286006,
            "auditor_fn_violation": 0.012781747287750937,
            "auditor_fp_violation": 0.023121823317951734,
            "ave_precision_score": 0.7202568081465663,
            "fpr": 0.15916575192096596,
            "logloss": 4.853991315959986,
            "mae": 0.3238088234029055,
            "precision": 0.7058823529411765,
            "recall": 0.7116564417177914
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(4)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6381578947368421,
            "auc_prc": 0.749691129440564,
            "auditor_fn_violation": 0.016871816638370125,
            "auditor_fp_violation": 0.018397503826680796,
            "ave_precision_score": 0.600707451493905,
            "fpr": 0.23026315789473684,
            "logloss": 0.6791264106353231,
            "mae": 0.43312443619626656,
            "precision": 0.6216216216216216,
            "recall": 0.7419354838709677
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7926851189918407,
            "auditor_fn_violation": 0.012622368282231036,
            "auditor_fp_violation": 0.018036530868115356,
            "ave_precision_score": 0.6560707979853778,
            "fpr": 0.19978046103183314,
            "logloss": 0.6347944525218667,
            "mae": 0.41448037323064524,
            "precision": 0.6795774647887324,
            "recall": 0.7893660531697342
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(5)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7980051098631868,
            "auditor_fn_violation": 0.008323901150726279,
            "auditor_fp_violation": 0.0062159032929078855,
            "ave_precision_score": 0.7993723857008049,
            "fpr": 0.06030701754385965,
            "logloss": 1.0493377833256101,
            "mae": 0.3501564269670488,
            "precision": 0.828125,
            "recall": 0.5698924731182796
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.8295583751814657,
            "auditor_fn_violation": 0.014362068694596155,
            "auditor_fp_violation": 0.005774603191118558,
            "ave_precision_score": 0.8299494722163767,
            "fpr": 0.04939626783754116,
            "logloss": 1.0170952668103432,
            "mae": 0.35269124126766327,
            "precision": 0.860248447204969,
            "recall": 0.5664621676891616
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(6)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.7882085232779342,
            "auditor_fn_violation": 0.006095548009809471,
            "auditor_fp_violation": 0.009861062051100915,
            "ave_precision_score": 0.7887598820219376,
            "fpr": 0.35855263157894735,
            "logloss": 0.7899502491176709,
            "mae": 0.3887418087351283,
            "precision": 0.5764248704663213,
            "recall": 0.956989247311828
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.7743018920424121,
            "auditor_fn_violation": 0.00432343612156802,
            "auditor_fp_violation": 0.013125516983055962,
            "ave_precision_score": 0.7747786732246034,
            "fpr": 0.32821075740944017,
            "logloss": 0.7629775615632225,
            "mae": 0.3772440832757425,
            "precision": 0.6086387434554974,
            "recall": 0.950920245398773
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(7)",
        "seed": 29759,
        "test": {
            "accuracy": 0.518640350877193,
            "auc_prc": 0.7566791852225514,
            "auditor_fn_violation": 0.0006437464629315224,
            "auditor_fp_violation": 0.0034734487224773397,
            "ave_precision_score": 0.5144026448174653,
            "fpr": 0.48026315789473684,
            "logloss": 16.625957303772168,
            "mae": 0.48135964912280704,
            "precision": 0.5144124168514412,
            "recall": 0.9978494623655914
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.769273127753304,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0015294894938638378,
            "ave_precision_score": 0.538546255506608,
            "fpr": 0.45993413830954993,
            "logloss": 15.885930121614521,
            "mae": 0.45993413830954993,
            "precision": 0.538546255506608,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(8)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7911617623146869,
            "auditor_fn_violation": 0.01651810979060555,
            "auditor_fp_violation": 0.011509478393971507,
            "ave_precision_score": 0.7624440028613554,
            "fpr": 0.07785087719298246,
            "logloss": 0.573525018190779,
            "mae": 0.3687535458603841,
            "precision": 0.7899408284023669,
            "recall": 0.5741935483870968
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.8269011741792547,
            "auditor_fn_violation": 0.01599850946958219,
            "auditor_fp_violation": 0.0027182253759995,
            "ave_precision_score": 0.8012990367903441,
            "fpr": 0.05817782656421515,
            "logloss": 0.5471525717078749,
            "mae": 0.35815885896118754,
            "precision": 0.8379204892966361,
            "recall": 0.5603271983640081
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(9)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5098684210526315,
            "auc_prc": 0.49313711684858413,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.4945419524942205,
            "fpr": 0.4901315789473684,
            "logloss": 0.6942598649013888,
            "mae": 0.4986235586305459,
            "precision": 0.5098684210526315,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5367727771679474,
            "auc_prc": 0.49591749696270354,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.4976916617020361,
            "fpr": 0.4632272228320527,
            "logloss": 0.6898184898050274,
            "mae": 0.49640769158969467,
            "precision": 0.5367727771679474,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(10)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.7977713385075538,
            "auditor_fn_violation": 0.010292869269949073,
            "auditor_fp_violation": 0.0129641076965344,
            "ave_precision_score": 0.7770398026499639,
            "fpr": 0.09978070175438597,
            "logloss": 0.5448509683916989,
            "mae": 0.36446964169705387,
            "precision": 0.7863849765258216,
            "recall": 0.7204301075268817
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.7953610278155088,
            "auditor_fn_violation": 0.0005252772857979926,
            "auditor_fp_violation": 0.002455506942529698,
            "ave_precision_score": 0.7720095141431049,
            "fpr": 0.07903402854006586,
            "logloss": 0.5571651619179219,
            "mae": 0.36404606717118315,
            "precision": 0.824390243902439,
            "recall": 0.6912065439672802
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(11)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.6523589093337627,
            "auditor_fn_violation": 0.0013511601584606742,
            "auditor_fp_violation": 0.0163934024098277,
            "ave_precision_score": 0.6481010227716822,
            "fpr": 0.10307017543859649,
            "logloss": 6.932943670520303,
            "mae": 0.46556628772540276,
            "precision": 0.6412213740458015,
            "recall": 0.36129032258064514
        },
        "train": {
            "accuracy": 0.5609220636663008,
            "auc_prc": 0.6935233959038885,
            "auditor_fn_violation": 0.010440447248916329,
            "auditor_fp_violation": 0.016673516421202683,
            "ave_precision_score": 0.6901471734858683,
            "fpr": 0.09220636663007684,
            "logloss": 7.019896167112536,
            "mae": 0.46820072941653534,
            "precision": 0.6731517509727627,
            "recall": 0.3537832310838446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(12)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5910087719298246,
            "auc_prc": 0.6585507017672532,
            "auditor_fn_violation": 0.009359083191850604,
            "auditor_fp_violation": 0.010704894226618005,
            "ave_precision_score": 0.6635559558393446,
            "fpr": 0.09978070175438597,
            "logloss": 7.2407236515723925,
            "mae": 0.41137897692805225,
            "precision": 0.6678832116788321,
            "recall": 0.3935483870967742
        },
        "train": {
            "accuracy": 0.6026344676180022,
            "auc_prc": 0.693680463680913,
            "auditor_fn_violation": 0.009623349248786158,
            "auditor_fp_violation": 0.016683921111637127,
            "ave_precision_score": 0.6975747676872428,
            "fpr": 0.07135016465422613,
            "logloss": 7.253849322895709,
            "mae": 0.4073101631097168,
            "precision": 0.7470817120622568,
            "recall": 0.39263803680981596
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(13)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5098684210526315,
            "auc_prc": 0.6077142988723161,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5191739971817397,
            "fpr": 0.4901315789473684,
            "logloss": 0.6929000448072217,
            "mae": 0.49906966756833226,
            "precision": 0.5098684210526315,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5367727771679474,
            "auc_prc": 0.6293596584582497,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5443054373729938,
            "fpr": 0.4632272228320527,
            "logloss": 0.6902169221653982,
            "mae": 0.49771073588162956,
            "precision": 0.5367727771679474,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(14)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8354373287247403,
            "auditor_fn_violation": 0.005086304470854557,
            "auditor_fp_violation": 0.009188939911299503,
            "ave_precision_score": 0.8357863024712304,
            "fpr": 0.13267543859649122,
            "logloss": 0.5147528791137076,
            "mae": 0.3382553947505361,
            "precision": 0.7489626556016598,
            "recall": 0.7763440860215054
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8714005042984037,
            "auditor_fn_violation": 0.010011695276320547,
            "auditor_fp_violation": 0.0037378850385753897,
            "ave_precision_score": 0.8717350701530561,
            "fpr": 0.10428100987925357,
            "logloss": 0.47361969642882346,
            "mae": 0.3219092958801018,
            "precision": 0.8020833333333334,
            "recall": 0.787321063394683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(15)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5679824561403509,
            "auc_prc": 0.6550071667861658,
            "auditor_fn_violation": 0.003426240332012829,
            "auditor_fp_violation": 0.01699929353585307,
            "ave_precision_score": 0.6527410116154233,
            "fpr": 0.40131578947368424,
            "logloss": 2.412768794536313,
            "mae": 0.4094128285028299,
            "precision": 0.5442092154420921,
            "recall": 0.9397849462365592
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.6508487885167688,
            "auditor_fn_violation": 0.00507768042938051,
            "auditor_fp_violation": 0.012631294187419692,
            "ave_precision_score": 0.6508609166617314,
            "fpr": 0.38419319429198684,
            "logloss": 2.463734695405938,
            "mae": 0.3959785245954756,
            "precision": 0.5689655172413793,
            "recall": 0.9447852760736196
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(16)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6348684210526315,
            "auc_prc": 0.667595765532919,
            "auditor_fn_violation": 0.01732220335785702,
            "auditor_fp_violation": 0.01892244593586876,
            "ave_precision_score": 0.6686907813635057,
            "fpr": 0.13048245614035087,
            "logloss": 0.7190520401605218,
            "mae": 0.429531260429566,
            "precision": 0.6783783783783783,
            "recall": 0.5397849462365591
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.7146053976631207,
            "auditor_fn_violation": 0.006819625616471264,
            "auditor_fp_violation": 0.01792468044594503,
            "ave_precision_score": 0.7157954557988397,
            "fpr": 0.12623490669593854,
            "logloss": 0.6924099816409236,
            "mae": 0.4268968126035312,
            "precision": 0.6933333333333334,
            "recall": 0.5316973415132924
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(17)",
        "seed": 29759,
        "test": {
            "accuracy": 0.4901315789473684,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.61023138556301,
            "mae": 0.5098684210526315,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.53947492547895,
            "mae": 0.5367727771679474,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(18)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5131578947368421,
            "auc_prc": 0.7603704947691583,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0015748263275638882,
            "ave_precision_score": 0.6874268963758469,
            "fpr": 0.4868421052631579,
            "logloss": 5.258096558353615,
            "mae": 0.47463825707765,
            "precision": 0.5115511551155115,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5378704720087816,
            "auc_prc": 0.7787545959636625,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.000629483771284097,
            "ave_precision_score": 0.7135411886300409,
            "fpr": 0.4621295279912184,
            "logloss": 4.817816083222605,
            "mae": 0.4487645507971097,
            "precision": 0.5373626373626373,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(19)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7080984353239481,
            "auditor_fn_violation": 0.005034427466515751,
            "auditor_fp_violation": 0.014556105027669847,
            "ave_precision_score": 0.7208152307504277,
            "fpr": 0.22039473684210525,
            "logloss": 0.593735168721475,
            "mae": 0.367877646009014,
            "precision": 0.6737012987012987,
            "recall": 0.8924731182795699
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.7625934877967478,
            "auditor_fn_violation": 0.006851052462630115,
            "auditor_fp_violation": 0.003886151877266277,
            "ave_precision_score": 0.7749539384967169,
            "fpr": 0.18221734357848518,
            "logloss": 0.5535897336093654,
            "mae": 0.35550849295969794,
            "precision": 0.7228714524207012,
            "recall": 0.885480572597137
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(20)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.7869716905901116,
            "auditor_fn_violation": 0.007675438596491229,
            "auditor_fp_violation": 0.00867380980415244,
            "ave_precision_score": 0.6907190538663374,
            "fpr": 0.0537280701754386,
            "logloss": 0.5936503266761863,
            "mae": 0.4182465372509078,
            "precision": 0.8157894736842105,
            "recall": 0.4666666666666667
        },
        "train": {
            "accuracy": 0.6509330406147091,
            "auc_prc": 0.7888783521675143,
            "auditor_fn_violation": 0.0063706706713447915,
            "auditor_fp_violation": 0.007751494373663648,
            "ave_precision_score": 0.7017690235680091,
            "fpr": 0.05598243688254665,
            "logloss": 0.6007761723621393,
            "mae": 0.42196361241304,
            "precision": 0.8131868131868132,
            "recall": 0.4539877300613497
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(21)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8294491140554932,
            "auditor_fn_violation": 0.015365025466893048,
            "auditor_fp_violation": 0.012318968562345463,
            "ave_precision_score": 0.7326473122232892,
            "fpr": 0.06578947368421052,
            "logloss": 0.5857972874227402,
            "mae": 0.36516924978544313,
            "precision": 0.8360655737704918,
            "recall": 0.6580645161290323
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8309859712417141,
            "auditor_fn_violation": 0.01808839473914596,
            "auditor_fp_violation": 0.004606676689851787,
            "ave_precision_score": 0.7376460166805288,
            "fpr": 0.05378704720087816,
            "logloss": 0.6262707400650998,
            "mae": 0.38017901243349594,
            "precision": 0.8607954545454546,
            "recall": 0.6196319018404908
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(22)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8326130136350737,
            "auditor_fn_violation": 0.009960384833050371,
            "auditor_fp_violation": 0.017860296714941715,
            "ave_precision_score": 0.8328481386744219,
            "fpr": 0.13486842105263158,
            "logloss": 0.5158018658762081,
            "mae": 0.3322309126705748,
            "precision": 0.7442827442827443,
            "recall": 0.7698924731182796
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8589043032671557,
            "auditor_fn_violation": 0.002729646066369013,
            "auditor_fp_violation": 0.0024815186686158158,
            "ave_precision_score": 0.8592044923096527,
            "fpr": 0.10098792535675083,
            "logloss": 0.4946167218879765,
            "mae": 0.32101448956553946,
            "precision": 0.8021505376344086,
            "recall": 0.7627811860940695
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(23)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5098684210526315,
            "auc_prc": 0.5357314657611771,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5108177702320317,
            "fpr": 0.4901315789473684,
            "logloss": 0.7145044446886781,
            "mae": 0.49849802608552735,
            "precision": 0.5098684210526315,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5367727771679474,
            "auc_prc": 0.6519120429919256,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.544621631996121,
            "fpr": 0.4632272228320527,
            "logloss": 0.6974744246645932,
            "mae": 0.49216808439217075,
            "precision": 0.5367727771679474,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(24)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7823791238777629,
            "auditor_fn_violation": 0.0003466327108092874,
            "auditor_fp_violation": 0.007719592605675257,
            "ave_precision_score": 0.7880348788643666,
            "fpr": 0.16776315789473684,
            "logloss": 0.5642592081456643,
            "mae": 0.3623949793338972,
            "precision": 0.7,
            "recall": 0.7677419354838709
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8250609391656945,
            "auditor_fn_violation": 0.007414490918763851,
            "auditor_fp_violation": 0.009166532272748563,
            "ave_precision_score": 0.8267543529307493,
            "fpr": 0.1207464324917673,
            "logloss": 0.5189646282733177,
            "mae": 0.34188349961285797,
            "precision": 0.7773279352226721,
            "recall": 0.7852760736196319
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(25)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8417972519228536,
            "auditor_fn_violation": 0.012002452367477838,
            "auditor_fp_violation": 0.011565897405706665,
            "ave_precision_score": 0.7805682094608126,
            "fpr": 0.1074561403508772,
            "logloss": 0.5186548765551575,
            "mae": 0.34364124544357,
            "precision": 0.7782805429864253,
            "recall": 0.7397849462365591
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8676318427303424,
            "auditor_fn_violation": 0.004013657209430748,
            "auditor_fp_violation": 0.005457260132867898,
            "ave_precision_score": 0.8161833817119568,
            "fpr": 0.07683863885839737,
            "logloss": 0.5032482636882303,
            "mae": 0.33770427843872725,
            "precision": 0.833729216152019,
            "recall": 0.7177914110429447
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(26)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.7684026834242762,
            "auditor_fn_violation": 0.003331918505942279,
            "auditor_fp_violation": 0.012083480513363947,
            "ave_precision_score": 0.7687793438608272,
            "fpr": 0.24342105263157895,
            "logloss": 0.6322025765544765,
            "mae": 0.41041905148007607,
            "precision": 0.642512077294686,
            "recall": 0.8580645161290322
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7850981859072372,
            "auditor_fn_violation": 0.005178695292033971,
            "auditor_fp_violation": 0.017625545595954657,
            "ave_precision_score": 0.7858278459501846,
            "fpr": 0.20636663007683864,
            "logloss": 0.5765073167077306,
            "mae": 0.38693302622053194,
            "precision": 0.6877076411960132,
            "recall": 0.8466257668711656
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(27)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7357500367205959,
            "auditor_fn_violation": 0.018635634785889452,
            "auditor_fp_violation": 0.04191687271870954,
            "ave_precision_score": 0.7367819509035097,
            "fpr": 0.23684210526315788,
            "logloss": 1.6629010154640664,
            "mae": 0.3246093612959748,
            "precision": 0.6423841059602649,
            "recall": 0.8344086021505376
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7910851690079228,
            "auditor_fn_violation": 0.02487659350945836,
            "auditor_fp_violation": 0.03270194203546958,
            "ave_precision_score": 0.7914803178771808,
            "fpr": 0.20856201975850713,
            "logloss": 1.6330811768398454,
            "mae": 0.2943186189168674,
            "precision": 0.6905537459283387,
            "recall": 0.8670756646216768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(28)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8097229831703194,
            "auditor_fn_violation": 0.012379739671760048,
            "auditor_fp_violation": 0.0024726245143059018,
            "ave_precision_score": 0.8041020882805106,
            "fpr": 0.07236842105263158,
            "logloss": 0.5557174168782903,
            "mae": 0.3237834751740978,
            "precision": 0.8156424581005587,
            "recall": 0.6279569892473118
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8298992939661436,
            "auditor_fn_violation": 0.010079038518089528,
            "auditor_fp_violation": 0.010357869327492834,
            "ave_precision_score": 0.8277363750516094,
            "fpr": 0.050493962678375415,
            "logloss": 0.5251257804864493,
            "mae": 0.3157572652308361,
            "precision": 0.8651026392961877,
            "recall": 0.6032719836400818
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(29)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6546052631578947,
            "auc_prc": 0.7382148219475118,
            "auditor_fn_violation": 0.013879456706281837,
            "auditor_fp_violation": 0.01704344754503709,
            "ave_precision_score": 0.7386845233096907,
            "fpr": 0.10087719298245613,
            "logloss": 1.001953473509827,
            "mae": 0.37622985412121396,
            "precision": 0.7245508982035929,
            "recall": 0.5204301075268817
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.793399104454127,
            "auditor_fn_violation": 0.006521070577962156,
            "auditor_fp_violation": 0.015716284901233477,
            "ave_precision_score": 0.7937686914487236,
            "fpr": 0.07574094401756312,
            "logloss": 0.9231724207372062,
            "mae": 0.35837568898994826,
            "precision": 0.7976539589442815,
            "recall": 0.556237218813906
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(30)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5131578947368421,
            "auc_prc": 0.7537211558526808,
            "auditor_fn_violation": 0.0019524617996604415,
            "auditor_fp_violation": 0.002973036618391634,
            "ave_precision_score": 0.5116507502883987,
            "fpr": 0.4824561403508772,
            "logloss": 0.6984524072367245,
            "mae": 0.4979506323579699,
            "precision": 0.51165371809101,
            "recall": 0.9913978494623656
        },
        "train": {
            "accuracy": 0.5466520307354555,
            "auc_prc": 0.7706423246107384,
            "auditor_fn_violation": 0.0006352712473539718,
            "auditor_fp_violation": 0.0003381524391195614,
            "ave_precision_score": 0.5422220307720832,
            "fpr": 0.4522502744237102,
            "logloss": 0.6854224050265498,
            "mae": 0.4934532692825624,
            "precision": 0.5422222222222223,
            "recall": 0.9979550102249489
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(31)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8111926842227407,
            "auditor_fn_violation": 0.008064516129032258,
            "auditor_fp_violation": 0.010447329173044466,
            "ave_precision_score": 0.670540392813084,
            "fpr": 0.18201754385964913,
            "logloss": 0.6024466285887263,
            "mae": 0.37925442334329873,
            "precision": 0.7014388489208633,
            "recall": 0.8387096774193549
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8394291239391682,
            "auditor_fn_violation": 0.007010431468150015,
            "auditor_fp_violation": 0.012529848455683827,
            "ave_precision_score": 0.7182973363971795,
            "fpr": 0.14818880351262348,
            "logloss": 0.557518841876918,
            "mae": 0.36071436380319355,
            "precision": 0.7527472527472527,
            "recall": 0.8404907975460123
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(32)",
        "seed": 29759,
        "test": {
            "accuracy": 0.48793859649122806,
            "auc_prc": 0.30441284663271073,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0008438321755170926,
            "ave_precision_score": 0.5090407470288624,
            "fpr": 0.0021929824561403508,
            "logloss": 17.579495028021714,
            "mae": 0.5136997251436441,
            "precision": 0.0,
            "recall": 0.0
        },
        "train": {
            "accuracy": 0.4621295279912184,
            "auc_prc": 0.2683863885839737,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0005410439025912882,
            "ave_precision_score": 0.5367727771679474,
            "fpr": 0.0010976948408342481,
            "logloss": 18.54138952662061,
            "mae": 0.5381865584169863,
            "precision": 0.0,
            "recall": 0.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(33)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6491228070175439,
            "auc_prc": 0.6590168416948782,
            "auditor_fn_violation": 0.030036785512167527,
            "auditor_fp_violation": 0.023340299854782372,
            "ave_precision_score": 0.6744190127200156,
            "fpr": 0.11074561403508772,
            "logloss": 0.626619247818523,
            "mae": 0.439394892209716,
            "precision": 0.7089337175792507,
            "recall": 0.5290322580645161
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6588930248717889,
            "auditor_fn_violation": 0.028715158290289786,
            "auditor_fp_violation": 0.019103011637646255,
            "ave_precision_score": 0.6621322577681097,
            "fpr": 0.1141602634467618,
            "logloss": 0.6478028296409376,
            "mae": 0.4493366825004833,
            "precision": 0.7086834733893558,
            "recall": 0.5173824130879345
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(34)",
        "seed": 29759,
        "test": {
            "accuracy": 0.4901315789473684,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.61023138556301,
            "mae": 0.5098684210526315,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.53947492547895,
            "mae": 0.5367727771679474,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(35)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.673535549342501,
            "auditor_fn_violation": 0.01993727598566309,
            "auditor_fp_violation": 0.035779465442128816,
            "ave_precision_score": 0.6749497345709805,
            "fpr": 0.20065789473684212,
            "logloss": 0.738367049365754,
            "mae": 0.3998938290464355,
            "precision": 0.6500956022944551,
            "recall": 0.7311827956989247
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7459507414510911,
            "auditor_fn_violation": 0.026715064009751308,
            "auditor_fp_violation": 0.03018400695033321,
            "ave_precision_score": 0.7471611966147221,
            "fpr": 0.1778265642151482,
            "logloss": 0.6193659469422451,
            "mae": 0.37255536609044615,
            "precision": 0.6937618147448015,
            "recall": 0.7505112474437627
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(36)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6030701754385965,
            "auc_prc": 0.6958952382091035,
            "auditor_fn_violation": 0.003779947179777403,
            "auditor_fp_violation": 0.0043712469092193566,
            "ave_precision_score": 0.6945223108187628,
            "fpr": 0.02631578947368421,
            "logloss": 6.165905622077283,
            "mae": 0.4145059539104726,
            "precision": 0.8410596026490066,
            "recall": 0.2731182795698925
        },
        "train": {
            "accuracy": 0.6037321624588364,
            "auc_prc": 0.7587855382813803,
            "auditor_fn_violation": 0.019318531288792518,
            "auditor_fp_violation": 0.005374022609392314,
            "ave_precision_score": 0.7561078525399876,
            "fpr": 0.01646542261251372,
            "logloss": 5.984445950019233,
            "mae": 0.4139055419528221,
            "precision": 0.9050632911392406,
            "recall": 0.29243353783231085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(37)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8253787785242126,
            "auditor_fn_violation": 0.0026315789473684236,
            "auditor_fp_violation": 0.009701617017936339,
            "ave_precision_score": 0.8256907396793398,
            "fpr": 0.10964912280701754,
            "logloss": 0.5538883940185267,
            "mae": 0.3133375151590276,
            "precision": 0.7732426303854876,
            "recall": 0.7333333333333333
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8389210602419142,
            "auditor_fn_violation": 0.004287519725957906,
            "auditor_fp_violation": 0.009176936963183002,
            "ave_precision_score": 0.8395087928894573,
            "fpr": 0.09001097694840834,
            "logloss": 0.5150324268131362,
            "mae": 0.2964947683952389,
            "precision": 0.8161434977578476,
            "recall": 0.7443762781186094
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(38)",
        "seed": 29759,
        "test": {
            "accuracy": 0.518640350877193,
            "auc_prc": 0.837746388924139,
            "auditor_fn_violation": 0.0006437464629315224,
            "auditor_fp_violation": 0.0034734487224773397,
            "ave_precision_score": 0.8380530670188544,
            "fpr": 0.48026315789473684,
            "logloss": 1.7059588556158822,
            "mae": 0.4597636537963441,
            "precision": 0.5144124168514412,
            "recall": 0.9978494623655914
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.8739648747750542,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0015294894938638378,
            "ave_precision_score": 0.8751473528887801,
            "fpr": 0.45993413830954993,
            "logloss": 1.527902243189994,
            "mae": 0.435826936235784,
            "precision": 0.538546255506608,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(39)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5383771929824561,
            "auc_prc": 0.7006767750081453,
            "auditor_fn_violation": 0.00379173740803622,
            "auditor_fp_violation": 0.011779308450096174,
            "ave_precision_score": 0.6958033522658319,
            "fpr": 0.4550438596491228,
            "logloss": 1.7982419462832526,
            "mae": 0.41697466784369264,
            "precision": 0.5251716247139588,
            "recall": 0.9870967741935484
        },
        "train": {
            "accuracy": 0.5609220636663008,
            "auc_prc": 0.770702809869337,
            "auditor_fn_violation": 0.0005477250330543079,
            "auditor_fp_violation": 0.008393984007990823,
            "ave_precision_score": 0.7624901649287283,
            "fpr": 0.43798024149286496,
            "logloss": 1.5135833732834911,
            "mae": 0.3909867935676974,
            "precision": 0.5501691093573844,
            "recall": 0.9979550102249489
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(40)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.7124320226789821,
            "auditor_fn_violation": 0.011214865119788722,
            "auditor_fp_violation": 0.009976353075081441,
            "ave_precision_score": 0.6877788968024695,
            "fpr": 0.11074561403508772,
            "logloss": 0.6513260495945099,
            "mae": 0.3980520601800986,
            "precision": 0.7645687645687645,
            "recall": 0.7053763440860215
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.7828095283289684,
            "auditor_fn_violation": 0.000309778912137276,
            "auditor_fp_violation": 0.008526643811030013,
            "ave_precision_score": 0.7576555321350698,
            "fpr": 0.07464324917672886,
            "logloss": 0.6188973785385098,
            "mae": 0.38325578121531806,
            "precision": 0.8349514563106796,
            "recall": 0.7034764826175869
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(41)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7495016208543113,
            "auditor_fn_violation": 0.007866440294284101,
            "auditor_fp_violation": 0.005600200164841637,
            "ave_precision_score": 0.7501481487495955,
            "fpr": 0.15350877192982457,
            "logloss": 0.6104571815775927,
            "mae": 0.38552239969597696,
            "precision": 0.7014925373134329,
            "recall": 0.7075268817204301
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7775569409477772,
            "auditor_fn_violation": 0.004505262874344244,
            "auditor_fp_violation": 0.006552353801093541,
            "ave_precision_score": 0.7787932828680896,
            "fpr": 0.132821075740944,
            "logloss": 0.5928176285884816,
            "mae": 0.3796749886366335,
            "precision": 0.7468619246861925,
            "recall": 0.7300613496932515
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(42)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5230263157894737,
            "auc_prc": 0.7574187224338357,
            "auditor_fn_violation": 0.0017331635540464064,
            "auditor_fp_violation": 0.005372071117390786,
            "ave_precision_score": 0.5409105846721679,
            "fpr": 0.47149122807017546,
            "logloss": 14.6148178457457,
            "mae": 0.4783285979361257,
            "precision": 0.5168539325842697,
            "recall": 0.989247311827957
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.7681360821232153,
            "auditor_fn_violation": 0.0033604277642717167,
            "auditor_fp_violation": 0.003797712008573468,
            "ave_precision_score": 0.5617818379897251,
            "fpr": 0.4500548847420417,
            "logloss": 14.064243311605908,
            "mae": 0.46013394064391616,
            "precision": 0.5393258426966292,
            "recall": 0.9815950920245399
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(43)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.6438082268905975,
            "auditor_fn_violation": 0.017475476325221652,
            "auditor_fp_violation": 0.014519310020016489,
            "ave_precision_score": 0.6398146166773335,
            "fpr": 0.19407894736842105,
            "logloss": 0.647170775625493,
            "mae": 0.4351788083655073,
            "precision": 0.6431451612903226,
            "recall": 0.6860215053763441
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.721865822120082,
            "auditor_fn_violation": 0.01090287084239662,
            "auditor_fp_violation": 0.016964847753367224,
            "ave_precision_score": 0.7056066244954736,
            "fpr": 0.16245883644346873,
            "logloss": 0.602936344491604,
            "mae": 0.4180150267256078,
            "precision": 0.7034068136272545,
            "recall": 0.7177914110429447
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(44)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7877577005745932,
            "auditor_fn_violation": 0.03982739105829089,
            "auditor_fp_violation": 0.05274196397032851,
            "ave_precision_score": 0.7860346113703169,
            "fpr": 0.16447368421052633,
            "logloss": 0.5782837870073317,
            "mae": 0.38133175850357237,
            "precision": 0.6894409937888198,
            "recall": 0.7161290322580646
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.7964761641218747,
            "auditor_fn_violation": 0.053113614783188434,
            "auditor_fp_violation": 0.04149390545257802,
            "ave_precision_score": 0.795402813444754,
            "fpr": 0.15477497255762898,
            "logloss": 0.5839660209509258,
            "mae": 0.3892111313754995,
            "precision": 0.7116564417177914,
            "recall": 0.7116564417177914
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(45)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.739042020817545,
            "auditor_fn_violation": 0.007368892661761943,
            "auditor_fp_violation": 0.012098198516425292,
            "ave_precision_score": 0.6388069831438523,
            "fpr": 0.08333333333333333,
            "logloss": 0.7812747824445766,
            "mae": 0.44342822937719656,
            "precision": 0.7540453074433657,
            "recall": 0.5010752688172043
        },
        "train": {
            "accuracy": 0.6333699231613611,
            "auc_prc": 0.7322343897303358,
            "auditor_fn_violation": 0.010350656259891048,
            "auditor_fp_violation": 0.015445762949937832,
            "ave_precision_score": 0.6443254725883628,
            "fpr": 0.10428100987925357,
            "logloss": 0.7537469783171268,
            "mae": 0.44499177389198546,
            "precision": 0.7246376811594203,
            "recall": 0.5112474437627812
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(46)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8469981391564128,
            "auditor_fn_violation": 0.013167326919449163,
            "auditor_fp_violation": 0.01662398445778877,
            "ave_precision_score": 0.8409555958884569,
            "fpr": 0.12171052631578948,
            "logloss": 0.5193077151516472,
            "mae": 0.3020429606835374,
            "precision": 0.7653276955602537,
            "recall": 0.7784946236559139
        },
        "train": {
            "accuracy": 0.7870472008781558,
            "auc_prc": 0.8727121700982301,
            "auditor_fn_violation": 0.01403433158465383,
            "auditor_fp_violation": 0.016215710042086974,
            "ave_precision_score": 0.8663638671827937,
            "fpr": 0.09220636663007684,
            "logloss": 0.48008546964414506,
            "mae": 0.28538475238179334,
            "precision": 0.8185745140388769,
            "recall": 0.7750511247443763
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(47)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.6269588010242545,
            "auditor_fn_violation": 0.017473118279569894,
            "auditor_fp_violation": 0.016972310530240592,
            "ave_precision_score": 0.6484818511531276,
            "fpr": 0.1524122807017544,
            "logloss": 0.7303577233742162,
            "mae": 0.38125607908678877,
            "precision": 0.6931567328918322,
            "recall": 0.6752688172043011
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7025121917898572,
            "auditor_fn_violation": 0.007154097050590491,
            "auditor_fp_violation": 0.02304638931230199,
            "ave_precision_score": 0.6736731264044913,
            "fpr": 0.14270032930845225,
            "logloss": 0.7230886462422207,
            "mae": 0.38075365450000137,
            "precision": 0.7161572052401747,
            "recall": 0.6707566462167689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(48)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.7943972281931432,
            "auditor_fn_violation": 0.03026551594038861,
            "auditor_fp_violation": 0.030834216413517017,
            "ave_precision_score": 0.7200759051705067,
            "fpr": 0.11403508771929824,
            "logloss": 0.5764068465833188,
            "mae": 0.36238673393075405,
            "precision": 0.7609195402298851,
            "recall": 0.7118279569892473
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8413232991877608,
            "auditor_fn_violation": 0.0280170333506181,
            "auditor_fp_violation": 0.022606791141446564,
            "ave_precision_score": 0.7743301785647781,
            "fpr": 0.0845225027442371,
            "logloss": 0.5121629224250565,
            "mae": 0.33916488340539547,
            "precision": 0.8221709006928406,
            "recall": 0.7280163599182005
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(49)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5603070175438597,
            "auc_prc": 0.7544445318254153,
            "auditor_fn_violation": 0.0020326353518204114,
            "auditor_fp_violation": 0.006718768397503835,
            "ave_precision_score": 0.535981526891912,
            "fpr": 0.4309210526315789,
            "logloss": 14.484462791837267,
            "mae": 0.44298488347754883,
            "precision": 0.5376470588235294,
            "recall": 0.9827956989247312
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.7710678227460093,
            "auditor_fn_violation": 0.001748679511267647,
            "auditor_fp_violation": 7.803517825833592e-06,
            "ave_precision_score": 0.5661535389190354,
            "fpr": 0.40504939626783754,
            "logloss": 13.52926580544072,
            "mae": 0.41399347947416654,
            "precision": 0.5663924794359577,
            "recall": 0.9856850715746421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(50)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5252192982456141,
            "auc_prc": 0.6513548285456408,
            "auditor_fn_violation": 0.002393416336540276,
            "auditor_fp_violation": 0.0074301385454688266,
            "ave_precision_score": 0.6475869199890337,
            "fpr": 0.4692982456140351,
            "logloss": 0.8391631010493071,
            "mae": 0.4664543720060273,
            "precision": 0.5180180180180181,
            "recall": 0.989247311827957
        },
        "train": {
            "accuracy": 0.5488474204171241,
            "auc_prc": 0.6640834681819823,
            "auditor_fn_violation": 0.0023682373355421917,
            "auditor_fp_violation": 0.00256995853730864,
            "ave_precision_score": 0.6646637344305387,
            "fpr": 0.4456641053787047,
            "logloss": 0.8166369378866274,
            "mae": 0.4553546936922879,
            "precision": 0.5438202247191011,
            "recall": 0.9897750511247444
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(51)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8000550844900668,
            "auditor_fn_violation": 0.004397755140539524,
            "auditor_fp_violation": 0.011538914400094198,
            "ave_precision_score": 0.6816694637029499,
            "fpr": 0.07675438596491228,
            "logloss": 0.6119644581375563,
            "mae": 0.40711756285868195,
            "precision": 0.7982708933717579,
            "recall": 0.5956989247311828
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.8176743460410031,
            "auditor_fn_violation": 0.005652342759142409,
            "auditor_fp_violation": 0.004011008162479649,
            "ave_precision_score": 0.7065825025796204,
            "fpr": 0.06147091108671789,
            "logloss": 0.5926925136603653,
            "mae": 0.4016986428448188,
            "precision": 0.8333333333333334,
            "recall": 0.5725971370143149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(52)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6513157894736842,
            "auc_prc": 0.7352000971884182,
            "auditor_fn_violation": 0.03128654970760234,
            "auditor_fp_violation": 0.006922367439852432,
            "ave_precision_score": 0.7128958613529717,
            "fpr": 0.13596491228070176,
            "logloss": 3.25695458455896,
            "mae": 0.3868415338761179,
            "precision": 0.6860759493670886,
            "recall": 0.5827956989247312
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7761484357323538,
            "auditor_fn_violation": 0.0464062279029988,
            "auditor_fp_violation": 0.0038445331155284846,
            "ave_precision_score": 0.7601544549121038,
            "fpr": 0.09659714599341383,
            "logloss": 3.240887912368094,
            "mae": 0.37766167525767663,
            "precision": 0.7640750670241286,
            "recall": 0.5828220858895705
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(53)",
        "seed": 29759,
        "test": {
            "accuracy": 0.48135964912280704,
            "auc_prc": 0.43193915267379546,
            "auditor_fn_violation": 0.0006437464629315351,
            "auditor_fp_violation": 0.003473448722477334,
            "ave_precision_score": 0.4324634804996982,
            "fpr": 0.009868421052631578,
            "logloss": 3.047030190834473,
            "mae": 0.5220833519761445,
            "precision": 0.1,
            "recall": 0.002150537634408602
        },
        "train": {
            "accuracy": 0.45993413830954993,
            "auc_prc": 0.4650900119767595,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.001529489493863834,
            "ave_precision_score": 0.4669530628375057,
            "fpr": 0.003293084522502744,
            "logloss": 2.862774704621676,
            "mae": 0.5420609874797315,
            "precision": 0.0,
            "recall": 0.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(54)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7708333333333334,
            "auc_prc": 0.8362555573149746,
            "auditor_fn_violation": 0.009889643463497461,
            "auditor_fp_violation": 0.015566741237882181,
            "ave_precision_score": 0.8295620037442021,
            "fpr": 0.1206140350877193,
            "logloss": 0.5130009666130549,
            "mae": 0.31507566277542265,
            "precision": 0.7689075630252101,
            "recall": 0.7870967741935484
        },
        "train": {
            "accuracy": 0.7925356750823271,
            "auc_prc": 0.8651387663647521,
            "auditor_fn_violation": 0.01119918110618009,
            "auditor_fp_violation": 0.012826382133065585,
            "ave_precision_score": 0.8582566152527558,
            "fpr": 0.09549945115257959,
            "logloss": 0.47049587824193045,
            "mae": 0.29397681257743735,
            "precision": 0.8164556962025317,
            "recall": 0.7914110429447853
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(55)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5603070175438597,
            "auc_prc": 0.7690086479303275,
            "auditor_fn_violation": 0.003324844368986984,
            "auditor_fp_violation": 0.014146453942462427,
            "ave_precision_score": 0.7695551699999295,
            "fpr": 0.4298245614035088,
            "logloss": 1.4504050868783986,
            "mae": 0.424348331563044,
            "precision": 0.5377358490566038,
            "recall": 0.9806451612903225
        },
        "train": {
            "accuracy": 0.5894621295279913,
            "auc_prc": 0.8354085612623174,
            "auditor_fn_violation": 0.0018115332035853542,
            "auditor_fp_violation": 0.011406141888763452,
            "ave_precision_score": 0.8357673743797225,
            "fpr": 0.407244785949506,
            "logloss": 1.215996888833386,
            "mae": 0.3889630951281764,
            "precision": 0.5670945157526255,
            "recall": 0.9938650306748467
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(56)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.7233516892276957,
            "auditor_fn_violation": 0.012002452367477838,
            "auditor_fp_violation": 0.011565897405706665,
            "ave_precision_score": 0.7202008669020681,
            "fpr": 0.1074561403508772,
            "logloss": 0.5721531925391464,
            "mae": 0.36185156972261895,
            "precision": 0.7782805429864253,
            "recall": 0.7397849462365591
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.791179945219013,
            "auditor_fn_violation": 0.004013657209430748,
            "auditor_fp_violation": 0.005457260132867898,
            "ave_precision_score": 0.7840824260135826,
            "fpr": 0.07683863885839737,
            "logloss": 0.5566871265082148,
            "mae": 0.3552438021687592,
            "precision": 0.833729216152019,
            "recall": 0.7177914110429447
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(57)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6096491228070176,
            "auc_prc": 0.6281685860273098,
            "auditor_fn_violation": 0.004909451046972277,
            "auditor_fp_violation": 0.016265846383296048,
            "ave_precision_score": 0.5864766646092665,
            "fpr": 0.1699561403508772,
            "logloss": 0.6770019322445134,
            "mae": 0.46710949116631556,
            "precision": 0.630071599045346,
            "recall": 0.567741935483871
        },
        "train": {
            "accuracy": 0.6465422612513722,
            "auc_prc": 0.668643996252538,
            "auditor_fn_violation": 0.013212744035072364,
            "auditor_fp_violation": 0.009340810837525557,
            "ave_precision_score": 0.6393538232555639,
            "fpr": 0.141602634467618,
            "logloss": 0.6570248437192588,
            "mae": 0.4573600460355028,
            "precision": 0.6964705882352941,
            "recall": 0.6053169734151329
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(58)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7742501300062574,
            "auditor_fn_violation": 0.034373231465761184,
            "auditor_fp_violation": 0.018238058793516226,
            "ave_precision_score": 0.7704354936990976,
            "fpr": 0.07017543859649122,
            "logloss": 3.033344781547502,
            "mae": 0.3705652240733909,
            "precision": 0.7955271565495208,
            "recall": 0.535483870967742
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.8071526909999335,
            "auditor_fn_violation": 0.05070497150258486,
            "auditor_fp_violation": 0.01367956674869031,
            "ave_precision_score": 0.804236221285865,
            "fpr": 0.06037321624588365,
            "logloss": 3.027841051482792,
            "mae": 0.3755995278483981,
            "precision": 0.8237179487179487,
            "recall": 0.5255623721881391
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(59)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7902410070841362,
            "auditor_fn_violation": 0.009578381437464636,
            "auditor_fp_violation": 0.027323972683386318,
            "ave_precision_score": 0.7853252170541407,
            "fpr": 0.12171052631578948,
            "logloss": 1.2198336412808377,
            "mae": 0.3592954733462098,
            "precision": 0.7471526195899773,
            "recall": 0.7053763440860215
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8206865524862705,
            "auditor_fn_violation": 0.008465045490359823,
            "auditor_fp_violation": 0.011169435181379767,
            "ave_precision_score": 0.8171208910144113,
            "fpr": 0.10208562019758508,
            "logloss": 1.3292662951212049,
            "mae": 0.3568025416871678,
            "precision": 0.7837209302325582,
            "recall": 0.689161554192229
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(60)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5098684210526315,
            "auc_prc": 0.7549342105263157,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5098684210526315,
            "fpr": 0.4901315789473684,
            "logloss": 0.6931985326871763,
            "mae": 0.49958640925194087,
            "precision": 0.5098684210526315,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5367727771679474,
            "auc_prc": 0.7683863885839737,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5367727771679474,
            "fpr": 0.4632272228320527,
            "logloss": 0.6909420593065425,
            "mae": 0.4984588334510669,
            "precision": 0.5367727771679474,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(61)",
        "seed": 29759,
        "test": {
            "accuracy": 0.4901315789473684,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.7531203376130855,
            "mae": 0.47290335772068876,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.7350776057117462,
            "mae": 0.47028881170747044,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(62)",
        "seed": 29759,
        "test": {
            "accuracy": 0.49890350877192985,
            "auc_prc": 0.7427085297363957,
            "auditor_fn_violation": 0.00573712507074137,
            "auditor_fp_violation": 0.001381039287256196,
            "ave_precision_score": 0.5046517246249907,
            "fpr": 0.48135964912280704,
            "logloss": 0.7044988147993902,
            "mae": 0.5021500784511629,
            "precision": 0.5045146726862303,
            "recall": 0.9612903225806452
        },
        "train": {
            "accuracy": 0.5411635565312843,
            "auc_prc": 0.7647972592597082,
            "auditor_fn_violation": 0.0025725118355747413,
            "auditor_fp_violation": 0.0015737094282102333,
            "ave_precision_score": 0.5400217254664202,
            "fpr": 0.446761800219539,
            "logloss": 0.6922472384931514,
            "mae": 0.49645492532643215,
            "precision": 0.5401129943502825,
            "recall": 0.9775051124744376
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(63)",
        "seed": 29759,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8409267167550083,
            "auditor_fn_violation": 0.009054895302773062,
            "auditor_fp_violation": 0.012252737548569414,
            "ave_precision_score": 0.7996721548511989,
            "fpr": 0.10635964912280702,
            "logloss": 0.5374181436162381,
            "mae": 0.3458725545359285,
            "precision": 0.7780320366132724,
            "recall": 0.7311827956989247
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8669554932745753,
            "auditor_fn_violation": 0.0071114463308034805,
            "auditor_fp_violation": 0.00792577293844065,
            "ave_precision_score": 0.8330546703886711,
            "fpr": 0.07793633369923161,
            "logloss": 0.5134836659025946,
            "mae": 0.3371324310678289,
            "precision": 0.8317535545023697,
            "recall": 0.7177914110429447
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(64)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.8237713827392793,
            "auditor_fn_violation": 0.0047467458970005694,
            "auditor_fp_violation": 0.018924898936378984,
            "ave_precision_score": 0.8239910220539542,
            "fpr": 0.19298245614035087,
            "logloss": 1.0829988389116891,
            "mae": 0.3592593025441989,
            "precision": 0.6823104693140795,
            "recall": 0.8129032258064516
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8423875061006507,
            "auditor_fn_violation": 0.007113691105529105,
            "auditor_fp_violation": 0.023051591657519216,
            "ave_precision_score": 0.842742878366232,
            "fpr": 0.1712403951701427,
            "logloss": 1.23217393696601,
            "mae": 0.35567629008028573,
            "precision": 0.7168784029038112,
            "recall": 0.8077709611451943
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(65)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.8227030835362488,
            "auditor_fn_violation": 0.007385398981324279,
            "auditor_fp_violation": 0.016101495349111043,
            "ave_precision_score": 0.802368664852195,
            "fpr": 0.24013157894736842,
            "logloss": 0.5560090506118273,
            "mae": 0.35637991074799447,
            "precision": 0.6540284360189573,
            "recall": 0.8903225806451613
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8460574745750377,
            "auditor_fn_violation": 0.005549083121763315,
            "auditor_fp_violation": 0.009639945687515939,
            "ave_precision_score": 0.8317452516238063,
            "fpr": 0.21295279912184412,
            "logloss": 0.5161766784405399,
            "mae": 0.3414368381975249,
            "precision": 0.694006309148265,
            "recall": 0.8997955010224948
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(66)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7940469514915304,
            "auditor_fn_violation": 0.009288341822297683,
            "auditor_fp_violation": 0.010668099218964637,
            "ave_precision_score": 0.7727089372289051,
            "fpr": 0.0581140350877193,
            "logloss": 0.5344179636716659,
            "mae": 0.32847037325662215,
            "precision": 0.8389057750759878,
            "recall": 0.5935483870967742
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8686132173216761,
            "auditor_fn_violation": 0.014503489502310994,
            "auditor_fp_violation": 0.010529546719661227,
            "ave_precision_score": 0.821752708671047,
            "fpr": 0.042810098792535674,
            "logloss": 0.5215666849477273,
            "mae": 0.3205094765605963,
            "precision": 0.878125,
            "recall": 0.5746421267893661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(67)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7808985079769618,
            "auditor_fn_violation": 0.055199490662139215,
            "auditor_fp_violation": 0.013729443855724326,
            "ave_precision_score": 0.7794978840878509,
            "fpr": 0.03837719298245614,
            "logloss": 0.6185715072692597,
            "mae": 0.3952701821792544,
            "precision": 0.8553719008264463,
            "recall": 0.44516129032258067
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.8463949690542732,
            "auditor_fn_violation": 0.05390377548661105,
            "auditor_fp_violation": 0.0032150493442443854,
            "ave_precision_score": 0.8449796357897502,
            "fpr": 0.018660812294182216,
            "logloss": 0.5800107951748164,
            "mae": 0.38144663228979486,
            "precision": 0.9300411522633745,
            "recall": 0.4621676891615542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(68)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5416666666666666,
            "auc_prc": 0.5656347619519315,
            "auditor_fn_violation": 0.015973401245048105,
            "auditor_fp_violation": 0.018100690764943674,
            "ave_precision_score": 0.5664100235500612,
            "fpr": 0.42214912280701755,
            "logloss": 0.7107905425041697,
            "mae": 0.492473676473948,
            "precision": 0.5287637698898409,
            "recall": 0.9290322580645162
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.6049236675610492,
            "auditor_fn_violation": 0.008797272149753413,
            "auditor_fp_violation": 0.016860800849022764,
            "ave_precision_score": 0.6093265650982668,
            "fpr": 0.40175631174533477,
            "logloss": 0.6919548036720502,
            "mae": 0.4837115648663659,
            "precision": 0.5585042219541616,
            "recall": 0.9468302658486708
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(69)",
        "seed": 29759,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7316915185880186,
            "auditor_fn_violation": 0.016779852857951333,
            "auditor_fp_violation": 0.023328034852231255,
            "ave_precision_score": 0.6592361487817453,
            "fpr": 0.18092105263157895,
            "logloss": 0.6525356009915477,
            "mae": 0.40741257637477757,
            "precision": 0.6726190476190477,
            "recall": 0.7290322580645161
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.750604671270548,
            "auditor_fn_violation": 0.012781747287750937,
            "auditor_fp_violation": 0.023644659012282743,
            "ave_precision_score": 0.6815704139474514,
            "fpr": 0.1602634467618002,
            "logloss": 0.6598715327343793,
            "mae": 0.40915542589060955,
            "precision": 0.7044534412955465,
            "recall": 0.7116564417177914
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(70)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8111043449257125,
            "auditor_fn_violation": 0.02679918883229581,
            "auditor_fp_violation": 0.025866890380313198,
            "ave_precision_score": 0.8114583281777991,
            "fpr": 0.10307017543859649,
            "logloss": 0.555873170651187,
            "mae": 0.33757207440845505,
            "precision": 0.7644110275689223,
            "recall": 0.6559139784946236
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8512652888569721,
            "auditor_fn_violation": 0.033682844758114304,
            "auditor_fp_violation": 0.015450965295155054,
            "ave_precision_score": 0.8517092462286044,
            "fpr": 0.08342480790340286,
            "logloss": 0.524647246252661,
            "mae": 0.3231671255942675,
            "precision": 0.8159806295399515,
            "recall": 0.689161554192229
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(71)",
        "seed": 29759,
        "test": {
            "accuracy": 0.518640350877193,
            "auc_prc": 0.8611421084476346,
            "auditor_fn_violation": 0.0006437464629315224,
            "auditor_fp_violation": 0.0034734487224773397,
            "ave_precision_score": 0.8585012013630965,
            "fpr": 0.48026315789473684,
            "logloss": 4.976659025498263,
            "mae": 0.4811812362127137,
            "precision": 0.5144124168514412,
            "recall": 0.9978494623655914
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.8743216962855527,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0015294894938638378,
            "ave_precision_score": 0.8736308407881987,
            "fpr": 0.45993413830954993,
            "logloss": 4.656302573268328,
            "mae": 0.45982596863768627,
            "precision": 0.538546255506608,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(72)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.772573760172382,
            "auditor_fn_violation": 0.006199302018487078,
            "auditor_fp_violation": 0.013812845873071945,
            "ave_precision_score": 0.5741280275036392,
            "fpr": 0.34539473684210525,
            "logloss": 0.7883637558029745,
            "mae": 0.4338171965720361,
            "precision": 0.5788770053475936,
            "recall": 0.9311827956989247
        },
        "train": {
            "accuracy": 0.6630076838638859,
            "auc_prc": 0.7979173399850207,
            "auditor_fn_violation": 0.0030551384015857093,
            "auditor_fp_violation": 0.016850396158588292,
            "ave_precision_score": 0.6181743921197189,
            "fpr": 0.305159165751921,
            "logloss": 0.7238347475380956,
            "mae": 0.40719339313674574,
            "precision": 0.6233062330623306,
            "recall": 0.9406952965235174
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(73)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.7436969232323252,
            "auditor_fn_violation": 0.018583757781550653,
            "auditor_fp_violation": 0.04052602142941246,
            "ave_precision_score": 0.7373110050522319,
            "fpr": 0.22697368421052633,
            "logloss": 0.6343388355185333,
            "mae": 0.40096687333145,
            "precision": 0.6368421052631579,
            "recall": 0.7806451612903226
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7975736477598233,
            "auditor_fn_violation": 0.018294914013904137,
            "auditor_fp_violation": 0.04286212224470792,
            "ave_precision_score": 0.7861629030467043,
            "fpr": 0.18990120746432493,
            "logloss": 0.5690779157518315,
            "mae": 0.3766099344044762,
            "precision": 0.6899641577060932,
            "recall": 0.787321063394683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(74)",
        "seed": 29759,
        "test": {
            "accuracy": 0.48355263157894735,
            "auc_prc": 0.5613162136735191,
            "auditor_fn_violation": 0.01701329937747596,
            "auditor_fp_violation": 0.020737666313434596,
            "ave_precision_score": 0.5162532111875666,
            "fpr": 0.24342105263157895,
            "logloss": 0.7858733719510181,
            "mae": 0.5013043093576766,
            "precision": 0.4931506849315068,
            "recall": 0.4645161290322581
        },
        "train": {
            "accuracy": 0.4994511525795829,
            "auc_prc": 0.5847408708580756,
            "auditor_fn_violation": 0.027705009663755192,
            "auditor_fp_violation": 0.019586829742848075,
            "ave_precision_score": 0.5442685617916638,
            "fpr": 0.2327113062568606,
            "logloss": 0.7900906934435314,
            "mae": 0.5038010430911762,
            "precision": 0.5361050328227571,
            "recall": 0.5010224948875256
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(75)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.7742640926976707,
            "auditor_fn_violation": 0.004661856253537069,
            "auditor_fp_violation": 0.006022116252600181,
            "ave_precision_score": 0.7712271282529418,
            "fpr": 0.06359649122807018,
            "logloss": 0.5984867986620164,
            "mae": 0.348205195858323,
            "precision": 0.8294117647058824,
            "recall": 0.6064516129032258
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8282238578057195,
            "auditor_fn_violation": 0.012393401260216536,
            "auditor_fp_violation": 0.007499180630628288,
            "ave_precision_score": 0.8297717056395562,
            "fpr": 0.038419319429198684,
            "logloss": 0.5728482068465686,
            "mae": 0.3368828864901047,
            "precision": 0.8939393939393939,
            "recall": 0.6032719836400818
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(76)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.7676183621451527,
            "auditor_fn_violation": 0.02540086776079985,
            "auditor_fp_violation": 0.03675085364417756,
            "ave_precision_score": 0.7681211349304062,
            "fpr": 0.15460526315789475,
            "logloss": 0.8901377681565732,
            "mae": 0.34212756710412245,
            "precision": 0.6921397379912664,
            "recall": 0.6817204301075269
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.8199550659734942,
            "auditor_fn_violation": 0.03485686193962006,
            "auditor_fp_violation": 0.028170699351267554,
            "ave_precision_score": 0.8202001655906515,
            "fpr": 0.12184412733260154,
            "logloss": 0.8219534375024848,
            "mae": 0.32237015798140767,
            "precision": 0.7527839643652561,
            "recall": 0.6912065439672802
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(77)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8122873991816397,
            "auditor_fn_violation": 0.007137804187889078,
            "auditor_fp_violation": 0.019354174025668202,
            "ave_precision_score": 0.7819920785443182,
            "fpr": 0.11403508771929824,
            "logloss": 0.5275746106920178,
            "mae": 0.3305579318926392,
            "precision": 0.7688888888888888,
            "recall": 0.7440860215053764
        },
        "train": {
            "accuracy": 0.7914379802414928,
            "auc_prc": 0.8751206240899826,
            "auditor_fn_violation": 0.004801573138127724,
            "auditor_fp_violation": 0.008230110133648253,
            "ave_precision_score": 0.8561231467875472,
            "fpr": 0.07683863885839737,
            "logloss": 0.47240764687823245,
            "mae": 0.30877810868748456,
            "precision": 0.8405466970387244,
            "recall": 0.754601226993865
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(78)",
        "seed": 29759,
        "test": {
            "accuracy": 0.518640350877193,
            "auc_prc": 0.7566822020538151,
            "auditor_fn_violation": 0.0006437464629315224,
            "auditor_fp_violation": 0.0034734487224773397,
            "ave_precision_score": 0.5144111259650858,
            "fpr": 0.48026315789473684,
            "logloss": 16.61076567426131,
            "mae": 0.4813596491661504,
            "precision": 0.5144124168514412,
            "recall": 0.9978494623655914
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.769273127753304,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0015294894938638378,
            "ave_precision_score": 0.538546255506608,
            "fpr": 0.45993413830954993,
            "logloss": 15.885930121659499,
            "mae": 0.45993413835452635,
            "precision": 0.538546255506608,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(79)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.6864769080551203,
            "auditor_fn_violation": 0.0274830220713073,
            "auditor_fp_violation": 0.02712773264256839,
            "ave_precision_score": 0.6871131735364959,
            "fpr": 0.15899122807017543,
            "logloss": 0.6339219253607855,
            "mae": 0.42821760635525136,
            "precision": 0.6784922394678492,
            "recall": 0.6580645161290323
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.7252768890003674,
            "auditor_fn_violation": 0.014445125359444557,
            "auditor_fp_violation": 0.023639456667065517,
            "ave_precision_score": 0.7261200280779019,
            "fpr": 0.1350164654226125,
            "logloss": 0.642230722359898,
            "mae": 0.429707033180075,
            "precision": 0.7235955056179775,
            "recall": 0.6584867075664622
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(80)",
        "seed": 29759,
        "test": {
            "accuracy": 0.4901315789473684,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.61023138556301,
            "mae": 0.5098684210526315,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4632272228320527,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.53947492547895,
            "mae": 0.5367727771679474,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(81)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7921576692242087,
            "auditor_fn_violation": 0.009917940011318621,
            "auditor_fp_violation": 0.005688508183209704,
            "ave_precision_score": 0.7808887635761479,
            "fpr": 0.06140350877192982,
            "logloss": 0.5628077979245952,
            "mae": 0.3475740285719423,
            "precision": 0.8233438485804416,
            "recall": 0.5612903225806452
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.8234560722657618,
            "auditor_fn_violation": 0.01478857589246632,
            "auditor_fp_violation": 0.008974045499711272,
            "ave_precision_score": 0.8157742275777544,
            "fpr": 0.059275521405049394,
            "logloss": 0.5424168447907716,
            "mae": 0.34655507766060983,
            "precision": 0.8348623853211009,
            "recall": 0.558282208588957
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(82)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5482456140350878,
            "auc_prc": 0.6493787382068743,
            "auditor_fn_violation": 0.008771929824561405,
            "auditor_fp_violation": 0.010459594175595591,
            "ave_precision_score": 0.6508690164637218,
            "fpr": 0.043859649122807015,
            "logloss": 0.9307901915884509,
            "mae": 0.4460026529411976,
            "precision": 0.6992481203007519,
            "recall": 0.2
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.7188302938694613,
            "auditor_fn_violation": 0.012927657644917054,
            "auditor_fp_violation": 0.007166230536725957,
            "ave_precision_score": 0.7195143298856368,
            "fpr": 0.03402854006586169,
            "logloss": 0.9443348371920676,
            "mae": 0.449630595112972,
            "precision": 0.7651515151515151,
            "recall": 0.2065439672801636
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(83)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8086672372098102,
            "auditor_fn_violation": 0.0064209583097528805,
            "auditor_fp_violation": 0.009127614898543903,
            "ave_precision_score": 0.7744666511331154,
            "fpr": 0.11074561403508772,
            "logloss": 0.5470373197345612,
            "mae": 0.34197646336989446,
            "precision": 0.7755555555555556,
            "recall": 0.7505376344086021
        },
        "train": {
            "accuracy": 0.7837541163556532,
            "auc_prc": 0.8380655973244657,
            "auditor_fn_violation": 0.008828698995912265,
            "auditor_fp_violation": 0.009720582038382906,
            "ave_precision_score": 0.8165565285074958,
            "fpr": 0.07793633369923161,
            "logloss": 0.48791694704941213,
            "mae": 0.3169300130166678,
            "precision": 0.836405529953917,
            "recall": 0.7423312883435583
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(84)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8288005234742397,
            "auditor_fn_violation": 0.02090407470288625,
            "auditor_fp_violation": 0.03210977667883355,
            "ave_precision_score": 0.8291682567142009,
            "fpr": 0.14912280701754385,
            "logloss": 0.8121298690220119,
            "mae": 0.2775175225247379,
            "precision": 0.7285429141716567,
            "recall": 0.7849462365591398
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8400760731820307,
            "auditor_fn_violation": 0.019509337140471275,
            "auditor_fp_violation": 0.028716945599076067,
            "ave_precision_score": 0.8404036792665228,
            "fpr": 0.12403951701427003,
            "logloss": 0.7607964230090578,
            "mae": 0.2636626825349796,
            "precision": 0.7693877551020408,
            "recall": 0.7709611451942741
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(85)",
        "seed": 29759,
        "test": {
            "accuracy": 0.506578947368421,
            "auc_prc": 0.598290454528149,
            "auditor_fn_violation": 0.01483918128654971,
            "auditor_fp_violation": 0.019906099140468626,
            "ave_precision_score": 0.5178250345718038,
            "fpr": 0.14912280701754385,
            "logloss": 1.2688555807087296,
            "mae": 0.5003776232540411,
            "precision": 0.5261324041811847,
            "recall": 0.3247311827956989
        },
        "train": {
            "accuracy": 0.5192096597145993,
            "auc_prc": 0.6438654733983719,
            "auditor_fn_violation": 0.008716460259630646,
            "auditor_fp_violation": 0.012053833868307834,
            "ave_precision_score": 0.5547849426525923,
            "fpr": 0.13721185510428102,
            "logloss": 1.270827744114488,
            "mae": 0.5047369093463463,
            "precision": 0.584717607973422,
            "recall": 0.35991820040899797
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(86)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.6680183703357641,
            "auditor_fn_violation": 0.01807677796642143,
            "auditor_fp_violation": 0.023703343930295537,
            "ave_precision_score": 0.6698711769468111,
            "fpr": 0.19078947368421054,
            "logloss": 0.6406328449645253,
            "mae": 0.45147981403567283,
            "precision": 0.6540755467196819,
            "recall": 0.7075268817204301
        },
        "train": {
            "accuracy": 0.6597145993413831,
            "auc_prc": 0.7079717937039218,
            "auditor_fn_violation": 0.009419074748753597,
            "auditor_fp_violation": 0.020354175662388608,
            "ave_precision_score": 0.7095563864807966,
            "fpr": 0.17453347969264543,
            "logloss": 0.6297544925965199,
            "mae": 0.451146053152229,
            "precision": 0.6800804828973843,
            "recall": 0.6912065439672802
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(87)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.7232395743330414,
            "auditor_fn_violation": 0.007371250707413699,
            "auditor_fp_violation": 0.011565897405706665,
            "ave_precision_score": 0.7157920419961384,
            "fpr": 0.1074561403508772,
            "logloss": 0.5513086250102425,
            "mae": 0.3596053164811772,
            "precision": 0.7772727272727272,
            "recall": 0.7354838709677419
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.7805802368729677,
            "auditor_fn_violation": 0.002529861115787724,
            "auditor_fp_violation": 0.005457260132867898,
            "ave_precision_score": 0.7712031961903094,
            "fpr": 0.07683863885839737,
            "logloss": 0.5322420809542417,
            "mae": 0.35147398114858686,
            "precision": 0.8329355608591885,
            "recall": 0.7137014314928425
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(88)",
        "seed": 29759,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8247189340664335,
            "auditor_fn_violation": 0.00794661384644407,
            "auditor_fp_violation": 0.016604360453706984,
            "ave_precision_score": 0.7391469416134834,
            "fpr": 0.14035087719298245,
            "logloss": 0.5884852665943648,
            "mae": 0.38531187114616233,
            "precision": 0.7403651115618661,
            "recall": 0.7849462365591398
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8482821917261378,
            "auditor_fn_violation": 0.008105881534258636,
            "auditor_fp_violation": 0.005543098828952095,
            "ave_precision_score": 0.7752620174838791,
            "fpr": 0.10647639956092206,
            "logloss": 0.5524493797176018,
            "mae": 0.372145439705655,
            "precision": 0.7962184873949579,
            "recall": 0.7750511247443763
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(89)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7294714019127914,
            "auditor_fn_violation": 0.012910299943406903,
            "auditor_fp_violation": 0.019665705090466667,
            "ave_precision_score": 0.7198943007249313,
            "fpr": 0.2149122807017544,
            "logloss": 0.6174131076883704,
            "mae": 0.3693341491915317,
            "precision": 0.6632302405498282,
            "recall": 0.8301075268817204
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.7797741205505335,
            "auditor_fn_violation": 0.008240568017796575,
            "auditor_fp_violation": 0.019074398738951528,
            "ave_precision_score": 0.7819458608420806,
            "fpr": 0.17014270032930845,
            "logloss": 0.5438617478325017,
            "mae": 0.3481517958074898,
            "precision": 0.7299651567944251,
            "recall": 0.8568507157464212
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(90)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.6051289091980868,
            "auditor_fn_violation": 0.010523957743821925,
            "auditor_fp_violation": 0.00690519643628086,
            "ave_precision_score": 0.6057933802961066,
            "fpr": 0.03399122807017544,
            "logloss": 6.845452921666835,
            "mae": 0.4617217129738059,
            "precision": 0.7703703703703704,
            "recall": 0.22365591397849463
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.6480372260200316,
            "auditor_fn_violation": 0.0059733455449078494,
            "auditor_fp_violation": 0.004549450892462322,
            "ave_precision_score": 0.6490278927502473,
            "fpr": 0.027442371020856202,
            "logloss": 6.6735984575626315,
            "mae": 0.4660385959878638,
            "precision": 0.8134328358208955,
            "recall": 0.2229038854805726
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(91)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.7878802255795638,
            "auditor_fn_violation": 0.007699019053008865,
            "auditor_fp_violation": 0.001979571411750857,
            "ave_precision_score": 0.7885484525066496,
            "fpr": 0.24013157894736842,
            "logloss": 0.604058075158343,
            "mae": 0.36850176129892614,
            "precision": 0.6444805194805194,
            "recall": 0.853763440860215
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.8517960541636196,
            "auditor_fn_violation": 0.0025388402146902544,
            "auditor_fp_violation": 0.007171432881943178,
            "ave_precision_score": 0.8519931688459372,
            "fpr": 0.21844127332601537,
            "logloss": 0.5614562352306969,
            "mae": 0.3534968437487157,
            "precision": 0.6737704918032786,
            "recall": 0.8404907975460123
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(92)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8335212638749682,
            "auditor_fn_violation": 0.012002452367477838,
            "auditor_fp_violation": 0.011565897405706665,
            "ave_precision_score": 0.7673208967584565,
            "fpr": 0.1074561403508772,
            "logloss": 0.5359304387724302,
            "mae": 0.35704414875720414,
            "precision": 0.7782805429864253,
            "recall": 0.7397849462365591
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8514464700270356,
            "auditor_fn_violation": 0.004013657209430748,
            "auditor_fp_violation": 0.005457260132867898,
            "ave_precision_score": 0.7949664078546715,
            "fpr": 0.07683863885839737,
            "logloss": 0.5242968105105956,
            "mae": 0.3518579643706328,
            "precision": 0.833729216152019,
            "recall": 0.7177914110429447
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(93)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7594369433107249,
            "auditor_fn_violation": 0.011424731182795701,
            "auditor_fp_violation": 0.016447368421052634,
            "ave_precision_score": 0.7587744917060217,
            "fpr": 0.16337719298245615,
            "logloss": 0.5557196528520825,
            "mae": 0.35021501732173194,
            "precision": 0.7072691552062869,
            "recall": 0.7741935483870968
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7999745559818616,
            "auditor_fn_violation": 0.010350656259891039,
            "auditor_fp_violation": 0.008979247844928505,
            "ave_precision_score": 0.7861314168418204,
            "fpr": 0.13830954994511527,
            "logloss": 0.5643123518250943,
            "mae": 0.3543154433713132,
            "precision": 0.7439024390243902,
            "recall": 0.7484662576687117
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(94)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7722279029430508,
            "auditor_fn_violation": 0.03240897943784192,
            "auditor_fp_violation": 0.05805516307547392,
            "ave_precision_score": 0.7729207724898459,
            "fpr": 0.21710526315789475,
            "logloss": 0.7438923051465309,
            "mae": 0.3182261298916991,
            "precision": 0.6721854304635762,
            "recall": 0.8731182795698925
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8209765739044712,
            "auditor_fn_violation": 0.03482319031873556,
            "auditor_fp_violation": 0.04304940667252798,
            "ave_precision_score": 0.8213506417438337,
            "fpr": 0.18221734357848518,
            "logloss": 0.7000006186394476,
            "mae": 0.3031685350906987,
            "precision": 0.717206132879046,
            "recall": 0.8609406952965235
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(95)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6282894736842105,
            "auc_prc": 0.7782431156138739,
            "auditor_fn_violation": 0.006578947368421054,
            "auditor_fp_violation": 0.0243803720711174,
            "ave_precision_score": 0.6537306510497632,
            "fpr": 0.33771929824561403,
            "logloss": 0.9342604153649972,
            "mae": 0.4046346658555465,
            "precision": 0.5849056603773585,
            "recall": 0.9333333333333333
        },
        "train": {
            "accuracy": 0.6597145993413831,
            "auc_prc": 0.784765020220014,
            "auditor_fn_violation": 0.005804987440485411,
            "auditor_fp_violation": 0.020858803148459334,
            "ave_precision_score": 0.6717786629524771,
            "fpr": 0.3040614709110867,
            "logloss": 0.9464603584572697,
            "mae": 0.39889991448337386,
            "precision": 0.6221009549795361,
            "recall": 0.9325153374233128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(96)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5526315789473685,
            "auc_prc": 0.6481011307887902,
            "auditor_fn_violation": 0.01930060365968687,
            "auditor_fp_violation": 0.010837356254170101,
            "ave_precision_score": 0.5476520501724338,
            "fpr": 0.06030701754385965,
            "logloss": 0.6828076156091781,
            "mae": 0.48572575314003125,
            "precision": 0.6706586826347305,
            "recall": 0.24086021505376345
        },
        "train": {
            "accuracy": 0.5477497255762898,
            "auc_prc": 0.6934776426932944,
            "auditor_fn_violation": 0.009544782133389012,
            "auditor_fp_violation": 0.008760749345805091,
            "ave_precision_score": 0.587636948644939,
            "fpr": 0.048298572996706916,
            "logloss": 0.6738663123988435,
            "mae": 0.48168609609457325,
            "precision": 0.7333333333333333,
            "recall": 0.2474437627811861
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(97)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.6081809917261015,
            "auditor_fn_violation": 0.05668977551405396,
            "auditor_fp_violation": 0.05367901016523412,
            "ave_precision_score": 0.6026968344971266,
            "fpr": 0.2774122807017544,
            "logloss": 0.6564758093892948,
            "mae": 0.45521859514216584,
            "precision": 0.5919354838709677,
            "recall": 0.789247311827957
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.6313237762533146,
            "auditor_fn_violation": 0.0546557750196979,
            "auditor_fp_violation": 0.0450731189620281,
            "ave_precision_score": 0.6206109737516314,
            "fpr": 0.2810098792535675,
            "logloss": 0.6517878914776869,
            "mae": 0.45127022595162186,
            "precision": 0.6018662519440124,
            "recall": 0.7914110429447853
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(98)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5646929824561403,
            "auc_prc": 0.6208052351387394,
            "auditor_fn_violation": 0.010297585361252595,
            "auditor_fp_violation": 0.018191451783821987,
            "ave_precision_score": 0.6218029635434841,
            "fpr": 0.40460526315789475,
            "logloss": 2.5107414952174305,
            "mae": 0.45185067633722437,
            "precision": 0.5421836228287841,
            "recall": 0.9397849462365592
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.6275298354148011,
            "auditor_fn_violation": 0.004595053863369542,
            "auditor_fp_violation": 0.02312442449056035,
            "ave_precision_score": 0.628596647402881,
            "fpr": 0.36443468715697036,
            "logloss": 2.2774195476480634,
            "mae": 0.40226220297826504,
            "precision": 0.5901234567901235,
            "recall": 0.9775051124744376
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(99)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8304480729029307,
            "auditor_fn_violation": 0.008962931522354275,
            "auditor_fp_violation": 0.01350622080929393,
            "ave_precision_score": 0.8307500588631864,
            "fpr": 0.13706140350877194,
            "logloss": 0.5192263766389243,
            "mae": 0.330450448162625,
            "precision": 0.7443762781186094,
            "recall": 0.7827956989247312
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.856622655788463,
            "auditor_fn_violation": 0.008105881534258636,
            "auditor_fp_violation": 0.005543098828952095,
            "ave_precision_score": 0.8570587280391464,
            "fpr": 0.10647639956092206,
            "logloss": 0.4870253290715149,
            "mae": 0.31792761565878186,
            "precision": 0.7962184873949579,
            "recall": 0.7750511247443763
        }
    }
]
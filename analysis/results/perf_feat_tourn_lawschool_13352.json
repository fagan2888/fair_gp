[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7900118409125297,
            "auditor_fn_violation": 0.013313880936651255,
            "auditor_fp_violation": 0.02988317568113833,
            "ave_precision_score": 0.7906493847437317,
            "fpr": 0.16666666666666666,
            "logloss": 0.7026581255537766,
            "mae": 0.306506703268671,
            "precision": 0.7093690248565966,
            "recall": 0.7761506276150628
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8393614712921545,
            "auditor_fn_violation": 0.006938999529559356,
            "auditor_fp_violation": 0.017790226730761954,
            "ave_precision_score": 0.8399950363879,
            "fpr": 0.15367727771679474,
            "logloss": 0.6026179059678154,
            "mae": 0.2739818139733924,
            "precision": 0.736346516007533,
            "recall": 0.8214285714285714
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8298399398132054,
            "auditor_fn_violation": 0.010948854877780228,
            "auditor_fp_violation": 0.023855000404236403,
            "ave_precision_score": 0.8302101430943953,
            "fpr": 0.18859649122807018,
            "logloss": 0.6252129033754477,
            "mae": 0.2888071234678807,
            "precision": 0.7039586919104991,
            "recall": 0.8556485355648535
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8607556685836335,
            "auditor_fn_violation": 0.007928308535269215,
            "auditor_fp_violation": 0.016591594433299276,
            "ave_precision_score": 0.8612060825062411,
            "fpr": 0.1690450054884742,
            "logloss": 0.5854365365588189,
            "mae": 0.26516204177937774,
            "precision": 0.7307692307692307,
            "recall": 0.8781512605042017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8252665240906971,
            "auditor_fn_violation": 0.023725959773911773,
            "auditor_fp_violation": 0.02514855687606112,
            "ave_precision_score": 0.825639660492546,
            "fpr": 0.12280701754385964,
            "logloss": 0.6307706864415732,
            "mae": 0.29285593976060664,
            "precision": 0.7611940298507462,
            "recall": 0.7468619246861925
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8802135081488959,
            "auditor_fn_violation": 0.016668357793172158,
            "auditor_fp_violation": 0.014497142208259204,
            "ave_precision_score": 0.8804303712061037,
            "fpr": 0.1119648737650933,
            "logloss": 0.5156945558207054,
            "mae": 0.2523974236358765,
            "precision": 0.7852631578947369,
            "recall": 0.7836134453781513
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8179472532512,
            "auditor_fn_violation": 0.017222711590692212,
            "auditor_fp_violation": 0.024807482415716713,
            "ave_precision_score": 0.8182810882010028,
            "fpr": 0.13925438596491227,
            "logloss": 0.578691217816495,
            "mae": 0.33885796963806025,
            "precision": 0.7386831275720165,
            "recall": 0.7510460251046025
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8472158300314048,
            "auditor_fn_violation": 0.02256731452185705,
            "auditor_fp_violation": 0.023081872894507742,
            "ave_precision_score": 0.8478549567335367,
            "fpr": 0.14489571899012074,
            "logloss": 0.5289187897583479,
            "mae": 0.31044779328448424,
            "precision": 0.7411764705882353,
            "recall": 0.7941176470588235
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8332415370708182,
            "auditor_fn_violation": 0.02674245760845629,
            "auditor_fp_violation": 0.02459525830705797,
            "ave_precision_score": 0.8335127028294358,
            "fpr": 0.12171052631578948,
            "logloss": 0.6111477471628337,
            "mae": 0.29154624416083297,
            "precision": 0.7592190889370932,
            "recall": 0.7322175732217573
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8778085942764822,
            "auditor_fn_violation": 0.01771993100203858,
            "auditor_fp_violation": 0.015650352650239095,
            "ave_precision_score": 0.8780539184771858,
            "fpr": 0.11086717892425905,
            "logloss": 0.5158239596130676,
            "mae": 0.2554782042910905,
            "precision": 0.7864693446088795,
            "recall": 0.7815126050420168
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8212706641366625,
            "auditor_fn_violation": 0.017220417675989138,
            "auditor_fp_violation": 0.020189081574905014,
            "ave_precision_score": 0.8215807646654583,
            "fpr": 0.14802631578947367,
            "logloss": 0.6377671654721134,
            "mae": 0.29632726400550863,
            "precision": 0.7352941176470589,
            "recall": 0.7845188284518828
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8680490222453289,
            "auditor_fn_violation": 0.0196962429318599,
            "auditor_fp_violation": 0.013338884893447898,
            "ave_precision_score": 0.8682969048861375,
            "fpr": 0.14928649835345773,
            "logloss": 0.5622859452865975,
            "mae": 0.2665889133110668,
            "precision": 0.7448405253283302,
            "recall": 0.8340336134453782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.8193755892560325,
            "auditor_fn_violation": 0.016268443074212737,
            "auditor_fp_violation": 0.024476513865308438,
            "ave_precision_score": 0.8196610667637985,
            "fpr": 0.13815789473684212,
            "logloss": 0.6222834708845462,
            "mae": 0.31269198041745605,
            "precision": 0.7296137339055794,
            "recall": 0.7112970711297071
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8607728906031495,
            "auditor_fn_violation": 0.016606093590015587,
            "auditor_fp_violation": 0.011506870055641777,
            "ave_precision_score": 0.8611144869857629,
            "fpr": 0.12184412733260154,
            "logloss": 0.5543914443655127,
            "mae": 0.27687895220663356,
            "precision": 0.7668067226890757,
            "recall": 0.7668067226890757
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8223258814693946,
            "auditor_fn_violation": 0.015424282463480883,
            "auditor_fp_violation": 0.02381710324197591,
            "ave_precision_score": 0.822626952357906,
            "fpr": 0.1524122807017544,
            "logloss": 0.6322117289036001,
            "mae": 0.29581641982151285,
            "precision": 0.7311411992263056,
            "recall": 0.7907949790794979
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8688514010306025,
            "auditor_fn_violation": 0.0172909998247378,
            "auditor_fp_violation": 0.014799954578144519,
            "ave_precision_score": 0.8690061577191068,
            "fpr": 0.14818880351262348,
            "logloss": 0.5524100164301744,
            "mae": 0.2660546389887325,
            "precision": 0.7452830188679245,
            "recall": 0.8298319327731093
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8081395015324513,
            "auditor_fn_violation": 0.028139451662629388,
            "auditor_fp_violation": 0.02649769585253457,
            "ave_precision_score": 0.8085277330976962,
            "fpr": 0.14035087719298245,
            "logloss": 0.6227497441289751,
            "mae": 0.31262924233995265,
            "precision": 0.7360824742268042,
            "recall": 0.7468619246861925
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8533764733076241,
            "auditor_fn_violation": 0.026649078951009604,
            "auditor_fp_violation": 0.021376029877487164,
            "ave_precision_score": 0.854608574289397,
            "fpr": 0.12403951701427003,
            "logloss": 0.5241941005384314,
            "mae": 0.2763203954038493,
            "precision": 0.77079107505071,
            "recall": 0.7983193277310925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 13352,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.82586318237359,
            "auditor_fn_violation": 0.02133570065330691,
            "auditor_fp_violation": 0.025504790201309732,
            "ave_precision_score": 0.8262446915500701,
            "fpr": 0.12609649122807018,
            "logloss": 0.6359202450469131,
            "mae": 0.2900178963575848,
            "precision": 0.7573839662447257,
            "recall": 0.7510460251046025
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8803242058712155,
            "auditor_fn_violation": 0.01698429097215176,
            "auditor_fp_violation": 0.01436087664181082,
            "ave_precision_score": 0.8805392819881834,
            "fpr": 0.1119648737650933,
            "logloss": 0.5187390732503725,
            "mae": 0.2509278114527823,
            "precision": 0.7861635220125787,
            "recall": 0.7878151260504201
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8294646505200605,
            "auditor_fn_violation": 0.022191330837554136,
            "auditor_fp_violation": 0.027675034360093786,
            "ave_precision_score": 0.829821872543773,
            "fpr": 0.10964912280701754,
            "logloss": 0.5874252748749647,
            "mae": 0.2935790256209689,
            "precision": 0.7767857142857143,
            "recall": 0.7280334728033473
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8818362107978915,
            "auditor_fn_violation": 0.018070455404994055,
            "auditor_fp_violation": 0.013712353482973113,
            "ave_precision_score": 0.8820516529176006,
            "fpr": 0.10098792535675083,
            "logloss": 0.49511622285755524,
            "mae": 0.25335057551644663,
            "precision": 0.8008658008658008,
            "recall": 0.7773109243697479
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.813262795678027,
            "auditor_fn_violation": 0.034532591940101304,
            "auditor_fp_violation": 0.03969096127415313,
            "ave_precision_score": 0.8136478725020977,
            "fpr": 0.14912280701754385,
            "logloss": 0.644991923229953,
            "mae": 0.32967920856244465,
            "precision": 0.7224489795918367,
            "recall": 0.7405857740585774
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8637860097209342,
            "auditor_fn_violation": 0.029730003966460354,
            "auditor_fp_violation": 0.03409414941267017,
            "ave_precision_score": 0.8641439906378513,
            "fpr": 0.145993413830955,
            "logloss": 0.5480230227962992,
            "mae": 0.29445565161405424,
            "precision": 0.744721689059501,
            "recall": 0.8151260504201681
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.8156864803592837,
            "auditor_fn_violation": 0.004404316229905311,
            "auditor_fp_violation": 0.012180147950521471,
            "ave_precision_score": 0.8159634875770924,
            "fpr": 0.18969298245614036,
            "logloss": 0.6271908951954489,
            "mae": 0.3414632064479775,
            "precision": 0.6894075403949731,
            "recall": 0.803347280334728
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8412244734089649,
            "auditor_fn_violation": 0.0030670885258603973,
            "auditor_fp_violation": 0.012546525858914677,
            "ave_precision_score": 0.8420777895610946,
            "fpr": 0.18880351262349068,
            "logloss": 0.5860039085940901,
            "mae": 0.31929007543893184,
            "precision": 0.6987740805604203,
            "recall": 0.8382352941176471
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.7920795232358172,
            "auditor_fn_violation": 0.012703699625633127,
            "auditor_fp_violation": 0.02839760692052713,
            "ave_precision_score": 0.7925423639296427,
            "fpr": 0.17543859649122806,
            "logloss": 0.7106007367764788,
            "mae": 0.3148901305840378,
            "precision": 0.7014925373134329,
            "recall": 0.7866108786610879
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8390648245997789,
            "auditor_fn_violation": 0.0011115313304246015,
            "auditor_fp_violation": 0.010704417275445707,
            "ave_precision_score": 0.8395696490666363,
            "fpr": 0.15806805708013172,
            "logloss": 0.6213251772114385,
            "mae": 0.28096129585492585,
            "precision": 0.7318435754189944,
            "recall": 0.8256302521008403
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8264361613318292,
            "auditor_fn_violation": 0.023707608456287162,
            "auditor_fp_violation": 0.026841296790363005,
            "ave_precision_score": 0.8267843278928048,
            "fpr": 0.1118421052631579,
            "logloss": 0.601047961258525,
            "mae": 0.29658639344588184,
            "precision": 0.7707865168539326,
            "recall": 0.7175732217573222
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8788049236152851,
            "auditor_fn_violation": 0.01966856995267921,
            "auditor_fp_violation": 0.013709830046557405,
            "ave_precision_score": 0.8790840670920792,
            "fpr": 0.10537870472008781,
            "logloss": 0.5037253519483088,
            "mae": 0.25402988389171915,
            "precision": 0.7935483870967742,
            "recall": 0.7752100840336135
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.78667091619448,
            "auditor_fn_violation": 0.022342729207957136,
            "auditor_fp_violation": 0.02940819791414019,
            "ave_precision_score": 0.7873335308969095,
            "fpr": 0.13706140350877194,
            "logloss": 0.6965155392940239,
            "mae": 0.30976468501179716,
            "precision": 0.7368421052631579,
            "recall": 0.7322175732217573
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8327603271876913,
            "auditor_fn_violation": 0.015157874346225873,
            "auditor_fp_violation": 0.016876742748274602,
            "ave_precision_score": 0.8334155455457631,
            "fpr": 0.132821075740944,
            "logloss": 0.6053025727786822,
            "mae": 0.27860682682629967,
            "precision": 0.75,
            "recall": 0.7626050420168067
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.8220788429150239,
            "auditor_fn_violation": 0.022540005872421647,
            "auditor_fp_violation": 0.018215902659875497,
            "ave_precision_score": 0.8223234160015752,
            "fpr": 0.07675438596491228,
            "logloss": 0.6569097895771631,
            "mae": 0.3356772071389347,
            "precision": 0.7994269340974212,
            "recall": 0.5836820083682008
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8554532043807016,
            "auditor_fn_violation": 0.022505050318700478,
            "auditor_fp_violation": 0.017262828519878374,
            "ave_precision_score": 0.8557560654202723,
            "fpr": 0.07354555433589462,
            "logloss": 0.565955562627244,
            "mae": 0.2989961705375308,
            "precision": 0.8273195876288659,
            "recall": 0.6743697478991597
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 13352,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8287252168591415,
            "auditor_fn_violation": 0.023122660207002863,
            "auditor_fp_violation": 0.025835758751718008,
            "ave_precision_score": 0.8290633157309016,
            "fpr": 0.14035087719298245,
            "logloss": 0.6200928328522285,
            "mae": 0.2908887722729697,
            "precision": 0.744,
            "recall": 0.7782426778242678
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8806391460173923,
            "auditor_fn_violation": 0.014463743785110091,
            "auditor_fp_violation": 0.014852946742874453,
            "ave_precision_score": 0.8808825185234108,
            "fpr": 0.1251372118551043,
            "logloss": 0.5141141387916524,
            "mae": 0.25287035791524,
            "precision": 0.7747035573122529,
            "recall": 0.8235294117647058
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8259185652399164,
            "auditor_fn_violation": 0.02379707112970712,
            "auditor_fp_violation": 0.025676590670223955,
            "ave_precision_score": 0.8262844562998254,
            "fpr": 0.12390350877192982,
            "logloss": 0.6290236719576787,
            "mae": 0.29223550527353637,
            "precision": 0.760593220338983,
            "recall": 0.7510460251046025
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8806196685139653,
            "auditor_fn_violation": 0.016668357793172158,
            "auditor_fp_violation": 0.014497142208259204,
            "ave_precision_score": 0.8808360689608796,
            "fpr": 0.1119648737650933,
            "logloss": 0.5146999287339772,
            "mae": 0.25202060329520304,
            "precision": 0.7852631578947369,
            "recall": 0.7836134453781513
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 13352,
        "test": {
            "accuracy": 0.6502192982456141,
            "auc_prc": 0.822600918509121,
            "auditor_fn_violation": 0.005500807457975485,
            "auditor_fp_violation": 0.015704584040747035,
            "ave_precision_score": 0.823056668959445,
            "fpr": 0.3223684210526316,
            "logloss": 0.9360555602261057,
            "mae": 0.35287407721681485,
            "precision": 0.606425702811245,
            "recall": 0.9476987447698745
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.863956363832919,
            "auditor_fn_violation": 0.0032953906041011363,
            "auditor_fp_violation": 0.014201900147621042,
            "ave_precision_score": 0.864155597625013,
            "fpr": 0.29857299670691545,
            "logloss": 0.8648161708387105,
            "mae": 0.33163651038900904,
            "precision": 0.6248275862068966,
            "recall": 0.9516806722689075
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8130936420985386,
            "auditor_fn_violation": 0.023742017176833295,
            "auditor_fp_violation": 0.029082282318699974,
            "ave_precision_score": 0.8134412646152632,
            "fpr": 0.14802631578947367,
            "logloss": 0.615451576376957,
            "mae": 0.3220603412667858,
            "precision": 0.7326732673267327,
            "recall": 0.7740585774058577
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.861852674745332,
            "auditor_fn_violation": 0.022046140080620613,
            "auditor_fp_violation": 0.023543661758582843,
            "ave_precision_score": 0.8621542172388622,
            "fpr": 0.14818880351262348,
            "logloss": 0.5347223191138427,
            "mae": 0.29304554495613605,
            "precision": 0.7418738049713193,
            "recall": 0.8151260504201681
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8196899304576457,
            "auditor_fn_violation": 0.012722050943257728,
            "auditor_fp_violation": 0.02915555016573693,
            "ave_precision_score": 0.8200956894653286,
            "fpr": 0.14692982456140352,
            "logloss": 0.6087324984232015,
            "mae": 0.2954407139631369,
            "precision": 0.7330677290836654,
            "recall": 0.7698744769874477
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8718326221877949,
            "auditor_fn_violation": 0.011761016151795516,
            "auditor_fp_violation": 0.016263547699256854,
            "ave_precision_score": 0.8720311805831847,
            "fpr": 0.13721185510428102,
            "logloss": 0.5292151695499135,
            "mae": 0.26424158726718927,
            "precision": 0.7534516765285996,
            "recall": 0.8025210084033614
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8235949551491228,
            "auditor_fn_violation": 0.01906701901196506,
            "auditor_fp_violation": 0.02338002263723826,
            "ave_precision_score": 0.8240189957697349,
            "fpr": 0.1074561403508772,
            "logloss": 0.6204003892985451,
            "mae": 0.29914535881436133,
            "precision": 0.7736720554272517,
            "recall": 0.700836820083682
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8761507306046878,
            "auditor_fn_violation": 0.013605881430508542,
            "auditor_fp_violation": 0.00804723872970211,
            "ave_precision_score": 0.8764058725823727,
            "fpr": 0.09440175631174534,
            "logloss": 0.5223510307823027,
            "mae": 0.2580499161106001,
            "precision": 0.8080357142857143,
            "recall": 0.7605042016806722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8026238720138368,
            "auditor_fn_violation": 0.01628450047713426,
            "auditor_fp_violation": 0.02478474411836042,
            "ave_precision_score": 0.8032434785057101,
            "fpr": 0.12719298245614036,
            "logloss": 0.6640456786749028,
            "mae": 0.3018839027742161,
            "precision": 0.7472766884531591,
            "recall": 0.7175732217573222
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8535910599934096,
            "auditor_fn_violation": 0.0037958103109520425,
            "auditor_fp_violation": 0.01922101517847004,
            "ave_precision_score": 0.8538556758771003,
            "fpr": 0.1141602634467618,
            "logloss": 0.5656381304902658,
            "mae": 0.2682698248526048,
            "precision": 0.7787234042553192,
            "recall": 0.7689075630252101
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.820620316171726,
            "auditor_fn_violation": 0.019353758349849523,
            "auditor_fp_violation": 0.034029125232435925,
            "ave_precision_score": 0.8209737233949901,
            "fpr": 0.18311403508771928,
            "logloss": 0.6800446386158334,
            "mae": 0.29939752637910727,
            "precision": 0.7039007092198581,
            "recall": 0.8305439330543933
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8704911351257811,
            "auditor_fn_violation": 0.013882611222315492,
            "auditor_fp_violation": 0.027104230541150947,
            "ave_precision_score": 0.8711552252088759,
            "fpr": 0.1668496158068057,
            "logloss": 0.575665923187485,
            "mae": 0.2690183969492346,
            "precision": 0.7275985663082437,
            "recall": 0.8529411764705882
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8382704639433871,
            "auditor_fn_violation": 0.013561623724583426,
            "auditor_fp_violation": 0.03170981890209395,
            "ave_precision_score": 0.8385720412438809,
            "fpr": 0.21820175438596492,
            "logloss": 0.572970531682959,
            "mae": 0.3311744364235844,
            "precision": 0.6841269841269841,
            "recall": 0.9016736401673641
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8676994562383614,
            "auditor_fn_violation": 0.01282642585025229,
            "auditor_fp_violation": 0.0197660774442636,
            "ave_precision_score": 0.8681432490169869,
            "fpr": 0.21075740944017562,
            "logloss": 0.5365379363349052,
            "mae": 0.31313083334759484,
            "precision": 0.6908212560386473,
            "recall": 0.9012605042016807
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8053745883954896,
            "auditor_fn_violation": 0.021062724803640902,
            "auditor_fp_violation": 0.02847592772253214,
            "ave_precision_score": 0.8058553223514098,
            "fpr": 0.12609649122807018,
            "logloss": 0.6318094166104619,
            "mae": 0.30809991060513103,
            "precision": 0.7466960352422908,
            "recall": 0.7092050209205021
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8477850610673712,
            "auditor_fn_violation": 0.011083028161868477,
            "auditor_fp_violation": 0.020571053660875385,
            "ave_precision_score": 0.8483498947786018,
            "fpr": 0.11745334796926454,
            "logloss": 0.5603186749517819,
            "mae": 0.2766226004339135,
            "precision": 0.7723404255319148,
            "recall": 0.7626050420168067
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8256613611197308,
            "auditor_fn_violation": 0.02379707112970712,
            "auditor_fp_violation": 0.02609345945508934,
            "ave_precision_score": 0.8260316470904774,
            "fpr": 0.125,
            "logloss": 0.6291446220814059,
            "mae": 0.2922368287743231,
            "precision": 0.758985200845666,
            "recall": 0.7510460251046025
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8803054581007661,
            "auditor_fn_violation": 0.016668357793172158,
            "auditor_fp_violation": 0.014497142208259204,
            "ave_precision_score": 0.880522298882924,
            "fpr": 0.1119648737650933,
            "logloss": 0.5150969539958522,
            "mae": 0.25245276482473594,
            "precision": 0.7852631578947369,
            "recall": 0.7836134453781513
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 13352,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.8039604100023049,
            "auditor_fn_violation": 0.02064982015708728,
            "auditor_fp_violation": 0.04462264532298489,
            "ave_precision_score": 0.8042619578656456,
            "fpr": 0.2817982456140351,
            "logloss": 0.8158026653608165,
            "mae": 0.34618560378083585,
            "precision": 0.623718887262079,
            "recall": 0.891213389121339
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.845036804309582,
            "auditor_fn_violation": 0.024352221679011892,
            "auditor_fp_violation": 0.04567924599719901,
            "ave_precision_score": 0.8462905941121885,
            "fpr": 0.2711306256860593,
            "logloss": 0.7601974295410272,
            "mae": 0.3287328325577399,
            "precision": 0.6378299120234604,
            "recall": 0.9138655462184874
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7418138032574815,
            "auditor_fn_violation": 0.009905123687880789,
            "auditor_fp_violation": 0.028347077370846477,
            "ave_precision_score": 0.7427383755952328,
            "fpr": 0.16666666666666666,
            "logloss": 0.95184237517644,
            "mae": 0.30970295543804177,
            "precision": 0.7065637065637066,
            "recall": 0.7656903765690377
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8019267066369178,
            "auditor_fn_violation": 0.005843610770323503,
            "auditor_fp_violation": 0.01839080459770116,
            "ave_precision_score": 0.8026371989501329,
            "fpr": 0.15587266739846323,
            "logloss": 0.8073214258447315,
            "mae": 0.2786166392843899,
            "precision": 0.7263969171483622,
            "recall": 0.792016806722689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7878208965207907,
            "auditor_fn_violation": 0.027719665271966534,
            "auditor_fp_violation": 0.03432219662058372,
            "ave_precision_score": 0.7885603187939534,
            "fpr": 0.15460526315789475,
            "logloss": 0.7428030342562295,
            "mae": 0.3099507109465361,
            "precision": 0.717434869739479,
            "recall": 0.7489539748953975
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8622183429623593,
            "auditor_fn_violation": 0.018033558099419797,
            "auditor_fp_violation": 0.017336008175933993,
            "ave_precision_score": 0.8624779548203508,
            "fpr": 0.1251372118551043,
            "logloss": 0.57068415518113,
            "mae": 0.2615392700119247,
            "precision": 0.7663934426229508,
            "recall": 0.7857142857142857
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.825814976351601,
            "auditor_fn_violation": 0.02352868310944726,
            "auditor_fp_violation": 0.024580099442153773,
            "ave_precision_score": 0.8261539858182823,
            "fpr": 0.12390350877192982,
            "logloss": 0.6282226466158181,
            "mae": 0.2947440903119793,
            "precision": 0.7616033755274262,
            "recall": 0.7552301255230126
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8748220410960934,
            "auditor_fn_violation": 0.018817625842872828,
            "auditor_fp_violation": 0.01599101656636007,
            "ave_precision_score": 0.8751352364008744,
            "fpr": 0.1163556531284303,
            "logloss": 0.5228732646423375,
            "mae": 0.2544655632316904,
            "precision": 0.7782426778242678,
            "recall": 0.7815126050420168
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8320212525140329,
            "auditor_fn_violation": 0.02442560375834986,
            "auditor_fp_violation": 0.02365793516048185,
            "ave_precision_score": 0.8323368952712642,
            "fpr": 0.13157894736842105,
            "logloss": 0.7640022061151661,
            "mae": 0.31253611793325137,
            "precision": 0.7540983606557377,
            "recall": 0.7698744769874477
        },
        "train": {
            "accuracy": 0.7837541163556532,
            "auc_prc": 0.8831845825376903,
            "auditor_fn_violation": 0.01689896595301128,
            "auditor_fp_violation": 0.01598849312994436,
            "ave_precision_score": 0.8834469001531523,
            "fpr": 0.12184412733260154,
            "logloss": 0.5118305656953597,
            "mae": 0.2759073804634622,
            "precision": 0.7784431137724551,
            "recall": 0.819327731092437
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8257038772340067,
            "auditor_fn_violation": 0.025067899875211047,
            "auditor_fp_violation": 0.025188980515805644,
            "ave_precision_score": 0.8260869824848923,
            "fpr": 0.1206140350877193,
            "logloss": 0.622921968903589,
            "mae": 0.2939336374833498,
            "precision": 0.7619047619047619,
            "recall": 0.7364016736401674
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8807528626682819,
            "auditor_fn_violation": 0.017387855251870234,
            "auditor_fp_violation": 0.014345736023316553,
            "ave_precision_score": 0.8809694377556335,
            "fpr": 0.10976948408342481,
            "logloss": 0.5103831319289632,
            "mae": 0.2532541450466155,
            "precision": 0.7867803837953091,
            "recall": 0.7752100840336135
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.8282167877378055,
            "auditor_fn_violation": 0.01071258166336343,
            "auditor_fp_violation": 0.030684069043576703,
            "ave_precision_score": 0.8285613680765147,
            "fpr": 0.25548245614035087,
            "logloss": 0.7964434533833613,
            "mae": 0.3204935624390808,
            "precision": 0.6573529411764706,
            "recall": 0.9351464435146444
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8718062992985479,
            "auditor_fn_violation": 0.00975011299799832,
            "auditor_fp_violation": 0.015186040349748296,
            "ave_precision_score": 0.872510899547451,
            "fpr": 0.2349066959385291,
            "logloss": 0.72719183118861,
            "mae": 0.29704218647012365,
            "precision": 0.676737160120846,
            "recall": 0.9411764705882353
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8285527884894307,
            "auditor_fn_violation": 0.019420281876238718,
            "auditor_fp_violation": 0.017235629396070824,
            "ave_precision_score": 0.8288661588382263,
            "fpr": 0.1206140350877193,
            "logloss": 0.5498778453785903,
            "mae": 0.32621682746914876,
            "precision": 0.7608695652173914,
            "recall": 0.7322175732217573
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8644893748935738,
            "auditor_fn_violation": 0.013476740860998624,
            "auditor_fp_violation": 0.012150346341648056,
            "ave_precision_score": 0.8649114635731789,
            "fpr": 0.1119648737650933,
            "logloss": 0.4888634345838958,
            "mae": 0.2934187974647387,
            "precision": 0.7883817427385892,
            "recall": 0.7983193277310925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8327480835973584,
            "auditor_fn_violation": 0.01959003156426632,
            "auditor_fp_violation": 0.023872685746624624,
            "ave_precision_score": 0.8330343056171242,
            "fpr": 0.12390350877192982,
            "logloss": 0.6138772135253652,
            "mae": 0.2912176438796905,
            "precision": 0.7554112554112554,
            "recall": 0.7301255230125523
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8773451642138458,
            "auditor_fn_violation": 0.01908513130828622,
            "auditor_fp_violation": 0.01410348612740831,
            "ave_precision_score": 0.877570065120381,
            "fpr": 0.1119648737650933,
            "logloss": 0.5208610456504593,
            "mae": 0.25543509793954006,
            "precision": 0.7870563674321504,
            "recall": 0.792016806722689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8301596316940449,
            "auditor_fn_violation": 0.018438486383322326,
            "auditor_fp_violation": 0.030817972350230427,
            "ave_precision_score": 0.830452749189094,
            "fpr": 0.14912280701754385,
            "logloss": 0.5895932479953776,
            "mae": 0.29265826943044787,
            "precision": 0.734375,
            "recall": 0.7866108786610879
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8754478392665287,
            "auditor_fn_violation": 0.021365846009095184,
            "auditor_fp_violation": 0.01876932006005779,
            "ave_precision_score": 0.8756764828466175,
            "fpr": 0.12952799121844127,
            "logloss": 0.5318935131462024,
            "mae": 0.26152654579773127,
            "precision": 0.768172888015717,
            "recall": 0.8214285714285714
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8173578338053669,
            "auditor_fn_violation": 0.019915767452103064,
            "auditor_fp_violation": 0.026912038159915923,
            "ave_precision_score": 0.8176951747733301,
            "fpr": 0.16337719298245615,
            "logloss": 0.6465297805978194,
            "mae": 0.32436007169463316,
            "precision": 0.7167300380228137,
            "recall": 0.7887029288702929
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8659441223944783,
            "auditor_fn_violation": 0.018575487275041744,
            "auditor_fp_violation": 0.018055187554411604,
            "ave_precision_score": 0.8663174774089548,
            "fpr": 0.14818880351262348,
            "logloss": 0.5238775619304337,
            "mae": 0.291246386817221,
            "precision": 0.7452830188679245,
            "recall": 0.8298319327731093
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8281437173106009,
            "auditor_fn_violation": 0.02179677750862512,
            "auditor_fp_violation": 0.030853343035006874,
            "ave_precision_score": 0.8284414537696907,
            "fpr": 0.1513157894736842,
            "logloss": 0.6014253821018393,
            "mae": 0.2949168900732618,
            "precision": 0.7267326732673267,
            "recall": 0.7677824267782427
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8700037360324315,
            "auditor_fn_violation": 0.014399173500355142,
            "auditor_fp_violation": 0.01623326646226832,
            "ave_precision_score": 0.8707494114288283,
            "fpr": 0.13062568605927552,
            "logloss": 0.5288944320465658,
            "mae": 0.2572547025923872,
            "precision": 0.7671232876712328,
            "recall": 0.8235294117647058
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8294718816673825,
            "auditor_fn_violation": 0.009788134038023935,
            "auditor_fp_violation": 0.016745492764168486,
            "ave_precision_score": 0.8298455636352154,
            "fpr": 0.10307017543859649,
            "logloss": 0.6597082976721684,
            "mae": 0.28414120041124447,
            "precision": 0.7839080459770115,
            "recall": 0.7133891213389121
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8689244322299409,
            "auditor_fn_violation": 0.008772334400280425,
            "auditor_fp_violation": 0.008630152541731334,
            "ave_precision_score": 0.8691417538720106,
            "fpr": 0.10428100987925357,
            "logloss": 0.6453624384981618,
            "mae": 0.2571844465629053,
            "precision": 0.7912087912087912,
            "recall": 0.7563025210084033
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7717132745328832,
            "auditor_fn_violation": 0.014300264258973794,
            "auditor_fp_violation": 0.02183887137197834,
            "ave_precision_score": 0.7218212153863699,
            "fpr": 0.14364035087719298,
            "logloss": 4.674616476653255,
            "mae": 0.2963811538083513,
            "precision": 0.7321063394683026,
            "recall": 0.7489539748953975
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8053875389831716,
            "auditor_fn_violation": 0.010762482819692092,
            "auditor_fp_violation": 0.016770758418814743,
            "ave_precision_score": 0.7646021439248721,
            "fpr": 0.13611416026344675,
            "logloss": 3.9225464632463316,
            "mae": 0.26979690920273186,
            "precision": 0.7474541751527495,
            "recall": 0.7710084033613446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.8130287454744585,
            "auditor_fn_violation": 0.017147012405490724,
            "auditor_fp_violation": 0.01397142048670062,
            "ave_precision_score": 0.8133053055176361,
            "fpr": 0.15350877192982457,
            "logloss": 0.6386985223790055,
            "mae": 0.33028790161816596,
            "precision": 0.7183098591549296,
            "recall": 0.7468619246861925
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8435219944743955,
            "auditor_fn_violation": 0.015902738702506254,
            "auditor_fp_violation": 0.011625471567180188,
            "ave_precision_score": 0.843887248444877,
            "fpr": 0.14928649835345773,
            "logloss": 0.5632897289638449,
            "mae": 0.30345618430676186,
            "precision": 0.734375,
            "recall": 0.7899159663865546
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8181663464667626,
            "auditor_fn_violation": 0.012832158849005362,
            "auditor_fp_violation": 0.026980253051984805,
            "ave_precision_score": 0.8185802434098426,
            "fpr": 0.15021929824561403,
            "logloss": 0.6196594642287194,
            "mae": 0.30435998348318544,
            "precision": 0.7248995983935743,
            "recall": 0.7552301255230126
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.867816972065135,
            "auditor_fn_violation": 0.006892877897591529,
            "auditor_fp_violation": 0.01931942919868277,
            "ave_precision_score": 0.8680881788787347,
            "fpr": 0.1350164654226125,
            "logloss": 0.5363689292605738,
            "mae": 0.2685029696399432,
            "precision": 0.7559523809523809,
            "recall": 0.8004201680672269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7766294479085842,
            "auditor_fn_violation": 0.01882615796814212,
            "auditor_fp_violation": 0.03067143665615652,
            "ave_precision_score": 0.7774981855104278,
            "fpr": 0.15789473684210525,
            "logloss": 0.7057229035389564,
            "mae": 0.30863358708336464,
            "precision": 0.7159763313609467,
            "recall": 0.7594142259414226
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8217569762094812,
            "auditor_fn_violation": 0.00561300261048437,
            "auditor_fp_violation": 0.02089405352208638,
            "ave_precision_score": 0.8224992858001827,
            "fpr": 0.150384193194292,
            "logloss": 0.6262999974637847,
            "mae": 0.280238862906085,
            "precision": 0.7355212355212355,
            "recall": 0.8004201680672269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8397580399107456,
            "auditor_fn_violation": 0.018165510533656316,
            "auditor_fp_violation": 0.027978211658177713,
            "ave_precision_score": 0.8400443213702291,
            "fpr": 0.12280701754385964,
            "logloss": 0.549395698889811,
            "mae": 0.2907443557858386,
            "precision": 0.7642105263157895,
            "recall": 0.7594142259414226
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8792369915048976,
            "auditor_fn_violation": 0.015868147478530385,
            "auditor_fp_violation": 0.010542917344840207,
            "ave_precision_score": 0.8795076415447103,
            "fpr": 0.11964873765093303,
            "logloss": 0.48202356187480844,
            "mae": 0.2569330833142682,
            "precision": 0.7793522267206477,
            "recall": 0.8088235294117647
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8196369197157457,
            "auditor_fn_violation": 0.014786574176025845,
            "auditor_fp_violation": 0.02724048023284017,
            "ave_precision_score": 0.8199530032078698,
            "fpr": 0.16228070175438597,
            "logloss": 0.6341323412077287,
            "mae": 0.296639880423834,
            "precision": 0.7186311787072244,
            "recall": 0.7907949790794979
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8678758056276275,
            "auditor_fn_violation": 0.013679676041657063,
            "auditor_fp_violation": 0.011683510604741541,
            "ave_precision_score": 0.8681172285667683,
            "fpr": 0.15367727771679474,
            "logloss": 0.5582660391750445,
            "mae": 0.2676412268004099,
            "precision": 0.7392923649906891,
            "recall": 0.8340336134453782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8197299887665157,
            "auditor_fn_violation": 0.01951203846436174,
            "auditor_fp_violation": 0.026760449510873963,
            "ave_precision_score": 0.8201331557543616,
            "fpr": 0.13925438596491227,
            "logloss": 0.715849954871642,
            "mae": 0.2866504583532608,
            "precision": 0.7402862985685071,
            "recall": 0.7573221757322176
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8721084826350516,
            "auditor_fn_violation": 0.010395815845547884,
            "auditor_fp_violation": 0.018532117036980968,
            "ave_precision_score": 0.8723770094252523,
            "fpr": 0.12184412733260154,
            "logloss": 0.5948317332782348,
            "mae": 0.2516890955139747,
            "precision": 0.7739307535641547,
            "recall": 0.7983193277310925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8284463746447598,
            "auditor_fn_violation": 0.019837774352198495,
            "auditor_fp_violation": 0.025380992804592127,
            "ave_precision_score": 0.8288935558150989,
            "fpr": 0.10855263157894737,
            "logloss": 0.5991772509595188,
            "mae": 0.2926662070840553,
            "precision": 0.7765237020316027,
            "recall": 0.7196652719665272
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8818830202941815,
            "auditor_fn_violation": 0.016359342858987727,
            "auditor_fp_violation": 0.014378540696720793,
            "ave_precision_score": 0.8820816073558959,
            "fpr": 0.09440175631174534,
            "logloss": 0.5028881280991756,
            "mae": 0.25525972238756767,
            "precision": 0.8080357142857143,
            "recall": 0.7605042016806722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8254695745877292,
            "auditor_fn_violation": 0.019245944358804962,
            "auditor_fp_violation": 0.02262713234699653,
            "ave_precision_score": 0.8257619328194572,
            "fpr": 0.125,
            "logloss": 0.6152076174202532,
            "mae": 0.2970953471805459,
            "precision": 0.7574468085106383,
            "recall": 0.7447698744769874
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8666224285561246,
            "auditor_fn_violation": 0.011761016151795513,
            "auditor_fp_violation": 0.013025978777899747,
            "ave_precision_score": 0.8669561000464199,
            "fpr": 0.11964873765093303,
            "logloss": 0.5389138844223799,
            "mae": 0.26414848626360765,
            "precision": 0.7743271221532091,
            "recall": 0.7857142857142857
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7344081769918012,
            "auditor_fn_violation": 0.017562210966747413,
            "auditor_fp_violation": 0.03056532460182715,
            "ave_precision_score": 0.7361303072950374,
            "fpr": 0.14144736842105263,
            "logloss": 0.8457103230677704,
            "mae": 0.3158799033848169,
            "precision": 0.7318087318087318,
            "recall": 0.7364016736401674
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.7825112679353058,
            "auditor_fn_violation": 0.005283232941914414,
            "auditor_fp_violation": 0.02245858409982715,
            "ave_precision_score": 0.7843050094179803,
            "fpr": 0.13721185510428102,
            "logloss": 0.6902382736076387,
            "mae": 0.2825199332476456,
            "precision": 0.751984126984127,
            "recall": 0.7962184873949579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8131752238270457,
            "auditor_fn_violation": 0.019915767452103064,
            "auditor_fp_violation": 0.026634125636672324,
            "ave_precision_score": 0.8135368642769212,
            "fpr": 0.1600877192982456,
            "logloss": 0.6184882069650184,
            "mae": 0.3230501615998787,
            "precision": 0.7208413001912046,
            "recall": 0.7887029288702929
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8637364886452892,
            "auditor_fn_violation": 0.01969163076866312,
            "auditor_fp_violation": 0.02354618519499856,
            "ave_precision_score": 0.8639945067909758,
            "fpr": 0.16355653128430298,
            "logloss": 0.5421377954082992,
            "mae": 0.2959902602828677,
            "precision": 0.7271062271062271,
            "recall": 0.8340336134453782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7781166840010825,
            "auditor_fn_violation": 0.019415694046832566,
            "auditor_fp_violation": 0.03067143665615652,
            "ave_precision_score": 0.7788974098779101,
            "fpr": 0.15789473684210525,
            "logloss": 0.7014143283618867,
            "mae": 0.30790464796814576,
            "precision": 0.7165354330708661,
            "recall": 0.7615062761506276
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8225272324516552,
            "auditor_fn_violation": 0.005354721471464549,
            "auditor_fp_violation": 0.02141640486013854,
            "ave_precision_score": 0.8232677323810949,
            "fpr": 0.15148188803512624,
            "logloss": 0.6235334143364767,
            "mae": 0.27963761659303527,
            "precision": 0.7346153846153847,
            "recall": 0.8025210084033614
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8101964561305195,
            "auditor_fn_violation": 0.020392901710342807,
            "auditor_fp_violation": 0.030934190314495926,
            "ave_precision_score": 0.8105736648341102,
            "fpr": 0.16776315789473684,
            "logloss": 0.6841431569485129,
            "mae": 0.30020450036041885,
            "precision": 0.711864406779661,
            "recall": 0.7907949790794979
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8618719447710192,
            "auditor_fn_violation": 0.021017627687738103,
            "auditor_fp_violation": 0.024406677012755975,
            "ave_precision_score": 0.8626199472612346,
            "fpr": 0.15477497255762898,
            "logloss": 0.5879870545028816,
            "mae": 0.26707643214319327,
            "precision": 0.7388888888888889,
            "recall": 0.8382352941176471
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8331989901206748,
            "auditor_fn_violation": 0.019908885707993842,
            "auditor_fp_violation": 0.02698530600695287,
            "ave_precision_score": 0.8334796253594863,
            "fpr": 0.12609649122807018,
            "logloss": 0.6141352817479909,
            "mae": 0.290624318606429,
            "precision": 0.7563559322033898,
            "recall": 0.7468619246861925
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8773241710736225,
            "auditor_fn_violation": 0.01988764770452638,
            "auditor_fp_violation": 0.013914228396229992,
            "ave_precision_score": 0.8775546265190481,
            "fpr": 0.1119648737650933,
            "logloss": 0.5206459892864416,
            "mae": 0.25520368860374865,
            "precision": 0.7875,
            "recall": 0.7941176470588235
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 13352,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8128032599225075,
            "auditor_fn_violation": 0.024668758716875877,
            "auditor_fp_violation": 0.029590104292990548,
            "ave_precision_score": 0.8131514246971923,
            "fpr": 0.14912280701754385,
            "logloss": 0.6157981440303323,
            "mae": 0.32214754241576965,
            "precision": 0.7306930693069307,
            "recall": 0.7719665271966527
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8621932477152803,
            "auditor_fn_violation": 0.022795616600097778,
            "auditor_fp_violation": 0.024739770619629817,
            "ave_precision_score": 0.8624789425736812,
            "fpr": 0.145993413830955,
            "logloss": 0.5343111699364944,
            "mae": 0.2929380039358466,
            "precision": 0.7442307692307693,
            "recall": 0.8130252100840336
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8261656932509955,
            "auditor_fn_violation": 0.024466894223005218,
            "auditor_fp_violation": 0.025188980515805644,
            "ave_precision_score": 0.8265332915070078,
            "fpr": 0.1206140350877193,
            "logloss": 0.6234513974947343,
            "mae": 0.29402454068761236,
            "precision": 0.7624190064794817,
            "recall": 0.7384937238493724
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.880393762494799,
            "auditor_fn_violation": 0.016742152404320674,
            "auditor_fp_violation": 0.013313650529290788,
            "ave_precision_score": 0.8806176393757379,
            "fpr": 0.10757409440175632,
            "logloss": 0.5113106096569402,
            "mae": 0.2533781135625454,
            "precision": 0.7896995708154506,
            "recall": 0.773109243697479
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8332118505153391,
            "auditor_fn_violation": 0.019908885707993842,
            "auditor_fp_violation": 0.02698530600695287,
            "ave_precision_score": 0.8334924808825386,
            "fpr": 0.12609649122807018,
            "logloss": 0.6141807849737314,
            "mae": 0.2906217874210278,
            "precision": 0.7563559322033898,
            "recall": 0.7468619246861925
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8773836017433629,
            "auditor_fn_violation": 0.01988764770452638,
            "auditor_fp_violation": 0.013914228396229992,
            "ave_precision_score": 0.8776131312974227,
            "fpr": 0.1119648737650933,
            "logloss": 0.5206582499754041,
            "mae": 0.25519324922088465,
            "precision": 0.7875,
            "recall": 0.7941176470588235
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 13352,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8273769082992186,
            "auditor_fn_violation": 0.023122660207002863,
            "auditor_fp_violation": 0.026932249979788183,
            "ave_precision_score": 0.8277408512941882,
            "fpr": 0.14035087719298245,
            "logloss": 0.6202402005914132,
            "mae": 0.2910142043422493,
            "precision": 0.744,
            "recall": 0.7782426778242678
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8811168560916433,
            "auditor_fn_violation": 0.01795515132507449,
            "auditor_fp_violation": 0.014852946742874453,
            "ave_precision_score": 0.8813332605342201,
            "fpr": 0.1251372118551043,
            "logloss": 0.5135277159638627,
            "mae": 0.2531484253532158,
            "precision": 0.7742574257425743,
            "recall": 0.8214285714285714
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 13352,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7441544494428729,
            "auditor_fn_violation": 0.007349702708654486,
            "auditor_fp_violation": 0.021606435443447328,
            "ave_precision_score": 0.744988786595345,
            "fpr": 0.20833333333333334,
            "logloss": 0.7526077979866871,
            "mae": 0.349025185016472,
            "precision": 0.6631205673758865,
            "recall": 0.7824267782426778
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7810482859196405,
            "auditor_fn_violation": 0.0012014685127618558,
            "auditor_fp_violation": 0.018759226314394955,
            "ave_precision_score": 0.7821596534052033,
            "fpr": 0.21295279912184412,
            "logloss": 0.7044317782476648,
            "mae": 0.3295566231394368,
            "precision": 0.6695059625212947,
            "recall": 0.8256302521008403
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8189848097682884,
            "auditor_fn_violation": 0.014687935843793585,
            "auditor_fp_violation": 0.029367774274395672,
            "ave_precision_score": 0.8195187998567957,
            "fpr": 0.1337719298245614,
            "logloss": 0.5942899357202451,
            "mae": 0.2942788126601635,
            "precision": 0.7474120082815735,
            "recall": 0.7552301255230126
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8735504657047611,
            "auditor_fn_violation": 0.006392458190740622,
            "auditor_fp_violation": 0.018383234288454023,
            "ave_precision_score": 0.8737420705718146,
            "fpr": 0.12623490669593854,
            "logloss": 0.5120063525229114,
            "mae": 0.2609124895209097,
            "precision": 0.7676767676767676,
            "recall": 0.7983193277310925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 13352,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7900692801535851,
            "auditor_fn_violation": 0.021092545694780893,
            "auditor_fp_violation": 0.029463780418788908,
            "ave_precision_score": 0.7904492218788924,
            "fpr": 0.20723684210526316,
            "logloss": 0.7151110238683345,
            "mae": 0.33611370418580727,
            "precision": 0.6752577319587629,
            "recall": 0.8221757322175732
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8376937928799896,
            "auditor_fn_violation": 0.02098995470855741,
            "auditor_fp_violation": 0.02958224510137906,
            "ave_precision_score": 0.8381513445756009,
            "fpr": 0.2052689352360044,
            "logloss": 0.6543370032202293,
            "mae": 0.3126274202006421,
            "precision": 0.6867671691792295,
            "recall": 0.8613445378151261
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8302340704712556,
            "auditor_fn_violation": 0.018438486383322326,
            "auditor_fp_violation": 0.030817972350230427,
            "ave_precision_score": 0.8305270794255969,
            "fpr": 0.14912280701754385,
            "logloss": 0.5894430222197556,
            "mae": 0.2926194426701777,
            "precision": 0.734375,
            "recall": 0.7866108786610879
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8754600574807853,
            "auditor_fn_violation": 0.021365846009095184,
            "auditor_fp_violation": 0.01876932006005779,
            "ave_precision_score": 0.8756886914052998,
            "fpr": 0.12952799121844127,
            "logloss": 0.5317439885801225,
            "mae": 0.26149188325098643,
            "precision": 0.768172888015717,
            "recall": 0.8214285714285714
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8258356657351313,
            "auditor_fn_violation": 0.025391341848344722,
            "auditor_fp_violation": 0.025504790201309732,
            "ave_precision_score": 0.8262048111415015,
            "fpr": 0.12609649122807018,
            "logloss": 0.62788854726917,
            "mae": 0.29211941865550545,
            "precision": 0.7563559322033898,
            "recall": 0.7468619246861925
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8803699979299484,
            "auditor_fn_violation": 0.01658303277403168,
            "auditor_fp_violation": 0.01475453272266172,
            "ave_precision_score": 0.8805881337074029,
            "fpr": 0.1119648737650933,
            "logloss": 0.5156037495429829,
            "mae": 0.25225709561432696,
            "precision": 0.7870563674321504,
            "recall": 0.792016806722689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8293926897864567,
            "auditor_fn_violation": 0.02045254349262278,
            "auditor_fp_violation": 0.025436575309240844,
            "ave_precision_score": 0.8297339359464342,
            "fpr": 0.1206140350877193,
            "logloss": 0.6164178067390943,
            "mae": 0.29127824312613165,
            "precision": 0.7624190064794817,
            "recall": 0.7384937238493724
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8792439775451595,
            "auditor_fn_violation": 0.018042782425813357,
            "auditor_fp_violation": 0.013174861526426688,
            "ave_precision_score": 0.8794663980206419,
            "fpr": 0.10757409440175632,
            "logloss": 0.5160689122543349,
            "mae": 0.25418825451396804,
            "precision": 0.7896995708154506,
            "recall": 0.773109243697479
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 13352,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.8073073516098411,
            "auditor_fn_violation": 0.013277178301402051,
            "auditor_fp_violation": 0.030065082059988684,
            "ave_precision_score": 0.8076706904933952,
            "fpr": 0.14144736842105263,
            "logloss": 0.6054601458932369,
            "mae": 0.32388442653009725,
            "precision": 0.720173535791757,
            "recall": 0.694560669456067
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8521710429222006,
            "auditor_fn_violation": 0.008117407226337298,
            "auditor_fp_violation": 0.016250930517178297,
            "ave_precision_score": 0.8524370416022945,
            "fpr": 0.12623490669593854,
            "logloss": 0.5385360323627064,
            "mae": 0.28856594048790085,
            "precision": 0.7589098532494759,
            "recall": 0.7605042016806722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8133901310673193,
            "auditor_fn_violation": 0.0161583351684651,
            "auditor_fp_violation": 0.028582039776861513,
            "ave_precision_score": 0.8138060792165389,
            "fpr": 0.13486842105263158,
            "logloss": 0.668881415049646,
            "mae": 0.2988880563168765,
            "precision": 0.7463917525773196,
            "recall": 0.7573221757322176
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8620706074353441,
            "auditor_fn_violation": 0.011341309300888303,
            "auditor_fp_violation": 0.020563483351628244,
            "ave_precision_score": 0.8626106926717573,
            "fpr": 0.12733260153677278,
            "logloss": 0.5329496447003886,
            "mae": 0.2620120797758795,
            "precision": 0.7670682730923695,
            "recall": 0.8025210084033614
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 13352,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.8044852048968453,
            "auditor_fn_violation": 0.016117044703809733,
            "auditor_fp_violation": 0.02993370523081899,
            "ave_precision_score": 0.804736386823995,
            "fpr": 0.2576754385964912,
            "logloss": 0.6864324955998712,
            "mae": 0.37610514292528185,
            "precision": 0.631083202511774,
            "recall": 0.8410041841004184
        },
        "train": {
            "accuracy": 0.6695938529088913,
            "auc_prc": 0.8386419195960856,
            "auditor_fn_violation": 0.01957402060714517,
            "auditor_fp_violation": 0.043589840644990344,
            "ave_precision_score": 0.8390572723109035,
            "fpr": 0.2601536772777168,
            "logloss": 0.6378001329295876,
            "mae": 0.3491794257726353,
            "precision": 0.6348228043143297,
            "recall": 0.865546218487395
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 13352,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.8072550046103775,
            "auditor_fn_violation": 0.007950708360860311,
            "auditor_fp_violation": 0.019231546608456628,
            "ave_precision_score": 0.8079362070402096,
            "fpr": 0.21052631578947367,
            "logloss": 0.6618493078718565,
            "mae": 0.31017666072033245,
            "precision": 0.6767676767676768,
            "recall": 0.8410041841004184
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8457828497317593,
            "auditor_fn_violation": 0.01039581584554788,
            "auditor_fp_violation": 0.0176161096180779,
            "ave_precision_score": 0.8461390812554851,
            "fpr": 0.18331503841931943,
            "logloss": 0.6329749285704798,
            "mae": 0.28707028180795274,
            "precision": 0.7155025553662692,
            "recall": 0.8823529411764706
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 13352,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8303951507467053,
            "auditor_fn_violation": 0.02123247449166851,
            "auditor_fp_violation": 0.025585637480798777,
            "ave_precision_score": 0.8307527951293945,
            "fpr": 0.12390350877192982,
            "logloss": 0.6278198729942517,
            "mae": 0.28878955515633825,
            "precision": 0.7626050420168067,
            "recall": 0.7594142259414226
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8825269471444676,
            "auditor_fn_violation": 0.01759540259572545,
            "auditor_fp_violation": 0.013656837881827474,
            "ave_precision_score": 0.882741278340481,
            "fpr": 0.1119648737650933,
            "logloss": 0.5154769654320539,
            "mae": 0.2506452230010958,
            "precision": 0.7875,
            "recall": 0.7941176470588235
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8306472443914885,
            "auditor_fn_violation": 0.02236566835498789,
            "auditor_fp_violation": 0.02572459374242057,
            "ave_precision_score": 0.8310009507290133,
            "fpr": 0.10526315789473684,
            "logloss": 0.5774204545117375,
            "mae": 0.29569902123541014,
            "precision": 0.7803203661327232,
            "recall": 0.7133891213389121
        },
        "train": {
            "accuracy": 0.7903402854006586,
            "auc_prc": 0.883299521098529,
            "auditor_fn_violation": 0.017267939008753886,
            "auditor_fp_violation": 0.01229418221734358,
            "ave_precision_score": 0.883515278692317,
            "fpr": 0.09549945115257959,
            "logloss": 0.4855806675829177,
            "mae": 0.25488561682533306,
            "precision": 0.8104575163398693,
            "recall": 0.7815126050420168
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7967495802576939,
            "auditor_fn_violation": 0.01618356823019893,
            "auditor_fp_violation": 0.02582817931926591,
            "ave_precision_score": 0.7970831776544153,
            "fpr": 0.15460526315789475,
            "logloss": 0.7692018054262045,
            "mae": 0.30490682687755716,
            "precision": 0.7191235059760956,
            "recall": 0.7552301255230126
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8467182203764543,
            "auditor_fn_violation": 0.0068306136944349645,
            "auditor_fp_violation": 0.013444869222907758,
            "ave_precision_score": 0.8469261472538547,
            "fpr": 0.13830954994511527,
            "logloss": 0.6727871944577223,
            "mae": 0.27323298119991873,
            "precision": 0.7459677419354839,
            "recall": 0.7773109243697479
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8087406980642354,
            "auditor_fn_violation": 0.02006487190780298,
            "auditor_fp_violation": 0.029463780418788922,
            "ave_precision_score": 0.8090831598739943,
            "fpr": 0.19188596491228072,
            "logloss": 0.8124148963380953,
            "mae": 0.3000895599520407,
            "precision": 0.7023809523809523,
            "recall": 0.8640167364016736
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8590634567769413,
            "auditor_fn_violation": 0.015626008910699297,
            "auditor_fp_violation": 0.01245315871153337,
            "ave_precision_score": 0.8593617375486575,
            "fpr": 0.1778265642151482,
            "logloss": 0.67785895986487,
            "mae": 0.27235185145738505,
            "precision": 0.7216494845360825,
            "recall": 0.8823529411764706
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7808898789597516,
            "auditor_fn_violation": 0.018383432430448512,
            "auditor_fp_violation": 0.028109588487347407,
            "ave_precision_score": 0.7816310058157957,
            "fpr": 0.17543859649122806,
            "logloss": 0.7037610074494931,
            "mae": 0.3135725995911056,
            "precision": 0.7037037037037037,
            "recall": 0.7949790794979079
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8256879562136994,
            "auditor_fn_violation": 0.010755564574896919,
            "auditor_fp_violation": 0.02239297475301867,
            "ave_precision_score": 0.8264205696887547,
            "fpr": 0.1690450054884742,
            "logloss": 0.6292896791198511,
            "mae": 0.2838683024859152,
            "precision": 0.7194899817850637,
            "recall": 0.8298319327731093
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7516777150798223,
            "auditor_fn_violation": 0.024260441899728407,
            "auditor_fp_violation": 0.03414786967418547,
            "ave_precision_score": 0.7528250977938639,
            "fpr": 0.13596491228070176,
            "logloss": 0.7618681068176079,
            "mae": 0.31914875811236443,
            "precision": 0.7310195227765727,
            "recall": 0.7050209205020921
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7916235359746208,
            "auditor_fn_violation": 0.017459343781420368,
            "auditor_fp_violation": 0.027631628752034523,
            "ave_precision_score": 0.7926912425621553,
            "fpr": 0.1350164654226125,
            "logloss": 0.6877223818990102,
            "mae": 0.2941618521778518,
            "precision": 0.7415966386554622,
            "recall": 0.7415966386554622
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8348116862041783,
            "auditor_fn_violation": 0.013093665125155985,
            "auditor_fp_violation": 0.025871129436494462,
            "ave_precision_score": 0.8351040968122724,
            "fpr": 0.23135964912280702,
            "logloss": 0.7457551771489648,
            "mae": 0.31215938454567405,
            "precision": 0.6692789968652038,
            "recall": 0.893305439330544
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8775910228847168,
            "auditor_fn_violation": 0.008172753184698688,
            "auditor_fp_violation": 0.020099171051137444,
            "ave_precision_score": 0.8777966695069718,
            "fpr": 0.21075740944017562,
            "logloss": 0.6634398520656866,
            "mae": 0.28718218912649596,
            "precision": 0.6913183279742765,
            "recall": 0.9033613445378151
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8047564034562089,
            "auditor_fn_violation": 0.029793364163546943,
            "auditor_fp_violation": 0.0276219783329291,
            "ave_precision_score": 0.8050702126876981,
            "fpr": 0.15021929824561403,
            "logloss": 0.6303089820419343,
            "mae": 0.3322111082036531,
            "precision": 0.7254509018036072,
            "recall": 0.7573221757322176
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8471836437713268,
            "auditor_fn_violation": 0.02415851082474703,
            "auditor_fp_violation": 0.024613598798844265,
            "ave_precision_score": 0.8475044630506551,
            "fpr": 0.145993413830955,
            "logloss": 0.5511518625130487,
            "mae": 0.3019330437325365,
            "precision": 0.7427466150870407,
            "recall": 0.8067226890756303
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7912516031574458,
            "auditor_fn_violation": 0.018009524333847178,
            "auditor_fp_violation": 0.02835465680329857,
            "ave_precision_score": 0.7918974510389877,
            "fpr": 0.1524122807017544,
            "logloss": 0.6676271939129076,
            "mae": 0.3093856162933598,
            "precision": 0.719758064516129,
            "recall": 0.7468619246861925
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8370684255248406,
            "auditor_fn_violation": 0.008401055262939423,
            "auditor_fp_violation": 0.021976607744426363,
            "ave_precision_score": 0.8377362011150189,
            "fpr": 0.141602634467618,
            "logloss": 0.5989925674336853,
            "mae": 0.2769752717755186,
            "precision": 0.7440476190476191,
            "recall": 0.7878151260504201
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8172357348750235,
            "auditor_fn_violation": 0.0066752917859502344,
            "auditor_fp_violation": 0.0192062818336163,
            "ave_precision_score": 0.8176098927726667,
            "fpr": 0.15350877192982457,
            "logloss": 0.6573716957690019,
            "mae": 0.2992282170269714,
            "precision": 0.7297297297297297,
            "recall": 0.7907949790794979
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8665371524586574,
            "auditor_fn_violation": 0.00963942108127554,
            "auditor_fp_violation": 0.010976948408342483,
            "ave_precision_score": 0.8667603737845833,
            "fpr": 0.15916575192096596,
            "logloss": 0.5603098617750797,
            "mae": 0.2696342016279726,
            "precision": 0.7309833024118738,
            "recall": 0.8277310924369747
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 13352,
        "test": {
            "accuracy": 0.5592105263157895,
            "auc_prc": 0.7910277919368705,
            "auditor_fn_violation": 0.004661234676649783,
            "auditor_fp_violation": 0.002238459050852943,
            "ave_precision_score": 0.7914308797576249,
            "fpr": 0.41885964912280704,
            "logloss": 1.1713768288632629,
            "mae": 0.421447371513064,
            "precision": 0.5452380952380952,
            "recall": 0.9581589958158996
        },
        "train": {
            "accuracy": 0.5762897914379802,
            "auc_prc": 0.8442609456609629,
            "auditor_fn_violation": 0.005410067429825937,
            "auditor_fp_violation": 0.01059843294598584,
            "ave_precision_score": 0.8445876388514542,
            "fpr": 0.40504939626783754,
            "logloss": 1.1250352534559591,
            "mae": 0.4068742801149557,
            "precision": 0.5543478260869565,
            "recall": 0.9642857142857143
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7878960024218564,
            "auditor_fn_violation": 0.027981171548117162,
            "auditor_fp_violation": 0.03124242056754791,
            "ave_precision_score": 0.7883510714090272,
            "fpr": 0.15570175438596492,
            "logloss": 0.7265862950896531,
            "mae": 0.32022450263060254,
            "precision": 0.7119675456389453,
            "recall": 0.7343096234309623
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8510624019660122,
            "auditor_fn_violation": 0.016647603058786632,
            "auditor_fp_violation": 0.012528861804004697,
            "ave_precision_score": 0.8513181657375398,
            "fpr": 0.12843029637760703,
            "logloss": 0.5816255200343661,
            "mae": 0.27491667854841284,
            "precision": 0.7592592592592593,
            "recall": 0.7752100840336135
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7879060620813269,
            "auditor_fn_violation": 0.027981171548117162,
            "auditor_fp_violation": 0.03124242056754791,
            "ave_precision_score": 0.7883611273304265,
            "fpr": 0.15570175438596492,
            "logloss": 0.7266438250989108,
            "mae": 0.3202146631208447,
            "precision": 0.7119675456389453,
            "recall": 0.7343096234309623
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8510501384701523,
            "auditor_fn_violation": 0.016647603058786632,
            "auditor_fp_violation": 0.012528861804004697,
            "ave_precision_score": 0.851305252593856,
            "fpr": 0.12843029637760703,
            "logloss": 0.5816721582517784,
            "mae": 0.27491304085720575,
            "precision": 0.7592592592592593,
            "recall": 0.7752100840336135
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8303650961017531,
            "auditor_fn_violation": 0.019227593041180366,
            "auditor_fp_violation": 0.024830220713073007,
            "ave_precision_score": 0.8306992437539868,
            "fpr": 0.12280701754385964,
            "logloss": 0.6123885472407319,
            "mae": 0.29076502427411316,
            "precision": 0.7617021276595745,
            "recall": 0.7489539748953975
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.879646205087976,
            "auditor_fn_violation": 0.01632013947181507,
            "auditor_fp_violation": 0.015011923237064234,
            "ave_precision_score": 0.8798731926533135,
            "fpr": 0.1119648737650933,
            "logloss": 0.5134810292505649,
            "mae": 0.2537993900533619,
            "precision": 0.7843551797040169,
            "recall": 0.7794117647058824
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8342534136883886,
            "auditor_fn_violation": 0.019557916758423256,
            "auditor_fp_violation": 0.022561443932411676,
            "ave_precision_score": 0.8345245721936501,
            "fpr": 0.13157894736842105,
            "logloss": 0.6031838018937128,
            "mae": 0.29352703447613404,
            "precision": 0.7505197505197505,
            "recall": 0.7552301255230126
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8728114478454305,
            "auditor_fn_violation": 0.019195823225008995,
            "auditor_fp_violation": 0.013475150459896288,
            "ave_precision_score": 0.873093702315987,
            "fpr": 0.1207464324917673,
            "logloss": 0.5256035093320255,
            "mae": 0.2592622594581654,
            "precision": 0.7736625514403292,
            "recall": 0.7899159663865546
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8117256648662327,
            "auditor_fn_violation": 0.014437899141158334,
            "auditor_fp_violation": 0.02117440779367774,
            "ave_precision_score": 0.8120986254013732,
            "fpr": 0.10416666666666667,
            "logloss": 0.640409637882974,
            "mae": 0.30683563648642775,
            "precision": 0.7732696897374701,
            "recall": 0.6778242677824268
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8422882233782227,
            "auditor_fn_violation": 0.013931038935881707,
            "auditor_fp_violation": 0.01596073532937154,
            "ave_precision_score": 0.8429028173766616,
            "fpr": 0.10428100987925357,
            "logloss": 0.5837073877323242,
            "mae": 0.2775967016833119,
            "precision": 0.7869955156950673,
            "recall": 0.7373949579831933
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8023647213472653,
            "auditor_fn_violation": 0.01675016516185862,
            "auditor_fp_violation": 0.021161775406257584,
            "ave_precision_score": 0.8029353606429844,
            "fpr": 0.10635964912280702,
            "logloss": 0.6288645140693311,
            "mae": 0.31250177603578777,
            "precision": 0.7706855791962175,
            "recall": 0.6820083682008368
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8437281138794255,
            "auditor_fn_violation": 0.0070289367118966176,
            "auditor_fp_violation": 0.01702057862397012,
            "ave_precision_score": 0.8444601091474022,
            "fpr": 0.10318331503841932,
            "logloss": 0.5591536231980707,
            "mae": 0.28156400515223673,
            "precision": 0.7853881278538812,
            "recall": 0.7226890756302521
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 13352,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7690755240773941,
            "auditor_fn_violation": 0.013713022094986423,
            "auditor_fp_violation": 0.023276437060392913,
            "ave_precision_score": 0.7695460173372634,
            "fpr": 0.22478070175438597,
            "logloss": 0.753475993138357,
            "mae": 0.3475744460755729,
            "precision": 0.6560402684563759,
            "recall": 0.8179916317991632
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.8087530765358921,
            "auditor_fn_violation": 0.004353882057762732,
            "auditor_fp_violation": 0.022539334065129903,
            "ave_precision_score": 0.8096164429593706,
            "fpr": 0.22283205268935236,
            "logloss": 0.6931403137733589,
            "mae": 0.3251850955421105,
            "precision": 0.6683006535947712,
            "recall": 0.8592436974789915
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7839543013535744,
            "auditor_fn_violation": 0.022342729207957136,
            "auditor_fp_violation": 0.029918546365914785,
            "ave_precision_score": 0.7846792598812621,
            "fpr": 0.13596491228070176,
            "logloss": 0.6997762367323656,
            "mae": 0.3104715445161871,
            "precision": 0.7383966244725738,
            "recall": 0.7322175732217573
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8318861200779696,
            "auditor_fn_violation": 0.014320766726009834,
            "auditor_fp_violation": 0.01752526590711231,
            "ave_precision_score": 0.8325391560682522,
            "fpr": 0.13172338090010977,
            "logloss": 0.6068352464712843,
            "mae": 0.2793055336415748,
            "precision": 0.7515527950310559,
            "recall": 0.7626050420168067
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7874742753525577,
            "auditor_fn_violation": 0.026898443808265435,
            "auditor_fp_violation": 0.0310327229363732,
            "ave_precision_score": 0.7879302910451594,
            "fpr": 0.15460526315789475,
            "logloss": 0.7263283258771734,
            "mae": 0.3208153141718875,
            "precision": 0.7104722792607803,
            "recall": 0.7238493723849372
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8510374049313156,
            "auditor_fn_violation": 0.018587017683033696,
            "auditor_fp_violation": 0.01510276694802983,
            "ave_precision_score": 0.851292442136907,
            "fpr": 0.12403951701427003,
            "logloss": 0.5804818845418159,
            "mae": 0.27510740505613734,
            "precision": 0.7640918580375783,
            "recall": 0.7689075630252101
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 13352,
        "test": {
            "accuracy": 0.5822368421052632,
            "auc_prc": 0.5650745237474191,
            "auditor_fn_violation": 0.021503156426631434,
            "auditor_fp_violation": 0.03159612741531248,
            "ave_precision_score": 0.5671846396734009,
            "fpr": 0.2324561403508772,
            "logloss": 1.0993645515554398,
            "mae": 0.44009373414672537,
            "precision": 0.5930902111324377,
            "recall": 0.6464435146443515
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.5932998450924083,
            "auditor_fn_violation": 0.018660812294182223,
            "auditor_fp_violation": 0.024757434674539794,
            "ave_precision_score": 0.5953628240604361,
            "fpr": 0.2217343578485181,
            "logloss": 1.0203451721544259,
            "mae": 0.42664307893697,
            "precision": 0.6023622047244095,
            "recall": 0.6428571428571429
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7917580752690127,
            "auditor_fn_violation": 0.011318175144975408,
            "auditor_fp_violation": 0.02687666747513947,
            "ave_precision_score": 0.7923822924829539,
            "fpr": 0.17982456140350878,
            "logloss": 0.7028764935769154,
            "mae": 0.3076131672929805,
            "precision": 0.7055655296229802,
            "recall": 0.8221757322175732
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8410415237816262,
            "auditor_fn_violation": 0.0061018919093433245,
            "auditor_fp_violation": 0.019549061912512475,
            "ave_precision_score": 0.8416596443693928,
            "fpr": 0.1734357848518112,
            "logloss": 0.6157320172379448,
            "mae": 0.2770516327284637,
            "precision": 0.7198581560283688,
            "recall": 0.8529411764705882
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8271287419070024,
            "auditor_fn_violation": 0.023627321441679516,
            "auditor_fp_violation": 0.02514855687606112,
            "ave_precision_score": 0.827474000211174,
            "fpr": 0.12280701754385964,
            "logloss": 0.6293147897348057,
            "mae": 0.29218325322448246,
            "precision": 0.7617021276595745,
            "recall": 0.7489539748953975
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8799424472067217,
            "auditor_fn_violation": 0.01761846341170936,
            "auditor_fp_violation": 0.014042923653431242,
            "ave_precision_score": 0.8801861596098213,
            "fpr": 0.11086717892425905,
            "logloss": 0.5153308256144989,
            "mae": 0.2516844704511483,
            "precision": 0.7864693446088795,
            "recall": 0.7815126050420168
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.8068422540272855,
            "auditor_fn_violation": 0.010024407252440727,
            "auditor_fp_violation": 0.024860538442881407,
            "ave_precision_score": 0.8071995691025102,
            "fpr": 0.16776315789473684,
            "logloss": 0.6898314548267495,
            "mae": 0.30734358577202653,
            "precision": 0.7080152671755725,
            "recall": 0.7761506276150628
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8466752938585823,
            "auditor_fn_violation": 0.0019394146242470653,
            "auditor_fp_violation": 0.007179176602697556,
            "ave_precision_score": 0.8469505162378848,
            "fpr": 0.15697036223929747,
            "logloss": 0.6022368763462982,
            "mae": 0.2772017133265397,
            "precision": 0.730188679245283,
            "recall": 0.8130252100840336
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7842299241739458,
            "auditor_fn_violation": 0.01734199515525215,
            "auditor_fp_violation": 0.032793677742743964,
            "ave_precision_score": 0.7849547427140859,
            "fpr": 0.15570175438596492,
            "logloss": 0.6826267540399856,
            "mae": 0.3110365501943087,
            "precision": 0.7171314741035857,
            "recall": 0.7531380753138075
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8315767677616962,
            "auditor_fn_violation": 0.007683863885839738,
            "auditor_fp_violation": 0.024058442787387866,
            "ave_precision_score": 0.8322303562332569,
            "fpr": 0.14489571899012074,
            "logloss": 0.600896582711157,
            "mae": 0.28150207911391334,
            "precision": 0.7421875,
            "recall": 0.7983193277310925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8367456202485557,
            "auditor_fn_violation": 0.01772278499596271,
            "auditor_fp_violation": 0.024671052631578944,
            "ave_precision_score": 0.8369962327043396,
            "fpr": 0.12828947368421054,
            "logloss": 0.6056956097451761,
            "mae": 0.28919878290533935,
            "precision": 0.755741127348643,
            "recall": 0.7573221757322176
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8749232442067705,
            "auditor_fn_violation": 0.02311616194227417,
            "auditor_fp_violation": 0.011491729437147508,
            "ave_precision_score": 0.8752102229078657,
            "fpr": 0.1141602634467618,
            "logloss": 0.5254354145136076,
            "mae": 0.2546513324745785,
            "precision": 0.7837837837837838,
            "recall": 0.792016806722689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7954839171423278,
            "auditor_fn_violation": 0.011607208397562943,
            "auditor_fp_violation": 0.028387501010590996,
            "ave_precision_score": 0.79591973397526,
            "fpr": 0.17105263157894737,
            "logloss": 0.7052378285889686,
            "mae": 0.3137409634409379,
            "precision": 0.706766917293233,
            "recall": 0.7866108786610879
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8422002389133295,
            "auditor_fn_violation": 0.003392246031233569,
            "auditor_fp_violation": 0.017457133123888115,
            "ave_precision_score": 0.8426674675474364,
            "fpr": 0.15477497255762898,
            "logloss": 0.6125696640265896,
            "mae": 0.2794272506666932,
            "precision": 0.7359550561797753,
            "recall": 0.8256302521008403
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.782649122694068,
            "auditor_fn_violation": 0.01679833737062322,
            "auditor_fp_violation": 0.027361751152073732,
            "ave_precision_score": 0.7832558727403219,
            "fpr": 0.16666666666666666,
            "logloss": 0.7194353146255223,
            "mae": 0.3163796453447838,
            "precision": 0.7071290944123314,
            "recall": 0.7677824267782427
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8232880289994386,
            "auditor_fn_violation": 0.009821601527548452,
            "auditor_fp_violation": 0.014270032930845231,
            "ave_precision_score": 0.824016121601888,
            "fpr": 0.15916575192096596,
            "logloss": 0.6430043066275395,
            "mae": 0.28556457152189857,
            "precision": 0.7253787878787878,
            "recall": 0.8046218487394958
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8262204403673812,
            "auditor_fn_violation": 0.02352868310944726,
            "auditor_fp_violation": 0.024580099442153773,
            "ave_precision_score": 0.8265589748733246,
            "fpr": 0.12390350877192982,
            "logloss": 0.6264471418691983,
            "mae": 0.29429594546260923,
            "precision": 0.7616033755274262,
            "recall": 0.7552301255230126
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8750434071035651,
            "auditor_fn_violation": 0.0197239159110406,
            "auditor_fp_violation": 0.01599101656636007,
            "ave_precision_score": 0.8753580999063373,
            "fpr": 0.1163556531284303,
            "logloss": 0.521921893738867,
            "mae": 0.2541895764469227,
            "precision": 0.778705636743215,
            "recall": 0.7836134453781513
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 13352,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.683518225986268,
            "auditor_fn_violation": 0.01164849886221831,
            "auditor_fp_violation": 0.034880548144554956,
            "ave_precision_score": 0.6841187229323766,
            "fpr": 0.22149122807017543,
            "logloss": 1.2055683136521127,
            "mae": 0.33151140361506526,
            "precision": 0.6638935108153078,
            "recall": 0.8347280334728033
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7428113844616159,
            "auditor_fn_violation": 0.007716149028217216,
            "auditor_fp_violation": 0.024588364434687156,
            "ave_precision_score": 0.7419497348451245,
            "fpr": 0.22283205268935236,
            "logloss": 1.0436610088924252,
            "mae": 0.30869688256940797,
            "precision": 0.6725806451612903,
            "recall": 0.8760504201680672
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 13352,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8227653666500128,
            "auditor_fn_violation": 0.027022315202231525,
            "auditor_fp_violation": 0.028713416606031206,
            "ave_precision_score": 0.8231302643040928,
            "fpr": 0.12828947368421054,
            "logloss": 0.6437843086986117,
            "mae": 0.29501453559356805,
            "precision": 0.7489270386266095,
            "recall": 0.7301255230125523
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8781432851525153,
            "auditor_fn_violation": 0.0167606010571078,
            "auditor_fp_violation": 0.015211274713905397,
            "ave_precision_score": 0.8783749410625572,
            "fpr": 0.11306256860592755,
            "logloss": 0.5234105570539382,
            "mae": 0.25262555448417473,
            "precision": 0.783157894736842,
            "recall": 0.7815126050420168
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8391102348848009,
            "auditor_fn_violation": 0.024879007864488805,
            "auditor_fp_violation": 0.020372754679060422,
            "ave_precision_score": 0.8394458935754892,
            "fpr": 0.13267543859649122,
            "logloss": 0.7423124611741952,
            "mae": 0.2665248645365371,
            "precision": 0.763671875,
            "recall": 0.7931034482758621
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8410511068044859,
            "auditor_fn_violation": 0.007186210476437661,
            "auditor_fp_violation": 0.01854860348823027,
            "ave_precision_score": 0.841315268189886,
            "fpr": 0.14050493962678376,
            "logloss": 0.7497749746455296,
            "mae": 0.26655016020501726,
            "precision": 0.7393075356415478,
            "recall": 0.7874186550976139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8467756864777041,
            "auditor_fn_violation": 0.02531493541155119,
            "auditor_fp_violation": 0.021296528911778256,
            "ave_precision_score": 0.8471045109910427,
            "fpr": 0.13048245614035087,
            "logloss": 0.7452510587004739,
            "mae": 0.26103279891603026,
            "precision": 0.7684824902723736,
            "recall": 0.8012170385395537
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8474668578315303,
            "auditor_fn_violation": 0.007186210476437663,
            "auditor_fp_violation": 0.023666300768386397,
            "ave_precision_score": 0.8477314903857307,
            "fpr": 0.13830954994511527,
            "logloss": 0.7551076215086876,
            "mae": 0.26275778997966703,
            "precision": 0.7412731006160165,
            "recall": 0.7830802603036876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 7376,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.835705503438066,
            "auditor_fn_violation": 0.019082950784669588,
            "auditor_fp_violation": 0.02168383368923503,
            "ave_precision_score": 0.8360342514240098,
            "fpr": 0.12828947368421054,
            "logloss": 0.7653054284323983,
            "mae": 0.26922826612979295,
            "precision": 0.7683168316831683,
            "recall": 0.7870182555780934
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8321759998842894,
            "auditor_fn_violation": 0.009203016398751344,
            "auditor_fp_violation": 0.02110013416270278,
            "ave_precision_score": 0.8325357728928918,
            "fpr": 0.14270032930845225,
            "logloss": 0.761389321986049,
            "mae": 0.2716884449006798,
            "precision": 0.731404958677686,
            "recall": 0.7678958785249458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 7376,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.8484637836644844,
            "auditor_fn_violation": 0.01909407138535996,
            "auditor_fp_violation": 0.015248817150274254,
            "ave_precision_score": 0.8487483836736108,
            "fpr": 0.10526315789473684,
            "logloss": 0.7233400275504966,
            "mae": 0.2645978827336146,
            "precision": 0.7974683544303798,
            "recall": 0.7667342799188641
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8426954489876191,
            "auditor_fn_violation": 0.00786720987877735,
            "auditor_fp_violation": 0.016353213806561774,
            "ave_precision_score": 0.8430037979648999,
            "fpr": 0.11964873765093303,
            "logloss": 0.7331194254307382,
            "mae": 0.2661020464449195,
            "precision": 0.7620087336244541,
            "recall": 0.7570498915401301
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 7376,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8481923513360212,
            "auditor_fn_violation": 0.020130511369702148,
            "auditor_fp_violation": 0.019383557342042463,
            "ave_precision_score": 0.8484821583187614,
            "fpr": 0.13048245614035087,
            "logloss": 0.7158826491605343,
            "mae": 0.26373260335076515,
            "precision": 0.7634194831013916,
            "recall": 0.7789046653144016
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8449627781742889,
            "auditor_fn_violation": 0.005328939379147614,
            "auditor_fp_violation": 0.019195023783388214,
            "ave_precision_score": 0.8452476554581055,
            "fpr": 0.1350164654226125,
            "logloss": 0.7303024579583149,
            "mae": 0.26484945584480685,
            "precision": 0.7448132780082988,
            "recall": 0.7787418655097614
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8453454862299825,
            "auditor_fn_violation": 0.022301252624461768,
            "auditor_fp_violation": 0.017138236402462006,
            "ave_precision_score": 0.8456444323049291,
            "fpr": 0.13048245614035087,
            "logloss": 0.7499818367287574,
            "mae": 0.263981892447098,
            "precision": 0.7652859960552268,
            "recall": 0.7870182555780934
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8404244993038401,
            "auditor_fn_violation": 0.010881703736686586,
            "auditor_fp_violation": 0.023232101475789732,
            "ave_precision_score": 0.8407815628818973,
            "fpr": 0.12952799121844127,
            "logloss": 0.753399161379604,
            "mae": 0.26354988753779013,
            "precision": 0.7551867219917012,
            "recall": 0.789587852494577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8479031869914135,
            "auditor_fn_violation": 0.023437778015017258,
            "auditor_fp_violation": 0.019341686555290376,
            "ave_precision_score": 0.8482164464346201,
            "fpr": 0.13267543859649122,
            "logloss": 0.7499002821147722,
            "mae": 0.261083994881942,
            "precision": 0.7645914396887159,
            "recall": 0.7971602434077079
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8446795632018164,
            "auditor_fn_violation": 0.004319345859595069,
            "auditor_fp_violation": 0.02182217343578486,
            "ave_precision_score": 0.8449488240718865,
            "fpr": 0.13830954994511527,
            "logloss": 0.7699590637841082,
            "mae": 0.2645881841642844,
            "precision": 0.7402061855670103,
            "recall": 0.7787418655097614
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8470508920891199,
            "auditor_fn_violation": 0.02140715632895627,
            "auditor_fp_violation": 0.02168121676506302,
            "ave_precision_score": 0.847377094772527,
            "fpr": 0.12938596491228072,
            "logloss": 0.7462952916783887,
            "mae": 0.2605682112233232,
            "precision": 0.7713178294573644,
            "recall": 0.8073022312373225
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8484599298598117,
            "auditor_fn_violation": 0.006667127015913005,
            "auditor_fp_violation": 0.023183315038419326,
            "ave_precision_score": 0.8487159778417006,
            "fpr": 0.13830954994511527,
            "logloss": 0.7571541920058816,
            "mae": 0.2624544767832567,
            "precision": 0.7428571428571429,
            "recall": 0.789587852494577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.81728938562408,
            "auditor_fn_violation": 0.023064125831820936,
            "auditor_fp_violation": 0.01555761420257087,
            "ave_precision_score": 0.7952712027961238,
            "fpr": 0.1425438596491228,
            "logloss": 1.6808739733031937,
            "mae": 0.2570595871605993,
            "precision": 0.7565543071161048,
            "recall": 0.8194726166328601
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.7885887377092815,
            "auditor_fn_violation": 0.005140831152627206,
            "auditor_fp_violation": 0.023978533967557027,
            "ave_precision_score": 0.7584949147410573,
            "fpr": 0.14270032930845225,
            "logloss": 2.087557358911912,
            "mae": 0.2654868950620707,
            "precision": 0.7394789579158316,
            "recall": 0.8004338394793926
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8471580079679062,
            "auditor_fn_violation": 0.031195509056617207,
            "auditor_fp_violation": 0.02377213917849517,
            "ave_precision_score": 0.8475110080907644,
            "fpr": 0.125,
            "logloss": 0.8882994063911392,
            "mae": 0.2620011071950641,
            "precision": 0.7733598409542743,
            "recall": 0.7890466531440162
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8388051388813548,
            "auditor_fn_violation": 0.022332494386517167,
            "auditor_fp_violation": 0.02351262349066959,
            "ave_precision_score": 0.8391176237045516,
            "fpr": 0.12843029637760703,
            "logloss": 0.9136970160764037,
            "mae": 0.26134217031688467,
            "precision": 0.7515923566878981,
            "recall": 0.7678958785249458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8465065498572579,
            "auditor_fn_violation": 0.022830593217323232,
            "auditor_fp_violation": 0.020867353347569405,
            "ave_precision_score": 0.846824307729225,
            "fpr": 0.13157894736842105,
            "logloss": 0.7555851415863575,
            "mae": 0.26084550119362104,
            "precision": 0.7665369649805448,
            "recall": 0.7991886409736308
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.847092515456666,
            "auditor_fn_violation": 0.008274380850106315,
            "auditor_fp_violation": 0.023666300768386397,
            "ave_precision_score": 0.8473525619604563,
            "fpr": 0.13830954994511527,
            "logloss": 0.76620176453022,
            "mae": 0.2627023582521444,
            "precision": 0.7423312883435583,
            "recall": 0.7874186550976139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8438239399932046,
            "auditor_fn_violation": 0.02500133447208284,
            "auditor_fp_violation": 0.022285726248796222,
            "ave_precision_score": 0.8441354288625932,
            "fpr": 0.12719298245614036,
            "logloss": 0.725802547395033,
            "mae": 0.2649032737318349,
            "precision": 0.7702970297029703,
            "recall": 0.7890466531440162
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8431680591849582,
            "auditor_fn_violation": 0.0062623371613754345,
            "auditor_fp_violation": 0.016665447005732404,
            "ave_precision_score": 0.8434800000087077,
            "fpr": 0.13611416026344675,
            "logloss": 0.7309805659829707,
            "mae": 0.2646819561357934,
            "precision": 0.7459016393442623,
            "recall": 0.789587852494577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8596183496584705,
            "auditor_fn_violation": 0.024222892423757164,
            "auditor_fp_violation": 0.007042142946865976,
            "ave_precision_score": 0.8598946132093117,
            "fpr": 0.12609649122807018,
            "logloss": 0.5094023725452532,
            "mae": 0.3025993378451746,
            "precision": 0.77,
            "recall": 0.7809330628803245
        },
        "train": {
            "accuracy": 0.8013172338090011,
            "auc_prc": 0.8649512150261135,
            "auditor_fn_violation": 0.015779660976591246,
            "auditor_fp_violation": 0.012733260153677279,
            "ave_precision_score": 0.8652011654133088,
            "fpr": 0.09549945115257959,
            "logloss": 0.477468845294336,
            "mae": 0.2876916304518214,
            "precision": 0.8083700440528634,
            "recall": 0.7960954446854663
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 7376,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8482038156451288,
            "auditor_fn_violation": 0.027034180278281916,
            "auditor_fp_violation": 0.027613783862998786,
            "ave_precision_score": 0.8485387011679559,
            "fpr": 0.14473684210526316,
            "logloss": 0.7776309452527576,
            "mae": 0.26008937525515546,
            "precision": 0.7532710280373832,
            "recall": 0.8174442190669371
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8458737503284799,
            "auditor_fn_violation": 0.010955518357219907,
            "auditor_fp_violation": 0.02613489449932919,
            "ave_precision_score": 0.8462208215017717,
            "fpr": 0.14709110867178923,
            "logloss": 0.7802400821642886,
            "mae": 0.26090978297096445,
            "precision": 0.7367387033398821,
            "recall": 0.8134490238611713
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8428246051340582,
            "auditor_fn_violation": 0.019888082274652146,
            "auditor_fp_violation": 0.012676380689193155,
            "ave_precision_score": 0.8431361485581013,
            "fpr": 0.11842105263157894,
            "logloss": 0.7333024855884261,
            "mae": 0.2645208056851882,
            "precision": 0.7804878048780488,
            "recall": 0.7789046653144016
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8411093192902848,
            "auditor_fn_violation": 0.0037597834136166588,
            "auditor_fp_violation": 0.020095133552872308,
            "ave_precision_score": 0.8414347265489583,
            "fpr": 0.13391877058177826,
            "logloss": 0.7247571653914286,
            "mae": 0.26602852833018864,
            "precision": 0.7420718816067653,
            "recall": 0.7613882863340564
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7801661654699055,
            "auditor_fn_violation": 0.024669940571509914,
            "auditor_fp_violation": 0.023929154628815483,
            "ave_precision_score": 0.7808837493091327,
            "fpr": 0.15460526315789475,
            "logloss": 0.9719821976028893,
            "mae": 0.30666419592827465,
            "precision": 0.7207920792079208,
            "recall": 0.7383367139959433
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7637298703833377,
            "auditor_fn_violation": 0.011979398577520839,
            "auditor_fp_violation": 0.03293084522502745,
            "ave_precision_score": 0.7641567536908237,
            "fpr": 0.1690450054884742,
            "logloss": 1.0098131015485383,
            "mae": 0.31325062531501363,
            "precision": 0.6913827655310621,
            "recall": 0.7483731019522777
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8555789366058757,
            "auditor_fn_violation": 0.015090655136827872,
            "auditor_fp_violation": 0.008489302013984847,
            "ave_precision_score": 0.8557938994690647,
            "fpr": 0.1162280701754386,
            "logloss": 0.5585827251153238,
            "mae": 0.3125908915486692,
            "precision": 0.7796257796257796,
            "recall": 0.7606490872210954
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8510380979103295,
            "auditor_fn_violation": 0.010107840779482395,
            "auditor_fp_violation": 0.011220880595194537,
            "ave_precision_score": 0.851466831895112,
            "fpr": 0.10318331503841932,
            "logloss": 0.5593903193823856,
            "mae": 0.30419067324811166,
            "precision": 0.7911111111111111,
            "recall": 0.7722342733188721
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.846062985082443,
            "auditor_fn_violation": 0.023904843244012673,
            "auditor_fp_violation": 0.01907999413808986,
            "ave_precision_score": 0.8463676979812453,
            "fpr": 0.11403508771929824,
            "logloss": 0.7156396808196999,
            "mae": 0.26552378113304487,
            "precision": 0.7868852459016393,
            "recall": 0.7789046653144016
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8419199690437321,
            "auditor_fn_violation": 0.003116881879939328,
            "auditor_fp_violation": 0.01862422246615441,
            "ave_precision_score": 0.8422905003471469,
            "fpr": 0.12403951701427003,
            "logloss": 0.7256368957240156,
            "mae": 0.26536730108905754,
            "precision": 0.7569892473118279,
            "recall": 0.7635574837310195
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8458691227106065,
            "auditor_fn_violation": 0.02272383545069571,
            "auditor_fp_violation": 0.01701262404220576,
            "ave_precision_score": 0.8461738545559583,
            "fpr": 0.13596491228070176,
            "logloss": 0.7626937031783093,
            "mae": 0.26170833748205796,
            "precision": 0.7596899224806202,
            "recall": 0.795131845841785
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8446297208844658,
            "auditor_fn_violation": 0.007581475863809646,
            "auditor_fp_violation": 0.01957311867300891,
            "ave_precision_score": 0.8449160323500997,
            "fpr": 0.1394072447859495,
            "logloss": 0.7746416309829255,
            "mae": 0.26332652463650846,
            "precision": 0.7434343434343434,
            "recall": 0.7982646420824295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8399299924069852,
            "auditor_fn_violation": 0.022125547133553972,
            "auditor_fp_violation": 0.013576602604362943,
            "ave_precision_score": 0.8402029802699779,
            "fpr": 0.12719298245614036,
            "logloss": 0.8679603535585673,
            "mae": 0.27302886919273783,
            "precision": 0.7661290322580645,
            "recall": 0.77079107505071
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8236984322490655,
            "auditor_fn_violation": 0.0038597903188553467,
            "auditor_fp_violation": 0.02257836321502622,
            "ave_precision_score": 0.8242720293873149,
            "fpr": 0.14050493962678376,
            "logloss": 0.9427617082525482,
            "mae": 0.2770270459358014,
            "precision": 0.7366255144032922,
            "recall": 0.7765726681127982
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8378902395269044,
            "auditor_fn_violation": 0.023673534749653037,
            "auditor_fp_violation": 0.021228488883306123,
            "ave_precision_score": 0.8382421913286835,
            "fpr": 0.12828947368421054,
            "logloss": 0.797040626235726,
            "mae": 0.2643330093874765,
            "precision": 0.7687747035573123,
            "recall": 0.7890466531440162
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8346121890380117,
            "auditor_fn_violation": 0.007679101652256946,
            "auditor_fp_violation": 0.02321746554457861,
            "ave_precision_score": 0.8349960131515883,
            "fpr": 0.14709110867178923,
            "logloss": 0.8077260411240621,
            "mae": 0.2673587869995285,
            "precision": 0.7298387096774194,
            "recall": 0.7852494577006508
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 7376,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8404452191254996,
            "auditor_fn_violation": 0.01701674317639942,
            "auditor_fp_violation": 0.01803322446928778,
            "ave_precision_score": 0.8407335963538128,
            "fpr": 0.12171052631578948,
            "logloss": 0.7424330960657214,
            "mae": 0.26693626599822656,
            "precision": 0.7748478701825557,
            "recall": 0.7748478701825557
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8327098423390971,
            "auditor_fn_violation": 0.007283836264884964,
            "auditor_fp_violation": 0.02317355775094524,
            "ave_precision_score": 0.833049941725104,
            "fpr": 0.13721185510428102,
            "logloss": 0.7494294234384692,
            "mae": 0.2722587128497602,
            "precision": 0.7362869198312236,
            "recall": 0.7570498915401301
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8393321908927097,
            "auditor_fn_violation": 0.012174833635813672,
            "auditor_fp_violation": 0.022136561570991923,
            "ave_precision_score": 0.839697476757973,
            "fpr": 0.24890350877192982,
            "logloss": 1.1596294851599431,
            "mae": 0.294419109515921,
            "precision": 0.6686131386861314,
            "recall": 0.9290060851926978
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.835729611645597,
            "auditor_fn_violation": 0.0018429843965416673,
            "auditor_fp_violation": 0.028379070618368113,
            "ave_precision_score": 0.8358270213011795,
            "fpr": 0.25905598243688255,
            "logloss": 1.1972619854758186,
            "mae": 0.30676822856541763,
            "precision": 0.642965204236006,
            "recall": 0.9219088937093276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8399280021728253,
            "auditor_fn_violation": 0.019672342621259033,
            "auditor_fp_violation": 0.018352489218272414,
            "ave_precision_score": 0.8402256497417133,
            "fpr": 0.13048245614035087,
            "logloss": 0.7360487812518131,
            "mae": 0.26784729811835195,
            "precision": 0.7643564356435644,
            "recall": 0.7829614604462475
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8349627262102233,
            "auditor_fn_violation": 0.008483919127749298,
            "auditor_fp_violation": 0.023080863519941458,
            "ave_precision_score": 0.8353023676072178,
            "fpr": 0.14050493962678376,
            "logloss": 0.7350837351063619,
            "mae": 0.2702809625201108,
            "precision": 0.7366255144032922,
            "recall": 0.7765726681127982
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 7376,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8380494494215044,
            "auditor_fn_violation": 0.030005604782747948,
            "auditor_fp_violation": 0.02174925679353515,
            "ave_precision_score": 0.8382513115147765,
            "fpr": 0.10197368421052631,
            "logloss": 1.8769395522301717,
            "mae": 0.27046396764376374,
            "precision": 0.7937915742793792,
            "recall": 0.7261663286004056
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8206533988834263,
            "auditor_fn_violation": 0.02806622362020235,
            "auditor_fp_violation": 0.024759116965483604,
            "ave_precision_score": 0.8207258001888078,
            "fpr": 0.10976948408342481,
            "logloss": 2.024185133147987,
            "mae": 0.2738666776940887,
            "precision": 0.7619047619047619,
            "recall": 0.6941431670281996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8311755840982784,
            "auditor_fn_violation": 0.010847033913383866,
            "auditor_fp_violation": 0.004668592722857272,
            "ave_precision_score": 0.8315231972591397,
            "fpr": 0.1875,
            "logloss": 0.6458859273005912,
            "mae": 0.3258322917732142,
            "precision": 0.7111486486486487,
            "recall": 0.8539553752535497
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8344123892075472,
            "auditor_fn_violation": 0.0027263787261501368,
            "auditor_fp_violation": 0.0013611416026344657,
            "ave_precision_score": 0.8347982066151833,
            "fpr": 0.1778265642151482,
            "logloss": 0.6470802750941724,
            "mae": 0.3170716193621866,
            "precision": 0.705989110707804,
            "recall": 0.8438177874186551
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8494801950992857,
            "auditor_fn_violation": 0.015764563538664105,
            "auditor_fp_violation": 0.02385326382782733,
            "ave_precision_score": 0.8497590701231521,
            "fpr": 0.20065789473684212,
            "logloss": 0.8200239434178117,
            "mae": 0.27402762168822736,
            "precision": 0.7067307692307693,
            "recall": 0.8945233265720081
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8455563392211647,
            "auditor_fn_violation": 0.0073147907831731225,
            "auditor_fp_violation": 0.03236004390779363,
            "ave_precision_score": 0.8459535461711578,
            "fpr": 0.21734357848518113,
            "logloss": 0.8413696708538773,
            "mae": 0.2845667899257421,
            "precision": 0.6710963455149501,
            "recall": 0.8763557483731019
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8458691227106065,
            "auditor_fn_violation": 0.02272383545069571,
            "auditor_fp_violation": 0.01701262404220576,
            "ave_precision_score": 0.8461738545559583,
            "fpr": 0.13596491228070176,
            "logloss": 0.7626936803521421,
            "mae": 0.26170833853979536,
            "precision": 0.7596899224806202,
            "recall": 0.795131845841785
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8446297208844658,
            "auditor_fn_violation": 0.007581475863809646,
            "auditor_fp_violation": 0.01957311867300891,
            "ave_precision_score": 0.8449160323500997,
            "fpr": 0.1394072447859495,
            "logloss": 0.7746416107380121,
            "mae": 0.263326528400773,
            "precision": 0.7434343434343434,
            "recall": 0.7982646420824295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8424701108408693,
            "auditor_fn_violation": 0.024262926586242482,
            "auditor_fp_violation": 0.02265994640539296,
            "ave_precision_score": 0.8427268839955506,
            "fpr": 0.13048245614035087,
            "logloss": 0.7232119918725339,
            "mae": 0.2666458745754272,
            "precision": 0.7648221343873518,
            "recall": 0.7849898580121704
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8414698972907917,
            "auditor_fn_violation": 0.00730288519921614,
            "auditor_fp_violation": 0.020300036589828038,
            "ave_precision_score": 0.8417991065562656,
            "fpr": 0.1394072447859495,
            "logloss": 0.7334620532959801,
            "mae": 0.26716435731170934,
            "precision": 0.7397540983606558,
            "recall": 0.7830802603036876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 7376,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8593627775129022,
            "auditor_fn_violation": 0.013544891640866877,
            "auditor_fp_violation": 0.003721266172591381,
            "ave_precision_score": 0.8595842220954514,
            "fpr": 0.11403508771929824,
            "logloss": 0.5668153473345722,
            "mae": 0.3111758299764808,
            "precision": 0.7828810020876826,
            "recall": 0.7606490872210954
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8517098170768302,
            "auditor_fn_violation": 0.008238664098235355,
            "auditor_fp_violation": 0.007435053055250646,
            "ave_precision_score": 0.8522323755987999,
            "fpr": 0.10098792535675083,
            "logloss": 0.5793519500470671,
            "mae": 0.30358321276245487,
            "precision": 0.7932584269662921,
            "recall": 0.7657266811279827
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8463089905333119,
            "auditor_fn_violation": 0.023820326678765883,
            "auditor_fp_violation": 0.020867353347569405,
            "ave_precision_score": 0.8466273151807948,
            "fpr": 0.13157894736842105,
            "logloss": 0.7567957372797867,
            "mae": 0.2609359111481997,
            "precision": 0.7660818713450293,
            "recall": 0.7971602434077079
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8467591895401885,
            "auditor_fn_violation": 0.008274380850106315,
            "auditor_fp_violation": 0.023666300768386397,
            "ave_precision_score": 0.847019797400568,
            "fpr": 0.13830954994511527,
            "logloss": 0.7673538184513046,
            "mae": 0.2628203248713958,
            "precision": 0.7423312883435583,
            "recall": 0.7874186550976139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8477119272724387,
            "auditor_fn_violation": 0.020557542436212235,
            "auditor_fp_violation": 0.019294581920194287,
            "ave_precision_score": 0.8480116737920953,
            "fpr": 0.12719298245614036,
            "logloss": 0.7657082249917024,
            "mae": 0.25864255168511224,
            "precision": 0.7716535433070866,
            "recall": 0.795131845841785
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8475142274723069,
            "auditor_fn_violation": 0.010188798750389909,
            "auditor_fp_violation": 0.020763507744846938,
            "ave_precision_score": 0.847755693619358,
            "fpr": 0.13391877058177826,
            "logloss": 0.7823361281938734,
            "mae": 0.26109670774678795,
            "precision": 0.7474120082815735,
            "recall": 0.7830802603036876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8516246722030292,
            "auditor_fn_violation": 0.01932093163944344,
            "auditor_fp_violation": 0.023146694301385926,
            "ave_precision_score": 0.8518842875584803,
            "fpr": 0.17982456140350878,
            "logloss": 0.7918675663093788,
            "mae": 0.2615045970581582,
            "precision": 0.7215619694397284,
            "recall": 0.8620689655172413
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8497157266438093,
            "auditor_fn_violation": 0.009969736005581343,
            "auditor_fp_violation": 0.02579582875960483,
            "ave_precision_score": 0.8500286934073626,
            "fpr": 0.18111964873765093,
            "logloss": 0.8189859390573305,
            "mae": 0.27010798224171256,
            "precision": 0.7053571428571429,
            "recall": 0.8568329718004338
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.836049725171246,
            "auditor_fn_violation": 0.019109640226326465,
            "auditor_fp_violation": 0.018844470962609394,
            "ave_precision_score": 0.8364063845908641,
            "fpr": 0.12938596491228072,
            "logloss": 0.7684811381124396,
            "mae": 0.26801150438426524,
            "precision": 0.7635270541082164,
            "recall": 0.7728194726166329
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8334700963077838,
            "auditor_fn_violation": 0.006167092489719531,
            "auditor_fp_violation": 0.019846322722283215,
            "ave_precision_score": 0.8338557065158714,
            "fpr": 0.13611416026344675,
            "logloss": 0.779042213651795,
            "mae": 0.26905109196113136,
            "precision": 0.7422037422037422,
            "recall": 0.7744034707158352
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8333609924571398,
            "auditor_fn_violation": 0.01954334365325078,
            "auditor_fp_violation": 0.020885671816773442,
            "ave_precision_score": 0.8337860735635921,
            "fpr": 0.125,
            "logloss": 0.7692339868158057,
            "mae": 0.2694236232736284,
            "precision": 0.7673469387755102,
            "recall": 0.7626774847870182
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.831257965204273,
            "auditor_fn_violation": 0.00751004236006772,
            "auditor_fp_violation": 0.02186608122941823,
            "ave_precision_score": 0.8316050786592702,
            "fpr": 0.13830954994511527,
            "logloss": 0.7751658253122753,
            "mae": 0.2713847607505705,
            "precision": 0.738045738045738,
            "recall": 0.7700650759219089
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.842637258651363,
            "auditor_fn_violation": 0.02174077434966728,
            "auditor_fp_violation": 0.017959950592471637,
            "ave_precision_score": 0.8430052738327947,
            "fpr": 0.125,
            "logloss": 0.7804145469519843,
            "mae": 0.26091689733713885,
            "precision": 0.7738095238095238,
            "recall": 0.7910750507099391
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8441772492914403,
            "auditor_fn_violation": 0.013865243076307654,
            "auditor_fp_violation": 0.021173313818758387,
            "ave_precision_score": 0.8444519169915551,
            "fpr": 0.13391877058177826,
            "logloss": 0.7833055268526725,
            "mae": 0.26316928680338125,
            "precision": 0.7436974789915967,
            "recall": 0.7678958785249458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7879032509043419,
            "auditor_fn_violation": 0.022290132023771395,
            "auditor_fp_violation": 0.019336452706946367,
            "ave_precision_score": 0.7886289782235055,
            "fpr": 0.14912280701754385,
            "logloss": 0.9097613614393031,
            "mae": 0.3044985145460652,
            "precision": 0.7246963562753036,
            "recall": 0.7261663286004056
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7735474805010871,
            "auditor_fn_violation": 0.01394381993042377,
            "auditor_fp_violation": 0.0331625808025369,
            "ave_precision_score": 0.7739744476198369,
            "fpr": 0.15916575192096596,
            "logloss": 0.9329358186303036,
            "mae": 0.307623446108441,
            "precision": 0.6997929606625258,
            "recall": 0.7331887201735358
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8596980436371079,
            "auditor_fn_violation": 0.024734440055514038,
            "auditor_fp_violation": 0.012715634551773228,
            "ave_precision_score": 0.8600510695335424,
            "fpr": 0.12719298245614036,
            "logloss": 0.5746242757600651,
            "mae": 0.29783776630286907,
            "precision": 0.7684630738522954,
            "recall": 0.7809330628803245
        },
        "train": {
            "accuracy": 0.7969264544456641,
            "auc_prc": 0.8664803254420056,
            "auditor_fn_violation": 0.005140831152627206,
            "auditor_fp_violation": 0.00854006586169045,
            "ave_precision_score": 0.8668500710747516,
            "fpr": 0.10208562019758508,
            "logloss": 0.5731509773456115,
            "mae": 0.28449290647177655,
            "precision": 0.7987012987012987,
            "recall": 0.8004338394793926
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 7376,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.7988499200644577,
            "auditor_fn_violation": 0.018179958008611802,
            "auditor_fp_violation": 0.004891031277477705,
            "ave_precision_score": 0.7992010379840878,
            "fpr": 0.07346491228070176,
            "logloss": 0.8899713252235228,
            "mae": 0.35778970831880075,
            "precision": 0.7981927710843374,
            "recall": 0.537525354969574
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7946750317208136,
            "auditor_fn_violation": 0.018846539403911223,
            "auditor_fp_violation": 0.00822051469691426,
            "ave_precision_score": 0.7952187723561858,
            "fpr": 0.050493962678375415,
            "logloss": 0.8714903434716643,
            "mae": 0.34127198684600185,
            "precision": 0.8357142857142857,
            "recall": 0.5075921908893709
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8477484887280519,
            "auditor_fn_violation": 0.020510835913312694,
            "auditor_fp_violation": 0.021335782774358335,
            "ave_precision_score": 0.8480423118025731,
            "fpr": 0.11842105263157894,
            "logloss": 0.6989465272475244,
            "mae": 0.26523538932162705,
            "precision": 0.780040733197556,
            "recall": 0.7768762677484787
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.84517701996245,
            "auditor_fn_violation": 0.001640589469272881,
            "auditor_fp_violation": 0.01877302110013416,
            "ave_precision_score": 0.8454985280487111,
            "fpr": 0.12733260153677278,
            "logloss": 0.7072174691703484,
            "mae": 0.2651683234498149,
            "precision": 0.7521367521367521,
            "recall": 0.7635574837310195
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8350228633133561,
            "auditor_fn_violation": 0.01867816091954023,
            "auditor_fp_violation": 0.01560471883766696,
            "ave_precision_score": 0.8362719950143042,
            "fpr": 0.12390350877192982,
            "logloss": 0.7172578848885027,
            "mae": 0.2702118279789949,
            "precision": 0.7735470941883767,
            "recall": 0.7829614604462475
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8259779173704818,
            "auditor_fn_violation": 0.005402753999680932,
            "auditor_fp_violation": 0.020944017563117465,
            "ave_precision_score": 0.8268148777209028,
            "fpr": 0.13830954994511527,
            "logloss": 0.7147868922270395,
            "mae": 0.27280889083211884,
            "precision": 0.738045738045738,
            "recall": 0.7700650759219089
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8509419081975435,
            "auditor_fn_violation": 0.023064125831820933,
            "auditor_fp_violation": 0.017747979734539215,
            "ave_precision_score": 0.851234909141484,
            "fpr": 0.1162280701754386,
            "logloss": 0.6895308312441185,
            "mae": 0.2627852152781035,
            "precision": 0.7845528455284553,
            "recall": 0.7829614604462475
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8509694075914876,
            "auditor_fn_violation": 0.00651235442447217,
            "auditor_fp_violation": 0.017265520185388464,
            "ave_precision_score": 0.8512654848924889,
            "fpr": 0.12403951701427003,
            "logloss": 0.6906031960919168,
            "mae": 0.2618473922294472,
            "precision": 0.7585470085470085,
            "recall": 0.7700650759219089
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8418185934564963,
            "auditor_fn_violation": 0.022428027472331948,
            "auditor_fp_violation": 0.017986119834191693,
            "ave_precision_score": 0.8421162601259203,
            "fpr": 0.12719298245614036,
            "logloss": 0.7105805257554636,
            "mae": 0.2694188287762378,
            "precision": 0.7707509881422925,
            "recall": 0.7910750507099391
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8386706701083242,
            "auditor_fn_violation": 0.0075219479440247075,
            "auditor_fp_violation": 0.02167093547993658,
            "ave_precision_score": 0.8390179582710153,
            "fpr": 0.14050493962678376,
            "logloss": 0.7020488940444856,
            "mae": 0.27086871652055733,
            "precision": 0.7355371900826446,
            "recall": 0.7722342733188721
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8452319630162075,
            "auditor_fn_violation": 0.022830593217323232,
            "auditor_fp_violation": 0.02220198467529205,
            "ave_precision_score": 0.8455616486895139,
            "fpr": 0.1337719298245614,
            "logloss": 0.7649514596393479,
            "mae": 0.2609239615871049,
            "precision": 0.7635658914728682,
            "recall": 0.7991886409736308
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8457980842480974,
            "auditor_fn_violation": 0.002016805922313685,
            "auditor_fp_violation": 0.023666300768386397,
            "ave_precision_score": 0.8460603016701874,
            "fpr": 0.13830954994511527,
            "logloss": 0.7754246215726728,
            "mae": 0.26295883049368146,
            "precision": 0.7423312883435583,
            "recall": 0.7874186550976139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8404992872689374,
            "auditor_fn_violation": 0.021135813672111318,
            "auditor_fp_violation": 0.016180442155508103,
            "ave_precision_score": 0.8408397999310495,
            "fpr": 0.13267543859649122,
            "logloss": 0.8040569707479397,
            "mae": 0.2624571910949962,
            "precision": 0.763671875,
            "recall": 0.7931034482758621
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8375749499104721,
            "auditor_fn_violation": 0.0079124510978139,
            "auditor_fp_violation": 0.025729967069154782,
            "ave_precision_score": 0.8378619628508717,
            "fpr": 0.14050493962678376,
            "logloss": 0.8000799811748801,
            "mae": 0.26701664299709754,
            "precision": 0.7366255144032922,
            "recall": 0.7765726681127982
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.845330810006582,
            "auditor_fn_violation": 0.024349667271627348,
            "auditor_fp_violation": 0.02020003768370808,
            "ave_precision_score": 0.8456516073080631,
            "fpr": 0.13048245614035087,
            "logloss": 0.7625891722471893,
            "mae": 0.2612964164954179,
            "precision": 0.7680311890838206,
            "recall": 0.7991886409736308
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8462467368356889,
            "auditor_fn_violation": 0.0052551247586142885,
            "auditor_fp_violation": 0.02067325283571168,
            "ave_precision_score": 0.8465091516510057,
            "fpr": 0.13721185510428102,
            "logloss": 0.7724074587387255,
            "mae": 0.262789677234465,
            "precision": 0.7433264887063655,
            "recall": 0.7852494577006508
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8442170966540459,
            "auditor_fn_violation": 0.020501939432760397,
            "auditor_fp_violation": 0.018389126156680487,
            "ave_precision_score": 0.8445400984843331,
            "fpr": 0.12390350877192982,
            "logloss": 0.761178936889704,
            "mae": 0.26217914146038024,
            "precision": 0.774,
            "recall": 0.7849898580121704
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8434711513704201,
            "auditor_fn_violation": 0.00954113498312979,
            "auditor_fp_violation": 0.022661300158555928,
            "ave_precision_score": 0.8437270227002525,
            "fpr": 0.13391877058177826,
            "logloss": 0.7513919096351505,
            "mae": 0.26610579392098654,
            "precision": 0.7442348008385744,
            "recall": 0.7700650759219089
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8379903396790286,
            "auditor_fn_violation": 0.02010827016832142,
            "auditor_fp_violation": 0.019744692877779177,
            "ave_precision_score": 0.8383152412992173,
            "fpr": 0.13157894736842105,
            "logloss": 0.76855337073058,
            "mae": 0.26721799251228767,
            "precision": 0.7619047619047619,
            "recall": 0.7789046653144016
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8358036210362445,
            "auditor_fn_violation": 0.006829042957728035,
            "auditor_fp_violation": 0.018980363458958413,
            "ave_precision_score": 0.8361275365410104,
            "fpr": 0.1394072447859495,
            "logloss": 0.7800095711354755,
            "mae": 0.2684023250206126,
            "precision": 0.7392197125256673,
            "recall": 0.7809110629067245
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8529511488939722,
            "auditor_fn_violation": 0.01898064125831821,
            "auditor_fp_violation": 0.016612234643888966,
            "ave_precision_score": 0.8532512477980224,
            "fpr": 0.11842105263157894,
            "logloss": 0.6898702196667907,
            "mae": 0.2603643806643366,
            "precision": 0.7835671342685371,
            "recall": 0.7931034482758621
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8541592569224427,
            "auditor_fn_violation": 0.005728967000102389,
            "auditor_fp_violation": 0.021602634467618006,
            "ave_precision_score": 0.8543960828037247,
            "fpr": 0.12843029637760703,
            "logloss": 0.6956796080185306,
            "mae": 0.2617168930914759,
            "precision": 0.7536842105263157,
            "recall": 0.7765726681127982
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8579348532454933,
            "auditor_fn_violation": 0.027238799330984668,
            "auditor_fp_violation": 0.010174601180756184,
            "ave_precision_score": 0.8583728677822842,
            "fpr": 0.10197368421052631,
            "logloss": 0.5806850786137934,
            "mae": 0.3035226623998263,
            "precision": 0.7956043956043956,
            "recall": 0.7342799188640974
        },
        "train": {
            "accuracy": 0.7892425905598244,
            "auc_prc": 0.8658413800400891,
            "auditor_fn_violation": 0.019217993623369237,
            "auditor_fp_violation": 0.012791803878521772,
            "ave_precision_score": 0.8661923615728477,
            "fpr": 0.07464324917672886,
            "logloss": 0.567691023594086,
            "mae": 0.28960332446822573,
            "precision": 0.8320987654320988,
            "recall": 0.7310195227765727
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8376144512499916,
            "auditor_fn_violation": 0.022686025408348454,
            "auditor_fp_violation": 0.019263178830130217,
            "ave_precision_score": 0.837965253066535,
            "fpr": 0.12719298245614036,
            "logloss": 0.77481099313556,
            "mae": 0.2666935531341141,
            "precision": 0.7675350701402806,
            "recall": 0.7768762677484787
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8364900473667496,
            "auditor_fn_violation": 0.007379080936540855,
            "auditor_fp_violation": 0.019758507135016472,
            "ave_precision_score": 0.836824786703428,
            "fpr": 0.13172338090010977,
            "logloss": 0.781677154627174,
            "mae": 0.26659987612475505,
            "precision": 0.7463002114164905,
            "recall": 0.7657266811279827
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7914711729056048,
            "auditor_fn_violation": 0.02110467599017829,
            "auditor_fp_violation": 0.018093413725243906,
            "ave_precision_score": 0.7920219120952358,
            "fpr": 0.13486842105263158,
            "logloss": 0.904720876885591,
            "mae": 0.3037590634443248,
            "precision": 0.7399577167019028,
            "recall": 0.7099391480730223
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7770694708318708,
            "auditor_fn_violation": 0.012734212600393846,
            "auditor_fp_violation": 0.03214294426149531,
            "ave_precision_score": 0.7774263763196492,
            "fpr": 0.150384193194292,
            "logloss": 0.9134950551764858,
            "mae": 0.3077816487329138,
            "precision": 0.7072649572649573,
            "recall": 0.7180043383947939
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8460867131165055,
            "auditor_fn_violation": 0.021124693071420945,
            "auditor_fp_violation": 0.01947253276389064,
            "ave_precision_score": 0.8464033862715352,
            "fpr": 0.12390350877192982,
            "logloss": 0.7307574492009045,
            "mae": 0.2610211748215948,
            "precision": 0.7762376237623763,
            "recall": 0.795131845841785
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8474941184721956,
            "auditor_fn_violation": 0.004400303830502587,
            "auditor_fp_violation": 0.019978046103183322,
            "ave_precision_score": 0.8477488450343409,
            "fpr": 0.13172338090010977,
            "logloss": 0.7347447716783002,
            "mae": 0.26417048147241945,
            "precision": 0.7484276729559748,
            "recall": 0.7744034707158352
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8465955339711819,
            "auditor_fn_violation": 0.02360013878509662,
            "auditor_fp_violation": 0.01986507138969141,
            "ave_precision_score": 0.8469183998043894,
            "fpr": 0.1425438596491228,
            "logloss": 0.8416642901712799,
            "mae": 0.2610686786174276,
            "precision": 0.7537878787878788,
            "recall": 0.8073022312373225
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8484115960635421,
            "auditor_fn_violation": 0.004900338356696063,
            "auditor_fp_violation": 0.021119648737650938,
            "ave_precision_score": 0.8486368828063253,
            "fpr": 0.141602634467618,
            "logloss": 0.8935492387542043,
            "mae": 0.2642799246891762,
            "precision": 0.7409638554216867,
            "recall": 0.8004338394793926
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8469966098569273,
            "auditor_fn_violation": 0.023168659478310382,
            "auditor_fp_violation": 0.019294581920194287,
            "ave_precision_score": 0.8473156053214792,
            "fpr": 0.12719298245614036,
            "logloss": 0.7533763576485452,
            "mae": 0.26065497267608817,
            "precision": 0.7721021611001965,
            "recall": 0.7971602434077079
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.845500049739395,
            "auditor_fn_violation": 0.01039833702803289,
            "auditor_fp_violation": 0.020670813513843152,
            "ave_precision_score": 0.8457896795024105,
            "fpr": 0.1394072447859495,
            "logloss": 0.7650314056893154,
            "mae": 0.2629481388999827,
            "precision": 0.741869918699187,
            "recall": 0.7917570498915402
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8413190096296652,
            "auditor_fn_violation": 0.022686025408348454,
            "auditor_fp_violation": 0.01737375957794247,
            "ave_precision_score": 0.8416491114138329,
            "fpr": 0.1206140350877193,
            "logloss": 0.768139106259628,
            "mae": 0.2650789603757236,
            "precision": 0.780439121756487,
            "recall": 0.7931034482758621
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8403764411614527,
            "auditor_fn_violation": 0.007938643382519267,
            "auditor_fp_violation": 0.01943163800463471,
            "ave_precision_score": 0.8406441867594744,
            "fpr": 0.1394072447859495,
            "logloss": 0.7755054033474179,
            "mae": 0.26690186712594866,
            "precision": 0.7365145228215768,
            "recall": 0.7700650759219089
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8525644148172771,
            "auditor_fn_violation": 0.02389594676346038,
            "auditor_fp_violation": 0.01676139932169325,
            "ave_precision_score": 0.8528379676511455,
            "fpr": 0.12390350877192982,
            "logloss": 0.7191215233068955,
            "mae": 0.2600112998055694,
            "precision": 0.7757936507936508,
            "recall": 0.7931034482758621
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8501381945969093,
            "auditor_fn_violation": 0.010191179867181306,
            "auditor_fp_violation": 0.024295645810464686,
            "ave_precision_score": 0.8504436082085545,
            "fpr": 0.12733260153677278,
            "logloss": 0.7182042258641143,
            "mae": 0.2594570694932962,
            "precision": 0.7573221757322176,
            "recall": 0.7852494577006508
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.838681781737616,
            "auditor_fn_violation": 0.022643767125725068,
            "auditor_fp_violation": 0.019370472721182433,
            "ave_precision_score": 0.8390602159890116,
            "fpr": 0.1337719298245614,
            "logloss": 0.7491909227667549,
            "mae": 0.26457982754502263,
            "precision": 0.7626459143968871,
            "recall": 0.795131845841785
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8394545170482466,
            "auditor_fn_violation": 0.010253088903757644,
            "auditor_fp_violation": 0.022729601170874497,
            "ave_precision_score": 0.8396962667142637,
            "fpr": 0.14489571899012074,
            "logloss": 0.7428890135625391,
            "mae": 0.269576302745232,
            "precision": 0.7322515212981744,
            "recall": 0.7830802603036876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8447671062775628,
            "auditor_fn_violation": 0.024349667271627348,
            "auditor_fp_violation": 0.021963844575639574,
            "ave_precision_score": 0.8450978982084104,
            "fpr": 0.13157894736842105,
            "logloss": 0.766000885543692,
            "mae": 0.26157239986074354,
            "precision": 0.7665369649805448,
            "recall": 0.7991886409736308
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8452393524646292,
            "auditor_fn_violation": 0.004628891042476747,
            "auditor_fp_violation": 0.02067325283571168,
            "ave_precision_score": 0.8455016052934732,
            "fpr": 0.13721185510428102,
            "logloss": 0.7773601301244711,
            "mae": 0.26341526564685835,
            "precision": 0.7433264887063655,
            "recall": 0.7852494577006508
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.848359789055517,
            "auditor_fn_violation": 0.020477474111241598,
            "auditor_fp_violation": 0.019577209730770845,
            "ave_precision_score": 0.8486707405362129,
            "fpr": 0.1600877192982456,
            "logloss": 0.7705808424807042,
            "mae": 0.2595603887573575,
            "precision": 0.7383512544802867,
            "recall": 0.8356997971602435
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8481036451369561,
            "auditor_fn_violation": 0.010705501094123167,
            "auditor_fp_violation": 0.02306378826686182,
            "ave_precision_score": 0.8483749806499931,
            "fpr": 0.15916575192096596,
            "logloss": 0.7953074788551183,
            "mae": 0.2652028937886508,
            "precision": 0.722753346080306,
            "recall": 0.8199566160520607
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8571789882345733,
            "auditor_fn_violation": 0.016718711077897592,
            "auditor_fp_violation": 0.011132395427710086,
            "ave_precision_score": 0.8574436941276715,
            "fpr": 0.12280701754385964,
            "logloss": 0.5039300655105445,
            "mae": 0.31238273483471324,
            "precision": 0.7704918032786885,
            "recall": 0.7626774847870182
        },
        "train": {
            "accuracy": 0.7936333699231614,
            "auc_prc": 0.8724958791455493,
            "auditor_fn_violation": 0.006679032599869997,
            "auditor_fp_violation": 0.007200878155872672,
            "ave_precision_score": 0.872716016198422,
            "fpr": 0.09440175631174534,
            "logloss": 0.45543429515056383,
            "mae": 0.29376903227883716,
            "precision": 0.8067415730337079,
            "recall": 0.7787418655097614
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8467756864777041,
            "auditor_fn_violation": 0.02531493541155119,
            "auditor_fp_violation": 0.021296528911778256,
            "ave_precision_score": 0.8471045109910427,
            "fpr": 0.13048245614035087,
            "logloss": 0.7452510434916192,
            "mae": 0.26103279848499333,
            "precision": 0.7684824902723736,
            "recall": 0.8012170385395537
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8474668578315303,
            "auditor_fn_violation": 0.007186210476437663,
            "auditor_fp_violation": 0.023666300768386397,
            "ave_precision_score": 0.8477314903857307,
            "fpr": 0.13830954994511527,
            "logloss": 0.75510760516385,
            "mae": 0.26275779153668155,
            "precision": 0.7412731006160165,
            "recall": 0.7830802603036876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8461003342825528,
            "auditor_fn_violation": 0.022686025408348458,
            "auditor_fp_violation": 0.023298475903362224,
            "ave_precision_score": 0.8464256575931773,
            "fpr": 0.1337719298245614,
            "logloss": 0.7549519203626768,
            "mae": 0.26079581697131626,
            "precision": 0.7640232108317214,
            "recall": 0.8012170385395537
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8466906711279628,
            "auditor_fn_violation": 0.007436227739534396,
            "auditor_fp_violation": 0.022253933406512987,
            "ave_precision_score": 0.8469509576265004,
            "fpr": 0.1394072447859495,
            "logloss": 0.7636183438360553,
            "mae": 0.26287850576649324,
            "precision": 0.7408163265306122,
            "recall": 0.7874186550976139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8466823161065367,
            "auditor_fn_violation": 0.022830593217323232,
            "auditor_fp_violation": 0.020867353347569405,
            "ave_precision_score": 0.8470016073531047,
            "fpr": 0.13157894736842105,
            "logloss": 0.7573371299403683,
            "mae": 0.2606241225712547,
            "precision": 0.7665369649805448,
            "recall": 0.7991886409736308
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.847240929316073,
            "auditor_fn_violation": 0.006143281321805555,
            "auditor_fp_violation": 0.021283083302841815,
            "ave_precision_score": 0.8475008427382787,
            "fpr": 0.13721185510428102,
            "logloss": 0.7678442842840352,
            "mae": 0.26246203350458014,
            "precision": 0.7443762781186094,
            "recall": 0.789587852494577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8373335588067482,
            "auditor_fn_violation": 0.021222554357496173,
            "auditor_fp_violation": 0.020388456224092452,
            "ave_precision_score": 0.8376696939706371,
            "fpr": 0.1524122807017544,
            "logloss": 0.8269503530313044,
            "mae": 0.26632222727849447,
            "precision": 0.7406716417910447,
            "recall": 0.8052738336713996
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8388186381742877,
            "auditor_fn_violation": 0.007924356681770888,
            "auditor_fp_violation": 0.02534943285766558,
            "ave_precision_score": 0.839068041423457,
            "fpr": 0.15148188803512624,
            "logloss": 0.8204138934760951,
            "mae": 0.2680434871453884,
            "precision": 0.7283464566929134,
            "recall": 0.8026030368763557
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8620681856358096,
            "auditor_fn_violation": 0.008053539019963703,
            "auditor_fp_violation": 0.01635839299920446,
            "ave_precision_score": 0.8623301450244085,
            "fpr": 0.2324561403508772,
            "logloss": 0.8039544596592985,
            "mae": 0.28800373308337857,
            "precision": 0.6831091180866966,
            "recall": 0.9269776876267748
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.8668935797349242,
            "auditor_fn_violation": 0.0033788047269930534,
            "auditor_fp_violation": 0.02653494328576657,
            "ave_precision_score": 0.8671054336219861,
            "fpr": 0.24039517014270034,
            "logloss": 0.79341287763917,
            "mae": 0.29596066895535184,
            "precision": 0.6594090202177294,
            "recall": 0.9197396963123644
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8466823161065367,
            "auditor_fn_violation": 0.022830593217323232,
            "auditor_fp_violation": 0.020867353347569405,
            "ave_precision_score": 0.8470016073531047,
            "fpr": 0.13157894736842105,
            "logloss": 0.7573371607927846,
            "mae": 0.26062412730910794,
            "precision": 0.7665369649805448,
            "recall": 0.7991886409736308
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.847240929316073,
            "auditor_fn_violation": 0.006143281321805555,
            "auditor_fp_violation": 0.021283083302841815,
            "ave_precision_score": 0.8475008427382787,
            "fpr": 0.13721185510428102,
            "logloss": 0.7678443180316497,
            "mae": 0.2624620384858816,
            "precision": 0.7443762781186094,
            "recall": 0.789587852494577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8477460365535635,
            "auditor_fn_violation": 0.023168659478310382,
            "auditor_fp_violation": 0.01821640916132814,
            "ave_precision_score": 0.8480632062696617,
            "fpr": 0.12390350877192982,
            "logloss": 0.7478757438046905,
            "mae": 0.26035824938169205,
            "precision": 0.7766798418972332,
            "recall": 0.7971602434077079
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8461862234888338,
            "auditor_fn_violation": 0.012924701943705635,
            "auditor_fp_violation": 0.02046103183315039,
            "ave_precision_score": 0.8464868524374302,
            "fpr": 0.13830954994511527,
            "logloss": 0.7614719482044906,
            "mae": 0.26336662246579806,
            "precision": 0.7423312883435583,
            "recall": 0.7874186550976139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8458510726442212,
            "auditor_fn_violation": 0.023820326678765883,
            "auditor_fp_violation": 0.021534669011430727,
            "ave_precision_score": 0.8461747842407596,
            "fpr": 0.13267543859649122,
            "logloss": 0.7599567867238557,
            "mae": 0.26102286364968447,
            "precision": 0.7645914396887159,
            "recall": 0.7971602434077079
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8463151042948474,
            "auditor_fn_violation": 0.008274380850106315,
            "auditor_fp_violation": 0.023666300768386397,
            "ave_precision_score": 0.8465763962964676,
            "fpr": 0.13830954994511527,
            "logloss": 0.7707426699001747,
            "mae": 0.26298474454439663,
            "precision": 0.7423312883435583,
            "recall": 0.7874186550976139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8478365964580707,
            "auditor_fn_violation": 0.023168659478310382,
            "auditor_fp_violation": 0.017266465686890258,
            "ave_precision_score": 0.848147790373126,
            "fpr": 0.12280701754385964,
            "logloss": 0.74523398636575,
            "mae": 0.26049940528195564,
            "precision": 0.7782178217821782,
            "recall": 0.7971602434077079
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8464459178992179,
            "auditor_fn_violation": 0.012924701943705635,
            "auditor_fp_violation": 0.02046103183315039,
            "ave_precision_score": 0.846762480817425,
            "fpr": 0.13830954994511527,
            "logloss": 0.7591310816594102,
            "mae": 0.2631899407910204,
            "precision": 0.7423312883435583,
            "recall": 0.7874186550976139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7751087133625134,
            "auditor_fn_violation": 0.016780986441763644,
            "auditor_fp_violation": 0.0017010007118033786,
            "ave_precision_score": 0.776749252277837,
            "fpr": 0.1611842105263158,
            "logloss": 0.8206376426108553,
            "mae": 0.30637949444754936,
            "precision": 0.7210626185958254,
            "recall": 0.77079107505071
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7821320750995242,
            "auditor_fn_violation": 0.004862240488033698,
            "auditor_fp_violation": 0.016158068057080137,
            "ave_precision_score": 0.782811441394701,
            "fpr": 0.1525795828759605,
            "logloss": 0.7953406232112201,
            "mae": 0.29488640865005,
            "precision": 0.7191919191919192,
            "recall": 0.7722342733188721
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8419456062766828,
            "auditor_fn_violation": 0.01780852994555355,
            "auditor_fp_violation": 0.018847087886781393,
            "ave_precision_score": 0.842280443416533,
            "fpr": 0.12171052631578948,
            "logloss": 0.7466665957521691,
            "mae": 0.26547272221096735,
            "precision": 0.7739307535641547,
            "recall": 0.77079107505071
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8376857960150824,
            "auditor_fn_violation": 0.007281455148093559,
            "auditor_fp_violation": 0.021709964629832906,
            "ave_precision_score": 0.8380358916320909,
            "fpr": 0.132821075740944,
            "logloss": 0.7597308783514535,
            "mae": 0.26748169846492814,
            "precision": 0.7479166666666667,
            "recall": 0.7787418655097614
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8677583483548363,
            "auditor_fn_violation": 0.027959414255720443,
            "auditor_fp_violation": 0.013113407025918017,
            "ave_precision_score": 0.8679629318178494,
            "fpr": 0.09539473684210527,
            "logloss": 0.5792290960422948,
            "mae": 0.29712839618007225,
            "precision": 0.8070953436807096,
            "recall": 0.7383367139959433
        },
        "train": {
            "accuracy": 0.7859495060373216,
            "auc_prc": 0.8677837388635032,
            "auditor_fn_violation": 0.0218229353931581,
            "auditor_fp_violation": 0.008527869252347848,
            "ave_precision_score": 0.8680261453157105,
            "fpr": 0.07464324917672886,
            "logloss": 0.5780734713655089,
            "mae": 0.2850411246859625,
            "precision": 0.8308457711442786,
            "recall": 0.7245119305856833
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8466823161065367,
            "auditor_fn_violation": 0.022830593217323232,
            "auditor_fp_violation": 0.020867353347569405,
            "ave_precision_score": 0.8470016073531047,
            "fpr": 0.13157894736842105,
            "logloss": 0.7573371723161428,
            "mae": 0.26062412515864847,
            "precision": 0.7665369649805448,
            "recall": 0.7991886409736308
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.847240929316073,
            "auditor_fn_violation": 0.006143281321805555,
            "auditor_fp_violation": 0.021283083302841815,
            "ave_precision_score": 0.8475008427382787,
            "fpr": 0.13721185510428102,
            "logloss": 0.7678443353030584,
            "mae": 0.2624620360171113,
            "precision": 0.7443762781186094,
            "recall": 0.789587852494577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8451476966715828,
            "auditor_fn_violation": 0.023417760933774602,
            "auditor_fp_violation": 0.01729001800443831,
            "ave_precision_score": 0.8454453440178562,
            "fpr": 0.125,
            "logloss": 0.7268757010745757,
            "mae": 0.2650897247733686,
            "precision": 0.7729083665338645,
            "recall": 0.7870182555780934
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8402793580501788,
            "auditor_fn_violation": 0.007855304294820358,
            "auditor_fp_violation": 0.016587388705939756,
            "ave_precision_score": 0.8406269445291451,
            "fpr": 0.13611416026344675,
            "logloss": 0.7427266331297325,
            "mae": 0.266721981759744,
            "precision": 0.7432712215320911,
            "recall": 0.7787418655097614
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8389717045117714,
            "auditor_fn_violation": 0.013616063485285225,
            "auditor_fp_violation": 0.02707469748356571,
            "ave_precision_score": 0.8393133831690843,
            "fpr": 0.23026315789473684,
            "logloss": 0.9856520067938559,
            "mae": 0.28412848484783904,
            "precision": 0.6813353566009105,
            "recall": 0.9107505070993914
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.8461657391060979,
            "auditor_fn_violation": 0.0024573125287222206,
            "auditor_fp_violation": 0.02698865715331139,
            "ave_precision_score": 0.8464269693963805,
            "fpr": 0.24588364434687157,
            "logloss": 1.007188837217128,
            "mae": 0.29590156565090814,
            "precision": 0.6505460218408736,
            "recall": 0.9045553145336226
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8462655579798428,
            "auditor_fn_violation": 0.021155830753353973,
            "auditor_fp_violation": 0.020100594565171877,
            "ave_precision_score": 0.8465648515556787,
            "fpr": 0.1162280701754386,
            "logloss": 0.7044256919749269,
            "mae": 0.2658362496962131,
            "precision": 0.7827868852459017,
            "recall": 0.7748478701825557
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8437874697159261,
            "auditor_fn_violation": 0.001640589469272881,
            "auditor_fp_violation": 0.017226491035492137,
            "ave_precision_score": 0.8441109155245126,
            "fpr": 0.1251372118551043,
            "logloss": 0.7129484858931538,
            "mae": 0.26591664616116595,
            "precision": 0.7553648068669528,
            "recall": 0.7635574837310195
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8445207573038567,
            "auditor_fn_violation": 0.02535941781431266,
            "auditor_fp_violation": 0.018449315412636603,
            "ave_precision_score": 0.8448141477178741,
            "fpr": 0.12390350877192982,
            "logloss": 0.7270692622492151,
            "mae": 0.26605429325611435,
            "precision": 0.774,
            "recall": 0.7849898580121704
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8408565862853246,
            "auditor_fn_violation": 0.011827007102871389,
            "auditor_fp_violation": 0.0184803024759117,
            "ave_precision_score": 0.8412048627022213,
            "fpr": 0.13611416026344675,
            "logloss": 0.742765310062558,
            "mae": 0.2670735352142374,
            "precision": 0.7453798767967146,
            "recall": 0.7874186550976139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8457823771028286,
            "auditor_fn_violation": 0.021155830753353973,
            "auditor_fp_violation": 0.0206108947787129,
            "ave_precision_score": 0.8460831544298395,
            "fpr": 0.11732456140350878,
            "logloss": 0.7015204983058382,
            "mae": 0.2661688066062567,
            "precision": 0.7811860940695297,
            "recall": 0.7748478701825557
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8436677092467546,
            "auditor_fn_violation": 0.0051146388679218354,
            "auditor_fp_violation": 0.01877302110013416,
            "ave_precision_score": 0.8439772923848193,
            "fpr": 0.12733260153677278,
            "logloss": 0.7068299754582743,
            "mae": 0.26655242843236443,
            "precision": 0.7526652452025586,
            "recall": 0.7657266811279827
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8433886573181006,
            "auditor_fn_violation": 0.022837265577737447,
            "auditor_fp_violation": 0.021963844575639574,
            "ave_precision_score": 0.8437162810817154,
            "fpr": 0.13157894736842105,
            "logloss": 0.7832917488838018,
            "mae": 0.2618445400987084,
            "precision": 0.7647058823529411,
            "recall": 0.7910750507099391
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8430488997351584,
            "auditor_fn_violation": 0.010862654802355405,
            "auditor_fp_violation": 0.022361263568727894,
            "ave_precision_score": 0.8433147600653192,
            "fpr": 0.1394072447859495,
            "logloss": 0.7933260926542358,
            "mae": 0.2641733392025282,
            "precision": 0.7397540983606558,
            "recall": 0.7830802603036876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8398155254769897,
            "auditor_fn_violation": 0.021589534180278282,
            "auditor_fp_violation": 0.017802935142151328,
            "ave_precision_score": 0.8401573469481387,
            "fpr": 0.12828947368421054,
            "logloss": 0.7583656303167492,
            "mae": 0.2653631236246407,
            "precision": 0.766,
            "recall": 0.7768762677484787
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8394508518674629,
            "auditor_fn_violation": 0.00609089675239481,
            "auditor_fp_violation": 0.019290157336260525,
            "ave_precision_score": 0.8398006547050326,
            "fpr": 0.1350164654226125,
            "logloss": 0.7634240513789055,
            "mae": 0.2648786944655221,
            "precision": 0.7442827442827443,
            "recall": 0.7765726681127982
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8458403872049688,
            "auditor_fn_violation": 0.024349667271627348,
            "auditor_fp_violation": 0.0226311602395009,
            "ave_precision_score": 0.8461699548660108,
            "fpr": 0.13267543859649122,
            "logloss": 0.7483652540858184,
            "mae": 0.26128963169170283,
            "precision": 0.7650485436893204,
            "recall": 0.7991886409736308
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8468529754204227,
            "auditor_fn_violation": 0.007450514440282785,
            "auditor_fp_violation": 0.023666300768386397,
            "ave_precision_score": 0.8471127039209618,
            "fpr": 0.13830954994511527,
            "logloss": 0.7581079776328526,
            "mae": 0.2631587067916274,
            "precision": 0.7418032786885246,
            "recall": 0.7852494577006508
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8394481211523481,
            "auditor_fn_violation": 0.022459165154264972,
            "auditor_fp_violation": 0.017098982539881926,
            "ave_precision_score": 0.8397724488933724,
            "fpr": 0.13706140350877194,
            "logloss": 0.7543652962907349,
            "mae": 0.26443389864307587,
            "precision": 0.7577519379844961,
            "recall": 0.7931034482758621
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8366922145122149,
            "auditor_fn_violation": 0.005676582430691646,
            "auditor_fp_violation": 0.02121234296865472,
            "ave_precision_score": 0.8370384113914847,
            "fpr": 0.150384193194292,
            "logloss": 0.7531822252273301,
            "mae": 0.2695508990490667,
            "precision": 0.7281746031746031,
            "recall": 0.7960954446854663
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8465065498572579,
            "auditor_fn_violation": 0.022830593217323232,
            "auditor_fp_violation": 0.020867353347569405,
            "ave_precision_score": 0.846824307729225,
            "fpr": 0.13157894736842105,
            "logloss": 0.7555851481647923,
            "mae": 0.26084550577793714,
            "precision": 0.7665369649805448,
            "recall": 0.7991886409736308
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.847092515456666,
            "auditor_fn_violation": 0.008274380850106315,
            "auditor_fp_violation": 0.023666300768386397,
            "ave_precision_score": 0.8473525619604563,
            "fpr": 0.13830954994511527,
            "logloss": 0.7662017807131891,
            "mae": 0.26270236342735875,
            "precision": 0.7423312883435583,
            "recall": 0.7874186550976139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8443055554946188,
            "auditor_fn_violation": 0.02377806839614249,
            "auditor_fp_violation": 0.01680850395678935,
            "ave_precision_score": 0.8446041805845196,
            "fpr": 0.11951754385964912,
            "logloss": 0.7355980952946728,
            "mae": 0.2644938357179195,
            "precision": 0.7797979797979798,
            "recall": 0.7829614604462475
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8411592901601157,
            "auditor_fn_violation": 0.0034430948803607898,
            "auditor_fp_violation": 0.01852421026954507,
            "ave_precision_score": 0.8415296648671158,
            "fpr": 0.12733260153677278,
            "logloss": 0.7458922995853787,
            "mae": 0.2658255623913223,
            "precision": 0.7547568710359408,
            "recall": 0.7744034707158352
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8478040530883191,
            "auditor_fn_violation": 0.023215366001209926,
            "auditor_fp_violation": 0.019723757484403137,
            "ave_precision_score": 0.8481243481397507,
            "fpr": 0.12609649122807018,
            "logloss": 0.7500069556949853,
            "mae": 0.25971839127934104,
            "precision": 0.7749510763209393,
            "recall": 0.8032454361054767
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8493623214468251,
            "auditor_fn_violation": 0.00678380173869148,
            "auditor_fp_violation": 0.021358702280765952,
            "ave_precision_score": 0.8496184119579979,
            "fpr": 0.13391877058177826,
            "logloss": 0.7581904479647376,
            "mae": 0.2610855327213569,
            "precision": 0.7489711934156379,
            "recall": 0.789587852494577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8470167880025973,
            "auditor_fn_violation": 0.02140715632895627,
            "auditor_fp_violation": 0.02168121676506302,
            "ave_precision_score": 0.8473430919055109,
            "fpr": 0.12938596491228072,
            "logloss": 0.7463478846815693,
            "mae": 0.2605607546266109,
            "precision": 0.7713178294573644,
            "recall": 0.8073022312373225
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8484924701233015,
            "auditor_fn_violation": 0.006667127015913005,
            "auditor_fp_violation": 0.023183315038419326,
            "ave_precision_score": 0.8487484487035657,
            "fpr": 0.13830954994511527,
            "logloss": 0.7571482988984612,
            "mae": 0.26243829901143084,
            "precision": 0.7428571428571429,
            "recall": 0.789587852494577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8262025793929083,
            "auditor_fn_violation": 0.03405572755417957,
            "auditor_fp_violation": 0.03353064941590253,
            "ave_precision_score": 0.8265346211544062,
            "fpr": 0.13267543859649122,
            "logloss": 1.4716885801495243,
            "mae": 0.276316734403672,
            "precision": 0.7570281124497992,
            "recall": 0.7647058823529411
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7977384906745619,
            "auditor_fn_violation": 0.02747094442235298,
            "auditor_fp_violation": 0.031445298207098425,
            "ave_precision_score": 0.7984322231164411,
            "fpr": 0.1350164654226125,
            "logloss": 1.343269782654994,
            "mae": 0.2855102027943703,
            "precision": 0.7296703296703296,
            "recall": 0.720173535791757
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 7376,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.7704019639631351,
            "auditor_fn_violation": 0.00396338208604676,
            "auditor_fp_violation": 0.009533454758614918,
            "ave_precision_score": 0.5655965109380311,
            "fpr": 0.41885964912280704,
            "logloss": 13.627738545356396,
            "mae": 0.42808056093427066,
            "precision": 0.5604142692750288,
            "recall": 0.9878296146044625
        },
        "train": {
            "accuracy": 0.5488474204171241,
            "auc_prc": 0.7483127981894867,
            "auditor_fn_violation": 0.001323900936017011,
            "auditor_fp_violation": 0.01473106476399563,
            "ave_precision_score": 0.5271722815892279,
            "fpr": 0.4489571899012075,
            "logloss": 14.742376193742446,
            "mae": 0.4534885975033124,
            "precision": 0.5288018433179723,
            "recall": 0.9956616052060737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8472214561066205,
            "auditor_fn_violation": 0.02140715632895627,
            "auditor_fp_violation": 0.02168121676506302,
            "ave_precision_score": 0.8475416821562294,
            "fpr": 0.12938596491228072,
            "logloss": 0.7464131053116428,
            "mae": 0.26045714803646186,
            "precision": 0.7713178294573644,
            "recall": 0.8073022312373225
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8486725583947583,
            "auditor_fn_violation": 0.006667127015913005,
            "auditor_fp_violation": 0.023183315038419326,
            "ave_precision_score": 0.8489282768668514,
            "fpr": 0.13830954994511527,
            "logloss": 0.7572373964464775,
            "mae": 0.26232226489228294,
            "precision": 0.7428571428571429,
            "recall": 0.789587852494577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 7376,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8407310105830159,
            "auditor_fn_violation": 0.023473363937226436,
            "auditor_fp_violation": 0.02067631788301302,
            "ave_precision_score": 0.8410994172504154,
            "fpr": 0.13486842105263158,
            "logloss": 0.7602339739777515,
            "mae": 0.263152247991153,
            "precision": 0.7620889748549323,
            "recall": 0.7991886409736308
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8392471239619017,
            "auditor_fn_violation": 0.010717406678080152,
            "auditor_fp_violation": 0.020822051469691433,
            "ave_precision_score": 0.8395517972857305,
            "fpr": 0.1437980241492865,
            "logloss": 0.7718730805714922,
            "mae": 0.2658139270798722,
            "precision": 0.7358870967741935,
            "recall": 0.7917570498915402
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8453441338773152,
            "auditor_fn_violation": 0.023820326678765883,
            "auditor_fp_violation": 0.021534669011430727,
            "ave_precision_score": 0.8456733304972377,
            "fpr": 0.13267543859649122,
            "logloss": 0.7656714605919052,
            "mae": 0.26096622629468724,
            "precision": 0.7645914396887159,
            "recall": 0.7971602434077079
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8458036452431034,
            "auditor_fn_violation": 0.0052551247586142885,
            "auditor_fp_violation": 0.023666300768386397,
            "ave_precision_score": 0.8460715136007129,
            "fpr": 0.13830954994511527,
            "logloss": 0.77732695672573,
            "mae": 0.26289472003799236,
            "precision": 0.7418032786885246,
            "recall": 0.7852494577006508
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8412981058423331,
            "auditor_fn_violation": 0.022443596313298463,
            "auditor_fp_violation": 0.020673700958841017,
            "ave_precision_score": 0.8416075868775337,
            "fpr": 0.1337719298245614,
            "logloss": 0.7438791031298794,
            "mae": 0.26634994241546367,
            "precision": 0.7598425196850394,
            "recall": 0.7829614604462475
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8394338667036951,
            "auditor_fn_violation": 0.004452688399913332,
            "auditor_fp_violation": 0.019148676667886333,
            "ave_precision_score": 0.839752340848678,
            "fpr": 0.13721185510428102,
            "logloss": 0.7566594146144737,
            "mae": 0.2667721641807142,
            "precision": 0.7433264887063655,
            "recall": 0.7852494577006508
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8376795292008522,
            "auditor_fn_violation": 0.017915287712181066,
            "auditor_fp_violation": 0.017331888791190388,
            "ave_precision_score": 0.8380206665648793,
            "fpr": 0.12280701754385964,
            "logloss": 0.8251568448758585,
            "mae": 0.26405933791735514,
            "precision": 0.7741935483870968,
            "recall": 0.7789046653144016
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8296036114410079,
            "auditor_fn_violation": 0.007148112607775298,
            "auditor_fp_violation": 0.022136845956824004,
            "ave_precision_score": 0.8299484128231229,
            "fpr": 0.13721185510428102,
            "logloss": 0.8337462886831019,
            "mae": 0.27083251097627,
            "precision": 0.7362869198312236,
            "recall": 0.7570498915401301
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7962179052795013,
            "auditor_fn_violation": 0.060313689904273875,
            "auditor_fp_violation": 0.05484287987271283,
            "ave_precision_score": 0.7967570948993234,
            "fpr": 0.17653508771929824,
            "logloss": 1.6558777589703562,
            "mae": 0.29829421146676854,
            "precision": 0.7067395264116576,
            "recall": 0.7870182555780934
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.755834312504349,
            "auditor_fn_violation": 0.062382878817823145,
            "auditor_fp_violation": 0.06376875228686424,
            "ave_precision_score": 0.7563107583123421,
            "fpr": 0.18551042810098792,
            "logloss": 1.634059882406515,
            "mae": 0.31532979181484294,
            "precision": 0.6718446601941748,
            "recall": 0.7505422993492408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8625153945019557,
            "auditor_fn_violation": 0.023344364969218184,
            "auditor_fp_violation": 0.014775153875141313,
            "ave_precision_score": 0.8627684842852024,
            "fpr": 0.125,
            "logloss": 0.5669563483732696,
            "mae": 0.30046743810358983,
            "precision": 0.772,
            "recall": 0.7829614604462475
        },
        "train": {
            "accuracy": 0.7903402854006586,
            "auc_prc": 0.8652745447457362,
            "auditor_fn_violation": 0.008307716485185883,
            "auditor_fp_violation": 0.009376753262593003,
            "ave_precision_score": 0.8655480935084887,
            "fpr": 0.10647639956092206,
            "logloss": 0.5810100622366537,
            "mae": 0.289817627179501,
            "precision": 0.790948275862069,
            "recall": 0.7960954446854663
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 7376,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.8497755835871852,
            "auditor_fn_violation": 0.02099124586313655,
            "auditor_fp_violation": 0.015620420382698995,
            "ave_precision_score": 0.8500836930267627,
            "fpr": 0.1162280701754386,
            "logloss": 0.7226621281459529,
            "mae": 0.26046982023295795,
            "precision": 0.7854251012145749,
            "recall": 0.7870182555780934
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8496863039690771,
            "auditor_fn_violation": 0.011472220700953165,
            "auditor_fp_violation": 0.01778265642151482,
            "ave_precision_score": 0.8499417147928103,
            "fpr": 0.12843029637760703,
            "logloss": 0.7304656259056069,
            "mae": 0.2620681969130874,
            "precision": 0.7521186440677966,
            "recall": 0.7700650759219089
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8455709784491873,
            "auditor_fn_violation": 0.037042720899612115,
            "auditor_fp_violation": 0.0324577105053804,
            "ave_precision_score": 0.8458230118517033,
            "fpr": 0.1513157894736842,
            "logloss": 0.996419218412094,
            "mae": 0.26190490436026104,
            "precision": 0.7439703153988868,
            "recall": 0.8133874239350912
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8308869987271594,
            "auditor_fn_violation": 0.01925847260882299,
            "auditor_fp_violation": 0.036528844981095265,
            "ave_precision_score": 0.8311903723717748,
            "fpr": 0.150384193194292,
            "logloss": 1.01781075301493,
            "mae": 0.2648482035645996,
            "precision": 0.7303149606299213,
            "recall": 0.8047722342733189
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8479050982204995,
            "auditor_fn_violation": 0.023215366001209926,
            "auditor_fp_violation": 0.019294581920194287,
            "ave_precision_score": 0.8482258319109057,
            "fpr": 0.12719298245614036,
            "logloss": 0.7491477016205987,
            "mae": 0.25965763830953914,
            "precision": 0.7734375,
            "recall": 0.8032454361054767
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.849523470825401,
            "auditor_fn_violation": 0.00678380173869148,
            "auditor_fp_violation": 0.021358702280765952,
            "ave_precision_score": 0.8497793375520726,
            "fpr": 0.13391877058177826,
            "logloss": 0.7573029881465623,
            "mae": 0.2610155296284004,
            "precision": 0.7489711934156379,
            "recall": 0.789587852494577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 7376,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8389655359975985,
            "auditor_fn_violation": 0.019641204939325998,
            "auditor_fp_violation": 0.01662793618892099,
            "ave_precision_score": 0.8393120931863362,
            "fpr": 0.12280701754385964,
            "logloss": 0.7665326933054609,
            "mae": 0.2648922919039063,
            "precision": 0.7755511022044088,
            "recall": 0.7849898580121704
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8370966136495634,
            "auditor_fn_violation": 0.004097901997995103,
            "auditor_fp_violation": 0.023790706183680944,
            "ave_precision_score": 0.8374116752644815,
            "fpr": 0.1350164654226125,
            "logloss": 0.7595200918959744,
            "mae": 0.26825797704462157,
            "precision": 0.7399577167019028,
            "recall": 0.7592190889370932
        }
    }
]
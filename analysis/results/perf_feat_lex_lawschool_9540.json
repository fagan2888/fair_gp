[
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(0)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8252790873746634,
            "auditor_fn_violation": 0.014724310776942364,
            "auditor_fp_violation": 0.009889207616196893,
            "ave_precision_score": 0.8255803814998759,
            "fpr": 0.13706140350877194,
            "logloss": 0.9310212713641078,
            "mae": 0.2815252174898591,
            "precision": 0.7464503042596349,
            "recall": 0.7510204081632653
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8456443250142232,
            "auditor_fn_violation": 0.01352719633597033,
            "auditor_fp_violation": 0.025355031838061773,
            "ave_precision_score": 0.8460337013636472,
            "fpr": 0.11745334796926454,
            "logloss": 0.781226497244709,
            "mae": 0.24633473783113238,
            "precision": 0.773784355179704,
            "recall": 0.7887931034482759
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(1)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8159581850654218,
            "auditor_fn_violation": 0.019698800572860723,
            "auditor_fp_violation": 0.026346969319032177,
            "ave_precision_score": 0.8162828570821267,
            "fpr": 0.15679824561403508,
            "logloss": 1.0191067104596143,
            "mae": 0.28725300386734776,
            "precision": 0.7270992366412213,
            "recall": 0.7775510204081633
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8426779675434939,
            "auditor_fn_violation": 0.01517847004050116,
            "auditor_fp_violation": 0.032388137037500896,
            "ave_precision_score": 0.8430014137827353,
            "fpr": 0.1394072447859495,
            "logloss": 0.8805702112183862,
            "mae": 0.24564938371599124,
            "precision": 0.7475149105367793,
            "recall": 0.8103448275862069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(2)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8058599340153529,
            "auditor_fn_violation": 0.013455513784461156,
            "auditor_fp_violation": 0.0037779579279953457,
            "ave_precision_score": 0.8061721063148395,
            "fpr": 0.10526315789473684,
            "logloss": 0.9672329284141419,
            "mae": 0.3050389838124163,
            "precision": 0.7741176470588236,
            "recall": 0.6714285714285714
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8209674660753091,
            "auditor_fn_violation": 0.011606230364510398,
            "auditor_fp_violation": 0.023407667165172383,
            "ave_precision_score": 0.8215075749210363,
            "fpr": 0.10318331503841932,
            "logloss": 0.815644533560727,
            "mae": 0.26865168060366634,
            "precision": 0.7777777777777778,
            "recall": 0.709051724137931
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(3)",
        "seed": 9540,
        "test": {
            "accuracy": 0.44298245614035087,
            "auc_prc": 0.4650060696397646,
            "auditor_fn_violation": 0.028385696383816692,
            "auditor_fp_violation": 0.033341647958759456,
            "ave_precision_score": 0.44225420406886673,
            "fpr": 0.26644736842105265,
            "logloss": 9.60922820357666,
            "mae": 0.5611338621174532,
            "precision": 0.4807692307692308,
            "recall": 0.45918367346938777
        },
        "train": {
            "accuracy": 0.43029637760702527,
            "auc_prc": 0.42223883270261486,
            "auditor_fn_violation": 0.027884760967485524,
            "auditor_fp_violation": 0.02255799733311723,
            "ave_precision_score": 0.3999467883941932,
            "fpr": 0.2810098792535675,
            "logloss": 10.489921730021665,
            "mae": 0.5805954117415187,
            "precision": 0.43982494529540483,
            "recall": 0.4331896551724138
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(4)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8240691149999708,
            "auditor_fn_violation": 0.01636680988184748,
            "auditor_fp_violation": 0.020750187079072096,
            "ave_precision_score": 0.8243563092042346,
            "fpr": 0.18859649122807018,
            "logloss": 0.9488988667638701,
            "mae": 0.28724214141426074,
            "precision": 0.7064846416382252,
            "recall": 0.8448979591836735
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8412136011357378,
            "auditor_fn_violation": 0.015204492978538176,
            "auditor_fp_violation": 0.03442145097085829,
            "ave_precision_score": 0.8416989641874433,
            "fpr": 0.17672886937431395,
            "logloss": 0.8387573632015239,
            "mae": 0.2538505587756082,
            "precision": 0.7165492957746479,
            "recall": 0.8771551724137931
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(5)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.828905370019061,
            "auditor_fn_violation": 0.04244987468671679,
            "auditor_fp_violation": 0.029636443003242707,
            "ave_precision_score": 0.8296439673188647,
            "fpr": 0.11293859649122807,
            "logloss": 0.7136527114209302,
            "mae": 0.2912531859771126,
            "precision": 0.7794432548179872,
            "recall": 0.7428571428571429
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8657463662837036,
            "auditor_fn_violation": 0.03546926454445664,
            "auditor_fp_violation": 0.026808802186549192,
            "ave_precision_score": 0.8659244266991295,
            "fpr": 0.10867178924259056,
            "logloss": 0.5726299436661184,
            "mae": 0.2596070803439136,
            "precision": 0.7809734513274337,
            "recall": 0.7607758620689655
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(6)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8354896152710087,
            "auditor_fn_violation": 0.024236931614751173,
            "auditor_fp_violation": 0.004999168537457391,
            "ave_precision_score": 0.836228328289281,
            "fpr": 0.08991228070175439,
            "logloss": 0.5693215391450385,
            "mae": 0.3302871388086555,
            "precision": 0.8114942528735632,
            "recall": 0.7204081632653061
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8459291191612164,
            "auditor_fn_violation": 0.01988152466028237,
            "auditor_fp_violation": 0.016156005274828904,
            "ave_precision_score": 0.8462245466707226,
            "fpr": 0.09220636663007684,
            "logloss": 0.5189624713670778,
            "mae": 0.3101491579206678,
            "precision": 0.7975903614457831,
            "recall": 0.7133620689655172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(7)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7880897378701571,
            "auditor_fn_violation": 0.016496598639455788,
            "auditor_fp_violation": 0.013035773675895902,
            "ave_precision_score": 0.7884575113080476,
            "fpr": 0.14144736842105263,
            "logloss": 1.1371398070182557,
            "mae": 0.31015024174645694,
            "precision": 0.728421052631579,
            "recall": 0.7061224489795919
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8020983453130861,
            "auditor_fn_violation": 0.011909042734395699,
            "auditor_fp_violation": 0.029114698060247978,
            "ave_precision_score": 0.8026149144085533,
            "fpr": 0.13062568605927552,
            "logloss": 0.9852585365935221,
            "mae": 0.2697684059012596,
            "precision": 0.7478813559322034,
            "recall": 0.7607758620689655
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(8)",
        "seed": 9540,
        "test": {
            "accuracy": 0.643640350877193,
            "auc_prc": 0.8402892682121456,
            "auditor_fn_violation": 0.004072681704260651,
            "auditor_fp_violation": 0.014108880019955118,
            "ave_precision_score": 0.7443872338336639,
            "fpr": 0.3344298245614035,
            "logloss": 6.282964433301997,
            "mae": 0.3582189040761566,
            "precision": 0.6064516129032258,
            "recall": 0.9591836734693877
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.8379938163762365,
            "auditor_fn_violation": 0.008821775994549379,
            "auditor_fp_violation": 0.018049344698281265,
            "ave_precision_score": 0.7402166248949664,
            "fpr": 0.3424807903402854,
            "logloss": 6.277626957923889,
            "mae": 0.36906932834231826,
            "precision": 0.5856573705179283,
            "recall": 0.9504310344827587
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(9)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.8620543451715655,
            "auditor_fn_violation": 0.03951843895452919,
            "auditor_fp_violation": 0.006981687037498962,
            "ave_precision_score": 0.8622630181173773,
            "fpr": 0.027412280701754384,
            "logloss": 0.6542678336144759,
            "mae": 0.3393395501783297,
            "precision": 0.9110320284697508,
            "recall": 0.5224489795918368
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8707700259548369,
            "auditor_fn_violation": 0.03722226427949582,
            "auditor_fp_violation": 0.008231483459678746,
            "ave_precision_score": 0.8711757050419677,
            "fpr": 0.024149286498353458,
            "logloss": 0.5789086886234998,
            "mae": 0.31119773971200176,
            "precision": 0.9163498098859315,
            "recall": 0.5193965517241379
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(10)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7706629205324176,
            "auditor_fn_violation": 0.047238632295023286,
            "auditor_fp_violation": 0.03742101105845182,
            "ave_precision_score": 0.7712442320074155,
            "fpr": 0.15460526315789475,
            "logloss": 0.7639836118494737,
            "mae": 0.345143137024008,
            "precision": 0.718562874251497,
            "recall": 0.7346938775510204
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8092169846771571,
            "auditor_fn_violation": 0.032481358113478936,
            "auditor_fp_violation": 0.037744003811235785,
            "ave_precision_score": 0.8095336113555355,
            "fpr": 0.14270032930845225,
            "logloss": 0.6226194192040838,
            "mae": 0.3014260386782461,
            "precision": 0.7352342158859471,
            "recall": 0.7780172413793104
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(11)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8182492127884791,
            "auditor_fn_violation": 0.010307017543859653,
            "auditor_fp_violation": 0.007425999833707495,
            "ave_precision_score": 0.8185617320703695,
            "fpr": 0.11513157894736842,
            "logloss": 0.8759774322044559,
            "mae": 0.2911027580421279,
            "precision": 0.7697368421052632,
            "recall": 0.7163265306122449
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8351883534302094,
            "auditor_fn_violation": 0.009330406147091106,
            "auditor_fp_violation": 0.02490563999047191,
            "ave_precision_score": 0.8356012682199032,
            "fpr": 0.10976948408342481,
            "logloss": 0.7566532127880222,
            "mae": 0.25753094530783455,
            "precision": 0.7767857142857143,
            "recall": 0.75
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(12)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.8112158621564096,
            "auditor_fn_violation": 0.009027031865377733,
            "auditor_fp_violation": 0.008675791968071842,
            "ave_precision_score": 0.8115890015373082,
            "fpr": 0.13048245614035087,
            "logloss": 1.0385670804906804,
            "mae": 0.2921865170599816,
            "precision": 0.7457264957264957,
            "recall": 0.7122448979591837
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.834110602642046,
            "auditor_fn_violation": 0.01468639993943753,
            "auditor_fp_violation": 0.028017003219413725,
            "ave_precision_score": 0.834412409658229,
            "fpr": 0.11525795828759605,
            "logloss": 0.9181271813846633,
            "mae": 0.2527803374426874,
            "precision": 0.7702407002188184,
            "recall": 0.7586206896551724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(13)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8102618318968238,
            "auditor_fn_violation": 0.017416308628714648,
            "auditor_fp_violation": 0.006895942462792052,
            "ave_precision_score": 0.8105523868003889,
            "fpr": 0.09649122807017543,
            "logloss": 1.1096133621639936,
            "mae": 0.30079135563012654,
            "precision": 0.784841075794621,
            "recall": 0.6551020408163265
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8310753529939028,
            "auditor_fn_violation": 0.015386653544797305,
            "auditor_fp_violation": 0.024559387255443658,
            "ave_precision_score": 0.8315220540317907,
            "fpr": 0.09659714599341383,
            "logloss": 0.929787186112074,
            "mae": 0.2636429550403913,
            "precision": 0.7864077669902912,
            "recall": 0.6982758620689655
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(14)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7310808716014224,
            "auditor_fn_violation": 0.013809076262083789,
            "auditor_fp_violation": 0.01546780161303733,
            "ave_precision_score": 0.7284062945200349,
            "fpr": 0.1611842105263158,
            "logloss": 1.5262083457097062,
            "mae": 0.30088871385249255,
            "precision": 0.7140077821011673,
            "recall": 0.7489795918367347
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.7356324629353534,
            "auditor_fn_violation": 0.01347988190317575,
            "auditor_fp_violation": 0.028837204733594128,
            "ave_precision_score": 0.7296654673521636,
            "fpr": 0.14050493962678376,
            "logloss": 1.4815508122176335,
            "mae": 0.2644775731357868,
            "precision": 0.7414141414141414,
            "recall": 0.790948275862069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(15)",
        "seed": 9540,
        "test": {
            "accuracy": 0.5975877192982456,
            "auc_prc": 0.5296537328608616,
            "auditor_fn_violation": 0.003513247404224857,
            "auditor_fp_violation": 0.024026669161054295,
            "ave_precision_score": 0.4795490969577294,
            "fpr": 0.2774122807017544,
            "logloss": 7.610368080679743,
            "mae": 0.42949555714943,
            "precision": 0.5977742448330684,
            "recall": 0.7673469387755102
        },
        "train": {
            "accuracy": 0.5433589462129528,
            "auc_prc": 0.47061147664722114,
            "auditor_fn_violation": 0.002491104886634627,
            "auditor_fp_violation": 0.010881176375249576,
            "ave_precision_score": 0.4297495396666283,
            "fpr": 0.3260153677277717,
            "logloss": 8.66973156348218,
            "mae": 0.46770104558819153,
            "precision": 0.5373831775700935,
            "recall": 0.7435344827586207
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(16)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8183601139302181,
            "auditor_fn_violation": 0.009018080916577161,
            "auditor_fp_violation": 0.00789629583437267,
            "ave_precision_score": 0.8187060145788781,
            "fpr": 0.13706140350877194,
            "logloss": 0.9730601650969777,
            "mae": 0.28042367930157186,
            "precision": 0.75,
            "recall": 0.7653061224489796
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8362901619435119,
            "auditor_fn_violation": 0.011189863355918093,
            "auditor_fp_violation": 0.019333672218988896,
            "ave_precision_score": 0.8366151110209505,
            "fpr": 0.13062568605927552,
            "logloss": 0.8359492742159407,
            "mae": 0.2523849591786184,
            "precision": 0.7510460251046025,
            "recall": 0.7737068965517241
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(17)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7844834696260337,
            "auditor_fn_violation": 0.000731740064446832,
            "auditor_fp_violation": 0.008192504365178347,
            "ave_precision_score": 0.7849970390263084,
            "fpr": 0.11732456140350878,
            "logloss": 0.8314384206722781,
            "mae": 0.28870719783798177,
            "precision": 0.7713675213675214,
            "recall": 0.736734693877551
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8032226025272717,
            "auditor_fn_violation": 0.008932964911616636,
            "auditor_fp_violation": 0.019527671978330965,
            "ave_precision_score": 0.8041004786780203,
            "fpr": 0.11306256860592755,
            "logloss": 0.7391404973958345,
            "mae": 0.260455409247748,
            "precision": 0.7775377969762419,
            "recall": 0.7758620689655172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(18)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8355441015043146,
            "auditor_fn_violation": 0.021585213032581457,
            "auditor_fp_violation": 0.006745239876943544,
            "ave_precision_score": 0.8362781514359547,
            "fpr": 0.08552631578947369,
            "logloss": 0.5705660871150876,
            "mae": 0.3303496921541585,
            "precision": 0.8186046511627907,
            "recall": 0.7183673469387755
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.845738802293535,
            "auditor_fn_violation": 0.019966690639312622,
            "auditor_fp_violation": 0.013572616074476263,
            "ave_precision_score": 0.8460409637791112,
            "fpr": 0.09110867178924259,
            "logloss": 0.5209101860902726,
            "mae": 0.31038125528670024,
            "precision": 0.7980535279805353,
            "recall": 0.7068965517241379
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(19)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8122319090068169,
            "auditor_fn_violation": 0.02506265664160401,
            "auditor_fp_violation": 0.028584123222748815,
            "ave_precision_score": 0.8125770887915339,
            "fpr": 0.15899122807017543,
            "logloss": 0.9869080520223268,
            "mae": 0.29117468312807054,
            "precision": 0.7184466019417476,
            "recall": 0.7551020408163265
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.838586311448965,
            "auditor_fn_violation": 0.022786630833869566,
            "auditor_fp_violation": 0.034070286849517586,
            "ave_precision_score": 0.8391065054625676,
            "fpr": 0.13391877058177826,
            "logloss": 0.8138475843275768,
            "mae": 0.25169708356339343,
            "precision": 0.7510204081632653,
            "recall": 0.7931034482758621
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(20)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8039410486985173,
            "auditor_fn_violation": 0.00782760472610097,
            "auditor_fp_violation": 0.012466741498295499,
            "ave_precision_score": 0.8042191189522978,
            "fpr": 0.21271929824561403,
            "logloss": 1.1008732792418907,
            "mae": 0.3011598261234757,
            "precision": 0.6835236541598695,
            "recall": 0.8551020408163266
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7939756637229995,
            "auditor_fn_violation": 0.012211855104281012,
            "auditor_fp_violation": 0.026548498711988947,
            "ave_precision_score": 0.7944430032988896,
            "fpr": 0.20856201975850713,
            "logloss": 1.0414181323857794,
            "mae": 0.2766953005512765,
            "precision": 0.6812080536912751,
            "recall": 0.875
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(21)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8246649452642402,
            "auditor_fn_violation": 0.013113139992839242,
            "auditor_fp_violation": 0.012144549763033183,
            "ave_precision_score": 0.8249449393115198,
            "fpr": 0.1425438596491228,
            "logloss": 0.9055249055295777,
            "mae": 0.28156879473278806,
            "precision": 0.74,
            "recall": 0.7551020408163265
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8436211310843333,
            "auditor_fn_violation": 0.01307770922442182,
            "auditor_fp_violation": 0.0274595608729498,
            "ave_precision_score": 0.8441669220896265,
            "fpr": 0.12733260153677278,
            "logloss": 0.7687641989503452,
            "mae": 0.24633430057460243,
            "precision": 0.7618069815195072,
            "recall": 0.7995689655172413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(22)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.8138392829903527,
            "auditor_fn_violation": 0.006261188686000716,
            "auditor_fp_violation": 0.018889789639976718,
            "ave_precision_score": 0.814080492735597,
            "fpr": 0.3366228070175439,
            "logloss": 2.0005700967624254,
            "mae": 0.357968172202226,
            "precision": 0.6053984575835476,
            "recall": 0.9612244897959183
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.8385774059024674,
            "auditor_fn_violation": 0.004272493281350543,
            "auditor_fp_violation": 0.020969163861037247,
            "ave_precision_score": 0.8382314797888514,
            "fpr": 0.3402854006586169,
            "logloss": 1.8768818233698878,
            "mae": 0.35166405545689594,
            "precision": 0.5926412614980289,
            "recall": 0.9719827586206896
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(23)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.829631551538335,
            "auditor_fn_violation": 0.04244987468671679,
            "auditor_fp_violation": 0.028966076328261415,
            "ave_precision_score": 0.830364303000303,
            "fpr": 0.11403508771929824,
            "logloss": 0.7098705039633667,
            "mae": 0.29099771824441495,
            "precision": 0.7777777777777778,
            "recall": 0.7428571428571429
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8658465780986514,
            "auditor_fn_violation": 0.03546926454445664,
            "auditor_fp_violation": 0.02596895512711896,
            "ave_precision_score": 0.866025916658457,
            "fpr": 0.10867178924259056,
            "logloss": 0.5707732781365831,
            "mae": 0.25945802612719715,
            "precision": 0.7809734513274337,
            "recall": 0.7607758620689655
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(24)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7941837134210947,
            "auditor_fn_violation": 0.017376029359112064,
            "auditor_fp_violation": 0.02132701421800949,
            "ave_precision_score": 0.7945100389166915,
            "fpr": 0.17543859649122806,
            "logloss": 1.0720339819891793,
            "mae": 0.2899348906383677,
            "precision": 0.7117117117117117,
            "recall": 0.8061224489795918
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.7962864322457033,
            "auditor_fn_violation": 0.01810013626556645,
            "auditor_fp_violation": 0.024326096405601932,
            "ave_precision_score": 0.7975443824289989,
            "fpr": 0.16465422612513722,
            "logloss": 1.0863163895747572,
            "mae": 0.25485654544152575,
            "precision": 0.7237569060773481,
            "recall": 0.8469827586206896
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(25)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.8136776049123146,
            "auditor_fn_violation": 0.011918188327962763,
            "auditor_fp_violation": 0.017042383803109677,
            "ave_precision_score": 0.8139916067586075,
            "fpr": 0.1787280701754386,
            "logloss": 0.9223354546358364,
            "mae": 0.2939776511884126,
            "precision": 0.7020109689213894,
            "recall": 0.7836734693877551
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8340156219762137,
            "auditor_fn_violation": 0.009813013361595823,
            "auditor_fp_violation": 0.0317914036005373,
            "ave_precision_score": 0.8344426482198659,
            "fpr": 0.14709110867178923,
            "logloss": 0.7990929056296004,
            "mae": 0.25344885544849505,
            "precision": 0.7398058252427184,
            "recall": 0.8211206896551724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(26)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8203851741774948,
            "auditor_fn_violation": 0.009107590404582892,
            "auditor_fp_violation": 0.014041323688367845,
            "ave_precision_score": 0.8206844586234978,
            "fpr": 0.1524122807017544,
            "logloss": 0.9005300062893123,
            "mae": 0.2907923056175921,
            "precision": 0.7252964426877471,
            "recall": 0.7489795918367347
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8366268629586645,
            "auditor_fn_violation": 0.011071577273931642,
            "auditor_fp_violation": 0.027051915809015833,
            "ave_precision_score": 0.8370037595988733,
            "fpr": 0.13172338090010977,
            "logloss": 0.7771156425231096,
            "mae": 0.2533177757416531,
            "precision": 0.7580645161290323,
            "recall": 0.8103448275862069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(27)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.818578248388957,
            "auditor_fn_violation": 0.010562119584675981,
            "auditor_fp_violation": 0.00789629583437267,
            "ave_precision_score": 0.8189227257057462,
            "fpr": 0.13706140350877194,
            "logloss": 0.971364129365045,
            "mae": 0.28045299812567076,
            "precision": 0.749498997995992,
            "recall": 0.763265306122449
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.836510443615736,
            "auditor_fn_violation": 0.011189863355918093,
            "auditor_fp_violation": 0.019333672218988896,
            "ave_precision_score": 0.8368508059476008,
            "fpr": 0.13062568605927552,
            "logloss": 0.833014720205134,
            "mae": 0.2519464691654173,
            "precision": 0.7510460251046025,
            "recall": 0.7737068965517241
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(28)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8235804944416849,
            "auditor_fn_violation": 0.018407626208378092,
            "auditor_fp_violation": 0.013241040991103356,
            "ave_precision_score": 0.8238952829521322,
            "fpr": 0.14583333333333334,
            "logloss": 0.9174483716262034,
            "mae": 0.2826783080236165,
            "precision": 0.740234375,
            "recall": 0.773469387755102
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8452058766730328,
            "auditor_fn_violation": 0.014395416177750864,
            "auditor_fp_violation": 0.02845657229437868,
            "ave_precision_score": 0.8455939880957797,
            "fpr": 0.12952799121844127,
            "logloss": 0.7768250285117688,
            "mae": 0.2482749453172071,
            "precision": 0.7586912065439673,
            "recall": 0.7995689655172413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(29)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8191465175842054,
            "auditor_fn_violation": 0.008592910848549943,
            "auditor_fp_violation": 0.015496383137939634,
            "ave_precision_score": 0.8194447852933799,
            "fpr": 0.17105263157894737,
            "logloss": 0.9465560188330798,
            "mae": 0.2888187514896361,
            "precision": 0.7089552238805971,
            "recall": 0.7755102040816326
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8393138894456881,
            "auditor_fn_violation": 0.011511601498921236,
            "auditor_fp_violation": 0.02936026737587086,
            "ave_precision_score": 0.8396594685085859,
            "fpr": 0.145993413830955,
            "logloss": 0.8177832565852323,
            "mae": 0.2518540639665423,
            "precision": 0.7397260273972602,
            "recall": 0.8146551724137931
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(30)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.8469474261016835,
            "auditor_fn_violation": 0.008203544575725029,
            "auditor_fp_violation": 0.02687702668994761,
            "ave_precision_score": 0.8471938109572674,
            "fpr": 0.31798245614035087,
            "logloss": 0.7308712001207316,
            "mae": 0.35650306896780404,
            "precision": 0.6102150537634409,
            "recall": 0.926530612244898
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.8540902334629442,
            "auditor_fn_violation": 0.004031189674098188,
            "auditor_fp_violation": 0.034536868549201044,
            "ave_precision_score": 0.8545037839101116,
            "fpr": 0.3062568605927552,
            "logloss": 0.6845405099182323,
            "mae": 0.34243786482572797,
            "precision": 0.6108786610878661,
            "recall": 0.9439655172413793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(31)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7985923986440264,
            "auditor_fn_violation": 0.007691102756892232,
            "auditor_fp_violation": 0.015330090629417158,
            "ave_precision_score": 0.7989363450705665,
            "fpr": 0.17763157894736842,
            "logloss": 1.160903852480433,
            "mae": 0.3044014217700757,
            "precision": 0.6960600375234521,
            "recall": 0.7571428571428571
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8181668644130908,
            "auditor_fn_violation": 0.009860327794390403,
            "auditor_fp_violation": 0.02844429382859753,
            "ave_precision_score": 0.8185726315594625,
            "fpr": 0.145993413830955,
            "logloss": 1.0129397066588384,
            "mae": 0.259390496517438,
            "precision": 0.73767258382643,
            "recall": 0.8060344827586207
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(32)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.8209685593930518,
            "auditor_fn_violation": 0.008293054063730758,
            "auditor_fp_violation": 0.013370957013386552,
            "ave_precision_score": 0.821258560083248,
            "fpr": 0.1699561403508772,
            "logloss": 0.9335439180087871,
            "mae": 0.2896934957770578,
            "precision": 0.7113594040968343,
            "recall": 0.7795918367346939
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.839245478961481,
            "auditor_fn_violation": 0.011812048147166814,
            "auditor_fp_violation": 0.0278352819258528,
            "ave_precision_score": 0.8396525039760127,
            "fpr": 0.14709110867178923,
            "logloss": 0.8075277194480607,
            "mae": 0.25195271662699154,
            "precision": 0.7398058252427184,
            "recall": 0.8211206896551724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(33)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8104636898038242,
            "auditor_fn_violation": 0.018101056211958465,
            "auditor_fp_violation": 0.006927122308140019,
            "ave_precision_score": 0.8107757802609568,
            "fpr": 0.08881578947368421,
            "logloss": 1.0971776534747475,
            "mae": 0.3025216551297148,
            "precision": 0.7964824120603015,
            "recall": 0.6469387755102041
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8284475251949839,
            "auditor_fn_violation": 0.016552954313183692,
            "auditor_fp_violation": 0.021158252234066855,
            "ave_precision_score": 0.8289445533904632,
            "fpr": 0.09001097694840834,
            "logloss": 0.9228163181016235,
            "mae": 0.26708835099742595,
            "precision": 0.7924050632911392,
            "recall": 0.6745689655172413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(34)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.8094801757910681,
            "auditor_fn_violation": 0.0044642857142857175,
            "auditor_fp_violation": 0.011110418225658936,
            "ave_precision_score": 0.8098231232400167,
            "fpr": 0.14912280701754385,
            "logloss": 0.9162506278632976,
            "mae": 0.2930586677411168,
            "precision": 0.7317554240631163,
            "recall": 0.7571428571428571
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8299790527596012,
            "auditor_fn_violation": 0.009699458722888835,
            "auditor_fp_violation": 0.02434083056453931,
            "ave_precision_score": 0.8304817003133852,
            "fpr": 0.13172338090010977,
            "logloss": 0.7892869781276224,
            "mae": 0.2570098827247412,
            "precision": 0.7551020408163265,
            "recall": 0.7974137931034483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(35)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7941627975282664,
            "auditor_fn_violation": 0.01203455066237021,
            "auditor_fp_violation": 0.015065061943959433,
            "ave_precision_score": 0.7945046483573831,
            "fpr": 0.16228070175438597,
            "logloss": 1.0479624399375798,
            "mae": 0.30697794188128613,
            "precision": 0.7103718199608611,
            "recall": 0.7408163265306122
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8109579111999503,
            "auditor_fn_violation": 0.011554184488436354,
            "auditor_fp_violation": 0.028260116841880376,
            "ave_precision_score": 0.811893828921286,
            "fpr": 0.15148188803512624,
            "logloss": 0.8910725517979059,
            "mae": 0.26595248271236027,
            "precision": 0.7294117647058823,
            "recall": 0.8017241379310345
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(36)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8141935435709711,
            "auditor_fn_violation": 0.017109738632295025,
            "auditor_fp_violation": 0.019903134613785647,
            "ave_precision_score": 0.8145059271237761,
            "fpr": 0.14364035087719298,
            "logloss": 0.9964896028163206,
            "mae": 0.29114555329360264,
            "precision": 0.7348178137651822,
            "recall": 0.7408163265306122
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8331560406509132,
            "auditor_fn_violation": 0.01833907415117908,
            "auditor_fp_violation": 0.031545834284914434,
            "ave_precision_score": 0.8336617262733398,
            "fpr": 0.13172338090010977,
            "logloss": 0.8456674436496465,
            "mae": 0.25341444710847494,
            "precision": 0.7505197505197505,
            "recall": 0.7780172413793104
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(37)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8137512909109197,
            "auditor_fn_violation": 0.0039675080558539254,
            "auditor_fp_violation": 0.0056981167373409865,
            "ave_precision_score": 0.8141093600393812,
            "fpr": 0.08881578947368421,
            "logloss": 1.0470609080482791,
            "mae": 0.30042989826411054,
            "precision": 0.7923076923076923,
            "recall": 0.6306122448979592
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.838263966316219,
            "auditor_fn_violation": 0.010655210265339339,
            "auditor_fp_violation": 0.018223698912373503,
            "ave_precision_score": 0.8388717078245802,
            "fpr": 0.0867178924259056,
            "logloss": 0.8454855780561081,
            "mae": 0.2592130104944207,
            "precision": 0.7958656330749354,
            "recall": 0.6637931034482759
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(38)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6019736842105263,
            "auc_prc": 0.6915830346782121,
            "auditor_fn_violation": 0.0880639097744361,
            "auditor_fp_violation": 0.08383740749979217,
            "ave_precision_score": 0.6523972948020236,
            "fpr": 0.24890350877192982,
            "logloss": 6.313715767195284,
            "mae": 0.39853357006982915,
            "precision": 0.6092943201376936,
            "recall": 0.7224489795918367
        },
        "train": {
            "accuracy": 0.6498353457738749,
            "auc_prc": 0.7373455918679548,
            "auditor_fn_violation": 0.06871475074756804,
            "auditor_fp_violation": 0.08900168706119835,
            "ave_precision_score": 0.7035528576320886,
            "fpr": 0.24698133918770582,
            "logloss": 5.0416651821956835,
            "mae": 0.350882192327353,
            "precision": 0.6218487394957983,
            "recall": 0.7974137931034483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(39)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8123609396001856,
            "auditor_fn_violation": 0.009689402076620127,
            "auditor_fp_violation": 0.015688658850918767,
            "ave_precision_score": 0.8126349471722655,
            "fpr": 0.15570175438596492,
            "logloss": 0.9800709302189751,
            "mae": 0.294074242364071,
            "precision": 0.7210216110019646,
            "recall": 0.7489795918367347
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8289561228976418,
            "auditor_fn_violation": 0.012564347628600635,
            "auditor_fp_violation": 0.028154522036162543,
            "ave_precision_score": 0.8294593647392117,
            "fpr": 0.13721185510428102,
            "logloss": 0.8545840541195142,
            "mae": 0.2572430984816629,
            "precision": 0.75,
            "recall": 0.8081896551724138
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(40)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8208082966699513,
            "auditor_fn_violation": 0.007556838524883641,
            "auditor_fp_violation": 0.007685831878273892,
            "ave_precision_score": 0.8211460142052864,
            "fpr": 0.1425438596491228,
            "logloss": 0.9671150412465973,
            "mae": 0.27844980813819836,
            "precision": 0.746588693957115,
            "recall": 0.7816326530612245
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8376047134622906,
            "auditor_fn_violation": 0.01000227109277414,
            "auditor_fp_violation": 0.023252958496329974,
            "ave_precision_score": 0.8379316973870422,
            "fpr": 0.13611416026344675,
            "logloss": 0.8337576134780349,
            "mae": 0.2518167454309947,
            "precision": 0.746938775510204,
            "recall": 0.7887931034482759
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(41)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8294991374685742,
            "auditor_fn_violation": 0.044128177586824205,
            "auditor_fp_violation": 0.02799690280202877,
            "ave_precision_score": 0.8302328367836245,
            "fpr": 0.11074561403508772,
            "logloss": 0.7258664582689524,
            "mae": 0.2898617610805166,
            "precision": 0.7809110629067245,
            "recall": 0.7346938775510204
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8677909997749869,
            "auditor_fn_violation": 0.03711580680570802,
            "auditor_fp_violation": 0.025033336034595805,
            "ave_precision_score": 0.8679669791491446,
            "fpr": 0.10757409440175632,
            "logloss": 0.5731095920750876,
            "mae": 0.25719989526352627,
            "precision": 0.7787810383747178,
            "recall": 0.7435344827586207
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(42)",
        "seed": 9540,
        "test": {
            "accuracy": 0.46710526315789475,
            "auc_prc": 0.47111329509492755,
            "auditor_fn_violation": 0.03590673111349804,
            "auditor_fp_violation": 0.024122807017543876,
            "ave_precision_score": 0.4489767023589832,
            "fpr": 0.25219298245614036,
            "logloss": 9.016107182028815,
            "mae": 0.5501397628732771,
            "precision": 0.5043103448275862,
            "recall": 0.4775510204081633
        },
        "train": {
            "accuracy": 0.446761800219539,
            "auc_prc": 0.42908725151082777,
            "auditor_fn_violation": 0.029791532609107094,
            "auditor_fp_violation": 0.01948592519467508,
            "ave_precision_score": 0.4067231489944584,
            "fpr": 0.27332601536772777,
            "logloss": 9.810824042415382,
            "mae": 0.5685597139347428,
            "precision": 0.45633187772925765,
            "recall": 0.4504310344827586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(43)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8142432804463934,
            "auditor_fn_violation": 0.013258592910848556,
            "auditor_fp_violation": 0.014243992683129628,
            "ave_precision_score": 0.8145476896469863,
            "fpr": 0.15570175438596492,
            "logloss": 1.0230053089098958,
            "mae": 0.29594912071947066,
            "precision": 0.7210216110019646,
            "recall": 0.7489795918367347
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.832639459617466,
            "auditor_fn_violation": 0.011466652787766384,
            "auditor_fp_violation": 0.028154522036162546,
            "ave_precision_score": 0.8330102151314394,
            "fpr": 0.13611416026344675,
            "logloss": 0.8819170840187461,
            "mae": 0.25689333835571576,
            "precision": 0.751503006012024,
            "recall": 0.8081896551724138
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(44)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.8027412883501072,
            "auditor_fn_violation": 0.006523003938417471,
            "auditor_fp_violation": 0.015943294254593836,
            "ave_precision_score": 0.8030946566358677,
            "fpr": 0.14692982456140352,
            "logloss": 1.0066901565656212,
            "mae": 0.2948829405863776,
            "precision": 0.7314629258517034,
            "recall": 0.7448979591836735
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8161148415477345,
            "auditor_fn_violation": 0.012039157424580792,
            "auditor_fp_violation": 0.03078948079279598,
            "ave_precision_score": 0.8167380406105127,
            "fpr": 0.13611416026344675,
            "logloss": 0.8749753014128608,
            "mae": 0.2602281892099599,
            "precision": 0.7464212678936605,
            "recall": 0.7866379310344828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(45)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.818182377956679,
            "auditor_fn_violation": 0.041604010025062664,
            "auditor_fp_violation": 0.03227893489648292,
            "ave_precision_score": 0.8187897334653056,
            "fpr": 0.14144736842105263,
            "logloss": 0.8445146721121207,
            "mae": 0.295133284212777,
            "precision": 0.7372708757637475,
            "recall": 0.7387755102040816
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8503621058467128,
            "auditor_fn_violation": 0.033049131307013896,
            "auditor_fp_violation": 0.037004840171210925,
            "ave_precision_score": 0.8507667725358615,
            "fpr": 0.13172338090010977,
            "logloss": 0.6713486467755694,
            "mae": 0.25758020863708897,
            "precision": 0.7530864197530864,
            "recall": 0.7887931034482759
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(46)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8163996714658743,
            "auditor_fn_violation": 0.012884890798424638,
            "auditor_fp_violation": 0.019154818325434437,
            "ave_precision_score": 0.8166973387003651,
            "fpr": 0.14144736842105263,
            "logloss": 1.0108255430016275,
            "mae": 0.28952714505809873,
            "precision": 0.7393939393939394,
            "recall": 0.746938775510204
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8348569002309039,
            "auditor_fn_violation": 0.013645482417956777,
            "auditor_fp_violation": 0.027285206658857565,
            "ave_precision_score": 0.8353693388535728,
            "fpr": 0.12623490669593854,
            "logloss": 0.8572211479610137,
            "mae": 0.2496256243593657,
            "precision": 0.7628865979381443,
            "recall": 0.7974137931034483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(47)",
        "seed": 9540,
        "test": {
            "accuracy": 0.44956140350877194,
            "auc_prc": 0.46714120904595646,
            "auditor_fn_violation": 0.03413668098818475,
            "auditor_fp_violation": 0.0371923588592334,
            "ave_precision_score": 0.44485614822660535,
            "fpr": 0.2883771929824561,
            "logloss": 9.267028730046057,
            "mae": 0.5529443343745382,
            "precision": 0.4883268482490272,
            "recall": 0.5122448979591837
        },
        "train": {
            "accuracy": 0.43688254665203075,
            "auc_prc": 0.42617659570007516,
            "auditor_fn_violation": 0.03087030167682351,
            "auditor_fp_violation": 0.036312334701154435,
            "ave_precision_score": 0.4036518286895768,
            "fpr": 0.305159165751921,
            "logloss": 10.075119858296592,
            "mae": 0.5745211706664386,
            "precision": 0.4516765285996055,
            "recall": 0.49353448275862066
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(48)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7897485681162609,
            "auditor_fn_violation": 0.016982187611886877,
            "auditor_fp_violation": 0.006932318949031349,
            "ave_precision_score": 0.7901055631215576,
            "fpr": 0.13157894736842105,
            "logloss": 1.1969405020804615,
            "mae": 0.31019805461079236,
            "precision": 0.737417943107221,
            "recall": 0.6877551020408164
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8002673949619278,
            "auditor_fn_violation": 0.016181536015746244,
            "auditor_fp_violation": 0.029738444121930076,
            "ave_precision_score": 0.80171138918168,
            "fpr": 0.12294182217343579,
            "logloss": 1.0081728922754403,
            "mae": 0.27135845469605113,
            "precision": 0.7522123893805309,
            "recall": 0.7327586206896551
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(49)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8195666267227761,
            "auditor_fn_violation": 0.016259398496240605,
            "auditor_fp_violation": 0.01106105013719132,
            "ave_precision_score": 0.819877983018846,
            "fpr": 0.14583333333333334,
            "logloss": 1.0150044620579217,
            "mae": 0.28700226001968693,
            "precision": 0.734,
            "recall": 0.7489795918367347
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8346527564109337,
            "auditor_fn_violation": 0.015001040917521488,
            "auditor_fp_violation": 0.0292620396496217,
            "ave_precision_score": 0.8354664844955293,
            "fpr": 0.12843029637760703,
            "logloss": 0.8772096357211354,
            "mae": 0.25053158234247846,
            "precision": 0.7592592592592593,
            "recall": 0.7952586206896551
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(50)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.8037625575001506,
            "auditor_fn_violation": 0.010186179735051922,
            "auditor_fp_violation": 0.010434854909786316,
            "ave_precision_score": 0.804105994198267,
            "fpr": 0.10197368421052631,
            "logloss": 1.0316086656796344,
            "mae": 0.30681916503083884,
            "precision": 0.7764423076923077,
            "recall": 0.6591836734693878
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8263957576520362,
            "auditor_fn_violation": 0.011163840417881076,
            "auditor_fp_violation": 0.023613945390295595,
            "ave_precision_score": 0.8269358146926509,
            "fpr": 0.09989023051591657,
            "logloss": 0.876131931750218,
            "mae": 0.2685999171363168,
            "precision": 0.785377358490566,
            "recall": 0.7176724137931034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(51)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.818184946120246,
            "auditor_fn_violation": 0.009868421052631578,
            "auditor_fp_violation": 0.008130144674482422,
            "ave_precision_score": 0.8185050333158617,
            "fpr": 0.12609649122807018,
            "logloss": 0.9687295609813873,
            "mae": 0.29095843790401654,
            "precision": 0.7526881720430108,
            "recall": 0.7142857142857143
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.838564601758361,
            "auditor_fn_violation": 0.011771830879291416,
            "auditor_fp_violation": 0.025671816255215286,
            "ave_precision_score": 0.8389125976917934,
            "fpr": 0.10976948408342481,
            "logloss": 0.8202101014897631,
            "mae": 0.2522441313507183,
            "precision": 0.7807017543859649,
            "recall": 0.7672413793103449
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(52)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8186757315193385,
            "auditor_fn_violation": 0.010318206229860368,
            "auditor_fp_violation": 0.011559927662758799,
            "ave_precision_score": 0.8190229772411014,
            "fpr": 0.1787280701754386,
            "logloss": 0.9716421315082173,
            "mae": 0.29120483705404354,
            "precision": 0.7057761732851986,
            "recall": 0.7979591836734694
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8390425736640457,
            "auditor_fn_violation": 0.009685264393050456,
            "auditor_fp_violation": 0.025350120451749322,
            "ave_precision_score": 0.8394114463305067,
            "fpr": 0.16136114160263446,
            "logloss": 0.8523944501818049,
            "mae": 0.2552826072702441,
            "precision": 0.7247191011235955,
            "recall": 0.834051724137931
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(53)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7768023789531378,
            "auditor_fn_violation": 0.020622986036519873,
            "auditor_fp_violation": 0.028163195310551267,
            "ave_precision_score": 0.7772132329240232,
            "fpr": 0.1787280701754386,
            "logloss": 1.1923804834353162,
            "mae": 0.2950546469356396,
            "precision": 0.7014652014652014,
            "recall": 0.7816326530612245
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.7767484779825361,
            "auditor_fn_violation": 0.0180551875544116,
            "auditor_fp_violation": 0.029264495342777933,
            "ave_precision_score": 0.7781614923595797,
            "fpr": 0.15916575192096596,
            "logloss": 1.1771797223068665,
            "mae": 0.26348950042197605,
            "precision": 0.725897920604915,
            "recall": 0.8275862068965517
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(54)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.8138678314290041,
            "auditor_fn_violation": 0.016619674185463666,
            "auditor_fp_violation": 0.008137939635819406,
            "ave_precision_score": 0.8141730293581348,
            "fpr": 0.10307017543859649,
            "logloss": 1.017232649155156,
            "mae": 0.2958652929590085,
            "precision": 0.7777777777777778,
            "recall": 0.6714285714285714
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8324644542999459,
            "auditor_fn_violation": 0.020643287028275108,
            "auditor_fp_violation": 0.026585334109332375,
            "ave_precision_score": 0.8329454230576444,
            "fpr": 0.09769484083424808,
            "logloss": 0.8604396197931805,
            "mae": 0.25838688682208527,
            "precision": 0.7910798122065728,
            "recall": 0.7262931034482759
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(55)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.8031124862231958,
            "auditor_fn_violation": 0.01113721804511279,
            "auditor_fp_violation": 0.008428951525733768,
            "ave_precision_score": 0.8034497320991523,
            "fpr": 0.14912280701754385,
            "logloss": 1.0449782738320836,
            "mae": 0.30083706807156035,
            "precision": 0.7241379310344828,
            "recall": 0.7285714285714285
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8209852409775313,
            "auditor_fn_violation": 0.01044702676104319,
            "auditor_fp_violation": 0.027069105661109438,
            "ave_precision_score": 0.8213145332582161,
            "fpr": 0.13062568605927552,
            "logloss": 0.9020441598061427,
            "mae": 0.259434110353001,
            "precision": 0.7525987525987526,
            "recall": 0.7801724137931034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(56)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7808864702436888,
            "auditor_fn_violation": 0.010266738274257076,
            "auditor_fp_violation": 0.013318990604473275,
            "ave_precision_score": 0.7813133350261812,
            "fpr": 0.16885964912280702,
            "logloss": 1.1786740792565793,
            "mae": 0.29385966089850807,
            "precision": 0.7077798861480076,
            "recall": 0.7612244897959184
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.7792616167893549,
            "auditor_fn_violation": 0.008857261819145316,
            "auditor_fp_violation": 0.026420802667865048,
            "ave_precision_score": 0.780746260001431,
            "fpr": 0.14928649835345773,
            "logloss": 1.189762864951729,
            "mae": 0.25903594275964065,
            "precision": 0.734375,
            "recall": 0.8103448275862069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(57)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8228870280551718,
            "auditor_fn_violation": 0.01861573576799141,
            "auditor_fp_violation": 0.01674097863141266,
            "ave_precision_score": 0.823193609355219,
            "fpr": 0.09758771929824561,
            "logloss": 0.898745386040693,
            "mae": 0.2926288373888528,
            "precision": 0.7890995260663507,
            "recall": 0.6795918367346939
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8450868446158274,
            "auditor_fn_violation": 0.02441188160036338,
            "auditor_fp_violation": 0.022560453026273464,
            "ave_precision_score": 0.8454987966723402,
            "fpr": 0.08781558726673985,
            "logloss": 0.7430409072737494,
            "mae": 0.25265980698149915,
            "precision": 0.8072289156626506,
            "recall": 0.7219827586206896
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(58)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.8136025730150303,
            "auditor_fn_violation": 0.021334586466165418,
            "auditor_fp_violation": 0.014399891909869464,
            "ave_precision_score": 0.8139188470392765,
            "fpr": 0.15460526315789475,
            "logloss": 1.0277927525513968,
            "mae": 0.29752475490856906,
            "precision": 0.7196819085487077,
            "recall": 0.7387755102040816
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8309519059466548,
            "auditor_fn_violation": 0.017899049926189486,
            "auditor_fp_violation": 0.02928168519487153,
            "ave_precision_score": 0.8316571097285181,
            "fpr": 0.1394072447859495,
            "logloss": 0.8867496066575378,
            "mae": 0.2631662838023001,
            "precision": 0.735966735966736,
            "recall": 0.7629310344827587
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(59)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8135132115307091,
            "auditor_fn_violation": 0.013377192982456146,
            "auditor_fp_violation": 0.0070622349713145485,
            "ave_precision_score": 0.8138265845931252,
            "fpr": 0.13157894736842105,
            "logloss": 1.0569032602983393,
            "mae": 0.293866205726554,
            "precision": 0.7435897435897436,
            "recall": 0.710204081632653
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8325483901265458,
            "auditor_fn_violation": 0.012621124947954122,
            "auditor_fp_violation": 0.02741535839613769,
            "ave_precision_score": 0.8329388271932389,
            "fpr": 0.11745334796926454,
            "logloss": 0.8731424469605276,
            "mae": 0.25303921041133914,
            "precision": 0.7693965517241379,
            "recall": 0.7693965517241379
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(60)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6019736842105263,
            "auc_prc": 0.6811287094420535,
            "auditor_fn_violation": 0.0904515753669889,
            "auditor_fp_violation": 0.08642013802278208,
            "ave_precision_score": 0.642303685882774,
            "fpr": 0.2543859649122807,
            "logloss": 6.4685124606652815,
            "mae": 0.4000734138666563,
            "precision": 0.6074450084602369,
            "recall": 0.7326530612244898
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.7357260091972527,
            "auditor_fn_violation": 0.06798374276089178,
            "auditor_fp_violation": 0.08921042097947779,
            "ave_precision_score": 0.7001933942018204,
            "fpr": 0.25466520307354557,
            "logloss": 5.2463578945594245,
            "mae": 0.3592999139888309,
            "precision": 0.615257048092869,
            "recall": 0.7995689655172413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(61)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.8137411453096088,
            "auditor_fn_violation": 0.009022556390977444,
            "auditor_fp_violation": 0.017029392200881358,
            "ave_precision_score": 0.8139880611501651,
            "fpr": 0.21710526315789475,
            "logloss": 1.067007414460256,
            "mae": 0.2970125550808958,
            "precision": 0.6832,
            "recall": 0.8714285714285714
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8322516938948769,
            "auditor_fn_violation": 0.014191964116734169,
            "auditor_fp_violation": 0.028147154956693853,
            "ave_precision_score": 0.8326433265462676,
            "fpr": 0.2030735455543359,
            "logloss": 0.9500036082712668,
            "mae": 0.26685323641451525,
            "precision": 0.6906354515050167,
            "recall": 0.8900862068965517
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(62)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8340303118920533,
            "auditor_fn_violation": 0.04295112781954887,
            "auditor_fp_violation": 0.025505113494637067,
            "ave_precision_score": 0.8347532513424731,
            "fpr": 0.12280701754385964,
            "logloss": 0.7284651859394079,
            "mae": 0.28597136896850034,
            "precision": 0.7681159420289855,
            "recall": 0.7571428571428571
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.86399293265637,
            "auditor_fn_violation": 0.03445436996101291,
            "auditor_fp_violation": 0.02749639627029323,
            "ave_precision_score": 0.8641687524368106,
            "fpr": 0.1207464324917673,
            "logloss": 0.5972070026225661,
            "mae": 0.2590529726059213,
            "precision": 0.7629310344827587,
            "recall": 0.7629310344827587
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(63)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.8463584988812198,
            "auditor_fn_violation": 0.009803526673827426,
            "auditor_fp_violation": 0.027284962999916857,
            "ave_precision_score": 0.8466152466180175,
            "fpr": 0.2817982456140351,
            "logloss": 0.6578245098642629,
            "mae": 0.3446015447381733,
            "precision": 0.6328571428571429,
            "recall": 0.9040816326530612
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.8568274866705078,
            "auditor_fn_violation": 0.011499772890722586,
            "auditor_fp_violation": 0.029505153272088365,
            "ave_precision_score": 0.8571941036241437,
            "fpr": 0.25466520307354557,
            "logloss": 0.6080003718863718,
            "mae": 0.3278241812754484,
            "precision": 0.6458015267175573,
            "recall": 0.9116379310344828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(64)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.8066137024712097,
            "auditor_fn_violation": 0.049483082706766914,
            "auditor_fp_violation": 0.03915668911615532,
            "ave_precision_score": 0.8073848895157212,
            "fpr": 0.14035087719298245,
            "logloss": 0.8191525343535854,
            "mae": 0.3122082870385851,
            "precision": 0.7360824742268042,
            "recall": 0.7285714285714285
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8490444664013098,
            "auditor_fn_violation": 0.040567394678072605,
            "auditor_fp_violation": 0.034522134390263665,
            "ave_precision_score": 0.8493101195073324,
            "fpr": 0.12184412733260154,
            "logloss": 0.6267806710737486,
            "mae": 0.27118463457755787,
            "precision": 0.7597402597402597,
            "recall": 0.7564655172413793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(65)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7695695338687341,
            "auditor_fn_violation": 0.03562477622627999,
            "auditor_fp_violation": 0.016738380310967,
            "ave_precision_score": 0.7701774957551922,
            "fpr": 0.1206140350877193,
            "logloss": 0.6517639183006579,
            "mae": 0.3998114975213212,
            "precision": 0.7465437788018433,
            "recall": 0.6612244897959184
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.760409396225115,
            "auditor_fn_violation": 0.028204133388848928,
            "auditor_fp_violation": 0.020075291552169975,
            "ave_precision_score": 0.7614897250832451,
            "fpr": 0.1119648737650933,
            "logloss": 0.5993934226900306,
            "mae": 0.38062807684253114,
            "precision": 0.7475247524752475,
            "recall": 0.6508620689655172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(66)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8190316349770419,
            "auditor_fn_violation": 0.01252237737200144,
            "auditor_fp_violation": 0.008621227238712904,
            "ave_precision_score": 0.8192817972192761,
            "fpr": 0.1513157894736842,
            "logloss": 0.6359122431666089,
            "mae": 0.3144482385292864,
            "precision": 0.7366412213740458,
            "recall": 0.7877551020408163
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8260083768548812,
            "auditor_fn_violation": 0.009576441197622926,
            "auditor_fp_violation": 0.023643413708170337,
            "ave_precision_score": 0.8262812217334956,
            "fpr": 0.14928649835345773,
            "logloss": 0.5614457741842188,
            "mae": 0.2917247311274643,
            "precision": 0.7312252964426877,
            "recall": 0.7974137931034483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(67)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.8029469700907028,
            "auditor_fn_violation": 0.03292606516290727,
            "auditor_fp_violation": 0.044129874449156066,
            "ave_precision_score": 0.8022730739430004,
            "fpr": 0.22587719298245615,
            "logloss": 1.110546692668647,
            "mae": 0.31345283398889257,
            "precision": 0.6724960254372019,
            "recall": 0.863265306122449
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8247975232432634,
            "auditor_fn_violation": 0.024277035466898824,
            "auditor_fp_violation": 0.04093394922117692,
            "ave_precision_score": 0.82429085340856,
            "fpr": 0.21295279912184412,
            "logloss": 0.9942980180064759,
            "mae": 0.29182126998235164,
            "precision": 0.6782752902155887,
            "recall": 0.8814655172413793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(68)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8077507223775244,
            "auditor_fn_violation": 0.016901629072681702,
            "auditor_fp_violation": 0.026682152656522823,
            "ave_precision_score": 0.808131140028565,
            "fpr": 0.15679824561403508,
            "logloss": 1.0267043258821407,
            "mae": 0.29078561567723543,
            "precision": 0.7217898832684825,
            "recall": 0.7571428571428571
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8321425887022028,
            "auditor_fn_violation": 0.018899750179794842,
            "auditor_fp_violation": 0.029600925305181276,
            "ave_precision_score": 0.8325071817998959,
            "fpr": 0.141602634467618,
            "logloss": 0.8837228282723254,
            "mae": 0.2547422054241851,
            "precision": 0.7409638554216867,
            "recall": 0.7952586206896551
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(69)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8215237574917311,
            "auditor_fn_violation": 0.012448532044396709,
            "auditor_fp_violation": 0.019185998170782412,
            "ave_precision_score": 0.8217974210541721,
            "fpr": 0.22587719298245615,
            "logloss": 1.0251053394292529,
            "mae": 0.2967454581839107,
            "precision": 0.6766091051805337,
            "recall": 0.8795918367346939
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8429313848674728,
            "auditor_fn_violation": 0.011916139899314886,
            "auditor_fp_violation": 0.03514833614510201,
            "ave_precision_score": 0.8434268613794645,
            "fpr": 0.2074643249176729,
            "logloss": 0.9181539367724598,
            "mae": 0.26833558963663756,
            "precision": 0.6881188118811881,
            "recall": 0.8987068965517241
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(70)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8039703920483977,
            "auditor_fn_violation": 0.0049722520587182255,
            "auditor_fp_violation": 0.012204311133283447,
            "ave_precision_score": 0.8043245725426187,
            "fpr": 0.15460526315789475,
            "logloss": 1.0012298239553146,
            "mae": 0.29389505988097747,
            "precision": 0.7218934911242604,
            "recall": 0.746938775510204
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.818785637590423,
            "auditor_fn_violation": 0.010352397895454031,
            "auditor_fp_violation": 0.030013481755427698,
            "ave_precision_score": 0.8193368289485451,
            "fpr": 0.14818880351262348,
            "logloss": 0.8749492269466101,
            "mae": 0.26051383205792694,
            "precision": 0.7326732673267327,
            "recall": 0.7974137931034483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(71)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8354271008457494,
            "auditor_fn_violation": 0.021585213032581457,
            "auditor_fp_violation": 0.007267502286521996,
            "ave_precision_score": 0.8361612514407273,
            "fpr": 0.08662280701754387,
            "logloss": 0.570857403127824,
            "mae": 0.3303717793047933,
            "precision": 0.8167053364269141,
            "recall": 0.7183673469387755
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.846095216838674,
            "auditor_fn_violation": 0.019966690639312622,
            "auditor_fp_violation": 0.013656109641788042,
            "ave_precision_score": 0.8463968887092135,
            "fpr": 0.09001097694840834,
            "logloss": 0.5203543267056036,
            "mae": 0.3101200438869384,
            "precision": 0.8,
            "recall": 0.7068965517241379
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(72)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7767848204470829,
            "auditor_fn_violation": 0.01821965628356606,
            "auditor_fp_violation": 0.026253429782988273,
            "ave_precision_score": 0.7771851982691904,
            "fpr": 0.18092105263157895,
            "logloss": 1.1956850029256265,
            "mae": 0.2950238255133323,
            "precision": 0.7005444646098004,
            "recall": 0.7877551020408163
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.7778472567037721,
            "auditor_fn_violation": 0.0180551875544116,
            "auditor_fp_violation": 0.029723709962992714,
            "ave_precision_score": 0.7792819306056704,
            "fpr": 0.1602634467618002,
            "logloss": 1.178053111546591,
            "mae": 0.26323926092457967,
            "precision": 0.7245283018867924,
            "recall": 0.8275862068965517
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(73)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7887574232184308,
            "auditor_fn_violation": 0.014912280701754394,
            "auditor_fp_violation": 0.018188243119647462,
            "ave_precision_score": 0.7892491793517141,
            "fpr": 0.17543859649122806,
            "logloss": 1.1755980606598655,
            "mae": 0.2904655962593419,
            "precision": 0.7101449275362319,
            "recall": 0.8
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.7894356478825859,
            "auditor_fn_violation": 0.016027764109163862,
            "auditor_fp_violation": 0.02757497845129256,
            "ave_precision_score": 0.790758837532403,
            "fpr": 0.15916575192096596,
            "logloss": 1.166341577861656,
            "mae": 0.254539427019582,
            "precision": 0.7284644194756554,
            "recall": 0.8383620689655172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(74)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.8095550322023013,
            "auditor_fn_violation": 0.008395989974937346,
            "auditor_fp_violation": 0.01491695767855659,
            "ave_precision_score": 0.8098574641404339,
            "fpr": 0.1875,
            "logloss": 0.9259803358757948,
            "mae": 0.2882317166403023,
            "precision": 0.7036395147313691,
            "recall": 0.8285714285714286
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8279350485019186,
            "auditor_fn_violation": 0.009380086301525417,
            "auditor_fp_violation": 0.024664982061161498,
            "ave_precision_score": 0.8283809574116285,
            "fpr": 0.16794731064763996,
            "logloss": 0.8234103324517263,
            "mae": 0.25875420640892643,
            "precision": 0.71875,
            "recall": 0.8426724137931034
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(75)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.8087004584134607,
            "auditor_fn_violation": 0.025248388829215897,
            "auditor_fp_violation": 0.0319489481998836,
            "ave_precision_score": 0.8090689746876643,
            "fpr": 0.16447368421052633,
            "logloss": 1.0584315939938544,
            "mae": 0.29224631154613295,
            "precision": 0.715370018975332,
            "recall": 0.7693877551020408
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8389469333397523,
            "auditor_fn_violation": 0.020160679813770396,
            "auditor_fp_violation": 0.03652843569890255,
            "ave_precision_score": 0.83945180646792,
            "fpr": 0.1394072447859495,
            "logloss": 0.8708712263055665,
            "mae": 0.25066801209884343,
            "precision": 0.746,
            "recall": 0.8038793103448276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(76)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.8204555647931667,
            "auditor_fn_violation": 0.011047708557107055,
            "auditor_fp_violation": 0.02014737673567807,
            "ave_precision_score": 0.820765595881614,
            "fpr": 0.22149122807017543,
            "logloss": 1.022437748372586,
            "mae": 0.29474767026600934,
            "precision": 0.6808846761453397,
            "recall": 0.8795918367346939
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.843431083035961,
            "auditor_fn_violation": 0.011916139899314886,
            "auditor_fp_violation": 0.03363071777455265,
            "ave_precision_score": 0.8438641786112322,
            "fpr": 0.2052689352360044,
            "logloss": 0.9109785791674566,
            "mae": 0.26573154413434397,
            "precision": 0.6903973509933775,
            "recall": 0.8987068965517241
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(77)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.8015154920382485,
            "auditor_fn_violation": 0.03390395631936986,
            "auditor_fp_violation": 0.03784193897064938,
            "ave_precision_score": 0.8019657973047338,
            "fpr": 0.17982456140350878,
            "logloss": 1.1162702430185598,
            "mae": 0.2970690185581765,
            "precision": 0.7055655296229802,
            "recall": 0.8020408163265306
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.836977084575205,
            "auditor_fn_violation": 0.023058688822438396,
            "auditor_fp_violation": 0.03959068506471979,
            "ave_precision_score": 0.837456420049479,
            "fpr": 0.16465422612513722,
            "logloss": 0.9116641796743408,
            "mae": 0.2558374285851195,
            "precision": 0.7217068645640075,
            "recall": 0.8383620689655172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(78)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8152909614082994,
            "auditor_fn_violation": 0.01651450053705694,
            "auditor_fp_violation": 0.019056082148499215,
            "ave_precision_score": 0.8155883936656314,
            "fpr": 0.125,
            "logloss": 0.9831068500120711,
            "mae": 0.2919330907049354,
            "precision": 0.7558886509635975,
            "recall": 0.7204081632653061
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8323339153424936,
            "auditor_fn_violation": 0.017712157916650893,
            "auditor_fp_violation": 0.029993836210177868,
            "ave_precision_score": 0.8328054806201868,
            "fpr": 0.1141602634467618,
            "logloss": 0.8397348152736241,
            "mae": 0.25386683779719066,
            "precision": 0.7724288840262582,
            "recall": 0.7607758620689655
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(79)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.8181132609362733,
            "auditor_fn_violation": 0.01096491228070176,
            "auditor_fp_violation": 0.019108048557412504,
            "ave_precision_score": 0.8184425015954158,
            "fpr": 0.21600877192982457,
            "logloss": 1.0479555069967097,
            "mae": 0.2940244982572035,
            "precision": 0.6842948717948718,
            "recall": 0.8714285714285714
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.840717048806797,
            "auditor_fn_violation": 0.013039857678186155,
            "auditor_fp_violation": 0.03354476851408463,
            "ave_precision_score": 0.8409438932287816,
            "fpr": 0.20197585071350166,
            "logloss": 0.9285418077276918,
            "mae": 0.2637231141011254,
            "precision": 0.6928213689482471,
            "recall": 0.8943965517241379
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(80)",
        "seed": 9540,
        "test": {
            "accuracy": 0.44846491228070173,
            "auc_prc": 0.46593458521980335,
            "auditor_fn_violation": 0.03483485499462943,
            "auditor_fp_violation": 0.03098237299409662,
            "ave_precision_score": 0.44318293439393563,
            "fpr": 0.25877192982456143,
            "logloss": 9.509797443804933,
            "mae": 0.5605357010421,
            "precision": 0.485838779956427,
            "recall": 0.45510204081632655
        },
        "train": {
            "accuracy": 0.43468715697036225,
            "auc_prc": 0.42279009147301194,
            "auditor_fn_violation": 0.028331882357394315,
            "auditor_fp_violation": 0.02159536561587557,
            "ave_precision_score": 0.4004294696634049,
            "fpr": 0.27552140504939626,
            "logloss": 10.427625419230706,
            "mae": 0.5803381489666536,
            "precision": 0.4434589800443459,
            "recall": 0.43103448275862066
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(81)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8405689511989777,
            "auditor_fn_violation": 0.019956140350877196,
            "auditor_fp_violation": 0.015148208198220672,
            "ave_precision_score": 0.8408055537328709,
            "fpr": 0.12938596491228072,
            "logloss": 0.570093646307606,
            "mae": 0.32901812721824525,
            "precision": 0.7586912065439673,
            "recall": 0.7571428571428571
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8475832925494551,
            "auditor_fn_violation": 0.015417407926113789,
            "auditor_fp_violation": 0.018177040742405164,
            "ave_precision_score": 0.8479501766059285,
            "fpr": 0.12403951701427003,
            "logloss": 0.5155467246365625,
            "mae": 0.30816666708193907,
            "precision": 0.7616033755274262,
            "recall": 0.7780172413793104
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(82)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8120512198840444,
            "auditor_fn_violation": 0.023055406373075547,
            "auditor_fp_violation": 0.028893323355782825,
            "ave_precision_score": 0.8124020414713272,
            "fpr": 0.15899122807017543,
            "logloss": 0.9957039087859736,
            "mae": 0.2901981574617279,
            "precision": 0.7222222222222222,
            "recall": 0.7693877551020408
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8407632056326159,
            "auditor_fn_violation": 0.021258374654604638,
            "auditor_fp_violation": 0.03419552720048525,
            "ave_precision_score": 0.8412742318086326,
            "fpr": 0.13611416026344675,
            "logloss": 0.8256531001945059,
            "mae": 0.24982342308665986,
            "precision": 0.7505030181086519,
            "recall": 0.8038793103448276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(83)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.826224630446009,
            "auditor_fn_violation": 0.014359559613319016,
            "auditor_fp_violation": 0.010746653363265987,
            "ave_precision_score": 0.826472791507852,
            "fpr": 0.12719298245614036,
            "logloss": 0.8130840385692621,
            "mae": 0.29925904798262726,
            "precision": 0.7461706783369803,
            "recall": 0.6959183673469388
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8469511212005966,
            "auditor_fn_violation": 0.014000340663916122,
            "auditor_fp_violation": 0.030349911717831034,
            "ave_precision_score": 0.8472072834951376,
            "fpr": 0.11964873765093303,
            "logloss": 0.6793024610659927,
            "mae": 0.25966621826106223,
            "precision": 0.7630434782608696,
            "recall": 0.7564655172413793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(84)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8265629070470222,
            "auditor_fn_violation": 0.01697994987468672,
            "auditor_fp_violation": 0.01142221667913861,
            "ave_precision_score": 0.8268598603059647,
            "fpr": 0.13815789473684212,
            "logloss": 0.8830594056598485,
            "mae": 0.282230769377838,
            "precision": 0.7439024390243902,
            "recall": 0.746938775510204
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.84809122973999,
            "auditor_fn_violation": 0.015727317460918287,
            "auditor_fp_violation": 0.02522488010078165,
            "ave_precision_score": 0.8484887318155059,
            "fpr": 0.11855104281009879,
            "logloss": 0.7443896408283275,
            "mae": 0.24665432807710103,
            "precision": 0.7711864406779662,
            "recall": 0.7844827586206896
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(85)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7892714314886672,
            "auditor_fn_violation": 0.008673469387755104,
            "auditor_fp_violation": 0.010201006069676562,
            "ave_precision_score": 0.7898184895699124,
            "fpr": 0.18530701754385964,
            "logloss": 0.7550365491485475,
            "mae": 0.34699165439003093,
            "precision": 0.699288256227758,
            "recall": 0.8020408163265306
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7735678179008513,
            "auditor_fn_violation": 0.0010929633975547922,
            "auditor_fp_violation": 0.01252649078992282,
            "ave_precision_score": 0.7739371931288863,
            "fpr": 0.18331503841931943,
            "logloss": 0.7850009182603738,
            "mae": 0.33736012752511324,
            "precision": 0.6941391941391941,
            "recall": 0.8168103448275862
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(86)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8138217437336517,
            "auditor_fn_violation": 0.03318788041532403,
            "auditor_fp_violation": 0.022761287104015968,
            "ave_precision_score": 0.8141298587813951,
            "fpr": 0.13815789473684212,
            "logloss": 1.0624728569099622,
            "mae": 0.2833679310966817,
            "precision": 0.7454545454545455,
            "recall": 0.753061224489796
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8444880900959615,
            "auditor_fn_violation": 0.0263399447367425,
            "auditor_fp_violation": 0.02794333242472687,
            "ave_precision_score": 0.8448595876449583,
            "fpr": 0.1350164654226125,
            "logloss": 0.8738679862086938,
            "mae": 0.24888322506108473,
            "precision": 0.7479508196721312,
            "recall": 0.7866379310344828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(87)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.837668672410182,
            "auditor_fn_violation": 0.04209183673469388,
            "auditor_fp_violation": 0.028778997256173614,
            "ave_precision_score": 0.8379402753422461,
            "fpr": 0.13048245614035087,
            "logloss": 0.6883764635657539,
            "mae": 0.2911292775212647,
            "precision": 0.7586206896551724,
            "recall": 0.763265306122449
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.866985820385537,
            "auditor_fn_violation": 0.03646523335478254,
            "auditor_fp_violation": 0.029033660186092426,
            "ave_precision_score": 0.8671919956614452,
            "fpr": 0.12184412733260154,
            "logloss": 0.5575493347404841,
            "mae": 0.25916175370296984,
            "precision": 0.7658227848101266,
            "recall": 0.7823275862068966
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(88)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.7656218786307286,
            "auditor_fn_violation": 0.024588256355173657,
            "auditor_fp_violation": 0.01805313045647294,
            "ave_precision_score": 0.7660972093006261,
            "fpr": 0.125,
            "logloss": 1.286187486578376,
            "mae": 0.32810967401273217,
            "precision": 0.7336448598130841,
            "recall": 0.6408163265306123
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.789184103045109,
            "auditor_fn_violation": 0.02571302850221432,
            "auditor_fp_violation": 0.03326973088058701,
            "ave_precision_score": 0.7899088635439352,
            "fpr": 0.13062568605927552,
            "logloss": 1.0247557483287963,
            "mae": 0.28567865693344546,
            "precision": 0.7331838565022422,
            "recall": 0.7047413793103449
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(89)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8212338022397097,
            "auditor_fn_violation": 0.013784461152882203,
            "auditor_fp_violation": 0.01679814168121726,
            "ave_precision_score": 0.8215228384508416,
            "fpr": 0.1611842105263158,
            "logloss": 0.8912358473415669,
            "mae": 0.2876053931148914,
            "precision": 0.7236842105263158,
            "recall": 0.7857142857142857
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8383728167890785,
            "auditor_fn_violation": 0.010714353306332572,
            "auditor_fp_violation": 0.02767811756385416,
            "ave_precision_score": 0.8388932054246216,
            "fpr": 0.14818880351262348,
            "logloss": 0.7671023970017534,
            "mae": 0.25359279529172024,
            "precision": 0.7383720930232558,
            "recall": 0.8211206896551724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(90)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.8096998914711587,
            "auditor_fn_violation": 0.012070354457572503,
            "auditor_fp_violation": 0.013625592417061618,
            "ave_precision_score": 0.8099682217340146,
            "fpr": 0.18201754385964913,
            "logloss": 1.0241517976887569,
            "mae": 0.29555104875785515,
            "precision": 0.6992753623188406,
            "recall": 0.7877551020408163
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8291992842702576,
            "auditor_fn_violation": 0.010714353306332572,
            "auditor_fp_violation": 0.026867738822298686,
            "ave_precision_score": 0.8295762281394463,
            "fpr": 0.16136114160263446,
            "logloss": 0.9066216396955209,
            "mae": 0.25834674227857857,
            "precision": 0.7215909090909091,
            "recall": 0.8211206896551724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(91)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.808668602164436,
            "auditor_fn_violation": 0.010671768707482996,
            "auditor_fp_violation": 0.01722426623430615,
            "ave_precision_score": 0.8089819536399336,
            "fpr": 0.18530701754385964,
            "logloss": 1.0441115418761526,
            "mae": 0.29958591755517777,
            "precision": 0.6927272727272727,
            "recall": 0.7775510204081633
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8265619511180236,
            "auditor_fn_violation": 0.01072381619289148,
            "auditor_fp_violation": 0.023726907275482118,
            "ave_precision_score": 0.826961273655368,
            "fpr": 0.15697036223929747,
            "logloss": 0.9152235615285702,
            "mae": 0.25711600311159527,
            "precision": 0.7281368821292775,
            "recall": 0.8254310344827587
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(92)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.8211191306812811,
            "auditor_fn_violation": 0.0091702470461869,
            "auditor_fp_violation": 0.01245634821651285,
            "ave_precision_score": 0.8214280750654155,
            "fpr": 0.17324561403508773,
            "logloss": 0.9738753612160025,
            "mae": 0.290010185503788,
            "precision": 0.7106227106227107,
            "recall": 0.7918367346938775
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8396997207089472,
            "auditor_fn_violation": 0.009869790680949314,
            "auditor_fp_violation": 0.03008715255011456,
            "ave_precision_score": 0.8401047930143153,
            "fpr": 0.15148188803512624,
            "logloss": 0.8477371019782376,
            "mae": 0.25205731802384335,
            "precision": 0.7335907335907336,
            "recall": 0.8189655172413793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(93)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7965045873762743,
            "auditor_fn_violation": 0.0101593268886502,
            "auditor_fp_violation": 0.011866529475347139,
            "ave_precision_score": 0.7968835599357138,
            "fpr": 0.15679824561403508,
            "logloss": 0.7663328180340531,
            "mae": 0.3151371396135498,
            "precision": 0.7157057654075547,
            "recall": 0.7346938775510204
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7719530437917463,
            "auditor_fn_violation": 0.008942427798175556,
            "auditor_fp_violation": 0.016831320892791813,
            "ave_precision_score": 0.7726918734582289,
            "fpr": 0.16355653128430298,
            "logloss": 0.7364304667542882,
            "mae": 0.30072325953294254,
            "precision": 0.7049504950495049,
            "recall": 0.7672413793103449
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(94)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.8156508031786859,
            "auditor_fn_violation": 0.0038847117794486253,
            "auditor_fp_violation": 0.01887679803774841,
            "ave_precision_score": 0.8159436769359122,
            "fpr": 0.16557017543859648,
            "logloss": 0.9134443308263299,
            "mae": 0.2951333796370387,
            "precision": 0.7112810707456979,
            "recall": 0.7591836734693878
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8255715596373983,
            "auditor_fn_violation": 0.008270562852492527,
            "auditor_fp_violation": 0.025352576144905548,
            "ave_precision_score": 0.8262935965033198,
            "fpr": 0.1437980241492865,
            "logloss": 0.7932312979162526,
            "mae": 0.25862451473443726,
            "precision": 0.7436399217221135,
            "recall": 0.8189655172413793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(95)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.8522689068683844,
            "auditor_fn_violation": 0.002730039384174723,
            "auditor_fp_violation": 0.013729525234888174,
            "ave_precision_score": 0.7784113258814533,
            "fpr": 0.31140350877192985,
            "logloss": 4.923829713886805,
            "mae": 0.3316553644338326,
            "precision": 0.623342175066313,
            "recall": 0.9591836734693877
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.8467638808582016,
            "auditor_fn_violation": 0.007719349710435671,
            "auditor_fp_violation": 0.0168730676764477,
            "ave_precision_score": 0.7703713654628276,
            "fpr": 0.3106476399560922,
            "logloss": 4.950689464182407,
            "mae": 0.33571073263805734,
            "precision": 0.6112637362637363,
            "recall": 0.959051724137931
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(96)",
        "seed": 9540,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.8617566119224075,
            "auditor_fn_violation": 0.00265843179377014,
            "auditor_fp_violation": 0.015184584684459967,
            "ave_precision_score": 0.8027502517915424,
            "fpr": 0.29714912280701755,
            "logloss": 4.262791681378083,
            "mae": 0.3224850199201048,
            "precision": 0.6332882273342354,
            "recall": 0.9551020408163265
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.8470614201065253,
            "auditor_fn_violation": 0.0072012566713350245,
            "auditor_fp_violation": 0.01707443451525845,
            "ave_precision_score": 0.7783754651627223,
            "fpr": 0.29857299670691545,
            "logloss": 4.495694895705099,
            "mae": 0.3252139581328779,
            "precision": 0.6201117318435754,
            "recall": 0.9568965517241379
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(97)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7813394670932049,
            "auditor_fn_violation": 0.015395631936985328,
            "auditor_fp_violation": 0.01706317036667499,
            "ave_precision_score": 0.7817698989081651,
            "fpr": 0.1699561403508772,
            "logloss": 1.1870669459222885,
            "mae": 0.293441783267592,
            "precision": 0.7075471698113207,
            "recall": 0.7653061224489796
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.7788045101495433,
            "auditor_fn_violation": 0.01193743139407245,
            "auditor_fp_violation": 0.025531841745310254,
            "ave_precision_score": 0.7800985543555924,
            "fpr": 0.1525795828759605,
            "logloss": 1.2013774374638087,
            "mae": 0.25962629869547366,
            "precision": 0.7306201550387597,
            "recall": 0.8125
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(98)",
        "seed": 9540,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.8174590598167107,
            "auditor_fn_violation": 0.0069392230576441165,
            "auditor_fp_violation": 0.008444541448407747,
            "ave_precision_score": 0.8177592902395034,
            "fpr": 0.1337719298245614,
            "logloss": 0.9011150412638581,
            "mae": 0.29319388880422775,
            "precision": 0.7453027139874739,
            "recall": 0.7285714285714285
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8331469990799947,
            "auditor_fn_violation": 0.011577841704833645,
            "auditor_fp_violation": 0.028238015603474317,
            "ave_precision_score": 0.8335564870054151,
            "fpr": 0.12184412733260154,
            "logloss": 0.7722945904802276,
            "mae": 0.25559829576085347,
            "precision": 0.7658227848101266,
            "recall": 0.7823275862068966
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(99)",
        "seed": 9540,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.8046427585952631,
            "auditor_fn_violation": 0.0049722520587182255,
            "auditor_fp_violation": 0.014976719048806852,
            "ave_precision_score": 0.8049864128027444,
            "fpr": 0.15789473684210525,
            "logloss": 1.0004018841836773,
            "mae": 0.2935839486029503,
            "precision": 0.7176470588235294,
            "recall": 0.746938775510204
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.818486544015195,
            "auditor_fn_violation": 0.010662307430258528,
            "auditor_fp_violation": 0.03149917611494609,
            "ave_precision_score": 0.8188226895872557,
            "fpr": 0.14270032930845225,
            "logloss": 0.8737014462132436,
            "mae": 0.26016036293796807,
            "precision": 0.7405189620758483,
            "recall": 0.7995689655172413
        }
    }
]
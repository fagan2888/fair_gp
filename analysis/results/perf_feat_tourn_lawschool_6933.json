[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 6933,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8658522805352833,
            "auditor_fn_violation": 0.03334703947368421,
            "auditor_fp_violation": 0.03300134015594542,
            "ave_precision_score": 0.8660715904242495,
            "fpr": 0.16885964912280702,
            "logloss": 1.137640731918928,
            "mae": 0.2683853796863327,
            "precision": 0.725,
            "recall": 0.8458333333333333
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.848265496662469,
            "auditor_fn_violation": 0.02859564534730231,
            "auditor_fp_violation": 0.038193249553512,
            "ave_precision_score": 0.8486112247206642,
            "fpr": 0.18221734357848518,
            "logloss": 1.2126978144381781,
            "mae": 0.2789366135473579,
            "precision": 0.7072310405643739,
            "recall": 0.8459915611814346
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.8740779596691712,
            "auditor_fn_violation": 0.0010736476608187138,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.874281451262119,
            "fpr": 0.0,
            "logloss": 3.803725480985805,
            "mae": 0.5150545494057702,
            "precision": 1.0,
            "recall": 0.004166666666666667
        },
        "train": {
            "accuracy": 0.4840834248079034,
            "auc_prc": 0.8476186056373334,
            "auditor_fn_violation": 0.0013709606450925747,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8480026160038824,
            "fpr": 0.0,
            "logloss": 3.72871086531883,
            "mae": 0.5072497899922221,
            "precision": 1.0,
            "recall": 0.008438818565400843
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.8753307326520386,
            "auditor_fn_violation": 0.0010736476608187138,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8755046858367508,
            "fpr": 0.0,
            "logloss": 4.104581279561767,
            "mae": 0.5189194439117344,
            "precision": 1.0,
            "recall": 0.004166666666666667
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 4.0276963613680525,
            "mae": 0.5123712041902396,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8732420266647918,
            "auditor_fn_violation": 0.02179276315789474,
            "auditor_fp_violation": 0.03420443469785575,
            "ave_precision_score": 0.8734613535171198,
            "fpr": 0.17982456140350878,
            "logloss": 0.812657291353239,
            "mae": 0.2633075485051817,
            "precision": 0.7191780821917808,
            "recall": 0.875
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8464189724247897,
            "auditor_fn_violation": 0.02456613264044242,
            "auditor_fp_violation": 0.03755271823906639,
            "ave_precision_score": 0.846817285007256,
            "fpr": 0.20197585071350166,
            "logloss": 0.9176115485366333,
            "mae": 0.2819659556909524,
            "precision": 0.6917922948073701,
            "recall": 0.8713080168776371
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 6933,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8758171179183581,
            "auditor_fn_violation": 0.016790021929824563,
            "auditor_fp_violation": 0.027615334632878494,
            "ave_precision_score": 0.8760256191752924,
            "fpr": 0.20175438596491227,
            "logloss": 0.842177803450969,
            "mae": 0.2685626087672733,
            "precision": 0.7003257328990228,
            "recall": 0.8958333333333334
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8499116776417495,
            "auditor_fn_violation": 0.018179123418879426,
            "auditor_fp_violation": 0.031810543396624535,
            "ave_precision_score": 0.850292033385475,
            "fpr": 0.21185510428100987,
            "logloss": 0.9446826304020005,
            "mae": 0.2856746456932227,
            "precision": 0.6841243862520459,
            "recall": 0.8818565400843882
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 6933,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8662221303080466,
            "auditor_fn_violation": 0.02208059210526316,
            "auditor_fp_violation": 0.029803240740740745,
            "ave_precision_score": 0.866437147937758,
            "fpr": 0.1787280701754386,
            "logloss": 0.8569119474260348,
            "mae": 0.26358737409337535,
            "precision": 0.7208904109589042,
            "recall": 0.8770833333333333
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8417823778303997,
            "auditor_fn_violation": 0.020934939580467517,
            "auditor_fp_violation": 0.03338800875141608,
            "ave_precision_score": 0.8421393875240233,
            "fpr": 0.1964873765093304,
            "logloss": 0.9591645862824244,
            "mae": 0.28285980511603176,
            "precision": 0.6960950764006791,
            "recall": 0.8649789029535865
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8061435436520272,
            "auditor_fn_violation": 0.03473364400584796,
            "auditor_fp_violation": 0.013523391812865496,
            "ave_precision_score": 0.8066283637949715,
            "fpr": 0.07894736842105263,
            "logloss": 0.8609155062013947,
            "mae": 0.31044441753029334,
            "precision": 0.7994428969359332,
            "recall": 0.5979166666666667
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7715421157786633,
            "auditor_fn_violation": 0.03169420167016355,
            "auditor_fp_violation": 0.025771965828282353,
            "ave_precision_score": 0.7721653284935648,
            "fpr": 0.10428100987925357,
            "logloss": 0.9363896575640176,
            "mae": 0.32216731997346504,
            "precision": 0.7538860103626943,
            "recall": 0.6139240506329114
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.8601761522849941,
            "auditor_fn_violation": 0.000584795321637421,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8604048610693382,
            "fpr": 0.0,
            "logloss": 3.8286599007331246,
            "mae": 0.5130881966445578,
            "precision": 1.0,
            "recall": 0.008333333333333333
        },
        "train": {
            "accuracy": 0.48518111964873767,
            "auc_prc": 0.8355893343397105,
            "auditor_fn_violation": 0.0021537050674596218,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8359813796955865,
            "fpr": 0.0,
            "logloss": 3.7219589869067846,
            "mae": 0.5046252771052369,
            "precision": 1.0,
            "recall": 0.010548523206751054
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 6933,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8670923504856407,
            "auditor_fn_violation": 0.0204312865497076,
            "auditor_fp_violation": 0.03183885640025991,
            "ave_precision_score": 0.867319523055326,
            "fpr": 0.18311403508771928,
            "logloss": 0.8492728117125558,
            "mae": 0.265378257889779,
            "precision": 0.7150170648464164,
            "recall": 0.8729166666666667
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.8434586195606805,
            "auditor_fn_violation": 0.026187200970788357,
            "auditor_fp_violation": 0.035786861321202595,
            "ave_precision_score": 0.8438199476298578,
            "fpr": 0.2052689352360044,
            "logloss": 0.9557738780781792,
            "mae": 0.2844752870624857,
            "precision": 0.6867671691792295,
            "recall": 0.8649789029535865
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 6933,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.8749877051671837,
            "auditor_fn_violation": 0.0004842836257309924,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8751720776863582,
            "fpr": 0.0,
            "logloss": 4.312419195532001,
            "mae": 0.5188752244936647,
            "precision": 1.0,
            "recall": 0.0020833333333333333
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 4.2423228072988275,
            "mae": 0.5124909433715773,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.8682779889940473,
            "auditor_fn_violation": 0.0010736476608187138,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8684932323134344,
            "fpr": 0.0,
            "logloss": 3.7661933421879117,
            "mae": 0.5138658972984903,
            "precision": 1.0,
            "recall": 0.004166666666666667
        },
        "train": {
            "accuracy": 0.4840834248079034,
            "auc_prc": 0.8428993082902401,
            "auditor_fn_violation": 0.0013709606450925747,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8432955615494713,
            "fpr": 0.0,
            "logloss": 3.6734175629005255,
            "mae": 0.5057603442824631,
            "precision": 1.0,
            "recall": 0.008438818565400843
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 6933,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.666299790928705,
            "auditor_fn_violation": 0.010297880116959065,
            "auditor_fp_violation": 0.010264376218323592,
            "ave_precision_score": 0.6623669700707207,
            "fpr": 0.28289473684210525,
            "logloss": 1.470737982036701,
            "mae": 0.3668646371814078,
            "precision": 0.6282420749279539,
            "recall": 0.9083333333333333
        },
        "train": {
            "accuracy": 0.6597145993413831,
            "auc_prc": 0.6632662083239534,
            "auditor_fn_violation": 0.00632216648834915,
            "auditor_fp_violation": 0.016189114986674427,
            "ave_precision_score": 0.6603232809074254,
            "fpr": 0.29637760702524696,
            "logloss": 1.3342407175430389,
            "mae": 0.3673902313366891,
            "precision": 0.6164772727272727,
            "recall": 0.9156118143459916
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.7339600701646242,
            "auditor_fn_violation": 0.003673245614035099,
            "auditor_fp_violation": 0.0001624431448992854,
            "ave_precision_score": 0.7351991364776367,
            "fpr": 0.0043859649122807015,
            "logloss": 3.425406147142947,
            "mae": 0.5028748457902358,
            "precision": 0.6666666666666666,
            "recall": 0.016666666666666666
        },
        "train": {
            "accuracy": 0.4840834248079034,
            "auc_prc": 0.7362114630907594,
            "auditor_fn_violation": 0.001841070460892899,
            "auditor_fp_violation": 0.0015674178047610317,
            "ave_precision_score": 0.7373813588120184,
            "fpr": 0.003293084522502744,
            "logloss": 3.011086013871335,
            "mae": 0.49561358167034,
            "precision": 0.7,
            "recall": 0.014767932489451477
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.869381794960558,
            "auditor_fn_violation": 0.025452302631578952,
            "auditor_fp_violation": 0.01972922758284601,
            "ave_precision_score": 0.8695440659151076,
            "fpr": 0.12171052631578948,
            "logloss": 1.042595975553671,
            "mae": 0.256181239241274,
            "precision": 0.7730061349693251,
            "recall": 0.7875
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.844007470112067,
            "auditor_fn_violation": 0.021342522474954495,
            "auditor_fp_violation": 0.029173061513613173,
            "ave_precision_score": 0.8443354022001741,
            "fpr": 0.1394072447859495,
            "logloss": 1.1222862181353663,
            "mae": 0.2734689044010443,
            "precision": 0.7439516129032258,
            "recall": 0.7784810126582279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 6933,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8703720534030188,
            "auditor_fn_violation": 0.021806469298245618,
            "auditor_fp_violation": 0.03133122157244965,
            "ave_precision_score": 0.8705983331925169,
            "fpr": 0.17653508771929824,
            "logloss": 0.8239971000115561,
            "mae": 0.2624179790077934,
            "precision": 0.7224137931034482,
            "recall": 0.8729166666666667
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8417766349114078,
            "auditor_fn_violation": 0.019392608854738386,
            "auditor_fp_violation": 0.03798978666539399,
            "ave_precision_score": 0.8421875426786574,
            "fpr": 0.20087815587266739,
            "logloss": 0.9363013090112032,
            "mae": 0.28217335432236124,
            "precision": 0.6929530201342282,
            "recall": 0.8713080168776371
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4791666666666667,
            "auc_prc": 0.8671166002151995,
            "auditor_fn_violation": 0.001793220029239764,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8673358893518703,
            "fpr": 0.0,
            "logloss": 3.3604561425938115,
            "mae": 0.5056812910750461,
            "precision": 1.0,
            "recall": 0.010416666666666666
        },
        "train": {
            "accuracy": 0.48957189901207465,
            "auc_prc": 0.8423181479810863,
            "auditor_fn_violation": 0.004029512706859907,
            "auditor_fp_violation": 0.0004948418390030821,
            "ave_precision_score": 0.8427085226041926,
            "fpr": 0.0010976948408342481,
            "logloss": 3.2766890479806254,
            "mae": 0.4962676750570591,
            "precision": 0.9090909090909091,
            "recall": 0.02109704641350211
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.8647910443821486,
            "auditor_fn_violation": 0.03848684210526317,
            "auditor_fp_violation": 0.004198140025990903,
            "ave_precision_score": 0.8650232570498524,
            "fpr": 0.015350877192982455,
            "logloss": 1.3700113264333362,
            "mae": 0.3453607553395302,
            "precision": 0.9306930693069307,
            "recall": 0.39166666666666666
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.8394213149912066,
            "auditor_fn_violation": 0.03838458225069127,
            "auditor_fp_violation": 0.006669061332757274,
            "ave_precision_score": 0.8398162647368209,
            "fpr": 0.01646542261251372,
            "logloss": 1.3631187234414688,
            "mae": 0.3438375363383079,
            "precision": 0.9264705882352942,
            "recall": 0.3987341772151899
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.8675966035632483,
            "auditor_fn_violation": 0.000584795321637421,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8678163434583113,
            "fpr": 0.0,
            "logloss": 3.764810070530259,
            "mae": 0.5133103321046588,
            "precision": 1.0,
            "recall": 0.008333333333333333
        },
        "train": {
            "accuracy": 0.48518111964873767,
            "auc_prc": 0.8427468494117765,
            "auditor_fn_violation": 0.0021537050674596218,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.843133036181629,
            "fpr": 0.0,
            "logloss": 3.6716978192940153,
            "mae": 0.5049154735118934,
            "precision": 1.0,
            "recall": 0.010548523206751054
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 6933,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 4.614936030665271,
            "mae": 0.5226549477667971,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 4.528346819142811,
            "mae": 0.5161950658044111,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8381710008623375,
            "auditor_fn_violation": 0.013760964912280705,
            "auditor_fp_violation": 0.021721694282001304,
            "ave_precision_score": 0.838802520272701,
            "fpr": 0.16337719298245615,
            "logloss": 0.682668023421326,
            "mae": 0.2625136811483225,
            "precision": 0.7315315315315315,
            "recall": 0.8458333333333333
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8175409137763088,
            "auditor_fn_violation": 0.012037590258768825,
            "auditor_fp_violation": 0.019788649785108022,
            "ave_precision_score": 0.8183779958383792,
            "fpr": 0.18660812294182216,
            "logloss": 0.7742918275054446,
            "mae": 0.2821506570295288,
            "precision": 0.7017543859649122,
            "recall": 0.8438818565400844
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.8030533052480394,
            "auditor_fn_violation": 0.07118055555555557,
            "auditor_fp_violation": 0.028874269005847955,
            "ave_precision_score": 0.80352995295362,
            "fpr": 0.06578947368421052,
            "logloss": 0.8826771329014776,
            "mae": 0.34246123488341507,
            "precision": 0.8125,
            "recall": 0.5416666666666666
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.8096031742788806,
            "auditor_fn_violation": 0.07736664397171004,
            "auditor_fp_violation": 0.030948965981507485,
            "ave_precision_score": 0.8100199054294369,
            "fpr": 0.07135016465422613,
            "logloss": 0.8628127557681596,
            "mae": 0.33045326885994614,
            "precision": 0.8036253776435045,
            "recall": 0.5611814345991561
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.8754894067928698,
            "auditor_fn_violation": 0.0010736476608187138,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8756632960050232,
            "fpr": 0.0,
            "logloss": 4.099659225608848,
            "mae": 0.5188915105717696,
            "precision": 1.0,
            "recall": 0.004166666666666667
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 4.023379921072725,
            "mae": 0.5123462360003112,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.6497335870452752,
            "auditor_fn_violation": 0.020803636695906434,
            "auditor_fp_violation": 0.030016447368421063,
            "ave_precision_score": 0.6494039271206771,
            "fpr": 0.3092105263157895,
            "logloss": 1.4227904012516739,
            "mae": 0.3770761178022393,
            "precision": 0.6044880785413744,
            "recall": 0.8979166666666667
        },
        "train": {
            "accuracy": 0.6311745334796927,
            "auc_prc": 0.6356797964417567,
            "auditor_fn_violation": 0.01379760730314441,
            "auditor_fp_violation": 0.007141296184191686,
            "ave_precision_score": 0.6349462717621494,
            "fpr": 0.32930845225027444,
            "logloss": 1.4509271172253717,
            "mae": 0.3842432465552243,
            "precision": 0.5934959349593496,
            "recall": 0.9240506329113924
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 6933,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.8698631195129778,
            "auditor_fn_violation": 0.0004842836257309924,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8700837808124777,
            "fpr": 0.0,
            "logloss": 4.115971251923203,
            "mae": 0.5179679878045124,
            "precision": 1.0,
            "recall": 0.0020833333333333333
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.8428396590454654,
            "auditor_fn_violation": 0.0010235888600184356,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8432060712030789,
            "fpr": 0.0,
            "logloss": 4.022752820680689,
            "mae": 0.510439215586701,
            "precision": 1.0,
            "recall": 0.004219409282700422
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8658923139645651,
            "auditor_fn_violation": 0.01769919590643275,
            "auditor_fp_violation": 0.03188454353476284,
            "ave_precision_score": 0.866136915309063,
            "fpr": 0.21271929824561403,
            "logloss": 0.8604093280121098,
            "mae": 0.2778704951663845,
            "precision": 0.692063492063492,
            "recall": 0.9083333333333333
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.8360510599099595,
            "auditor_fn_violation": 0.018331967004312045,
            "auditor_fp_violation": 0.033925552678048865,
            "ave_precision_score": 0.836585887741008,
            "fpr": 0.22283205268935236,
            "logloss": 0.9795631919680602,
            "mae": 0.29235960771151287,
            "precision": 0.6767515923566879,
            "recall": 0.8966244725738397
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 6933,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.8801364569149115,
            "auditor_fn_violation": 0.0004248903508771966,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.880302242869279,
            "fpr": 0.0,
            "logloss": 4.064777658005574,
            "mae": 0.5191765682115801,
            "precision": 1.0,
            "recall": 0.0020833333333333333
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 4.003420444676227,
            "mae": 0.5130025481374997,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.8473558911650492,
            "auditor_fn_violation": 0.036896929824561414,
            "auditor_fp_violation": 0.006337820825211176,
            "ave_precision_score": 0.8476280061592781,
            "fpr": 0.01206140350877193,
            "logloss": 1.7410385018701,
            "mae": 0.38131055250445567,
            "precision": 0.9308176100628931,
            "recall": 0.30833333333333335
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.8231862295139115,
            "auditor_fn_violation": 0.039646699736460615,
            "auditor_fp_violation": 0.0054558196665720525,
            "ave_precision_score": 0.8236829003718851,
            "fpr": 0.013172338090010977,
            "logloss": 1.6805916496182178,
            "mae": 0.3760905188538812,
            "precision": 0.922077922077922,
            "recall": 0.29957805907172996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.8678449324910005,
            "auditor_fn_violation": 0.000584795321637421,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8680516466193967,
            "fpr": 0.0,
            "logloss": 3.8720021532871582,
            "mae": 0.5130145386095442,
            "precision": 1.0,
            "recall": 0.008333333333333333
        },
        "train": {
            "accuracy": 0.4862788144895719,
            "auc_prc": 0.8393418004614395,
            "auditor_fn_violation": 0.0029040281232197308,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8397390529596627,
            "fpr": 0.0,
            "logloss": 3.794017491413868,
            "mae": 0.5047626840305014,
            "precision": 1.0,
            "recall": 0.012658227848101266
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 6933,
        "test": {
            "accuracy": 0.5032894736842105,
            "auc_prc": 0.858803046508358,
            "auditor_fn_violation": 0.008552631578947378,
            "auditor_fp_violation": 0.0005228638726445744,
            "ave_precision_score": 0.8590354582657068,
            "fpr": 0.0010964912280701754,
            "logloss": 2.9471663224007236,
            "mae": 0.4832524606563258,
            "precision": 0.9655172413793104,
            "recall": 0.058333333333333334
        },
        "train": {
            "accuracy": 0.5104281009879253,
            "auc_prc": 0.8274741541920598,
            "auditor_fn_violation": 0.010810209951506904,
            "auditor_fp_violation": 0.0009143270527772684,
            "ave_precision_score": 0.8279074375365969,
            "fpr": 0.0021953896816684962,
            "logloss": 2.8860161975919163,
            "mae": 0.4720161541912929,
            "precision": 0.9375,
            "recall": 0.06329113924050633
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 6933,
        "test": {
            "accuracy": 0.5032894736842105,
            "auc_prc": 0.8612654221569971,
            "auditor_fn_violation": 0.004925073099415209,
            "auditor_fp_violation": 0.0005228638726445744,
            "ave_precision_score": 0.861497521053206,
            "fpr": 0.0010964912280701754,
            "logloss": 2.557489868360174,
            "mae": 0.47996574553578225,
            "precision": 0.9655172413793104,
            "recall": 0.058333333333333334
        },
        "train": {
            "accuracy": 0.5148188803512623,
            "auc_prc": 0.8320493486687863,
            "auditor_fn_violation": 0.00649353656898573,
            "auditor_fp_violation": 0.0009143270527772684,
            "ave_precision_score": 0.8324690942036018,
            "fpr": 0.0021953896816684962,
            "logloss": 2.5166675364279683,
            "mae": 0.4698809355096478,
            "precision": 0.9444444444444444,
            "recall": 0.07172995780590717
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7902555903730776,
            "auditor_fn_violation": 0.014519371345029241,
            "auditor_fp_violation": 0.01369091130604289,
            "ave_precision_score": 0.7863247102057829,
            "fpr": 0.26973684210526316,
            "logloss": 1.4253080049849391,
            "mae": 0.3233069210092421,
            "precision": 0.6377025036818851,
            "recall": 0.9020833333333333
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.7589873574556972,
            "auditor_fn_violation": 0.013834660293552316,
            "auditor_fp_violation": 0.020780845350621836,
            "ave_precision_score": 0.7561854517747864,
            "fpr": 0.2689352360043908,
            "logloss": 1.5715474363733621,
            "mae": 0.33205593359426827,
            "precision": 0.6386430678466076,
            "recall": 0.9135021097046413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4769736842105263,
            "auc_prc": 0.8723252889981871,
            "auditor_fn_violation": 0.0016447368421052572,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8725427153559939,
            "fpr": 0.0,
            "logloss": 4.076006357765803,
            "mae": 0.5160057707625083,
            "precision": 1.0,
            "recall": 0.00625
        },
        "train": {
            "accuracy": 0.4840834248079034,
            "auc_prc": 0.8442079039466044,
            "auditor_fn_violation": 0.0015006461115202395,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8446147799723255,
            "fpr": 0.0,
            "logloss": 3.989127584151106,
            "mae": 0.508613474403153,
            "precision": 1.0,
            "recall": 0.008438818565400843
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8617544620800619,
            "auditor_fn_violation": 0.0063368055555555565,
            "auditor_fp_violation": 0.021929824561403518,
            "ave_precision_score": 0.8619627346139753,
            "fpr": 0.15789473684210525,
            "logloss": 0.8206002744256041,
            "mae": 0.2583746131725715,
            "precision": 0.7381818181818182,
            "recall": 0.8458333333333333
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.834758023668764,
            "auditor_fn_violation": 0.013028757752180337,
            "auditor_fp_violation": 0.023453493658739995,
            "ave_precision_score": 0.8351783533944472,
            "fpr": 0.1778265642151482,
            "logloss": 0.9079194953949612,
            "mae": 0.2799861113907665,
            "precision": 0.7086330935251799,
            "recall": 0.8312236286919831
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.8675799920663497,
            "auditor_fn_violation": 0.0010736476608187138,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8677976918402732,
            "fpr": 0.0,
            "logloss": 3.8735207086370584,
            "mae": 0.5150668681533109,
            "precision": 1.0,
            "recall": 0.004166666666666667
        },
        "train": {
            "accuracy": 0.4840834248079034,
            "auc_prc": 0.8426379730962005,
            "auditor_fn_violation": 0.0013709606450925747,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8430246544729082,
            "fpr": 0.0,
            "logloss": 3.7772240948777207,
            "mae": 0.5069832300977758,
            "precision": 1.0,
            "recall": 0.008438818565400843
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 6933,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.8361137227190059,
            "auditor_fn_violation": 0.0004842836257309924,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8366485148729512,
            "fpr": 0.0,
            "logloss": 4.329823548747398,
            "mae": 0.5237419552908402,
            "precision": 1.0,
            "recall": 0.0020833333333333333
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 4.16156315006476,
            "mae": 0.5175028754070377,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 6933,
        "test": {
            "accuracy": 0.5010964912280702,
            "auc_prc": 0.8611555649615693,
            "auditor_fn_violation": 0.008840460526315794,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.861367856827799,
            "fpr": 0.0,
            "logloss": 2.964540604525777,
            "mae": 0.48488080972864295,
            "precision": 1.0,
            "recall": 0.052083333333333336
        },
        "train": {
            "accuracy": 0.5093304061470911,
            "auc_prc": 0.8289566243958272,
            "auditor_fn_violation": 0.010474417225935244,
            "auditor_fp_violation": 0.0009143270527772684,
            "ave_precision_score": 0.8293783725799826,
            "fpr": 0.0021953896816684962,
            "logloss": 2.9070071120565655,
            "mae": 0.4739500502271411,
            "precision": 0.9354838709677419,
            "recall": 0.06118143459915612
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.8671816949233884,
            "auditor_fn_violation": 0.03369883040935673,
            "auditor_fp_violation": 0.00388594460688759,
            "ave_precision_score": 0.8674093656325909,
            "fpr": 0.01425438596491228,
            "logloss": 1.3604728017131795,
            "mae": 0.34791056838237333,
            "precision": 0.9312169312169312,
            "recall": 0.36666666666666664
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.8412531581895965,
            "auditor_fn_violation": 0.036214666499928216,
            "auditor_fp_violation": 0.005747198617457116,
            "ave_precision_score": 0.8416453366369209,
            "fpr": 0.014270032930845226,
            "logloss": 1.3569423087854378,
            "mae": 0.34612226800625956,
            "precision": 0.934010152284264,
            "recall": 0.3881856540084388
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4824561403508772,
            "auc_prc": 0.8659051705504495,
            "auditor_fn_violation": 0.002686403508771937,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8661011587407145,
            "fpr": 0.0,
            "logloss": 3.3406294107757937,
            "mae": 0.5019570714129394,
            "precision": 1.0,
            "recall": 0.016666666666666666
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0.8351611878767693,
            "auditor_fn_violation": 0.005449105401862838,
            "auditor_fp_violation": 0.0009143270527772684,
            "ave_precision_score": 0.8356058701801485,
            "fpr": 0.0021953896816684962,
            "logloss": 3.256519816920044,
            "mae": 0.49233020771701125,
            "precision": 0.8666666666666667,
            "recall": 0.027426160337552744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8599200673796474,
            "auditor_fn_violation": 0.040816885964912286,
            "auditor_fp_violation": 0.008584104938271605,
            "ave_precision_score": 0.8602561496241774,
            "fpr": 0.02412280701754386,
            "logloss": 0.87085432592772,
            "mae": 0.32048173142817704,
            "precision": 0.9147286821705426,
            "recall": 0.49166666666666664
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8541956992616977,
            "auditor_fn_violation": 0.05022301268601759,
            "auditor_fp_violation": 0.011343683984456442,
            "ave_precision_score": 0.8544741437669374,
            "fpr": 0.03951701427003293,
            "logloss": 0.8386080620675824,
            "mae": 0.31168167053503454,
            "precision": 0.8762886597938144,
            "recall": 0.5379746835443038
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 6933,
        "test": {
            "accuracy": 0.5427631578947368,
            "auc_prc": 0.7392906894461404,
            "auditor_fn_violation": 0.029861111111111123,
            "auditor_fp_violation": 0.00907397254710851,
            "ave_precision_score": 0.7378663968273053,
            "fpr": 0.027412280701754384,
            "logloss": 3.8201214466777826,
            "mae": 0.4483051044278202,
            "precision": 0.7787610619469026,
            "recall": 0.18333333333333332
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.7202375013127076,
            "auditor_fn_violation": 0.03440601740564224,
            "auditor_fp_violation": 0.01315977865247283,
            "ave_precision_score": 0.7184025023355428,
            "fpr": 0.03402854006586169,
            "logloss": 3.457842559798137,
            "mae": 0.4337387399675509,
            "precision": 0.7785714285714286,
            "recall": 0.229957805907173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.8675469830218105,
            "auditor_fn_violation": 0.000584795321637421,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8677681486187545,
            "fpr": 0.0,
            "logloss": 3.764562515258043,
            "mae": 0.5133085393580924,
            "precision": 1.0,
            "recall": 0.008333333333333333
        },
        "train": {
            "accuracy": 0.48518111964873767,
            "auc_prc": 0.8427221515174846,
            "auditor_fn_violation": 0.0021537050674596218,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8431084391281304,
            "fpr": 0.0,
            "logloss": 3.6715328530418527,
            "mae": 0.5049114493447304,
            "precision": 1.0,
            "recall": 0.010548523206751054
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7729767280957576,
            "auditor_fn_violation": 0.004673793859649124,
            "auditor_fp_violation": 0.029970760233918137,
            "ave_precision_score": 0.7622407602243951,
            "fpr": 0.20394736842105263,
            "logloss": 1.719394613948809,
            "mae": 0.3025845235588697,
            "precision": 0.694078947368421,
            "recall": 0.8791666666666667
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7290434626797584,
            "auditor_fn_violation": 0.00682932929455738,
            "auditor_fp_violation": 0.030700289118252132,
            "ave_precision_score": 0.7190795322729739,
            "fpr": 0.2305159165751921,
            "logloss": 1.950491611797499,
            "mae": 0.3278334904908057,
            "precision": 0.6618357487922706,
            "recall": 0.8670886075949367
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.8722158301122585,
            "auditor_fn_violation": 0.0010736476608187138,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8724377261521011,
            "fpr": 0.0,
            "logloss": 4.053449041033595,
            "mae": 0.5167380780186479,
            "precision": 1.0,
            "recall": 0.004166666666666667
        },
        "train": {
            "accuracy": 0.4829857299670692,
            "auc_prc": 0.8439942309835439,
            "auditor_fn_violation": 0.001097694840834254,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8444107284003788,
            "fpr": 0.0,
            "logloss": 3.9634385081179984,
            "mae": 0.5091086295295928,
            "precision": 1.0,
            "recall": 0.006329113924050633
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.8599806446266294,
            "auditor_fn_violation": 0.015784904970760235,
            "auditor_fp_violation": 0.0026016284925276167,
            "ave_precision_score": 0.8602089281736202,
            "fpr": 0.01206140350877193,
            "logloss": 1.4335966075280084,
            "mae": 0.35659440199360665,
            "precision": 0.9392265193370166,
            "recall": 0.3541666666666667
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.8271366742838495,
            "auditor_fn_violation": 0.01615510381784752,
            "auditor_fp_violation": 0.007397508709969934,
            "ave_precision_score": 0.8275050966472419,
            "fpr": 0.020856201975850714,
            "logloss": 1.4696584597616928,
            "mae": 0.35887783803036166,
            "precision": 0.898936170212766,
            "recall": 0.35654008438818563
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.8672536267512663,
            "auditor_fn_violation": 0.03380847953216375,
            "auditor_fp_violation": 0.00388594460688759,
            "ave_precision_score": 0.8674839913596628,
            "fpr": 0.01425438596491228,
            "logloss": 1.3686286203627558,
            "mae": 0.3489446688305192,
            "precision": 0.9312169312169312,
            "recall": 0.36666666666666664
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.8417579519559253,
            "auditor_fn_violation": 0.037849629701677115,
            "auditor_fp_violation": 0.005747198617457116,
            "ave_precision_score": 0.8421306153963432,
            "fpr": 0.014270032930845226,
            "logloss": 1.3656994861912604,
            "mae": 0.3469655971720076,
            "precision": 0.9333333333333333,
            "recall": 0.38396624472573837
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 6933,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8474608063625226,
            "auditor_fn_violation": 0.029970760233918134,
            "auditor_fp_violation": 0.04086460363872645,
            "ave_precision_score": 0.8477404584422814,
            "fpr": 0.16228070175438597,
            "logloss": 0.7731566094657476,
            "mae": 0.26957101542830625,
            "precision": 0.7299270072992701,
            "recall": 0.8333333333333334
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8155660484546694,
            "auditor_fn_violation": 0.023051591657519212,
            "auditor_fp_violation": 0.04276237292989071,
            "ave_precision_score": 0.8160480167818477,
            "fpr": 0.1877058177826564,
            "logloss": 0.904370748344035,
            "mae": 0.28803715339213964,
            "precision": 0.7015706806282722,
            "recall": 0.8481012658227848
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.81207564897087,
            "auditor_fn_violation": 0.01781798245614035,
            "auditor_fp_violation": 0.019432261208577002,
            "ave_precision_score": 0.8128622609247032,
            "fpr": 0.07675438596491228,
            "logloss": 0.7574597482640499,
            "mae": 0.3045696585635728,
            "precision": 0.8076923076923077,
            "recall": 0.6125
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8265407584819175,
            "auditor_fn_violation": 0.01790817342652161,
            "auditor_fp_violation": 0.016121294023968433,
            "ave_precision_score": 0.8269975561490269,
            "fpr": 0.06695938529088913,
            "logloss": 0.6950783062385011,
            "mae": 0.28931162669553107,
            "precision": 0.8305555555555556,
            "recall": 0.630801687763713
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 6933,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.8719723207820583,
            "auditor_fn_violation": 0.0004842836257309924,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8721849248648345,
            "fpr": 0.0,
            "logloss": 4.076345169942981,
            "mae": 0.519094567978748,
            "precision": 1.0,
            "recall": 0.0020833333333333333
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 4.01454967509095,
            "mae": 0.5135166403442644,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8485922006503137,
            "auditor_fn_violation": 0.009925529970760233,
            "auditor_fp_violation": 0.001797027290448343,
            "ave_precision_score": 0.8493819484080574,
            "fpr": 0.19846491228070176,
            "logloss": 0.7344228216495405,
            "mae": 0.30488675763103457,
            "precision": 0.7013201320132013,
            "recall": 0.8854166666666666
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8494214564337984,
            "auditor_fn_violation": 0.010490627909238702,
            "auditor_fp_violation": 0.007887326773957757,
            "ave_precision_score": 0.8497256739572445,
            "fpr": 0.21405049396267836,
            "logloss": 0.7384129012438564,
            "mae": 0.3000766473457332,
            "precision": 0.6844660194174758,
            "recall": 0.8924050632911392
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 6933,
        "test": {
            "accuracy": 0.5603070175438597,
            "auc_prc": 0.6759027023059514,
            "auditor_fn_violation": 0.03329906798245614,
            "auditor_fp_violation": 0.04807809454191034,
            "ave_precision_score": 0.6768065879096299,
            "fpr": 0.3925438596491228,
            "logloss": 1.596763539488326,
            "mae": 0.4269920582573598,
            "precision": 0.549685534591195,
            "recall": 0.9104166666666667
        },
        "train": {
            "accuracy": 0.5740944017563118,
            "auc_prc": 0.6781522215930524,
            "auditor_fn_violation": 0.028190378264715828,
            "auditor_fp_violation": 0.05345045427485577,
            "ave_precision_score": 0.6788866708768795,
            "fpr": 0.38529088913282106,
            "logloss": 1.6290832854136206,
            "mae": 0.41685721912405077,
            "precision": 0.5545685279187818,
            "recall": 0.9219409282700421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.8645925232515167,
            "auditor_fn_violation": 0.0010736476608187138,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.864803463747398,
            "fpr": 0.0,
            "logloss": 3.708764052296753,
            "mae": 0.5126146436968372,
            "precision": 1.0,
            "recall": 0.004166666666666667
        },
        "train": {
            "accuracy": 0.48518111964873767,
            "auc_prc": 0.838325157847654,
            "auditor_fn_violation": 0.0021537050674596218,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8386979768617157,
            "fpr": 0.0,
            "logloss": 3.612208547491254,
            "mae": 0.5043655837167155,
            "precision": 1.0,
            "recall": 0.010548523206751054
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8563947054365482,
            "auditor_fn_violation": 0.04259411549707602,
            "auditor_fp_violation": 0.02649599983755686,
            "ave_precision_score": 0.8565849274688206,
            "fpr": 0.11951754385964912,
            "logloss": 0.9184106538288715,
            "mae": 0.2607147909086267,
            "precision": 0.7729166666666667,
            "recall": 0.7729166666666667
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8276670114715458,
            "auditor_fn_violation": 0.030640507255438685,
            "auditor_fp_violation": 0.03700763864991071,
            "ave_precision_score": 0.828245657421129,
            "fpr": 0.14489571899012074,
            "logloss": 1.0051414320073666,
            "mae": 0.2755731687221744,
            "precision": 0.7354709418837675,
            "recall": 0.7742616033755274
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 6933,
        "test": {
            "accuracy": 0.48026315789473684,
            "auc_prc": 0.8668208498197248,
            "auditor_fn_violation": 0.0015762061403509081,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8670427285844757,
            "fpr": 0.0,
            "logloss": 3.485767819261277,
            "mae": 0.5074159584402245,
            "precision": 1.0,
            "recall": 0.0125
        },
        "train": {
            "accuracy": 0.4884742041712404,
            "auc_prc": 0.842056294006337,
            "auditor_fn_violation": 0.0031680306798760605,
            "auditor_fp_violation": 0.0004948418390030821,
            "ave_precision_score": 0.8424486739665997,
            "fpr": 0.0010976948408342481,
            "logloss": 3.3988730617571017,
            "mae": 0.4982165459133496,
            "precision": 0.9,
            "recall": 0.0189873417721519
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.8203278036769914,
            "auditor_fn_violation": 0.06576891447368421,
            "auditor_fp_violation": 0.023787768031189087,
            "ave_precision_score": 0.8205905649758056,
            "fpr": 0.05482456140350877,
            "logloss": 0.9831122358737995,
            "mae": 0.33558121170704036,
            "precision": 0.8281786941580757,
            "recall": 0.5020833333333333
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.8152436207636427,
            "auditor_fn_violation": 0.060929011102002266,
            "auditor_fp_violation": 0.024774746487753296,
            "ave_precision_score": 0.8156197255278236,
            "fpr": 0.06695938529088913,
            "logloss": 0.9477599212427397,
            "mae": 0.32560432875227524,
            "precision": 0.8117283950617284,
            "recall": 0.5548523206751055
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.8606943168723608,
            "auditor_fn_violation": 0.000584795321637421,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8609135820836842,
            "fpr": 0.0,
            "logloss": 3.7469375110929715,
            "mae": 0.5107719742605162,
            "precision": 1.0,
            "recall": 0.008333333333333333
        },
        "train": {
            "accuracy": 0.4862788144895719,
            "auc_prc": 0.8346761002378227,
            "auditor_fn_violation": 0.002542761466742651,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8350531624687977,
            "fpr": 0.0,
            "logloss": 3.6568665630053148,
            "mae": 0.5026123506757196,
            "precision": 1.0,
            "recall": 0.012658227848101266
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.8673210741196368,
            "auditor_fn_violation": 0.03392269736842106,
            "auditor_fp_violation": 0.00388594460688759,
            "ave_precision_score": 0.8675471148492939,
            "fpr": 0.01425438596491228,
            "logloss": 1.339796513617252,
            "mae": 0.3460325157764413,
            "precision": 0.9322916666666666,
            "recall": 0.3729166666666667
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.8415733148893205,
            "auditor_fn_violation": 0.03589508445766003,
            "auditor_fp_violation": 0.005747198617457116,
            "ave_precision_score": 0.841969501970967,
            "fpr": 0.014270032930845226,
            "logloss": 1.3345930382280844,
            "mae": 0.3440583558744687,
            "precision": 0.935,
            "recall": 0.39451476793248946
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8619991773085933,
            "auditor_fn_violation": 0.02012061403508772,
            "auditor_fp_violation": 0.029727095516569196,
            "ave_precision_score": 0.8622230188963931,
            "fpr": 0.19078947368421054,
            "logloss": 0.9097111764249202,
            "mae": 0.2646622083313974,
            "precision": 0.7090301003344481,
            "recall": 0.8833333333333333
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8329730259117697,
            "auditor_fn_violation": 0.01726206190628373,
            "auditor_fp_violation": 0.032064244034895155,
            "ave_precision_score": 0.833400233338296,
            "fpr": 0.1986827661909989,
            "logloss": 1.0277666187679482,
            "mae": 0.28560580390086876,
            "precision": 0.6947723440134908,
            "recall": 0.869198312236287
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7629130850019472,
            "auditor_fn_violation": 0.06681743421052633,
            "auditor_fp_violation": 0.026879264132553614,
            "ave_precision_score": 0.7637449710055783,
            "fpr": 0.0756578947368421,
            "logloss": 1.4969648595430227,
            "mae": 0.33394901843702396,
            "precision": 0.7870370370370371,
            "recall": 0.53125
        },
        "train": {
            "accuracy": 0.6630076838638859,
            "auc_prc": 0.7377167186234176,
            "auditor_fn_violation": 0.06902277369422948,
            "auditor_fp_violation": 0.036166156334854704,
            "ave_precision_score": 0.738500765375578,
            "fpr": 0.10098792535675083,
            "logloss": 1.4725341905513922,
            "mae": 0.34395462624699147,
            "precision": 0.7378917378917379,
            "recall": 0.5464135021097046
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8621838123903642,
            "auditor_fn_violation": 0.02108918128654971,
            "auditor_fp_violation": 0.036468486029889544,
            "ave_precision_score": 0.8624128132246147,
            "fpr": 0.19298245614035087,
            "logloss": 0.8941740067922531,
            "mae": 0.2751993950496837,
            "precision": 0.7066666666666667,
            "recall": 0.8833333333333333
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.8384305527244342,
            "auditor_fn_violation": 0.024584659135646374,
            "auditor_fp_violation": 0.04209923462787643,
            "ave_precision_score": 0.8387664781359653,
            "fpr": 0.22063666300768386,
            "logloss": 1.0039933511063452,
            "mae": 0.2964009936450345,
            "precision": 0.6710310965630114,
            "recall": 0.8649789029535865
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 6933,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.8518612107852109,
            "auditor_fn_violation": 0.0004842836257309924,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.852203680533655,
            "fpr": 0.0,
            "logloss": 4.054445549218808,
            "mae": 0.5227858060854812,
            "precision": 1.0,
            "recall": 0.0020833333333333333
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 3.8988464303716497,
            "mae": 0.5163215905139442,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.8742576988254676,
            "auditor_fn_violation": 0.019928728070175445,
            "auditor_fp_violation": 0.0005127111760883695,
            "ave_precision_score": 0.8744544929424376,
            "fpr": 0.007675438596491228,
            "logloss": 1.4580845804430744,
            "mae": 0.36241940509962045,
            "precision": 0.9570552147239264,
            "recall": 0.325
        },
        "train": {
            "accuracy": 0.6399560922063666,
            "auc_prc": 0.8437860511062184,
            "auditor_fn_violation": 0.024149286498353472,
            "auditor_fp_violation": 0.006088815318494776,
            "ave_precision_score": 0.8442091826210212,
            "fpr": 0.013172338090010977,
            "logloss": 1.4702657944446065,
            "mae": 0.3590240505674153,
            "precision": 0.9294117647058824,
            "recall": 0.3333333333333333
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4769736842105263,
            "auc_prc": 0.8760889994950484,
            "auditor_fn_violation": 0.001466557017543858,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8762621196920842,
            "fpr": 0.0,
            "logloss": 3.2328928717207934,
            "mae": 0.5089989126554973,
            "precision": 1.0,
            "recall": 0.00625
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 3.2007403048753393,
            "mae": 0.502812703622366,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7300957758640185,
            "auditor_fn_violation": 0.004719480994152054,
            "auditor_fp_violation": 0.031498741065627035,
            "ave_precision_score": 0.7076240389843279,
            "fpr": 0.14364035087719298,
            "logloss": 2.2541676479596178,
            "mae": 0.3018768045697614,
            "precision": 0.7321063394683026,
            "recall": 0.7458333333333333
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.6905298471971208,
            "auditor_fn_violation": 0.013478025260876216,
            "auditor_fp_violation": 0.02959757050240263,
            "ave_precision_score": 0.669451586857887,
            "fpr": 0.17453347969264543,
            "logloss": 2.4543317300079877,
            "mae": 0.3266244675968206,
            "precision": 0.6912621359223301,
            "recall": 0.7510548523206751
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6513157894736842,
            "auc_prc": 0.8441018305918772,
            "auditor_fn_violation": 0.04577850877192983,
            "auditor_fp_violation": 0.006578947368421053,
            "ave_precision_score": 0.8445276307765186,
            "fpr": 0.019736842105263157,
            "logloss": 1.5373203704915956,
            "mae": 0.3584876964739016,
            "precision": 0.9090909090909091,
            "recall": 0.375
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.813294268960628,
            "auditor_fn_violation": 0.03763657500683164,
            "auditor_fp_violation": 0.008088277774568144,
            "ave_precision_score": 0.8138836687635937,
            "fpr": 0.025246981339187707,
            "logloss": 1.5314600922036576,
            "mae": 0.3576880522680236,
            "precision": 0.8844221105527639,
            "recall": 0.37130801687763715
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.725516794167727,
            "auditor_fn_violation": 0.004605263157894738,
            "auditor_fp_violation": 0.016493055555555563,
            "ave_precision_score": 0.7103800951197412,
            "fpr": 0.2565789473684211,
            "logloss": 2.127433588126275,
            "mae": 0.3433444211751489,
            "precision": 0.6465256797583081,
            "recall": 0.8916666666666667
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.7056760669457198,
            "auditor_fn_violation": 0.009316511275688147,
            "auditor_fp_violation": 0.01830412426809879,
            "ave_precision_score": 0.6938842484627001,
            "fpr": 0.2689352360043908,
            "logloss": 2.138669171176832,
            "mae": 0.35560884319549374,
            "precision": 0.6321321321321322,
            "recall": 0.8881856540084389
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.7940930638939305,
            "auditor_fn_violation": 0.003426535087719302,
            "auditor_fp_violation": 0.019686078622482138,
            "ave_precision_score": 0.7876782338178671,
            "fpr": 0.20833333333333334,
            "logloss": 1.2723433360225802,
            "mae": 0.3032807942881686,
            "precision": 0.6900489396411092,
            "recall": 0.88125
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7606478164660655,
            "auditor_fn_violation": 0.009559671525240034,
            "auditor_fp_violation": 0.03136593930777406,
            "ave_precision_score": 0.7548653015398719,
            "fpr": 0.22502744237102085,
            "logloss": 1.420548512128395,
            "mae": 0.31429205205706884,
            "precision": 0.6709470304975923,
            "recall": 0.8818565400843882
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.848881949944331,
            "auditor_fn_violation": 0.03000502558479532,
            "auditor_fp_violation": 0.03640249350227421,
            "ave_precision_score": 0.8491990353841412,
            "fpr": 0.18530701754385964,
            "logloss": 0.8145561681509644,
            "mae": 0.2713164540019195,
            "precision": 0.7096219931271478,
            "recall": 0.8604166666666667
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.8149356386831431,
            "auditor_fn_violation": 0.02276443098185793,
            "auditor_fp_violation": 0.04066243497351215,
            "ave_precision_score": 0.8154453646019726,
            "fpr": 0.20965971459934138,
            "logloss": 0.9929650231200412,
            "mae": 0.29271710288401,
            "precision": 0.6784511784511784,
            "recall": 0.8502109704641351
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4824561403508772,
            "auc_prc": 0.8711920983921189,
            "auditor_fn_violation": 0.002686403508771937,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8713947903201504,
            "fpr": 0.0,
            "logloss": 2.9381875280332643,
            "mae": 0.496316226106384,
            "precision": 1.0,
            "recall": 0.016666666666666666
        },
        "train": {
            "accuracy": 0.49286498353457736,
            "auc_prc": 0.8463068078394034,
            "auditor_fn_violation": 0.004080460568670769,
            "auditor_fp_violation": 0.0004948418390030821,
            "ave_precision_score": 0.8466366078166936,
            "fpr": 0.0010976948408342481,
            "logloss": 2.8693136397401213,
            "mae": 0.4867100638145413,
            "precision": 0.9285714285714286,
            "recall": 0.027426160337552744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7336969087595346,
            "auditor_fn_violation": 0.009585160818713453,
            "auditor_fp_violation": 0.018599740090968167,
            "ave_precision_score": 0.7319571992526217,
            "fpr": 0.2324561403508772,
            "logloss": 1.3788548505277067,
            "mae": 0.2915195465101325,
            "precision": 0.6728395061728395,
            "recall": 0.9083333333333333
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7126593972682915,
            "auditor_fn_violation": 0.0066371169068163635,
            "auditor_fp_violation": 0.023817717347346324,
            "ave_precision_score": 0.7109526803734931,
            "fpr": 0.24698133918770582,
            "logloss": 1.4622528637270429,
            "mae": 0.3156913264561726,
            "precision": 0.655436447166922,
            "recall": 0.9029535864978903
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8757438244308803,
            "auditor_fn_violation": 0.019739126461988308,
            "auditor_fp_violation": 0.0245796783625731,
            "ave_precision_score": 0.8759140879033344,
            "fpr": 0.15899122807017543,
            "logloss": 0.7230837861804543,
            "mae": 0.2522089144068311,
            "precision": 0.7382671480144405,
            "recall": 0.8520833333333333
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.84793824791582,
            "auditor_fn_violation": 0.016442264493508784,
            "auditor_fp_violation": 0.030657587030622425,
            "ave_precision_score": 0.8482947616638041,
            "fpr": 0.1800219538968167,
            "logloss": 0.8289851652294171,
            "mae": 0.27606412790782614,
            "precision": 0.708185053380783,
            "recall": 0.8396624472573839
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 6933,
        "test": {
            "accuracy": 0.48026315789473684,
            "auc_prc": 0.8675471635760859,
            "auditor_fn_violation": 0.002302631578947398,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8677608731687428,
            "fpr": 0.0,
            "logloss": 3.334655811169567,
            "mae": 0.5048568417190645,
            "precision": 1.0,
            "recall": 0.0125
        },
        "train": {
            "accuracy": 0.48957189901207465,
            "auc_prc": 0.8422787740227269,
            "auditor_fn_violation": 0.004029512706859907,
            "auditor_fp_violation": 0.0004948418390030821,
            "ave_precision_score": 0.8426677635992129,
            "fpr": 0.0010976948408342481,
            "logloss": 3.252222395544273,
            "mae": 0.4953486596618202,
            "precision": 0.9090909090909091,
            "recall": 0.02109704641350211
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8387953880325305,
            "auditor_fn_violation": 0.031067251461988306,
            "auditor_fp_violation": 0.03565627030539312,
            "ave_precision_score": 0.8395797522159408,
            "fpr": 0.16666666666666666,
            "logloss": 0.8337124437030807,
            "mae": 0.2705988911266783,
            "precision": 0.7246376811594203,
            "recall": 0.8333333333333334
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8139534093897426,
            "auditor_fn_violation": 0.021768631864645428,
            "auditor_fp_violation": 0.037422100088669634,
            "ave_precision_score": 0.8145321553078115,
            "fpr": 0.18990120746432493,
            "logloss": 1.0306972179649443,
            "mae": 0.2847471167886243,
            "precision": 0.6980802792321117,
            "recall": 0.8438818565400844
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 6933,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.83397932737552,
            "auditor_fn_violation": 0.0004842836257309924,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.834522734642306,
            "fpr": 0.0,
            "logloss": 3.680746334624967,
            "mae": 0.5203463574414852,
            "precision": 1.0,
            "recall": 0.0020833333333333333
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.83194033139012,
            "auditor_fn_violation": 0.0010235888600184356,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8322514363890414,
            "fpr": 0.0,
            "logloss": 3.5233716067756298,
            "mae": 0.513021212752555,
            "precision": 1.0,
            "recall": 0.004219409282700422
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7598707809033284,
            "auditor_fn_violation": 0.0056537828947368415,
            "auditor_fp_violation": 0.016041260558804418,
            "ave_precision_score": 0.7578763543574253,
            "fpr": 0.24561403508771928,
            "logloss": 1.3414586448013015,
            "mae": 0.31462889650636205,
            "precision": 0.6600910470409712,
            "recall": 0.90625
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7222808667446984,
            "auditor_fn_violation": 0.005900688722459208,
            "auditor_fp_violation": 0.02698018371945231,
            "ave_precision_score": 0.7235120402235026,
            "fpr": 0.2524698133918771,
            "logloss": 1.4801643456844007,
            "mae": 0.3270599469782735,
            "precision": 0.6536144578313253,
            "recall": 0.9156118143459916
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8311857076554079,
            "auditor_fn_violation": 0.027419133771929827,
            "auditor_fp_violation": 0.028513848278102667,
            "ave_precision_score": 0.8314968123744819,
            "fpr": 0.19188596491228072,
            "logloss": 0.8640097739038485,
            "mae": 0.2835662304849853,
            "precision": 0.7013651877133106,
            "recall": 0.85625
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.8046090400116508,
            "auditor_fn_violation": 0.024269708717179157,
            "auditor_fp_violation": 0.04868289178537428,
            "ave_precision_score": 0.8052682437550438,
            "fpr": 0.21514818880351264,
            "logloss": 1.0001917804728557,
            "mae": 0.3007841136141994,
            "precision": 0.6733333333333333,
            "recall": 0.8523206751054853
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8659892468955732,
            "auditor_fn_violation": 0.011835252192982456,
            "auditor_fp_violation": 0.020239400584795324,
            "ave_precision_score": 0.866177405836493,
            "fpr": 0.18201754385964913,
            "logloss": 0.7028413046926972,
            "mae": 0.26240758049428686,
            "precision": 0.7200674536256324,
            "recall": 0.8895833333333333
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.8356279103505773,
            "auditor_fn_violation": 0.010986211655944456,
            "auditor_fp_violation": 0.023503731408892586,
            "ave_precision_score": 0.8363526626499028,
            "fpr": 0.20636663007683864,
            "logloss": 0.8239726985468226,
            "mae": 0.28813784648207613,
            "precision": 0.6835016835016835,
            "recall": 0.8565400843881856
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4791666666666667,
            "auc_prc": 0.7364214384685633,
            "auditor_fn_violation": 0.009786184210526318,
            "auditor_fp_violation": 0.008464810753736193,
            "ave_precision_score": 0.7276331638065999,
            "fpr": 0.025219298245614034,
            "logloss": 5.137488586976147,
            "mae": 0.5065971944499619,
            "precision": 0.5490196078431373,
            "recall": 0.058333333333333334
        },
        "train": {
            "accuracy": 0.48737650933040616,
            "auc_prc": 0.714267617521108,
            "auditor_fn_violation": 0.014270032930845224,
            "auditor_fp_violation": 0.01382542884199474,
            "ave_precision_score": 0.7013117322866027,
            "fpr": 0.03512623490669594,
            "logloss": 5.024471040928004,
            "mae": 0.4953995730625027,
            "precision": 0.5492957746478874,
            "recall": 0.08227848101265822
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 6933,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.7557124236568831,
            "auditor_fn_violation": 0.008580043859649122,
            "auditor_fp_violation": 0.024589831059129305,
            "ave_precision_score": 0.7494330910794818,
            "fpr": 0.19956140350877194,
            "logloss": 1.5536368126890137,
            "mae": 0.30501850250263063,
            "precision": 0.6961602671118531,
            "recall": 0.86875
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7184267043749433,
            "auditor_fn_violation": 0.01015251937176655,
            "auditor_fp_violation": 0.026565722280693396,
            "ave_precision_score": 0.7140857516474268,
            "fpr": 0.21844127332601537,
            "logloss": 1.6867203407149682,
            "mae": 0.32434594597568417,
            "precision": 0.6699834162520729,
            "recall": 0.8523206751054853
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 6933,
        "test": {
            "accuracy": 0.5877192982456141,
            "auc_prc": 0.8698786994533502,
            "auditor_fn_violation": 0.020202850877193007,
            "auditor_fp_violation": 0.0018985542560103967,
            "ave_precision_score": 0.8700985522133202,
            "fpr": 0.0043859649122807015,
            "logloss": 1.7855440602868398,
            "mae": 0.40651963773204164,
            "precision": 0.9642857142857143,
            "recall": 0.225
        },
        "train": {
            "accuracy": 0.5949506037321625,
            "auc_prc": 0.8447696928305444,
            "auditor_fn_violation": 0.026469730022648637,
            "auditor_fp_violation": 0.00302431255918635,
            "ave_precision_score": 0.8451195661622735,
            "fpr": 0.007683863885839737,
            "logloss": 1.7622012308696493,
            "mae": 0.3988037419905049,
            "precision": 0.9411764705882353,
            "recall": 0.23628691983122363
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.8755314309170095,
            "auditor_fn_violation": 0.0010736476608187138,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8757074204604113,
            "fpr": 0.0,
            "logloss": 4.10444141471438,
            "mae": 0.5191056301548853,
            "precision": 1.0,
            "recall": 0.004166666666666667
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 4.027286497333493,
            "mae": 0.5125730815525954,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 6933,
        "test": {
            "accuracy": 0.5537280701754386,
            "auc_prc": 0.8645600801962583,
            "auditor_fn_violation": 0.011787280701754388,
            "auditor_fp_violation": 0.0005228638726445744,
            "ave_precision_score": 0.864748488917471,
            "fpr": 0.0010964912280701754,
            "logloss": 1.938310615912646,
            "mae": 0.436896048084426,
            "precision": 0.9866666666666667,
            "recall": 0.15416666666666667
        },
        "train": {
            "accuracy": 0.5565312843029637,
            "auc_prc": 0.832721826219837,
            "auditor_fn_violation": 0.014617404715919357,
            "auditor_fp_violation": 0.0019793673560123282,
            "ave_precision_score": 0.8331548975867111,
            "fpr": 0.0043907793633369925,
            "logloss": 1.911722246589018,
            "mae": 0.42959748892637284,
            "precision": 0.9487179487179487,
            "recall": 0.15611814345991562
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 6933,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8534295692173676,
            "auditor_fn_violation": 0.011554276315789475,
            "auditor_fp_violation": 0.003959551656920084,
            "ave_precision_score": 0.8542123868570335,
            "fpr": 0.19407894736842105,
            "logloss": 0.7237881146453151,
            "mae": 0.3026063647514348,
            "precision": 0.7079207920792079,
            "recall": 0.89375
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.854627426125111,
            "auditor_fn_violation": 0.008343870277480583,
            "auditor_fp_violation": 0.011220601496582576,
            "ave_precision_score": 0.8549195162803104,
            "fpr": 0.20965971459934138,
            "logloss": 0.7285496654120062,
            "mae": 0.2986445030469435,
            "precision": 0.6889250814332247,
            "recall": 0.8924050632911392
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6206140350877193,
            "auc_prc": 0.8792099146299415,
            "auditor_fn_violation": 0.019371345029239793,
            "auditor_fp_violation": 0.0008071393762183239,
            "ave_precision_score": 0.879382957814344,
            "fpr": 0.006578947368421052,
            "logloss": 1.53112165280295,
            "mae": 0.3758432300375659,
            "precision": 0.958904109589041,
            "recall": 0.2916666666666667
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.8488742576335163,
            "auditor_fn_violation": 0.021330943415452015,
            "auditor_fp_violation": 0.004837895339695107,
            "ave_precision_score": 0.849191624044655,
            "fpr": 0.009879253567508232,
            "logloss": 1.5418100042152414,
            "mae": 0.37105883286865904,
            "precision": 0.9407894736842105,
            "recall": 0.30168776371308015
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 6933,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 5.281685711949169,
            "mae": 0.5250105586198788,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 5.152192544330762,
            "mae": 0.5187721146266984,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 6933,
        "test": {
            "accuracy": 0.48026315789473684,
            "auc_prc": 0.8666246047114443,
            "auditor_fn_violation": 0.0015762061403509081,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8668465615614829,
            "fpr": 0.0,
            "logloss": 3.485342620770704,
            "mae": 0.5073312068808853,
            "precision": 1.0,
            "recall": 0.0125
        },
        "train": {
            "accuracy": 0.4884742041712404,
            "auc_prc": 0.8419253333877639,
            "auditor_fn_violation": 0.0031680306798760605,
            "auditor_fp_violation": 0.0004948418390030821,
            "ave_precision_score": 0.8423179263538103,
            "fpr": 0.0010976948408342481,
            "logloss": 3.3978979203842714,
            "mae": 0.49810805680297987,
            "precision": 0.9,
            "recall": 0.0189873417721519
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6282894736842105,
            "auc_prc": 0.8739991546444861,
            "auditor_fn_violation": 0.02260142543859649,
            "auditor_fp_violation": 0.0008071393762183239,
            "ave_precision_score": 0.8742113515400916,
            "fpr": 0.006578947368421052,
            "logloss": 1.5186243453687056,
            "mae": 0.37296023867266664,
            "precision": 0.9607843137254902,
            "recall": 0.30625
        },
        "train": {
            "accuracy": 0.6256860592755215,
            "auc_prc": 0.8459717849641294,
            "auditor_fn_violation": 0.027511845377871054,
            "auditor_fp_violation": 0.005387998703866047,
            "ave_precision_score": 0.8463465176144602,
            "fpr": 0.012074643249176729,
            "logloss": 1.5248027348764008,
            "mae": 0.3690924808541262,
            "precision": 0.9290322580645162,
            "recall": 0.3037974683544304
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.8480066350989203,
            "auditor_fn_violation": 0.0010736476608187138,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.848268084002683,
            "fpr": 0.0,
            "logloss": 4.661652908057258,
            "mae": 0.5219775511995427,
            "precision": 1.0,
            "recall": 0.004166666666666667
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.8247336727611809,
            "auditor_fn_violation": 0.0010235888600184356,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8251946652610017,
            "fpr": 0.0,
            "logloss": 4.5206523771801095,
            "mae": 0.5149663186857766,
            "precision": 1.0,
            "recall": 0.004219409282700422
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 6933,
        "test": {
            "accuracy": 0.48355263157894735,
            "auc_prc": 0.8679069248037565,
            "auditor_fn_violation": 0.0031181469298245707,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8681211214104032,
            "fpr": 0.0,
            "logloss": 3.205225313957718,
            "mae": 0.5008597049729375,
            "precision": 1.0,
            "recall": 0.01875
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.8425597730331158,
            "auditor_fn_violation": 0.004126776806680663,
            "auditor_fp_violation": 0.0004948418390030821,
            "ave_precision_score": 0.842950332547428,
            "fpr": 0.0010976948408342481,
            "logloss": 3.126514452842996,
            "mae": 0.4909224182088301,
            "precision": 0.9166666666666666,
            "recall": 0.023206751054852322
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 6933,
        "test": {
            "accuracy": 0.5285087719298246,
            "auc_prc": 0.7637063605364048,
            "auditor_fn_violation": 0.024273574561403517,
            "auditor_fp_violation": 0.0068962191358024685,
            "ave_precision_score": 0.7644237955024475,
            "fpr": 0.020833333333333332,
            "logloss": 3.200363603464735,
            "mae": 0.45639123200159704,
            "precision": 0.7840909090909091,
            "recall": 0.14375
        },
        "train": {
            "accuracy": 0.5477497255762898,
            "auc_prc": 0.7361354401797037,
            "auditor_fn_violation": 0.03311611017706699,
            "auditor_fp_violation": 0.010580070182136964,
            "ave_precision_score": 0.7368553721505536,
            "fpr": 0.029637760702524697,
            "logloss": 2.9868369823999963,
            "mae": 0.44501851279917465,
            "precision": 0.7672413793103449,
            "recall": 0.1877637130801688
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6567982456140351,
            "auc_prc": 0.7550956293636796,
            "auditor_fn_violation": 0.011924342105263162,
            "auditor_fp_violation": 0.016868705328135152,
            "ave_precision_score": 0.7560055873253261,
            "fpr": 0.0668859649122807,
            "logloss": 1.2152897867536674,
            "mae": 0.34859930647526555,
            "precision": 0.7889273356401384,
            "recall": 0.475
        },
        "train": {
            "accuracy": 0.6586169045005489,
            "auc_prc": 0.7348817055893682,
            "auditor_fn_violation": 0.01705363883523926,
            "auditor_fp_violation": 0.014573971319268438,
            "ave_precision_score": 0.7358593036651193,
            "fpr": 0.0801317233809001,
            "logloss": 1.2019134244666532,
            "mae": 0.3526798737665672,
            "precision": 0.7637540453074434,
            "recall": 0.4978902953586498
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.8192721674699529,
            "auditor_fn_violation": 0.0493078399122807,
            "auditor_fp_violation": 0.008599333983105912,
            "ave_precision_score": 0.8196731231255698,
            "fpr": 0.02412280701754386,
            "logloss": 1.8008661832593549,
            "mae": 0.3516789900474511,
            "precision": 0.893719806763285,
            "recall": 0.3854166666666667
        },
        "train": {
            "accuracy": 0.6597145993413831,
            "auc_prc": 0.8011173053100419,
            "auditor_fn_violation": 0.060498270088510345,
            "auditor_fp_violation": 0.012458962037844096,
            "ave_precision_score": 0.8016428219158791,
            "fpr": 0.03402854006586169,
            "logloss": 1.7335912755178247,
            "mae": 0.3494776936920746,
            "precision": 0.8628318584070797,
            "recall": 0.41139240506329117
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 6933,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.8110819952785018,
            "auditor_fn_violation": 0.047583150584795335,
            "auditor_fp_violation": 0.06573363385315141,
            "ave_precision_score": 0.811394173817308,
            "fpr": 0.26535087719298245,
            "logloss": 1.0215679676354967,
            "mae": 0.33993102012478965,
            "precision": 0.6288343558282209,
            "recall": 0.8541666666666666
        },
        "train": {
            "accuracy": 0.6695938529088913,
            "auc_prc": 0.8086575513684047,
            "auditor_fn_violation": 0.033104531117564506,
            "auditor_fp_violation": 0.07095328642802062,
            "ave_precision_score": 0.8090543300623921,
            "fpr": 0.270032930845225,
            "logloss": 1.0821484403164026,
            "mae": 0.33685930065212255,
            "precision": 0.6300751879699248,
            "recall": 0.8839662447257384
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 6933,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 5.328797133324976,
            "mae": 0.5250441010417721,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 5.2026759678519765,
            "mae": 0.5188199093737855,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.867483395386372,
            "auditor_fn_violation": 0.01800986842105264,
            "auditor_fp_violation": 0.01989928525016245,
            "ave_precision_score": 0.8676635616904729,
            "fpr": 0.16885964912280702,
            "logloss": 0.8742689826548501,
            "mae": 0.2530709897747646,
            "precision": 0.7288732394366197,
            "recall": 0.8625
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8337106561693265,
            "auditor_fn_violation": 0.013635500470109821,
            "auditor_fp_violation": 0.02529219531432505,
            "ave_precision_score": 0.8341388256482757,
            "fpr": 0.18441273326015367,
            "logloss": 1.0185992889575501,
            "mae": 0.2832578114496792,
            "precision": 0.7031802120141343,
            "recall": 0.8396624472573839
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 6933,
        "test": {
            "accuracy": 0.5646929824561403,
            "auc_prc": 0.84283393689702,
            "auditor_fn_violation": 0.036079130116959075,
            "auditor_fp_violation": 0.0033503898635477585,
            "ave_precision_score": 0.8430791220122575,
            "fpr": 0.006578947368421052,
            "logloss": 2.165007877817716,
            "mae": 0.42685259677264364,
            "precision": 0.9368421052631579,
            "recall": 0.18541666666666667
        },
        "train": {
            "accuracy": 0.5762897914379802,
            "auc_prc": 0.8084121120335923,
            "auditor_fn_violation": 0.03961659418175418,
            "auditor_fp_violation": 0.0035342257232352106,
            "ave_precision_score": 0.8088354391056078,
            "fpr": 0.007683863885839737,
            "logloss": 2.145631047757615,
            "mae": 0.41545286618423843,
            "precision": 0.9313725490196079,
            "recall": 0.20042194092827004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.8695400253489854,
            "auditor_fn_violation": 0.0010736476608187138,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8697549121515221,
            "fpr": 0.0,
            "logloss": 3.7496306410858167,
            "mae": 0.5142128235637113,
            "precision": 1.0,
            "recall": 0.004166666666666667
        },
        "train": {
            "accuracy": 0.4840834248079034,
            "auc_prc": 0.8443507384383104,
            "auditor_fn_violation": 0.0013709606450925747,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8447306530748544,
            "fpr": 0.0,
            "logloss": 3.6564790503192355,
            "mae": 0.5059463529483771,
            "precision": 1.0,
            "recall": 0.008438818565400843
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7883771929824561,
            "auc_prc": 0.8709415673282279,
            "auditor_fn_violation": 0.010014619883040941,
            "auditor_fp_violation": 0.01268071799870046,
            "ave_precision_score": 0.8711745907948436,
            "fpr": 0.11074561403508772,
            "logloss": 0.7314276172523362,
            "mae": 0.24832415037686054,
            "precision": 0.7934560327198364,
            "recall": 0.8083333333333333
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8543636078234529,
            "auditor_fn_violation": 0.009596724515647945,
            "auditor_fp_violation": 0.01749278460313434,
            "ave_precision_score": 0.8545834914632117,
            "fpr": 0.1394072447859495,
            "logloss": 0.789626123288922,
            "mae": 0.26872317011573826,
            "precision": 0.7465069860279441,
            "recall": 0.7890295358649789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7724634756686296,
            "auditor_fn_violation": 0.005811403508771931,
            "auditor_fp_violation": 0.025221836419753087,
            "ave_precision_score": 0.7637551030588388,
            "fpr": 0.19846491228070176,
            "logloss": 1.5997675250164618,
            "mae": 0.29915069262861355,
            "precision": 0.697324414715719,
            "recall": 0.86875
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7315936336658837,
            "auditor_fn_violation": 0.00757270491461602,
            "auditor_fp_violation": 0.028565184736766756,
            "ave_precision_score": 0.7237875675399222,
            "fpr": 0.21844127332601537,
            "logloss": 1.7866278321357092,
            "mae": 0.3211866761854819,
            "precision": 0.6721581548599671,
            "recall": 0.8607594936708861
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 6933,
        "test": {
            "accuracy": 0.7752192982456141,
            "auc_prc": 0.85936954286381,
            "auditor_fn_violation": 0.009539473684210526,
            "auditor_fp_violation": 0.019617547920727754,
            "ave_precision_score": 0.8595918697762098,
            "fpr": 0.14583333333333334,
            "logloss": 0.7555747140251551,
            "mae": 0.2612545960601552,
            "precision": 0.7541589648798521,
            "recall": 0.85
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8325341600511706,
            "auditor_fn_violation": 0.009684725367866725,
            "auditor_fp_violation": 0.015515929134629644,
            "ave_precision_score": 0.8328506516049772,
            "fpr": 0.17453347969264543,
            "logloss": 0.855141087659763,
            "mae": 0.28677599279298116,
            "precision": 0.7087912087912088,
            "recall": 0.8164556962025317
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 6933,
        "test": {
            "accuracy": 0.4967105263157895,
            "auc_prc": 0.8711294712120246,
            "auditor_fn_violation": 0.008107182017543872,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.8712512702615198,
            "fpr": 0.0,
            "logloss": 2.9248050014582434,
            "mae": 0.48837852347151584,
            "precision": 1.0,
            "recall": 0.04375
        },
        "train": {
            "accuracy": 0.5005488474204172,
            "auc_prc": 0.8452032940519116,
            "auditor_fn_violation": 0.008774611290972506,
            "auditor_fp_violation": 0.0009143270527772684,
            "ave_precision_score": 0.8455233941425049,
            "fpr": 0.0021953896816684962,
            "logloss": 2.8655820078114087,
            "mae": 0.4786249000506114,
            "precision": 0.9130434782608695,
            "recall": 0.04430379746835443
        }
    }
]
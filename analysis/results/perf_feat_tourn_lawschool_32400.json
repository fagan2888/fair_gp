[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.6626690644777883,
            "auditor_fn_violation": 0.022600405973611723,
            "auditor_fp_violation": 0.02951303492375799,
            "ave_precision_score": 0.6561500472333001,
            "fpr": 0.21052631578947367,
            "logloss": 2.47330852317107,
            "mae": 0.3084751449989125,
            "precision": 0.6867862969004894,
            "recall": 0.8698347107438017
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.6319638326041805,
            "auditor_fn_violation": 0.02677674755354182,
            "auditor_fp_violation": 0.03673917426465648,
            "ave_precision_score": 0.6280837892721194,
            "fpr": 0.21734357848518113,
            "logloss": 2.9742076058464004,
            "mae": 0.32422000791713135,
            "precision": 0.6615384615384615,
            "recall": 0.823404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8462365237770613,
            "auditor_fn_violation": 0.029664165579237355,
            "auditor_fp_violation": 0.013721511723233321,
            "ave_precision_score": 0.8466072570716608,
            "fpr": 0.07675438596491228,
            "logloss": 1.0322111364146518,
            "mae": 0.26963940905934863,
            "precision": 0.8254364089775561,
            "recall": 0.6838842975206612
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8385791805818283,
            "auditor_fn_violation": 0.03184249246794499,
            "auditor_fp_violation": 0.01665210540857397,
            "ave_precision_score": 0.8387450809806305,
            "fpr": 0.0867178924259056,
            "logloss": 1.2138199779728895,
            "mae": 0.28079344199043793,
            "precision": 0.7948051948051948,
            "recall": 0.6510638297872341
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.6673796862909482,
            "auditor_fn_violation": 0.025742623604465713,
            "auditor_fp_violation": 0.04051125184456469,
            "ave_precision_score": 0.6608770330497467,
            "fpr": 0.21600877192982457,
            "logloss": 2.380653763595173,
            "mae": 0.3103665187137306,
            "precision": 0.6781045751633987,
            "recall": 0.8574380165289256
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.63214852652802,
            "auditor_fn_violation": 0.02989466800569867,
            "auditor_fp_violation": 0.04061470911086718,
            "ave_precision_score": 0.6292154397705269,
            "fpr": 0.21514818880351264,
            "logloss": 2.7947470089320263,
            "mae": 0.3243657456176165,
            "precision": 0.6614853195164075,
            "recall": 0.8148936170212766
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.6603156175437769,
            "auditor_fn_violation": 0.02040742351747137,
            "auditor_fp_violation": 0.03502111001803575,
            "ave_precision_score": 0.653820690413681,
            "fpr": 0.23026315789473684,
            "logloss": 2.4882740386868134,
            "mae": 0.3107241693748295,
            "precision": 0.6671949286846276,
            "recall": 0.8698347107438017
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.628006244363229,
            "auditor_fn_violation": 0.02692388537263237,
            "auditor_fp_violation": 0.034103213184285795,
            "ave_precision_score": 0.6250313388950304,
            "fpr": 0.2283205268935236,
            "logloss": 3.0113950062239994,
            "mae": 0.3267689178055645,
            "precision": 0.6510067114093959,
            "recall": 0.825531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.666872392555143,
            "auditor_fn_violation": 0.02016275192112513,
            "auditor_fp_violation": 0.03480078701426464,
            "ave_precision_score": 0.660406125311419,
            "fpr": 0.2236842105263158,
            "logloss": 2.3676066380415155,
            "mae": 0.30970211063478864,
            "precision": 0.6761904761904762,
            "recall": 0.8801652892561983
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.6282253868915759,
            "auditor_fn_violation": 0.027535791858374015,
            "auditor_fp_violation": 0.041316636424053706,
            "ave_precision_score": 0.6253042778181304,
            "fpr": 0.2239297475301866,
            "logloss": 2.7372876853367756,
            "mae": 0.33061160122648936,
            "precision": 0.6542372881355932,
            "recall": 0.8212765957446808
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.6726757212169668,
            "auditor_fn_violation": 0.011218645788023777,
            "auditor_fp_violation": 0.03023548942449582,
            "ave_precision_score": 0.666991137348365,
            "fpr": 0.20394736842105263,
            "logloss": 2.1935377970648116,
            "mae": 0.3035998404902195,
            "precision": 0.6879194630872483,
            "recall": 0.8471074380165289
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.6376474898448554,
            "auditor_fn_violation": 0.01987995422378962,
            "auditor_fp_violation": 0.024930865137859026,
            "ave_precision_score": 0.6347144330764499,
            "fpr": 0.21185510428100987,
            "logloss": 2.7014874652308802,
            "mae": 0.3225386768708525,
            "precision": 0.6637630662020906,
            "recall": 0.8106382978723404
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.6830683472416725,
            "auditor_fn_violation": 0.008676779759315646,
            "auditor_fp_violation": 0.02131753156255124,
            "ave_precision_score": 0.6804172754792353,
            "fpr": 0.16337719298245615,
            "logloss": 1.995955636632273,
            "mae": 0.30478714179747274,
            "precision": 0.71939736346516,
            "recall": 0.7892561983471075
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.6447162183315629,
            "auditor_fn_violation": 0.02157554242473784,
            "auditor_fp_violation": 0.018349674300748478,
            "ave_precision_score": 0.6418646889582729,
            "fpr": 0.16245883644346873,
            "logloss": 2.451718141057548,
            "mae": 0.3233772044323131,
            "precision": 0.7045908183632734,
            "recall": 0.7510638297872341
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.836792287527601,
            "auditor_fn_violation": 0.011159743366681163,
            "auditor_fp_violation": 0.005707902934907361,
            "ave_precision_score": 0.8371214307427426,
            "fpr": 0.09649122807017543,
            "logloss": 0.7931211871507954,
            "mae": 0.27104147193250644,
            "precision": 0.8009049773755657,
            "recall": 0.731404958677686
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8390219933756112,
            "auditor_fn_violation": 0.009066492281103308,
            "auditor_fp_violation": 0.01914867666788633,
            "ave_precision_score": 0.8391013131087196,
            "fpr": 0.10757409440175632,
            "logloss": 1.029475703092764,
            "mae": 0.2793749269122474,
            "precision": 0.7715617715617715,
            "recall": 0.7042553191489361
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.6682328770088237,
            "auditor_fn_violation": 0.020325866318689288,
            "auditor_fp_violation": 0.03666584686014102,
            "ave_precision_score": 0.6617840428302906,
            "fpr": 0.2149122807017544,
            "logloss": 2.3088216973952016,
            "mae": 0.30996470387636826,
            "precision": 0.6838709677419355,
            "recall": 0.8760330578512396
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.6317158416469713,
            "auditor_fn_violation": 0.027489081439615107,
            "auditor_fp_violation": 0.04418657327548656,
            "ave_precision_score": 0.6288061269423081,
            "fpr": 0.22283205268935236,
            "logloss": 2.6317618246812224,
            "mae": 0.3296152120184679,
            "precision": 0.6547619047619048,
            "recall": 0.8191489361702128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.6699911790121418,
            "auditor_fn_violation": 0.018069450485718428,
            "auditor_fp_violation": 0.034267912772585674,
            "ave_precision_score": 0.6635734110193141,
            "fpr": 0.2138157894736842,
            "logloss": 2.3149317447042397,
            "mae": 0.30750993787167996,
            "precision": 0.6839546191247974,
            "recall": 0.871900826446281
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6321339321746365,
            "auditor_fn_violation": 0.02852605273606278,
            "auditor_fp_violation": 0.04121956137010238,
            "ave_precision_score": 0.6292385732416923,
            "fpr": 0.21953896816684962,
            "logloss": 2.630722675468964,
            "mae": 0.3294838348607914,
            "precision": 0.6539792387543253,
            "recall": 0.8042553191489362
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.6633291496456857,
            "auditor_fn_violation": 0.019274684645498044,
            "auditor_fp_violation": 0.027870859977045418,
            "ave_precision_score": 0.65682589902446,
            "fpr": 0.20942982456140352,
            "logloss": 2.4434593943299134,
            "mae": 0.3077872118008568,
            "precision": 0.6873977086743044,
            "recall": 0.8677685950413223
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6314574729174529,
            "auditor_fn_violation": 0.02535908634420908,
            "auditor_fp_violation": 0.03520090802512005,
            "ave_precision_score": 0.6284733396239637,
            "fpr": 0.21624588364434688,
            "logloss": 2.9673127628929556,
            "mae": 0.32432528505878216,
            "precision": 0.6597582037996546,
            "recall": 0.8127659574468085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.654753767884434,
            "auditor_fn_violation": 0.014852472089314197,
            "auditor_fp_violation": 0.028329439252336452,
            "ave_precision_score": 0.64570776440837,
            "fpr": 0.2225877192982456,
            "logloss": 2.5423858088729414,
            "mae": 0.3115852605266958,
            "precision": 0.6762360446570973,
            "recall": 0.8760330578512396
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.6174751828789231,
            "auditor_fn_violation": 0.02324777541630661,
            "auditor_fp_violation": 0.032550012321064546,
            "ave_precision_score": 0.6102341813512706,
            "fpr": 0.2239297475301866,
            "logloss": 2.8111039818115495,
            "mae": 0.32813961832703337,
            "precision": 0.6554054054054054,
            "recall": 0.825531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.6616968320610098,
            "auditor_fn_violation": 0.023946099753516022,
            "auditor_fp_violation": 0.029390063944909017,
            "ave_precision_score": 0.6550588916911808,
            "fpr": 0.21271929824561403,
            "logloss": 2.517992939010529,
            "mae": 0.3087252696516579,
            "precision": 0.685064935064935,
            "recall": 0.871900826446281
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.6294058566986913,
            "auditor_fn_violation": 0.02541046780484387,
            "auditor_fp_violation": 0.03902914989632882,
            "ave_precision_score": 0.6255250194073534,
            "fpr": 0.21514818880351264,
            "logloss": 2.9999819826075984,
            "mae": 0.32413045825382764,
            "precision": 0.6655290102389079,
            "recall": 0.8297872340425532
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.6816128124420103,
            "auditor_fn_violation": 0.021832409018413802,
            "auditor_fp_violation": 0.036204705689457294,
            "ave_precision_score": 0.6759615598328801,
            "fpr": 0.22039473684210525,
            "logloss": 2.2595483167443886,
            "mae": 0.3058265063643485,
            "precision": 0.6789137380191693,
            "recall": 0.878099173553719
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.6403418926982923,
            "auditor_fn_violation": 0.03115584931218909,
            "auditor_fp_violation": 0.03884495620421605,
            "ave_precision_score": 0.6374411071158438,
            "fpr": 0.21624588364434688,
            "logloss": 2.6375251373838733,
            "mae": 0.3225291046814583,
            "precision": 0.6609294320137694,
            "recall": 0.8170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.6882241296411484,
            "auditor_fn_violation": 0.01912063215890967,
            "auditor_fp_violation": 0.029092884079357288,
            "ave_precision_score": 0.6805703277213377,
            "fpr": 0.20394736842105263,
            "logloss": 2.2897540151975795,
            "mae": 0.29950752452592855,
            "precision": 0.6925619834710743,
            "recall": 0.8657024793388429
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.6431461918750251,
            "auditor_fn_violation": 0.02455566714155593,
            "auditor_fp_violation": 0.035850564155409693,
            "ave_precision_score": 0.6385781946199199,
            "fpr": 0.20965971459934138,
            "logloss": 2.6857449572789336,
            "mae": 0.3180861658536442,
            "precision": 0.6678260869565218,
            "recall": 0.8170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.6810320710602142,
            "auditor_fn_violation": 0.020731386834855738,
            "auditor_fp_violation": 0.0311705812428267,
            "ave_precision_score": 0.6775986719159155,
            "fpr": 0.22039473684210525,
            "logloss": 2.055387003812341,
            "mae": 0.3083599222839815,
            "precision": 0.6737012987012987,
            "recall": 0.8574380165289256
        },
        "train": {
            "accuracy": 0.6717892425905598,
            "auc_prc": 0.6405723496706067,
            "auditor_fn_violation": 0.02775766634747881,
            "auditor_fp_violation": 0.03304285490266359,
            "ave_precision_score": 0.6378419695919462,
            "fpr": 0.2239297475301866,
            "logloss": 2.3394988898371376,
            "mae": 0.3339406076664917,
            "precision": 0.6476683937823834,
            "recall": 0.7978723404255319
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.6392041835342658,
            "auditor_fn_violation": 0.015976149050311732,
            "auditor_fp_violation": 0.02786573618626005,
            "ave_precision_score": 0.6345584557882296,
            "fpr": 0.2225877192982456,
            "logloss": 3.4019252295106983,
            "mae": 0.3200977676113196,
            "precision": 0.6666666666666666,
            "recall": 0.8388429752066116
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.6085436536586741,
            "auditor_fn_violation": 0.02327580166756195,
            "auditor_fp_violation": 0.03218660314473395,
            "ave_precision_score": 0.6036545882100963,
            "fpr": 0.21844127332601537,
            "logloss": 3.501482877751672,
            "mae": 0.33011504027227656,
            "precision": 0.6551126516464472,
            "recall": 0.8042553191489362
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.6747186475605175,
            "auditor_fn_violation": 0.01922937509061911,
            "auditor_fp_violation": 0.03337637317593048,
            "ave_precision_score": 0.6682590424697858,
            "fpr": 0.2149122807017544,
            "logloss": 2.369352860861698,
            "mae": 0.30785864993675727,
            "precision": 0.6838709677419355,
            "recall": 0.8760330578512396
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.6383232213236185,
            "auditor_fn_violation": 0.026970595791391272,
            "auditor_fp_violation": 0.03909137749501557,
            "ave_precision_score": 0.6353625237076735,
            "fpr": 0.21405049396267836,
            "logloss": 2.8361687218605436,
            "mae": 0.32142065635018546,
            "precision": 0.6649484536082474,
            "recall": 0.823404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.6767722547260935,
            "auditor_fn_violation": 0.022154106858054228,
            "auditor_fp_violation": 0.03133710444335138,
            "ave_precision_score": 0.6710288919070857,
            "fpr": 0.20833333333333334,
            "logloss": 2.3245885646602997,
            "mae": 0.3071618538737646,
            "precision": 0.6900489396411092,
            "recall": 0.8739669421487604
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.6393551899930678,
            "auditor_fn_violation": 0.029401873087792237,
            "auditor_fp_violation": 0.03590781354620152,
            "ave_precision_score": 0.6364076250373667,
            "fpr": 0.21844127332601537,
            "logloss": 2.8139882361114568,
            "mae": 0.3237628977452305,
            "precision": 0.656896551724138,
            "recall": 0.8106382978723404
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.6529261183936212,
            "auditor_fn_violation": 0.007548571842830219,
            "auditor_fp_violation": 0.009696774061321528,
            "ave_precision_score": 0.6462233244471594,
            "fpr": 0.17434210526315788,
            "logloss": 2.5061452884246798,
            "mae": 0.3132960910205684,
            "precision": 0.7103825136612022,
            "recall": 0.8057851239669421
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.6293712816035147,
            "auditor_fn_violation": 0.009996029614405495,
            "auditor_fp_violation": 0.010521442385955482,
            "ave_precision_score": 0.6245899785765745,
            "fpr": 0.18111964873765093,
            "logloss": 2.9415511444159343,
            "mae": 0.3277324658359111,
            "precision": 0.6857142857142857,
            "recall": 0.7659574468085106
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.6553108357378831,
            "auditor_fn_violation": 0.01726294040887343,
            "auditor_fp_violation": 0.033852885718970334,
            "ave_precision_score": 0.6479157451161798,
            "fpr": 0.22149122807017543,
            "logloss": 2.5895735979375316,
            "mae": 0.3089375108271163,
            "precision": 0.6736672051696284,
            "recall": 0.8615702479338843
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.6245630454721343,
            "auditor_fn_violation": 0.02445290422028634,
            "auditor_fp_violation": 0.03148218672759995,
            "ave_precision_score": 0.6189665281763855,
            "fpr": 0.2261251372118551,
            "logloss": 3.085668125818577,
            "mae": 0.3253102966758038,
            "precision": 0.6484641638225256,
            "recall": 0.8085106382978723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8340018777528346,
            "auditor_fn_violation": 0.023957427142235758,
            "auditor_fp_violation": 0.02765822265945237,
            "ave_precision_score": 0.8344938976032813,
            "fpr": 0.14144736842105263,
            "logloss": 1.011688034535078,
            "mae": 0.2621182520411702,
            "precision": 0.7528735632183908,
            "recall": 0.8119834710743802
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.837686420429463,
            "auditor_fn_violation": 0.026676320153210174,
            "auditor_fp_violation": 0.028204036828782007,
            "ave_precision_score": 0.8378546325806793,
            "fpr": 0.1350164654226125,
            "logloss": 1.1779651677428442,
            "mae": 0.2726728248389997,
            "precision": 0.7463917525773196,
            "recall": 0.7702127659574468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.6629344360308346,
            "auditor_fn_violation": 0.019555603885747425,
            "auditor_fp_violation": 0.030968191506804393,
            "ave_precision_score": 0.656250263061709,
            "fpr": 0.21271929824561403,
            "logloss": 2.5202613882779032,
            "mae": 0.3090726376300838,
            "precision": 0.6855753646677472,
            "recall": 0.8739669421487604
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.6307837640133354,
            "auditor_fn_violation": 0.021626923885372632,
            "auditor_fp_violation": 0.0355518716817133,
            "ave_precision_score": 0.6268733610375421,
            "fpr": 0.21624588364434688,
            "logloss": 2.9807683967777785,
            "mae": 0.3212084880784959,
            "precision": 0.6632478632478632,
            "recall": 0.825531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.6688663436676595,
            "auditor_fn_violation": 0.022770316804407713,
            "auditor_fp_violation": 0.031275618953926866,
            "ave_precision_score": 0.6624059009790871,
            "fpr": 0.21271929824561403,
            "logloss": 2.3780617935741692,
            "mae": 0.30690265655081805,
            "precision": 0.6835236541598695,
            "recall": 0.8657024793388429
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.6349144853827915,
            "auditor_fn_violation": 0.025069481747903872,
            "auditor_fp_violation": 0.03498435598169017,
            "ave_precision_score": 0.6310288193317737,
            "fpr": 0.21844127332601537,
            "logloss": 2.93310576988855,
            "mae": 0.32239901504094504,
            "precision": 0.6615646258503401,
            "recall": 0.8276595744680851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 32400,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.6559150887065407,
            "auditor_fn_violation": 0.01634768740031898,
            "auditor_fp_violation": 0.032694909001475654,
            "ave_precision_score": 0.6485205055871646,
            "fpr": 0.22149122807017543,
            "logloss": 2.570649383982632,
            "mae": 0.30876900723287487,
            "precision": 0.6741935483870968,
            "recall": 0.8636363636363636
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.6245684702910488,
            "auditor_fn_violation": 0.023364551463203865,
            "auditor_fp_violation": 0.03219158135262887,
            "ave_precision_score": 0.6197747843353372,
            "fpr": 0.22722283205268934,
            "logloss": 3.070111576550714,
            "mae": 0.3250644163904305,
            "precision": 0.649746192893401,
            "recall": 0.8170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 32400,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.6473040501000569,
            "auditor_fn_violation": 0.023080687255328407,
            "auditor_fp_violation": 0.034380636169863914,
            "ave_precision_score": 0.63654622978777,
            "fpr": 0.22807017543859648,
            "logloss": 3.0994769535095146,
            "mae": 0.3155094526545985,
            "precision": 0.6708860759493671,
            "recall": 0.8760330578512396
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.615955066586781,
            "auditor_fn_violation": 0.03187518976107621,
            "auditor_fp_violation": 0.03956181814108739,
            "ave_precision_score": 0.6067933638795521,
            "fpr": 0.22941822173435786,
            "logloss": 3.539662576912546,
            "mae": 0.3307785444731189,
            "precision": 0.6463620981387479,
            "recall": 0.8127659574468085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.6530875770998381,
            "auditor_fn_violation": 0.00975967812092214,
            "auditor_fp_violation": 0.019941793736678145,
            "ave_precision_score": 0.6456525021173416,
            "fpr": 0.19956140350877194,
            "logloss": 2.462382591476517,
            "mae": 0.30503737739622555,
            "precision": 0.6936026936026936,
            "recall": 0.8512396694214877
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.6244455760827947,
            "auditor_fn_violation": 0.017343578485181123,
            "auditor_fp_violation": 0.019594226274483453,
            "ave_precision_score": 0.6179936185636592,
            "fpr": 0.20197585071350166,
            "logloss": 2.9846579167885565,
            "mae": 0.32193241403138273,
            "precision": 0.6714285714285714,
            "recall": 0.8
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.6622932661545703,
            "auditor_fn_violation": 0.021832409018413802,
            "auditor_fp_violation": 0.03175725528775209,
            "ave_precision_score": 0.6557590995655985,
            "fpr": 0.21600877192982457,
            "logloss": 2.4707426169579505,
            "mae": 0.30912194989341196,
            "precision": 0.6832797427652733,
            "recall": 0.878099173553719
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.6313342904029176,
            "auditor_fn_violation": 0.025597309479879492,
            "auditor_fp_violation": 0.03673917426465648,
            "ave_precision_score": 0.6274386193773387,
            "fpr": 0.21734357848518113,
            "logloss": 2.9892174356023786,
            "mae": 0.32435286250191514,
            "precision": 0.660377358490566,
            "recall": 0.8191489361702128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.649672891274166,
            "auditor_fn_violation": 0.014159235899666524,
            "auditor_fp_violation": 0.029615510739465493,
            "ave_precision_score": 0.6421069885670483,
            "fpr": 0.22587719298245615,
            "logloss": 2.564487993097535,
            "mae": 0.31161941719499825,
            "precision": 0.671451355661882,
            "recall": 0.8698347107438017
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.6169745915687072,
            "auditor_fn_violation": 0.02160823971786907,
            "auditor_fp_violation": 0.02884622564722927,
            "ave_precision_score": 0.610515462560444,
            "fpr": 0.22502744237102085,
            "logloss": 3.009918131226727,
            "mae": 0.32444299416970646,
            "precision": 0.6537162162162162,
            "recall": 0.823404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 32400,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8352713939592387,
            "auditor_fn_violation": 0.01871284616499928,
            "auditor_fp_violation": 0.03185973110345959,
            "ave_precision_score": 0.8356389595599845,
            "fpr": 0.16228070175438597,
            "logloss": 0.979060732058755,
            "mae": 0.26720235463166203,
            "precision": 0.7318840579710145,
            "recall": 0.8347107438016529
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8360653248643848,
            "auditor_fn_violation": 0.027820725412803327,
            "auditor_fp_violation": 0.03179830292892863,
            "ave_precision_score": 0.8362085615047672,
            "fpr": 0.15367727771679474,
            "logloss": 0.9492032631821629,
            "mae": 0.2797393922403555,
            "precision": 0.724950884086444,
            "recall": 0.7851063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.812641238943763,
            "auditor_fn_violation": 0.009700775699579528,
            "auditor_fp_violation": 0.00825442695523857,
            "ave_precision_score": 0.8131942497926297,
            "fpr": 0.14583333333333334,
            "logloss": 0.880513857371426,
            "mae": 0.27466702556163897,
            "precision": 0.7417475728155339,
            "recall": 0.7892561983471075
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8197001205253863,
            "auditor_fn_violation": 0.01027629212695892,
            "auditor_fp_violation": 0.02330299115621368,
            "ave_precision_score": 0.8199194224358705,
            "fpr": 0.14270032930845225,
            "logloss": 1.1120540740256408,
            "mae": 0.2829307964473609,
            "precision": 0.7297297297297297,
            "recall": 0.7468085106382979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.786435835903978,
            "auditor_fn_violation": 0.0261209583877048,
            "auditor_fp_violation": 0.023292752910313165,
            "ave_precision_score": 0.7868291410965464,
            "fpr": 0.12390350877192982,
            "logloss": 2.183092412750551,
            "mae": 0.2730320819947342,
            "precision": 0.7640918580375783,
            "recall": 0.756198347107438
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7751039179280264,
            "auditor_fn_violation": 0.03589462129527991,
            "auditor_fp_violation": 0.028403165144579613,
            "ave_precision_score": 0.7755873186197365,
            "fpr": 0.12733260153677278,
            "logloss": 2.151032245447207,
            "mae": 0.2935711927253514,
            "precision": 0.7393258426966293,
            "recall": 0.7
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.6632468037727467,
            "auditor_fn_violation": 0.019274684645498044,
            "auditor_fp_violation": 0.027870859977045418,
            "ave_precision_score": 0.6567434842843994,
            "fpr": 0.20942982456140352,
            "logloss": 2.444005950555318,
            "mae": 0.30783845939206606,
            "precision": 0.6873977086743044,
            "recall": 0.8677685950413223
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6314228621073286,
            "auditor_fn_violation": 0.02535908634420908,
            "auditor_fp_violation": 0.03520090802512005,
            "ave_precision_score": 0.6284367218543986,
            "fpr": 0.21624588364434688,
            "logloss": 2.9682494178391887,
            "mae": 0.32433267596890203,
            "precision": 0.6597582037996546,
            "recall": 0.8127659574468085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.6597959226390775,
            "auditor_fn_violation": 0.008438904596201249,
            "auditor_fp_violation": 0.027068986719134287,
            "ave_precision_score": 0.6533164019382538,
            "fpr": 0.26973684210526316,
            "logloss": 2.3518865340849193,
            "mae": 0.3204479210068061,
            "precision": 0.6419213973799127,
            "recall": 0.9111570247933884
        },
        "train": {
            "accuracy": 0.6663007683863886,
            "auc_prc": 0.6278379250652136,
            "auditor_fn_violation": 0.01040708129948385,
            "auditor_fp_violation": 0.02658860836687402,
            "ave_precision_score": 0.624983089297016,
            "fpr": 0.2722283205268935,
            "logloss": 2.7516897496553105,
            "mae": 0.3384040996771571,
            "precision": 0.6253776435045317,
            "recall": 0.8808510638297873
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.5585791809346375,
            "auditor_fn_violation": 0.01711794983326084,
            "auditor_fp_violation": 0.032687223315297595,
            "ave_precision_score": 0.5370233178039877,
            "fpr": 0.31030701754385964,
            "logloss": 4.966846584731748,
            "mae": 0.37695758811657354,
            "precision": 0.6107290233837689,
            "recall": 0.9173553719008265
        },
        "train": {
            "accuracy": 0.6114160263446762,
            "auc_prc": 0.5329076300619283,
            "auditor_fn_violation": 0.024887311114744147,
            "auditor_fp_violation": 0.03260726171185636,
            "ave_precision_score": 0.512625635583636,
            "fpr": 0.3238199780461032,
            "logloss": 5.112038286983958,
            "mae": 0.3983180627598944,
            "precision": 0.5821529745042493,
            "recall": 0.874468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8110626523044303,
            "auditor_fn_violation": 0.02160812672176309,
            "auditor_fp_violation": 0.029195359895064766,
            "ave_precision_score": 0.8124393400938423,
            "fpr": 0.15899122807017543,
            "logloss": 1.1326835380808609,
            "mae": 0.2678227872634604,
            "precision": 0.7284644194756554,
            "recall": 0.8037190082644629
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8199540649117649,
            "auditor_fn_violation": 0.027820725412803324,
            "auditor_fp_violation": 0.032438002643428394,
            "ave_precision_score": 0.8200763989707919,
            "fpr": 0.14818880351262348,
            "logloss": 1.369371127182124,
            "mae": 0.28268174475886015,
            "precision": 0.7227926078028748,
            "recall": 0.7489361702127659
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.6651422960970799,
            "auditor_fn_violation": 0.021576410033347832,
            "auditor_fp_violation": 0.038377192982456135,
            "ave_precision_score": 0.6587291419588864,
            "fpr": 0.23464912280701755,
            "logloss": 2.449537992834309,
            "mae": 0.3103556597295134,
            "precision": 0.664576802507837,
            "recall": 0.8760330578512396
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.6312982524210429,
            "auditor_fn_violation": 0.02855174346638017,
            "auditor_fp_violation": 0.03376718415137735,
            "ave_precision_score": 0.6283427179722985,
            "fpr": 0.2305159165751921,
            "logloss": 2.9296163483635764,
            "mae": 0.32686963767178884,
            "precision": 0.6494156928213689,
            "recall": 0.8276595744680851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.81616666455192,
            "auditor_fn_violation": 0.015803972741771786,
            "auditor_fp_violation": 0.025373011969175283,
            "ave_precision_score": 0.8083033676491036,
            "fpr": 0.13157894736842105,
            "logloss": 3.430703948361955,
            "mae": 0.27937388997297996,
            "precision": 0.7540983606557377,
            "recall": 0.7603305785123967
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8187291004476986,
            "auditor_fn_violation": 0.027489081439615107,
            "auditor_fp_violation": 0.027302981199797887,
            "ave_precision_score": 0.8097390571572499,
            "fpr": 0.13062568605927552,
            "logloss": 3.4798080380006864,
            "mae": 0.28511171543267644,
            "precision": 0.7373068432671082,
            "recall": 0.7106382978723405
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.6643041825247791,
            "auditor_fn_violation": 0.006470204436711618,
            "auditor_fp_violation": 0.011559272011805217,
            "ave_precision_score": 0.6600702062493305,
            "fpr": 0.17982456140350878,
            "logloss": 2.2906486737262814,
            "mae": 0.323128169893399,
            "precision": 0.6940298507462687,
            "recall": 0.768595041322314
        },
        "train": {
            "accuracy": 0.6695938529088913,
            "auc_prc": 0.627246019749193,
            "auditor_fn_violation": 0.020949622813368526,
            "auditor_fp_violation": 0.02271556262461077,
            "ave_precision_score": 0.6243964035909958,
            "fpr": 0.18880351262349068,
            "logloss": 2.730419909076472,
            "mae": 0.33746911174682137,
            "precision": 0.6647173489278753,
            "recall": 0.725531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.702418975583132,
            "auditor_fn_violation": 0.005351058431201974,
            "auditor_fp_violation": 0.016088703066076415,
            "ave_precision_score": 0.7015293962934099,
            "fpr": 0.17105263157894737,
            "logloss": 1.8122572570737872,
            "mae": 0.29132308333205237,
            "precision": 0.7179023508137432,
            "recall": 0.8202479338842975
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.6733105912710529,
            "auditor_fn_violation": 0.018179694980965503,
            "auditor_fp_violation": 0.019843136669230453,
            "ave_precision_score": 0.6714176727185746,
            "fpr": 0.17014270032930845,
            "logloss": 2.1650615597837604,
            "mae": 0.31266869608860615,
            "precision": 0.7013487475915221,
            "recall": 0.774468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.6677086798361895,
            "auditor_fn_violation": 0.018830651007684503,
            "auditor_fp_violation": 0.029103131660928018,
            "ave_precision_score": 0.6611994576922073,
            "fpr": 0.21162280701754385,
            "logloss": 2.410733229082493,
            "mae": 0.3061716257666605,
            "precision": 0.6871961102106969,
            "recall": 0.8760330578512396
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.6355268160236682,
            "auditor_fn_violation": 0.023404255319148935,
            "auditor_fp_violation": 0.03344608974215374,
            "ave_precision_score": 0.6325446824559791,
            "fpr": 0.21734357848518113,
            "logloss": 2.9100611324086834,
            "mae": 0.3215706132162871,
            "precision": 0.6626916524701874,
            "recall": 0.8276595744680851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.6554227567245231,
            "auditor_fn_violation": 0.01634768740031898,
            "auditor_fp_violation": 0.0329280414822102,
            "ave_precision_score": 0.6480267113904921,
            "fpr": 0.22478070175438597,
            "logloss": 2.5918598409940405,
            "mae": 0.3094916204604893,
            "precision": 0.6709470304975923,
            "recall": 0.8636363636363636
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.624237492466605,
            "auditor_fn_violation": 0.02705934558703319,
            "auditor_fp_violation": 0.03219158135262887,
            "ave_precision_score": 0.618601167602451,
            "fpr": 0.22722283205268934,
            "logloss": 3.08628411498903,
            "mae": 0.32517407903933127,
            "precision": 0.649746192893401,
            "recall": 0.8170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.6526679251842806,
            "auditor_fn_violation": 0.015151515151515155,
            "auditor_fp_violation": 0.023764141662567646,
            "ave_precision_score": 0.6452329887102081,
            "fpr": 0.20614035087719298,
            "logloss": 2.489179825861722,
            "mae": 0.3055319384207008,
            "precision": 0.6887417218543046,
            "recall": 0.859504132231405
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.6237049604904206,
            "auditor_fn_violation": 0.02075343905458113,
            "auditor_fp_violation": 0.026222710086595942,
            "ave_precision_score": 0.617279768399748,
            "fpr": 0.21514818880351264,
            "logloss": 3.0062552554094397,
            "mae": 0.3222129134758379,
            "precision": 0.6632302405498282,
            "recall": 0.8212765957446808
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.6946981084961312,
            "auditor_fn_violation": 0.003543207191532552,
            "auditor_fp_violation": 0.003335587801278905,
            "ave_precision_score": 0.6938307398922136,
            "fpr": 0.16447368421052633,
            "logloss": 1.7952057643577002,
            "mae": 0.29999952476553154,
            "precision": 0.714828897338403,
            "recall": 0.7768595041322314
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.6636854957212116,
            "auditor_fn_violation": 0.012429642431744402,
            "auditor_fp_violation": 0.007668929262154923,
            "ave_precision_score": 0.6618168584040625,
            "fpr": 0.17233809001097694,
            "logloss": 2.177244460934274,
            "mae": 0.32451128770643134,
            "precision": 0.6884920634920635,
            "recall": 0.7382978723404255
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8377678146884804,
            "auditor_fn_violation": 0.025294059011164275,
            "auditor_fp_violation": 0.0323823577635678,
            "ave_precision_score": 0.8382161844859946,
            "fpr": 0.16666666666666666,
            "logloss": 1.0398736192948277,
            "mae": 0.2642651297566872,
            "precision": 0.7280858676207513,
            "recall": 0.8409090909090909
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8343900089464453,
            "auditor_fn_violation": 0.02836490179134456,
            "auditor_fp_violation": 0.035014225229059795,
            "ave_precision_score": 0.8345978115276291,
            "fpr": 0.1525795828759605,
            "logloss": 1.082393853583281,
            "mae": 0.27470977679535247,
            "precision": 0.7263779527559056,
            "recall": 0.7851063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.6615469476923244,
            "auditor_fn_violation": 0.02391664854284472,
            "auditor_fp_violation": 0.028262829972126594,
            "ave_precision_score": 0.6549497643105174,
            "fpr": 0.20175438596491227,
            "logloss": 2.4705331645679016,
            "mae": 0.3077527184795416,
            "precision": 0.6897133220910624,
            "recall": 0.8450413223140496
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.629950188910258,
            "auditor_fn_violation": 0.029497629446247987,
            "auditor_fp_violation": 0.03713743089625166,
            "ave_precision_score": 0.6260553738827435,
            "fpr": 0.2030735455543359,
            "logloss": 2.9755356881881374,
            "mae": 0.32409873386716215,
            "precision": 0.6666666666666666,
            "recall": 0.7872340425531915
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.6465561441055269,
            "auditor_fn_violation": 0.011100840945338555,
            "auditor_fp_violation": 0.004754877848827678,
            "ave_precision_score": 0.6407486631120389,
            "fpr": 0.15789473684210525,
            "logloss": 2.3808290944762787,
            "mae": 0.32514663049448944,
            "precision": 0.7230769230769231,
            "recall": 0.7768595041322314
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.6186522464048202,
            "auditor_fn_violation": 0.012695891818670153,
            "auditor_fp_violation": 0.010481616722795965,
            "ave_precision_score": 0.6157084642086061,
            "fpr": 0.1800219538968167,
            "logloss": 2.820438321225707,
            "mae": 0.3427107692568474,
            "precision": 0.6739562624254473,
            "recall": 0.7212765957446808
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.6847311373146487,
            "auditor_fn_violation": 0.019501232419892708,
            "auditor_fp_violation": 0.031234628627643885,
            "ave_precision_score": 0.6790926951866698,
            "fpr": 0.21600877192982457,
            "logloss": 2.256285044600514,
            "mae": 0.3063622457060713,
            "precision": 0.6807131280388979,
            "recall": 0.8677685950413223
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.641212539163782,
            "auditor_fn_violation": 0.030735455543358946,
            "auditor_fp_violation": 0.03909137749501557,
            "ave_precision_score": 0.6383634767312233,
            "fpr": 0.21405049396267836,
            "logloss": 2.491779938818959,
            "mae": 0.3233383282320682,
            "precision": 0.6608695652173913,
            "recall": 0.8085106382978723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.6678253342108259,
            "auditor_fn_violation": 0.022079346092503987,
            "auditor_fp_violation": 0.04045745204131825,
            "ave_precision_score": 0.6613218984217466,
            "fpr": 0.25,
            "logloss": 2.510085170193836,
            "mae": 0.31143523680306323,
            "precision": 0.65402124430956,
            "recall": 0.890495867768595
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6321547061752516,
            "auditor_fn_violation": 0.023822313567041134,
            "auditor_fp_violation": 0.03474291289878557,
            "ave_precision_score": 0.6291989737735952,
            "fpr": 0.23819978046103182,
            "logloss": 2.968734555201487,
            "mae": 0.32682920190547954,
            "precision": 0.6454248366013072,
            "recall": 0.8404255319148937
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.6653083327679773,
            "auditor_fn_violation": 0.003733507322024071,
            "auditor_fp_violation": 0.0046395925561567505,
            "ave_precision_score": 0.6587312881030798,
            "fpr": 0.24451754385964913,
            "logloss": 2.4999249962731462,
            "mae": 0.3128055843662159,
            "precision": 0.6605783866057838,
            "recall": 0.8966942148760331
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6330513909320072,
            "auditor_fn_violation": 0.0065768269612537095,
            "auditor_fp_violation": 0.01866827960602463,
            "ave_precision_score": 0.6300968110355123,
            "fpr": 0.2502744237102086,
            "logloss": 2.7810886011270872,
            "mae": 0.32552630351251327,
            "precision": 0.6403785488958991,
            "recall": 0.8638297872340426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.6628195500305806,
            "auditor_fn_violation": 0.021068943018703785,
            "auditor_fp_violation": 0.04111842105263159,
            "ave_precision_score": 0.6561813656013649,
            "fpr": 0.23464912280701755,
            "logloss": 2.5288816518075286,
            "mae": 0.31264385413612616,
            "precision": 0.6661466458658346,
            "recall": 0.8822314049586777
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.6312735225646533,
            "auditor_fn_violation": 0.02896045963052059,
            "auditor_fp_violation": 0.04168253470433181,
            "ave_precision_score": 0.6273775362394323,
            "fpr": 0.23380900109769484,
            "logloss": 3.0086340730588392,
            "mae": 0.32705911121333636,
            "precision": 0.6502463054187192,
            "recall": 0.8425531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.6631944683521112,
            "auditor_fn_violation": 0.020217123386979844,
            "auditor_fp_violation": 0.029513034923757998,
            "ave_precision_score": 0.6567026137385792,
            "fpr": 0.20833333333333334,
            "logloss": 2.443238949529763,
            "mae": 0.30739384949205484,
            "precision": 0.6880131362889984,
            "recall": 0.8657024793388429
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6312862009561673,
            "auditor_fn_violation": 0.024595370997500993,
            "auditor_fp_violation": 0.03219904866447129,
            "ave_precision_score": 0.6283029419522254,
            "fpr": 0.21514818880351264,
            "logloss": 2.9771527679114076,
            "mae": 0.32503841300783615,
            "precision": 0.6603119584055459,
            "recall": 0.8106382978723404
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.6704918890226175,
            "auditor_fn_violation": 0.018069450485718428,
            "auditor_fp_violation": 0.034267912772585674,
            "ave_precision_score": 0.6640982089448482,
            "fpr": 0.2138157894736842,
            "logloss": 2.311627857430876,
            "mae": 0.307464382739523,
            "precision": 0.6839546191247974,
            "recall": 0.871900826446281
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6324087549057333,
            "auditor_fn_violation": 0.02852605273606278,
            "auditor_fp_violation": 0.04121956137010238,
            "ave_precision_score": 0.629513344121089,
            "fpr": 0.21953896816684962,
            "logloss": 2.6275504087334616,
            "mae": 0.32942406750949965,
            "precision": 0.6539792387543253,
            "recall": 0.8042553191489362
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.6715201110980147,
            "auditor_fn_violation": 0.017081702189357697,
            "auditor_fp_violation": 0.02383587473356288,
            "ave_precision_score": 0.6658746605683421,
            "fpr": 0.19956140350877194,
            "logloss": 2.264060750120289,
            "mae": 0.3103254310688672,
            "precision": 0.6899488926746167,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.6340594953384247,
            "auditor_fn_violation": 0.026017703248709622,
            "auditor_fp_violation": 0.03086986715652233,
            "ave_precision_score": 0.631168584886643,
            "fpr": 0.20417124039517015,
            "logloss": 2.705365900125268,
            "mae": 0.3294331778859414,
            "precision": 0.662431941923775,
            "recall": 0.776595744680851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.6665036018455023,
            "auditor_fn_violation": 0.021576410033347832,
            "auditor_fp_violation": 0.03782894736842106,
            "ave_precision_score": 0.6600528437773372,
            "fpr": 0.23464912280701755,
            "logloss": 2.441802243578665,
            "mae": 0.30993221436954305,
            "precision": 0.664576802507837,
            "recall": 0.8760330578512396
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.6319982548583774,
            "auditor_fn_violation": 0.02855174346638017,
            "auditor_fp_violation": 0.03376718415137735,
            "ave_precision_score": 0.6290425651031537,
            "fpr": 0.2305159165751921,
            "logloss": 2.9211849598917516,
            "mae": 0.3267291509741934,
            "precision": 0.6494156928213689,
            "recall": 0.8276595744680851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.6546785415724465,
            "auditor_fn_violation": 0.01306274467159635,
            "auditor_fp_violation": 0.025106574848335797,
            "ave_precision_score": 0.6472615034535006,
            "fpr": 0.2149122807017544,
            "logloss": 2.5091520955992865,
            "mae": 0.3052486040496537,
            "precision": 0.6823338735818476,
            "recall": 0.8698347107438017
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.6246530491134729,
            "auditor_fn_violation": 0.01659387626410071,
            "auditor_fp_violation": 0.027524511451122703,
            "ave_precision_score": 0.618201566791498,
            "fpr": 0.2217343578485181,
            "logloss": 3.022022224670837,
            "mae": 0.3223116254367198,
            "precision": 0.6582064297800339,
            "recall": 0.8276595744680851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.6695746629638808,
            "auditor_fn_violation": 0.018114760040597363,
            "auditor_fp_violation": 0.035159452369240875,
            "ave_precision_score": 0.6630987681886047,
            "fpr": 0.2149122807017544,
            "logloss": 2.344376364279346,
            "mae": 0.3082163821694149,
            "precision": 0.6843800322061192,
            "recall": 0.878099173553719
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6314797142643005,
            "auditor_fn_violation": 0.02984795758693977,
            "auditor_fp_violation": 0.04097314007930286,
            "ave_precision_score": 0.6285845623592043,
            "fpr": 0.2217343578485181,
            "logloss": 2.6392958950407617,
            "mae": 0.3299356315305405,
            "precision": 0.6529209621993127,
            "recall": 0.8085106382978723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.6518438990241628,
            "auditor_fn_violation": 0.017992424242424244,
            "auditor_fp_violation": 0.031091162485653398,
            "ave_precision_score": 0.6443477964984663,
            "fpr": 0.2236842105263158,
            "logloss": 2.576511915017324,
            "mae": 0.30770104546622706,
            "precision": 0.6720257234726688,
            "recall": 0.8636363636363636
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.6212882444332493,
            "auditor_fn_violation": 0.02348132751010113,
            "auditor_fp_violation": 0.029443610594622045,
            "ave_precision_score": 0.6148328352242038,
            "fpr": 0.2283205268935236,
            "logloss": 3.0523889099865684,
            "mae": 0.32473352302588365,
            "precision": 0.6504201680672269,
            "recall": 0.823404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.6434364030471834,
            "auditor_fn_violation": 0.004476584022038574,
            "auditor_fp_violation": 0.01628084522052796,
            "ave_precision_score": 0.6377777095125687,
            "fpr": 0.16337719298245615,
            "logloss": 2.1115968119537407,
            "mae": 0.36781545925087383,
            "precision": 0.7066929133858267,
            "recall": 0.7417355371900827
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.6116578727347757,
            "auditor_fn_violation": 0.0052689352360043885,
            "auditor_fp_violation": 0.006055989904194389,
            "ave_precision_score": 0.6087719952639302,
            "fpr": 0.16245883644346873,
            "logloss": 2.3905228256287767,
            "mae": 0.37527631637849873,
            "precision": 0.689727463312369,
            "recall": 0.7
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.6826014513218315,
            "auditor_fn_violation": 0.024621212121212124,
            "auditor_fp_violation": 0.03534647073290704,
            "ave_precision_score": 0.6782905049761281,
            "fpr": 0.19407894736842105,
            "logloss": 2.75791364000422,
            "mae": 0.3056314697055697,
            "precision": 0.6910994764397905,
            "recall": 0.8181818181818182
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.6349166518403269,
            "auditor_fn_violation": 0.03945161968377047,
            "auditor_fp_violation": 0.033406264078994205,
            "ave_precision_score": 0.6319519737010776,
            "fpr": 0.20197585071350166,
            "logloss": 3.2357107293752354,
            "mae": 0.33550063997848617,
            "precision": 0.6579925650557621,
            "recall": 0.7531914893617021
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8109696818325111,
            "auditor_fn_violation": 0.008665452370595914,
            "auditor_fp_violation": 0.017016109198229223,
            "ave_precision_score": 0.7775937769627088,
            "fpr": 0.12938596491228072,
            "logloss": 2.283032457918118,
            "mae": 0.265297382093512,
            "precision": 0.7625754527162978,
            "recall": 0.7830578512396694
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7805527208605677,
            "auditor_fn_violation": 0.01536305672980358,
            "auditor_fp_violation": 0.022633422194344256,
            "ave_precision_score": 0.7448062081431834,
            "fpr": 0.13062568605927552,
            "logloss": 2.724138441778538,
            "mae": 0.2827452420603471,
            "precision": 0.7440860215053764,
            "recall": 0.7361702127659574
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.6690997811115326,
            "auditor_fn_violation": 0.019165941713788607,
            "auditor_fp_violation": 0.027996392851287095,
            "ave_precision_score": 0.6619487780635727,
            "fpr": 0.21052631578947367,
            "logloss": 2.4037116136226504,
            "mae": 0.3057673361041242,
            "precision": 0.6872964169381107,
            "recall": 0.871900826446281
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.6359024601290584,
            "auditor_fn_violation": 0.021813765560408257,
            "auditor_fp_violation": 0.03187048694340525,
            "ave_precision_score": 0.6329369092848327,
            "fpr": 0.21405049396267836,
            "logloss": 2.866846856826273,
            "mae": 0.32163374844978554,
            "precision": 0.6637931034482759,
            "recall": 0.8191489361702128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7450980847925657,
            "auditor_fn_violation": 0.02098285486443381,
            "auditor_fp_violation": 0.022759878668634204,
            "ave_precision_score": 0.7459577374857118,
            "fpr": 0.13596491228070176,
            "logloss": 1.8950362849133464,
            "mae": 0.32161599323713974,
            "precision": 0.7225950782997763,
            "recall": 0.6673553719008265
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7525284063239985,
            "auditor_fn_violation": 0.022238830371114283,
            "auditor_fp_violation": 0.0304915233565069,
            "ave_precision_score": 0.7527240648401166,
            "fpr": 0.13062568605927552,
            "logloss": 1.9605010812048609,
            "mae": 0.3270921778879451,
            "precision": 0.7139423076923077,
            "recall": 0.6319148936170212
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.6633312344157124,
            "auditor_fn_violation": 0.019274684645498044,
            "auditor_fp_violation": 0.027870859977045418,
            "ave_precision_score": 0.6568342109953234,
            "fpr": 0.20942982456140352,
            "logloss": 2.443387843330749,
            "mae": 0.3077898510872436,
            "precision": 0.6873977086743044,
            "recall": 0.8677685950413223
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6314458143743646,
            "auditor_fn_violation": 0.02535908634420908,
            "auditor_fp_violation": 0.03520090802512005,
            "ave_precision_score": 0.6284616902705455,
            "fpr": 0.21624588364434688,
            "logloss": 2.9672072794702284,
            "mae": 0.32432794898132644,
            "precision": 0.6597582037996546,
            "recall": 0.8127659574468085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.6613877677486872,
            "auditor_fn_violation": 0.019412878787878795,
            "auditor_fp_violation": 0.022083538284964746,
            "ave_precision_score": 0.655525629800405,
            "fpr": 0.20833333333333334,
            "logloss": 2.3912072415622503,
            "mae": 0.3179452202595068,
            "precision": 0.6785109983079526,
            "recall": 0.8285123966942148
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.6252866884259843,
            "auditor_fn_violation": 0.023728892729523315,
            "auditor_fp_violation": 0.021543194665352424,
            "ave_precision_score": 0.6221871379113262,
            "fpr": 0.21075740944017562,
            "logloss": 2.849200403327247,
            "mae": 0.33772032874815844,
            "precision": 0.6552962298025135,
            "recall": 0.776595744680851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7014333610171024,
            "auditor_fn_violation": 0.02145860519066261,
            "auditor_fp_violation": 0.03453691178881784,
            "ave_precision_score": 0.7005362572490483,
            "fpr": 0.1962719298245614,
            "logloss": 1.981782798458259,
            "mae": 0.2955339451626812,
            "precision": 0.6981450252951096,
            "recall": 0.8553719008264463
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.6705480212026962,
            "auditor_fn_violation": 0.03210640633393278,
            "auditor_fp_violation": 0.0337796296711147,
            "ave_precision_score": 0.6686578262926244,
            "fpr": 0.1986827661909989,
            "logloss": 2.31312078197673,
            "mae": 0.31816434389968284,
            "precision": 0.6732851985559567,
            "recall": 0.7936170212765957
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.6645037630530195,
            "auditor_fn_violation": 0.020307742496737714,
            "auditor_fp_violation": 0.03266929004754879,
            "ave_precision_score": 0.6579961990434707,
            "fpr": 0.21710526315789475,
            "logloss": 2.44343850393543,
            "mae": 0.3072500581035348,
            "precision": 0.6821829855537721,
            "recall": 0.878099173553719
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.6329843328110508,
            "auditor_fn_violation": 0.025447836139850993,
            "auditor_fp_violation": 0.03457365383035761,
            "ave_precision_score": 0.6300095084979869,
            "fpr": 0.22063666300768386,
            "logloss": 2.9660040404931003,
            "mae": 0.3235086012432756,
            "precision": 0.6593220338983051,
            "recall": 0.8276595744680851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.663982798770212,
            "auditor_fn_violation": 0.013411628244164128,
            "auditor_fp_violation": 0.02119199868830956,
            "ave_precision_score": 0.655577004911952,
            "fpr": 0.1875,
            "logloss": 2.7090548963569954,
            "mae": 0.305246578926434,
            "precision": 0.7026086956521739,
            "recall": 0.8347107438016529
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.6346534930449425,
            "auditor_fn_violation": 0.02144008221033702,
            "auditor_fp_violation": 0.024450468075997333,
            "ave_precision_score": 0.6273511227797267,
            "fpr": 0.18880351262349068,
            "logloss": 3.0376825530104044,
            "mae": 0.3223466370062279,
            "precision": 0.6814814814814815,
            "recall": 0.7829787234042553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.6840097534008689,
            "auditor_fn_violation": 0.017897274177178485,
            "auditor_fp_violation": 0.03805183226758485,
            "ave_precision_score": 0.6781051418263927,
            "fpr": 0.2949561403508772,
            "logloss": 2.647070203711257,
            "mae": 0.3337945257026674,
            "precision": 0.6248256624825662,
            "recall": 0.9256198347107438
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.6400833100538124,
            "auditor_fn_violation": 0.01933344232431044,
            "auditor_fp_violation": 0.03906897555948835,
            "ave_precision_score": 0.6370551250117286,
            "fpr": 0.287596048298573,
            "logloss": 2.970325844446828,
            "mae": 0.33877753697844243,
            "precision": 0.6208393632416788,
            "recall": 0.9127659574468086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.6356182780318769,
            "auditor_fn_violation": 0.006560823546469481,
            "auditor_fp_violation": 0.007099012133136579,
            "ave_precision_score": 0.6280973634413104,
            "fpr": 0.20723684210526316,
            "logloss": 2.603186881164951,
            "mae": 0.33123795051729404,
            "precision": 0.6812816188870152,
            "recall": 0.8347107438016529
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.5763374567264011,
            "auditor_fn_violation": 0.009972674405026041,
            "auditor_fp_violation": 0.01721464290070219,
            "ave_precision_score": 0.5714791232484397,
            "fpr": 0.2327113062568606,
            "logloss": 3.184649085420825,
            "mae": 0.36022521917327155,
            "precision": 0.6418918918918919,
            "recall": 0.8085106382978723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.6725167773943759,
            "auditor_fn_violation": 0.020307742496737714,
            "auditor_fp_violation": 0.03825166010821447,
            "ave_precision_score": 0.6667767521300235,
            "fpr": 0.23574561403508773,
            "logloss": 2.38289560477836,
            "mae": 0.30943132888895303,
            "precision": 0.6640625,
            "recall": 0.878099173553719
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.6353014746324298,
            "auditor_fn_violation": 0.02855174346638017,
            "auditor_fp_violation": 0.03480762960141979,
            "ave_precision_score": 0.6323527185240336,
            "fpr": 0.2283205268935236,
            "logloss": 2.88983408243525,
            "mae": 0.32621837269718945,
            "precision": 0.6515912897822446,
            "recall": 0.8276595744680851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.6616636671230602,
            "auditor_fn_violation": 0.022765785848919824,
            "auditor_fp_violation": 0.026520741105099204,
            "ave_precision_score": 0.6551617708532635,
            "fpr": 0.19956140350877194,
            "logloss": 2.422345176993492,
            "mae": 0.30738310342851727,
            "precision": 0.6899488926746167,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6305667935714783,
            "auditor_fn_violation": 0.02577948011303922,
            "auditor_fp_violation": 0.03614676752515863,
            "ave_precision_score": 0.627583494798942,
            "fpr": 0.20197585071350166,
            "logloss": 2.9499409073628713,
            "mae": 0.3244712484429037,
            "precision": 0.6672694394213382,
            "recall": 0.7851063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.6609171049001884,
            "auditor_fn_violation": 0.022765785848919824,
            "auditor_fp_violation": 0.02801432611903592,
            "ave_precision_score": 0.6543661440811499,
            "fpr": 0.19846491228070176,
            "logloss": 2.4491174964501803,
            "mae": 0.30743549856481284,
            "precision": 0.6911262798634812,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.6299878099752176,
            "auditor_fn_violation": 0.028264474391012918,
            "auditor_fp_violation": 0.03614676752515863,
            "ave_precision_score": 0.6270029930601622,
            "fpr": 0.20197585071350166,
            "logloss": 2.954672722716892,
            "mae": 0.3244913624864896,
            "precision": 0.6666666666666666,
            "recall": 0.7829787234042553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.6623205527692299,
            "auditor_fn_violation": 0.013157894736842106,
            "auditor_fp_violation": 0.0199110509919659,
            "ave_precision_score": 0.6557686125653241,
            "fpr": 0.17982456140350878,
            "logloss": 2.377629350269191,
            "mae": 0.3080526377569024,
            "precision": 0.7071428571428572,
            "recall": 0.8181818181818182
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.6327181235873562,
            "auditor_fn_violation": 0.019578672022794685,
            "auditor_fp_violation": 0.025134971661551566,
            "ave_precision_score": 0.629742863779666,
            "fpr": 0.1877058177826564,
            "logloss": 2.8676577047860374,
            "mae": 0.3220287094030495,
            "precision": 0.6856617647058824,
            "recall": 0.7936170212765957
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.6681811963850679,
            "auditor_fn_violation": 0.018830651007684503,
            "auditor_fp_violation": 0.029103131660928018,
            "ave_precision_score": 0.6616785417654867,
            "fpr": 0.21162280701754385,
            "logloss": 2.40573739146159,
            "mae": 0.30574358905265414,
            "precision": 0.6871961102106969,
            "recall": 0.8760330578512396
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.6362253554913111,
            "auditor_fn_violation": 0.022164093701100035,
            "auditor_fp_violation": 0.03043676306966256,
            "ave_precision_score": 0.6332593801504092,
            "fpr": 0.21624588364434688,
            "logloss": 2.869864922790276,
            "mae": 0.32163584901481057,
            "precision": 0.6620926243567753,
            "recall": 0.8212765957446808
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.7731482216235087,
            "auditor_fn_violation": 0.008669983326083807,
            "auditor_fp_violation": 0.014797507788162006,
            "ave_precision_score": 0.774560605240361,
            "fpr": 0.15789473684210525,
            "logloss": 1.0037815678128295,
            "mae": 0.28274812481442774,
            "precision": 0.7288135593220338,
            "recall": 0.7995867768595041
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.7838776114111566,
            "auditor_fn_violation": 0.007305509493892612,
            "auditor_fp_violation": 0.023006787786464766,
            "ave_precision_score": 0.785163328067457,
            "fpr": 0.15806805708013172,
            "logloss": 1.0155584385653182,
            "mae": 0.2925599978579054,
            "precision": 0.7159763313609467,
            "recall": 0.7723404255319148
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.6223036380372691,
            "auditor_fn_violation": 0.006091869653472527,
            "auditor_fp_violation": 0.019203967863584193,
            "ave_precision_score": 0.6029804543091525,
            "fpr": 0.20175438596491227,
            "logloss": 3.233940506301266,
            "mae": 0.33849441412785874,
            "precision": 0.6696588868940754,
            "recall": 0.7706611570247934
        },
        "train": {
            "accuracy": 0.6498353457738749,
            "auc_prc": 0.6091723024289137,
            "auditor_fn_violation": 0.010388397131980294,
            "auditor_fp_violation": 0.02577218227210387,
            "ave_precision_score": 0.5855689907588759,
            "fpr": 0.21185510428100987,
            "logloss": 3.6990210075915266,
            "mae": 0.3575037279371219,
            "precision": 0.6405959031657356,
            "recall": 0.7319148936170212
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.827578032307643,
            "auditor_fn_violation": 0.02209293895896767,
            "auditor_fp_violation": 0.03219790129529432,
            "ave_precision_score": 0.8280590589397125,
            "fpr": 0.16228070175438597,
            "logloss": 1.1992621404915844,
            "mae": 0.2625831473721795,
            "precision": 0.7338129496402878,
            "recall": 0.8429752066115702
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8336897272128537,
            "auditor_fn_violation": 0.024663101104701404,
            "auditor_fp_violation": 0.03378211877506217,
            "ave_precision_score": 0.8335964872931705,
            "fpr": 0.16355653128430298,
            "logloss": 1.3427801999976936,
            "mae": 0.2729310331609654,
            "precision": 0.7156488549618321,
            "recall": 0.7978723404255319
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.6630447741059791,
            "auditor_fn_violation": 0.021207137161084532,
            "auditor_fp_violation": 0.04135411542875883,
            "ave_precision_score": 0.6565766497321613,
            "fpr": 0.24232456140350878,
            "logloss": 2.4823977467602756,
            "mae": 0.31180593409388574,
            "precision": 0.66,
            "recall": 0.8863636363636364
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.6308154436532463,
            "auditor_fn_violation": 0.027218161010813466,
            "auditor_fp_violation": 0.03457116472641014,
            "ave_precision_score": 0.6278454971099747,
            "fpr": 0.2349066959385291,
            "logloss": 2.9722462466873014,
            "mae": 0.32765326542211787,
            "precision": 0.6462809917355372,
            "recall": 0.8319148936170213
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.6704873602625401,
            "auditor_fn_violation": 0.02013556618819777,
            "auditor_fp_violation": 0.02269839317920971,
            "ave_precision_score": 0.6670085918618465,
            "fpr": 0.20065789473684212,
            "logloss": 2.284368215676782,
            "mae": 0.31469369848285966,
            "precision": 0.6861063464837049,
            "recall": 0.8264462809917356
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.6323667084454481,
            "auditor_fn_violation": 0.028096316883480858,
            "auditor_fp_violation": 0.028622206291956963,
            "ave_precision_score": 0.6295279704719687,
            "fpr": 0.20417124039517015,
            "logloss": 2.6677449925009813,
            "mae": 0.33611743892946383,
            "precision": 0.6593406593406593,
            "recall": 0.7659574468085106
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.6566733112573548,
            "auditor_fn_violation": 0.0059536755110917786,
            "auditor_fp_violation": 0.0026489998360386966,
            "ave_precision_score": 0.6501258849252375,
            "fpr": 0.17324561403508773,
            "logloss": 2.1982279564785765,
            "mae": 0.3145261120115021,
            "precision": 0.7074074074074074,
            "recall": 0.7892561983471075
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.6219118996743109,
            "auditor_fn_violation": 0.005161501272858912,
            "auditor_fp_violation": 0.012353422891293372,
            "ave_precision_score": 0.6190171134498131,
            "fpr": 0.19209659714599342,
            "logloss": 2.610273612497788,
            "mae": 0.33989934672120475,
            "precision": 0.6634615384615384,
            "recall": 0.7340425531914894
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.6732720031411303,
            "auditor_fn_violation": 0.018590510366826154,
            "auditor_fp_violation": 0.032182529922938206,
            "ave_precision_score": 0.6668516980517707,
            "fpr": 0.21271929824561403,
            "logloss": 2.2733794676350816,
            "mae": 0.30530501816600447,
            "precision": 0.6830065359477124,
            "recall": 0.8636363636363636
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.6350201489017233,
            "auditor_fn_violation": 0.026900530163252917,
            "auditor_fp_violation": 0.0377472613633818,
            "ave_precision_score": 0.6321231098166988,
            "fpr": 0.21624588364434688,
            "logloss": 2.5854390946603667,
            "mae": 0.3265077530428678,
            "precision": 0.6567944250871081,
            "recall": 0.8021276595744681
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.6548505349522922,
            "auditor_fn_violation": 0.005382775119617226,
            "auditor_fp_violation": 0.0050623052959501615,
            "ave_precision_score": 0.6482692531136512,
            "fpr": 0.17324561403508773,
            "logloss": 2.4296259042676858,
            "mae": 0.3129320156174585,
            "precision": 0.7148014440433214,
            "recall": 0.8181818181818182
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.6294637399013308,
            "auditor_fn_violation": 0.00624518298806549,
            "auditor_fp_violation": 0.012087088768914076,
            "ave_precision_score": 0.6264854970104795,
            "fpr": 0.18990120746432493,
            "logloss": 2.871967890353883,
            "mae": 0.3281511977376826,
            "precision": 0.6766355140186916,
            "recall": 0.7702127659574468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.6821446277937492,
            "auditor_fn_violation": 0.01715646295490793,
            "auditor_fp_violation": 0.024932365961633063,
            "ave_precision_score": 0.6011830862385081,
            "fpr": 0.3157894736842105,
            "logloss": 6.277968476421154,
            "mae": 0.3579516630511658,
            "precision": 0.6092265943012212,
            "recall": 0.9276859504132231
        },
        "train": {
            "accuracy": 0.6663007683863886,
            "auc_prc": 0.684605848713497,
            "auditor_fn_violation": 0.016920849195413035,
            "auditor_fp_violation": 0.03176345547366405,
            "ave_precision_score": 0.608423563837215,
            "fpr": 0.29747530186608123,
            "logloss": 5.856281788440008,
            "mae": 0.3392431965901956,
            "precision": 0.617231638418079,
            "recall": 0.9297872340425531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.6523693016011882,
            "auditor_fn_violation": 0.0020049478033927802,
            "auditor_fp_violation": 0.0006148548942449594,
            "ave_precision_score": 0.6467455895815784,
            "fpr": 0.14912280701754385,
            "logloss": 2.252220972496786,
            "mae": 0.33766195776806573,
            "precision": 0.7062634989200864,
            "recall": 0.6756198347107438
        },
        "train": {
            "accuracy": 0.6509330406147091,
            "auc_prc": 0.6197780365395957,
            "auditor_fn_violation": 0.01414157927925824,
            "auditor_fp_violation": 0.008579941306928917,
            "ave_precision_score": 0.6169161540469495,
            "fpr": 0.16355653128430298,
            "logloss": 2.746610310242708,
            "mae": 0.3512190415814152,
            "precision": 0.6688888888888889,
            "recall": 0.6404255319148936
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.6681509996002246,
            "auditor_fn_violation": 0.019138755980861243,
            "auditor_fp_violation": 0.03548481308411215,
            "ave_precision_score": 0.6617206184859561,
            "fpr": 0.2225877192982456,
            "logloss": 2.3432799835427143,
            "mae": 0.30880726251562873,
            "precision": 0.6787974683544303,
            "recall": 0.8863636363636364
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.6304361060435586,
            "auditor_fn_violation": 0.03015157530887265,
            "auditor_fp_violation": 0.03828490781603531,
            "ave_precision_score": 0.6275391847482577,
            "fpr": 0.2239297475301866,
            "logloss": 2.6739411110266804,
            "mae": 0.33046799929740917,
            "precision": 0.6530612244897959,
            "recall": 0.8170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.6856951838159291,
            "auditor_fn_violation": 0.018069450485718428,
            "auditor_fp_violation": 0.03233111985571406,
            "ave_precision_score": 0.680052110900846,
            "fpr": 0.21600877192982457,
            "logloss": 2.2574768325676016,
            "mae": 0.30614927314526913,
            "precision": 0.6817447495961227,
            "recall": 0.871900826446281
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6416408141992317,
            "auditor_fn_violation": 0.030735455543358946,
            "auditor_fp_violation": 0.03909137749501557,
            "ave_precision_score": 0.6388170177493984,
            "fpr": 0.21405049396267836,
            "logloss": 2.4958594776671843,
            "mae": 0.32329827170465886,
            "precision": 0.6608695652173913,
            "recall": 0.8085106382978723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.6635917718697566,
            "auditor_fn_violation": 0.02073138683485573,
            "auditor_fp_violation": 0.029454111329726185,
            "ave_precision_score": 0.6570905451668297,
            "fpr": 0.20723684210526316,
            "logloss": 2.423761546103176,
            "mae": 0.30676270486622204,
            "precision": 0.685,
            "recall": 0.8491735537190083
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.6321739986384491,
            "auditor_fn_violation": 0.023051591657519216,
            "auditor_fp_violation": 0.0365375568449114,
            "ave_precision_score": 0.6291966803017944,
            "fpr": 0.2074643249176729,
            "logloss": 2.934226716719647,
            "mae": 0.32389453814295266,
            "precision": 0.6654867256637168,
            "recall": 0.8
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.655872475045596,
            "auditor_fn_violation": 0.01942647165434247,
            "auditor_fp_violation": 0.023997274143302185,
            "ave_precision_score": 0.6484434451967228,
            "fpr": 0.18969298245614036,
            "logloss": 2.593548302425291,
            "mae": 0.30688891094483917,
            "precision": 0.6996527777777778,
            "recall": 0.8326446280991735
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.6242090996440073,
            "auditor_fn_violation": 0.025389448116402365,
            "auditor_fp_violation": 0.027616608297179115,
            "ave_precision_score": 0.618567995107743,
            "fpr": 0.19978046103183314,
            "logloss": 3.062806575444087,
            "mae": 0.3233377200787419,
            "precision": 0.6696914700544465,
            "recall": 0.7851063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 32400,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.6953168808916814,
            "auditor_fn_violation": 0.01584928229665072,
            "auditor_fp_violation": 0.012258669454008855,
            "ave_precision_score": 0.6858301822749988,
            "fpr": 0.2050438596491228,
            "logloss": 1.7621968417857874,
            "mae": 0.32054400917991627,
            "precision": 0.6736474694589878,
            "recall": 0.7975206611570248
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.6690445266828422,
            "auditor_fn_violation": 0.01932176471962071,
            "auditor_fp_violation": 0.021891669217998226,
            "ave_precision_score": 0.6581871608136539,
            "fpr": 0.1986827661909989,
            "logloss": 2.055017977019899,
            "mae": 0.33565918557562147,
            "precision": 0.6660516605166051,
            "recall": 0.7680851063829788
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.6633262561314801,
            "auditor_fn_violation": 0.021044022763520374,
            "auditor_fp_violation": 0.028237211018199714,
            "ave_precision_score": 0.6568253154165349,
            "fpr": 0.20285087719298245,
            "logloss": 2.419370072215049,
            "mae": 0.3067942878911252,
            "precision": 0.6885521885521886,
            "recall": 0.8450413223140496
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.6320750340609105,
            "auditor_fn_violation": 0.023224420206927155,
            "auditor_fp_violation": 0.03647781835017213,
            "ave_precision_score": 0.6290978045500342,
            "fpr": 0.20417124039517015,
            "logloss": 2.9311130894514035,
            "mae": 0.3239798377636254,
            "precision": 0.6678571428571428,
            "recall": 0.7957446808510639
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.6664836148738419,
            "auditor_fn_violation": 0.020652095113817603,
            "auditor_fp_violation": 0.027553184948352193,
            "ave_precision_score": 0.6599750198390608,
            "fpr": 0.20065789473684212,
            "logloss": 2.381480527854775,
            "mae": 0.3052923865488291,
            "precision": 0.6903553299492385,
            "recall": 0.8429752066115702
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.6349669014962632,
            "auditor_fn_violation": 0.021206530116542496,
            "auditor_fp_violation": 0.035472220355394275,
            "ave_precision_score": 0.6319927241091841,
            "fpr": 0.2030735455543359,
            "logloss": 2.8654560776751956,
            "mae": 0.32106935362429406,
            "precision": 0.6708185053380783,
            "recall": 0.8021276595744681
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 32400,
        "test": {
            "accuracy": 0.5504385964912281,
            "auc_prc": 0.7668604369144124,
            "auditor_fn_violation": 0.0012052341597796144,
            "auditor_fp_violation": 0.0017010985407443769,
            "ave_precision_score": 0.5422400284387101,
            "fpr": 0.4473684210526316,
            "logloss": 15.213864316305944,
            "mae": 0.4499201807050811,
            "precision": 0.5415730337078651,
            "recall": 0.9958677685950413
        },
        "train": {
            "accuracy": 0.5323819978046103,
            "auc_prc": 0.7530785688666887,
            "auditor_fn_violation": 0.001746969661583016,
            "auditor_fp_violation": 0.005378953630482559,
            "ave_precision_score": 0.5243778667486549,
            "fpr": 0.4588364434687157,
            "logloss": 15.598483438048751,
            "mae": 0.46773203249406753,
            "precision": 0.525,
            "recall": 0.9829787234042553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.6615379173383861,
            "auditor_fn_violation": 0.022765785848919824,
            "auditor_fp_violation": 0.0266232169208067,
            "ave_precision_score": 0.654985291460292,
            "fpr": 0.19846491228070176,
            "logloss": 2.439627442683126,
            "mae": 0.30791428094180917,
            "precision": 0.6911262798634812,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.6300858492414447,
            "auditor_fn_violation": 0.029025854216783057,
            "auditor_fp_violation": 0.03727682071730998,
            "ave_precision_score": 0.6270967532355881,
            "fpr": 0.20087815587266739,
            "logloss": 2.943549179081895,
            "mae": 0.32460255975635527,
            "precision": 0.6672727272727272,
            "recall": 0.7808510638297872
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.6633313763407958,
            "auditor_fn_violation": 0.011979846309989852,
            "auditor_fp_violation": 0.02232435645187736,
            "ave_precision_score": 0.6567886928158293,
            "fpr": 0.18640350877192982,
            "logloss": 2.404478355682727,
            "mae": 0.3048437733051738,
            "precision": 0.7048611111111112,
            "recall": 0.8388429752066116
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.6327352887602211,
            "auditor_fn_violation": 0.019651073171870995,
            "auditor_fp_violation": 0.025314187145769395,
            "ave_precision_score": 0.6297478651234812,
            "fpr": 0.1942919868276619,
            "logloss": 2.8978838264357454,
            "mae": 0.3201018655793061,
            "precision": 0.6805054151624549,
            "recall": 0.8021276595744681
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 32400,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.6550090247655663,
            "auditor_fn_violation": 0.01726294040887343,
            "auditor_fp_violation": 0.03343017297917692,
            "ave_precision_score": 0.6476096730515966,
            "fpr": 0.22039473684210525,
            "logloss": 2.5897218181863164,
            "mae": 0.3089800607870852,
            "precision": 0.6747572815533981,
            "recall": 0.8615702479338843
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.6244879742755511,
            "auditor_fn_violation": 0.02445290422028634,
            "auditor_fp_violation": 0.03148218672759995,
            "ave_precision_score": 0.6188606199347992,
            "fpr": 0.2261251372118551,
            "logloss": 3.0871126452976103,
            "mae": 0.32536767460873023,
            "precision": 0.6484641638225256,
            "recall": 0.8085106382978723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7154053320706952,
            "auditor_fn_violation": 0.02791974771639844,
            "auditor_fp_violation": 0.018563494015412362,
            "ave_precision_score": 0.6642371131879389,
            "fpr": 0.16885964912280702,
            "logloss": 3.6161454665114228,
            "mae": 0.3315900987918551,
            "precision": 0.7137546468401487,
            "recall": 0.7933884297520661
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.6851100565819621,
            "auditor_fn_violation": 0.028577434196697577,
            "auditor_fp_violation": 0.023835659400972254,
            "ave_precision_score": 0.627745036728894,
            "fpr": 0.18111964873765093,
            "logloss": 4.112166740330158,
            "mae": 0.3544868895896016,
            "precision": 0.6802325581395349,
            "recall": 0.7468085106382979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.6599548431632037,
            "auditor_fn_violation": 0.02550927939683921,
            "auditor_fp_violation": 0.03025854648303001,
            "ave_precision_score": 0.6533593862822601,
            "fpr": 0.2050438596491228,
            "logloss": 2.4644888228552553,
            "mae": 0.3107405391709461,
            "precision": 0.6878130217028381,
            "recall": 0.8512396694214877
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.6302988820464859,
            "auditor_fn_violation": 0.02774598874278908,
            "auditor_fp_violation": 0.03866822982394568,
            "ave_precision_score": 0.6264008048867356,
            "fpr": 0.20965971459934138,
            "logloss": 2.9644817744391676,
            "mae": 0.32428916638905797,
            "precision": 0.6654991243432574,
            "recall": 0.8085106382978723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 32400,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8326312152808977,
            "auditor_fn_violation": 0.009904668696534724,
            "auditor_fp_violation": 0.00900250040990326,
            "ave_precision_score": 0.8329509760523633,
            "fpr": 0.1162280701754386,
            "logloss": 0.8825649660914164,
            "mae": 0.2707455906013568,
            "precision": 0.7763713080168776,
            "recall": 0.7603305785123967
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8227967578958986,
            "auditor_fn_violation": 0.009535931989630297,
            "auditor_fp_violation": 0.017568095661242907,
            "ave_precision_score": 0.8229870207068969,
            "fpr": 0.11306256860592755,
            "logloss": 1.1394029022404097,
            "mae": 0.2821292619282107,
            "precision": 0.7659090909090909,
            "recall": 0.7170212765957447
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 32400,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.6528388431377492,
            "auditor_fn_violation": 0.01439937654052487,
            "auditor_fp_violation": 0.027494261354320388,
            "ave_precision_score": 0.645410725478792,
            "fpr": 0.2138157894736842,
            "logloss": 2.519754600230768,
            "mae": 0.30607844412521856,
            "precision": 0.6834415584415584,
            "recall": 0.8698347107438017
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.6227880961837421,
            "auditor_fn_violation": 0.025559941144872363,
            "auditor_fp_violation": 0.02840565424852707,
            "ave_precision_score": 0.6163375808822957,
            "fpr": 0.2239297475301866,
            "logloss": 3.0392558769510307,
            "mae": 0.325409320977714,
            "precision": 0.6530612244897959,
            "recall": 0.8170212765957446
        }
    }
]
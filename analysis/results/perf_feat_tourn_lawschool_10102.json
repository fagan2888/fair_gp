[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8106313276117243,
            "auditor_fn_violation": 0.013106129056546945,
            "auditor_fp_violation": 0.01562622925025569,
            "ave_precision_score": 0.8112248848701837,
            "fpr": 0.1600877192982456,
            "logloss": 0.7376149390311526,
            "mae": 0.2822833848276995,
            "precision": 0.71484375,
            "recall": 0.7854077253218884
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8608105976072506,
            "auditor_fn_violation": 0.010875726548019656,
            "auditor_fp_violation": 0.031002742939590454,
            "ave_precision_score": 0.8609980237593,
            "fpr": 0.14050493962678376,
            "logloss": 0.6740777715620602,
            "mae": 0.2582090676585413,
            "precision": 0.7561904761904762,
            "recall": 0.8135245901639344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8122049149397497,
            "auditor_fn_violation": 0.009124877644755669,
            "auditor_fp_violation": 0.015631146251278427,
            "ave_precision_score": 0.812816953684384,
            "fpr": 0.16666666666666666,
            "logloss": 0.7243121554787105,
            "mae": 0.2821426635814045,
            "precision": 0.7099236641221374,
            "recall": 0.7982832618025751
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8622036017348448,
            "auditor_fn_violation": 0.013788666750643329,
            "auditor_fp_violation": 0.03040069754225347,
            "ave_precision_score": 0.8623795690785838,
            "fpr": 0.14928649835345773,
            "logloss": 0.6712028134414774,
            "mae": 0.25969771518731666,
            "precision": 0.7453183520599251,
            "recall": 0.8155737704918032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8158963926707599,
            "auditor_fn_violation": 0.014477919584368653,
            "auditor_fp_violation": 0.015345960191959726,
            "ave_precision_score": 0.8164191667324578,
            "fpr": 0.15570175438596492,
            "logloss": 0.7362419028198125,
            "mae": 0.27970198885790887,
            "precision": 0.7199211045364892,
            "recall": 0.7832618025751072
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8618148767215557,
            "auditor_fn_violation": 0.015601662737758907,
            "auditor_fp_violation": 0.030971602660417853,
            "ave_precision_score": 0.8620041081362979,
            "fpr": 0.13721185510428102,
            "logloss": 0.6779927827101938,
            "mae": 0.2564534673649825,
            "precision": 0.7619047619047619,
            "recall": 0.819672131147541
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8211512649866337,
            "auditor_fn_violation": 0.013506136586100453,
            "auditor_fp_violation": 0.010419125167178038,
            "ave_precision_score": 0.8215759961409549,
            "fpr": 0.13157894736842105,
            "logloss": 0.6861121994943176,
            "mae": 0.28040497450508545,
            "precision": 0.744136460554371,
            "recall": 0.7489270386266095
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.861453760943987,
            "auditor_fn_violation": 0.011750733296143678,
            "auditor_fp_violation": 0.0268584907863699,
            "ave_precision_score": 0.861636852837805,
            "fpr": 0.1141602634467618,
            "logloss": 0.6568101728605308,
            "mae": 0.2641613205749942,
            "precision": 0.7805907172995781,
            "recall": 0.7581967213114754
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8127269149906776,
            "auditor_fn_violation": 0.0076472027708756884,
            "auditor_fp_violation": 0.01794213673196444,
            "ave_precision_score": 0.813336344069888,
            "fpr": 0.16228070175438597,
            "logloss": 0.7218211644309735,
            "mae": 0.2819564769905241,
            "precision": 0.7148362235067437,
            "recall": 0.796137339055794
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8619270888753714,
            "auditor_fn_violation": 0.011849705781792664,
            "auditor_fp_violation": 0.029894668005698673,
            "ave_precision_score": 0.8621047069666679,
            "fpr": 0.14818880351262348,
            "logloss": 0.6707437460963254,
            "mae": 0.2597503651726195,
            "precision": 0.7471910112359551,
            "recall": 0.8176229508196722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8070097973652142,
            "auditor_fn_violation": 0.01299083276861682,
            "auditor_fp_violation": 0.01709149555503108,
            "ave_precision_score": 0.8075699113339909,
            "fpr": 0.17653508771929824,
            "logloss": 0.8366611789045576,
            "mae": 0.281332225959238,
            "precision": 0.7007434944237918,
            "recall": 0.8090128755364807
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8538904377594505,
            "auditor_fn_violation": 0.015979557682964135,
            "auditor_fp_violation": 0.03036955726308087,
            "ave_precision_score": 0.8540925583885576,
            "fpr": 0.15477497255762898,
            "logloss": 0.7821093894163336,
            "mae": 0.25881395971855775,
            "precision": 0.7393715341959335,
            "recall": 0.819672131147541
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8153065868969724,
            "auditor_fn_violation": 0.014127324749642347,
            "auditor_fp_violation": 0.015754071276846825,
            "ave_precision_score": 0.815897879807167,
            "fpr": 0.16228070175438597,
            "logloss": 0.7184309052465521,
            "mae": 0.2784728243716113,
            "precision": 0.7164750957854407,
            "recall": 0.8025751072961373
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8646073639634332,
            "auditor_fn_violation": 0.012407550700905155,
            "auditor_fp_violation": 0.02825461330260826,
            "ave_precision_score": 0.864783423479917,
            "fpr": 0.145993413830955,
            "logloss": 0.6681407533544225,
            "mae": 0.25784492463082886,
            "precision": 0.7485822306238186,
            "recall": 0.8114754098360656
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7692562233397069,
            "auditor_fn_violation": 0.027045214968752358,
            "auditor_fp_violation": 0.03291686334670758,
            "ave_precision_score": 0.7712162601491434,
            "fpr": 0.19846491228070176,
            "logloss": 0.5905360983307536,
            "mae": 0.3557514025847093,
            "precision": 0.6732851985559567,
            "recall": 0.8004291845493562
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8139923495803183,
            "auditor_fn_violation": 0.03161721041550449,
            "auditor_fp_violation": 0.034075250484620595,
            "ave_precision_score": 0.8143487022862798,
            "fpr": 0.16794731064763996,
            "logloss": 0.548318666112869,
            "mae": 0.33749325378796924,
            "precision": 0.7262969588550984,
            "recall": 0.8319672131147541
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7328804050989469,
            "auditor_fn_violation": 0.019310951735562083,
            "auditor_fp_violation": 0.020857918338447013,
            "ave_precision_score": 0.733620790177441,
            "fpr": 0.17763157894736842,
            "logloss": 1.0615474203101807,
            "mae": 0.3368774447520257,
            "precision": 0.670061099796334,
            "recall": 0.7060085836909872
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.770465196594116,
            "auditor_fn_violation": 0.013723434885101946,
            "auditor_fp_violation": 0.028412909721735657,
            "ave_precision_score": 0.7709828367400882,
            "fpr": 0.15367727771679474,
            "logloss": 1.0167551590427726,
            "mae": 0.30870261780539393,
            "precision": 0.716024340770791,
            "recall": 0.7233606557377049
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8123497356918312,
            "auditor_fn_violation": 0.008037798358557335,
            "auditor_fp_violation": 0.01651128943434821,
            "ave_precision_score": 0.8127745678130156,
            "fpr": 0.1524122807017544,
            "logloss": 0.7616235263870669,
            "mae": 0.2850381344263873,
            "precision": 0.7231075697211156,
            "recall": 0.778969957081545
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8460291811231921,
            "auditor_fn_violation": 0.013732432383797306,
            "auditor_fp_violation": 0.02101968844150688,
            "ave_precision_score": 0.8462535529529526,
            "fpr": 0.13830954994511527,
            "logloss": 0.7329671506872711,
            "mae": 0.2691338956531163,
            "precision": 0.7504950495049505,
            "recall": 0.7766393442622951
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.833038141689311,
            "auditor_fn_violation": 0.01376967095851216,
            "auditor_fp_violation": 0.024071178506805135,
            "ave_precision_score": 0.8333737220798215,
            "fpr": 0.16776315789473684,
            "logloss": 0.6965669669952554,
            "mae": 0.27668734095161185,
            "precision": 0.7150837988826816,
            "recall": 0.8240343347639485
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8622525657538447,
            "auditor_fn_violation": 0.017090748771841427,
            "auditor_fp_violation": 0.03207448754778087,
            "ave_precision_score": 0.8624621658625065,
            "fpr": 0.14489571899012074,
            "logloss": 0.6723083763270619,
            "mae": 0.2576188369960553,
            "precision": 0.7586837294332724,
            "recall": 0.8504098360655737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8142420923570249,
            "auditor_fn_violation": 0.015454408553572777,
            "auditor_fp_violation": 0.017688911179293526,
            "ave_precision_score": 0.8148068134743061,
            "fpr": 0.16337719298245615,
            "logloss": 0.7301207010886487,
            "mae": 0.28026124918408274,
            "precision": 0.7140115163147792,
            "recall": 0.7982832618025751
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8640370256266696,
            "auditor_fn_violation": 0.015124795306904682,
            "auditor_fp_violation": 0.03155807791816854,
            "ave_precision_score": 0.8642192616259324,
            "fpr": 0.1394072447859495,
            "logloss": 0.6664680178817332,
            "mae": 0.256200917669462,
            "precision": 0.7608286252354048,
            "recall": 0.8278688524590164
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8274822341510739,
            "auditor_fn_violation": 0.008456629771854528,
            "auditor_fp_violation": 0.013566005821729208,
            "ave_precision_score": 0.8278188130040274,
            "fpr": 0.15350877192982457,
            "logloss": 0.6557557440576145,
            "mae": 0.2784315896199985,
            "precision": 0.726027397260274,
            "recall": 0.796137339055794
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8638530724256266,
            "auditor_fn_violation": 0.008097748825826425,
            "auditor_fp_violation": 0.025566169200706885,
            "ave_precision_score": 0.8640393986164759,
            "fpr": 0.14489571899012074,
            "logloss": 0.6317776026878315,
            "mae": 0.262410821906347,
            "precision": 0.7518796992481203,
            "recall": 0.819672131147541
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8001418668813389,
            "auditor_fn_violation": 0.013955556810481141,
            "auditor_fp_violation": 0.02053093777043506,
            "ave_precision_score": 0.8030884680069033,
            "fpr": 0.17653508771929824,
            "logloss": 0.964933805186276,
            "mae": 0.2788517986268753,
            "precision": 0.7029520295202952,
            "recall": 0.8175965665236051
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8416311738766669,
            "auditor_fn_violation": 0.021103633189973187,
            "auditor_fp_violation": 0.0355907440710206,
            "ave_precision_score": 0.841866340965107,
            "fpr": 0.150384193194292,
            "logloss": 0.9286897032526839,
            "mae": 0.25862881621802014,
            "precision": 0.7458256029684601,
            "recall": 0.8237704918032787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 10102,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8256659406393643,
            "auditor_fn_violation": 0.01748503501242376,
            "auditor_fp_violation": 0.01507552513570923,
            "ave_precision_score": 0.8271281963822795,
            "fpr": 0.1787280701754386,
            "logloss": 0.5443859374810536,
            "mae": 0.30628099410872284,
            "precision": 0.7078853046594982,
            "recall": 0.8476394849785408
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.872204591881587,
            "auditor_fn_violation": 0.014108077954328698,
            "auditor_fp_violation": 0.02128957086100277,
            "ave_precision_score": 0.8723664094795021,
            "fpr": 0.16575192096597147,
            "logloss": 0.5070896008069125,
            "mae": 0.29277365952173023,
            "precision": 0.7373913043478261,
            "recall": 0.8688524590163934
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.81391039228841,
            "auditor_fn_violation": 0.013840260522551011,
            "auditor_fp_violation": 0.017214420580599483,
            "ave_precision_score": 0.8144145575391232,
            "fpr": 0.16885964912280702,
            "logloss": 0.7579440784028607,
            "mae": 0.2816913926168065,
            "precision": 0.7094339622641509,
            "recall": 0.8068669527896996
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8586689982914905,
            "auditor_fn_violation": 0.013889888610966154,
            "auditor_fp_violation": 0.02930819274794799,
            "ave_precision_score": 0.8588665745446014,
            "fpr": 0.1437980241492865,
            "logloss": 0.6925131880736741,
            "mae": 0.26008639592871974,
            "precision": 0.7518939393939394,
            "recall": 0.8135245901639344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7166759746005636,
            "auditor_fn_violation": 0.011727279572321368,
            "auditor_fp_violation": 0.017111163559122025,
            "ave_precision_score": 0.7178346984040238,
            "fpr": 0.18530701754385964,
            "logloss": 1.0272637008319918,
            "mae": 0.34210928490860115,
            "precision": 0.6653465346534654,
            "recall": 0.721030042918455
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7673690843382881,
            "auditor_fn_violation": 0.017707077432473777,
            "auditor_fp_violation": 0.026928556414508267,
            "ave_precision_score": 0.7679338976434548,
            "fpr": 0.15916575192096596,
            "logloss": 0.9507955713846098,
            "mae": 0.31106947848046995,
            "precision": 0.7117296222664016,
            "recall": 0.7336065573770492
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7839555357155497,
            "auditor_fn_violation": 0.04071606053761012,
            "auditor_fp_violation": 0.05772559200692315,
            "ave_precision_score": 0.7795952606829253,
            "fpr": 0.21929824561403508,
            "logloss": 1.328261413238845,
            "mae": 0.3111986551857237,
            "precision": 0.6621621621621622,
            "recall": 0.8412017167381974
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8082073057936175,
            "auditor_fn_violation": 0.03752856705835778,
            "auditor_fp_violation": 0.04497953824156035,
            "ave_precision_score": 0.8055750628928009,
            "fpr": 0.1964873765093304,
            "logloss": 1.233336745975998,
            "mae": 0.2910780293137786,
            "precision": 0.7001675041876047,
            "recall": 0.8565573770491803
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5504385964912281,
            "auc_prc": 0.5067023004618,
            "auditor_fn_violation": 0.011567276560499976,
            "auditor_fp_violation": 0.02002940366611597,
            "ave_precision_score": 0.47753586783946833,
            "fpr": 0.24232456140350878,
            "logloss": 9.075742872096301,
            "mae": 0.4668646324170924,
            "precision": 0.5562248995983936,
            "recall": 0.5944206008583691
        },
        "train": {
            "accuracy": 0.5949506037321625,
            "auc_prc": 0.5662114130055641,
            "auditor_fn_violation": 0.012884418131759369,
            "auditor_fp_violation": 0.014550295443398649,
            "ave_precision_score": 0.5382969082465575,
            "fpr": 0.2074643249176729,
            "logloss": 8.052914959584982,
            "mae": 0.4220416533698898,
            "precision": 0.6197183098591549,
            "recall": 0.6311475409836066
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8080200750258577,
            "auditor_fn_violation": 0.012339055793991416,
            "auditor_fp_violation": 0.019825348123672412,
            "ave_precision_score": 0.8086423257550452,
            "fpr": 0.17324561403508773,
            "logloss": 0.8299385360504119,
            "mae": 0.28060757372998046,
            "precision": 0.7030075187969925,
            "recall": 0.8025751072961373
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8517372373021006,
            "auditor_fn_violation": 0.010680030951395515,
            "auditor_fp_violation": 0.031781249918905526,
            "ave_precision_score": 0.8519391684856128,
            "fpr": 0.14709110867178923,
            "logloss": 0.7891502989621411,
            "mae": 0.26143446199855985,
            "precision": 0.74573055028463,
            "recall": 0.805327868852459
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8148022459115929,
            "auditor_fn_violation": 0.01138844966493487,
            "auditor_fp_violation": 0.017140665565258444,
            "ave_precision_score": 0.815319540086163,
            "fpr": 0.16776315789473684,
            "logloss": 0.7982037410812033,
            "mae": 0.27997558909206977,
            "precision": 0.711864406779661,
            "recall": 0.8111587982832618
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8549055022331814,
            "auditor_fn_violation": 0.015293498407442735,
            "auditor_fp_violation": 0.03132452582437402,
            "ave_precision_score": 0.8551076303696895,
            "fpr": 0.150384193194292,
            "logloss": 0.7528698113879785,
            "mae": 0.2581620223007185,
            "precision": 0.7424812030075187,
            "recall": 0.8094262295081968
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8323016164330383,
            "auditor_fn_violation": 0.015470879451848508,
            "auditor_fp_violation": 0.018446129336794907,
            "ave_precision_score": 0.8329462748664143,
            "fpr": 0.16337719298245615,
            "logloss": 0.7139282525272284,
            "mae": 0.27049748687226316,
            "precision": 0.7230483271375465,
            "recall": 0.8347639484978541
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.86700256273363,
            "auditor_fn_violation": 0.014467977902143207,
            "auditor_fp_violation": 0.028698362280817853,
            "ave_precision_score": 0.8672271457614356,
            "fpr": 0.1394072447859495,
            "logloss": 0.6878168221903924,
            "mae": 0.2526231972422016,
            "precision": 0.7661141804788214,
            "recall": 0.8524590163934426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8161155930032822,
            "auditor_fn_violation": 0.009404882915443114,
            "auditor_fp_violation": 0.014765754071276857,
            "ave_precision_score": 0.8167193963304161,
            "fpr": 0.17543859649122806,
            "logloss": 0.7747077639388777,
            "mae": 0.2803425178532742,
            "precision": 0.702048417132216,
            "recall": 0.8090128755364807
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.86074090920978,
            "auditor_fn_violation": 0.009827517950009898,
            "auditor_fp_violation": 0.030398102518989084,
            "ave_precision_score": 0.8609198414476158,
            "fpr": 0.14928649835345773,
            "logloss": 0.7301216753113438,
            "mae": 0.26017182761586377,
            "precision": 0.7457943925233644,
            "recall": 0.8176229508196722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8276640659370696,
            "auditor_fn_violation": 0.013063775318123635,
            "auditor_fp_violation": 0.01870427189048855,
            "ave_precision_score": 0.8280605257167026,
            "fpr": 0.16885964912280702,
            "logloss": 0.7416515065211082,
            "mae": 0.27729279296959636,
            "precision": 0.7126865671641791,
            "recall": 0.8197424892703863
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8577947799002986,
            "auditor_fn_violation": 0.016411437620341546,
            "auditor_fp_violation": 0.03262463247983019,
            "ave_precision_score": 0.8580026697815061,
            "fpr": 0.1525795828759605,
            "logloss": 0.7177888939967106,
            "mae": 0.25941264165628386,
            "precision": 0.7481884057971014,
            "recall": 0.8463114754098361
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8043947517773072,
            "auditor_fn_violation": 0.010541374896468642,
            "auditor_fp_violation": 0.020380969239241608,
            "ave_precision_score": 0.8049847774214741,
            "fpr": 0.17214912280701755,
            "logloss": 0.9097667896237474,
            "mae": 0.2822434240160177,
            "precision": 0.7015209125475285,
            "recall": 0.7918454935622318
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8474432792158453,
            "auditor_fn_violation": 0.01080599593313059,
            "auditor_fp_violation": 0.03220683373426443,
            "ave_precision_score": 0.8476540356425026,
            "fpr": 0.14818880351262348,
            "logloss": 0.8654927009872178,
            "mae": 0.2617910397000867,
            "precision": 0.7443181818181818,
            "recall": 0.805327868852459
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8047778680228351,
            "auditor_fn_violation": 0.016600312476470144,
            "auditor_fp_violation": 0.018846864920147907,
            "ave_precision_score": 0.8063711084381795,
            "fpr": 0.16885964912280702,
            "logloss": 0.7623132385044566,
            "mae": 0.28099116964648924,
            "precision": 0.7066666666666667,
            "recall": 0.796137339055794
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8586586193547124,
            "auditor_fn_violation": 0.015471199006676149,
            "auditor_fp_violation": 0.031046858335084984,
            "ave_precision_score": 0.8588544280526234,
            "fpr": 0.13611416026344675,
            "logloss": 0.6897451385739656,
            "mae": 0.2597940536675436,
            "precision": 0.761996161228407,
            "recall": 0.8135245901639344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.7205095034695213,
            "auditor_fn_violation": 0.015186168210225132,
            "auditor_fp_violation": 0.013629926835024788,
            "ave_precision_score": 0.7221819904466844,
            "fpr": 0.18092105263157895,
            "logloss": 1.066213426908619,
            "mae": 0.3384086172611672,
            "precision": 0.665314401622718,
            "recall": 0.703862660944206
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7714969692798235,
            "auditor_fn_violation": 0.019049954112756656,
            "auditor_fp_violation": 0.027842004603571273,
            "ave_precision_score": 0.7720392139977457,
            "fpr": 0.145993413830955,
            "logloss": 1.0086432575870266,
            "mae": 0.3112128790912221,
            "precision": 0.7240663900414938,
            "recall": 0.7151639344262295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8152607953183326,
            "auditor_fn_violation": 0.01154845267675627,
            "auditor_fp_violation": 0.013133309731728434,
            "ave_precision_score": 0.8158272569614688,
            "fpr": 0.17324561403508773,
            "logloss": 0.7327581338485243,
            "mae": 0.2804466295631495,
            "precision": 0.7068645640074211,
            "recall": 0.8175965665236051
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.864472739820214,
            "auditor_fn_violation": 0.007341958935415956,
            "auditor_fp_violation": 0.03081849628781923,
            "ave_precision_score": 0.8646492021424804,
            "fpr": 0.15587266739846323,
            "logloss": 0.6775079749788684,
            "mae": 0.25812029644682305,
            "precision": 0.7418181818181818,
            "recall": 0.8360655737704918
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.7915063778209281,
            "auditor_fn_violation": 0.013960262781417059,
            "auditor_fp_violation": 0.024226064039021326,
            "ave_precision_score": 0.7892702809799699,
            "fpr": 0.15350877192982457,
            "logloss": 0.9736575800736167,
            "mae": 0.28125385295847777,
            "precision": 0.7281553398058253,
            "recall": 0.8047210300429185
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8230566303128466,
            "auditor_fn_violation": 0.018867754764175564,
            "auditor_fp_violation": 0.02886703879300278,
            "ave_precision_score": 0.8207244171592075,
            "fpr": 0.13830954994511527,
            "logloss": 0.8723794995965148,
            "mae": 0.26031932939707997,
            "precision": 0.7609108159392789,
            "recall": 0.8217213114754098
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8063688962862718,
            "auditor_fn_violation": 0.009011934342293509,
            "auditor_fp_violation": 0.012307253559908745,
            "ave_precision_score": 0.8068919801142713,
            "fpr": 0.13596491228070176,
            "logloss": 0.724495839899922,
            "mae": 0.28682628766409146,
            "precision": 0.7327586206896551,
            "recall": 0.7296137339055794
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8550264294669634,
            "auditor_fn_violation": 0.010119936657609187,
            "auditor_fp_violation": 0.024006560218812364,
            "ave_precision_score": 0.8552428370407275,
            "fpr": 0.1163556531284303,
            "logloss": 0.6737572158649805,
            "mae": 0.26685298403965135,
            "precision": 0.7758985200845666,
            "recall": 0.7520491803278688
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.706013897144056,
            "auditor_fn_violation": 0.005505985995030495,
            "auditor_fp_violation": 0.02161267799543703,
            "ave_precision_score": 0.7068558667969087,
            "fpr": 0.1962719298245614,
            "logloss": 1.0656574908425824,
            "mae": 0.3435234886433114,
            "precision": 0.6616257088846881,
            "recall": 0.7510729613733905
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7581651572594965,
            "auditor_fn_violation": 0.016638624462399453,
            "auditor_fp_violation": 0.023549836124280858,
            "ave_precision_score": 0.7588428299518661,
            "fpr": 0.1690450054884742,
            "logloss": 0.9819660648808612,
            "mae": 0.3147088889901335,
            "precision": 0.7055449330783938,
            "recall": 0.7561475409836066
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8112603661644723,
            "auditor_fn_violation": 0.014517920337323995,
            "auditor_fp_violation": 0.0188862009283298,
            "ave_precision_score": 0.8117914639152886,
            "fpr": 0.17324561403508773,
            "logloss": 0.7596007532945194,
            "mae": 0.28329101832996595,
            "precision": 0.7030075187969925,
            "recall": 0.8025751072961373
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8570947117094883,
            "auditor_fn_violation": 0.013741429882492667,
            "auditor_fp_violation": 0.03100793298611922,
            "ave_precision_score": 0.857293993724135,
            "fpr": 0.1437980241492865,
            "logloss": 0.6927666842948773,
            "mae": 0.26144948387272504,
            "precision": 0.7528301886792453,
            "recall": 0.8176229508196722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8005034734948063,
            "auditor_fn_violation": 0.01501675325653189,
            "auditor_fp_violation": 0.02091692235071985,
            "ave_precision_score": 0.8019385100821781,
            "fpr": 0.16228070175438597,
            "logloss": 0.8458267556462337,
            "mae": 0.28425816647761626,
            "precision": 0.7086614173228346,
            "recall": 0.7725321888412017
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8460681270586526,
            "auditor_fn_violation": 0.008635349372874343,
            "auditor_fp_violation": 0.03084444652046306,
            "ave_precision_score": 0.8462896666215478,
            "fpr": 0.14050493962678376,
            "logloss": 0.7905156772983272,
            "mae": 0.266757684822837,
            "precision": 0.7504873294346979,
            "recall": 0.7889344262295082
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.8368210974857812,
            "auditor_fn_violation": 0.01932271666290189,
            "auditor_fp_violation": 0.06498554401699316,
            "ave_precision_score": 0.8372181684041815,
            "fpr": 0.30372807017543857,
            "logloss": 0.6798235518631949,
            "mae": 0.3520487307431993,
            "precision": 0.6109550561797753,
            "recall": 0.9334763948497854
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.8535450245638463,
            "auditor_fn_violation": 0.020388332043691854,
            "auditor_fp_violation": 0.053257662454943934,
            "ave_precision_score": 0.8537525269716305,
            "fpr": 0.27442371020856204,
            "logloss": 0.6499069011215345,
            "mae": 0.33830240007917245,
            "precision": 0.6458923512747875,
            "recall": 0.9344262295081968
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7772503304197352,
            "auditor_fn_violation": 0.009691947142534452,
            "auditor_fp_violation": 0.011953229486271739,
            "ave_precision_score": 0.7437573526443125,
            "fpr": 0.17543859649122806,
            "logloss": 3.472402241621068,
            "mae": 0.2913357177227345,
            "precision": 0.6952380952380952,
            "recall": 0.7832618025751072
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.7982323142823838,
            "auditor_fn_violation": 0.013089111227078874,
            "auditor_fp_violation": 0.03293084522502745,
            "ave_precision_score": 0.7652840876470092,
            "fpr": 0.15477497255762898,
            "logloss": 3.631443090255373,
            "mae": 0.26739393419982727,
            "precision": 0.7359550561797753,
            "recall": 0.805327868852459
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8097788391260676,
            "auditor_fn_violation": 0.011266094420600862,
            "auditor_fp_violation": 0.015326292187868785,
            "ave_precision_score": 0.8103598495953911,
            "fpr": 0.17324561403508773,
            "logloss": 0.7055890070071562,
            "mae": 0.2860991731190258,
            "precision": 0.7063197026022305,
            "recall": 0.8154506437768241
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8615646671409466,
            "auditor_fn_violation": 0.011885695776574112,
            "auditor_fp_violation": 0.02931338279447675,
            "ave_precision_score": 0.8617602942658119,
            "fpr": 0.15587266739846323,
            "logloss": 0.6409947612193774,
            "mae": 0.2609062893478444,
            "precision": 0.7413479052823315,
            "recall": 0.8340163934426229
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8086666610958451,
            "auditor_fn_violation": 0.005703636774339285,
            "auditor_fp_violation": 0.015542640232869168,
            "ave_precision_score": 0.8091355403153295,
            "fpr": 0.12938596491228072,
            "logloss": 0.7145963937412905,
            "mae": 0.2906422614921032,
            "precision": 0.7371937639198218,
            "recall": 0.7103004291845494
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8467835946656382,
            "auditor_fn_violation": 0.012443540695686603,
            "auditor_fp_violation": 0.021105324209231537,
            "ave_precision_score": 0.8470283820728373,
            "fpr": 0.10976948408342481,
            "logloss": 0.6882661210365364,
            "mae": 0.2742336923050064,
            "precision": 0.7816593886462883,
            "recall": 0.7336065573770492
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8034552604753473,
            "auditor_fn_violation": 0.007849559521120396,
            "auditor_fp_violation": 0.018401876327590275,
            "ave_precision_score": 0.8039263543656936,
            "fpr": 0.15679824561403508,
            "logloss": 0.7217015150292613,
            "mae": 0.2899713391460801,
            "precision": 0.7145708582834331,
            "recall": 0.7682403433476395
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8486914574503169,
            "auditor_fn_violation": 0.008178726314084684,
            "auditor_fp_violation": 0.0307121003339795,
            "ave_precision_score": 0.8489310102817363,
            "fpr": 0.1350164654226125,
            "logloss": 0.6707720348914546,
            "mae": 0.26807969938697457,
            "precision": 0.7554671968190855,
            "recall": 0.7786885245901639
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.811936642294123,
            "auditor_fn_violation": 0.01190610646788646,
            "auditor_fp_violation": 0.017774958697191413,
            "ave_precision_score": 0.8125240530162243,
            "fpr": 0.16447368421052633,
            "logloss": 0.7513425104444935,
            "mae": 0.2825876333540767,
            "precision": 0.7137404580152672,
            "recall": 0.8025751072961373
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.858749979987117,
            "auditor_fn_violation": 0.012731460653938207,
            "auditor_fp_violation": 0.028796973164864424,
            "ave_precision_score": 0.858942807662819,
            "fpr": 0.14818880351262348,
            "logloss": 0.6924952908936158,
            "mae": 0.2582442234013793,
            "precision": 0.7452830188679245,
            "recall": 0.8094262295081968
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8149977505010934,
            "auditor_fn_violation": 0.019823902567577743,
            "auditor_fp_violation": 0.01645720242309811,
            "ave_precision_score": 0.8155783223684017,
            "fpr": 0.16885964912280702,
            "logloss": 0.7289241307157395,
            "mae": 0.28005581329067597,
            "precision": 0.7077798861480076,
            "recall": 0.8004291845493562
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8639202235204536,
            "auditor_fn_violation": 0.014602940382573645,
            "auditor_fp_violation": 0.034822617184763066,
            "ave_precision_score": 0.8641011588225511,
            "fpr": 0.14709110867178923,
            "logloss": 0.6692540789659487,
            "mae": 0.25667078683776334,
            "precision": 0.75139146567718,
            "recall": 0.8299180327868853
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8092444258569031,
            "auditor_fn_violation": 0.009251938860025602,
            "auditor_fp_violation": 0.01234413106757927,
            "ave_precision_score": 0.8097365971998703,
            "fpr": 0.13486842105263158,
            "logloss": 0.7163579172516155,
            "mae": 0.28518039880993346,
            "precision": 0.7377398720682303,
            "recall": 0.7424892703862661
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8570189204478686,
            "auditor_fn_violation": 0.010000719799895638,
            "auditor_fp_violation": 0.02345901031002744,
            "ave_precision_score": 0.8572314835759677,
            "fpr": 0.1163556531284303,
            "logloss": 0.664863904797481,
            "mae": 0.26519595993282846,
            "precision": 0.7773109243697479,
            "recall": 0.7581967213114754
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.822297345840566,
            "auditor_fn_violation": 0.010524903998192912,
            "auditor_fp_violation": 0.011736881441271337,
            "ave_precision_score": 0.8227340087279584,
            "fpr": 0.14035087719298245,
            "logloss": 0.6813547858525648,
            "mae": 0.2775014856258771,
            "precision": 0.7349896480331263,
            "recall": 0.7618025751072961
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8645911503625796,
            "auditor_fn_violation": 0.012758453150024291,
            "auditor_fp_violation": 0.024351698312975376,
            "ave_precision_score": 0.8647695758396025,
            "fpr": 0.12733260153677278,
            "logloss": 0.6485980115683242,
            "mae": 0.2610658224765428,
            "precision": 0.7665995975855131,
            "recall": 0.7807377049180327
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8160270233656448,
            "auditor_fn_violation": 0.016230893758000155,
            "auditor_fp_violation": 0.011547576901896003,
            "ave_precision_score": 0.8165618114978552,
            "fpr": 0.17214912280701755,
            "logloss": 0.7519490284056344,
            "mae": 0.28005536822848004,
            "precision": 0.7048872180451128,
            "recall": 0.8047210300429185
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8625010053311871,
            "auditor_fn_violation": 0.013143096219251051,
            "auditor_fp_violation": 0.02939382851567264,
            "ave_precision_score": 0.8626878144813535,
            "fpr": 0.1525795828759605,
            "logloss": 0.6932673739283896,
            "mae": 0.2559302676812149,
            "precision": 0.7454212454212454,
            "recall": 0.8340163934426229
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8166166479444253,
            "auditor_fn_violation": 0.013379075370830518,
            "auditor_fp_violation": 0.012956297694909927,
            "ave_precision_score": 0.8172072422901899,
            "fpr": 0.14912280701754385,
            "logloss": 0.7285753439540834,
            "mae": 0.27809321561870276,
            "precision": 0.7306930693069307,
            "recall": 0.7918454935622318
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8621098442846833,
            "auditor_fn_violation": 0.014571449137139878,
            "auditor_fp_violation": 0.030945652427774015,
            "ave_precision_score": 0.8622926768559316,
            "fpr": 0.1350164654226125,
            "logloss": 0.6759242609482761,
            "mae": 0.2578681304249461,
            "precision": 0.7652671755725191,
            "recall": 0.8217213114754098
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8042423164320172,
            "auditor_fn_violation": 0.010527256983660872,
            "auditor_fp_violation": 0.016924317520258046,
            "ave_precision_score": 0.8048311174423108,
            "fpr": 0.1600877192982456,
            "logloss": 0.8301695963906441,
            "mae": 0.28473421685989553,
            "precision": 0.7114624505928854,
            "recall": 0.7725321888412017
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8508758417950155,
            "auditor_fn_violation": 0.011026434651166981,
            "auditor_fp_violation": 0.02763959278894935,
            "ave_precision_score": 0.8510806272359956,
            "fpr": 0.13611416026344675,
            "logloss": 0.7647875109002934,
            "mae": 0.2624259297139761,
            "precision": 0.7549407114624506,
            "recall": 0.7827868852459017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 10102,
        "test": {
            "accuracy": 0.5131578947368421,
            "auc_prc": 0.694426205843616,
            "auditor_fn_violation": 0.0006282471199457873,
            "auditor_fp_violation": 0.0006957556447171843,
            "ave_precision_score": 0.6881086701540826,
            "fpr": 0.4857456140350877,
            "logloss": 4.097239099363982,
            "mae": 0.48552610902716525,
            "precision": 0.512114537444934,
            "recall": 0.9978540772532188
        },
        "train": {
            "accuracy": 0.5455543358946213,
            "auc_prc": 0.7565697253185427,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.002545717822360287,
            "ave_precision_score": 0.7512273784116871,
            "fpr": 0.4544456641053787,
            "logloss": 3.5642591964871793,
            "mae": 0.45197456805036607,
            "precision": 0.541019955654102,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8270447934974949,
            "auditor_fn_violation": 0.010734319704841507,
            "auditor_fp_violation": 0.01662683895838251,
            "ave_precision_score": 0.827384832409238,
            "fpr": 0.14364035087719298,
            "logloss": 0.7208541719937059,
            "mae": 0.2781663226560171,
            "precision": 0.7331975560081466,
            "recall": 0.7725321888412017
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8532228683075392,
            "auditor_fn_violation": 0.014040596714113474,
            "auditor_fp_violation": 0.02742420585800552,
            "ave_precision_score": 0.8534417036120574,
            "fpr": 0.13062568605927552,
            "logloss": 0.7009077796442758,
            "mae": 0.26298649364527293,
            "precision": 0.7605633802816901,
            "recall": 0.7745901639344263
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8149647275765921,
            "auditor_fn_violation": 0.01316730667871396,
            "auditor_fp_violation": 0.0051185980646683985,
            "ave_precision_score": 0.8158363519482756,
            "fpr": 0.06578947368421052,
            "logloss": 0.806496246567215,
            "mae": 0.2987570692262344,
            "precision": 0.8214285714285714,
            "recall": 0.592274678111588
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8595998368264527,
            "auditor_fn_violation": 0.008196721311475407,
            "auditor_fp_violation": 0.016154019820787693,
            "ave_precision_score": 0.8598019137666648,
            "fpr": 0.06256860592755215,
            "logloss": 0.7864169545831758,
            "mae": 0.2910106980262684,
            "precision": 0.8416666666666667,
            "recall": 0.6209016393442623
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8017804090467794,
            "auditor_fn_violation": 0.011195504856562009,
            "auditor_fp_violation": 0.011092754307292901,
            "ave_precision_score": 0.8023826499383905,
            "fpr": 0.12938596491228072,
            "logloss": 0.82344603475773,
            "mae": 0.2864395508435851,
            "precision": 0.739514348785872,
            "recall": 0.7188841201716738
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8515646011792531,
            "auditor_fn_violation": 0.013676198016951292,
            "auditor_fp_violation": 0.024725381663046612,
            "ave_precision_score": 0.8517705905681412,
            "fpr": 0.1119648737650933,
            "logloss": 0.7855466086663777,
            "mae": 0.26865549701689917,
            "precision": 0.7796976241900648,
            "recall": 0.7397540983606558
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.82306357791909,
            "auditor_fn_violation": 0.014997929372788198,
            "auditor_fp_violation": 0.01748731413736134,
            "ave_precision_score": 0.8235847715093287,
            "fpr": 0.17653508771929824,
            "logloss": 0.6977062322147857,
            "mae": 0.2787541551184133,
            "precision": 0.7045871559633028,
            "recall": 0.8240343347639485
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8667880123420226,
            "auditor_fn_violation": 0.01619549765165284,
            "auditor_fp_violation": 0.03141275661536306,
            "ave_precision_score": 0.8669689023032069,
            "fpr": 0.15148188803512624,
            "logloss": 0.65668126982211,
            "mae": 0.25732136275789075,
            "precision": 0.7509025270758123,
            "recall": 0.8524590163934426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8091967518063973,
            "auditor_fn_violation": 0.012597884195467209,
            "auditor_fp_violation": 0.020518645267878218,
            "ave_precision_score": 0.809795869430917,
            "fpr": 0.16228070175438597,
            "logloss": 0.7866464842631932,
            "mae": 0.28075080826551113,
            "precision": 0.7153846153846154,
            "recall": 0.7982832618025751
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.854605066830366,
            "auditor_fn_violation": 0.013226323082183153,
            "auditor_fp_violation": 0.03016714544845895,
            "ave_precision_score": 0.8548019388567115,
            "fpr": 0.14489571899012074,
            "logloss": 0.7367177521497424,
            "mae": 0.26119899647325073,
            "precision": 0.7480916030534351,
            "recall": 0.8032786885245902
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8121838265546003,
            "auditor_fn_violation": 0.009124877644755669,
            "auditor_fp_violation": 0.015631146251278427,
            "ave_precision_score": 0.812795873074371,
            "fpr": 0.16666666666666666,
            "logloss": 0.7243776137588047,
            "mae": 0.28214903981482403,
            "precision": 0.7099236641221374,
            "recall": 0.7982832618025751
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8622199327968086,
            "auditor_fn_violation": 0.013788666750643329,
            "auditor_fp_violation": 0.03040069754225347,
            "ave_precision_score": 0.8623958435471855,
            "fpr": 0.14928649835345773,
            "logloss": 0.6713940423733388,
            "mae": 0.25971148516465176,
            "precision": 0.7453183520599251,
            "recall": 0.8155737704918032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8166883931842751,
            "auditor_fn_violation": 0.01324260221368873,
            "auditor_fp_violation": 0.01954507906537645,
            "ave_precision_score": 0.8175365562115573,
            "fpr": 0.16337719298245615,
            "logloss": 0.6877600338112491,
            "mae": 0.280740927298879,
            "precision": 0.7140115163147792,
            "recall": 0.7982832618025751
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8637194880836745,
            "auditor_fn_violation": 0.01354123553652085,
            "auditor_fp_violation": 0.03161776345324936,
            "ave_precision_score": 0.8638968782599863,
            "fpr": 0.150384193194292,
            "logloss": 0.6478074076287766,
            "mae": 0.26148156827189706,
            "precision": 0.7467652495378928,
            "recall": 0.8278688524590164
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8180768865139915,
            "auditor_fn_violation": 0.0073389616745727,
            "auditor_fp_violation": 0.01764219966957753,
            "ave_precision_score": 0.818568985501934,
            "fpr": 0.1600877192982456,
            "logloss": 0.7103160662515893,
            "mae": 0.28227470871887556,
            "precision": 0.7176015473887815,
            "recall": 0.796137339055794
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8571252316230573,
            "auditor_fn_violation": 0.017985999892030022,
            "auditor_fp_violation": 0.026531517855057574,
            "ave_precision_score": 0.8573222247546246,
            "fpr": 0.14709110867178923,
            "logloss": 0.6830273101999397,
            "mae": 0.263635754682472,
            "precision": 0.7462121212121212,
            "recall": 0.8073770491803278
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.811758160159173,
            "auditor_fn_violation": 0.01190610646788646,
            "auditor_fp_violation": 0.017774958697191413,
            "ave_precision_score": 0.8123200767397405,
            "fpr": 0.16447368421052633,
            "logloss": 0.7571925246219537,
            "mae": 0.2826422653753517,
            "precision": 0.7137404580152672,
            "recall": 0.8025751072961373
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8581681158867925,
            "auditor_fn_violation": 0.012731460653938207,
            "auditor_fp_violation": 0.028205307860584974,
            "ave_precision_score": 0.8583620549324784,
            "fpr": 0.14928649835345773,
            "logloss": 0.6982029031744512,
            "mae": 0.2583070790080532,
            "precision": 0.743879472693032,
            "recall": 0.8094262295081968
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8160149534980805,
            "auditor_fn_violation": 0.016452074391988557,
            "auditor_fp_violation": 0.024331779561010142,
            "ave_precision_score": 0.8167074991394025,
            "fpr": 0.1787280701754386,
            "logloss": 0.7579155116442635,
            "mae": 0.2826553207090324,
            "precision": 0.6981481481481482,
            "recall": 0.8090128755364807
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8539176733625453,
            "auditor_fn_violation": 0.019830487124579372,
            "auditor_fp_violation": 0.0360967736075754,
            "ave_precision_score": 0.854147829984057,
            "fpr": 0.145993413830955,
            "logloss": 0.7059819586225452,
            "mae": 0.2620682767768022,
            "precision": 0.7532467532467533,
            "recall": 0.8319672131147541
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8057339638590435,
            "auditor_fn_violation": 0.015515586175739777,
            "auditor_fp_violation": 0.01301038470616003,
            "ave_precision_score": 0.8062850962985209,
            "fpr": 0.15679824561403508,
            "logloss": 0.8719619560948357,
            "mae": 0.2872793331202338,
            "precision": 0.7145708582834331,
            "recall": 0.7682403433476395
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.852172993045067,
            "auditor_fn_violation": 0.01287767000773785,
            "auditor_fp_violation": 0.030120435029700045,
            "ave_precision_score": 0.8523716530329057,
            "fpr": 0.13172338090010977,
            "logloss": 0.7893605784157302,
            "mae": 0.26432700208849463,
            "precision": 0.7623762376237624,
            "recall": 0.7889344262295082
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8047077816195086,
            "auditor_fn_violation": 0.007496611700926135,
            "auditor_fp_violation": 0.012508850601840926,
            "ave_precision_score": 0.8052958964901903,
            "fpr": 0.15570175438596492,
            "logloss": 0.7422523118929787,
            "mae": 0.28543964678864847,
            "precision": 0.7182539682539683,
            "recall": 0.776824034334764
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8571148440913918,
            "auditor_fn_violation": 0.014931349084954387,
            "auditor_fp_violation": 0.02891374921176169,
            "ave_precision_score": 0.857320368494674,
            "fpr": 0.132821075740944,
            "logloss": 0.6755343176705333,
            "mae": 0.26130880478883445,
            "precision": 0.7632093933463796,
            "recall": 0.7991803278688525
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.818655365308686,
            "auditor_fn_violation": 0.013092011143739181,
            "auditor_fp_violation": 0.01506814963417513,
            "ave_precision_score": 0.8191602153523543,
            "fpr": 0.15021929824561403,
            "logloss": 0.709433749720304,
            "mae": 0.28037069597632464,
            "precision": 0.7287128712871287,
            "recall": 0.7896995708154506
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8607169206259826,
            "auditor_fn_violation": 0.013937125479116807,
            "auditor_fp_violation": 0.03182536531440004,
            "ave_precision_score": 0.8609105962395847,
            "fpr": 0.141602634467618,
            "logloss": 0.6622829340683248,
            "mae": 0.25881302686044755,
            "precision": 0.7570621468926554,
            "recall": 0.8237704918032787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8072581565292618,
            "auditor_fn_violation": 0.011717867630449515,
            "auditor_fp_violation": 0.016929234521280784,
            "ave_precision_score": 0.8077848561263198,
            "fpr": 0.1787280701754386,
            "logloss": 0.8385502960944716,
            "mae": 0.28194663465914865,
            "precision": 0.6987060998151571,
            "recall": 0.8111587982832618
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8535131074316151,
            "auditor_fn_violation": 0.015979557682964135,
            "auditor_fp_violation": 0.028270183442194566,
            "ave_precision_score": 0.85371467050145,
            "fpr": 0.15587266739846323,
            "logloss": 0.7836446752948972,
            "mae": 0.2591443157273506,
            "precision": 0.7380073800738007,
            "recall": 0.819672131147541
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 10102,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8129413404459821,
            "auditor_fn_violation": 0.0074401400496950515,
            "auditor_fp_violation": 0.011751632444339553,
            "ave_precision_score": 0.8133115560803807,
            "fpr": 0.1337719298245614,
            "logloss": 0.7692363092213343,
            "mae": 0.2823254863341179,
            "precision": 0.7436974789915967,
            "recall": 0.759656652360515
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8431696022244977,
            "auditor_fn_violation": 0.010216659768584333,
            "auditor_fp_violation": 0.020775756254654828,
            "ave_precision_score": 0.8433946473997679,
            "fpr": 0.12294182217343579,
            "logloss": 0.756596354881691,
            "mae": 0.27085244099973166,
            "precision": 0.7676348547717843,
            "recall": 0.7581967213114754
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.806666003573687,
            "auditor_fn_violation": 0.010842557036367741,
            "auditor_fp_violation": 0.022136338604358437,
            "ave_precision_score": 0.8072902961157065,
            "fpr": 0.17982456140350878,
            "logloss": 0.8529483588249622,
            "mae": 0.2821911536266203,
            "precision": 0.6979742173112339,
            "recall": 0.8133047210300429
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8524468379596274,
            "auditor_fn_violation": 0.01018741789782441,
            "auditor_fp_violation": 0.031635928616100054,
            "ave_precision_score": 0.852645608339472,
            "fpr": 0.150384193194292,
            "logloss": 0.8023457845614751,
            "mae": 0.2607762470233746,
            "precision": 0.7462962962962963,
            "recall": 0.8258196721311475
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8119568164220816,
            "auditor_fn_violation": 0.01190610646788646,
            "auditor_fp_violation": 0.017693828180316267,
            "ave_precision_score": 0.8125188226760701,
            "fpr": 0.16337719298245615,
            "logloss": 0.7527936149366058,
            "mae": 0.28266988809196986,
            "precision": 0.7151051625239006,
            "recall": 0.8025751072961373
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8586527989948431,
            "auditor_fn_violation": 0.012731460653938207,
            "auditor_fp_violation": 0.028796973164864424,
            "ave_precision_score": 0.8588464359166599,
            "fpr": 0.14818880351262348,
            "logloss": 0.693581388080925,
            "mae": 0.25830596087959895,
            "precision": 0.7452830188679245,
            "recall": 0.8094262295081968
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8190181120970035,
            "auditor_fn_violation": 0.007863677433928165,
            "auditor_fp_violation": 0.014829675084572424,
            "ave_precision_score": 0.8194070648491331,
            "fpr": 0.14912280701754385,
            "logloss": 0.7127006768632856,
            "mae": 0.2827782928665807,
            "precision": 0.7246963562753036,
            "recall": 0.7682403433476395
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8517268926358501,
            "auditor_fn_violation": 0.012076892623850567,
            "auditor_fp_violation": 0.027608452509776754,
            "ave_precision_score": 0.8519544010118568,
            "fpr": 0.12733260153677278,
            "logloss": 0.6908198658201129,
            "mae": 0.26567421849608924,
            "precision": 0.7647058823529411,
            "recall": 0.7725409836065574
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8081989184867263,
            "auditor_fn_violation": 0.0077154393494465855,
            "auditor_fp_violation": 0.014013452914798209,
            "ave_precision_score": 0.8086874417909233,
            "fpr": 0.13596491228070176,
            "logloss": 0.7193784171104383,
            "mae": 0.28591636328124576,
            "precision": 0.7378435517970402,
            "recall": 0.7489270386266095
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8554787192944844,
            "auditor_fn_violation": 0.011075920893991478,
            "auditor_fp_violation": 0.024987479012749354,
            "ave_precision_score": 0.8556942973399722,
            "fpr": 0.11964873765093303,
            "logloss": 0.6698066698551859,
            "mae": 0.26636329436869477,
            "precision": 0.7733887733887734,
            "recall": 0.7622950819672131
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.827738192399512,
            "auditor_fn_violation": 0.009251938860025604,
            "auditor_fp_violation": 0.012444929588545353,
            "ave_precision_score": 0.8280746181897854,
            "fpr": 0.15021929824561403,
            "logloss": 0.6490121390874496,
            "mae": 0.27879822036960267,
            "precision": 0.7318982387475538,
            "recall": 0.8025751072961373
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8646910306094986,
            "auditor_fn_violation": 0.01263248816828922,
            "auditor_fp_violation": 0.026627533715839768,
            "ave_precision_score": 0.8648793611830756,
            "fpr": 0.14050493962678376,
            "logloss": 0.6209460535779368,
            "mae": 0.26108912296322945,
            "precision": 0.7557251908396947,
            "recall": 0.8114754098360656
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8355567907003872,
            "auditor_fn_violation": 0.01242846924177397,
            "auditor_fp_violation": 0.011382857367634334,
            "ave_precision_score": 0.8360050796685229,
            "fpr": 0.14035087719298245,
            "logloss": 0.6448805742657044,
            "mae": 0.2699815349100926,
            "precision": 0.7414141414141414,
            "recall": 0.7875536480686696
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8698677108116777,
            "auditor_fn_violation": 0.014321768548343565,
            "auditor_fp_violation": 0.02813264720918223,
            "ave_precision_score": 0.8700382017560324,
            "fpr": 0.12952799121844127,
            "logloss": 0.6304254341183514,
            "mae": 0.25806066048712695,
            "precision": 0.768172888015717,
            "recall": 0.8012295081967213
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8177021623629432,
            "auditor_fn_violation": 0.02009214291092539,
            "auditor_fp_violation": 0.01948607505310361,
            "ave_precision_score": 0.8180878717984951,
            "fpr": 0.09868421052631579,
            "logloss": 0.7610626172354922,
            "mae": 0.2857986310475297,
            "precision": 0.7788697788697788,
            "recall": 0.6802575107296137
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8578391871405687,
            "auditor_fn_violation": 0.011822713285706568,
            "auditor_fp_violation": 0.0244113838480562,
            "ave_precision_score": 0.8580253926714236,
            "fpr": 0.09440175631174534,
            "logloss": 0.7291670294187137,
            "mae": 0.2757202550467373,
            "precision": 0.7990654205607477,
            "recall": 0.7008196721311475
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8173667741037549,
            "auditor_fn_violation": 0.007651908741811613,
            "auditor_fp_violation": 0.020808748328219658,
            "ave_precision_score": 0.8177596176424644,
            "fpr": 0.13706140350877194,
            "logloss": 0.6962800379561159,
            "mae": 0.28239761048900197,
            "precision": 0.7368421052631579,
            "recall": 0.7510729613733905
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8438003867414271,
            "auditor_fn_violation": 0.010023213546634043,
            "auditor_fp_violation": 0.020368337602146604,
            "ave_precision_score": 0.8440380752141029,
            "fpr": 0.12733260153677278,
            "logloss": 0.7275900005336994,
            "mae": 0.2746774163936831,
            "precision": 0.7613168724279835,
            "recall": 0.7581967213114754
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 10102,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8152412944859491,
            "auditor_fn_violation": 0.013637903772306302,
            "auditor_fp_violation": 0.014330599480764698,
            "ave_precision_score": 0.8158073852582949,
            "fpr": 0.16776315789473684,
            "logloss": 0.7235193072436862,
            "mae": 0.2798144857395232,
            "precision": 0.7096774193548387,
            "recall": 0.8025751072961373
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.860723811337951,
            "auditor_fn_violation": 0.010725018444872326,
            "auditor_fp_violation": 0.029736371586571275,
            "ave_precision_score": 0.8609141491648733,
            "fpr": 0.15587266739846323,
            "logloss": 0.6777534893589364,
            "mae": 0.25917989697446847,
            "precision": 0.7380073800738007,
            "recall": 0.819672131147541
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8037383782248925,
            "auditor_fn_violation": 0.011000207062721184,
            "auditor_fp_violation": 0.012122866021556135,
            "ave_precision_score": 0.8043812893922391,
            "fpr": 0.15021929824561403,
            "logloss": 0.9031032473868431,
            "mae": 0.2792945604560998,
            "precision": 0.7237903225806451,
            "recall": 0.7703862660944206
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8430219471251875,
            "auditor_fn_violation": 0.01034262475031941,
            "auditor_fp_violation": 0.0275072466024658,
            "ave_precision_score": 0.8432491566962395,
            "fpr": 0.13611416026344675,
            "logloss": 0.8753943531490503,
            "mae": 0.2649733772749813,
            "precision": 0.7529880478087649,
            "recall": 0.7745901639344263
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8079688639062947,
            "auditor_fn_violation": 0.01744974023040434,
            "auditor_fp_violation": 0.014396978994571635,
            "ave_precision_score": 0.8088863440415142,
            "fpr": 0.18969298245614036,
            "logloss": 0.7780805540601472,
            "mae": 0.288687829399057,
            "precision": 0.6882882882882883,
            "recall": 0.8197424892703863
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.85757228112934,
            "auditor_fn_violation": 0.019083694732864268,
            "auditor_fp_violation": 0.03237032019992059,
            "ave_precision_score": 0.8577734416677905,
            "fpr": 0.15806805708013172,
            "logloss": 0.7078871758672538,
            "mae": 0.2604134153940166,
            "precision": 0.7410071942446043,
            "recall": 0.8442622950819673
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7998792528442723,
            "auditor_fn_violation": 0.005785991265717942,
            "auditor_fp_violation": 0.020449807253559934,
            "ave_precision_score": 0.8002084032402406,
            "fpr": 0.2576754385964912,
            "logloss": 1.112771001176069,
            "mae": 0.3115077357193769,
            "precision": 0.6460843373493976,
            "recall": 0.9206008583690987
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8447105064812366,
            "auditor_fn_violation": 0.007429684547695742,
            "auditor_fp_violation": 0.03154250777858225,
            "ave_precision_score": 0.8449325099649077,
            "fpr": 0.2349066959385291,
            "logloss": 1.0313832608566997,
            "mae": 0.2861537270646506,
            "precision": 0.6762481089258698,
            "recall": 0.9159836065573771
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8145767500425802,
            "auditor_fn_violation": 0.017623861155033515,
            "auditor_fp_violation": 0.01538775470065298,
            "ave_precision_score": 0.8151427684560237,
            "fpr": 0.16776315789473684,
            "logloss": 0.735424727006605,
            "mae": 0.2795086413542586,
            "precision": 0.7091254752851711,
            "recall": 0.8004291845493562
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8642454989760351,
            "auditor_fn_violation": 0.014710910366917995,
            "auditor_fp_violation": 0.03286596964341786,
            "ave_precision_score": 0.8644282388892797,
            "fpr": 0.145993413830955,
            "logloss": 0.6731985791868917,
            "mae": 0.2557559987831327,
            "precision": 0.7532467532467533,
            "recall": 0.8319672131147541
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8503933928713338,
            "auditor_fn_violation": 0.015176756268353285,
            "auditor_fp_violation": 0.027254936669026838,
            "ave_precision_score": 0.8506553963720904,
            "fpr": 0.15570175438596492,
            "logloss": 0.5913267366483116,
            "mae": 0.27684584600857515,
            "precision": 0.7305502846299811,
            "recall": 0.8261802575107297
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8690182285564946,
            "auditor_fn_violation": 0.018426877328102793,
            "auditor_fp_violation": 0.0314542769875932,
            "ave_precision_score": 0.8692588264718211,
            "fpr": 0.14709110867178923,
            "logloss": 0.5795167578213116,
            "mae": 0.26227126263479084,
            "precision": 0.7536764705882353,
            "recall": 0.8401639344262295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8150405712598071,
            "auditor_fn_violation": 0.009788419546720885,
            "auditor_fp_violation": 0.014330599480764698,
            "ave_precision_score": 0.8156072248309965,
            "fpr": 0.16776315789473684,
            "logloss": 0.7244189874850228,
            "mae": 0.28027326838648586,
            "precision": 0.7102272727272727,
            "recall": 0.8047210300429185
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8640460437190983,
            "auditor_fn_violation": 0.010725018444872326,
            "auditor_fp_violation": 0.029271862422246615,
            "ave_precision_score": 0.8642231586060882,
            "fpr": 0.15477497255762898,
            "logloss": 0.6702838315455375,
            "mae": 0.2581136607203222,
            "precision": 0.7393715341959335,
            "recall": 0.819672131147541
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8192557234648061,
            "auditor_fn_violation": 0.018228578420299683,
            "auditor_fp_violation": 0.02548481630084179,
            "ave_precision_score": 0.8197808816512444,
            "fpr": 0.14912280701754385,
            "logloss": 0.7412908387114847,
            "mae": 0.277046079781025,
            "precision": 0.7252525252525253,
            "recall": 0.7703862660944206
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8573669001639137,
            "auditor_fn_violation": 0.022617462345467965,
            "auditor_fp_violation": 0.0322587341995521,
            "ave_precision_score": 0.8575783583818645,
            "fpr": 0.14270032930845225,
            "logloss": 0.7014841900118308,
            "mae": 0.2603759147923669,
            "precision": 0.7533206831119544,
            "recall": 0.8135245901639344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.7168315704364357,
            "auditor_fn_violation": 0.010870792861983287,
            "auditor_fp_violation": 0.019972858154354498,
            "ave_precision_score": 0.7176653713363023,
            "fpr": 0.20065789473684212,
            "logloss": 1.1569129081854488,
            "mae": 0.3398549791557007,
            "precision": 0.6579439252336449,
            "recall": 0.7553648068669528
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7634147781143391,
            "auditor_fn_violation": 0.018102967375069733,
            "auditor_fp_violation": 0.030475953216920593,
            "ave_precision_score": 0.7639862220010512,
            "fpr": 0.1734357848518112,
            "logloss": 1.036646786420627,
            "mae": 0.30910822297513957,
            "precision": 0.6996197718631179,
            "recall": 0.7540983606557377
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8113336168906574,
            "auditor_fn_violation": 0.007960149838114605,
            "auditor_fp_violation": 0.01197535599087405,
            "ave_precision_score": 0.8117651290150193,
            "fpr": 0.14802631578947367,
            "logloss": 0.7256238334040347,
            "mae": 0.28518342284563575,
            "precision": 0.725609756097561,
            "recall": 0.7660944206008584
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8548125882932721,
            "auditor_fn_violation": 0.014476975400838574,
            "auditor_fp_violation": 0.027092042880164424,
            "ave_precision_score": 0.8550277672806341,
            "fpr": 0.12843029637760703,
            "logloss": 0.6768828349393939,
            "mae": 0.26434341935330735,
            "precision": 0.7669322709163346,
            "recall": 0.7889344262295082
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8107740879468145,
            "auditor_fn_violation": 0.015228521948648448,
            "auditor_fp_violation": 0.012533435606954618,
            "ave_precision_score": 0.8113894142289254,
            "fpr": 0.15679824561403508,
            "logloss": 0.7502158817776485,
            "mae": 0.282880881122066,
            "precision": 0.718503937007874,
            "recall": 0.7832618025751072
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.85590935702565,
            "auditor_fn_violation": 0.015358730272984114,
            "auditor_fp_violation": 0.032393675409300056,
            "ave_precision_score": 0.8561119539304156,
            "fpr": 0.13721185510428102,
            "logloss": 0.684809978649732,
            "mae": 0.26176391234989654,
            "precision": 0.7563352826510721,
            "recall": 0.7950819672131147
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7733314749336757,
            "auditor_fn_violation": 0.011506098938332958,
            "auditor_fp_violation": 0.01884194791912517,
            "ave_precision_score": 0.7745192703930638,
            "fpr": 0.1875,
            "logloss": 0.843777562806615,
            "mae": 0.29062010255681064,
            "precision": 0.6929982046678635,
            "recall": 0.8283261802575107
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8385225493683622,
            "auditor_fn_violation": 0.007485918914541756,
            "auditor_fp_violation": 0.031114328939958955,
            "ave_precision_score": 0.8388722875781297,
            "fpr": 0.17233809001097694,
            "logloss": 0.7521160632429708,
            "mae": 0.26817228660544085,
            "precision": 0.7260034904013961,
            "recall": 0.8524590163934426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8064295331560397,
            "auditor_fn_violation": 0.009011934342293509,
            "auditor_fp_violation": 0.012307253559908745,
            "ave_precision_score": 0.8069479746863955,
            "fpr": 0.13596491228070176,
            "logloss": 0.7242061866708444,
            "mae": 0.28678369970498113,
            "precision": 0.7327586206896551,
            "recall": 0.7296137339055794
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8550733216558224,
            "auditor_fn_violation": 0.010119936657609187,
            "auditor_fp_violation": 0.024006560218812364,
            "ave_precision_score": 0.8552896224651785,
            "fpr": 0.1163556531284303,
            "logloss": 0.6734758011878813,
            "mae": 0.2668099364971038,
            "precision": 0.7758985200845666,
            "recall": 0.7520491803278688
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8149886370969641,
            "auditor_fn_violation": 0.019823902567577743,
            "auditor_fp_violation": 0.01645720242309811,
            "ave_precision_score": 0.8155692192803171,
            "fpr": 0.16885964912280702,
            "logloss": 0.7289181531609316,
            "mae": 0.28005510053426996,
            "precision": 0.7077798861480076,
            "recall": 0.8004291845493562
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.863935160511188,
            "auditor_fn_violation": 0.014602940382573645,
            "auditor_fp_violation": 0.034822617184763066,
            "ave_precision_score": 0.8641160755050556,
            "fpr": 0.14709110867178923,
            "logloss": 0.6692371841511894,
            "mae": 0.25666971996366095,
            "precision": 0.75139146567718,
            "recall": 0.8299180327868853
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8145866117478237,
            "auditor_fn_violation": 0.016132068368345756,
            "auditor_fp_violation": 0.016167099362756673,
            "ave_precision_score": 0.8151499333815683,
            "fpr": 0.16228070175438597,
            "logloss": 0.7309800286974701,
            "mae": 0.2799737067663325,
            "precision": 0.7148362235067437,
            "recall": 0.796137339055794
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8642320580804296,
            "auditor_fn_violation": 0.015558924618955932,
            "auditor_fp_violation": 0.03133490591743155,
            "ave_precision_score": 0.8644128938818163,
            "fpr": 0.14050493962678376,
            "logloss": 0.6674618912544156,
            "mae": 0.25593532034041466,
            "precision": 0.7598499061913696,
            "recall": 0.8299180327868853
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8073576094689296,
            "auditor_fn_violation": 0.00883781341766433,
            "auditor_fp_violation": 0.016845645503894266,
            "ave_precision_score": 0.8079319910691194,
            "fpr": 0.16447368421052633,
            "logloss": 0.8008949000592128,
            "mae": 0.2815917665058371,
            "precision": 0.715370018975332,
            "recall": 0.8090128755364807
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8541530339663564,
            "auditor_fn_violation": 0.007512911410627848,
            "auditor_fp_violation": 0.029204391817372645,
            "ave_precision_score": 0.8543512245474312,
            "fpr": 0.14709110867178923,
            "logloss": 0.747213407415625,
            "mae": 0.26674768230837737,
            "precision": 0.7428023032629558,
            "recall": 0.7930327868852459
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.8448377614297617,
            "auditor_fn_violation": 0.015242639861456216,
            "auditor_fp_violation": 0.013959365903548113,
            "ave_precision_score": 0.8451779353931139,
            "fpr": 0.10087719298245613,
            "logloss": 0.488903046664871,
            "mae": 0.31868730511173216,
            "precision": 0.7946428571428571,
            "recall": 0.7639484978540773
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8639039526471223,
            "auditor_fn_violation": 0.0020469309531950136,
            "auditor_fp_violation": 0.012720804042008243,
            "ave_precision_score": 0.8640896559842463,
            "fpr": 0.10098792535675083,
            "logloss": 0.48634691635127497,
            "mae": 0.31921192862627096,
            "precision": 0.8004338394793926,
            "recall": 0.7561475409836066
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8105613108718743,
            "auditor_fn_violation": 0.015228521948648448,
            "auditor_fp_violation": 0.012533435606954618,
            "ave_precision_score": 0.8111774546997694,
            "fpr": 0.15679824561403508,
            "logloss": 0.7517774389596931,
            "mae": 0.2829035838371975,
            "precision": 0.718503937007874,
            "recall": 0.7832618025751072
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8557361168146125,
            "auditor_fn_violation": 0.015358730272984114,
            "auditor_fp_violation": 0.032393675409300056,
            "ave_precision_score": 0.8559390297561235,
            "fpr": 0.13721185510428102,
            "logloss": 0.6863590563671046,
            "mae": 0.26177445256067083,
            "precision": 0.7563352826510721,
            "recall": 0.7950819672131147
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8046280735326098,
            "auditor_fn_violation": 0.01071549582109781,
            "auditor_fp_violation": 0.013359491778774294,
            "ave_precision_score": 0.8051930338051421,
            "fpr": 0.16666666666666666,
            "logloss": 0.9080699457593807,
            "mae": 0.2821500471485869,
            "precision": 0.7088122605363985,
            "recall": 0.7939914163090128
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8467671680477196,
            "auditor_fn_violation": 0.00970155296827482,
            "auditor_fp_violation": 0.031781249918905526,
            "ave_precision_score": 0.8469803896493557,
            "fpr": 0.14709110867178923,
            "logloss": 0.8672579150032088,
            "mae": 0.2620294877575075,
            "precision": 0.7466918714555766,
            "recall": 0.8094262295081968
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8123015204736675,
            "auditor_fn_violation": 0.009124877644755669,
            "auditor_fp_violation": 0.015631146251278427,
            "ave_precision_score": 0.8129133782070403,
            "fpr": 0.16666666666666666,
            "logloss": 0.723431195570238,
            "mae": 0.28211706750170473,
            "precision": 0.7099236641221374,
            "recall": 0.7982832618025751
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8622442087782008,
            "auditor_fn_violation": 0.013788666750643329,
            "auditor_fp_violation": 0.03040069754225347,
            "ave_precision_score": 0.8624202680522869,
            "fpr": 0.14928649835345773,
            "logloss": 0.6702495801332469,
            "mae": 0.25968790034962924,
            "precision": 0.7453183520599251,
            "recall": 0.8155737704918032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8221002156126219,
            "auditor_fn_violation": 0.014692041261953165,
            "auditor_fp_violation": 0.020557981276060112,
            "ave_precision_score": 0.8226172484957208,
            "fpr": 0.20175438596491227,
            "logloss": 0.75377566253444,
            "mae": 0.2873103356298134,
            "precision": 0.688135593220339,
            "recall": 0.871244635193133
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8655336279162791,
            "auditor_fn_violation": 0.019218657213294705,
            "auditor_fp_violation": 0.032427410711737034,
            "ave_precision_score": 0.8657251703878606,
            "fpr": 0.1690450054884742,
            "logloss": 0.6962961556032591,
            "mae": 0.259893694041746,
            "precision": 0.7372013651877133,
            "recall": 0.8852459016393442
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8147941510608583,
            "auditor_fn_violation": 0.009976658384157823,
            "auditor_fp_violation": 0.01478296357485643,
            "ave_precision_score": 0.815197057495026,
            "fpr": 0.14583333333333334,
            "logloss": 0.7017990870714833,
            "mae": 0.2835774400712831,
            "precision": 0.7291242362525459,
            "recall": 0.7682403433476395
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8501795550935167,
            "auditor_fn_violation": 0.011087167767360673,
            "auditor_fp_violation": 0.022670123237654827,
            "ave_precision_score": 0.8504075896909812,
            "fpr": 0.12733260153677278,
            "logloss": 0.6906999028375972,
            "mae": 0.2715141449416467,
            "precision": 0.7637474541751528,
            "recall": 0.7684426229508197
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.7209062097568423,
            "auditor_fn_violation": 0.013727317220088856,
            "auditor_fp_violation": 0.022421524663677136,
            "ave_precision_score": 0.7225572637131699,
            "fpr": 0.1875,
            "logloss": 1.0267334738500054,
            "mae": 0.3393860414769067,
            "precision": 0.6653620352250489,
            "recall": 0.7296137339055794
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7705096840883475,
            "auditor_fn_violation": 0.01369419301434202,
            "auditor_fp_violation": 0.025099065013117847,
            "ave_precision_score": 0.7710440647894294,
            "fpr": 0.15697036223929747,
            "logloss": 0.9439099546989288,
            "mae": 0.3062115799288758,
            "precision": 0.717391304347826,
            "recall": 0.7438524590163934
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8105488917238424,
            "auditor_fn_violation": 0.015228521948648448,
            "auditor_fp_violation": 0.012533435606954618,
            "ave_precision_score": 0.8111650484605367,
            "fpr": 0.15679824561403508,
            "logloss": 0.7518223922096896,
            "mae": 0.2829120133089893,
            "precision": 0.718503937007874,
            "recall": 0.7832618025751072
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8557117215497833,
            "auditor_fn_violation": 0.016296719511975668,
            "auditor_fp_violation": 0.032393675409300056,
            "ave_precision_score": 0.8559145846893846,
            "fpr": 0.13721185510428102,
            "logloss": 0.6864097607809945,
            "mae": 0.2617941436341698,
            "precision": 0.7568093385214008,
            "recall": 0.7971311475409836
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8214905083910725,
            "auditor_fn_violation": 0.011360213839319334,
            "auditor_fp_violation": 0.014455983006844475,
            "ave_precision_score": 0.8218807868212882,
            "fpr": 0.15460526315789475,
            "logloss": 0.7215041743655821,
            "mae": 0.28341701622095644,
            "precision": 0.7229862475442044,
            "recall": 0.7896995708154506
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8518588026503009,
            "auditor_fn_violation": 0.012684223785787556,
            "auditor_fp_violation": 0.02716470353156716,
            "ave_precision_score": 0.852059094462503,
            "fpr": 0.1394072447859495,
            "logloss": 0.6942206582693676,
            "mae": 0.2680092477241173,
            "precision": 0.748015873015873,
            "recall": 0.7725409836065574
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8065056642295594,
            "auditor_fn_violation": 0.008654280551163315,
            "auditor_fp_violation": 0.014829675084572424,
            "ave_precision_score": 0.8069843764148833,
            "fpr": 0.14912280701754385,
            "logloss": 0.8100962822310094,
            "mae": 0.2854568589092519,
            "precision": 0.7224489795918367,
            "recall": 0.759656652360515
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8449879837303933,
            "auditor_fn_violation": 0.009589084234582787,
            "auditor_fp_violation": 0.028566016094334283,
            "ave_precision_score": 0.8452035017534661,
            "fpr": 0.13611416026344675,
            "logloss": 0.7806630247510108,
            "mae": 0.26969629493685165,
            "precision": 0.7505030181086519,
            "recall": 0.764344262295082
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 10102,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.721708196137589,
            "auditor_fn_violation": 0.013873202319102483,
            "auditor_fp_violation": 0.021462709464243574,
            "ave_precision_score": 0.7234263013687514,
            "fpr": 0.20175438596491227,
            "logloss": 0.9983473664213903,
            "mae": 0.3398124369796193,
            "precision": 0.6579925650557621,
            "recall": 0.759656652360515
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7686861092003606,
            "auditor_fn_violation": 0.015235014665922877,
            "auditor_fp_violation": 0.025869786922639765,
            "ave_precision_score": 0.7692936116360176,
            "fpr": 0.16465422612513722,
            "logloss": 0.9223245454993321,
            "mae": 0.31101029040748124,
            "precision": 0.7142857142857143,
            "recall": 0.7684426229508197
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 10102,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8106245015860549,
            "auditor_fn_violation": 0.01655325276711091,
            "auditor_fp_violation": 0.01562622925025569,
            "ave_precision_score": 0.8112180731133017,
            "fpr": 0.1600877192982456,
            "logloss": 0.7375037492739172,
            "mae": 0.282272107528303,
            "precision": 0.7153996101364523,
            "recall": 0.7875536480686696
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8608046493932364,
            "auditor_fn_violation": 0.010875726548019656,
            "auditor_fp_violation": 0.031002742939590454,
            "ave_precision_score": 0.8609921052820688,
            "fpr": 0.14050493962678376,
            "logloss": 0.6737382860395236,
            "mae": 0.2581855486851669,
            "precision": 0.7561904761904762,
            "recall": 0.8135245901639344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8102020264561167,
            "auditor_fn_violation": 0.012644943904826442,
            "auditor_fp_violation": 0.01265881913303438,
            "ave_precision_score": 0.8107795990582511,
            "fpr": 0.1611842105263158,
            "logloss": 0.7286357229876814,
            "mae": 0.28205428943928124,
            "precision": 0.7151162790697675,
            "recall": 0.7918454935622318
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8618996386723554,
            "auditor_fn_violation": 0.010925212790844149,
            "auditor_fp_violation": 0.030994957869797307,
            "ave_precision_score": 0.8620848739307103,
            "fpr": 0.14270032930845225,
            "logloss": 0.6649735974035414,
            "mae": 0.258624209772781,
            "precision": 0.7542533081285444,
            "recall": 0.8176229508196722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8156523095643361,
            "auditor_fn_violation": 0.019826255553045704,
            "auditor_fp_violation": 0.01192864448115806,
            "ave_precision_score": 0.8162527510732177,
            "fpr": 0.13596491228070176,
            "logloss": 0.7304818699352813,
            "mae": 0.27873805234326116,
            "precision": 0.7394957983193278,
            "recall": 0.7553648068669528
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8573490159276091,
            "auditor_fn_violation": 0.015354231523636429,
            "auditor_fp_violation": 0.029520984655627438,
            "ave_precision_score": 0.8575404776920955,
            "fpr": 0.12403951701427003,
            "logloss": 0.6817788699906538,
            "mae": 0.26224919236616706,
            "precision": 0.7703252032520326,
            "recall": 0.7766393442622951
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 10102,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8155511238645331,
            "auditor_fn_violation": 0.017414445448384917,
            "auditor_fp_violation": 0.011097671308315632,
            "ave_precision_score": 0.8170583701770918,
            "fpr": 0.15021929824561403,
            "logloss": 0.7364603115950388,
            "mae": 0.2735061001418848,
            "precision": 0.7313725490196078,
            "recall": 0.8004291845493562
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8648608189118724,
            "auditor_fn_violation": 0.017052509402386142,
            "auditor_fp_violation": 0.030789951031911008,
            "ave_precision_score": 0.8650352806575381,
            "fpr": 0.1394072447859495,
            "logloss": 0.6953546633679023,
            "mae": 0.2551040095429977,
            "precision": 0.7612781954887218,
            "recall": 0.8299180327868853
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.6724540872440794,
            "auditor_fn_violation": 0.026906207827260466,
            "auditor_fp_violation": 0.013553026710921455,
            "ave_precision_score": 0.6689537907528957,
            "fpr": 0.13925438596491227,
            "logloss": 3.737201602331345,
            "mae": 0.35019639125535085,
            "precision": 0.6939759036144578,
            "recall": 0.6153846153846154
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.7190800431688955,
            "auditor_fn_violation": 0.03697831262168377,
            "auditor_fp_violation": 0.014683282753276947,
            "ave_precision_score": 0.7175911319249518,
            "fpr": 0.1207464324917673,
            "logloss": 3.5001108533397804,
            "mae": 0.33602465990833236,
            "precision": 0.7380952380952381,
            "recall": 0.6378600823045267
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7667386712975944,
            "auditor_fn_violation": 0.007403658719448199,
            "auditor_fp_violation": 0.016565908013276436,
            "ave_precision_score": 0.7696895629602325,
            "fpr": 0.09868421052631579,
            "logloss": 4.941181890007979,
            "mae": 0.33946057865263407,
            "precision": 0.7527472527472527,
            "recall": 0.5854700854700855
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.7751991568825936,
            "auditor_fn_violation": 0.008944180184575353,
            "auditor_fp_violation": 0.013376380189836638,
            "ave_precision_score": 0.7763353320761712,
            "fpr": 0.09440175631174534,
            "logloss": 4.863471296970577,
            "mae": 0.33569260616273305,
            "precision": 0.7712765957446809,
            "recall": 0.5967078189300411
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 9271,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.6695758852102388,
            "auditor_fn_violation": 0.011358524516419256,
            "auditor_fp_violation": 0.013355460723881774,
            "ave_precision_score": 0.6558755984225981,
            "fpr": 0.18859649122807018,
            "logloss": 3.367893750541534,
            "mae": 0.3275478924340958,
            "precision": 0.6772983114446529,
            "recall": 0.7713675213675214
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.6927175457970203,
            "auditor_fn_violation": 0.015074105694913112,
            "auditor_fp_violation": 0.02586943888422549,
            "ave_precision_score": 0.6818807752499481,
            "fpr": 0.1778265642151482,
            "logloss": 3.2652719833884207,
            "mae": 0.32763604108947053,
            "precision": 0.6925996204933587,
            "recall": 0.7510288065843621
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.6681248503344587,
            "auditor_fn_violation": 0.03027534113060429,
            "auditor_fp_violation": 0.016439959696538647,
            "ave_precision_score": 0.6651523545228369,
            "fpr": 0.13486842105263158,
            "logloss": 3.8140284442694816,
            "mae": 0.3553180967289408,
            "precision": 0.6992665036674817,
            "recall": 0.6111111111111112
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7160488016502382,
            "auditor_fn_violation": 0.03885523528162874,
            "auditor_fp_violation": 0.007709691999741723,
            "ave_precision_score": 0.714598332916541,
            "fpr": 0.1207464324917673,
            "logloss": 3.6190592100381926,
            "mae": 0.34009663219383013,
            "precision": 0.7399527186761229,
            "recall": 0.6440329218106996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.694915845564174,
            "auditor_fn_violation": 0.013134465437097023,
            "auditor_fp_violation": 0.021514935988620205,
            "ave_precision_score": 0.6761782140384096,
            "fpr": 0.17763157894736842,
            "logloss": 3.603746363445818,
            "mae": 0.3263333115460386,
            "precision": 0.6792079207920793,
            "recall": 0.7329059829059829
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7083733596586193,
            "auditor_fn_violation": 0.007927796072691791,
            "auditor_fp_violation": 0.023426099309097954,
            "ave_precision_score": 0.6911392433370553,
            "fpr": 0.17233809001097694,
            "logloss": 3.708414137654665,
            "mae": 0.3249017583837073,
            "precision": 0.693359375,
            "recall": 0.7304526748971193
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6107456140350878,
            "auc_prc": 0.6923759462190391,
            "auditor_fn_violation": 0.021381578947368425,
            "auditor_fp_violation": 0.014901414572467204,
            "ave_precision_score": 0.6931673016210931,
            "fpr": 0.13267543859649122,
            "logloss": 3.426515282982991,
            "mae": 0.44047675892892235,
            "precision": 0.6591549295774648,
            "recall": 0.5
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.7199984033656027,
            "auditor_fn_violation": 0.03177668460019967,
            "auditor_fp_violation": 0.01444308129398851,
            "ave_precision_score": 0.7211841158995292,
            "fpr": 0.10647639956092206,
            "logloss": 3.5113486596819388,
            "mae": 0.430379036189991,
            "precision": 0.7275280898876404,
            "recall": 0.5329218106995884
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.680334446303811,
            "auditor_fn_violation": 0.013980263157894742,
            "auditor_fp_violation": 0.01568180022127392,
            "ave_precision_score": 0.6649502211416034,
            "fpr": 0.18640350877192982,
            "logloss": 3.3734009313637507,
            "mae": 0.32813592067656683,
            "precision": 0.6737044145873321,
            "recall": 0.75
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7048953329832764,
            "auditor_fn_violation": 0.017039114977888004,
            "auditor_fp_violation": 0.021770517207980886,
            "ave_precision_score": 0.6898506712526246,
            "fpr": 0.1690450054884742,
            "logloss": 3.3456353015923073,
            "mae": 0.3245727480724805,
            "precision": 0.7027027027027027,
            "recall": 0.7489711934156379
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.673817951477237,
            "auditor_fn_violation": 0.030514319988004208,
            "auditor_fp_violation": 0.014797692429271385,
            "ave_precision_score": 0.6703230854179731,
            "fpr": 0.1337719298245614,
            "logloss": 3.625902665414153,
            "mae": 0.3495790820102302,
            "precision": 0.7038834951456311,
            "recall": 0.6196581196581197
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7200869134971405,
            "auditor_fn_violation": 0.0332560881408302,
            "auditor_fp_violation": 0.012932136630722544,
            "ave_precision_score": 0.7186475650239532,
            "fpr": 0.1141602634467618,
            "logloss": 3.3911370360729762,
            "mae": 0.33545135558011085,
            "precision": 0.7463414634146341,
            "recall": 0.6296296296296297
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 9271,
        "test": {
            "accuracy": 0.5142543859649122,
            "auc_prc": 0.7469076050027876,
            "auditor_fn_violation": 0.004449224021592444,
            "auditor_fp_violation": 0.00332898688161848,
            "ave_precision_score": 0.5125599449944511,
            "fpr": 0.4692982456140351,
            "logloss": 16.707354719753205,
            "mae": 0.4873771482300965,
            "precision": 0.5141884222474461,
            "recall": 0.967948717948718
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.7613286385196343,
            "auditor_fn_violation": 0.0052987491699529755,
            "auditor_fp_violation": 0.005837153741848002,
            "ave_precision_score": 0.5369844840081652,
            "fpr": 0.446761800219539,
            "logloss": 15.791288726415912,
            "mae": 0.4613455763147941,
            "precision": 0.5380249716231555,
            "recall": 0.9753086419753086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.7267342512798229,
            "auditor_fn_violation": 0.02938034188034188,
            "auditor_fp_violation": 0.01757596412201676,
            "ave_precision_score": 0.7313766493390179,
            "fpr": 0.10635964912280702,
            "logloss": 5.765498112736315,
            "mae": 0.36116420155871104,
            "precision": 0.7188405797101449,
            "recall": 0.5299145299145299
        },
        "train": {
            "accuracy": 0.6564215148188803,
            "auc_prc": 0.7513900814784178,
            "auditor_fn_violation": 0.038369629539284376,
            "auditor_fp_violation": 0.0074281655582101105,
            "ave_precision_score": 0.7535714488534153,
            "fpr": 0.09769484083424808,
            "logloss": 5.6257921750003765,
            "mae": 0.3518237046795034,
            "precision": 0.7464387464387464,
            "recall": 0.5390946502057613
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.6703088742733259,
            "auditor_fn_violation": 0.01423329959514171,
            "auditor_fp_violation": 0.01827485380116959,
            "ave_precision_score": 0.6584311170265491,
            "fpr": 0.16228070175438597,
            "logloss": 3.2706292259571548,
            "mae": 0.3250464847895697,
            "precision": 0.6923076923076923,
            "recall": 0.7115384615384616
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.6886837311580809,
            "auditor_fn_violation": 0.013343994073351313,
            "auditor_fp_violation": 0.026125137211855106,
            "ave_precision_score": 0.6794709755689665,
            "fpr": 0.16136114160263446,
            "logloss": 3.1974210043506233,
            "mae": 0.3288807539202432,
            "precision": 0.704225352112676,
            "recall": 0.720164609053498
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 9271,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.7491393092364396,
            "auditor_fn_violation": 0.027805892937471897,
            "auditor_fp_violation": 0.01468409198672357,
            "ave_precision_score": 0.7499257300794164,
            "fpr": 0.1118421052631579,
            "logloss": 3.473593212752229,
            "mae": 0.34814607347648535,
            "precision": 0.7243243243243244,
            "recall": 0.5726495726495726
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7760352119167344,
            "auditor_fn_violation": 0.03671179412123429,
            "auditor_fp_violation": 0.010669593852908896,
            "ave_precision_score": 0.7766210747222531,
            "fpr": 0.09440175631174534,
            "logloss": 3.485525215121438,
            "mae": 0.33290414237556953,
            "precision": 0.7700534759358288,
            "recall": 0.5925925925925926
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6513157894736842,
            "auc_prc": 0.6803431122106101,
            "auditor_fn_violation": 0.026179899535162695,
            "auditor_fp_violation": 0.022216295242611036,
            "ave_precision_score": 0.6811507438674813,
            "fpr": 0.12280701754385964,
            "logloss": 3.674796453510286,
            "mae": 0.3694208171319509,
            "precision": 0.7005347593582888,
            "recall": 0.5598290598290598
        },
        "train": {
            "accuracy": 0.6531284302963776,
            "auc_prc": 0.6963044694703158,
            "auditor_fn_violation": 0.02729781861383276,
            "auditor_fp_violation": 0.007534060825208243,
            "ave_precision_score": 0.6970401637744741,
            "fpr": 0.11745334796926454,
            "logloss": 3.733132060506807,
            "mae": 0.3597656173704172,
            "precision": 0.7213541666666666,
            "recall": 0.5699588477366255
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.6889948864754745,
            "auditor_fn_violation": 0.0096622432148748,
            "auditor_fp_violation": 0.01824768847795164,
            "ave_precision_score": 0.6847637585491793,
            "fpr": 0.14802631578947367,
            "logloss": 3.1208476850924267,
            "mae": 0.3323206629132054,
            "precision": 0.6986607142857143,
            "recall": 0.6688034188034188
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7242152470455894,
            "auditor_fn_violation": 0.009919908931983573,
            "auditor_fp_violation": 0.023340866533221413,
            "ave_precision_score": 0.7209021028785099,
            "fpr": 0.13391877058177826,
            "logloss": 2.9368616312863938,
            "mae": 0.3205684647906261,
            "precision": 0.7318681318681318,
            "recall": 0.6851851851851852
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.6726479990387125,
            "auditor_fn_violation": 0.03207939721097616,
            "auditor_fp_violation": 0.013365339023233771,
            "ave_precision_score": 0.6688222281994987,
            "fpr": 0.12828947368421054,
            "logloss": 3.7911550248268164,
            "mae": 0.3491555687988161,
            "precision": 0.706766917293233,
            "recall": 0.6025641025641025
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7166932706886895,
            "auditor_fn_violation": 0.03213806561775827,
            "auditor_fp_violation": 0.01486407955059082,
            "ave_precision_score": 0.7143649273328063,
            "fpr": 0.11525795828759605,
            "logloss": 3.571693897935679,
            "mae": 0.3382983353539836,
            "precision": 0.7388059701492538,
            "recall": 0.6111111111111112
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7542465702222317,
            "auditor_fn_violation": 0.013387501874343982,
            "auditor_fp_violation": 0.015158250355618777,
            "ave_precision_score": 0.757213370761463,
            "fpr": 0.08223684210526316,
            "logloss": 5.155952384227645,
            "mae": 0.3319798011148062,
            "precision": 0.7699386503067485,
            "recall": 0.5363247863247863
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.7611353660523218,
            "auditor_fn_violation": 0.00227670041061919,
            "auditor_fp_violation": 0.017526958093885193,
            "ave_precision_score": 0.7622834979459324,
            "fpr": 0.0845225027442371,
            "logloss": 5.138157787496022,
            "mae": 0.33629053570587486,
            "precision": 0.7761627906976745,
            "recall": 0.5493827160493827
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.6725920002343776,
            "auditor_fn_violation": 0.026906207827260466,
            "auditor_fp_violation": 0.015079223960802912,
            "ave_precision_score": 0.669085577877394,
            "fpr": 0.13706140350877194,
            "logloss": 3.7298042127863402,
            "mae": 0.34986905043066197,
            "precision": 0.6973365617433414,
            "recall": 0.6153846153846154
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.7192956453815628,
            "auditor_fn_violation": 0.03697831262168377,
            "auditor_fp_violation": 0.014683282753276947,
            "ave_precision_score": 0.7177808733364907,
            "fpr": 0.1207464324917673,
            "logloss": 3.492514382345235,
            "mae": 0.3358272776879502,
            "precision": 0.7380952380952381,
            "recall": 0.6378600823045267
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.6711825834532914,
            "auditor_fn_violation": 0.01639113810166442,
            "auditor_fp_violation": 0.022255808440018968,
            "ave_precision_score": 0.6479538741590918,
            "fpr": 0.19736842105263158,
            "logloss": 3.9295420220528516,
            "mae": 0.33233953741519034,
            "precision": 0.6616541353383458,
            "recall": 0.7521367521367521
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6960677873354602,
            "auditor_fn_violation": 0.011243466908791949,
            "auditor_fp_violation": 0.027656744366242658,
            "ave_precision_score": 0.6770488263160396,
            "fpr": 0.18331503841931943,
            "logloss": 3.705585548254355,
            "mae": 0.3289384119939204,
            "precision": 0.6878504672897197,
            "recall": 0.757201646090535
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7755610443463941,
            "auditor_fn_violation": 0.006787468136152352,
            "auditor_fp_violation": 0.024883436067646602,
            "ave_precision_score": 0.7761849865432617,
            "fpr": 0.13267543859649122,
            "logloss": 2.416220269073848,
            "mae": 0.31709508339653647,
            "precision": 0.7199074074074074,
            "recall": 0.6645299145299145
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.776829148520376,
            "auditor_fn_violation": 0.014362636816594622,
            "auditor_fp_violation": 0.03302124362368439,
            "ave_precision_score": 0.7775381464319135,
            "fpr": 0.13172338090010977,
            "logloss": 2.435455985036211,
            "mae": 0.3153820616115952,
            "precision": 0.7339246119733924,
            "recall": 0.6810699588477366
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.6733838905620627,
            "auditor_fn_violation": 0.029267881241565454,
            "auditor_fp_violation": 0.013869132290184926,
            "ave_precision_score": 0.669879232750124,
            "fpr": 0.13925438596491227,
            "logloss": 3.7468443191223106,
            "mae": 0.35027064932533564,
            "precision": 0.6939759036144578,
            "recall": 0.6153846153846154
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7186558594466532,
            "auditor_fn_violation": 0.035785755263740386,
            "auditor_fp_violation": 0.014280364176406019,
            "ave_precision_score": 0.7170105250531617,
            "fpr": 0.11964873765093303,
            "logloss": 3.5128108304881174,
            "mae": 0.33613309503796096,
            "precision": 0.7398568019093079,
            "recall": 0.6378600823045267
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.6718196397283405,
            "auditor_fn_violation": 0.03211454116059379,
            "auditor_fp_violation": 0.010727833096254153,
            "ave_precision_score": 0.6691877600622924,
            "fpr": 0.13157894736842105,
            "logloss": 3.6925391022938676,
            "mae": 0.34965757365539607,
            "precision": 0.7037037037037037,
            "recall": 0.6089743589743589
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7191737795458436,
            "auditor_fn_violation": 0.03259205052106625,
            "auditor_fp_violation": 0.015920449409181896,
            "ave_precision_score": 0.718011348953717,
            "fpr": 0.11306256860592755,
            "logloss": 3.4782114651096454,
            "mae": 0.3370053544780045,
            "precision": 0.745679012345679,
            "recall": 0.6213991769547325
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.6796614785331201,
            "auditor_fn_violation": 0.013980263157894742,
            "auditor_fp_violation": 0.01568180022127392,
            "ave_precision_score": 0.6634764274365164,
            "fpr": 0.18640350877192982,
            "logloss": 3.4152868795990687,
            "mae": 0.327833589493324,
            "precision": 0.6737044145873321,
            "recall": 0.75
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7050763598161722,
            "auditor_fn_violation": 0.017039114977888004,
            "auditor_fp_violation": 0.021770517207980886,
            "ave_precision_score": 0.6900569995535748,
            "fpr": 0.1690450054884742,
            "logloss": 3.348062566557871,
            "mae": 0.3242356759357158,
            "precision": 0.7027027027027027,
            "recall": 0.7489711934156379
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.6763343667347432,
            "auditor_fn_violation": 0.02793241115609537,
            "auditor_fp_violation": 0.01126126126126127,
            "ave_precision_score": 0.6721135529591593,
            "fpr": 0.14364035087719298,
            "logloss": 3.7337025115166247,
            "mae": 0.3515585258937157,
            "precision": 0.6895734597156398,
            "recall": 0.6217948717948718
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.7194121577170571,
            "auditor_fn_violation": 0.030934215103016173,
            "auditor_fp_violation": 0.017232517595402598,
            "ave_precision_score": 0.7160499853881153,
            "fpr": 0.12843029637760703,
            "logloss": 3.5314970791751406,
            "mae": 0.33522594509189635,
            "precision": 0.7247058823529412,
            "recall": 0.6337448559670782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.6804152871693083,
            "auditor_fn_violation": 0.013980263157894742,
            "auditor_fp_violation": 0.01568180022127392,
            "ave_precision_score": 0.6650163270338354,
            "fpr": 0.18640350877192982,
            "logloss": 3.3718612648408612,
            "mae": 0.3281312245650146,
            "precision": 0.6737044145873321,
            "recall": 0.75
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7049943708327226,
            "auditor_fn_violation": 0.017039114977888004,
            "auditor_fp_violation": 0.021770517207980886,
            "ave_precision_score": 0.6899329777638723,
            "fpr": 0.1690450054884742,
            "logloss": 3.344037335035917,
            "mae": 0.32451114747148646,
            "precision": 0.7027027027027027,
            "recall": 0.7489711934156379
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6326754385964912,
            "auc_prc": 0.6401731096021392,
            "auditor_fn_violation": 0.013026690658269608,
            "auditor_fp_violation": 0.02062588904694168,
            "ave_precision_score": 0.6459475775978495,
            "fpr": 0.08991228070175439,
            "logloss": 6.674434219101792,
            "mae": 0.4072587799693856,
            "precision": 0.7239057239057239,
            "recall": 0.4594017094017094
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.6523443530967655,
            "auditor_fn_violation": 0.008515040226224534,
            "auditor_fp_violation": 0.016504164783366696,
            "ave_precision_score": 0.6539793776718346,
            "fpr": 0.08342480790340286,
            "logloss": 6.71993852160413,
            "mae": 0.41456376124484984,
            "precision": 0.745819397993311,
            "recall": 0.4588477366255144
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.6736987532039453,
            "auditor_fn_violation": 0.030514319988004208,
            "auditor_fp_violation": 0.014797692429271385,
            "ave_precision_score": 0.670122804393517,
            "fpr": 0.1337719298245614,
            "logloss": 3.643779008597507,
            "mae": 0.34964100301478873,
            "precision": 0.7038834951456311,
            "recall": 0.6196581196581197
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7200115973024183,
            "auditor_fn_violation": 0.0332560881408302,
            "auditor_fp_violation": 0.012932136630722544,
            "ave_precision_score": 0.718522922258569,
            "fpr": 0.1141602634467618,
            "logloss": 3.408889372970911,
            "mae": 0.33566634016296465,
            "precision": 0.7463414634146341,
            "recall": 0.6296296296296297
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7930383786088469,
            "auditor_fn_violation": 0.01543522267206478,
            "auditor_fp_violation": 0.01452850877192983,
            "ave_precision_score": 0.793623139304235,
            "fpr": 0.12171052631578948,
            "logloss": 2.3777039414207515,
            "mae": 0.3147158184860298,
            "precision": 0.7318840579710145,
            "recall": 0.6474358974358975
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.8098715080167682,
            "auditor_fn_violation": 0.020390923915744024,
            "auditor_fp_violation": 0.017098211403112296,
            "ave_precision_score": 0.8105570758907503,
            "fpr": 0.10757409440175632,
            "logloss": 2.285019438746963,
            "mae": 0.29988517012248933,
            "precision": 0.7699530516431925,
            "recall": 0.6748971193415638
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6673060968931077,
            "auditor_fn_violation": 0.035303268855900446,
            "auditor_fp_violation": 0.014876718824087247,
            "ave_precision_score": 0.6638666995206025,
            "fpr": 0.12719298245614036,
            "logloss": 4.238218530675788,
            "mae": 0.3530663959809414,
            "precision": 0.7055837563451777,
            "recall": 0.594017094017094
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.7134178389062147,
            "auditor_fn_violation": 0.03495683755471535,
            "auditor_fp_violation": 0.013655323819978043,
            "ave_precision_score": 0.7117715258673525,
            "fpr": 0.1119648737650933,
            "logloss": 4.045985078400989,
            "mae": 0.3426832092529531,
            "precision": 0.7404580152671756,
            "recall": 0.5987654320987654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 9271,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7612444151628097,
            "auditor_fn_violation": 0.013380473084420454,
            "auditor_fp_violation": 0.01433341235972815,
            "ave_precision_score": 0.7603583882291101,
            "fpr": 0.1425438596491228,
            "logloss": 2.3848254064439915,
            "mae": 0.31540322810596544,
            "precision": 0.7167755991285403,
            "recall": 0.7029914529914529
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7572373776133394,
            "auditor_fn_violation": 0.016429284510757862,
            "auditor_fp_violation": 0.021119648737650938,
            "ave_precision_score": 0.7569773526419601,
            "fpr": 0.132821075740944,
            "logloss": 2.5736275580577854,
            "mae": 0.30809935744735006,
            "precision": 0.7397849462365591,
            "recall": 0.7078189300411523
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.6716384610299656,
            "auditor_fn_violation": 0.03238866396761134,
            "auditor_fp_violation": 0.014237098941046314,
            "ave_precision_score": 0.6698458785863866,
            "fpr": 0.13048245614035087,
            "logloss": 3.6562485832411222,
            "mae": 0.34990843676008726,
            "precision": 0.7076167076167076,
            "recall": 0.6153846153846154
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7202360003193476,
            "auditor_fn_violation": 0.030599937661774473,
            "auditor_fp_violation": 0.007841415380641835,
            "ave_precision_score": 0.7201916766264681,
            "fpr": 0.1141602634467618,
            "logloss": 3.4479398761449946,
            "mae": 0.3389619114499345,
            "precision": 0.7419354838709677,
            "recall": 0.6152263374485597
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.6847881149686643,
            "auditor_fn_violation": 0.010983655720497832,
            "auditor_fp_violation": 0.030242413466097677,
            "ave_precision_score": 0.6785079497715574,
            "fpr": 0.17434210526315788,
            "logloss": 2.978651272670572,
            "mae": 0.3243492435458444,
            "precision": 0.68762278978389,
            "recall": 0.7478632478632479
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.7008166665072524,
            "auditor_fn_violation": 0.012603162987356182,
            "auditor_fp_violation": 0.03383482921159683,
            "ave_precision_score": 0.6942057156048551,
            "fpr": 0.16465422612513722,
            "logloss": 3.0130641861362064,
            "mae": 0.32539256755392426,
            "precision": 0.7023809523809523,
            "recall": 0.7283950617283951
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 9271,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7466105722035109,
            "auditor_fn_violation": 0.006250937171989805,
            "auditor_fp_violation": 0.00034574047731943336,
            "ave_precision_score": 0.5119905899468649,
            "fpr": 0.47149122807017546,
            "logloss": 16.74615281543429,
            "mae": 0.488715417591824,
            "precision": 0.5124716553287982,
            "recall": 0.9658119658119658
        },
        "train": {
            "accuracy": 0.5389681668496158,
            "auc_prc": 0.7613273947712189,
            "auditor_fn_violation": 0.004792815745370935,
            "auditor_fp_violation": 0.005837153741848002,
            "ave_precision_score": 0.5369832403529406,
            "fpr": 0.446761800219539,
            "logloss": 15.800276663657272,
            "mae": 0.4612197170237821,
            "precision": 0.5375,
            "recall": 0.9732510288065843
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6381578947368421,
            "auc_prc": 0.6811280785102738,
            "auditor_fn_violation": 0.04937959214275005,
            "auditor_fp_violation": 0.04595878773510352,
            "ave_precision_score": 0.6593292141068787,
            "fpr": 0.12938596491228072,
            "logloss": 2.43620942904392,
            "mae": 0.3784700264492389,
            "precision": 0.6844919786096256,
            "recall": 0.5470085470085471
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.6867806560195303,
            "auditor_fn_violation": 0.041224539577997314,
            "auditor_fp_violation": 0.03640730935623426,
            "ave_precision_score": 0.6624007328369479,
            "fpr": 0.11855104281009879,
            "logloss": 2.6245623793528363,
            "mae": 0.38166762715018937,
            "precision": 0.7142857142857143,
            "recall": 0.5555555555555556
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.6714758055902814,
            "auditor_fn_violation": 0.03224574523916629,
            "auditor_fp_violation": 0.010727833096254153,
            "ave_precision_score": 0.6696009418064823,
            "fpr": 0.13157894736842105,
            "logloss": 3.702710602654702,
            "mae": 0.35034665491084527,
            "precision": 0.7022332506203474,
            "recall": 0.6047008547008547
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.7185445172294457,
            "auditor_fn_violation": 0.03236167012237265,
            "auditor_fp_violation": 0.015920449409181896,
            "ave_precision_score": 0.7178681203942082,
            "fpr": 0.11306256860592755,
            "logloss": 3.5152799997150117,
            "mae": 0.3392985216241553,
            "precision": 0.7444168734491315,
            "recall": 0.6172839506172839
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.6879039665567641,
            "auditor_fn_violation": 0.018661437246963564,
            "auditor_fp_violation": 0.009110261577366845,
            "ave_precision_score": 0.6903460727475991,
            "fpr": 0.0800438596491228,
            "logloss": 4.960894081393383,
            "mae": 0.38148650159467234,
            "precision": 0.7689873417721519,
            "recall": 0.5192307692307693
        },
        "train": {
            "accuracy": 0.6630076838638859,
            "auc_prc": 0.7260803770432496,
            "auditor_fn_violation": 0.02540282690300985,
            "auditor_fp_violation": 0.009527991218441273,
            "ave_precision_score": 0.7268717880859652,
            "fpr": 0.07464324917672886,
            "logloss": 4.925131065096408,
            "mae": 0.3755989026789228,
            "precision": 0.7841269841269841,
            "recall": 0.5082304526748971
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.6622134871152057,
            "auditor_fn_violation": 0.031102395411605935,
            "auditor_fp_violation": 0.00655919076971709,
            "ave_precision_score": 0.6632075526330048,
            "fpr": 0.11403508771929824,
            "logloss": 3.68359357323053,
            "mae": 0.37147387332443094,
            "precision": 0.7255936675461742,
            "recall": 0.5876068376068376
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.7070026702733561,
            "auditor_fn_violation": 0.03830412922985188,
            "auditor_fp_violation": 0.005491057015561439,
            "ave_precision_score": 0.7078675612155712,
            "fpr": 0.09769484083424808,
            "logloss": 3.6418341020315528,
            "mae": 0.3688929723758167,
            "precision": 0.7620320855614974,
            "recall": 0.5864197530864198
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.7263381638556683,
            "auditor_fn_violation": 0.025687884240515827,
            "auditor_fp_violation": 0.014451951951951958,
            "ave_precision_score": 0.7309873515860901,
            "fpr": 0.09649122807017543,
            "logloss": 5.785840807477441,
            "mae": 0.3603069795139759,
            "precision": 0.7349397590361446,
            "recall": 0.5213675213675214
        },
        "train": {
            "accuracy": 0.6553238199780461,
            "auc_prc": 0.7498326868106637,
            "auditor_fn_violation": 0.038934287379219695,
            "auditor_fp_violation": 0.010757409440175636,
            "ave_precision_score": 0.7520165275354764,
            "fpr": 0.09330406147091108,
            "logloss": 5.646675645943422,
            "mae": 0.3539914008805455,
            "precision": 0.7514619883040936,
            "recall": 0.5288065843621399
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 9271,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7790101086088139,
            "auditor_fn_violation": 0.009924651372019798,
            "auditor_fp_violation": 0.006578947368421054,
            "ave_precision_score": 0.7796330867048314,
            "fpr": 0.12171052631578948,
            "logloss": 2.631900552304793,
            "mae": 0.3221305748394409,
            "precision": 0.7299270072992701,
            "recall": 0.6410256410256411
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.783157319089238,
            "auditor_fn_violation": 0.013680530145952758,
            "auditor_fp_violation": 0.016739200619874734,
            "ave_precision_score": 0.7839156339056725,
            "fpr": 0.12843029637760703,
            "logloss": 2.5200834924376982,
            "mae": 0.3191699322189055,
            "precision": 0.7340909090909091,
            "recall": 0.6646090534979424
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 9271,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.797585748249012,
            "auditor_fn_violation": 0.01676366396761134,
            "auditor_fp_violation": 0.010977260154891736,
            "ave_precision_score": 0.7981573903183943,
            "fpr": 0.12390350877192982,
            "logloss": 2.1628135500694956,
            "mae": 0.3087433741308908,
            "precision": 0.735981308411215,
            "recall": 0.6730769230769231
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8160251429780372,
            "auditor_fn_violation": 0.020201198881525755,
            "auditor_fp_violation": 0.018420610834893783,
            "ave_precision_score": 0.8166164638304156,
            "fpr": 0.1141602634467618,
            "logloss": 2.185965310762611,
            "mae": 0.2994851641556518,
            "precision": 0.7630979498861048,
            "recall": 0.6893004115226338
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.6716426145656236,
            "auditor_fn_violation": 0.03238866396761134,
            "auditor_fp_violation": 0.014237098941046314,
            "ave_precision_score": 0.6699031428264668,
            "fpr": 0.13048245614035087,
            "logloss": 3.6562349657200306,
            "mae": 0.34987359370263904,
            "precision": 0.7076167076167076,
            "recall": 0.6153846153846154
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7201882315659918,
            "auditor_fn_violation": 0.030599937661774473,
            "auditor_fp_violation": 0.007841415380641835,
            "ave_precision_score": 0.7201156410015825,
            "fpr": 0.1141602634467618,
            "logloss": 3.447675812601362,
            "mae": 0.33890327405310644,
            "precision": 0.7419354838709677,
            "recall": 0.6152263374485597
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6491228070175439,
            "auc_prc": 0.7355369922625814,
            "auditor_fn_violation": 0.00708502024291498,
            "auditor_fp_violation": 0.015113798008534856,
            "ave_precision_score": 0.7362095721463622,
            "fpr": 0.11403508771929824,
            "logloss": 2.143950400747666,
            "mae": 0.41094615885640134,
            "precision": 0.7078651685393258,
            "recall": 0.5384615384615384
        },
        "train": {
            "accuracy": 0.6630076838638859,
            "auc_prc": 0.7488136572457273,
            "auditor_fn_violation": 0.015001829491401402,
            "auditor_fp_violation": 0.02043003809646801,
            "ave_precision_score": 0.7497110044562341,
            "fpr": 0.09989023051591657,
            "logloss": 2.156225416845888,
            "mae": 0.4081370508258779,
            "precision": 0.7479224376731302,
            "recall": 0.5555555555555556
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.7279908720798861,
            "auditor_fn_violation": 0.028283850652271705,
            "auditor_fp_violation": 0.017440137505926986,
            "ave_precision_score": 0.7326281574274417,
            "fpr": 0.10307017543859649,
            "logloss": 5.7479574677914975,
            "mae": 0.36037576169993973,
            "precision": 0.7251461988304093,
            "recall": 0.5299145299145299
        },
        "train": {
            "accuracy": 0.6575192096597146,
            "auc_prc": 0.7526073464081227,
            "auditor_fn_violation": 0.038369629539284376,
            "auditor_fp_violation": 0.008856460256989732,
            "ave_precision_score": 0.7547816645388595,
            "fpr": 0.09659714599341383,
            "logloss": 5.612277274945711,
            "mae": 0.3517747490152986,
            "precision": 0.7485714285714286,
            "recall": 0.5390946502057613
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.7270704502163017,
            "auditor_fn_violation": 0.030779071075123704,
            "auditor_fp_violation": 0.015180476529160748,
            "ave_precision_score": 0.7317323055361085,
            "fpr": 0.10635964912280702,
            "logloss": 5.7407252867508065,
            "mae": 0.36176890293933095,
            "precision": 0.7212643678160919,
            "recall": 0.5363247863247863
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.753301318399182,
            "auditor_fn_violation": 0.035979997560678136,
            "auditor_fp_violation": 0.007474656163233685,
            "ave_precision_score": 0.7554780081349806,
            "fpr": 0.10098792535675083,
            "logloss": 5.599595870897096,
            "mae": 0.35160149211502867,
            "precision": 0.7458563535911602,
            "recall": 0.5555555555555556
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.6947407924491127,
            "auditor_fn_violation": 0.024244639376218333,
            "auditor_fp_violation": 0.016348585427532803,
            "ave_precision_score": 0.6968008123546454,
            "fpr": 0.11403508771929824,
            "logloss": 2.205748370691506,
            "mae": 0.38125585437234627,
            "precision": 0.7360406091370558,
            "recall": 0.6196581196581197
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6868347617806716,
            "auditor_fn_violation": 0.03649044824797966,
            "auditor_fp_violation": 0.012601536772777172,
            "ave_precision_score": 0.6886307697175071,
            "fpr": 0.1119648737650933,
            "logloss": 2.3035730734724655,
            "mae": 0.3895550880131424,
            "precision": 0.7437185929648241,
            "recall": 0.6090534979423868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 9271,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.7310367344978035,
            "auditor_fn_violation": 0.011752136752136752,
            "auditor_fp_violation": 0.01361476608187134,
            "ave_precision_score": 0.7198983581697533,
            "fpr": 0.17214912280701755,
            "logloss": 1.5836623497876898,
            "mae": 0.2885305138432639,
            "precision": 0.7097966728280961,
            "recall": 0.8205128205128205
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.7549329715446392,
            "auditor_fn_violation": 0.012698025504465318,
            "auditor_fp_violation": 0.024833731516756,
            "ave_precision_score": 0.7438797723910723,
            "fpr": 0.14818880351262348,
            "logloss": 1.5184560553297002,
            "mae": 0.27156442531405006,
            "precision": 0.7486033519553073,
            "recall": 0.8271604938271605
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.6710831281795684,
            "auditor_fn_violation": 0.01388186009896537,
            "auditor_fp_violation": 0.019499762920815556,
            "ave_precision_score": 0.6475154234990935,
            "fpr": 0.19517543859649122,
            "logloss": 3.900992237422036,
            "mae": 0.3321096959242048,
            "precision": 0.664783427495292,
            "recall": 0.7542735042735043
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.6943587256954793,
            "auditor_fn_violation": 0.011243466908791949,
            "auditor_fp_violation": 0.027517272551171958,
            "ave_precision_score": 0.674565620476395,
            "fpr": 0.1877058177826564,
            "logloss": 3.7016416810499306,
            "mae": 0.3292297781201011,
            "precision": 0.6827458256029685,
            "recall": 0.757201646090535
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 9271,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.7277083778818457,
            "auditor_fn_violation": 0.023630791722896994,
            "auditor_fp_violation": 0.013046763869132292,
            "ave_precision_score": 0.7322297482795274,
            "fpr": 0.0756578947368421,
            "logloss": 5.728079079649778,
            "mae": 0.3588302113826672,
            "precision": 0.7730263157894737,
            "recall": 0.5021367521367521
        },
        "train": {
            "accuracy": 0.6630076838638859,
            "auc_prc": 0.7518456312807777,
            "auditor_fn_violation": 0.036386551205431564,
            "auditor_fp_violation": 0.01433718602699038,
            "ave_precision_score": 0.7540554175965293,
            "fpr": 0.07793633369923161,
            "logloss": 5.556806908716068,
            "mae": 0.3547421772118144,
            "precision": 0.778816199376947,
            "recall": 0.51440329218107
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.6715310847179476,
            "auditor_fn_violation": 0.011358524516419256,
            "auditor_fp_violation": 0.012498518255097204,
            "ave_precision_score": 0.655741538854694,
            "fpr": 0.1875,
            "logloss": 3.4357931609573606,
            "mae": 0.3271574137124717,
            "precision": 0.6785714285714286,
            "recall": 0.7713675213675214
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.6945653396266744,
            "auditor_fn_violation": 0.015074105694913112,
            "auditor_fp_violation": 0.027623167818170084,
            "ave_precision_score": 0.6809977681103009,
            "fpr": 0.1756311745334797,
            "logloss": 3.3532947062046228,
            "mae": 0.32770991522888576,
            "precision": 0.6952380952380952,
            "recall": 0.7510288065843621
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.6879511544157997,
            "auditor_fn_violation": 0.020664642375168702,
            "auditor_fp_violation": 0.010461119013750592,
            "ave_precision_score": 0.6732686503852375,
            "fpr": 0.15789473684210525,
            "logloss": 3.337911136557026,
            "mae": 0.3288027765951764,
            "precision": 0.6923076923076923,
            "recall": 0.6923076923076923
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7331625041153519,
            "auditor_fn_violation": 0.01777317016980391,
            "auditor_fp_violation": 0.019985794537353913,
            "ave_precision_score": 0.7214066049148014,
            "fpr": 0.141602634467618,
            "logloss": 3.0680579268932133,
            "mae": 0.30995908585314563,
            "precision": 0.7301255230125523,
            "recall": 0.7181069958847737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.6735002384360853,
            "auditor_fn_violation": 0.004407051282051286,
            "auditor_fp_violation": 0.01825509720246563,
            "ave_precision_score": 0.6316388205484958,
            "fpr": 0.17214912280701755,
            "logloss": 5.799988922888525,
            "mae": 0.37672825275277494,
            "precision": 0.6715481171548117,
            "recall": 0.6858974358974359
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7063528128741489,
            "auditor_fn_violation": 0.015896247509858928,
            "auditor_fp_violation": 0.01885710595983729,
            "ave_precision_score": 0.6670982921620255,
            "fpr": 0.1525795828759605,
            "logloss": 5.460526921412479,
            "mae": 0.34636955332747943,
            "precision": 0.7214428857715431,
            "recall": 0.7407407407407407
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6118421052631579,
            "auc_prc": 0.6070833190369613,
            "auditor_fn_violation": 0.023096603688708955,
            "auditor_fp_violation": 0.046388493756914825,
            "ave_precision_score": 0.6064213492484154,
            "fpr": 0.20175438596491227,
            "logloss": 2.2422682150388757,
            "mae": 0.4534500299967005,
            "precision": 0.6182572614107884,
            "recall": 0.6367521367521367
        },
        "train": {
            "accuracy": 0.6201975850713501,
            "auc_prc": 0.6265219050654427,
            "auditor_fn_violation": 0.023534938768503835,
            "auditor_fp_violation": 0.03537160198876479,
            "ave_precision_score": 0.626446973549782,
            "fpr": 0.18441273326015367,
            "logloss": 2.4615242994415034,
            "mae": 0.4511780646753041,
            "precision": 0.6470588235294118,
            "recall": 0.6337448559670782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.5850381528333659,
            "auditor_fn_violation": 0.010955540560803723,
            "auditor_fp_violation": 0.021312430851904544,
            "ave_precision_score": 0.5679788580099048,
            "fpr": 0.11293859649122807,
            "logloss": 6.81635175739089,
            "mae": 0.41934633317354514,
            "precision": 0.6750788643533123,
            "recall": 0.45726495726495725
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.6205763714685413,
            "auditor_fn_violation": 0.012874198750525144,
            "auditor_fp_violation": 0.020417124039517013,
            "ave_precision_score": 0.6063218716228211,
            "fpr": 0.09769484083424808,
            "logloss": 6.94448657385076,
            "mae": 0.4148870964960254,
            "precision": 0.7100977198697068,
            "recall": 0.448559670781893
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 9271,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.8111120530775733,
            "auditor_fn_violation": 0.01241752886489729,
            "auditor_fp_violation": 0.01644736842105264,
            "ave_precision_score": 0.8114182826268332,
            "fpr": 0.125,
            "logloss": 1.3241406549290708,
            "mae": 0.3025211074723607,
            "precision": 0.7367205542725174,
            "recall": 0.6816239316239316
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8217333907734652,
            "auditor_fn_violation": 0.01465851752472072,
            "auditor_fp_violation": 0.020566927100148517,
            "ave_precision_score": 0.8221932248111342,
            "fpr": 0.11086717892425905,
            "logloss": 1.3366155205428694,
            "mae": 0.29044269348532054,
            "precision": 0.769406392694064,
            "recall": 0.6934156378600823
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 9271,
        "test": {
            "accuracy": 0.643640350877193,
            "auc_prc": 0.7193314962019277,
            "auditor_fn_violation": 0.02549342105263158,
            "auditor_fp_violation": 0.01735617196143512,
            "ave_precision_score": 0.7235882605556913,
            "fpr": 0.11403508771929824,
            "logloss": 5.849920862544628,
            "mae": 0.36225202112680877,
            "precision": 0.7037037037037037,
            "recall": 0.5277777777777778
        },
        "train": {
            "accuracy": 0.6586169045005489,
            "auc_prc": 0.7396884562674737,
            "auditor_fn_violation": 0.03451640443956581,
            "auditor_fp_violation": 0.011777619939303933,
            "ave_precision_score": 0.7417910782846133,
            "fpr": 0.10428100987925357,
            "logloss": 5.722729323529001,
            "mae": 0.3531963209843637,
            "precision": 0.7397260273972602,
            "recall": 0.5555555555555556
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7650033869240362,
            "auditor_fn_violation": 0.02895392862498126,
            "auditor_fp_violation": 0.011034060376165646,
            "ave_precision_score": 0.7657405336489551,
            "fpr": 0.11951754385964912,
            "logloss": 2.826772594070147,
            "mae": 0.3399805958578876,
            "precision": 0.7183462532299741,
            "recall": 0.594017094017094
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.786056882443872,
            "auditor_fn_violation": 0.030132400970308032,
            "auditor_fp_violation": 0.014706528055788728,
            "ave_precision_score": 0.7866429813472219,
            "fpr": 0.09989023051591657,
            "logloss": 2.756387626454575,
            "mae": 0.32818770119315815,
            "precision": 0.7617801047120419,
            "recall": 0.5987654320987654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.6849349096758641,
            "auditor_fn_violation": 0.01332658569500675,
            "auditor_fp_violation": 0.009260905642484593,
            "ave_precision_score": 0.6731470664047036,
            "fpr": 0.17653508771929824,
            "logloss": 3.18129353152117,
            "mae": 0.32493217932377266,
            "precision": 0.6836935166994106,
            "recall": 0.7435897435897436
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.70414076270634,
            "auditor_fn_violation": 0.017251426325703687,
            "auditor_fp_violation": 0.02178601407632208,
            "ave_precision_score": 0.6912839406448258,
            "fpr": 0.15916575192096596,
            "logloss": 3.2704642575947287,
            "mae": 0.3195764055974333,
            "precision": 0.7094188376753507,
            "recall": 0.7283950617283951
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.6718221098020807,
            "auditor_fn_violation": 0.03211454116059379,
            "auditor_fp_violation": 0.010727833096254153,
            "ave_precision_score": 0.6692192860260143,
            "fpr": 0.13157894736842105,
            "logloss": 3.689128099558498,
            "mae": 0.3496196302794639,
            "precision": 0.7037037037037037,
            "recall": 0.6089743589743589
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.7192074952997701,
            "auditor_fn_violation": 0.03215839329999593,
            "auditor_fp_violation": 0.015920449409181896,
            "ave_precision_score": 0.7180488573612738,
            "fpr": 0.11306256860592755,
            "logloss": 3.474817013221003,
            "mae": 0.33692903559748455,
            "precision": 0.7463054187192119,
            "recall": 0.6234567901234568
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.7241979670648034,
            "auditor_fn_violation": 0.008743814664867297,
            "auditor_fp_violation": 0.009105322427690854,
            "ave_precision_score": 0.7216400809565311,
            "fpr": 0.1611842105263158,
            "logloss": 2.40792667944511,
            "mae": 0.35071490838763136,
            "precision": 0.6931106471816284,
            "recall": 0.7094017094017094
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7269329825254472,
            "auditor_fn_violation": 0.014125480523821786,
            "auditor_fp_violation": 0.018397365532382,
            "ave_precision_score": 0.7254544563442205,
            "fpr": 0.14928649835345773,
            "logloss": 2.5168320466414986,
            "mae": 0.35001355946411034,
            "precision": 0.7195876288659794,
            "recall": 0.7181069958847737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.6683133159944469,
            "auditor_fn_violation": 0.03362104513420304,
            "auditor_fp_violation": 0.013365339023233771,
            "ave_precision_score": 0.6645030107670951,
            "fpr": 0.12828947368421054,
            "logloss": 3.8999788613638073,
            "mae": 0.353509984234029,
            "precision": 0.7037974683544304,
            "recall": 0.594017094017094
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.7146785259127223,
            "auditor_fn_violation": 0.03308217352613012,
            "auditor_fp_violation": 0.01486407955059082,
            "ave_precision_score": 0.7124196786790722,
            "fpr": 0.11525795828759605,
            "logloss": 3.7128210085884277,
            "mae": 0.34050097172247384,
            "precision": 0.7375,
            "recall": 0.6069958847736625
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.7603846038214335,
            "auditor_fn_violation": 0.014802631578947375,
            "auditor_fp_violation": 0.026525703334913865,
            "ave_precision_score": 0.7270651990278865,
            "fpr": 0.15899122807017543,
            "logloss": 4.138942690054006,
            "mae": 0.3268452919034696,
            "precision": 0.6861471861471862,
            "recall": 0.6773504273504274
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7667578392864473,
            "auditor_fn_violation": 0.013773134031702156,
            "auditor_fp_violation": 0.021393426745011948,
            "ave_precision_score": 0.7250749442235044,
            "fpr": 0.15697036223929747,
            "logloss": 4.423869141880948,
            "mae": 0.3153988438178281,
            "precision": 0.709349593495935,
            "recall": 0.7181069958847737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.6831118359295745,
            "auditor_fn_violation": 0.015252474134053083,
            "auditor_fp_violation": 0.010876007586533903,
            "ave_precision_score": 0.6731151699628671,
            "fpr": 0.16776315789473684,
            "logloss": 3.1347960950287828,
            "mae": 0.3230132561953292,
            "precision": 0.6915322580645161,
            "recall": 0.7329059829059829
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7044057592137818,
            "auditor_fn_violation": 0.015469366182867835,
            "auditor_fp_violation": 0.0255440046490605,
            "ave_precision_score": 0.6928196681399863,
            "fpr": 0.1602634467618002,
            "logloss": 3.2169956843073715,
            "mae": 0.3193606510292791,
            "precision": 0.709741550695825,
            "recall": 0.7345679012345679
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.7525809480311053,
            "auditor_fn_violation": 0.0299613885140201,
            "auditor_fp_violation": 0.01507428481112692,
            "ave_precision_score": 0.7533298063545609,
            "fpr": 0.10964912280701754,
            "logloss": 3.2536149086736845,
            "mae": 0.3468693196143302,
            "precision": 0.7282608695652174,
            "recall": 0.5726495726495726
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7791018112308445,
            "auditor_fn_violation": 0.03313863931012364,
            "auditor_fp_violation": 0.01671078969458255,
            "ave_precision_score": 0.7796860751130491,
            "fpr": 0.09110867178924259,
            "logloss": 3.2478349961931587,
            "mae": 0.3331951256813372,
            "precision": 0.7750677506775068,
            "recall": 0.588477366255144
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.7197668909928823,
            "auditor_fn_violation": 0.007230281901334539,
            "auditor_fp_violation": 0.031146277856804173,
            "ave_precision_score": 0.721748209794004,
            "fpr": 0.19298245614035087,
            "logloss": 5.197415729303318,
            "mae": 0.3885165506223316,
            "precision": 0.6247334754797441,
            "recall": 0.6260683760683761
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.7298035273557475,
            "auditor_fn_violation": 0.007209551300294079,
            "auditor_fp_violation": 0.016870924000774848,
            "ave_precision_score": 0.7316948874472458,
            "fpr": 0.1602634467618002,
            "logloss": 5.155522217720245,
            "mae": 0.3761925404048196,
            "precision": 0.6784140969162996,
            "recall": 0.6337448559670782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6162280701754386,
            "auc_prc": 0.7359127061560637,
            "auditor_fn_violation": 0.03542041535462589,
            "auditor_fp_violation": 0.006704895685158843,
            "ave_precision_score": 0.7384739302028358,
            "fpr": 0.020833333333333332,
            "logloss": 4.357896995604129,
            "mae": 0.4007475626685182,
            "precision": 0.8782051282051282,
            "recall": 0.29273504273504275
        },
        "train": {
            "accuracy": 0.5960482985729967,
            "auc_prc": 0.7573865572981576,
            "auditor_fn_violation": 0.03143111400215927,
            "auditor_fp_violation": 0.009086330470717375,
            "ave_precision_score": 0.7597091534276292,
            "fpr": 0.02305159165751921,
            "logloss": 4.585449013193097,
            "mae": 0.41410697395368695,
            "precision": 0.86875,
            "recall": 0.28600823045267487
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7036627489179434,
            "auditor_fn_violation": 0.019905533063427804,
            "auditor_fp_violation": 0.0060356409040619625,
            "ave_precision_score": 0.6998684409721904,
            "fpr": 0.13596491228070176,
            "logloss": 2.52990821285275,
            "mae": 0.32671016888368937,
            "precision": 0.7168949771689498,
            "recall": 0.6709401709401709
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7450732903891036,
            "auditor_fn_violation": 0.016528664290586484,
            "auditor_fp_violation": 0.015780977594111196,
            "ave_precision_score": 0.7434150944972326,
            "fpr": 0.1207464324917673,
            "logloss": 2.424175264713762,
            "mae": 0.3116874000445327,
            "precision": 0.751131221719457,
            "recall": 0.6831275720164609
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 9271,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.719286967936547,
            "auditor_fn_violation": 0.014092723796671171,
            "auditor_fp_violation": 0.010841433538801955,
            "ave_precision_score": 0.7001889510787718,
            "fpr": 0.18530701754385964,
            "logloss": 2.112163411792883,
            "mae": 0.29615071680763755,
            "precision": 0.6893382352941176,
            "recall": 0.8012820512820513
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.7453755872359716,
            "auditor_fn_violation": 0.014019324849913951,
            "auditor_fp_violation": 0.028992057854975144,
            "ave_precision_score": 0.725238154446864,
            "fpr": 0.1712403951701427,
            "logloss": 2.050379136363568,
            "mae": 0.2793659383649003,
            "precision": 0.7209302325581395,
            "recall": 0.8292181069958847
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7071709819618376,
            "auditor_fn_violation": 0.014985380116959069,
            "auditor_fp_violation": 0.0072531412991939355,
            "ave_precision_score": 0.7017850886233101,
            "fpr": 0.14583333333333334,
            "logloss": 2.542889951757387,
            "mae": 0.3261346810662781,
            "precision": 0.7057522123893806,
            "recall": 0.6816239316239316
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.745308230690318,
            "auditor_fn_violation": 0.024515184778631554,
            "auditor_fp_violation": 0.014574804674888626,
            "ave_precision_score": 0.7413869989307413,
            "fpr": 0.12843029637760703,
            "logloss": 2.4750955127247694,
            "mae": 0.3105242153911841,
            "precision": 0.7450980392156863,
            "recall": 0.7037037037037037
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.689919688405594,
            "auditor_fn_violation": 0.019443975858449553,
            "auditor_fp_violation": 0.011794689426268382,
            "ave_precision_score": 0.6860466298857093,
            "fpr": 0.14802631578947367,
            "logloss": 3.00278894054284,
            "mae": 0.32856483777422885,
            "precision": 0.7026431718061674,
            "recall": 0.6816239316239316
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7318590801539216,
            "auditor_fn_violation": 0.01831298306478207,
            "auditor_fp_violation": 0.01779557047846582,
            "ave_precision_score": 0.7296496973298724,
            "fpr": 0.12623490669593854,
            "logloss": 2.786907383848242,
            "mae": 0.31264608836724644,
            "precision": 0.7472527472527473,
            "recall": 0.6995884773662552
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 9271,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.6859733191158888,
            "auditor_fn_violation": 0.009709101814364974,
            "auditor_fp_violation": 0.013254208155523948,
            "ave_precision_score": 0.6775689271158636,
            "fpr": 0.18311403508771928,
            "logloss": 3.02812045451109,
            "mae": 0.3251573839237163,
            "precision": 0.6806883365200764,
            "recall": 0.7606837606837606
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7097012345605519,
            "auditor_fn_violation": 0.014527516905855726,
            "auditor_fp_violation": 0.022049460838122297,
            "ave_precision_score": 0.7024841148512482,
            "fpr": 0.15806805708013172,
            "logloss": 2.999057091677205,
            "mae": 0.3184137453518789,
            "precision": 0.7176470588235294,
            "recall": 0.7530864197530864
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.6551094361692705,
            "auditor_fn_violation": 0.019736842105263174,
            "auditor_fp_violation": 0.017855026078710295,
            "ave_precision_score": 0.6566436960904121,
            "fpr": 0.14692982456140352,
            "logloss": 3.237601127387392,
            "mae": 0.38163116941769176,
            "precision": 0.6616161616161617,
            "recall": 0.5598290598290598
        },
        "train": {
            "accuracy": 0.6531284302963776,
            "auc_prc": 0.703130910510372,
            "auditor_fn_violation": 0.03503137238958681,
            "auditor_fp_violation": 0.011273971718215276,
            "ave_precision_score": 0.7040574862783788,
            "fpr": 0.13611416026344675,
            "logloss": 3.1288580755439064,
            "mae": 0.3694094649129883,
            "precision": 0.7033492822966507,
            "recall": 0.6049382716049383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.7135249289810133,
            "auditor_fn_violation": 0.023565189683610748,
            "auditor_fp_violation": 0.01609668879405721,
            "ave_precision_score": 0.7182503914676082,
            "fpr": 0.09978070175438597,
            "logloss": 5.896229772198744,
            "mae": 0.36335167732675816,
            "precision": 0.723404255319149,
            "recall": 0.5085470085470085
        },
        "train": {
            "accuracy": 0.6597145993413831,
            "auc_prc": 0.7388262861817333,
            "auditor_fn_violation": 0.03743003889363202,
            "auditor_fp_violation": 0.00944275844256473,
            "ave_precision_score": 0.7410677351484587,
            "fpr": 0.09549945115257959,
            "logloss": 5.728970000702007,
            "mae": 0.3529798204730484,
            "precision": 0.7514285714285714,
            "recall": 0.5411522633744856
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 9271,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.743598304102987,
            "auditor_fn_violation": 0.018872300944669376,
            "auditor_fp_violation": 0.014879188398925245,
            "ave_precision_score": 0.7457060752579052,
            "fpr": 0.10416666666666667,
            "logloss": 4.37875316221639,
            "mae": 0.34933194884496954,
            "precision": 0.7331460674157303,
            "recall": 0.5576923076923077
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.76474327012107,
            "auditor_fn_violation": 0.034014988277703245,
            "auditor_fp_violation": 0.010713501646542261,
            "ave_precision_score": 0.7667411205912299,
            "fpr": 0.09659714599341383,
            "logloss": 4.546725796270895,
            "mae": 0.33715142714878005,
            "precision": 0.7615176151761518,
            "recall": 0.5781893004115226
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.6716496861674786,
            "auditor_fn_violation": 0.03238866396761134,
            "auditor_fp_violation": 0.014237098941046314,
            "ave_precision_score": 0.6698621444765905,
            "fpr": 0.13048245614035087,
            "logloss": 3.6600954945111255,
            "mae": 0.3499304485267605,
            "precision": 0.7076167076167076,
            "recall": 0.6153846153846154
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7200922440172538,
            "auditor_fn_violation": 0.030599937661774473,
            "auditor_fp_violation": 0.007841415380641835,
            "ave_precision_score": 0.7200481414329066,
            "fpr": 0.1141602634467618,
            "logloss": 3.4525843459571597,
            "mae": 0.33911196890296014,
            "precision": 0.7419354838709677,
            "recall": 0.6152263374485597
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.7014098756356059,
            "auditor_fn_violation": 0.021020767731294056,
            "auditor_fp_violation": 0.01435316895843212,
            "ave_precision_score": 0.6968047282202108,
            "fpr": 0.14035087719298245,
            "logloss": 2.720053378530911,
            "mae": 0.32932256386176745,
            "precision": 0.7077625570776256,
            "recall": 0.6623931623931624
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7409395513994301,
            "auditor_fn_violation": 0.020856201975850718,
            "auditor_fp_violation": 0.01618389617098212,
            "ave_precision_score": 0.7381903735346195,
            "fpr": 0.12184412733260154,
            "logloss": 2.6151742955873285,
            "mae": 0.31660226485764814,
            "precision": 0.7448275862068966,
            "recall": 0.6666666666666666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6546052631578947,
            "auc_prc": 0.7285374307730272,
            "auditor_fn_violation": 0.028663405308142157,
            "auditor_fp_violation": 0.014669274537695596,
            "ave_precision_score": 0.7331777009300502,
            "fpr": 0.10526315789473684,
            "logloss": 5.723566155445554,
            "mae": 0.3594884235869989,
            "precision": 0.7217391304347827,
            "recall": 0.532051282051282
        },
        "train": {
            "accuracy": 0.6608122941822173,
            "auc_prc": 0.7544079912309516,
            "auditor_fn_violation": 0.0368586051596175,
            "auditor_fp_violation": 0.007342932782333569,
            "ave_precision_score": 0.7565759278336588,
            "fpr": 0.09659714599341383,
            "logloss": 5.590870556776858,
            "mae": 0.3511994945867025,
            "precision": 0.7507082152974505,
            "recall": 0.5452674897119342
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.6935145628987389,
            "auditor_fn_violation": 0.014043522267206485,
            "auditor_fp_violation": 0.008431128496917972,
            "ave_precision_score": 0.6899785599468204,
            "fpr": 0.17763157894736842,
            "logloss": 2.71818601248849,
            "mae": 0.326060280104957,
            "precision": 0.6785714285714286,
            "recall": 0.7307692307692307
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7153238820300344,
            "auditor_fn_violation": 0.01769185944085322,
            "auditor_fp_violation": 0.023965906889649388,
            "ave_precision_score": 0.7125527153871405,
            "fpr": 0.1525795828759605,
            "logloss": 2.817408894473951,
            "mae": 0.3161012163726394,
            "precision": 0.7186234817813765,
            "recall": 0.7304526748971193
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.6714081691584849,
            "auditor_fn_violation": 0.03118908382066277,
            "auditor_fp_violation": 0.012905998103366527,
            "ave_precision_score": 0.6689877694720912,
            "fpr": 0.13267543859649122,
            "logloss": 3.6554670190186287,
            "mae": 0.34958450865986834,
            "precision": 0.7027027027027027,
            "recall": 0.6111111111111112
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7184091891213823,
            "auditor_fn_violation": 0.03172473607892561,
            "auditor_fp_violation": 0.015920449409181896,
            "ave_precision_score": 0.7174479786817634,
            "fpr": 0.11306256860592755,
            "logloss": 3.4406163021954383,
            "mae": 0.3366147048874342,
            "precision": 0.7469287469287469,
            "recall": 0.6255144032921811
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.6803760026231896,
            "auditor_fn_violation": 0.013980263157894742,
            "auditor_fp_violation": 0.01568180022127392,
            "ave_precision_score": 0.6649848395787966,
            "fpr": 0.18640350877192982,
            "logloss": 3.3730945322106627,
            "mae": 0.328122957059215,
            "precision": 0.6737044145873321,
            "recall": 0.75
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7049052085699092,
            "auditor_fn_violation": 0.017039114977888004,
            "auditor_fp_violation": 0.021770517207980886,
            "ave_precision_score": 0.689860527023044,
            "fpr": 0.1690450054884742,
            "logloss": 3.345465081326849,
            "mae": 0.32454601316865384,
            "precision": 0.7027027027027027,
            "recall": 0.7489711934156379
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.6767204839048699,
            "auditor_fn_violation": 0.025697255960413858,
            "auditor_fp_violation": 0.012878832780148576,
            "ave_precision_score": 0.6722624117935503,
            "fpr": 0.14364035087719298,
            "logloss": 3.568920070092816,
            "mae": 0.351398198758229,
            "precision": 0.6903073286052009,
            "recall": 0.6239316239316239
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.7169385527135493,
            "auditor_fn_violation": 0.03688345010457463,
            "auditor_fp_violation": 0.015892038483889714,
            "ave_precision_score": 0.7135640079613389,
            "fpr": 0.12403951701427003,
            "logloss": 3.3927331499547866,
            "mae": 0.33609601234368963,
            "precision": 0.7328605200945626,
            "recall": 0.6378600823045267
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.6540101649071794,
            "auditor_fn_violation": 0.02273579247263458,
            "auditor_fp_violation": 0.012992433222696378,
            "ave_precision_score": 0.6553005467434914,
            "fpr": 0.10416666666666667,
            "logloss": 3.273061451729846,
            "mae": 0.3765286260673595,
            "precision": 0.739010989010989,
            "recall": 0.5747863247863247
        },
        "train": {
            "accuracy": 0.6717892425905598,
            "auc_prc": 0.7056703748390198,
            "auditor_fn_violation": 0.03646786193438224,
            "auditor_fp_violation": 0.0089055336734035,
            "ave_precision_score": 0.7064583221336933,
            "fpr": 0.09110867178924259,
            "logloss": 3.1642892640885623,
            "mae": 0.377529614097594,
            "precision": 0.7648725212464589,
            "recall": 0.5555555555555556
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.6658484799590104,
            "auditor_fn_violation": 0.011360867446393768,
            "auditor_fp_violation": 0.05016941283388653,
            "ave_precision_score": 0.6491400255925429,
            "fpr": 0.23793859649122806,
            "logloss": 3.751167068637168,
            "mae": 0.36394900655388235,
            "precision": 0.6346801346801347,
            "recall": 0.8055555555555556
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.66997599346705,
            "auditor_fn_violation": 0.007927796072691793,
            "auditor_fp_violation": 0.04583973655323821,
            "ave_precision_score": 0.6597590138494085,
            "fpr": 0.2239297475301866,
            "logloss": 3.573022075875881,
            "mae": 0.35459303005915205,
            "precision": 0.66,
            "recall": 0.8148148148148148
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7025349871522734,
            "auditor_fn_violation": 0.01057130004498426,
            "auditor_fp_violation": 0.01849217638691323,
            "ave_precision_score": 0.6576248162949673,
            "fpr": 0.19298245614035087,
            "logloss": 3.668615681444306,
            "mae": 0.31529559244603467,
            "precision": 0.6764705882352942,
            "recall": 0.7863247863247863
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.6796653250278335,
            "auditor_fn_violation": 0.015367727771679473,
            "auditor_fp_violation": 0.02128236585523342,
            "ave_precision_score": 0.6309208479013211,
            "fpr": 0.20965971459934138,
            "logloss": 4.483695675830882,
            "mae": 0.3417330073715583,
            "precision": 0.6607460035523979,
            "recall": 0.7654320987654321
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.6736996081718989,
            "auditor_fn_violation": 0.030514319988004208,
            "auditor_fp_violation": 0.014797692429271385,
            "ave_precision_score": 0.6702048531680397,
            "fpr": 0.1337719298245614,
            "logloss": 3.6328925731389177,
            "mae": 0.34960723853626063,
            "precision": 0.7038834951456311,
            "recall": 0.6196581196581197
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7200712048058752,
            "auditor_fn_violation": 0.0332560881408302,
            "auditor_fp_violation": 0.012932136630722544,
            "ave_precision_score": 0.7186124657651671,
            "fpr": 0.1141602634467618,
            "logloss": 3.3984837721470793,
            "mae": 0.3355560885431552,
            "precision": 0.7463414634146341,
            "recall": 0.6296296296296297
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.7526922663117996,
            "auditor_fn_violation": 0.0299613885140201,
            "auditor_fp_violation": 0.01519776355302671,
            "ave_precision_score": 0.7534413646954801,
            "fpr": 0.11074561403508772,
            "logloss": 3.23391217143167,
            "mae": 0.3468023820335489,
            "precision": 0.7262872628726287,
            "recall": 0.5726495726495726
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.778952582416541,
            "auditor_fn_violation": 0.03313863931012364,
            "auditor_fp_violation": 0.011558080971137084,
            "ave_precision_score": 0.7795372803960017,
            "fpr": 0.09220636663007684,
            "logloss": 3.225354054278229,
            "mae": 0.3331197059442382,
            "precision": 0.772972972972973,
            "recall": 0.588477366255144
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 9271,
        "test": {
            "accuracy": 0.5328947368421053,
            "auc_prc": 0.5430662560130974,
            "auditor_fn_violation": 0.010964912280701773,
            "auditor_fp_violation": 0.009898055950687535,
            "ave_precision_score": 0.5202695863215572,
            "fpr": 0.20175438596491227,
            "logloss": 8.062607326421421,
            "mae": 0.47665396924225373,
            "precision": 0.551219512195122,
            "recall": 0.4829059829059829
        },
        "train": {
            "accuracy": 0.5214050493962679,
            "auc_prc": 0.5776443104791876,
            "auditor_fn_violation": 0.01951457494816441,
            "auditor_fp_violation": 0.01224510880092981,
            "ave_precision_score": 0.5544871650330131,
            "fpr": 0.20197585071350166,
            "logloss": 8.064182253686067,
            "mae": 0.48054582373965943,
            "precision": 0.5598086124401914,
            "recall": 0.48148148148148145
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6469298245614035,
            "auc_prc": 0.7228616423672125,
            "auditor_fn_violation": 0.02676563202878993,
            "auditor_fp_violation": 0.01920341394025605,
            "ave_precision_score": 0.7275426489756274,
            "fpr": 0.10526315789473684,
            "logloss": 5.778234321100365,
            "mae": 0.364061857946662,
            "precision": 0.7159763313609467,
            "recall": 0.5170940170940171
        },
        "train": {
            "accuracy": 0.6630076838638859,
            "auc_prc": 0.7510742266296961,
            "auditor_fn_violation": 0.03486197503760623,
            "auditor_fp_violation": 0.011519338800284112,
            "ave_precision_score": 0.7532938414304166,
            "fpr": 0.09440175631174534,
            "logloss": 5.588780126844735,
            "mae": 0.35063033060034277,
            "precision": 0.7549857549857549,
            "recall": 0.5452674897119342
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.6672957568828315,
            "auditor_fn_violation": 0.035303268855900446,
            "auditor_fp_violation": 0.014876718824087247,
            "ave_precision_score": 0.6638500579728688,
            "fpr": 0.12719298245614036,
            "logloss": 4.239727274648837,
            "mae": 0.3530864418881953,
            "precision": 0.7055837563451777,
            "recall": 0.594017094017094
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.7134405691717481,
            "auditor_fn_violation": 0.03495683755471535,
            "auditor_fp_violation": 0.013655323819978043,
            "ave_precision_score": 0.7118039876732046,
            "fpr": 0.1119648737650933,
            "logloss": 4.047982194217439,
            "mae": 0.342705417180949,
            "precision": 0.7404580152671756,
            "recall": 0.5987654320987654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.6727197196925073,
            "auditor_fn_violation": 0.03207939721097616,
            "auditor_fp_violation": 0.013365339023233771,
            "ave_precision_score": 0.6688399729830921,
            "fpr": 0.12828947368421054,
            "logloss": 3.8050216539789443,
            "mae": 0.34892509067657074,
            "precision": 0.706766917293233,
            "recall": 0.6025641025641025
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7165298704000764,
            "auditor_fn_violation": 0.03213806561775827,
            "auditor_fp_violation": 0.01486407955059082,
            "ave_precision_score": 0.7137185967266655,
            "fpr": 0.11525795828759605,
            "logloss": 3.604705535556567,
            "mae": 0.33810690304346036,
            "precision": 0.7388059701492538,
            "recall": 0.6111111111111112
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 9271,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.7689178843908306,
            "auditor_fn_violation": 0.03289473684210526,
            "auditor_fp_violation": 0.025846570254464993,
            "ave_precision_score": 0.7640911288153849,
            "fpr": 0.12938596491228072,
            "logloss": 1.2879824849713173,
            "mae": 0.30092222509515715,
            "precision": 0.7551867219917012,
            "recall": 0.7777777777777778
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.7654138442070647,
            "auditor_fn_violation": 0.02120177257389113,
            "auditor_fp_violation": 0.03076128365726093,
            "ave_precision_score": 0.7603539780781798,
            "fpr": 0.13172338090010977,
            "logloss": 1.4603752532160887,
            "mae": 0.3087545237885828,
            "precision": 0.7575757575757576,
            "recall": 0.7716049382716049
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.6551123816086499,
            "auditor_fn_violation": 0.019736842105263174,
            "auditor_fp_violation": 0.017855026078710295,
            "ave_precision_score": 0.6566466401519695,
            "fpr": 0.14692982456140352,
            "logloss": 3.2375760902929054,
            "mae": 0.3816306495942285,
            "precision": 0.6616161616161617,
            "recall": 0.5598290598290598
        },
        "train": {
            "accuracy": 0.6531284302963776,
            "auc_prc": 0.703130910510372,
            "auditor_fn_violation": 0.03503137238958681,
            "auditor_fp_violation": 0.011273971718215276,
            "ave_precision_score": 0.7040574862783788,
            "fpr": 0.13611416026344675,
            "logloss": 3.128832592320438,
            "mae": 0.36940923576902407,
            "precision": 0.7033492822966507,
            "recall": 0.6049382716049383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 9271,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.743519822568462,
            "auditor_fn_violation": 0.021704903283850652,
            "auditor_fp_violation": 0.021322309151256515,
            "ave_precision_score": 0.734246090346258,
            "fpr": 0.18311403508771928,
            "logloss": 1.4601998299719672,
            "mae": 0.2903534006197864,
            "precision": 0.6990990990990991,
            "recall": 0.8290598290598291
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7489871591265859,
            "auditor_fn_violation": 0.015372245034398956,
            "auditor_fp_violation": 0.0300871698844192,
            "ave_precision_score": 0.7404169403641697,
            "fpr": 0.18331503841931943,
            "logloss": 1.451952001598876,
            "mae": 0.29028547096329543,
            "precision": 0.7085514834205934,
            "recall": 0.8353909465020576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.6717938020636312,
            "auditor_fn_violation": 0.03211454116059379,
            "auditor_fp_violation": 0.010727833096254153,
            "ave_precision_score": 0.6691618740064946,
            "fpr": 0.13157894736842105,
            "logloss": 3.6963964265034877,
            "mae": 0.34970094353770387,
            "precision": 0.7037037037037037,
            "recall": 0.6089743589743589
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7190685720613694,
            "auditor_fn_violation": 0.03259205052106625,
            "auditor_fp_violation": 0.015920449409181896,
            "ave_precision_score": 0.717941405914031,
            "fpr": 0.11306256860592755,
            "logloss": 3.4823700338184236,
            "mae": 0.3370919714506064,
            "precision": 0.745679012345679,
            "recall": 0.6213991769547325
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.7655969551925996,
            "auditor_fn_violation": 0.022674876293297355,
            "auditor_fp_violation": 0.014728544333807493,
            "ave_precision_score": 0.7663403436059018,
            "fpr": 0.1118421052631579,
            "logloss": 2.810965094011667,
            "mae": 0.3389109064194584,
            "precision": 0.7287234042553191,
            "recall": 0.5854700854700855
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7884031044962001,
            "auditor_fn_violation": 0.029208620744173873,
            "auditor_fp_violation": 0.014055659585458773,
            "ave_precision_score": 0.7889797321394827,
            "fpr": 0.09440175631174534,
            "logloss": 2.738582844433645,
            "mae": 0.3287907737301671,
            "precision": 0.7675675675675676,
            "recall": 0.5843621399176955
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.6755628932488713,
            "auditor_fn_violation": 0.028471285050232418,
            "auditor_fp_violation": 0.011809506875296355,
            "ave_precision_score": 0.6716606329832038,
            "fpr": 0.1337719298245614,
            "logloss": 3.5733766366077067,
            "mae": 0.3481923713170856,
            "precision": 0.7053140096618358,
            "recall": 0.6239316239316239
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7206444262213583,
            "auditor_fn_violation": 0.030482488831067932,
            "auditor_fp_violation": 0.012960547556014721,
            "ave_precision_score": 0.7189100471818329,
            "fpr": 0.11306256860592755,
            "logloss": 3.3282925239716157,
            "mae": 0.3337819527102932,
            "precision": 0.7469287469287469,
            "recall": 0.6255144032921811
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.6753051968086636,
            "auditor_fn_violation": 0.02948108786924577,
            "auditor_fp_violation": 0.011883594120436224,
            "ave_precision_score": 0.6719256191012142,
            "fpr": 0.13815789473684212,
            "logloss": 3.718412852095366,
            "mae": 0.35225027966275485,
            "precision": 0.6949152542372882,
            "recall": 0.6132478632478633
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.719866429688016,
            "auditor_fn_violation": 0.032433946325884366,
            "auditor_fp_violation": 0.011718215277329375,
            "ave_precision_score": 0.7177139296651838,
            "fpr": 0.12184412733260154,
            "logloss": 3.5007379886056396,
            "mae": 0.33608889356651767,
            "precision": 0.7350835322195705,
            "recall": 0.6337448559670782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.6705821596865029,
            "auditor_fn_violation": 0.03224574523916629,
            "auditor_fp_violation": 0.012258969495811602,
            "ave_precision_score": 0.6679697152929116,
            "fpr": 0.13048245614035087,
            "logloss": 3.7459276572199083,
            "mae": 0.35038064677950165,
            "precision": 0.7039800995024875,
            "recall": 0.6047008547008547
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7176573007154673,
            "auditor_fn_violation": 0.03359940010751085,
            "auditor_fp_violation": 0.015920449409181896,
            "ave_precision_score": 0.7165642477148176,
            "fpr": 0.11306256860592755,
            "logloss": 3.536614596407537,
            "mae": 0.3389956262523866,
            "precision": 0.745049504950495,
            "recall": 0.6193415637860082
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.6737008842886115,
            "auditor_fn_violation": 0.029624006597690815,
            "auditor_fp_violation": 0.01144894894894895,
            "ave_precision_score": 0.6715909542506561,
            "fpr": 0.12719298245614036,
            "logloss": 3.6613720633715827,
            "mae": 0.35022223295946514,
            "precision": 0.71,
            "recall": 0.6068376068376068
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7185595271706278,
            "auditor_fn_violation": 0.033811711455326535,
            "auditor_fp_violation": 0.0075547233163298306,
            "ave_precision_score": 0.7178704275860244,
            "fpr": 0.11525795828759605,
            "logloss": 3.4797263737483153,
            "mae": 0.3397812699385726,
            "precision": 0.7426470588235294,
            "recall": 0.6234567901234568
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 9271,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.658659339521916,
            "auditor_fn_violation": 0.0008996851102114403,
            "auditor_fp_violation": 0.001908981349770824,
            "ave_precision_score": 0.6594854340197769,
            "fpr": 0.03837719298245614,
            "logloss": 3.9419528634910375,
            "mae": 0.4436909064974273,
            "precision": 0.7666666666666667,
            "recall": 0.24572649572649571
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.6812413519981388,
            "auditor_fn_violation": 0.0071011369950265005,
            "auditor_fp_violation": 0.007105314134435333,
            "ave_precision_score": 0.6820668848306393,
            "fpr": 0.025246981339187707,
            "logloss": 4.054037875725935,
            "mae": 0.45205245653959536,
            "precision": 0.8368794326241135,
            "recall": 0.24279835390946503
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7846006314589226,
            "auditor_fn_violation": 0.006857756035387614,
            "auditor_fp_violation": 0.01565710447289395,
            "ave_precision_score": 0.7853271691979502,
            "fpr": 0.12938596491228072,
            "logloss": 2.5034684427898237,
            "mae": 0.3164951185352659,
            "precision": 0.7249417249417249,
            "recall": 0.6645299145299145
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.800242037390702,
            "auditor_fn_violation": 0.01693973519805939,
            "auditor_fp_violation": 0.02243171692387164,
            "ave_precision_score": 0.8008776089021507,
            "fpr": 0.12294182217343579,
            "logloss": 2.3714195825375444,
            "mae": 0.30383966242866917,
            "precision": 0.7494407158836689,
            "recall": 0.6893004115226338
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 9271,
        "test": {
            "accuracy": 0.6535087719298246,
            "auc_prc": 0.729014431184362,
            "auditor_fn_violation": 0.029563090418353585,
            "auditor_fp_violation": 0.013760470997313104,
            "ave_precision_score": 0.7337764848483397,
            "fpr": 0.09978070175438597,
            "logloss": 5.786772242902497,
            "mae": 0.3637514840508966,
            "precision": 0.7275449101796407,
            "recall": 0.5192307692307693
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.7540390751171702,
            "auditor_fn_violation": 0.04075474425517116,
            "auditor_fp_violation": 0.0069425970168528455,
            "ave_precision_score": 0.7564525468598697,
            "fpr": 0.09001097694840834,
            "logloss": 5.613815125894212,
            "mae": 0.35383519402021374,
            "precision": 0.7636887608069164,
            "recall": 0.5452674897119342
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7757407530474035,
            "auditor_fn_violation": 0.008814360238682248,
            "auditor_fp_violation": 0.017528232695753647,
            "ave_precision_score": 0.6818969622307544,
            "fpr": 0.29276315789473684,
            "logloss": 6.042027007627936,
            "mae": 0.3315995257689902,
            "precision": 0.6296809986130375,
            "recall": 0.924643584521385
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.7513918903972366,
            "auditor_fn_violation": 0.01118558155303668,
            "auditor_fp_violation": 0.020513172338090014,
            "ave_precision_score": 0.6448429260573145,
            "fpr": 0.2996706915477497,
            "logloss": 6.820101133466071,
            "mae": 0.3431813598297167,
            "precision": 0.6105563480741797,
            "recall": 0.9244060475161987
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.7708473920513003,
            "auditor_fn_violation": 0.004928627577089362,
            "auditor_fp_violation": 0.015481101804392222,
            "ave_precision_score": 0.6706568441307156,
            "fpr": 0.3519736842105263,
            "logloss": 6.4689784635093215,
            "mae": 0.3762327178455204,
            "precision": 0.5967336683417085,
            "recall": 0.9674134419551935
        },
        "train": {
            "accuracy": 0.5971459934138309,
            "auc_prc": 0.7455417050931363,
            "auditor_fn_violation": 0.0002797580803854024,
            "auditor_fp_violation": 0.020770444566410536,
            "ave_precision_score": 0.6306246036099946,
            "fpr": 0.3918770581778266,
            "logloss": 7.300179143438651,
            "mae": 0.4023724465896032,
            "precision": 0.5592592592592592,
            "recall": 0.978401727861771
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7593197306844339,
            "auditor_fn_violation": 0.009562475435023404,
            "auditor_fp_violation": 0.024055090219610786,
            "ave_precision_score": 0.7567331430717728,
            "fpr": 0.18969298245614036,
            "logloss": 1.252994291433715,
            "mae": 0.28780457396755454,
            "precision": 0.6996527777777778,
            "recall": 0.8207739307535642
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7257148772773205,
            "auditor_fn_violation": 0.005720815660762508,
            "auditor_fp_violation": 0.01563235063509488,
            "ave_precision_score": 0.724090418277888,
            "fpr": 0.1942919868276619,
            "logloss": 1.455440656862775,
            "mae": 0.2835989022130641,
            "precision": 0.6916376306620209,
            "recall": 0.857451403887689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 14289,
        "test": {
            "accuracy": 0.5910087719298246,
            "auc_prc": 0.7658233076085271,
            "auditor_fn_violation": 0.0015766248615428598,
            "auditor_fp_violation": 0.005485060632579077,
            "ave_precision_score": 0.6627373998359081,
            "fpr": 0.40021929824561403,
            "logloss": 6.458661021498103,
            "mae": 0.39331575923445106,
            "precision": 0.5695754716981132,
            "recall": 0.9837067209775967
        },
        "train": {
            "accuracy": 0.562019758507135,
            "auc_prc": 0.7432607869898656,
            "auditor_fn_violation": 0.00010194574115739251,
            "auditor_fp_violation": 0.0103129410381057,
            "ave_precision_score": 0.625059024775317,
            "fpr": 0.43578485181119647,
            "logloss": 7.415685883681996,
            "mae": 0.42136126839548516,
            "precision": 0.5372960372960373,
            "recall": 0.9956803455723542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.7715681245456171,
            "auditor_fn_violation": 0.005938024797227283,
            "auditor_fp_violation": 0.017819935825311498,
            "ave_precision_score": 0.6710241353235937,
            "fpr": 0.32346491228070173,
            "logloss": 6.413138872963456,
            "mae": 0.3459342425836886,
            "precision": 0.6173800259403373,
            "recall": 0.9694501018329938
        },
        "train": {
            "accuracy": 0.6180021953896817,
            "auc_prc": 0.7462017478263939,
            "auditor_fn_violation": 0.0019203732636625074,
            "auditor_fp_violation": 0.021701525011761015,
            "ave_precision_score": 0.6291265513752372,
            "fpr": 0.3677277716794731,
            "logloss": 7.365110490949069,
            "mae": 0.3785530879796418,
            "precision": 0.5732484076433121,
            "recall": 0.9719222462203023
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.7714210563550182,
            "auditor_fn_violation": 0.005768303140743917,
            "auditor_fp_violation": 0.01874713505854899,
            "ave_precision_score": 0.6712259131273558,
            "fpr": 0.3168859649122807,
            "logloss": 6.425181781312449,
            "mae": 0.34444998668966426,
            "precision": 0.6187335092348285,
            "recall": 0.955193482688391
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.7459707028209083,
            "auditor_fn_violation": 0.0011948989196122275,
            "auditor_fp_violation": 0.026579896503057875,
            "ave_precision_score": 0.6299021618414634,
            "fpr": 0.3600439077936334,
            "logloss": 7.28896563909656,
            "mae": 0.37665727619427,
            "precision": 0.5767741935483871,
            "recall": 0.9654427645788337
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7263701075129804,
            "auditor_fn_violation": 0.0306191267374138,
            "auditor_fp_violation": 0.07246218277284662,
            "ave_precision_score": 0.6222172823099819,
            "fpr": 0.26864035087719296,
            "logloss": 6.84771133757657,
            "mae": 0.3287717463556778,
            "precision": 0.6407624633431085,
            "recall": 0.890020366598778
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.7068957760677306,
            "auditor_fn_violation": 0.021515293046589203,
            "auditor_fp_violation": 0.07562333385604517,
            "ave_precision_score": 0.5914861426274605,
            "fpr": 0.2897914379802415,
            "logloss": 7.510777094138502,
            "mae": 0.3401461579383192,
            "precision": 0.6168359941944848,
            "recall": 0.91792656587473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.7412293138326579,
            "auditor_fn_violation": 0.021384928716904283,
            "auditor_fp_violation": 0.007232674917698045,
            "ave_precision_score": 0.7100836970925226,
            "fpr": 0.16885964912280702,
            "logloss": 4.169478864869682,
            "mae": 0.3373164310269384,
            "precision": 0.6882591093117408,
            "recall": 0.6924643584521385
        },
        "train": {
            "accuracy": 0.6630076838638859,
            "auc_prc": 0.7306170918721542,
            "auditor_fn_violation": 0.013072763180043292,
            "auditor_fp_violation": 0.023220656264701267,
            "ave_precision_score": 0.6953005629778217,
            "fpr": 0.1942919868276619,
            "logloss": 4.427379808420251,
            "mae": 0.3413360092867949,
            "precision": 0.6529411764705882,
            "recall": 0.7192224622030238
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.7713940274800624,
            "auditor_fn_violation": 0.005768303140743917,
            "auditor_fp_violation": 0.01866900029170314,
            "ave_precision_score": 0.670634448765351,
            "fpr": 0.3157894736842105,
            "logloss": 6.461041645250965,
            "mae": 0.34411069697784274,
            "precision": 0.619550858652576,
            "recall": 0.955193482688391
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.7457941406282237,
            "auditor_fn_violation": 0.000455199588423706,
            "auditor_fp_violation": 0.02068468715697036,
            "ave_precision_score": 0.6297259010911649,
            "fpr": 0.3611416026344676,
            "logloss": 7.311553827938783,
            "mae": 0.37792227987059085,
            "precision": 0.5765765765765766,
            "recall": 0.9676025917926566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7786141761553423,
            "auditor_fn_violation": 0.003724943723871799,
            "auditor_fp_violation": 0.009188648581072654,
            "ave_precision_score": 0.6829376656334739,
            "fpr": 0.31030701754385964,
            "logloss": 5.991204057478463,
            "mae": 0.3344027236837479,
            "precision": 0.6266490765171504,
            "recall": 0.9674134419551935
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.7560889034629651,
            "auditor_fn_violation": 0.003264634548226264,
            "auditor_fp_violation": 0.02022894778108831,
            "ave_precision_score": 0.6469898421858618,
            "fpr": 0.3336992316136114,
            "logloss": 6.712351133569902,
            "mae": 0.3574613091087744,
            "precision": 0.5924932975871313,
            "recall": 0.9546436285097192
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 14289,
        "test": {
            "accuracy": 0.5537280701754386,
            "auc_prc": 0.7682550161414138,
            "auditor_fn_violation": 0.001648086611641119,
            "auditor_fp_violation": 0.004557861399341591,
            "ave_precision_score": 0.5466869866883288,
            "fpr": 0.44298245614035087,
            "logloss": 15.113039976970317,
            "mae": 0.4477683338340157,
            "precision": 0.547085201793722,
            "recall": 0.9938900203665988
        },
        "train": {
            "accuracy": 0.5257958287596048,
            "auc_prc": 0.7581140297398742,
            "auditor_fn_violation": 0.001156965620576918,
            "auditor_fp_violation": 0.007110514348439702,
            "ave_precision_score": 0.5203921217645575,
            "fpr": 0.47200878155872666,
            "logloss": 16.070933340837833,
            "mae": 0.47423912714368766,
            "precision": 0.5173961840628507,
            "recall": 0.9956803455723542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.7742493038627687,
            "auditor_fn_violation": 0.004694143709579448,
            "auditor_fp_violation": 0.013881943576280373,
            "ave_precision_score": 0.6756856420749915,
            "fpr": 0.31359649122807015,
            "logloss": 6.257007979443318,
            "mae": 0.34226825865537475,
            "precision": 0.6226912928759895,
            "recall": 0.9613034623217923
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.7457862383884298,
            "auditor_fn_violation": 0.00044808709485458557,
            "auditor_fp_violation": 0.017741982907323206,
            "ave_precision_score": 0.6304420561104519,
            "fpr": 0.36553238199780463,
            "logloss": 7.264326336057362,
            "mae": 0.3796703506206928,
            "precision": 0.5741687979539642,
            "recall": 0.9697624190064795
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.7715265453224063,
            "auditor_fn_violation": 0.003916997177260871,
            "auditor_fp_violation": 0.013272492394882702,
            "ave_precision_score": 0.6698272524813127,
            "fpr": 0.34539473684210525,
            "logloss": 6.5182308901652775,
            "mae": 0.3631464514856545,
            "precision": 0.6032745591939547,
            "recall": 0.9755600814663951
        },
        "train": {
            "accuracy": 0.605927552140505,
            "auc_prc": 0.7464753878469599,
            "auditor_fn_violation": 0.0008250492540179664,
            "auditor_fp_violation": 0.022777167947310654,
            "ave_precision_score": 0.629973577685679,
            "fpr": 0.38419319429198684,
            "logloss": 7.366439853586205,
            "mae": 0.39738987230851813,
            "precision": 0.5646766169154229,
            "recall": 0.980561555075594
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7594937724487869,
            "auditor_fn_violation": 0.007447654268053026,
            "auditor_fp_violation": 0.027206525815726967,
            "ave_precision_score": 0.7568906593919736,
            "fpr": 0.23903508771929824,
            "logloss": 1.6045765982304105,
            "mae": 0.29775116090966375,
            "precision": 0.6726726726726727,
            "recall": 0.9124236252545825
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7237767260042389,
            "auditor_fn_violation": 0.006714193929249657,
            "auditor_fp_violation": 0.027425219538968176,
            "ave_precision_score": 0.722301794878564,
            "fpr": 0.2535675082327113,
            "logloss": 2.038216594689639,
            "mae": 0.3135457251293689,
            "precision": 0.6467889908256881,
            "recall": 0.9136069114470843
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7591124793469087,
            "auditor_fn_violation": 0.008662504019723445,
            "auditor_fp_violation": 0.02387277576363712,
            "ave_precision_score": 0.7565415928932646,
            "fpr": 0.21052631578947367,
            "logloss": 1.1878122798562292,
            "mae": 0.2878931270477575,
            "precision": 0.6888168557536467,
            "recall": 0.8655804480651731
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7262445253958323,
            "auditor_fn_violation": 0.006910972917995319,
            "auditor_fp_violation": 0.019141053787047214,
            "ave_precision_score": 0.7247255351825229,
            "fpr": 0.21514818880351264,
            "logloss": 1.402122035905802,
            "mae": 0.2904211497528092,
            "precision": 0.679214402618658,
            "recall": 0.896328293736501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 14289,
        "test": {
            "accuracy": 0.5888157894736842,
            "auc_prc": 0.7637224571631898,
            "auditor_fn_violation": 0.0016905170257619612,
            "auditor_fp_violation": 0.005466829186981718,
            "ave_precision_score": 0.6610808773061524,
            "fpr": 0.40460526315789475,
            "logloss": 6.573580603509776,
            "mae": 0.39942544968793653,
            "precision": 0.5679156908665105,
            "recall": 0.9877800407331976
        },
        "train": {
            "accuracy": 0.5598243688254665,
            "auc_prc": 0.7422628030098902,
            "auditor_fn_violation": 0.00010194574115739251,
            "auditor_fp_violation": 0.009141739846322725,
            "ave_precision_score": 0.6243237261150434,
            "fpr": 0.43798024149286496,
            "logloss": 7.530762125810268,
            "mae": 0.4263855244134503,
            "precision": 0.536046511627907,
            "recall": 0.9956803455723542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 14289,
        "test": {
            "accuracy": 0.555921052631579,
            "auc_prc": 0.7670413961250067,
            "auditor_fn_violation": 0.001648086611641119,
            "auditor_fp_violation": 0.0028987998499812725,
            "ave_precision_score": 0.5470158575785966,
            "fpr": 0.4407894736842105,
            "logloss": 15.040999989717122,
            "mae": 0.44504377371808745,
            "precision": 0.5483146067415731,
            "recall": 0.9938900203665988
        },
        "train": {
            "accuracy": 0.5246981339187706,
            "auc_prc": 0.7579063658183451,
            "auditor_fn_violation": 0.001156965620576918,
            "auditor_fp_violation": 0.005579132037008009,
            "ave_precision_score": 0.52101875643099,
            "fpr": 0.47310647639956094,
            "logloss": 16.02001598797807,
            "mae": 0.4741522898735054,
            "precision": 0.5168161434977578,
            "recall": 0.9956803455723542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 14289,
        "test": {
            "accuracy": 0.5657894736842105,
            "auc_prc": 0.7605586875946674,
            "auditor_fn_violation": 0.0007615142744845826,
            "auditor_fp_violation": 0.001870025419844157,
            "ave_precision_score": 0.6562300358585407,
            "fpr": 0.4298245614035088,
            "logloss": 6.804387277204571,
            "mae": 0.42290026508271694,
            "precision": 0.5540386803185438,
            "recall": 0.9918533604887984
        },
        "train": {
            "accuracy": 0.5389681668496158,
            "auc_prc": 0.7386807308077745,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.003841931942919872,
            "ave_precision_score": 0.6184451336806042,
            "fpr": 0.4610318331503842,
            "logloss": 7.846343480522783,
            "mae": 0.4473734101164027,
            "precision": 0.5243488108720272,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.7548547024015454,
            "auditor_fn_violation": 0.006089881016186087,
            "auditor_fp_violation": 0.008047880985123152,
            "ave_precision_score": 0.6821715577387268,
            "fpr": 0.31359649122807015,
            "logloss": 5.0291468267706,
            "mae": 0.3436841922220314,
            "precision": 0.6221928665785997,
            "recall": 0.9592668024439919
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7461399731959809,
            "auditor_fn_violation": 0.0017781233922801,
            "auditor_fp_violation": 0.012883213109612672,
            "ave_precision_score": 0.6615140781578465,
            "fpr": 0.3578485181119649,
            "logloss": 5.66657661703018,
            "mae": 0.37587805995317825,
            "precision": 0.5777202072538861,
            "recall": 0.9632829373650108
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.7708395932190578,
            "auditor_fn_violation": 0.00531050130417694,
            "auditor_fp_violation": 0.013631912322373632,
            "ave_precision_score": 0.6703926255832514,
            "fpr": 0.3267543859649123,
            "logloss": 6.40363101340183,
            "mae": 0.34532211650449635,
            "precision": 0.6144890038809832,
            "recall": 0.9674134419551935
        },
        "train": {
            "accuracy": 0.6212952799121844,
            "auc_prc": 0.7458220597723364,
            "auditor_fn_violation": 0.000547662004822271,
            "auditor_fp_violation": 0.024747138152736416,
            "ave_precision_score": 0.6293193070198569,
            "fpr": 0.3633369923161361,
            "logloss": 7.3364890916247,
            "mae": 0.37713690248177,
            "precision": 0.5756410256410256,
            "recall": 0.9697624190064795
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6546052631578947,
            "auc_prc": 0.7708265039307458,
            "auditor_fn_violation": 0.00577946903919677,
            "auditor_fp_violation": 0.013631912322373632,
            "ave_precision_score": 0.6703795569546729,
            "fpr": 0.3267543859649123,
            "logloss": 6.403982584513059,
            "mae": 0.34529591166197826,
            "precision": 0.6139896373056994,
            "recall": 0.9653767820773931
        },
        "train": {
            "accuracy": 0.6212952799121844,
            "auc_prc": 0.7462761951842898,
            "auditor_fn_violation": 0.000547662004822271,
            "auditor_fp_violation": 0.024747138152736416,
            "ave_precision_score": 0.6302034772346929,
            "fpr": 0.3633369923161361,
            "logloss": 7.31032836072337,
            "mae": 0.37710169113178554,
            "precision": 0.5756410256410256,
            "recall": 0.9697624190064795
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.7712877311172607,
            "auditor_fn_violation": 0.005000089327187623,
            "auditor_fp_violation": 0.0185231487269242,
            "ave_precision_score": 0.6699217460303278,
            "fpr": 0.3267543859649123,
            "logloss": 6.42629359010831,
            "mae": 0.3462086977069224,
            "precision": 0.615979381443299,
            "recall": 0.9735234215885947
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.7461617880451641,
            "auditor_fn_violation": 0.0019203732636625074,
            "auditor_fp_violation": 0.02277716794731064,
            "ave_precision_score": 0.6286543400835639,
            "fpr": 0.36882546652030734,
            "logloss": 7.4030441603642325,
            "mae": 0.3829099315848912,
            "precision": 0.5725190839694656,
            "recall": 0.9719222462203023
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.7801254602430285,
            "auditor_fn_violation": 0.00717520634580341,
            "auditor_fp_violation": 0.011069091969829572,
            "ave_precision_score": 0.6854273563029457,
            "fpr": 0.2949561403508772,
            "logloss": 5.867594066795818,
            "mae": 0.3267212856173118,
            "precision": 0.6335149863760218,
            "recall": 0.9470468431771895
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.7571479749847542,
            "auditor_fn_violation": 0.005898627999990517,
            "auditor_fp_violation": 0.017729731848831757,
            "ave_precision_score": 0.6485064362012051,
            "fpr": 0.31174533479692645,
            "logloss": 6.594484200327432,
            "mae": 0.3421887519488667,
            "precision": 0.607192254495159,
            "recall": 0.9481641468682506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 14289,
        "test": {
            "accuracy": 0.5394736842105263,
            "auc_prc": 0.768133622239852,
            "auditor_fn_violation": 0.001683817486690249,
            "auditor_fp_violation": 0.0036410801350168893,
            "ave_precision_score": 0.5465656036319272,
            "fpr": 0.4550438596491228,
            "logloss": 15.13010324973622,
            "mae": 0.4589459786902329,
            "precision": 0.5394006659267481,
            "recall": 0.9898167006109979
        },
        "train": {
            "accuracy": 0.5170142700329309,
            "auc_prc": 0.7580965314109679,
            "auditor_fn_violation": 0.00045994125080311906,
            "auditor_fp_violation": 0.0026511290575505905,
            "ave_precision_score": 0.5203746240685776,
            "fpr": 0.4818880351262349,
            "logloss": 16.082170018609773,
            "mae": 0.481531372304053,
            "precision": 0.5127635960044395,
            "recall": 0.9978401727861771
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.7693684153641026,
            "auditor_fn_violation": 0.003916997177260871,
            "auditor_fp_violation": 0.005607471767304248,
            "ave_precision_score": 0.6666323476784741,
            "fpr": 0.35635964912280704,
            "logloss": 6.669759901131911,
            "mae": 0.36875367778555845,
            "precision": 0.595771144278607,
            "recall": 0.9755600814663951
        },
        "train": {
            "accuracy": 0.5982436882546652,
            "auc_prc": 0.7459182812275612,
            "auditor_fn_violation": 0.0005595161607708052,
            "auditor_fp_violation": 0.02130214050493963,
            "ave_precision_score": 0.6273886438992047,
            "fpr": 0.3929747530186608,
            "logloss": 7.567547723269167,
            "mae": 0.4004168995953437,
            "precision": 0.5596555965559655,
            "recall": 0.9827213822894169
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6546052631578947,
            "auc_prc": 0.7462012027063526,
            "auditor_fn_violation": 0.004517722514024369,
            "auditor_fp_violation": 0.015298787348418575,
            "ave_precision_score": 0.6773198973874858,
            "fpr": 0.3168859649122807,
            "logloss": 4.717277678540217,
            "mae": 0.35892514904691397,
            "precision": 0.616710875331565,
            "recall": 0.9470468431771895
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7373770789872363,
            "auditor_fn_violation": 0.0031342388327923894,
            "auditor_fp_violation": 0.010940195232868146,
            "ave_precision_score": 0.6576114147584029,
            "fpr": 0.3545554335894621,
            "logloss": 5.23072302741212,
            "mae": 0.3829353660385714,
            "precision": 0.5783289817232375,
            "recall": 0.9568034557235421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.731507256270528,
            "auditor_fn_violation": 0.0008776396183942584,
            "auditor_fp_violation": 0.026198587323415432,
            "ave_precision_score": 0.7061188955940402,
            "fpr": 0.18640350877192982,
            "logloss": 2.754290272378422,
            "mae": 0.3047624471120108,
            "precision": 0.6947935368043088,
            "recall": 0.7881873727087576
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.6868817404377061,
            "auditor_fn_violation": 0.008634567192912165,
            "auditor_fp_violation": 0.030130253253881144,
            "ave_precision_score": 0.6559504716588345,
            "fpr": 0.2052689352360044,
            "logloss": 3.1134328497515087,
            "mae": 0.30368913634237443,
            "precision": 0.6730769230769231,
            "recall": 0.8315334773218143
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.7804447742848422,
            "auditor_fn_violation": 0.004080019294672527,
            "auditor_fp_violation": 0.01385329416177023,
            "ave_precision_score": 0.6857450018549534,
            "fpr": 0.3223684210526316,
            "logloss": 5.85936278096755,
            "mae": 0.3421657151031883,
            "precision": 0.6181818181818182,
            "recall": 0.9694501018329938
        },
        "train": {
            "accuracy": 0.6399560922063666,
            "auc_prc": 0.7566849844010406,
            "auditor_fn_violation": 0.002292593760446476,
            "auditor_fp_violation": 0.02657989650305789,
            "ave_precision_score": 0.6480437081571863,
            "fpr": 0.3424807903402854,
            "logloss": 6.655359713604338,
            "mae": 0.3666973820915843,
            "precision": 0.5889328063241107,
            "recall": 0.9654427645788337
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.7580618420925774,
            "auditor_fn_violation": 0.006862561189123522,
            "auditor_fp_violation": 0.00617264658082262,
            "ave_precision_score": 0.679127476626918,
            "fpr": 0.28289473684210525,
            "logloss": 5.269925662903683,
            "mae": 0.3260438700854078,
            "precision": 0.6406685236768802,
            "recall": 0.9368635437881874
        },
        "train": {
            "accuracy": 0.6575192096597146,
            "auc_prc": 0.744925863931873,
            "auditor_fn_violation": 0.0015054778054638185,
            "auditor_fp_violation": 0.014216128273482847,
            "ave_precision_score": 0.6526732656918154,
            "fpr": 0.31284302963776073,
            "logloss": 5.915419516800367,
            "mae": 0.3474397460087286,
            "precision": 0.6047156726768377,
            "recall": 0.9416846652267818
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7534790887518583,
            "auditor_fn_violation": 0.006398059813484833,
            "auditor_fp_violation": 0.02049735383589616,
            "ave_precision_score": 0.7500333175605967,
            "fpr": 0.11842105263157894,
            "logloss": 1.713212159329651,
            "mae": 0.3010342432794006,
            "precision": 0.7605321507760532,
            "recall": 0.6985743380855397
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7450975173816422,
            "auditor_fn_violation": 0.01681156396621091,
            "auditor_fp_violation": 0.022953583189587578,
            "ave_precision_score": 0.7431086279621456,
            "fpr": 0.1141602634467618,
            "logloss": 1.484703902955989,
            "mae": 0.2861803221809215,
            "precision": 0.7564402810304449,
            "recall": 0.6976241900647948
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7680336115239159,
            "auditor_fn_violation": 0.011366884625004467,
            "auditor_fp_violation": 0.00135433595866151,
            "ave_precision_score": 0.7096468710908042,
            "fpr": 0.20285087719298245,
            "logloss": 3.9476646799293174,
            "mae": 0.29290551715466134,
            "precision": 0.6942148760330579,
            "recall": 0.8553971486761711
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7692387337136778,
            "auditor_fn_violation": 0.002752535011249594,
            "auditor_fp_violation": 0.009984612670534743,
            "ave_precision_score": 0.7035098574417932,
            "fpr": 0.21624588364434688,
            "logloss": 4.115649163085992,
            "mae": 0.29625547871832264,
            "precision": 0.6677908937605397,
            "recall": 0.8552915766738661
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6502192982456141,
            "auc_prc": 0.770781527499721,
            "auditor_fn_violation": 0.00523903955407868,
            "auditor_fp_violation": 0.016642705338167273,
            "ave_precision_score": 0.6699073383780232,
            "fpr": 0.33223684210526316,
            "logloss": 6.654748035836753,
            "mae": 0.35644145505839275,
            "precision": 0.6105398457583547,
            "recall": 0.9674134419551935
        },
        "train": {
            "accuracy": 0.610318331503842,
            "auc_prc": 0.7490877845700076,
            "auditor_fn_violation": 0.0019369690819904556,
            "auditor_fp_violation": 0.022039654226125138,
            "ave_precision_score": 0.6306964965103411,
            "fpr": 0.3765093304061471,
            "logloss": 7.6451685455869365,
            "mae": 0.39273937765145683,
            "precision": 0.5680100755667506,
            "recall": 0.9740820734341252
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.7581257742639005,
            "auditor_fn_violation": 0.006862561189123522,
            "auditor_fp_violation": 0.00617264658082262,
            "ave_precision_score": 0.6791930977328381,
            "fpr": 0.28289473684210525,
            "logloss": 5.26931189689276,
            "mae": 0.32601475598121116,
            "precision": 0.6406685236768802,
            "recall": 0.9368635437881874
        },
        "train": {
            "accuracy": 0.6575192096597146,
            "auc_prc": 0.7449221974416605,
            "auditor_fn_violation": 0.0015054778054638185,
            "auditor_fp_violation": 0.014216128273482847,
            "ave_precision_score": 0.652675265695955,
            "fpr": 0.31284302963776073,
            "logloss": 5.915453662900593,
            "mae": 0.3473904605827265,
            "precision": 0.6047156726768377,
            "recall": 0.9416846652267818
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.7735454393873734,
            "auditor_fn_violation": 0.005989387930110408,
            "auditor_fp_violation": 0.015457661374338464,
            "ave_precision_score": 0.6771977242960375,
            "fpr": 0.2949561403508772,
            "logloss": 6.0410278532428245,
            "mae": 0.3290697782676384,
            "precision": 0.6320109439124487,
            "recall": 0.9409368635437881
        },
        "train": {
            "accuracy": 0.6553238199780461,
            "auc_prc": 0.7503709411478172,
            "auditor_fn_violation": 0.003364209458193949,
            "auditor_fp_violation": 0.015306472479222202,
            "ave_precision_score": 0.6395821063475166,
            "fpr": 0.3194291986827662,
            "logloss": 6.854343682209123,
            "mae": 0.34958673515439354,
            "precision": 0.6019151846785226,
            "recall": 0.9503239740820735
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6535087719298246,
            "auc_prc": 0.770309565906732,
            "auditor_fn_violation": 0.00577946903919677,
            "auditor_fp_violation": 0.014238759011543114,
            "ave_precision_score": 0.6692914806796642,
            "fpr": 0.32785087719298245,
            "logloss": 6.44914731327203,
            "mae": 0.3452381096273807,
            "precision": 0.6131953428201811,
            "recall": 0.9653767820773931
        },
        "train": {
            "accuracy": 0.6212952799121844,
            "auc_prc": 0.7455754216987587,
            "auditor_fn_violation": 0.0009933782684871495,
            "auditor_fp_violation": 0.02397777167947311,
            "ave_precision_score": 0.6289237664937326,
            "fpr": 0.36223929747530187,
            "logloss": 7.370139300260027,
            "mae": 0.3783401253000576,
            "precision": 0.5758354755784062,
            "recall": 0.9676025917926566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.7697732171330467,
            "auditor_fn_violation": 0.005489155679422591,
            "auditor_fp_violation": 0.01650727174230113,
            "ave_precision_score": 0.6681835952868773,
            "fpr": 0.32456140350877194,
            "logloss": 6.514517396651591,
            "mae": 0.34492040433205895,
            "precision": 0.6160830090791181,
            "recall": 0.9674134419551935
        },
        "train": {
            "accuracy": 0.6289791437980241,
            "auc_prc": 0.7460288491504697,
            "auditor_fn_violation": 0.0007136201881017468,
            "auditor_fp_violation": 0.027559981182374172,
            "ave_precision_score": 0.6283778791003299,
            "fpr": 0.3556531284302964,
            "logloss": 7.378044960714728,
            "mae": 0.3744304934629722,
            "precision": 0.5808538163001293,
            "recall": 0.9697624190064795
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6546052631578947,
            "auc_prc": 0.7615435284907657,
            "auditor_fn_violation": 0.01008503948261693,
            "auditor_fp_violation": 0.00886048256032005,
            "ave_precision_score": 0.6611980417321158,
            "fpr": 0.3092105263157895,
            "logloss": 6.521106924792262,
            "mae": 0.35685205175233653,
            "precision": 0.6189189189189189,
            "recall": 0.9327902240325866
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7452259990614791,
            "auditor_fn_violation": 0.002961168155943793,
            "auditor_fp_violation": 0.01913125294025405,
            "ave_precision_score": 0.6297266383145638,
            "fpr": 0.3512623490669594,
            "logloss": 7.340433089802631,
            "mae": 0.3798833721203683,
            "precision": 0.5795006570302234,
            "recall": 0.9524838012958964
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7114425838680452,
            "auditor_fn_violation": 0.015408939864937294,
            "auditor_fp_violation": 0.015051360586740012,
            "ave_precision_score": 0.7070881162482511,
            "fpr": 0.16337719298245615,
            "logloss": 1.2275826570566597,
            "mae": 0.3260057866234208,
            "precision": 0.7225325884543762,
            "recall": 0.790224032586558
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.6679023118785715,
            "auditor_fn_violation": 0.0046657957813429855,
            "auditor_fp_violation": 0.018190371648110396,
            "ave_precision_score": 0.6654413987776158,
            "fpr": 0.21624588364434688,
            "logloss": 1.4696676453718551,
            "mae": 0.33730367997604815,
            "precision": 0.6632478632478632,
            "recall": 0.838012958963283
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 14289,
        "test": {
            "accuracy": 0.5789473684210527,
            "auc_prc": 0.6749305584775491,
            "auditor_fn_violation": 0.10716136063172188,
            "auditor_fp_violation": 0.09007115472767431,
            "ave_precision_score": 0.6713482578660328,
            "fpr": 0.22039473684210525,
            "logloss": 2.5725288196976592,
            "mae": 0.4176342316718132,
            "precision": 0.6051080550098232,
            "recall": 0.6272912423625254
        },
        "train": {
            "accuracy": 0.5543358946212953,
            "auc_prc": 0.6199569255405045,
            "auditor_fn_violation": 0.10875713916542001,
            "auditor_fp_violation": 0.1004145758193508,
            "ave_precision_score": 0.6135165933363873,
            "fpr": 0.24698133918770582,
            "logloss": 2.9445673852166254,
            "mae": 0.4324560966021482,
            "precision": 0.5562130177514792,
            "recall": 0.6090712742980562
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7639805210888032,
            "auditor_fn_violation": 0.00974782934934077,
            "auditor_fp_violation": 0.024055090219610786,
            "ave_precision_score": 0.7613691708624437,
            "fpr": 0.18969298245614036,
            "logloss": 1.2468832156996124,
            "mae": 0.28625805909894747,
            "precision": 0.6996527777777778,
            "recall": 0.8207739307535642
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7285948438851952,
            "auditor_fn_violation": 0.006405985874587773,
            "auditor_fp_violation": 0.01563235063509488,
            "ave_precision_score": 0.7277872693275966,
            "fpr": 0.1942919868276619,
            "logloss": 1.4476754589569012,
            "mae": 0.2832987217726669,
            "precision": 0.6905594405594405,
            "recall": 0.8531317494600432
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.7465804105006828,
            "auditor_fn_violation": 0.005469057062207454,
            "auditor_fp_violation": 0.011600408384381387,
            "ave_precision_score": 0.6582702218373776,
            "fpr": 0.3651315789473684,
            "logloss": 5.927099687374295,
            "mae": 0.37656357026443993,
            "precision": 0.5888888888888889,
            "recall": 0.9714867617107943
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.7368023205259657,
            "auditor_fn_violation": 0.0019867565369742985,
            "auditor_fp_violation": 0.008869766347812455,
            "ave_precision_score": 0.6376228362481203,
            "fpr": 0.3973655323819978,
            "logloss": 6.599905878218906,
            "mae": 0.40305731377075454,
            "precision": 0.5563725490196079,
            "recall": 0.980561555075594
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 14289,
        "test": {
            "accuracy": 0.555921052631579,
            "auc_prc": 0.7681883113076955,
            "auditor_fn_violation": 0.0031755815199914247,
            "auditor_fp_violation": 0.0053965078968204435,
            "ave_precision_score": 0.5474374338416994,
            "fpr": 0.4375,
            "logloss": 15.061327042199713,
            "mae": 0.4433896840864992,
            "precision": 0.5486425339366516,
            "recall": 0.9877800407331976
        },
        "train": {
            "accuracy": 0.531284302963776,
            "auc_prc": 0.7592930615835924,
            "auditor_fn_violation": 0.0007017660321532129,
            "auditor_fp_violation": 0.008820762113846645,
            "ave_precision_score": 0.5227463520549438,
            "fpr": 0.4654226125137212,
            "logloss": 15.91806315691191,
            "mae": 0.46905598039711854,
            "precision": 0.5203619909502263,
            "recall": 0.9935205183585313
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.723366009769907,
            "auditor_fn_violation": 0.01184031871940544,
            "auditor_fp_violation": 0.0128427511772305,
            "ave_precision_score": 0.7207594561906945,
            "fpr": 0.22916666666666666,
            "logloss": 1.2631274827489176,
            "mae": 0.32153571525847374,
            "precision": 0.6749611197511665,
            "recall": 0.8839103869653768
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7004086990822644,
            "auditor_fn_violation": 0.001550523598068248,
            "auditor_fp_violation": 0.01860935784851811,
            "ave_precision_score": 0.6971644125455418,
            "fpr": 0.27332601536772777,
            "logloss": 1.5416253529172952,
            "mae": 0.33299531658426945,
            "precision": 0.6348973607038123,
            "recall": 0.9352051835853131
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.7727891802570178,
            "auditor_fn_violation": 0.004939793475542217,
            "auditor_fp_violation": 0.017580322540317547,
            "ave_precision_score": 0.6725963712326946,
            "fpr": 0.31798245614035087,
            "logloss": 6.393274335331442,
            "mae": 0.34458015690848287,
            "precision": 0.6199213630406291,
            "recall": 0.9633401221995926
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7469566568455265,
            "auditor_fn_violation": 0.000739699331188522,
            "auditor_fp_violation": 0.020339207307511372,
            "ave_precision_score": 0.6314726234355814,
            "fpr": 0.3589462129527991,
            "logloss": 7.233999143807856,
            "mae": 0.37651788520863844,
            "precision": 0.5775193798449613,
            "recall": 0.9654427645788337
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.7712464416568334,
            "auditor_fn_violation": 0.006382427555650838,
            "auditor_fp_violation": 0.01765064383047883,
            "ave_precision_score": 0.6710522283517192,
            "fpr": 0.3168859649122807,
            "logloss": 6.425198244497268,
            "mae": 0.34417939658284846,
            "precision": 0.618229854689564,
            "recall": 0.9531568228105907
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.7439823910182748,
            "auditor_fn_violation": 0.0011948989196122275,
            "auditor_fp_violation": 0.028618472636035758,
            "ave_precision_score": 0.6272035152777963,
            "fpr": 0.3600439077936334,
            "logloss": 7.344985517955304,
            "mae": 0.3761078349789339,
            "precision": 0.5767741935483871,
            "recall": 0.9654427645788337
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.7540856194887504,
            "auditor_fn_violation": 0.009484314145853438,
            "auditor_fp_violation": 0.02444576405384007,
            "ave_precision_score": 0.7515203713877359,
            "fpr": 0.18969298245614036,
            "logloss": 1.1541188791525145,
            "mae": 0.2861631858448407,
            "precision": 0.7087542087542088,
            "recall": 0.8574338085539714
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7060964567673229,
            "auditor_fn_violation": 0.008942775247574046,
            "auditor_fp_violation": 0.015950878155872667,
            "ave_precision_score": 0.7035638414160088,
            "fpr": 0.21185510428100987,
            "logloss": 1.4889908045437912,
            "mae": 0.2970249727445238,
            "precision": 0.6756302521008404,
            "recall": 0.8682505399568035
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.7725450888746146,
            "auditor_fn_violation": 0.006382427555650838,
            "auditor_fp_violation": 0.012655227736800449,
            "ave_precision_score": 0.6728249153669266,
            "fpr": 0.31030701754385964,
            "logloss": 6.336768603374469,
            "mae": 0.339765380181208,
            "precision": 0.6231691078561917,
            "recall": 0.9531568228105907
        },
        "train": {
            "accuracy": 0.6300768386388584,
            "auc_prc": 0.7477050082228014,
            "auditor_fn_violation": 0.0010336823987121652,
            "auditor_fp_violation": 0.02162311823741573,
            "ave_precision_score": 0.6340929333653968,
            "fpr": 0.34796926454445665,
            "logloss": 7.110259090867279,
            "mae": 0.36867894208664276,
            "precision": 0.5828947368421052,
            "recall": 0.9568034557235421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.780401315790511,
            "auditor_fn_violation": 0.005574016507664274,
            "auditor_fp_violation": 0.005901779389090304,
            "ave_precision_score": 0.6861959549634074,
            "fpr": 0.30372807017543857,
            "logloss": 5.890774576667588,
            "mae": 0.3314335232173203,
            "precision": 0.6296791443850267,
            "recall": 0.9592668024439919
        },
        "train": {
            "accuracy": 0.6487376509330406,
            "auc_prc": 0.7575858140833975,
            "auditor_fn_violation": 0.003264634548226264,
            "auditor_fp_violation": 0.014813979927865772,
            "ave_precision_score": 0.6494086674903451,
            "fpr": 0.32821075740944017,
            "logloss": 6.628043605672589,
            "mae": 0.3512895131056477,
            "precision": 0.5964912280701754,
            "recall": 0.9546436285097192
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.770236300362563,
            "auditor_fn_violation": 0.009861721513559867,
            "auditor_fp_violation": 0.008540130016252031,
            "ave_precision_score": 0.6777664887782995,
            "fpr": 0.23355263157894737,
            "logloss": 5.818045795267855,
            "mae": 0.3113038328616946,
            "precision": 0.6697674418604651,
            "recall": 0.879837067209776
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.7462416586226626,
            "auditor_fn_violation": 0.017515700829553834,
            "auditor_fp_violation": 0.01257448643562805,
            "ave_precision_score": 0.6422604647444876,
            "fpr": 0.2524698133918771,
            "logloss": 6.463507613113482,
            "mae": 0.3295642322701873,
            "precision": 0.6349206349206349,
            "recall": 0.8639308855291576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6567982456140351,
            "auc_prc": 0.7709255790206883,
            "auditor_fn_violation": 0.00577946903919677,
            "auditor_fp_violation": 0.017398008084343882,
            "ave_precision_score": 0.6704845728266776,
            "fpr": 0.32456140350877194,
            "logloss": 6.385114201174562,
            "mae": 0.3445613113793099,
            "precision": 0.6155844155844156,
            "recall": 0.9653767820773931
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7477976315514157,
            "auditor_fn_violation": 0.000547662004822271,
            "auditor_fp_violation": 0.02344607574094401,
            "ave_precision_score": 0.6330322988095358,
            "fpr": 0.3611416026344676,
            "logloss": 7.227181538700961,
            "mae": 0.3753925857873771,
            "precision": 0.577120822622108,
            "recall": 0.9697624190064795
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.769570574473169,
            "auditor_fn_violation": 0.004531121592167792,
            "auditor_fp_violation": 0.014465349835396086,
            "ave_precision_score": 0.667737162306847,
            "fpr": 0.34210526315789475,
            "logloss": 6.43792221127103,
            "mae": 0.35816045441236366,
            "precision": 0.6055625790139064,
            "recall": 0.9755600814663951
        },
        "train": {
            "accuracy": 0.6114160263446762,
            "auc_prc": 0.7450232154948377,
            "auditor_fn_violation": 0.0008961741897091699,
            "auditor_fp_violation": 0.023830758977575677,
            "ave_precision_score": 0.6272415579174085,
            "fpr": 0.3754116355653128,
            "logloss": 7.387926746562174,
            "mae": 0.38979752391664174,
            "precision": 0.5687263556116016,
            "recall": 0.9740820734341252
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7714467502549012,
            "auditor_fn_violation": 0.004515489334333798,
            "auditor_fp_violation": 0.010227840980122527,
            "ave_precision_score": 0.6753994296147262,
            "fpr": 0.30701754385964913,
            "logloss": 6.0957833781374875,
            "mae": 0.33957590004206795,
            "precision": 0.6251673360107095,
            "recall": 0.9511201629327902
        },
        "train": {
            "accuracy": 0.6410537870472008,
            "auc_prc": 0.7498906452469783,
            "auditor_fn_violation": 0.004253271154333999,
            "auditor_fp_violation": 0.019209659714599356,
            "ave_precision_score": 0.6386507496412612,
            "fpr": 0.3380900109769484,
            "logloss": 6.9202927277716215,
            "mae": 0.3608927658108486,
            "precision": 0.5904255319148937,
            "recall": 0.958963282937365
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6535087719298246,
            "auc_prc": 0.7712318110258897,
            "auditor_fn_violation": 0.004939793475542217,
            "auditor_fp_violation": 0.01889298662332792,
            "ave_precision_score": 0.6704708621690537,
            "fpr": 0.3267543859649123,
            "logloss": 6.417951937520123,
            "mae": 0.354786399185793,
            "precision": 0.6134889753566797,
            "recall": 0.9633401221995926
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.7454235930715517,
            "auditor_fn_violation": 0.0013655987652711173,
            "auditor_fp_violation": 0.01952818723537715,
            "ave_precision_score": 0.6295052583483272,
            "fpr": 0.37102085620197583,
            "logloss": 7.2814461924185325,
            "mae": 0.3871477227038259,
            "precision": 0.5710659898477157,
            "recall": 0.9719222462203023
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 14289,
        "test": {
            "accuracy": 0.555921052631579,
            "auc_prc": 0.7681731852145905,
            "auditor_fn_violation": 0.0025346589487976563,
            "auditor_fp_violation": 0.007099845814060093,
            "ave_precision_score": 0.5474223092428596,
            "fpr": 0.43859649122807015,
            "logloss": 15.065713930171565,
            "mae": 0.44390372720260163,
            "precision": 0.5485327313769752,
            "recall": 0.9898167006109979
        },
        "train": {
            "accuracy": 0.5290889132821076,
            "auc_prc": 0.7592980874866474,
            "auditor_fn_violation": 0.0007017660321532129,
            "auditor_fp_violation": 0.007757370236788466,
            "ave_precision_score": 0.522751377760013,
            "fpr": 0.4676180021953897,
            "logloss": 15.91869403009364,
            "mae": 0.4695153991050885,
            "precision": 0.5191873589164786,
            "recall": 0.9935205183585313
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.765178711875803,
            "auditor_fn_violation": 0.012865348197377354,
            "auditor_fp_violation": 0.012587510938867355,
            "ave_precision_score": 0.7042453234031618,
            "fpr": 0.24780701754385964,
            "logloss": 4.294816143148624,
            "mae": 0.3061284880604879,
            "precision": 0.6606606606606606,
            "recall": 0.8961303462321792
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7550449549228797,
            "auditor_fn_violation": 0.013224496376184528,
            "auditor_fp_violation": 0.014250431237258906,
            "ave_precision_score": 0.6819195419316996,
            "fpr": 0.25905598243688255,
            "logloss": 4.784346862960031,
            "mae": 0.31941306075048936,
            "precision": 0.6380368098159509,
            "recall": 0.8984881209503239
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8029478173462375,
            "auditor_fn_violation": 0.012282488298138428,
            "auditor_fp_violation": 0.01843459599116556,
            "ave_precision_score": 0.8011115864586751,
            "fpr": 0.15350877192982457,
            "logloss": 1.1942654137568949,
            "mae": 0.28225568150072317,
            "precision": 0.7297297297297297,
            "recall": 0.769857433808554
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.7959163846313666,
            "auditor_fn_violation": 0.0035538759533704925,
            "auditor_fp_violation": 0.024325701740630392,
            "ave_precision_score": 0.7952452644826424,
            "fpr": 0.14928649835345773,
            "logloss": 1.0903827478818338,
            "mae": 0.26957756091103696,
            "precision": 0.7306930693069307,
            "recall": 0.796976241900648
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.7515459856889711,
            "auditor_fn_violation": 0.008407921534998393,
            "auditor_fp_violation": 0.01797099637454682,
            "ave_precision_score": 0.7489845790607806,
            "fpr": 0.19298245614035087,
            "logloss": 1.1451344449170524,
            "mae": 0.29062802041069025,
            "precision": 0.7046979865771812,
            "recall": 0.8553971486761711
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7170236694898269,
            "auditor_fn_violation": 0.008309763319922333,
            "auditor_fp_violation": 0.005645287752861851,
            "ave_precision_score": 0.7162427862322595,
            "fpr": 0.20965971459934138,
            "logloss": 1.3596116004144105,
            "mae": 0.29512254443780345,
            "precision": 0.6795302013422819,
            "recall": 0.8747300215982722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7439804989153538,
            "auditor_fn_violation": 0.011815753742809168,
            "auditor_fp_violation": 0.027253406675834486,
            "ave_precision_score": 0.7388030677037638,
            "fpr": 0.18311403508771928,
            "logloss": 1.527375077744178,
            "mae": 0.2972829098271735,
            "precision": 0.6969147005444646,
            "recall": 0.7820773930753564
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7081003528992367,
            "auditor_fn_violation": 0.002290222929256769,
            "auditor_fp_violation": 0.020476419162615653,
            "ave_precision_score": 0.703137273379273,
            "fpr": 0.18111964873765093,
            "logloss": 1.7624728955898847,
            "mae": 0.2937817315147862,
            "precision": 0.6944444444444444,
            "recall": 0.8099352051835853
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7863809997496232,
            "auditor_fn_violation": 0.009948815521492122,
            "auditor_fp_violation": 0.022364774763512108,
            "ave_precision_score": 0.7725218329838472,
            "fpr": 0.16557017543859648,
            "logloss": 1.9914666040453146,
            "mae": 0.284567671750955,
            "precision": 0.7188081936685289,
            "recall": 0.7861507128309573
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7428004136984071,
            "auditor_fn_violation": 0.009220162496769746,
            "auditor_fp_violation": 0.019912870472008782,
            "ave_precision_score": 0.725617169577413,
            "fpr": 0.17672886937431395,
            "logloss": 2.1410072714521697,
            "mae": 0.2834508648185555,
            "precision": 0.6990654205607477,
            "recall": 0.8077753779697624
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.7715528321208892,
            "auditor_fn_violation": 0.005768303140743917,
            "auditor_fp_violation": 0.01874713505854899,
            "ave_precision_score": 0.671359102676617,
            "fpr": 0.3168859649122807,
            "logloss": 6.422722564120101,
            "mae": 0.3441370932099945,
            "precision": 0.6187335092348285,
            "recall": 0.955193482688391
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.7460331959518443,
            "auditor_fn_violation": 0.000455199588423706,
            "auditor_fp_violation": 0.029517700329308463,
            "ave_precision_score": 0.6299690784648786,
            "fpr": 0.3611416026344676,
            "logloss": 7.284317529932343,
            "mae": 0.3767095720978945,
            "precision": 0.5765765765765766,
            "recall": 0.9676025917926566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7600442787487578,
            "auditor_fn_violation": 0.007809429377925468,
            "auditor_fp_violation": 0.020656227861816066,
            "ave_precision_score": 0.7576537021237355,
            "fpr": 0.18859649122807018,
            "logloss": 1.3083335585444151,
            "mae": 0.2883358147330705,
            "precision": 0.7019064124783362,
            "recall": 0.824847250509165
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.724193917672627,
            "auditor_fn_violation": 0.003681900837614665,
            "auditor_fp_violation": 0.01400786027912812,
            "ave_precision_score": 0.7234840055068164,
            "fpr": 0.2052689352360044,
            "logloss": 1.5119132405825977,
            "mae": 0.28700552002813734,
            "precision": 0.6797945205479452,
            "recall": 0.857451403887689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.7707592713072466,
            "auditor_fn_violation": 0.00531050130417694,
            "auditor_fp_violation": 0.014108534400133358,
            "ave_precision_score": 0.6703123041045735,
            "fpr": 0.32456140350877194,
            "logloss": 6.4121294025586,
            "mae": 0.3459629450418473,
            "precision": 0.6160830090791181,
            "recall": 0.9674134419551935
        },
        "train": {
            "accuracy": 0.6201975850713501,
            "auc_prc": 0.7448855775056872,
            "auditor_fn_violation": 0.000547662004822271,
            "auditor_fp_violation": 0.023943468715697042,
            "ave_precision_score": 0.6275309259711228,
            "fpr": 0.36443468715697036,
            "logloss": 7.379596207344546,
            "mae": 0.3776914847163529,
            "precision": 0.5749039692701664,
            "recall": 0.9697624190064795
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.7141455657916527,
            "auditor_fn_violation": 0.01794583199342552,
            "auditor_fp_violation": 0.015171167229236993,
            "ave_precision_score": 0.6604079790483712,
            "fpr": 0.19517543859649122,
            "logloss": 5.462113355357011,
            "mae": 0.3754501431994116,
            "precision": 0.6482213438735178,
            "recall": 0.6680244399185336
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.7154374948433744,
            "auditor_fn_violation": 0.020175773424404873,
            "auditor_fp_violation": 0.018854379018347193,
            "ave_precision_score": 0.6522132917543626,
            "fpr": 0.21185510428100987,
            "logloss": 5.576226362858728,
            "mae": 0.368181948200205,
            "precision": 0.6245136186770428,
            "recall": 0.693304535637149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7664953391262018,
            "auditor_fn_violation": 0.014051166613070354,
            "auditor_fp_violation": 0.013764741426011593,
            "ave_precision_score": 0.6915622008947896,
            "fpr": 0.24780701754385964,
            "logloss": 5.045106321324205,
            "mae": 0.3113550766934164,
            "precision": 0.6591251885369532,
            "recall": 0.890020366598778
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7558988890307361,
            "auditor_fn_violation": 0.017458800881000873,
            "auditor_fp_violation": 0.022218519680100364,
            "ave_precision_score": 0.6735770362534601,
            "fpr": 0.25686059275521406,
            "logloss": 5.272908663906044,
            "mae": 0.3218597273672286,
            "precision": 0.6360808709175739,
            "recall": 0.8833693304535637
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.7476640779427284,
            "auditor_fn_violation": 0.0039884589273591325,
            "auditor_fp_violation": 0.007797849731216426,
            "ave_precision_score": 0.6609131988183012,
            "fpr": 0.3793859649122807,
            "logloss": 5.864945888226508,
            "mae": 0.3808722492304106,
            "precision": 0.5811138014527845,
            "recall": 0.9775967413441955
        },
        "train": {
            "accuracy": 0.5883644346871569,
            "auc_prc": 0.7384443046897937,
            "auditor_fn_violation": 7.823742926032442e-05,
            "auditor_fp_violation": 0.01074417829700486,
            "ave_precision_score": 0.6392943104100477,
            "fpr": 0.40504939626783754,
            "logloss": 6.564342559104332,
            "mae": 0.40705460696778917,
            "precision": 0.5532687651331719,
            "recall": 0.9870410367170627
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7355402853785762,
            "auditor_fn_violation": 0.00885679065280309,
            "auditor_fp_violation": 0.029665166479143225,
            "ave_precision_score": 0.7308132289095309,
            "fpr": 0.19736842105263158,
            "logloss": 1.3953316683926993,
            "mae": 0.2872135911198001,
            "precision": 0.7004991680532446,
            "recall": 0.8574338085539714
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7026423004556264,
            "auditor_fn_violation": 0.006247140184877419,
            "auditor_fp_violation": 0.02880468872510585,
            "ave_precision_score": 0.6984889384696078,
            "fpr": 0.20636663007683864,
            "logloss": 1.6764394065953203,
            "mae": 0.2979652117742367,
            "precision": 0.6824324324324325,
            "recall": 0.8725701943844493
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.6219169614843951,
            "auditor_fn_violation": 0.014008736198949523,
            "auditor_fp_violation": 0.011743655456932114,
            "ave_precision_score": 0.615954453997035,
            "fpr": 0.15460526315789475,
            "logloss": 2.6109827099180003,
            "mae": 0.37790024689764845,
            "precision": 0.6809954751131222,
            "recall": 0.6130346232179226
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.6089980215615233,
            "auditor_fn_violation": 0.009409828991946296,
            "auditor_fp_violation": 0.015286870785635879,
            "ave_precision_score": 0.6063106594615525,
            "fpr": 0.1394072447859495,
            "logloss": 2.2075671649923416,
            "mae": 0.3751524204661986,
            "precision": 0.6675392670157068,
            "recall": 0.550755939524838
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 14289,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.7682912269368805,
            "auditor_fn_violation": 0.002027727159038125,
            "auditor_fp_violation": 0.0061544151352252345,
            "ave_precision_score": 0.5467231940381694,
            "fpr": 0.4407894736842105,
            "logloss": 15.094743187166664,
            "mae": 0.44439878304403563,
            "precision": 0.547806524184477,
            "recall": 0.9918533604887984
        },
        "train": {
            "accuracy": 0.5257958287596048,
            "auc_prc": 0.7581052742561527,
            "auditor_fn_violation": 0.001156965620576918,
            "auditor_fp_violation": 0.007110514348439702,
            "ave_precision_score": 0.5203833666029763,
            "fpr": 0.47200878155872666,
            "logloss": 16.06479038341684,
            "mae": 0.47392132839526907,
            "precision": 0.5173961840628507,
            "recall": 0.9956803455723542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.7724994829648112,
            "auditor_fn_violation": 0.006762068103047844,
            "auditor_fp_violation": 0.0157597824728091,
            "ave_precision_score": 0.6726295479616893,
            "fpr": 0.3256578947368421,
            "logloss": 6.349366370073176,
            "mae": 0.34551869674268937,
            "precision": 0.6147859922178989,
            "recall": 0.9653767820773931
        },
        "train": {
            "accuracy": 0.6212952799121844,
            "auc_prc": 0.7458499221005519,
            "auditor_fn_violation": 0.00032717470417953864,
            "auditor_fp_violation": 0.021434451936647343,
            "ave_precision_score": 0.6290747009492447,
            "fpr": 0.36553238199780463,
            "logloss": 7.367767036877328,
            "mae": 0.384322221996277,
            "precision": 0.5752551020408163,
            "recall": 0.9740820734341252
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.7706601274642718,
            "auditor_fn_violation": 0.00441722942794869,
            "auditor_fp_violation": 0.01813247489269493,
            "ave_precision_score": 0.6700101271332495,
            "fpr": 0.3190789473684211,
            "logloss": 6.4113270345911415,
            "mae": 0.34360183326501564,
            "precision": 0.6191099476439791,
            "recall": 0.9633401221995926
        },
        "train": {
            "accuracy": 0.6311745334796927,
            "auc_prc": 0.7465288424756231,
            "auditor_fn_violation": 0.0018113150289359953,
            "auditor_fp_violation": 0.02504361376822958,
            "ave_precision_score": 0.6300220307163636,
            "fpr": 0.35236004390779363,
            "logloss": 7.3493753546760345,
            "mae": 0.3730544040766946,
            "precision": 0.5825747724317295,
            "recall": 0.9676025917926566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7521498847990936,
            "auditor_fn_violation": 0.0061926072819523375,
            "auditor_fp_violation": 0.021289119473267497,
            "ave_precision_score": 0.7494748236136922,
            "fpr": 0.17982456140350878,
            "logloss": 1.2896329562230184,
            "mae": 0.29019740662869103,
            "precision": 0.7060931899641577,
            "recall": 0.8024439918533605
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.7132221155243736,
            "auditor_fn_violation": 0.004426341831182598,
            "auditor_fp_violation": 0.016315959698917994,
            "ave_precision_score": 0.7099519322807786,
            "fpr": 0.18111964873765093,
            "logloss": 1.518231559153121,
            "mae": 0.2865212436280831,
            "precision": 0.701627486437613,
            "recall": 0.838012958963283
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.7720225899632787,
            "auditor_fn_violation": 0.005768303140743917,
            "auditor_fp_violation": 0.01866900029170314,
            "ave_precision_score": 0.6717283222037645,
            "fpr": 0.3157894736842105,
            "logloss": 6.432343694172584,
            "mae": 0.34375220456586186,
            "precision": 0.619550858652576,
            "recall": 0.955193482688391
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.7459393864256224,
            "auditor_fn_violation": 0.000455199588423706,
            "auditor_fp_violation": 0.02068468715697036,
            "ave_precision_score": 0.6298740087702577,
            "fpr": 0.3611416026344676,
            "logloss": 7.303418267297193,
            "mae": 0.37751052450377465,
            "precision": 0.5765765765765766,
            "recall": 0.9676025917926566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.7759115908988405,
            "auditor_fn_violation": 0.0050693178975953134,
            "auditor_fp_violation": 0.026563216235362754,
            "ave_precision_score": 0.6843147507828251,
            "fpr": 0.27960526315789475,
            "logloss": 6.649432698648632,
            "mae": 0.3223540068372866,
            "precision": 0.6382978723404256,
            "recall": 0.9164969450101833
        },
        "train": {
            "accuracy": 0.6630076838638859,
            "auc_prc": 0.7591540727272873,
            "auditor_fn_violation": 0.005286953553046163,
            "auditor_fp_violation": 0.028295044691861393,
            "ave_precision_score": 0.6558422782430023,
            "fpr": 0.300768386388584,
            "logloss": 7.098850901402333,
            "mae": 0.3327060297040969,
            "precision": 0.6107954545454546,
            "recall": 0.9287257019438445
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 14289,
        "test": {
            "accuracy": 0.5625,
            "auc_prc": 0.7699939265751521,
            "auditor_fn_violation": 0.0031487833637045774,
            "auditor_fp_violation": 0.009214693503354583,
            "ave_precision_score": 0.5519023303377594,
            "fpr": 0.43201754385964913,
            "logloss": 14.80572292541707,
            "mae": 0.4381853681861844,
            "precision": 0.5522727272727272,
            "recall": 0.9898167006109979
        },
        "train": {
            "accuracy": 0.5334796926454446,
            "auc_prc": 0.7573228554028554,
            "auditor_fn_violation": 0.002968280649512913,
            "auditor_fp_violation": 0.009433315038419324,
            "ave_precision_score": 0.5240611229720179,
            "fpr": 0.4588364434687157,
            "logloss": 15.839192024176882,
            "mae": 0.46641278156369687,
            "precision": 0.5217391304347826,
            "recall": 0.9848812095032398
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6502192982456141,
            "auc_prc": 0.7709319594628735,
            "auditor_fn_violation": 0.00531050130417694,
            "auditor_fp_violation": 0.01431689377838898,
            "ave_precision_score": 0.6699181369996288,
            "fpr": 0.33223684210526316,
            "logloss": 6.417208423810449,
            "mae": 0.3534717918517887,
            "precision": 0.6105398457583547,
            "recall": 0.9674134419551935
        },
        "train": {
            "accuracy": 0.6125137211855104,
            "auc_prc": 0.7451647422927375,
            "auditor_fn_violation": 0.0019203732636625074,
            "auditor_fp_violation": 0.02270856201975852,
            "ave_precision_score": 0.6278085610583606,
            "fpr": 0.3732162458836443,
            "logloss": 7.372065682527448,
            "mae": 0.38664736074030515,
            "precision": 0.569620253164557,
            "recall": 0.9719222462203023
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.779228255851268,
            "auditor_fn_violation": 0.004823668131632544,
            "auditor_fp_violation": 0.01031899820810935,
            "ave_precision_score": 0.6850171139223566,
            "fpr": 0.3092105263157895,
            "logloss": 5.927581504542284,
            "mae": 0.3350375803820418,
            "precision": 0.624,
            "recall": 0.9531568228105907
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.7549835147232324,
            "auditor_fn_violation": 0.0011356281398695574,
            "auditor_fp_violation": 0.023419123412262828,
            "ave_precision_score": 0.6454198411548786,
            "fpr": 0.32711306256860595,
            "logloss": 6.740718170298887,
            "mae": 0.35596522580948275,
            "precision": 0.5962059620596206,
            "recall": 0.9503239740820735
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7578465588766274,
            "auditor_fn_violation": 0.009517811841211993,
            "auditor_fp_violation": 0.012459890819685801,
            "ave_precision_score": 0.6757753741794518,
            "fpr": 0.27960526315789475,
            "logloss": 5.357410212468163,
            "mae": 0.32327731974880325,
            "precision": 0.6393210749646393,
            "recall": 0.9205702647657841
        },
        "train": {
            "accuracy": 0.6608122941822173,
            "auc_prc": 0.741057750150447,
            "auditor_fn_violation": 0.0028426265964584525,
            "auditor_fp_violation": 0.012084444095969904,
            "ave_precision_score": 0.6452390455082248,
            "fpr": 0.300768386388584,
            "logloss": 6.0344920448123585,
            "mae": 0.3398291215491552,
            "precision": 0.6096866096866097,
            "recall": 0.9244060475161987
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.7540820883495482,
            "auditor_fn_violation": 0.009517811841211993,
            "auditor_fp_violation": 0.007347272575738651,
            "ave_precision_score": 0.6942502498519062,
            "fpr": 0.2708333333333333,
            "logloss": 4.259350419233918,
            "mae": 0.3195627131705103,
            "precision": 0.6466380543633763,
            "recall": 0.9205702647657841
        },
        "train": {
            "accuracy": 0.6663007683863886,
            "auc_prc": 0.7462257417684102,
            "auditor_fn_violation": 0.000512099536976668,
            "auditor_fp_violation": 0.016342912027599202,
            "ave_precision_score": 0.6765677768095333,
            "fpr": 0.29637760702524696,
            "logloss": 4.758377776191045,
            "mae": 0.3391368679762854,
            "precision": 0.6137339055793991,
            "recall": 0.9265658747300216
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.7693696062926744,
            "auditor_fn_violation": 0.005837531711151606,
            "auditor_fp_violation": 0.018481476851273094,
            "ave_precision_score": 0.6676740449957661,
            "fpr": 0.32785087719298245,
            "logloss": 6.6084075880508335,
            "mae": 0.3510597361606064,
            "precision": 0.6141935483870967,
            "recall": 0.9694501018329938
        },
        "train": {
            "accuracy": 0.6180021953896817,
            "auc_prc": 0.7467874884813452,
            "auditor_fn_violation": 0.0012423155434063634,
            "auditor_fp_violation": 0.022750215618629457,
            "ave_precision_score": 0.6291177847307455,
            "fpr": 0.3677277716794731,
            "logloss": 7.481604644247607,
            "mae": 0.38557427102322517,
            "precision": 0.5732484076433121,
            "recall": 0.9719222462203023
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.772340910290187,
            "auditor_fn_violation": 0.006226104977310898,
            "auditor_fp_violation": 0.011980664249697882,
            "ave_precision_score": 0.6733093176530779,
            "fpr": 0.31359649122807015,
            "logloss": 6.258336152892341,
            "mae": 0.34418833453051584,
            "precision": 0.6211920529801325,
            "recall": 0.955193482688391
        },
        "train": {
            "accuracy": 0.6366630076838639,
            "auc_prc": 0.7487403865348039,
            "auditor_fn_violation": 0.004253271154333999,
            "auditor_fp_violation": 0.023953269562490203,
            "ave_precision_score": 0.6355795973525682,
            "fpr": 0.3424807903402854,
            "logloss": 7.06885820059709,
            "mae": 0.36590599708784827,
            "precision": 0.5873015873015873,
            "recall": 0.958963282937365
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 14289,
        "test": {
            "accuracy": 0.4649122807017544,
            "auc_prc": 0.7197609892659389,
            "auditor_fn_violation": 0.0007146175009826038,
            "auditor_fp_violation": 0.0010391923990498814,
            "ave_precision_score": 0.7207491588224386,
            "fpr": 0.0043859649122807015,
            "logloss": 18.057734069276528,
            "mae": 0.5358501365735007,
            "precision": 0.6363636363636364,
            "recall": 0.014256619144602852
        },
        "train": {
            "accuracy": 0.4862788144895719,
            "auc_prc": 0.6947287141771222,
            "auditor_fn_violation": 0.0007444409935679394,
            "auditor_fp_violation": 0.0031730241492864985,
            "ave_precision_score": 0.6965870185276709,
            "fpr": 0.007683863885839737,
            "logloss": 17.25282422603915,
            "mae": 0.513629015658955,
            "precision": 0.2222222222222222,
            "recall": 0.004319654427645789
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.7684896888612528,
            "auditor_fn_violation": 0.0045244220530960815,
            "auditor_fp_violation": 0.014843001208484388,
            "ave_precision_score": 0.6723719513016719,
            "fpr": 0.3190789473684211,
            "logloss": 6.201794806903855,
            "mae": 0.34288673171757084,
            "precision": 0.6186107470511141,
            "recall": 0.9613034623217923
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7509079647487461,
            "auditor_fn_violation": 0.000490762056269308,
            "auditor_fp_violation": 0.02706993884271601,
            "ave_precision_score": 0.6413280158368814,
            "fpr": 0.3556531284302964,
            "logloss": 6.9692224815034445,
            "mae": 0.37383282440800797,
            "precision": 0.5786736020806242,
            "recall": 0.9611231101511879
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7360772762025624,
            "auditor_fn_violation": 0.01087335191338836,
            "auditor_fp_violation": 0.020822915364420574,
            "ave_precision_score": 0.7302062432759169,
            "fpr": 0.25548245614035087,
            "logloss": 1.7055523840585511,
            "mae": 0.32430536864555043,
            "precision": 0.6485671191553545,
            "recall": 0.8757637474541752
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.7169575012331343,
            "auditor_fn_violation": 0.006021911221855271,
            "auditor_fp_violation": 0.02393366786890388,
            "ave_precision_score": 0.7086658248249291,
            "fpr": 0.270032930845225,
            "logloss": 1.9875741405091714,
            "mae": 0.33471799712390055,
            "precision": 0.6267071320182094,
            "recall": 0.8920086393088553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6567982456140351,
            "auc_prc": 0.638610364278771,
            "auditor_fn_violation": 0.018606853181834422,
            "auditor_fp_violation": 0.01531962328624412,
            "ave_precision_score": 0.6362760058718766,
            "fpr": 0.25548245614035087,
            "logloss": 1.351264349656097,
            "mae": 0.3827826864103452,
            "precision": 0.6381987577639752,
            "recall": 0.8370672097759674
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6051354838445195,
            "auditor_fn_violation": 0.01663375162698291,
            "auditor_fp_violation": 0.012937117766975093,
            "ave_precision_score": 0.6045482380498276,
            "fpr": 0.31613611416026344,
            "logloss": 1.6571675696019608,
            "mae": 0.3992373444460451,
            "precision": 0.5879828326180258,
            "recall": 0.8876889848812095
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7538979425162263,
            "auditor_fn_violation": 0.0134928716904277,
            "auditor_fp_violation": 0.020804683918823196,
            "ave_precision_score": 0.7503619980253082,
            "fpr": 0.20065789473684212,
            "logloss": 1.680753571764293,
            "mae": 0.30928165496053356,
            "precision": 0.685025817555938,
            "recall": 0.8105906313645621
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7710380506042015,
            "auditor_fn_violation": 0.005417349268480035,
            "auditor_fp_violation": 0.026839618943076692,
            "ave_precision_score": 0.7658771121250942,
            "fpr": 0.21405049396267836,
            "logloss": 1.4673253369352202,
            "mae": 0.3041684103450844,
            "precision": 0.6683673469387755,
            "recall": 0.8488120950323974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 14289,
        "test": {
            "accuracy": 0.4594298245614035,
            "auc_prc": 0.5597404704561493,
            "auditor_fn_violation": 0.0006252903133597741,
            "auditor_fp_violation": 0.0015861357669708714,
            "ave_precision_score": 0.5712663987345356,
            "fpr": 0.005482456140350877,
            "logloss": 12.03640838785828,
            "mae": 0.535590737002803,
            "precision": 0.375,
            "recall": 0.006109979633401222
        },
        "train": {
            "accuracy": 0.4972557628979144,
            "auc_prc": 0.5543119815562599,
            "auditor_fn_violation": 0.002105298096459654,
            "auditor_fp_violation": 0.0012300062725419476,
            "ave_precision_score": 0.5697191869057412,
            "fpr": 0.0021953896816684962,
            "logloss": 11.36457431046656,
            "mae": 0.4978360500626414,
            "precision": 0.7777777777777778,
            "recall": 0.01511879049676026
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.7729933568304049,
            "auditor_fn_violation": 0.006844695751598957,
            "auditor_fp_violation": 0.015892611576447068,
            "ave_precision_score": 0.6732684028298009,
            "fpr": 0.3059210526315789,
            "logloss": 6.317788926434743,
            "mae": 0.3389047335161073,
            "precision": 0.6260053619302949,
            "recall": 0.9511201629327902
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.7474121673412624,
            "auditor_fn_violation": 0.001081099022506301,
            "auditor_fp_violation": 0.021233534577387483,
            "ave_precision_score": 0.6333689531156431,
            "fpr": 0.34577387486278816,
            "logloss": 7.110965559484546,
            "mae": 0.36795922928998415,
            "precision": 0.5844327176781002,
            "recall": 0.9568034557235421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 14289,
        "test": {
            "accuracy": 0.5657894736842105,
            "auc_prc": 0.7697776672038623,
            "auditor_fn_violation": 0.007172973166112839,
            "auditor_fp_violation": 0.014835187731799806,
            "ave_precision_score": 0.5790050137904993,
            "fpr": 0.40021929824561403,
            "logloss": 13.890734005210494,
            "mae": 0.43318221427228143,
            "precision": 0.5575757575757576,
            "recall": 0.9368635437881874
        },
        "train": {
            "accuracy": 0.5477497255762898,
            "auc_prc": 0.7606157611460836,
            "auditor_fn_violation": 0.004606525001600311,
            "auditor_fp_violation": 0.013976007527050336,
            "ave_precision_score": 0.5535450196950852,
            "fpr": 0.42590559824368823,
            "logloss": 14.588640037840758,
            "mae": 0.45194167243040995,
            "precision": 0.5308343409915357,
            "recall": 0.9481641468682506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.7710160189825505,
            "auditor_fn_violation": 0.00577946903919677,
            "auditor_fp_violation": 0.015572259032379061,
            "ave_precision_score": 0.670574941496176,
            "fpr": 0.3256578947368421,
            "logloss": 6.388407920390272,
            "mae": 0.3449026849265417,
            "precision": 0.6147859922178989,
            "recall": 0.9653767820773931
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.7473404810881452,
            "auditor_fn_violation": 0.000547662004822271,
            "auditor_fp_violation": 0.02344607574094401,
            "ave_precision_score": 0.6321390181208866,
            "fpr": 0.3611416026344676,
            "logloss": 7.252892105642029,
            "mae": 0.37607022694794884,
            "precision": 0.577120822622108,
            "recall": 0.9697624190064795
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.7709685471321863,
            "auditor_fn_violation": 0.00531050130417694,
            "auditor_fp_violation": 0.017051610617993925,
            "ave_precision_score": 0.6705244732632119,
            "fpr": 0.32346491228070173,
            "logloss": 6.402052695033134,
            "mae": 0.3444092313594139,
            "precision": 0.6168831168831169,
            "recall": 0.9674134419551935
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7475925117289741,
            "auditor_fn_violation": 0.0009933782684871495,
            "auditor_fp_violation": 0.028672377293398152,
            "ave_precision_score": 0.6328113338047252,
            "fpr": 0.3589462129527991,
            "logloss": 7.252937832727505,
            "mae": 0.37654856507991696,
            "precision": 0.5780645161290323,
            "recall": 0.9676025917926566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.7714475722518895,
            "auditor_fn_violation": 0.005768303140743917,
            "auditor_fp_violation": 0.013978309788723609,
            "ave_precision_score": 0.6706878551724627,
            "fpr": 0.31469298245614036,
            "logloss": 6.454863792830016,
            "mae": 0.34326167757322573,
            "precision": 0.6203703703703703,
            "recall": 0.955193482688391
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.7463891763407209,
            "auditor_fn_violation": 0.000455199588423706,
            "auditor_fp_violation": 0.020339207307511372,
            "ave_precision_score": 0.630757175050428,
            "fpr": 0.3589462129527991,
            "logloss": 7.274539752828294,
            "mae": 0.37671174358944126,
            "precision": 0.5780645161290323,
            "recall": 0.9676025917926566
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 14289,
        "test": {
            "accuracy": 0.5975877192982456,
            "auc_prc": 0.7575777494327048,
            "auditor_fn_violation": 0.007204237681780827,
            "auditor_fp_violation": 0.011386840021669394,
            "ave_precision_score": 0.6552708886709967,
            "fpr": 0.37719298245614036,
            "logloss": 7.118678323368176,
            "mae": 0.3976725967729725,
            "precision": 0.5763546798029556,
            "recall": 0.9531568228105907
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7407243893313137,
            "auditor_fn_violation": 0.004419229337613475,
            "auditor_fp_violation": 0.01011447389054416,
            "ave_precision_score": 0.6218631656118765,
            "fpr": 0.4039517014270033,
            "logloss": 8.01626774184628,
            "mae": 0.42008112507957746,
            "precision": 0.5451174289245982,
            "recall": 0.9524838012958964
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.7561162962622521,
            "auditor_fn_violation": 0.007405223853932187,
            "auditor_fp_violation": 0.023995186898362313,
            "ave_precision_score": 0.7535370571563753,
            "fpr": 0.18969298245614036,
            "logloss": 1.2968024997418945,
            "mae": 0.29058521059538844,
            "precision": 0.6991304347826087,
            "recall": 0.8187372708757638
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7140206724669054,
            "auditor_fn_violation": 0.004582816689703245,
            "auditor_fp_violation": 0.018984240238356594,
            "ave_precision_score": 0.7123590060929474,
            "fpr": 0.1964873765093304,
            "logloss": 1.4875962112593706,
            "mae": 0.2873761725594229,
            "precision": 0.6903114186851211,
            "recall": 0.8617710583153347
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 14289,
        "test": {
            "accuracy": 0.5548245614035088,
            "auc_prc": 0.7685835080695099,
            "auditor_fn_violation": 0.001648086611641119,
            "auditor_fp_violation": 0.004784452223194561,
            "ave_precision_score": 0.5473201794487376,
            "fpr": 0.4418859649122807,
            "logloss": 15.072344416791095,
            "mae": 0.4463233197380249,
            "precision": 0.547699214365881,
            "recall": 0.9938900203665988
        },
        "train": {
            "accuracy": 0.5290889132821076,
            "auc_prc": 0.7590083008312917,
            "auditor_fn_violation": 0.001156965620576918,
            "auditor_fp_violation": 0.004459385290889129,
            "ave_precision_score": 0.5221667885250593,
            "fpr": 0.46871569703622395,
            "logloss": 15.957771906060582,
            "mae": 0.4707734186354947,
            "precision": 0.5191441441441441,
            "recall": 0.9956803455723542
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 14289,
        "test": {
            "accuracy": 0.5844298245614035,
            "auc_prc": 0.6794841538252282,
            "auditor_fn_violation": 0.005051452460070748,
            "auditor_fp_violation": 0.007555631953994261,
            "ave_precision_score": 0.6485892061033056,
            "fpr": 0.3991228070175439,
            "logloss": 3.9203866517116954,
            "mae": 0.4130554757199813,
            "precision": 0.5666666666666667,
            "recall": 0.9694501018329938
        },
        "train": {
            "accuracy": 0.5565312843029637,
            "auc_prc": 0.6829607774338349,
            "auditor_fn_violation": 0.0033594677958145346,
            "auditor_fp_violation": 0.011241571271757895,
            "ave_precision_score": 0.6475699142484805,
            "fpr": 0.43468715697036225,
            "logloss": 4.162790911715689,
            "mae": 0.436477136967395,
            "precision": 0.5346650998824912,
            "recall": 0.9827213822894169
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.7694191374161954,
            "auditor_fn_violation": 0.005938024797227283,
            "auditor_fp_violation": 0.0169526399133225,
            "ave_precision_score": 0.6670234058753942,
            "fpr": 0.32456140350877194,
            "logloss": 6.5085699757995,
            "mae": 0.34763007564803655,
            "precision": 0.616580310880829,
            "recall": 0.9694501018329938
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.744861145043812,
            "auditor_fn_violation": 0.0008961741897091699,
            "auditor_fp_violation": 0.022211169045005492,
            "ave_precision_score": 0.626933296702073,
            "fpr": 0.3699231613611416,
            "logloss": 7.430568825000894,
            "mae": 0.3805975157907191,
            "precision": 0.5723350253807107,
            "recall": 0.9740820734341252
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7635934539903808,
            "auditor_fn_violation": 0.007282398970950799,
            "auditor_fp_violation": 0.015551423094553492,
            "ave_precision_score": 0.6883912043986585,
            "fpr": 0.29276315789473684,
            "logloss": 5.101910412133527,
            "mae": 0.32924260757382445,
            "precision": 0.635743519781719,
            "recall": 0.9490835030549898
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.7507069558747068,
            "auditor_fn_violation": 0.004260383647903119,
            "auditor_fp_violation": 0.01762437274580524,
            "ave_precision_score": 0.6636304496807098,
            "fpr": 0.33260153677277715,
            "logloss": 5.73950445204183,
            "mae": 0.36131353675215255,
            "precision": 0.592741935483871,
            "recall": 0.9524838012958964
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.761004753042933,
            "auditor_fn_violation": 0.007697770393396934,
            "auditor_fp_violation": 0.01387933908405219,
            "ave_precision_score": 0.661338135189235,
            "fpr": 0.3157894736842105,
            "logloss": 6.529289510689696,
            "mae": 0.355715813695768,
            "precision": 0.6175298804780877,
            "recall": 0.9470468431771895
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.7438978513030451,
            "auditor_fn_violation": 0.0013537446093225833,
            "auditor_fp_violation": 0.020836600282264393,
            "ave_precision_score": 0.6289909613611765,
            "fpr": 0.3600439077936334,
            "logloss": 7.339802289157824,
            "mae": 0.38453585388907924,
            "precision": 0.5751295336787565,
            "recall": 0.958963282937365
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 14289,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.772466099207004,
            "auditor_fn_violation": 0.005332833101082646,
            "auditor_fp_violation": 0.01662707838479811,
            "ave_precision_score": 0.6744756987093634,
            "fpr": 0.31030701754385964,
            "logloss": 6.178839634192897,
            "mae": 0.34219131954006515,
            "precision": 0.6221628838451269,
            "recall": 0.9490835030549898
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.7508038496095614,
            "auditor_fn_violation": 0.001574231909965315,
            "auditor_fp_violation": 0.021145326956249016,
            "ave_precision_score": 0.6415062413947002,
            "fpr": 0.33260153677277715,
            "logloss": 6.805806236624097,
            "mae": 0.3610850979294292,
            "precision": 0.5932885906040268,
            "recall": 0.9546436285097192
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 14289,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7580989257457412,
            "auditor_fn_violation": 0.011612534390967235,
            "auditor_fp_violation": 0.021268283535441936,
            "ave_precision_score": 0.7554235407909468,
            "fpr": 0.18421052631578946,
            "logloss": 1.2247391212319874,
            "mae": 0.2851553230497753,
            "precision": 0.7047451669595782,
            "recall": 0.8167006109979633
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7217951961385396,
            "auditor_fn_violation": 0.004134729594848663,
            "auditor_fp_violation": 0.011675258742355341,
            "ave_precision_score": 0.7192533298149024,
            "fpr": 0.1942919868276619,
            "logloss": 1.4643605085918925,
            "mae": 0.28451671603358114,
            "precision": 0.6900175131348512,
            "recall": 0.8509719222462203
        }
    }
]
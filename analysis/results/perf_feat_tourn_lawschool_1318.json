[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.6019024426395302,
            "auditor_fn_violation": 0.01938350293613452,
            "auditor_fp_violation": 0.010295823665893275,
            "ave_precision_score": 0.6025409892915096,
            "fpr": 0.05701754385964912,
            "logloss": 6.960575705384753,
            "mae": 0.4458924746425617,
            "precision": 0.7305699481865285,
            "recall": 0.29313929313929316
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.6103283098453736,
            "auditor_fn_violation": 0.020257459335395668,
            "auditor_fp_violation": 0.011793954157456562,
            "ave_precision_score": 0.6118345411029349,
            "fpr": 0.05159165751920966,
            "logloss": 6.929770766199933,
            "mae": 0.431222941349707,
            "precision": 0.7539267015706806,
            "recall": 0.3044397463002114
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 1318,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7936324290131508,
            "auditor_fn_violation": 0.020794580005106325,
            "auditor_fp_violation": 0.024578194325721496,
            "ave_precision_score": 0.7941994833289439,
            "fpr": 0.17763157894736842,
            "logloss": 1.1477420462993038,
            "mae": 0.29209278893292745,
            "precision": 0.7086330935251799,
            "recall": 0.8191268191268192
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8032500461163835,
            "auditor_fn_violation": 0.022304323710904777,
            "auditor_fp_violation": 0.030397124941731954,
            "ave_precision_score": 0.8035979489590358,
            "fpr": 0.17453347969264543,
            "logloss": 0.9599351097153855,
            "mae": 0.2858764534786766,
            "precision": 0.7071823204419889,
            "recall": 0.8118393234672304
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 1318,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7519062103916146,
            "auditor_fn_violation": 0.01969580916949338,
            "auditor_fp_violation": 0.02268032319778565,
            "ave_precision_score": 0.7502838742653142,
            "fpr": 0.1600877192982456,
            "logloss": 1.573030437325427,
            "mae": 0.307807635963361,
            "precision": 0.7074148296593187,
            "recall": 0.7338877338877339
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7364638043514259,
            "auditor_fn_violation": 0.019189933697375056,
            "auditor_fp_violation": 0.024179360329609195,
            "ave_precision_score": 0.7368095231696092,
            "fpr": 0.15477497255762898,
            "logloss": 1.437408587891139,
            "mae": 0.3151638757001185,
            "precision": 0.7056367432150313,
            "recall": 0.7145877378435518
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 1318,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.7591246133584839,
            "auditor_fn_violation": 0.02085157019367546,
            "auditor_fp_violation": 0.009624190987910614,
            "ave_precision_score": 0.7578906300651662,
            "fpr": 0.08662280701754387,
            "logloss": 1.88083948632656,
            "mae": 0.3249149659917266,
            "precision": 0.7762039660056658,
            "recall": 0.5696465696465697
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7661211938881765,
            "auditor_fn_violation": 0.014446406731909509,
            "auditor_fp_violation": 0.01197189099238631,
            "ave_precision_score": 0.7664378595776155,
            "fpr": 0.07793633369923161,
            "logloss": 1.6355384125861558,
            "mae": 0.320028780412659,
            "precision": 0.7893175074183977,
            "recall": 0.5623678646934461
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 1318,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7713899094438655,
            "auditor_fn_violation": 0.01423614910457016,
            "auditor_fp_violation": 0.019721577726218104,
            "ave_precision_score": 0.768878607894758,
            "fpr": 0.17982456140350878,
            "logloss": 1.8064619360511345,
            "mae": 0.2920962357681199,
            "precision": 0.6990825688073394,
            "recall": 0.7920997920997921
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.7768882515004093,
            "auditor_fn_violation": 0.015878283511602382,
            "auditor_fp_violation": 0.023001468605426324,
            "ave_precision_score": 0.7757366405367299,
            "fpr": 0.17233809001097694,
            "logloss": 1.5620754362147948,
            "mae": 0.2677605699118281,
            "precision": 0.7119266055045872,
            "recall": 0.8202959830866807
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4857456140350877,
            "auc_prc": 0.6053144990076991,
            "auditor_fn_violation": 0.0015752088120509395,
            "auditor_fp_violation": 0.001915679570155087,
            "ave_precision_score": 0.6050430224483989,
            "fpr": 0.01206140350877193,
            "logloss": 6.145183516008598,
            "mae": 0.5121717555496176,
            "precision": 0.6764705882352942,
            "recall": 0.04781704781704782
        },
        "train": {
            "accuracy": 0.4840834248079034,
            "auc_prc": 0.5980641252500921,
            "auditor_fn_violation": 0.000893472544865105,
            "auditor_fp_violation": 0.003460996747013919,
            "ave_precision_score": 0.5995410205288532,
            "fpr": 0.014270032930845226,
            "logloss": 6.044800723487976,
            "mae": 0.5064427676560092,
            "precision": 0.5517241379310345,
            "recall": 0.03382663847780127
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 1318,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.6064731286599827,
            "auditor_fn_violation": 0.002680818470292155,
            "auditor_fp_violation": 0.0015518785362478122,
            "ave_precision_score": 0.6062362976813234,
            "fpr": 0.008771929824561403,
            "logloss": 8.362123251529772,
            "mae": 0.5302387057745527,
            "precision": 0.42857142857142855,
            "recall": 0.012474012474012475
        },
        "train": {
            "accuracy": 0.47420417124039516,
            "auc_prc": 0.6012487818170588,
            "auditor_fn_violation": 0.000689250248895931,
            "auditor_fp_violation": 0.0031477276714333693,
            "ave_precision_score": 0.6028691897576823,
            "fpr": 0.007683863885839737,
            "logloss": 8.093322907335082,
            "mae": 0.5255260578249303,
            "precision": 0.125,
            "recall": 0.0021141649048625794
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 1318,
        "test": {
            "accuracy": 0.49780701754385964,
            "auc_prc": 0.707208770613176,
            "auditor_fn_violation": 0.009239249370828335,
            "auditor_fp_violation": 0.0019029592542842025,
            "ave_precision_score": 0.706115558944419,
            "fpr": 0.013157894736842105,
            "logloss": 4.211045412058549,
            "mae": 0.4934218450656747,
            "precision": 0.7446808510638298,
            "recall": 0.07276507276507277
        },
        "train": {
            "accuracy": 0.49396267837541163,
            "auc_prc": 0.7022744767923969,
            "auditor_fn_violation": 0.01007419303184245,
            "auditor_fp_violation": 0.005338105047892576,
            "ave_precision_score": 0.7037101312243512,
            "fpr": 0.013172338090010977,
            "logloss": 4.1193370463764385,
            "mae": 0.4956613409988775,
            "precision": 0.6666666666666666,
            "recall": 0.0507399577167019
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4923245614035088,
            "auc_prc": 0.6167490856726126,
            "auditor_fn_violation": 0.008534850640113799,
            "auditor_fp_violation": 0.0036380103390727396,
            "ave_precision_score": 0.6164053885922082,
            "fpr": 0.013157894736842105,
            "logloss": 5.824566018228511,
            "mae": 0.5043593342697252,
            "precision": 0.7142857142857143,
            "recall": 0.062370062370062374
        },
        "train": {
            "accuracy": 0.48518111964873767,
            "auc_prc": 0.6095720356467891,
            "auditor_fn_violation": 0.003197935498244389,
            "auditor_fp_violation": 0.004363211684685905,
            "ave_precision_score": 0.6110643755613208,
            "fpr": 0.014270032930845226,
            "logloss": 5.734633541625253,
            "mae": 0.5049880456507245,
            "precision": 0.5666666666666667,
            "recall": 0.035940803382663845
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.6177764076586028,
            "auditor_fn_violation": 0.022091676696939875,
            "auditor_fp_violation": 0.01443755851345301,
            "ave_precision_score": 0.6162404220504432,
            "fpr": 0.07236842105263158,
            "logloss": 4.733730311084518,
            "mae": 0.4418319820542118,
            "precision": 0.7013574660633484,
            "recall": 0.32224532224532226
        },
        "train": {
            "accuracy": 0.5861690450054885,
            "auc_prc": 0.6073884693055387,
            "auditor_fn_violation": 0.015258654499968684,
            "auditor_fp_violation": 0.015598293811306762,
            "ave_precision_score": 0.6077962793007379,
            "fpr": 0.06476399560922064,
            "logloss": 4.6846554544421375,
            "mae": 0.43103156860767994,
            "precision": 0.7242990654205608,
            "recall": 0.3276955602536998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 1318,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.7529569356014366,
            "auditor_fn_violation": 0.010508990772148667,
            "auditor_fp_violation": 0.018894757194610663,
            "ave_precision_score": 0.7511125125463387,
            "fpr": 0.15350877192982457,
            "logloss": 1.723970604923094,
            "mae": 0.2997327925257153,
            "precision": 0.7194388777555111,
            "recall": 0.7463617463617463
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.7604475481290616,
            "auditor_fn_violation": 0.009261945263783267,
            "auditor_fp_violation": 0.019761013287621112,
            "ave_precision_score": 0.7587065329781715,
            "fpr": 0.13062568605927552,
            "logloss": 1.4966195139694856,
            "mae": 0.2790826173841216,
            "precision": 0.7484143763213531,
            "recall": 0.7484143763213531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4868421052631579,
            "auc_prc": 0.5952286680209045,
            "auditor_fn_violation": 0.009006729401466258,
            "auditor_fp_violation": 0.003999267309805838,
            "ave_precision_score": 0.5942140022395082,
            "fpr": 0.01644736842105263,
            "logloss": 7.20722674002203,
            "mae": 0.5051956323098885,
            "precision": 0.6511627906976745,
            "recall": 0.058212058212058215
        },
        "train": {
            "accuracy": 0.49286498353457736,
            "auc_prc": 0.592122869620618,
            "auditor_fn_violation": 0.0020932785336839174,
            "auditor_fp_violation": 0.004751665338405786,
            "ave_precision_score": 0.5935601193086055,
            "fpr": 0.01646542261251372,
            "logloss": 6.845519033554601,
            "mae": 0.49928956421324416,
            "precision": 0.6341463414634146,
            "recall": 0.05496828752642706
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4692982456140351,
            "auc_prc": 0.5855102339764701,
            "auditor_fn_violation": 0.001951344056607245,
            "auditor_fp_violation": 0.0026152969430536903,
            "ave_precision_score": 0.5852261586769458,
            "fpr": 0.010964912280701754,
            "logloss": 8.127523299840503,
            "mae": 0.5290503405039697,
            "precision": 0.4117647058823529,
            "recall": 0.014553014553014554
        },
        "train": {
            "accuracy": 0.47310647639956094,
            "auc_prc": 0.5855747206939069,
            "auditor_fn_violation": 0.0010744877617468545,
            "auditor_fp_violation": 0.004190287154965441,
            "ave_precision_score": 0.5871268907564119,
            "fpr": 0.010976948408342482,
            "logloss": 7.7439605788945824,
            "mae": 0.5268147304568306,
            "precision": 0.23076923076923078,
            "recall": 0.006342494714587738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5219298245614035,
            "auc_prc": 0.6021743687389189,
            "auditor_fn_violation": 0.006702046175730391,
            "auditor_fp_violation": 0.006054870354540644,
            "ave_precision_score": 0.6011598270271006,
            "fpr": 0.02850877192982456,
            "logloss": 6.2663591984056435,
            "mae": 0.47626731156905133,
            "precision": 0.7319587628865979,
            "recall": 0.14760914760914762
        },
        "train": {
            "accuracy": 0.5334796926454446,
            "auc_prc": 0.5936726753816977,
            "auditor_fn_violation": 0.003214180453605572,
            "auditor_fp_violation": 0.0058643970948679,
            "ave_precision_score": 0.5951237656582609,
            "fpr": 0.025246981339187707,
            "logloss": 6.2117550679555205,
            "mae": 0.4681220963983079,
            "precision": 0.7553191489361702,
            "recall": 0.15010570824524314
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5635964912280702,
            "auc_prc": 0.6072305994863727,
            "auditor_fn_violation": 0.02176113360323888,
            "auditor_fp_violation": 0.011562767126633292,
            "ave_precision_score": 0.6052723688780248,
            "fpr": 0.07236842105263158,
            "logloss": 5.574183819885081,
            "mae": 0.44455043567840075,
            "precision": 0.6930232558139535,
            "recall": 0.3097713097713098
        },
        "train": {
            "accuracy": 0.5773874862788145,
            "auc_prc": 0.5953744678583888,
            "auditor_fn_violation": 0.013448502331151099,
            "auditor_fp_violation": 0.01526246936228441,
            "ave_precision_score": 0.5968197195650298,
            "fpr": 0.06915477497255763,
            "logloss": 5.554571637421813,
            "mae": 0.43431034574687644,
            "precision": 0.705607476635514,
            "recall": 0.3192389006342495
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 1318,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7460304188850126,
            "auditor_fn_violation": 0.01156216945690631,
            "auditor_fp_violation": 0.017169882362518832,
            "ave_precision_score": 0.7445582996265132,
            "fpr": 0.1206140350877193,
            "logloss": 1.782446376386387,
            "mae": 0.30333654731476317,
            "precision": 0.7539149888143176,
            "recall": 0.7006237006237006
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.7532826666160882,
            "auditor_fn_violation": 0.013413691712519986,
            "auditor_fp_violation": 0.017683412778370905,
            "ave_precision_score": 0.7517313481571652,
            "fpr": 0.1163556531284303,
            "logloss": 1.556943378627874,
            "mae": 0.2845787032403952,
            "precision": 0.7607223476297968,
            "recall": 0.7124735729386892
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5581140350877193,
            "auc_prc": 0.7044493064911537,
            "auditor_fn_violation": 0.013196648065069129,
            "auditor_fp_violation": 0.006393230756706153,
            "ave_precision_score": 0.7022546292024512,
            "fpr": 0.03728070175438596,
            "logloss": 3.198940068496306,
            "mae": 0.4394606290864071,
            "precision": 0.7671232876712328,
            "recall": 0.23284823284823286
        },
        "train": {
            "accuracy": 0.566410537870472,
            "auc_prc": 0.7238074669330146,
            "auditor_fn_violation": 0.00640747453603249,
            "auditor_fp_violation": 0.006120024660541631,
            "ave_precision_score": 0.7233883616581358,
            "fpr": 0.029637760702524697,
            "logloss": 2.817917929006033,
            "mae": 0.4316383044824665,
            "precision": 0.7954545454545454,
            "recall": 0.2219873150105708
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5504385964912281,
            "auc_prc": 0.7133824228699166,
            "auditor_fn_violation": 0.013235401393296143,
            "auditor_fp_violation": 0.003027435177270322,
            "ave_precision_score": 0.7121181763696353,
            "fpr": 0.01425438596491228,
            "logloss": 3.2729250096036298,
            "mae": 0.4478627389477858,
            "precision": 0.865979381443299,
            "recall": 0.17463617463617465
        },
        "train": {
            "accuracy": 0.5433589462129528,
            "auc_prc": 0.7488936776559049,
            "auditor_fn_violation": 0.0094638468518437,
            "auditor_fp_violation": 0.003318146048549189,
            "ave_precision_score": 0.7501810073742385,
            "fpr": 0.010976948408342482,
            "logloss": 2.9208137545053963,
            "mae": 0.44060473180750354,
            "precision": 0.8701298701298701,
            "recall": 0.1416490486257928
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.5999746662602643,
            "auditor_fn_violation": 0.006608582266477012,
            "auditor_fp_violation": 0.0026152969430536903,
            "ave_precision_score": 0.5996365251459228,
            "fpr": 0.010964912280701754,
            "logloss": 6.7659737647248,
            "mae": 0.5185135775781067,
            "precision": 0.5652173913043478,
            "recall": 0.02702702702702703
        },
        "train": {
            "accuracy": 0.47530186608122943,
            "auc_prc": 0.6014474163080795,
            "auditor_fn_violation": 0.001745172347372837,
            "auditor_fp_violation": 0.004774220711847587,
            "ave_precision_score": 0.6029399090704421,
            "fpr": 0.012074643249176729,
            "logloss": 6.486528995165772,
            "mae": 0.5196221778800308,
            "precision": 0.35294117647058826,
            "recall": 0.012684989429175475
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 1318,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.6472071930046073,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.000610575161802418,
            "ave_precision_score": 0.6490138652842277,
            "fpr": 0.0010964912280701754,
            "logloss": 6.7593263021744825,
            "mae": 0.5282630323190314,
            "precision": 0.0,
            "recall": 0.0
        },
        "train": {
            "accuracy": 0.47859495060373214,
            "auc_prc": 0.6518239114977307,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0012731255231593563,
            "ave_precision_score": 0.6536000970300785,
            "fpr": 0.0021953896816684962,
            "logloss": 6.714511392044826,
            "mae": 0.5208748741856536,
            "precision": 0.0,
            "recall": 0.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 1318,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.617207141496545,
            "auditor_fn_violation": 0.00710553671079987,
            "auditor_fp_violation": 0.0036380103390727396,
            "ave_precision_score": 0.6168631028709031,
            "fpr": 0.013157894736842105,
            "logloss": 5.87918257600245,
            "mae": 0.5059238073121839,
            "precision": 0.7073170731707317,
            "recall": 0.060291060291060294
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.6096030538687451,
            "auditor_fn_violation": 0.0026734555108690506,
            "auditor_fp_violation": 0.004363211684685905,
            "ave_precision_score": 0.6110998411195707,
            "fpr": 0.014270032930845226,
            "logloss": 5.784769525451288,
            "mae": 0.5067140311009375,
            "precision": 0.5185185185185185,
            "recall": 0.02959830866807611
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5252192982456141,
            "auc_prc": 0.5668158713264078,
            "auditor_fn_violation": 0.023315825947404896,
            "auditor_fp_violation": 0.02777099360931331,
            "ave_precision_score": 0.5651044704179591,
            "fpr": 0.19736842105263158,
            "logloss": 9.046535647025605,
            "mae": 0.47914899434884706,
            "precision": 0.5588235294117647,
            "recall": 0.47401247401247404
        },
        "train": {
            "accuracy": 0.5082327113062569,
            "auc_prc": 0.5653667184090799,
            "auditor_fn_violation": 0.018175784341255472,
            "auditor_fp_violation": 0.029908425183826304,
            "ave_precision_score": 0.5669014325239166,
            "fpr": 0.21405049396267836,
            "logloss": 8.970630439850906,
            "mae": 0.4872309304787707,
            "precision": 0.5301204819277109,
            "recall": 0.46511627906976744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.5916420850497008,
            "auditor_fn_violation": 0.001951344056607245,
            "auditor_fp_violation": 0.00500417226360565,
            "ave_precision_score": 0.5913429438261832,
            "fpr": 0.01206140350877193,
            "logloss": 7.794502539026786,
            "mae": 0.5305631289480073,
            "precision": 0.3888888888888889,
            "recall": 0.014553014553014554
        },
        "train": {
            "accuracy": 0.47200878155872666,
            "auc_prc": 0.5908732597818813,
            "auditor_fn_violation": 0.0013692176661569,
            "auditor_fp_violation": 0.004741640727987208,
            "ave_precision_score": 0.5923622666226129,
            "fpr": 0.010976948408342482,
            "logloss": 7.541279004060212,
            "mae": 0.5282194815063581,
            "precision": 0.16666666666666666,
            "recall": 0.004228329809725159
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5778508771929824,
            "auc_prc": 0.632438703653052,
            "auditor_fn_violation": 0.01558795637743008,
            "auditor_fp_violation": 0.0076652623437945214,
            "ave_precision_score": 0.6308859163420252,
            "fpr": 0.051535087719298246,
            "logloss": 4.162601458522538,
            "mae": 0.4442528550309326,
            "precision": 0.7526315789473684,
            "recall": 0.2972972972972973
        },
        "train": {
            "accuracy": 0.5817782656421515,
            "auc_prc": 0.6227018971713558,
            "auditor_fn_violation": 0.01135058238164971,
            "auditor_fp_violation": 0.012854056709221141,
            "ave_precision_score": 0.6231063816669393,
            "fpr": 0.05378704720087816,
            "logloss": 4.124207833625348,
            "mae": 0.4332923839038695,
            "precision": 0.7421052631578947,
            "recall": 0.29809725158562367
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 1318,
        "test": {
            "accuracy": 0.6041666666666666,
            "auc_prc": 0.6901980377726916,
            "auditor_fn_violation": 0.00984790458474669,
            "auditor_fp_violation": 0.0023074652989783077,
            "ave_precision_score": 0.6554874404637868,
            "fpr": 0.10307017543859649,
            "logloss": 5.264424647766372,
            "mae": 0.39669839072214347,
            "precision": 0.6948051948051948,
            "recall": 0.44490644490644493
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.6931036624792064,
            "auditor_fn_violation": 0.017289273920116595,
            "auditor_fp_violation": 0.01997403625901589,
            "ave_precision_score": 0.6588105374232088,
            "fpr": 0.11306256860592755,
            "logloss": 4.93596452255102,
            "mae": 0.39309110097713634,
            "precision": 0.6801242236024845,
            "recall": 0.4630021141649049
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 1318,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.6245175639280067,
            "auditor_fn_violation": 0.0035311120837436686,
            "auditor_fp_violation": 0.0015518785362478122,
            "ave_precision_score": 0.6241962441876341,
            "fpr": 0.008771929824561403,
            "logloss": 6.873975804518311,
            "mae": 0.5253138738587873,
            "precision": 0.5555555555555556,
            "recall": 0.02079002079002079
        },
        "train": {
            "accuracy": 0.47420417124039516,
            "auc_prc": 0.6152154971886962,
            "auditor_fn_violation": 0.0013692176661569,
            "auditor_fp_violation": 0.0035136259517114517,
            "ave_precision_score": 0.6167529946037432,
            "fpr": 0.008781558726673985,
            "logloss": 6.785661095592378,
            "mae": 0.5245648466634663,
            "precision": 0.2,
            "recall": 0.004228329809725159
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.5758978695628244,
            "auditor_fn_violation": 0.002680818470292155,
            "auditor_fp_violation": 0.0026152969430536903,
            "ave_precision_score": 0.5756271150071188,
            "fpr": 0.010964912280701754,
            "logloss": 8.443348954588197,
            "mae": 0.5304996443188351,
            "precision": 0.375,
            "recall": 0.012474012474012475
        },
        "train": {
            "accuracy": 0.47310647639956094,
            "auc_prc": 0.5723349539657065,
            "auditor_fn_violation": 0.0010744877617468545,
            "auditor_fp_violation": 0.004190287154965441,
            "ave_precision_score": 0.5739075489194472,
            "fpr": 0.010976948408342482,
            "logloss": 8.0543074368577,
            "mae": 0.5269819595099255,
            "precision": 0.23076923076923078,
            "recall": 0.006342494714587738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5252192982456141,
            "auc_prc": 0.5961615940655738,
            "auditor_fn_violation": 0.015113798008534867,
            "auditor_fp_violation": 0.009573309724427078,
            "ave_precision_score": 0.5947081977311366,
            "fpr": 0.05592105263157895,
            "logloss": 3.991247809867497,
            "mae": 0.48167966074718516,
            "precision": 0.66,
            "recall": 0.20582120582120583
        },
        "train": {
            "accuracy": 0.5225027442371021,
            "auc_prc": 0.5524734370461424,
            "auditor_fn_violation": 0.009860687904238333,
            "auditor_fp_violation": 0.011558375812619986,
            "ave_precision_score": 0.552990079659008,
            "fpr": 0.06366630076838639,
            "logloss": 4.172790606438876,
            "mae": 0.48458281294224037,
            "precision": 0.6233766233766234,
            "recall": 0.2029598308668076
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5219298245614035,
            "auc_prc": 0.6004905941221979,
            "auditor_fn_violation": 0.006097950176897542,
            "auditor_fp_violation": 0.005632555867627306,
            "ave_precision_score": 0.6003016932743142,
            "fpr": 0.027412280701754384,
            "logloss": 6.24493181970997,
            "mae": 0.47725247508070284,
            "precision": 0.7368421052631579,
            "recall": 0.14553014553014554
        },
        "train": {
            "accuracy": 0.5268935236004391,
            "auc_prc": 0.5928634501509116,
            "auditor_fn_violation": 0.005961898617554309,
            "auditor_fp_violation": 0.0058643970948679,
            "ave_precision_score": 0.594317163457924,
            "fpr": 0.025246981339187707,
            "logloss": 6.164077941274041,
            "mae": 0.4696898237204134,
            "precision": 0.7386363636363636,
            "recall": 0.13742071881606766
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4725877192982456,
            "auc_prc": 0.6106249453194663,
            "auditor_fn_violation": 0.0027606047342889502,
            "auditor_fp_violation": 0.002111572434566696,
            "ave_precision_score": 0.6102965075564499,
            "fpr": 0.009868421052631578,
            "logloss": 6.9852011838981545,
            "mae": 0.5231126117382632,
            "precision": 0.5,
            "recall": 0.018711018711018712
        },
        "train": {
            "accuracy": 0.47420417124039516,
            "auc_prc": 0.6034089258619894,
            "auditor_fn_violation": 0.00024599503832649534,
            "auditor_fp_violation": 0.003909598063245267,
            "ave_precision_score": 0.6049152620970715,
            "fpr": 0.009879253567508232,
            "logloss": 6.862536016103649,
            "mae": 0.5245146377329524,
            "precision": 0.25,
            "recall": 0.006342494714587738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 1318,
        "test": {
            "accuracy": 0.48355263157894735,
            "auc_prc": 0.6101470771645185,
            "auditor_fn_violation": 0.0086214757267389,
            "auditor_fp_violation": 0.002139557129482639,
            "ave_precision_score": 0.6098090162167534,
            "fpr": 0.01206140350877193,
            "logloss": 6.351556647857373,
            "mae": 0.5109984758468924,
            "precision": 0.65625,
            "recall": 0.04365904365904366
        },
        "train": {
            "accuracy": 0.47530186608122943,
            "auc_prc": 0.6031542814186222,
            "auditor_fn_violation": 0.0014806116457764457,
            "auditor_fp_violation": 0.004363211684685905,
            "ave_precision_score": 0.6046242278695383,
            "fpr": 0.014270032930845226,
            "logloss": 6.259126663130589,
            "mae": 0.5125329174456134,
            "precision": 0.38095238095238093,
            "recall": 0.016913319238900635
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5208333333333334,
            "auc_prc": 0.562573586329874,
            "auditor_fn_violation": 0.014140405587774017,
            "auditor_fp_violation": 0.020029409370293492,
            "ave_precision_score": 0.562360726150926,
            "fpr": 0.08771929824561403,
            "logloss": 4.721852907110455,
            "mae": 0.48104793129421053,
            "precision": 0.6078431372549019,
            "recall": 0.2577962577962578
        },
        "train": {
            "accuracy": 0.5389681668496158,
            "auc_prc": 0.5463572385673043,
            "auditor_fn_violation": 0.014070452050693549,
            "auditor_fp_violation": 0.015041927933075707,
            "ave_precision_score": 0.5478272561892678,
            "fpr": 0.08342480790340286,
            "logloss": 4.597958100733789,
            "mae": 0.47196990755014806,
            "precision": 0.6292682926829268,
            "recall": 0.2727272727272727
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4769736842105263,
            "auc_prc": 0.6295889894148787,
            "auditor_fn_violation": 0.006193693693693725,
            "auditor_fp_violation": 0.002111572434566696,
            "ave_precision_score": 0.6292575008862719,
            "fpr": 0.009868421052631578,
            "logloss": 6.307139209284985,
            "mae": 0.5206278111755143,
            "precision": 0.5909090909090909,
            "recall": 0.02702702702702703
        },
        "train": {
            "accuracy": 0.47420417124039516,
            "auc_prc": 0.6248614986559283,
            "auditor_fn_violation": 0.0012972757209859234,
            "auditor_fp_violation": 0.003794315043431625,
            "ave_precision_score": 0.6263840635155178,
            "fpr": 0.010976948408342482,
            "logloss": 6.181049454118379,
            "mae": 0.5215069199028726,
            "precision": 0.2857142857142857,
            "recall": 0.008456659619450317
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 1318,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.6030443727441819,
            "auditor_fn_violation": 0.001951344056607245,
            "auditor_fp_violation": 0.002111572434566696,
            "ave_precision_score": 0.6027498342201146,
            "fpr": 0.009868421052631578,
            "logloss": 8.483717781968023,
            "mae": 0.5276001952193674,
            "precision": 0.4375,
            "recall": 0.014553014553014554
        },
        "train": {
            "accuracy": 0.47310647639956094,
            "auc_prc": 0.5956017384428687,
            "auditor_fn_violation": 0.000689250248895931,
            "auditor_fp_violation": 0.0035136259517114517,
            "ave_precision_score": 0.5971719741406027,
            "fpr": 0.008781558726673985,
            "logloss": 8.260219399809172,
            "mae": 0.5260988215009936,
            "precision": 0.1111111111111111,
            "recall": 0.0021141649048625794
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 1318,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.7695005280104772,
            "auditor_fn_violation": 0.02016084910821754,
            "auditor_fp_violation": 0.014358692555053534,
            "ave_precision_score": 0.7697196729236485,
            "fpr": 0.11842105263157894,
            "logloss": 1.9057411196521483,
            "mae": 0.33534856622287673,
            "precision": 0.7365853658536585,
            "recall": 0.6278586278586279
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7923293149337397,
            "auditor_fn_violation": 0.021751995228624553,
            "auditor_fp_violation": 0.010134881133181966,
            "ave_precision_score": 0.7923731738720067,
            "fpr": 0.09769484083424808,
            "logloss": 2.2180433277805855,
            "mae": 0.3265225525632348,
            "precision": 0.7657894736842106,
            "recall": 0.6152219873150105
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.6134070237218566,
            "auditor_fn_violation": 0.004470310391363049,
            "auditor_fp_violation": 0.001607847926079701,
            "ave_precision_score": 0.6147946791732133,
            "fpr": 0.006578947368421052,
            "logloss": 6.564965238560123,
            "mae": 0.5157540170314705,
            "precision": 0.6470588235294118,
            "recall": 0.02286902286902287
        },
        "train": {
            "accuracy": 0.48518111964873767,
            "auc_prc": 0.6259835750398937,
            "auditor_fn_violation": 0.0022487659635695594,
            "auditor_fp_violation": 0.0016014315143677731,
            "ave_precision_score": 0.627577763129398,
            "fpr": 0.003293084522502744,
            "logloss": 6.506934890009769,
            "mae": 0.5065664063963413,
            "precision": 0.7,
            "recall": 0.014799154334038054
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 1318,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.5966751936823279,
            "auditor_fn_violation": 0.002680818470292155,
            "auditor_fp_violation": 0.0015518785362478122,
            "ave_precision_score": 0.5964387845377235,
            "fpr": 0.008771929824561403,
            "logloss": 8.732150229147706,
            "mae": 0.5303565908683516,
            "precision": 0.42857142857142855,
            "recall": 0.012474012474012475
        },
        "train": {
            "accuracy": 0.47310647639956094,
            "auc_prc": 0.5903723875831709,
            "auditor_fn_violation": 0.000689250248895931,
            "auditor_fp_violation": 0.0035136259517114517,
            "ave_precision_score": 0.5919799588653206,
            "fpr": 0.008781558726673985,
            "logloss": 8.474899318214428,
            "mae": 0.5260292312588601,
            "precision": 0.1111111111111111,
            "recall": 0.0021141649048625794
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5635964912280702,
            "auc_prc": 0.6060709850316975,
            "auditor_fn_violation": 0.01573841047525258,
            "auditor_fp_violation": 0.007334534131151546,
            "ave_precision_score": 0.6062896718285453,
            "fpr": 0.0625,
            "logloss": 6.298057554546839,
            "mae": 0.44884484883912734,
            "precision": 0.7106598984771574,
            "recall": 0.2910602910602911
        },
        "train": {
            "accuracy": 0.5740944017563118,
            "auc_prc": 0.6216242177357105,
            "auditor_fn_violation": 0.01697597835243662,
            "auditor_fp_violation": 0.014676029652797616,
            "ave_precision_score": 0.6231877606784155,
            "fpr": 0.04939626783754116,
            "logloss": 6.1643387284567615,
            "mae": 0.4355093274312318,
            "precision": 0.7428571428571429,
            "recall": 0.2748414376321353
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 1318,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.6169044724531848,
            "auditor_fn_violation": 0.00710553671079987,
            "auditor_fp_violation": 0.0036380103390727396,
            "ave_precision_score": 0.6165600951920247,
            "fpr": 0.013157894736842105,
            "logloss": 5.892911641978677,
            "mae": 0.5058737734728176,
            "precision": 0.7073170731707317,
            "recall": 0.060291060291060294
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.609221759504064,
            "auditor_fn_violation": 0.0026734555108690506,
            "auditor_fp_violation": 0.004363211684685905,
            "ave_precision_score": 0.610718924378201,
            "fpr": 0.014270032930845226,
            "logloss": 5.798967069788678,
            "mae": 0.5066497425284839,
            "precision": 0.5185185185185185,
            "recall": 0.02959830866807611
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.5909690618496435,
            "auditor_fn_violation": 0.023124338913812596,
            "auditor_fp_violation": 0.014773374852444336,
            "ave_precision_score": 0.5892054014898743,
            "fpr": 0.11732456140350878,
            "logloss": 5.870487432049248,
            "mae": 0.44161543414504584,
            "precision": 0.6503267973856209,
            "recall": 0.41372141372141374
        },
        "train": {
            "accuracy": 0.5686059275521405,
            "auc_prc": 0.5846401986176724,
            "auditor_fn_violation": 0.011533918306440204,
            "auditor_fp_violation": 0.02087374504408323,
            "ave_precision_score": 0.5860317126567143,
            "fpr": 0.11964873765093303,
            "logloss": 5.682915028063699,
            "mae": 0.4388644266500045,
            "precision": 0.6342281879194631,
            "recall": 0.39957716701902746
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 1318,
        "test": {
            "accuracy": 0.49451754385964913,
            "auc_prc": 0.6369026430787939,
            "auditor_fn_violation": 0.008304610278294516,
            "auditor_fp_violation": 0.004123926405340497,
            "ave_precision_score": 0.6365479469685196,
            "fpr": 0.01425438596491228,
            "logloss": 4.966977239576941,
            "mae": 0.5028284965990639,
            "precision": 0.717391304347826,
            "recall": 0.06860706860706861
        },
        "train": {
            "accuracy": 0.4884742041712404,
            "auc_prc": 0.6297647295270569,
            "auditor_fn_violation": 0.0019726017224294145,
            "auditor_fp_violation": 0.000541328962603191,
            "ave_precision_score": 0.6302140184737077,
            "fpr": 0.013172338090010977,
            "logloss": 4.871593778256963,
            "mae": 0.5007028970606674,
            "precision": 0.6129032258064516,
            "recall": 0.040169133192389
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 1318,
        "test": {
            "accuracy": 0.6041666666666666,
            "auc_prc": 0.7573362911932486,
            "auditor_fn_violation": 0.0032666776087828726,
            "auditor_fp_violation": 0.012824622461024967,
            "ave_precision_score": 0.757822271146702,
            "fpr": 0.3782894736842105,
            "logloss": 1.2992454148362524,
            "mae": 0.384530283389275,
            "precision": 0.5740740740740741,
            "recall": 0.9667359667359667
        },
        "train": {
            "accuracy": 0.5982436882546652,
            "auc_prc": 0.7679638004343993,
            "auditor_fn_violation": 0.005502398451623684,
            "auditor_fp_violation": 0.00989930278834538,
            "ave_precision_score": 0.7683326711818379,
            "fpr": 0.38419319429198684,
            "logloss": 1.300948188794189,
            "mae": 0.38171729707717517,
            "precision": 0.5662949194547707,
            "recall": 0.9661733615221987
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5603070175438597,
            "auc_prc": 0.59870671743785,
            "auditor_fn_violation": 0.021049896049896058,
            "auditor_fp_violation": 0.020154068465828155,
            "ave_precision_score": 0.5993023958540615,
            "fpr": 0.14912280701754385,
            "logloss": 6.6351736119102585,
            "mae": 0.4450553675163935,
            "precision": 0.6136363636363636,
            "recall": 0.4490644490644491
        },
        "train": {
            "accuracy": 0.5598243688254665,
            "auc_prc": 0.6101703022100767,
            "auditor_fn_violation": 0.01406348992696733,
            "auditor_fp_violation": 0.02968788375461759,
            "ave_precision_score": 0.6117463037418605,
            "fpr": 0.14709110867178923,
            "logloss": 6.646246407719129,
            "mae": 0.4429888132875738,
            "precision": 0.6058823529411764,
            "recall": 0.4355179704016913
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5657894736842105,
            "auc_prc": 0.5996406492555553,
            "auditor_fn_violation": 0.022091676696939868,
            "auditor_fp_violation": 0.008006166809134203,
            "ave_precision_score": 0.5993865378471863,
            "fpr": 0.05701754385964912,
            "logloss": 9.45162096855496,
            "mae": 0.4470475929923253,
            "precision": 0.7248677248677249,
            "recall": 0.28482328482328484
        },
        "train": {
            "accuracy": 0.5675082327113062,
            "auc_prc": 0.5876353663468812,
            "auditor_fn_violation": 0.008261720155116127,
            "auditor_fp_violation": 0.015317604719586583,
            "ave_precision_score": 0.5892195045025642,
            "fpr": 0.06147091108671789,
            "logloss": 9.302661780488041,
            "mae": 0.4380619718411911,
            "precision": 0.7068062827225131,
            "recall": 0.2854122621564482
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4692982456140351,
            "auc_prc": 0.5697517022022003,
            "auditor_fn_violation": 0.002680818470292155,
            "auditor_fp_violation": 0.0020199861602963327,
            "ave_precision_score": 0.5694660426783318,
            "fpr": 0.009868421052631578,
            "logloss": 8.882996535154634,
            "mae": 0.5315298964492501,
            "precision": 0.4,
            "recall": 0.012474012474012475
        },
        "train": {
            "accuracy": 0.47200878155872666,
            "auc_prc": 0.5657238160651467,
            "auditor_fn_violation": 0.000689250248895931,
            "auditor_fp_violation": 0.003571267461618273,
            "ave_precision_score": 0.5672958782415449,
            "fpr": 0.009879253567508232,
            "logloss": 8.426119953649716,
            "mae": 0.5273994695697948,
            "precision": 0.1,
            "recall": 0.0021141649048625794
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.5613471544824288,
            "auditor_fn_violation": 0.0006154940365466923,
            "auditor_fp_violation": 0.0016129360524280542,
            "ave_precision_score": 0.5627697291645939,
            "fpr": 0.005482456140350877,
            "logloss": 9.767280088265153,
            "mae": 0.5313874755722108,
            "precision": 0.16666666666666666,
            "recall": 0.002079002079002079
        },
        "train": {
            "accuracy": 0.4774972557628979,
            "auc_prc": 0.5719874050472529,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0016014315143677731,
            "ave_precision_score": 0.5737654186743466,
            "fpr": 0.003293084522502744,
            "logloss": 9.423259209647457,
            "mae": 0.522250154804887,
            "precision": 0.0,
            "recall": 0.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 1318,
        "test": {
            "accuracy": 0.49890350877192985,
            "auc_prc": 0.7530242559814433,
            "auditor_fn_violation": 0.006617700696648077,
            "auditor_fp_violation": 0.00159258354703464,
            "ave_precision_score": 0.7547230162896211,
            "fpr": 0.006578947368421052,
            "logloss": 2.8371676052519446,
            "mae": 0.486054655344878,
            "precision": 0.8333333333333334,
            "recall": 0.062370062370062374
        },
        "train": {
            "accuracy": 0.5126234906695939,
            "auc_prc": 0.7655798243996417,
            "auditor_fn_violation": 0.00576927986112884,
            "auditor_fp_violation": 0.0018921452165065239,
            "ave_precision_score": 0.767009667594577,
            "fpr": 0.007683863885839737,
            "logloss": 2.670258187401699,
            "mae": 0.4742772666432045,
            "precision": 0.8372093023255814,
            "recall": 0.07610993657505286
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 1318,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.6171772501772659,
            "auditor_fn_violation": 0.00710553671079987,
            "auditor_fp_violation": 0.0036380103390727396,
            "ave_precision_score": 0.6168339968140422,
            "fpr": 0.013157894736842105,
            "logloss": 5.855520922868732,
            "mae": 0.5055823553761475,
            "precision": 0.7073170731707317,
            "recall": 0.060291060291060294
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.6099126194701683,
            "auditor_fn_violation": 0.0026734555108690506,
            "auditor_fp_violation": 0.004363211684685905,
            "ave_precision_score": 0.6114105384542803,
            "fpr": 0.014270032930845226,
            "logloss": 5.760975648982116,
            "mae": 0.5062965281614035,
            "precision": 0.5185185185185185,
            "recall": 0.02959830866807611
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.6091247273766132,
            "auditor_fn_violation": 0.007331217857533653,
            "auditor_fp_violation": 0.002055603044734807,
            "ave_precision_score": 0.6087899640898566,
            "fpr": 0.01206140350877193,
            "logloss": 6.639484654087813,
            "mae": 0.5153567331187243,
            "precision": 0.5925925925925926,
            "recall": 0.033264033264033266
        },
        "train": {
            "accuracy": 0.47200878155872666,
            "auc_prc": 0.6040458090268701,
            "auditor_fn_violation": 0.0005430456506452812,
            "auditor_fp_violation": 0.004450927025848458,
            "ave_precision_score": 0.6055083571368154,
            "fpr": 0.013172338090010977,
            "logloss": 6.54709162110823,
            "mae": 0.5171972879180373,
            "precision": 0.25,
            "recall": 0.008456659619450317
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 1318,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.6036123895617529,
            "auditor_fn_violation": 0.0027606047342889502,
            "auditor_fp_violation": 0.0026152969430536903,
            "ave_precision_score": 0.6033054392466195,
            "fpr": 0.010964912280701754,
            "logloss": 7.500879826856948,
            "mae": 0.5261385597240955,
            "precision": 0.47368421052631576,
            "recall": 0.018711018711018712
        },
        "train": {
            "accuracy": 0.47310647639956094,
            "auc_prc": 0.5986441574968977,
            "auditor_fn_violation": 0.001724285976194179,
            "auditor_fp_violation": 0.004774220711847587,
            "ave_precision_score": 0.6001693269804678,
            "fpr": 0.012074643249176729,
            "logloss": 7.1772802923700505,
            "mae": 0.5254199710697098,
            "precision": 0.26666666666666666,
            "recall": 0.008456659619450317
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4692982456140351,
            "auc_prc": 0.5848645911651611,
            "auditor_fn_violation": 0.001951344056607245,
            "auditor_fp_violation": 0.0026152969430536903,
            "ave_precision_score": 0.5845852685649625,
            "fpr": 0.010964912280701754,
            "logloss": 8.128583518601891,
            "mae": 0.5294438258804302,
            "precision": 0.4117647058823529,
            "recall": 0.014553014553014554
        },
        "train": {
            "accuracy": 0.47200878155872666,
            "auc_prc": 0.5833641534397545,
            "auditor_fn_violation": 0.0013692176661569,
            "auditor_fp_violation": 0.004190287154965441,
            "ave_precision_score": 0.5849310980967313,
            "fpr": 0.010976948408342482,
            "logloss": 7.748629833090579,
            "mae": 0.5269521316522473,
            "precision": 0.16666666666666666,
            "recall": 0.004228329809725159
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 1318,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7569982668651554,
            "auditor_fn_violation": 0.018052212131159505,
            "auditor_fp_violation": 0.01819513982171206,
            "ave_precision_score": 0.7553801476018209,
            "fpr": 0.13706140350877194,
            "logloss": 1.56523632853873,
            "mae": 0.3090776858382399,
            "precision": 0.7264770240700219,
            "recall": 0.6902286902286903
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7505490999048106,
            "auditor_fn_violation": 0.019988257217981784,
            "auditor_fp_violation": 0.01921467201980863,
            "ave_precision_score": 0.7508646634067161,
            "fpr": 0.13721185510428102,
            "logloss": 1.4005844492803678,
            "mae": 0.3084907125137313,
            "precision": 0.7209821428571429,
            "recall": 0.6828752642706131
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 1318,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7682686470707883,
            "auditor_fn_violation": 0.022593190356348252,
            "auditor_fp_violation": 0.01628200431473115,
            "ave_precision_score": 0.7656970938793275,
            "fpr": 0.13157894736842105,
            "logloss": 1.5584300645619753,
            "mae": 0.30280423686995644,
            "precision": 0.7321428571428571,
            "recall": 0.681912681912682
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7620470218093125,
            "auditor_fn_violation": 0.02009036836596636,
            "auditor_fp_violation": 0.024765800039095984,
            "ave_precision_score": 0.7614947546192152,
            "fpr": 0.11855104281009879,
            "logloss": 1.3908501387153778,
            "mae": 0.2963367575580744,
            "precision": 0.7511520737327189,
            "recall": 0.6892177589852009
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5219298245614035,
            "auc_prc": 0.5769007224692538,
            "auditor_fn_violation": 0.013597858992595826,
            "auditor_fp_violation": 0.018452090202303904,
            "ave_precision_score": 0.5662014484417625,
            "fpr": 0.051535087719298246,
            "logloss": 8.418139288171151,
            "mae": 0.4782558936145487,
            "precision": 0.6618705035971223,
            "recall": 0.19126819126819128
        },
        "train": {
            "accuracy": 0.5214050493962679,
            "auc_prc": 0.5669134891127456,
            "auditor_fn_violation": 0.015685664755176926,
            "auditor_fp_violation": 0.02100657113212938,
            "ave_precision_score": 0.555813016057495,
            "fpr": 0.05817782656421515,
            "logloss": 8.19631964222322,
            "mae": 0.47792778584366563,
            "precision": 0.6293706293706294,
            "recall": 0.19027484143763213
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 1318,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.6181428907855384,
            "auditor_fn_violation": 0.00710553671079987,
            "auditor_fp_violation": 0.0036380103390727396,
            "ave_precision_score": 0.6177992874026981,
            "fpr": 0.013157894736842105,
            "logloss": 5.836634321531162,
            "mae": 0.5059155158734061,
            "precision": 0.7073170731707317,
            "recall": 0.060291060291060294
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.6106384559490328,
            "auditor_fn_violation": 0.0026734555108690506,
            "auditor_fp_violation": 0.004363211684685905,
            "ave_precision_score": 0.6121396164928586,
            "fpr": 0.014270032930845226,
            "logloss": 5.741075571136351,
            "mae": 0.5067164906881172,
            "precision": 0.5185185185185185,
            "recall": 0.02959830866807611
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5614035087719298,
            "auc_prc": 0.5933756632336212,
            "auditor_fn_violation": 0.0208492905861327,
            "auditor_fp_violation": 0.00413410265803721,
            "ave_precision_score": 0.5931039146338271,
            "fpr": 0.07456140350877193,
            "logloss": 6.509556579457206,
            "mae": 0.44690203533133177,
            "precision": 0.6866359447004609,
            "recall": 0.3097713097713098
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.5856997282780101,
            "auditor_fn_violation": 0.01868866078908712,
            "auditor_fp_violation": 0.015397801602935211,
            "ave_precision_score": 0.5871827181855993,
            "fpr": 0.06805708013172337,
            "logloss": 6.47586029470097,
            "mae": 0.43951046377360925,
            "precision": 0.7142857142857143,
            "recall": 0.3276955602536998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 1318,
        "test": {
            "accuracy": 0.48464912280701755,
            "auc_prc": 0.6257649522984119,
            "auditor_fn_violation": 0.0077871393660867315,
            "auditor_fp_violation": 0.002139557129482639,
            "ave_precision_score": 0.6254189861140362,
            "fpr": 0.01206140350877193,
            "logloss": 5.66094644731397,
            "mae": 0.5101846454249235,
            "precision": 0.6666666666666666,
            "recall": 0.04573804573804574
        },
        "train": {
            "accuracy": 0.4774972557628979,
            "auc_prc": 0.6178580472971453,
            "auditor_fn_violation": 0.002044543667600366,
            "auditor_fp_violation": 0.0037441919913387377,
            "ave_precision_score": 0.6193644909302649,
            "fpr": 0.013172338090010977,
            "logloss": 5.559652659932938,
            "mae": 0.510906803097976,
            "precision": 0.42857142857142855,
            "recall": 0.019027484143763214
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 1318,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.5797150577508767,
            "auditor_fn_violation": 0.0038684940000729734,
            "auditor_fp_violation": 0.0036380103390727396,
            "ave_precision_score": 0.5794571454099305,
            "fpr": 0.013157894736842105,
            "logloss": 7.895833735551419,
            "mae": 0.5286111584635198,
            "precision": 0.4782608695652174,
            "recall": 0.02286902286902287
        },
        "train": {
            "accuracy": 0.47530186608122943,
            "auc_prc": 0.5729254679089286,
            "auditor_fn_violation": 0.0029612232915528594,
            "auditor_fp_violation": 0.003571267461618273,
            "ave_precision_score": 0.5745478642748133,
            "fpr": 0.009879253567508232,
            "logloss": 7.542587224907022,
            "mae": 0.5248137019986996,
            "precision": 0.3076923076923077,
            "recall": 0.008456659619450317
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4769736842105263,
            "auc_prc": 0.6138199824546194,
            "auditor_fn_violation": 0.005797041981252504,
            "auditor_fp_violation": 0.0026661782065372248,
            "ave_precision_score": 0.6135403230785772,
            "fpr": 0.010964912280701754,
            "logloss": 6.13065333708874,
            "mae": 0.5160787897696117,
            "precision": 0.5833333333333334,
            "recall": 0.029106029106029108
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0.6043733695613434,
            "auditor_fn_violation": 0.0008772275895039033,
            "auditor_fp_violation": 0.003295590675107389,
            "ave_precision_score": 0.6058696893018944,
            "fpr": 0.012074643249176729,
            "logloss": 6.0300934399293915,
            "mae": 0.5135552464303781,
            "precision": 0.47619047619047616,
            "recall": 0.021141649048625793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 1318,
        "test": {
            "accuracy": 0.6502192982456141,
            "auc_prc": 0.7687884127925544,
            "auditor_fn_violation": 0.014708027865922603,
            "auditor_fp_violation": 0.010608743436317016,
            "ave_precision_score": 0.7325040235081495,
            "fpr": 0.08333333333333333,
            "logloss": 5.067866759783508,
            "mae": 0.34889913319825466,
            "precision": 0.7579617834394905,
            "recall": 0.49480249480249483
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.7749897471743158,
            "auditor_fn_violation": 0.019533398467868647,
            "auditor_fp_violation": 0.008796595642301857,
            "ave_precision_score": 0.7351234708544339,
            "fpr": 0.08122941822173436,
            "logloss": 4.827790805456028,
            "mae": 0.3289893411922313,
            "precision": 0.7694704049844237,
            "recall": 0.5221987315010571
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 1318,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.6171945095691485,
            "auditor_fn_violation": 0.00710553671079987,
            "auditor_fp_violation": 0.0036380103390727396,
            "ave_precision_score": 0.6168507768496327,
            "fpr": 0.013157894736842105,
            "logloss": 5.8785877053840405,
            "mae": 0.5059503346052105,
            "precision": 0.7073170731707317,
            "recall": 0.060291060291060294
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.6096489597066659,
            "auditor_fn_violation": 0.0026734555108690506,
            "auditor_fp_violation": 0.004363211684685905,
            "ave_precision_score": 0.611145643988509,
            "fpr": 0.014270032930845226,
            "logloss": 5.784204383886035,
            "mae": 0.5067498529739709,
            "precision": 0.5185185185185185,
            "recall": 0.02959830866807611
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 1318,
        "test": {
            "accuracy": 0.49451754385964913,
            "auc_prc": 0.6216256898108294,
            "auditor_fn_violation": 0.007714191924718239,
            "auditor_fp_violation": 0.0036380103390727396,
            "ave_precision_score": 0.621277947995674,
            "fpr": 0.013157894736842105,
            "logloss": 5.60616096751594,
            "mae": 0.5028937357279669,
            "precision": 0.7272727272727273,
            "recall": 0.06652806652806653
        },
        "train": {
            "accuracy": 0.48518111964873767,
            "auc_prc": 0.6136802974255556,
            "auditor_fn_violation": 0.0037316983172546916,
            "auditor_fp_violation": 0.0037842904330130478,
            "ave_precision_score": 0.6151866817105025,
            "fpr": 0.015367727771679473,
            "logloss": 5.509065570067508,
            "mae": 0.5027528187225425,
            "precision": 0.5625,
            "recall": 0.03805496828752643
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5515350877192983,
            "auc_prc": 0.6303105518330687,
            "auditor_fn_violation": 0.0009118430171061755,
            "auditor_fp_violation": 0.005983636585663702,
            "ave_precision_score": 0.6208525236559533,
            "fpr": 0.43640350877192985,
            "logloss": 5.2899141864303765,
            "mae": 0.4476252818790755,
            "precision": 0.5414746543778802,
            "recall": 0.9771309771309772
        },
        "train": {
            "accuracy": 0.5477497255762898,
            "auc_prc": 0.6224951507226013,
            "auditor_fn_violation": 0.003935920613223859,
            "auditor_fp_violation": 0.002959766226085053,
            "ave_precision_score": 0.6126493069665221,
            "fpr": 0.442371020856202,
            "logloss": 5.335876428323101,
            "mae": 0.45177857331365534,
            "precision": 0.5351787773933102,
            "recall": 0.9809725158562368
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4824561403508772,
            "auc_prc": 0.6123642304377659,
            "auditor_fn_violation": 0.004771218587008073,
            "auditor_fp_violation": 0.001190621565514715,
            "ave_precision_score": 0.6137479644202376,
            "fpr": 0.007675438596491228,
            "logloss": 6.513179745990736,
            "mae": 0.509991472186776,
            "precision": 0.6956521739130435,
            "recall": 0.033264033264033266
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.6265608464752556,
            "auditor_fn_violation": 0.0007217401596182844,
            "auditor_fp_violation": 0.00205504513580841,
            "ave_precision_score": 0.628151099327152,
            "fpr": 0.0043907793633369925,
            "logloss": 6.444775714641304,
            "mae": 0.5004333933190143,
            "precision": 0.7647058823529411,
            "recall": 0.02748414376321353
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4923245614035088,
            "auc_prc": 0.617180916081586,
            "auditor_fn_violation": 0.008534850640113799,
            "auditor_fp_violation": 0.0036380103390727396,
            "ave_precision_score": 0.6168365555296238,
            "fpr": 0.013157894736842105,
            "logloss": 5.8185943211129025,
            "mae": 0.5047695153168662,
            "precision": 0.7142857142857143,
            "recall": 0.062370062370062374
        },
        "train": {
            "accuracy": 0.4840834248079034,
            "auc_prc": 0.6100626463405946,
            "auditor_fn_violation": 0.00247387463071736,
            "auditor_fp_violation": 0.004363211684685905,
            "ave_precision_score": 0.6115540404790656,
            "fpr": 0.014270032930845226,
            "logloss": 5.721826622059138,
            "mae": 0.5052344613422877,
            "precision": 0.5517241379310345,
            "recall": 0.03382663847780127
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5449561403508771,
            "auc_prc": 0.5831602836786265,
            "auditor_fn_violation": 0.008076649524017954,
            "auditor_fp_violation": 0.009123010542597797,
            "ave_precision_score": 0.5821075823735733,
            "fpr": 0.08333333333333333,
            "logloss": 5.180944188129358,
            "mae": 0.46546794893392923,
            "precision": 0.6513761467889908,
            "recall": 0.29521829521829523
        },
        "train": {
            "accuracy": 0.5510428100987925,
            "auc_prc": 0.5802537654907219,
            "auditor_fn_violation": 0.017402988607644897,
            "auditor_fp_violation": 0.016375201118746525,
            "ave_precision_score": 0.5807376746509164,
            "fpr": 0.08562019758507135,
            "logloss": 4.873103266863523,
            "mae": 0.4586591817530384,
            "precision": 0.6454545454545455,
            "recall": 0.30021141649048627
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 1318,
        "test": {
            "accuracy": 0.46600877192982454,
            "auc_prc": 0.5556331448333006,
            "auditor_fn_violation": 0.0006154940365466923,
            "auditor_fp_violation": 0.00305287580901209,
            "ave_precision_score": 0.5573716675952717,
            "fpr": 0.007675438596491228,
            "logloss": 9.041035207346633,
            "mae": 0.534108719063924,
            "precision": 0.125,
            "recall": 0.002079002079002079
        },
        "train": {
            "accuracy": 0.47530186608122943,
            "auc_prc": 0.5640230066440173,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0018345037065997023,
            "ave_precision_score": 0.565651503145502,
            "fpr": 0.005488474204171241,
            "logloss": 8.741652553101332,
            "mae": 0.5244018791604221,
            "precision": 0.0,
            "recall": 0.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5339912280701754,
            "auc_prc": 0.6116097172835329,
            "auditor_fn_violation": 0.0068889739942371775,
            "auditor_fp_violation": 0.007438840721292791,
            "ave_precision_score": 0.6112315684920835,
            "fpr": 0.03179824561403509,
            "logloss": 5.651464731676229,
            "mae": 0.4690643036290002,
            "precision": 0.7456140350877193,
            "recall": 0.17671517671517672
        },
        "train": {
            "accuracy": 0.5323819978046103,
            "auc_prc": 0.595882167403758,
            "auditor_fn_violation": 0.007860237686904036,
            "auditor_fp_violation": 0.007353051742026676,
            "ave_precision_score": 0.5973824506473762,
            "fpr": 0.03732162458836443,
            "logloss": 5.585890588549507,
            "mae": 0.46593003272732403,
            "precision": 0.7043478260869566,
            "recall": 0.17124735729386892
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5054824561403509,
            "auc_prc": 0.6535722750924664,
            "auditor_fn_violation": 0.006050078418499487,
            "auditor_fp_violation": 0.005095758537876013,
            "ave_precision_score": 0.6531866278532612,
            "fpr": 0.01644736842105263,
            "logloss": 4.897549349392126,
            "mae": 0.4908877978702625,
            "precision": 0.75,
            "recall": 0.09355509355509356
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0.6471755745884935,
            "auditor_fn_violation": 0.0023323114482842015,
            "auditor_fp_violation": 0.002298141938458917,
            "ave_precision_score": 0.6476167255680403,
            "fpr": 0.014270032930845226,
            "logloss": 4.825197451340724,
            "mae": 0.4882000961935659,
            "precision": 0.7111111111111111,
            "recall": 0.06765327695560254
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.6474557014958254,
            "auditor_fn_violation": 0.0019923769923770166,
            "auditor_fp_violation": 8.649814792200922e-05,
            "ave_precision_score": 0.6490268736315455,
            "fpr": 0.0043859649122807015,
            "logloss": 5.013474510144471,
            "mae": 0.5160035881372409,
            "precision": 0.6923076923076923,
            "recall": 0.018711018711018712
        },
        "train": {
            "accuracy": 0.4862788144895719,
            "auc_prc": 0.6450146290723624,
            "auditor_fn_violation": 0.002179144726307321,
            "auditor_fp_violation": 0.0016189745826002836,
            "ave_precision_score": 0.6466236756607795,
            "fpr": 0.0043907793633369925,
            "logloss": 5.008223715175469,
            "mae": 0.5061311209410246,
            "precision": 0.6923076923076923,
            "recall": 0.019027484143763214
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 1318,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.5961000551011508,
            "auditor_fn_violation": 0.002680818470292155,
            "auditor_fp_violation": 0.0015518785362478122,
            "ave_precision_score": 0.5959354770097498,
            "fpr": 0.008771929824561403,
            "logloss": 10.326568347446662,
            "mae": 0.5295170892231846,
            "precision": 0.42857142857142855,
            "recall": 0.012474012474012475
        },
        "train": {
            "accuracy": 0.47310647639956094,
            "auc_prc": 0.5840470296124844,
            "auditor_fn_violation": 0.000689250248895931,
            "auditor_fp_violation": 0.0035136259517114517,
            "ave_precision_score": 0.5856171217529833,
            "fpr": 0.008781558726673985,
            "logloss": 10.344514725394697,
            "mae": 0.526279623912585,
            "precision": 0.1111111111111111,
            "recall": 0.0021141649048625794
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4857456140350877,
            "auc_prc": 0.6172230413965658,
            "auditor_fn_violation": 0.0021633475580844007,
            "auditor_fp_violation": 0.0035133512435380795,
            "ave_precision_score": 0.6169468258687808,
            "fpr": 0.015350877192982455,
            "logloss": 5.535685969160318,
            "mae": 0.5087495629435466,
            "precision": 0.65,
            "recall": 0.05405405405405406
        },
        "train": {
            "accuracy": 0.48957189901207465,
            "auc_prc": 0.6088010146522356,
            "auditor_fn_violation": 0.0027291525006788242,
            "auditor_fp_violation": 0.002857013969294618,
            "ave_precision_score": 0.6103020121598718,
            "fpr": 0.013172338090010977,
            "logloss": 5.451427451234763,
            "mae": 0.5025699729935381,
            "precision": 0.625,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 1318,
        "test": {
            "accuracy": 0.48355263157894735,
            "auc_prc": 0.5918764843192759,
            "auditor_fn_violation": 0.007652642521063606,
            "auditor_fp_violation": 0.0023176415516750117,
            "ave_precision_score": 0.5915829598909108,
            "fpr": 0.009868421052631578,
            "logloss": 9.95684160714991,
            "mae": 0.51124574508074,
            "precision": 0.6785714285714286,
            "recall": 0.0395010395010395
        },
        "train": {
            "accuracy": 0.4796926454445664,
            "auc_prc": 0.592420554226021,
            "auditor_fn_violation": 0.002297500829653087,
            "auditor_fp_violation": 0.004067485677337865,
            "ave_precision_score": 0.593878013922691,
            "fpr": 0.009879253567508232,
            "logloss": 9.769324304696088,
            "mae": 0.5117038450622249,
            "precision": 0.47058823529411764,
            "recall": 0.016913319238900635
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.5819100635718275,
            "auditor_fn_violation": 0.002680818470292155,
            "auditor_fp_violation": 0.0026152969430536903,
            "ave_precision_score": 0.5816185378485723,
            "fpr": 0.010964912280701754,
            "logloss": 7.954094077281544,
            "mae": 0.5293409686115684,
            "precision": 0.375,
            "recall": 0.012474012474012475
        },
        "train": {
            "accuracy": 0.47310647639956094,
            "auc_prc": 0.582144509229962,
            "auditor_fn_violation": 0.0010744877617468545,
            "auditor_fp_violation": 0.004190287154965441,
            "ave_precision_score": 0.5836664253356179,
            "fpr": 0.010976948408342482,
            "logloss": 7.604209577685465,
            "mae": 0.5263847397175715,
            "precision": 0.23076923076923078,
            "recall": 0.006342494714587738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5614035087719298,
            "auc_prc": 0.6190551506413919,
            "auditor_fn_violation": 0.024587846956268013,
            "auditor_fp_violation": 0.015244026539667036,
            "ave_precision_score": 0.6175187880505977,
            "fpr": 0.09539473684210527,
            "logloss": 4.588345178997261,
            "mae": 0.44035064270325797,
            "precision": 0.6588235294117647,
            "recall": 0.3492723492723493
        },
        "train": {
            "accuracy": 0.579582875960483,
            "auc_prc": 0.6050300124578479,
            "auditor_fn_violation": 0.012578236865373415,
            "auditor_fp_violation": 0.018938995233297756,
            "ave_precision_score": 0.6065160078163694,
            "fpr": 0.0867178924259056,
            "logloss": 4.547317805397649,
            "mae": 0.4304520434287842,
            "precision": 0.6814516129032258,
            "recall": 0.3572938689217759
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 1318,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.772503440344009,
            "auditor_fn_violation": 0.009079676842834738,
            "auditor_fp_violation": 0.00489223348394188,
            "ave_precision_score": 0.7221677211566996,
            "fpr": 0.28728070175438597,
            "logloss": 3.5138963500220792,
            "mae": 0.34131704815510383,
            "precision": 0.6273115220483642,
            "recall": 0.9168399168399168
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7813536255404687,
            "auditor_fn_violation": 0.01043622346560595,
            "auditor_fp_violation": 0.003834413485105954,
            "ave_precision_score": 0.731895130234447,
            "fpr": 0.27661909989023054,
            "logloss": 3.281000179965143,
            "mae": 0.33188747727222856,
            "precision": 0.6363636363636364,
            "recall": 0.9323467230443975
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5405701754385965,
            "auc_prc": 0.5574677488690989,
            "auditor_fn_violation": 0.02391308312360945,
            "auditor_fp_violation": 0.02243609313306469,
            "ave_precision_score": 0.5558834424180459,
            "fpr": 0.15789473684210525,
            "logloss": 8.540938231590745,
            "mae": 0.4685778577097718,
            "precision": 0.5885714285714285,
            "recall": 0.4282744282744283
        },
        "train": {
            "accuracy": 0.531284302963776,
            "auc_prc": 0.5386755742702063,
            "auditor_fn_violation": 0.016423649870156404,
            "auditor_fp_violation": 0.02859018891378334,
            "ave_precision_score": 0.5402129918428404,
            "fpr": 0.1690450054884742,
            "logloss": 8.53768447348057,
            "mae": 0.4689700279983484,
            "precision": 0.5649717514124294,
            "recall": 0.42283298097251587
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 1318,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7545854907088508,
            "auditor_fn_violation": 0.0168850530692636,
            "auditor_fp_violation": 0.016544042821671358,
            "ave_precision_score": 0.75339767458103,
            "fpr": 0.13596491228070176,
            "logloss": 1.5796731591224868,
            "mae": 0.3109894354512869,
            "precision": 0.7268722466960352,
            "recall": 0.6860706860706861
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7499138973366921,
            "auditor_fn_violation": 0.020159989603228567,
            "auditor_fp_violation": 0.020750943566455658,
            "ave_precision_score": 0.7502499679159524,
            "fpr": 0.13611416026344675,
            "logloss": 1.3896410044855987,
            "mae": 0.3109520967194004,
            "precision": 0.7207207207207207,
            "recall": 0.6765327695560254
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.5481096316464343,
            "auditor_fn_violation": 0.000494674836780115,
            "auditor_fp_violation": 0.0024295803313387878,
            "ave_precision_score": 0.5493714315115542,
            "fpr": 0.005482456140350877,
            "logloss": 12.127032202546609,
            "mae": 0.5316115702236235,
            "precision": 0.16666666666666666,
            "recall": 0.002079002079002079
        },
        "train": {
            "accuracy": 0.4807903402854007,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 11.700180466011473,
            "mae": 0.5192071198069858,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.692973613838585,
            "auditor_fn_violation": 0.007563737826895739,
            "auditor_fp_violation": 0.0014933650832417477,
            "ave_precision_score": 0.6925334257255955,
            "fpr": 0.010964912280701754,
            "logloss": 3.9602417799931766,
            "mae": 0.48919467829667546,
            "precision": 0.8181818181818182,
            "recall": 0.09355509355509356
        },
        "train": {
            "accuracy": 0.49286498353457736,
            "auc_prc": 0.6865683376310819,
            "auditor_fn_violation": 0.0022975008296530924,
            "auditor_fp_violation": 0.002032489762366611,
            "ave_precision_score": 0.6869862206812835,
            "fpr": 0.012074643249176729,
            "logloss": 4.111855401694793,
            "mae": 0.49195663959316227,
            "precision": 0.6666666666666666,
            "recall": 0.046511627906976744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4692982456140351,
            "auc_prc": 0.5816816151045374,
            "auditor_fn_violation": 0.001951344056607245,
            "auditor_fp_violation": 0.0026152969430536903,
            "ave_precision_score": 0.5813997919384726,
            "fpr": 0.010964912280701754,
            "logloss": 8.350010941852405,
            "mae": 0.5292187155351334,
            "precision": 0.4117647058823529,
            "recall": 0.014553014553014554
        },
        "train": {
            "accuracy": 0.47200878155872666,
            "auc_prc": 0.5797018206937969,
            "auditor_fn_violation": 0.0013692176661569,
            "auditor_fp_violation": 0.004190287154965441,
            "ave_precision_score": 0.5812608632114602,
            "fpr": 0.010976948408342482,
            "logloss": 7.978617773833323,
            "mae": 0.5269834752766676,
            "precision": 0.16666666666666666,
            "recall": 0.004228329809725159
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 1318,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.7509056259358096,
            "auditor_fn_violation": 0.007604770762665513,
            "auditor_fp_violation": 0.007512618553343919,
            "ave_precision_score": 0.7487325434892323,
            "fpr": 0.03179824561403509,
            "logloss": 2.8181383991921782,
            "mae": 0.39763968766798863,
            "precision": 0.8406593406593407,
            "recall": 0.3180873180873181
        },
        "train": {
            "accuracy": 0.605927552140505,
            "auc_prc": 0.766062320119761,
            "auditor_fn_violation": 0.01878845122916296,
            "auditor_fp_violation": 0.00516267436556747,
            "ave_precision_score": 0.7656057895898944,
            "fpr": 0.02854006586169045,
            "logloss": 2.5324797888889985,
            "mae": 0.39487637750997584,
            "precision": 0.8433734939759037,
            "recall": 0.2959830866807611
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4857456140350877,
            "auc_prc": 0.5826987821706799,
            "auditor_fn_violation": 0.005660265528686587,
            "auditor_fp_violation": 0.004716693124923678,
            "ave_precision_score": 0.5824498428259803,
            "fpr": 0.015350877192982455,
            "logloss": 9.36394064809546,
            "mae": 0.5103394622229305,
            "precision": 0.65,
            "recall": 0.05405405405405406
        },
        "train": {
            "accuracy": 0.47859495060373214,
            "auc_prc": 0.5783580017003305,
            "auditor_fn_violation": 0.0010512806826594291,
            "auditor_fp_violation": 0.004977219072823783,
            "ave_precision_score": 0.5798293467096944,
            "fpr": 0.01646542261251372,
            "logloss": 9.263990653766509,
            "mae": 0.5077759093337653,
            "precision": 0.4642857142857143,
            "recall": 0.02748414376321353
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4682017543859649,
            "auc_prc": 0.5796026829122135,
            "auditor_fn_violation": 0.0006154940365466923,
            "auditor_fp_violation": 0.0016129360524280542,
            "ave_precision_score": 0.5810220325533142,
            "fpr": 0.005482456140350877,
            "logloss": 10.124493716829823,
            "mae": 0.5313215845804001,
            "precision": 0.16666666666666666,
            "recall": 0.002079002079002079
        },
        "train": {
            "accuracy": 0.4774972557628979,
            "auc_prc": 0.5881947974288995,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0016014315143677731,
            "ave_precision_score": 0.5899593455061528,
            "fpr": 0.003293084522502744,
            "logloss": 9.930693122557523,
            "mae": 0.5223092759306102,
            "precision": 0.0,
            "recall": 0.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4923245614035088,
            "auc_prc": 0.61780515171835,
            "auditor_fn_violation": 0.008534850640113799,
            "auditor_fp_violation": 0.0036380103390727396,
            "ave_precision_score": 0.6174610280350463,
            "fpr": 0.013157894736842105,
            "logloss": 5.818757488205027,
            "mae": 0.5049957776064777,
            "precision": 0.7142857142857143,
            "recall": 0.062370062370062374
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.61054080380626,
            "auditor_fn_violation": 0.0026734555108690506,
            "auditor_fp_violation": 0.004363211684685905,
            "ave_precision_score": 0.612031930645562,
            "fpr": 0.014270032930845226,
            "logloss": 5.722636291884675,
            "mae": 0.5055755708672323,
            "precision": 0.5185185185185185,
            "recall": 0.02959830866807611
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 1318,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.5924006234123068,
            "auditor_fn_violation": 0.0027606047342889502,
            "auditor_fp_violation": 0.0026152969430536903,
            "ave_precision_score": 0.592104256336243,
            "fpr": 0.010964912280701754,
            "logloss": 8.083385837248956,
            "mae": 0.5281061935515793,
            "precision": 0.47368421052631576,
            "recall": 0.018711018711018712
        },
        "train": {
            "accuracy": 0.47310647639956094,
            "auc_prc": 0.5904927395840833,
            "auditor_fn_violation": 0.001949394643342025,
            "auditor_fp_violation": 0.004190287154965441,
            "ave_precision_score": 0.5920293856263046,
            "fpr": 0.010976948408342482,
            "logloss": 7.7763296402332545,
            "mae": 0.5270165681381278,
            "precision": 0.23076923076923078,
            "recall": 0.006342494714587738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4791666666666667,
            "auc_prc": 0.614829646472046,
            "auditor_fn_violation": 0.003934602618813153,
            "auditor_fp_violation": 0.001607847926079701,
            "ave_precision_score": 0.6162124331644527,
            "fpr": 0.006578947368421052,
            "logloss": 6.45542886714885,
            "mae": 0.5146185567099036,
            "precision": 0.6666666666666666,
            "recall": 0.02494802494802495
        },
        "train": {
            "accuracy": 0.48518111964873767,
            "auc_prc": 0.6272430411467835,
            "auditor_fn_violation": 0.002634003476420462,
            "auditor_fp_violation": 0.00205504513580841,
            "ave_precision_score": 0.6288332154679619,
            "fpr": 0.0043907793633369925,
            "logloss": 6.397708497786016,
            "mae": 0.5052833090864042,
            "precision": 0.6666666666666666,
            "recall": 0.016913319238900635
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4956140350877193,
            "auc_prc": 0.6137185049153869,
            "auditor_fn_violation": 0.005288689499215823,
            "auditor_fp_violation": 0.004123926405340497,
            "ave_precision_score": 0.6133729416770812,
            "fpr": 0.01425438596491228,
            "logloss": 5.788778147226626,
            "mae": 0.4990076802562161,
            "precision": 0.723404255319149,
            "recall": 0.07068607068607069
        },
        "train": {
            "accuracy": 0.49396267837541163,
            "auc_prc": 0.6048857000969544,
            "auditor_fn_violation": 0.0015200636802250199,
            "auditor_fp_violation": 0.0034660090522232088,
            "ave_precision_score": 0.6063604855397802,
            "fpr": 0.01646542261251372,
            "logloss": 5.681888952561323,
            "mae": 0.4964329634363254,
            "precision": 0.6428571428571429,
            "recall": 0.05708245243128964
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 1318,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7766727755919551,
            "auditor_fn_violation": 0.02129609366451472,
            "auditor_fp_violation": 0.01795090975699109,
            "ave_precision_score": 0.77409250928883,
            "fpr": 0.125,
            "logloss": 1.4671779049924776,
            "mae": 0.2898691148430528,
            "precision": 0.7510917030567685,
            "recall": 0.7151767151767152
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.7743220039061124,
            "auditor_fn_violation": 0.015177429723162755,
            "auditor_fp_violation": 0.022886185585612683,
            "ave_precision_score": 0.7737833982874733,
            "fpr": 0.10867178924259056,
            "logloss": 1.2701310928094043,
            "mae": 0.27788369271701024,
            "precision": 0.7760180995475113,
            "recall": 0.7251585623678647
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 1318,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7047390478613235,
            "auditor_fn_violation": 0.015811357916621076,
            "auditor_fp_violation": 0.007901860218992963,
            "ave_precision_score": 0.7015722963175534,
            "fpr": 0.15460526315789475,
            "logloss": 1.760090748478381,
            "mae": 0.34030364480686226,
            "precision": 0.7157258064516129,
            "recall": 0.738045738045738
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7327106485897436,
            "auditor_fn_violation": 0.016801925259281095,
            "auditor_fp_violation": 0.012006977128851333,
            "ave_precision_score": 0.7314792626764469,
            "fpr": 0.1350164654226125,
            "logloss": 1.4634154370660204,
            "mae": 0.322131834420698,
            "precision": 0.7337662337662337,
            "recall": 0.7167019027484144
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 1318,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.566411920833725,
            "auditor_fn_violation": 0.001951344056607245,
            "auditor_fp_violation": 0.0020199861602963327,
            "ave_precision_score": 0.5661565753156951,
            "fpr": 0.009868421052631578,
            "logloss": 9.545269686745185,
            "mae": 0.5304620866179349,
            "precision": 0.4375,
            "recall": 0.014553014553014554
        },
        "train": {
            "accuracy": 0.47091108671789245,
            "auc_prc": 0.5617634148557371,
            "auditor_fn_violation": 0.000689250248895931,
            "auditor_fp_violation": 0.004190287154965441,
            "ave_precision_score": 0.5633465471965229,
            "fpr": 0.010976948408342482,
            "logloss": 9.144607077518488,
            "mae": 0.5281090147290933,
            "precision": 0.09090909090909091,
            "recall": 0.0021141649048625794
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.611869417675538,
            "auditor_fn_violation": 0.02427782033045191,
            "auditor_fp_violation": 0.01336141979077625,
            "ave_precision_score": 0.6096927476970695,
            "fpr": 0.07236842105263158,
            "logloss": 5.343098902888163,
            "mae": 0.438181626365389,
            "precision": 0.7053571428571429,
            "recall": 0.3284823284823285
        },
        "train": {
            "accuracy": 0.5883644346871569,
            "auc_prc": 0.6042755996769253,
            "auditor_fn_violation": 0.008939366864468369,
            "auditor_fp_violation": 0.009560972186718393,
            "ave_precision_score": 0.6046567555616595,
            "fpr": 0.05817782656421515,
            "logloss": 5.037274699017556,
            "mae": 0.4292959229795528,
            "precision": 0.7401960784313726,
            "recall": 0.3192389006342495
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 1318,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.752076051508631,
            "auditor_fn_violation": 0.011106247948353229,
            "auditor_fp_violation": 0.007861155208206131,
            "ave_precision_score": 0.7503377456100334,
            "fpr": 0.044956140350877194,
            "logloss": 2.433022362244321,
            "mae": 0.3804854426141343,
            "precision": 0.8101851851851852,
            "recall": 0.36382536382536385
        },
        "train": {
            "accuracy": 0.6114160263446762,
            "auc_prc": 0.7619511346425389,
            "auditor_fn_violation": 0.018366082389772196,
            "auditor_fp_violation": 0.0060448400824023,
            "ave_precision_score": 0.7624369288082264,
            "fpr": 0.03951701427003293,
            "logloss": 2.1937141207921926,
            "mae": 0.37995222868566625,
            "precision": 0.8115183246073299,
            "recall": 0.3276955602536998
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 1318,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7374194490634736,
            "auditor_fn_violation": 0.015936736331473175,
            "auditor_fp_violation": 0.02093255179712623,
            "ave_precision_score": 0.7357571350793084,
            "fpr": 0.12938596491228072,
            "logloss": 1.6619774254411313,
            "mae": 0.3109865440024041,
            "precision": 0.7377777777777778,
            "recall": 0.6902286902286903
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.740642757570315,
            "auditor_fn_violation": 0.011865779537390082,
            "auditor_fp_violation": 0.018284889403485563,
            "ave_precision_score": 0.7402101568720765,
            "fpr": 0.13391877058177826,
            "logloss": 1.4205568643770818,
            "mae": 0.30358078573022135,
            "precision": 0.7324561403508771,
            "recall": 0.7061310782241015
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 1318,
        "test": {
            "accuracy": 0.49451754385964913,
            "auc_prc": 0.6186903574055063,
            "auditor_fn_violation": 0.00889958784695627,
            "auditor_fp_violation": 0.004609842471608256,
            "ave_precision_score": 0.6183420393338204,
            "fpr": 0.015350877192982455,
            "logloss": 5.700509248015881,
            "mae": 0.5012129271277436,
            "precision": 0.7083333333333334,
            "recall": 0.07068607068607069
        },
        "train": {
            "accuracy": 0.48737650933040616,
            "auc_prc": 0.610699879325607,
            "auditor_fn_violation": 0.0023276700324667255,
            "auditor_fp_violation": 0.0037842904330130478,
            "ave_precision_score": 0.6121939681884232,
            "fpr": 0.015367727771679473,
            "logloss": 5.603595016673516,
            "mae": 0.5007266468810881,
            "precision": 0.5882352941176471,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 1318,
        "test": {
            "accuracy": 0.48903508771929827,
            "auc_prc": 0.6048408533285587,
            "auditor_fn_violation": 0.00810856402961669,
            "auditor_fp_violation": 0.002449932836732202,
            "ave_precision_score": 0.6045059284900458,
            "fpr": 0.013157894736842105,
            "logloss": 6.523666242989146,
            "mae": 0.5082811446447076,
            "precision": 0.6923076923076923,
            "recall": 0.056133056133056136
        },
        "train": {
            "accuracy": 0.4807903402854007,
            "auc_prc": 0.5992319659519683,
            "auditor_fn_violation": 0.0012462201469936472,
            "auditor_fp_violation": 0.004363211684685905,
            "ave_precision_score": 0.6006968065946467,
            "fpr": 0.014270032930845226,
            "logloss": 6.433152310621897,
            "mae": 0.5092269650929112,
            "precision": 0.5,
            "recall": 0.02748414376321353
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 1318,
        "test": {
            "accuracy": 0.4276315789473684,
            "auc_prc": 0.5744816444562975,
            "auditor_fn_violation": 0.024521738337527816,
            "auditor_fp_violation": 0.03203738755240771,
            "ave_precision_score": 0.5767662426819413,
            "fpr": 0.31469298245614036,
            "logloss": 9.68118739494127,
            "mae": 0.5712735881397232,
            "precision": 0.46153846153846156,
            "recall": 0.5114345114345115
        },
        "train": {
            "accuracy": 0.424807903402854,
            "auc_prc": 0.5857876877432218,
            "auditor_fn_violation": 0.025947835127627338,
            "auditor_fp_violation": 0.043907793633369926,
            "ave_precision_score": 0.5911347439108259,
            "fpr": 0.3227222832052689,
            "logloss": 9.57356059654086,
            "mae": 0.5738858543839948,
            "precision": 0.45251396648044695,
            "recall": 0.5137420718816068
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5614035087719298,
            "auc_prc": 0.5961687752348147,
            "auditor_fn_violation": 0.020810537257905676,
            "auditor_fp_violation": 0.018523323971180852,
            "ave_precision_score": 0.5952461249731412,
            "fpr": 0.09539473684210527,
            "logloss": 9.374488029915396,
            "mae": 0.44392253683579885,
            "precision": 0.6588235294117647,
            "recall": 0.3492723492723493
        },
        "train": {
            "accuracy": 0.5806805708013172,
            "auc_prc": 0.5907291818629994,
            "auditor_fn_violation": 0.0183614409739547,
            "auditor_fp_violation": 0.01653308873283912,
            "ave_precision_score": 0.5921729300671026,
            "fpr": 0.0889132821075741,
            "logloss": 9.243620220116753,
            "mae": 0.43277543941587193,
            "precision": 0.6798418972332015,
            "recall": 0.36363636363636365
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 1318,
        "test": {
            "accuracy": 0.5142543859649122,
            "auc_prc": 0.694085595922195,
            "auditor_fn_violation": 0.008933781960097762,
            "auditor_fp_violation": 0.004485183376073595,
            "ave_precision_score": 0.6936396813246415,
            "fpr": 0.017543859649122806,
            "logloss": 3.4951908997548533,
            "mae": 0.47578230863099713,
            "precision": 0.7714285714285715,
            "recall": 0.11226611226611227
        },
        "train": {
            "accuracy": 0.5093304061470911,
            "auc_prc": 0.6864993196644699,
            "auditor_fn_violation": 0.0032745188592328387,
            "auditor_fp_violation": 0.0033908244740838756,
            "ave_precision_score": 0.6869375897871076,
            "fpr": 0.01646542261251372,
            "logloss": 3.3122708752835734,
            "mae": 0.47703038589324254,
            "precision": 0.7321428571428571,
            "recall": 0.08668076109936575
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 1318,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.6197217984566211,
            "auditor_fn_violation": 0.00710553671079987,
            "auditor_fp_violation": 0.0036380103390727396,
            "ave_precision_score": 0.6193773991810079,
            "fpr": 0.013157894736842105,
            "logloss": 5.777772848060552,
            "mae": 0.5061004953602243,
            "precision": 0.7073170731707317,
            "recall": 0.060291060291060294
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.6118875581658795,
            "auditor_fn_violation": 0.0026734555108690506,
            "auditor_fp_violation": 0.004363211684685905,
            "ave_precision_score": 0.6133894071529213,
            "fpr": 0.014270032930845226,
            "logloss": 5.681470827900509,
            "mae": 0.5068622913977163,
            "precision": 0.5185185185185185,
            "recall": 0.02959830866807611
        }
    }
]
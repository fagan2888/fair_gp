[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7960286644042862,
            "auditor_fn_violation": 0.021573464912280713,
            "auditor_fp_violation": 0.015091983430799224,
            "ave_precision_score": 0.7964617968570479,
            "fpr": 0.1074561403508772,
            "logloss": 1.5340980437653606,
            "mae": 0.2940919594960581,
            "precision": 0.7644230769230769,
            "recall": 0.6625
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8207741518852042,
            "auditor_fn_violation": 0.021940001945281996,
            "auditor_fp_violation": 0.01451117413157769,
            "ave_precision_score": 0.8210691972309215,
            "fpr": 0.09549945115257959,
            "logloss": 1.4691598768417433,
            "mae": 0.27643020710202654,
            "precision": 0.7857142857142857,
            "recall": 0.6729957805907173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8126197722768452,
            "auditor_fn_violation": 0.021050347222222227,
            "auditor_fp_violation": 0.021381578947368425,
            "ave_precision_score": 0.8131170183314894,
            "fpr": 0.11842105263157894,
            "logloss": 1.2857444727702814,
            "mae": 0.27805391273411795,
            "precision": 0.7626373626373626,
            "recall": 0.7229166666666667
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8366508590592419,
            "auditor_fn_violation": 0.028459012445173154,
            "auditor_fp_violation": 0.028703338549686392,
            "ave_precision_score": 0.8368991687813692,
            "fpr": 0.11086717892425905,
            "logloss": 1.2242953563743022,
            "mae": 0.2646519098454883,
            "precision": 0.7725225225225225,
            "recall": 0.7236286919831224
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8024612835453095,
            "auditor_fn_violation": 0.02321134868421053,
            "auditor_fp_violation": 0.012835546621182586,
            "ave_precision_score": 0.802882633284998,
            "fpr": 0.10635964912280702,
            "logloss": 1.3958797248844639,
            "mae": 0.2930083906675491,
            "precision": 0.7679425837320574,
            "recall": 0.66875
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8263172989017578,
            "auditor_fn_violation": 0.02543224629122725,
            "auditor_fp_violation": 0.012883471026633547,
            "ave_precision_score": 0.8265894021248252,
            "fpr": 0.09440175631174534,
            "logloss": 1.34373898224631,
            "mae": 0.2751058539185778,
            "precision": 0.7912621359223301,
            "recall": 0.6877637130801688
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 10197,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8009381633518564,
            "auditor_fn_violation": 0.03488669590643275,
            "auditor_fp_violation": 0.02749604044834308,
            "ave_precision_score": 0.8009211561923956,
            "fpr": 0.14144736842105263,
            "logloss": 0.6757486020962421,
            "mae": 0.31757725086177624,
            "precision": 0.7383367139959433,
            "recall": 0.7583333333333333
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.825427483749141,
            "auditor_fn_violation": 0.044665064124831524,
            "auditor_fp_violation": 0.026658662118475687,
            "ave_precision_score": 0.8251226624559338,
            "fpr": 0.0867178924259056,
            "logloss": 0.707914408417927,
            "mae": 0.3214957425828158,
            "precision": 0.799492385786802,
            "recall": 0.6645569620253164
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 10197,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7888947931498872,
            "auditor_fn_violation": 0.02553910818713451,
            "auditor_fp_violation": 0.017949967511371023,
            "ave_precision_score": 0.7895065128180779,
            "fpr": 0.1074561403508772,
            "logloss": 1.3042651619133296,
            "mae": 0.30689899613238114,
            "precision": 0.7598039215686274,
            "recall": 0.6458333333333334
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8025134384023339,
            "auditor_fn_violation": 0.01823933452829228,
            "auditor_fp_violation": 0.02005993363593205,
            "ave_precision_score": 0.8028640615932674,
            "fpr": 0.09659714599341383,
            "logloss": 1.2890997044255479,
            "mae": 0.2896967217081246,
            "precision": 0.7843137254901961,
            "recall": 0.6751054852320675
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7830716968091038,
            "auditor_fn_violation": 0.014535361842105262,
            "auditor_fp_violation": 0.02210495857699805,
            "ave_precision_score": 0.783812016952009,
            "fpr": 0.13706140350877194,
            "logloss": 1.04377729859794,
            "mae": 0.30455414861010466,
            "precision": 0.7258771929824561,
            "recall": 0.6895833333333333
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8020626999927742,
            "auditor_fn_violation": 0.0048817314862417615,
            "auditor_fp_violation": 0.01394097566734572,
            "ave_precision_score": 0.8024201255794399,
            "fpr": 0.12733260153677278,
            "logloss": 0.9558012544032959,
            "mae": 0.2860355832491954,
            "precision": 0.7494600431965442,
            "recall": 0.7320675105485233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.7879045749897589,
            "auditor_fn_violation": 0.019152046783625733,
            "auditor_fp_violation": 0.0168458617608837,
            "ave_precision_score": 0.7459575453143372,
            "fpr": 0.12609649122807018,
            "logloss": 4.4598375858685735,
            "mae": 0.28184826176226574,
            "precision": 0.7537473233404711,
            "recall": 0.7333333333333333
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8057613219194472,
            "auditor_fn_violation": 0.025321087320003526,
            "auditor_fp_violation": 0.02398852569786515,
            "ave_precision_score": 0.7747221920165361,
            "fpr": 0.1207464324917673,
            "logloss": 3.7769585606687004,
            "mae": 0.26770989343423657,
            "precision": 0.7608695652173914,
            "recall": 0.7383966244725738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 10197,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7879991197506238,
            "auditor_fn_violation": 0.02155975877192983,
            "auditor_fp_violation": 0.017914433073424305,
            "ave_precision_score": 0.7886165109960455,
            "fpr": 0.1162280701754386,
            "logloss": 1.2273538977455705,
            "mae": 0.30636409342025456,
            "precision": 0.75,
            "recall": 0.6625
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8029554371923644,
            "auditor_fn_violation": 0.019149448605186494,
            "auditor_fp_violation": 0.018630669644090662,
            "ave_precision_score": 0.8033170609048518,
            "fpr": 0.10757409440175632,
            "logloss": 1.1771460491260406,
            "mae": 0.29044986365935344,
            "precision": 0.7672209026128266,
            "recall": 0.6814345991561181
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8132890325546207,
            "auditor_fn_violation": 0.014962536549707611,
            "auditor_fp_violation": 0.014949845679012351,
            "ave_precision_score": 0.8136719220580728,
            "fpr": 0.1206140350877193,
            "logloss": 1.0669170034658209,
            "mae": 0.2776574844580894,
            "precision": 0.7608695652173914,
            "recall": 0.7291666666666666
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.845188390853538,
            "auditor_fn_violation": 0.023417489937797294,
            "auditor_fp_violation": 0.023003865794874248,
            "ave_precision_score": 0.8453806991947507,
            "fpr": 0.10318331503841932,
            "logloss": 0.9343563111125444,
            "mae": 0.256286238905923,
            "precision": 0.7892376681614349,
            "recall": 0.7426160337552743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7863954373089889,
            "auditor_fn_violation": 0.020118329678362582,
            "auditor_fp_violation": 0.01814286874593892,
            "ave_precision_score": 0.7867532127355493,
            "fpr": 0.11732456140350878,
            "logloss": 1.274385649254244,
            "mae": 0.305385708089561,
            "precision": 0.7476415094339622,
            "recall": 0.6604166666666667
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.804279659900419,
            "auditor_fn_violation": 0.018179123418879426,
            "auditor_fp_violation": 0.02343339855867895,
            "ave_precision_score": 0.8046260423164446,
            "fpr": 0.10867178924259056,
            "logloss": 1.2271146514339004,
            "mae": 0.28838307746178127,
            "precision": 0.7686915887850467,
            "recall": 0.6940928270042194
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7730802565580739,
            "auditor_fn_violation": 0.009466374269005852,
            "auditor_fp_violation": 0.010980141325536066,
            "ave_precision_score": 0.7742551674190533,
            "fpr": 0.08223684210526316,
            "logloss": 1.075304723140394,
            "mae": 0.3290731534489666,
            "precision": 0.8021108179419525,
            "recall": 0.6333333333333333
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7934385321811799,
            "auditor_fn_violation": 0.007947866442496078,
            "auditor_fp_violation": 0.011647622372879656,
            "ave_precision_score": 0.7938909260762447,
            "fpr": 0.05598243688254665,
            "logloss": 1.1921282446013017,
            "mae": 0.34054571592158,
            "precision": 0.8365384615384616,
            "recall": 0.5506329113924051
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7863285749461859,
            "auditor_fn_violation": 0.021536915204678365,
            "auditor_fp_violation": 0.017645386614684865,
            "ave_precision_score": 0.7869496134046519,
            "fpr": 0.1162280701754386,
            "logloss": 1.2183107054652282,
            "mae": 0.30645627225217875,
            "precision": 0.7488151658767772,
            "recall": 0.6583333333333333
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8038176497784211,
            "auditor_fn_violation": 0.017850278129009252,
            "auditor_fp_violation": 0.02074065515049974,
            "ave_precision_score": 0.8041720386808999,
            "fpr": 0.10867178924259056,
            "logloss": 1.162015634875077,
            "mae": 0.2886339591095772,
            "precision": 0.7681498829039812,
            "recall": 0.6919831223628692
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7987776901700278,
            "auditor_fn_violation": 0.01872258771929825,
            "auditor_fp_violation": 0.015838206627680314,
            "ave_precision_score": 0.7992001243307065,
            "fpr": 0.1074561403508772,
            "logloss": 1.4662382403710281,
            "mae": 0.2922295681278854,
            "precision": 0.7677725118483413,
            "recall": 0.675
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8233963957690386,
            "auditor_fn_violation": 0.021222100256128806,
            "auditor_fp_violation": 0.012612187175809522,
            "ave_precision_score": 0.8236802808217665,
            "fpr": 0.09659714599341383,
            "logloss": 1.4120311051684544,
            "mae": 0.2750768906949342,
            "precision": 0.7853658536585366,
            "recall": 0.679324894514768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.81196843203809,
            "auditor_fn_violation": 0.022395833333333337,
            "auditor_fp_violation": 0.01803372725795972,
            "ave_precision_score": 0.8124348165763489,
            "fpr": 0.11293859649122807,
            "logloss": 1.3176739430001365,
            "mae": 0.27994424849372057,
            "precision": 0.7669683257918553,
            "recall": 0.70625
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.836710407147626,
            "auditor_fn_violation": 0.030318609401269993,
            "auditor_fp_violation": 0.023360553820957683,
            "ave_precision_score": 0.8369518667001622,
            "fpr": 0.10647639956092206,
            "logloss": 1.2689960308923347,
            "mae": 0.2645853687156453,
            "precision": 0.7790432801822323,
            "recall": 0.7215189873417721
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.7740478658205313,
            "auditor_fn_violation": 0.01960206505847954,
            "auditor_fp_violation": 0.016645346003898633,
            "ave_precision_score": 0.7746835196044428,
            "fpr": 0.11513157894736842,
            "logloss": 1.456991433037365,
            "mae": 0.3156374048448638,
            "precision": 0.7400990099009901,
            "recall": 0.6229166666666667
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7922336142091064,
            "auditor_fn_violation": 0.013987503878984944,
            "auditor_fp_violation": 0.02158967312807863,
            "ave_precision_score": 0.7926262743021768,
            "fpr": 0.10867178924259056,
            "logloss": 1.411747725612754,
            "mae": 0.2971728155181302,
            "precision": 0.7561576354679803,
            "recall": 0.6476793248945147
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7879546581878563,
            "auditor_fn_violation": 0.022242781432748538,
            "auditor_fp_violation": 0.01903122969460689,
            "ave_precision_score": 0.7885972440205854,
            "fpr": 0.1162280701754386,
            "logloss": 1.346849429632901,
            "mae": 0.3045034322135857,
            "precision": 0.747016706443914,
            "recall": 0.6520833333333333
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8033632149495297,
            "auditor_fn_violation": 0.020061878493981205,
            "auditor_fp_violation": 0.02281296234429437,
            "ave_precision_score": 0.8037157650654666,
            "fpr": 0.10428100987925357,
            "logloss": 1.334203307963314,
            "mae": 0.28702621013444785,
            "precision": 0.7738095238095238,
            "recall": 0.6856540084388185
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.781982824179871,
            "auditor_fn_violation": 0.01958150584795322,
            "auditor_fp_violation": 0.019554093567251466,
            "ave_precision_score": 0.782612262415421,
            "fpr": 0.10964912280701754,
            "logloss": 1.5403917680848773,
            "mae": 0.31210545727414374,
            "precision": 0.7487437185929648,
            "recall": 0.6208333333333333
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7961705717248502,
            "auditor_fn_violation": 0.022468007058594676,
            "auditor_fp_violation": 0.020087564398515976,
            "ave_precision_score": 0.7965577415354781,
            "fpr": 0.10537870472008781,
            "logloss": 1.5487952590884515,
            "mae": 0.29427752496554177,
            "precision": 0.7647058823529411,
            "recall": 0.6582278481012658
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7871142926121838,
            "auditor_fn_violation": 0.024067982456140356,
            "auditor_fp_violation": 0.017396645549057833,
            "ave_precision_score": 0.7877301051046214,
            "fpr": 0.1162280701754386,
            "logloss": 1.3115928134891783,
            "mae": 0.30892837615445945,
            "precision": 0.7464114832535885,
            "recall": 0.65
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8025555403770728,
            "auditor_fn_violation": 0.020198511396110366,
            "auditor_fp_violation": 0.022750165156603625,
            "ave_precision_score": 0.8029052495596715,
            "fpr": 0.10647639956092206,
            "logloss": 1.2895183424102061,
            "mae": 0.2894330824763443,
            "precision": 0.7690476190476191,
            "recall": 0.6814345991561181
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.795973655836219,
            "auditor_fn_violation": 0.014939692982456147,
            "auditor_fp_violation": 0.017795138888888888,
            "ave_precision_score": 0.7963857513240764,
            "fpr": 0.12828947368421054,
            "logloss": 1.4625433075479422,
            "mae": 0.2882787293313185,
            "precision": 0.7439824945295405,
            "recall": 0.7083333333333334
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.822228147002482,
            "auditor_fn_violation": 0.01675026747627451,
            "auditor_fp_violation": 0.018896929719899425,
            "ave_precision_score": 0.8225189997745889,
            "fpr": 0.11745334796926454,
            "logloss": 1.4023805402041767,
            "mae": 0.2685681842858983,
            "precision": 0.7663755458515283,
            "recall": 0.740506329113924
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.793477757405079,
            "auditor_fn_violation": 0.020355902777777775,
            "auditor_fp_violation": 0.01780782975958415,
            "ave_precision_score": 0.7939329513685268,
            "fpr": 0.11074561403508772,
            "logloss": 1.4700691956857417,
            "mae": 0.29500070898925757,
            "precision": 0.7617924528301887,
            "recall": 0.6729166666666667
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.820435586259233,
            "auditor_fn_violation": 0.016078682025131196,
            "auditor_fp_violation": 0.012416259950214394,
            "ave_precision_score": 0.8207395035936133,
            "fpr": 0.09879253567508232,
            "logloss": 1.394405561664834,
            "mae": 0.2759262576095803,
            "precision": 0.7831325301204819,
            "recall": 0.6856540084388185
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7860881262108339,
            "auditor_fn_violation": 0.021536915204678365,
            "auditor_fp_violation": 0.017645386614684865,
            "ave_precision_score": 0.7867137832187969,
            "fpr": 0.1162280701754386,
            "logloss": 1.2238154541624708,
            "mae": 0.3067114349817292,
            "precision": 0.7488151658767772,
            "recall": 0.6583333333333333
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8037232197521336,
            "auditor_fn_violation": 0.018781234513007925,
            "auditor_fp_violation": 0.01896977445762069,
            "ave_precision_score": 0.8040723612837607,
            "fpr": 0.10647639956092206,
            "logloss": 1.164344412333334,
            "mae": 0.28870646850238896,
            "precision": 0.7717647058823529,
            "recall": 0.6919831223628692
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7872324038327948,
            "auditor_fn_violation": 0.020764802631578948,
            "auditor_fp_violation": 0.019330734243014947,
            "ave_precision_score": 0.7878430050304488,
            "fpr": 0.11403508771929824,
            "logloss": 1.2762776397754099,
            "mae": 0.30511444647888025,
            "precision": 0.7517899761336515,
            "recall": 0.65625
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8035521577487292,
            "auditor_fn_violation": 0.020474093012269174,
            "auditor_fp_violation": 0.023131972057763364,
            "ave_precision_score": 0.8039012272720629,
            "fpr": 0.11086717892425905,
            "logloss": 1.2386118353856523,
            "mae": 0.28918004760426713,
            "precision": 0.764018691588785,
            "recall": 0.689873417721519
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7828870541515355,
            "auditor_fn_violation": 0.020997807017543866,
            "auditor_fp_violation": 0.015817901234567902,
            "ave_precision_score": 0.783505326846877,
            "fpr": 0.11403508771929824,
            "logloss": 1.3203769440583035,
            "mae": 0.30797071484610977,
            "precision": 0.75,
            "recall": 0.65
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.7992515168005478,
            "auditor_fn_violation": 0.01895260459364449,
            "auditor_fp_violation": 0.021858445091395028,
            "ave_precision_score": 0.7996271164377173,
            "fpr": 0.11086717892425905,
            "logloss": 1.2871168381318832,
            "mae": 0.29020439504816,
            "precision": 0.7623529411764706,
            "recall": 0.6835443037974683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8127821536002774,
            "auditor_fn_violation": 0.012015716374269007,
            "auditor_fp_violation": 0.019726689408706956,
            "ave_precision_score": 0.8131359561102887,
            "fpr": 0.1425438596491228,
            "logloss": 1.0455879142695015,
            "mae": 0.27536168016271023,
            "precision": 0.74,
            "recall": 0.7708333333333334
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8429676740735422,
            "auditor_fn_violation": 0.013711922262826129,
            "auditor_fp_violation": 0.02677923271884192,
            "ave_precision_score": 0.8432029426087863,
            "fpr": 0.13830954994511527,
            "logloss": 0.931391968188388,
            "mae": 0.2567723827364181,
            "precision": 0.7464788732394366,
            "recall": 0.7827004219409283
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8044059920730333,
            "auditor_fn_violation": 0.02098638523391813,
            "auditor_fp_violation": 0.013378715886939575,
            "ave_precision_score": 0.8047988972096742,
            "fpr": 0.11513157894736842,
            "logloss": 1.3537017425802624,
            "mae": 0.2887895267847693,
            "precision": 0.7580645161290323,
            "recall": 0.6854166666666667
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8302319553713319,
            "auditor_fn_violation": 0.024269708717179157,
            "auditor_fp_violation": 0.020826059325759153,
            "ave_precision_score": 0.830494062666979,
            "fpr": 0.10757409440175632,
            "logloss": 1.2684269725518917,
            "mae": 0.27003322002432695,
            "precision": 0.7715617715617715,
            "recall": 0.6983122362869199
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8106061184771374,
            "auditor_fn_violation": 0.023108552631578953,
            "auditor_fp_violation": 0.01699561403508772,
            "ave_precision_score": 0.8109328931813354,
            "fpr": 0.11293859649122807,
            "logloss": 1.3639053091522702,
            "mae": 0.2811441923394064,
            "precision": 0.7669683257918553,
            "recall": 0.70625
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8347968743164979,
            "auditor_fn_violation": 0.031080511516532586,
            "auditor_fp_violation": 0.020195575561344062,
            "ave_precision_score": 0.8350393980860416,
            "fpr": 0.10647639956092206,
            "logloss": 1.325411248152643,
            "mae": 0.26648754173045974,
            "precision": 0.7785388127853882,
            "recall": 0.7194092827004219
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7987506160341814,
            "auditor_fn_violation": 0.01872258771929825,
            "auditor_fp_violation": 0.015838206627680314,
            "ave_precision_score": 0.7991731401399034,
            "fpr": 0.1074561403508772,
            "logloss": 1.466519688583531,
            "mae": 0.29224499280169824,
            "precision": 0.7677725118483413,
            "recall": 0.675
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8233890248671043,
            "auditor_fn_violation": 0.021222100256128806,
            "auditor_fp_violation": 0.012612187175809522,
            "ave_precision_score": 0.8236729261289226,
            "fpr": 0.09659714599341383,
            "logloss": 1.4122391897917983,
            "mae": 0.2750888275905229,
            "precision": 0.7853658536585366,
            "recall": 0.679324894514768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7821575692588201,
            "auditor_fn_violation": 0.02354714912280703,
            "auditor_fp_violation": 0.022665895061728402,
            "ave_precision_score": 0.7828869043591667,
            "fpr": 0.1206140350877193,
            "logloss": 1.4987979155750955,
            "mae": 0.30125457025381497,
            "precision": 0.7477064220183486,
            "recall": 0.6791666666666667
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8002017852045036,
            "auditor_fn_violation": 0.029179229946226844,
            "auditor_fp_violation": 0.029602594277417876,
            "ave_precision_score": 0.8005578411827486,
            "fpr": 0.1163556531284303,
            "logloss": 1.5397625065296214,
            "mae": 0.28823937150614437,
            "precision": 0.759090909090909,
            "recall": 0.7046413502109705
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8243912239050022,
            "auditor_fn_violation": 0.0192297149122807,
            "auditor_fp_violation": 0.01557423651721897,
            "ave_precision_score": 0.8248507440737705,
            "fpr": 0.09758771929824561,
            "logloss": 0.942333692751094,
            "mae": 0.31588336946497153,
            "precision": 0.7935034802784223,
            "recall": 0.7125
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8361500934796311,
            "auditor_fn_violation": 0.02266253525823618,
            "auditor_fp_violation": 0.005531176291800951,
            "ave_precision_score": 0.8363869040139008,
            "fpr": 0.06037321624588365,
            "logloss": 1.0362630699042659,
            "mae": 0.32779012916845257,
            "precision": 0.8414985590778098,
            "recall": 0.6160337552742616
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7859802974793715,
            "auditor_fn_violation": 0.021536915204678365,
            "auditor_fp_violation": 0.017645386614684865,
            "ave_precision_score": 0.786602793530488,
            "fpr": 0.1162280701754386,
            "logloss": 1.224500331449237,
            "mae": 0.3067237423326385,
            "precision": 0.7488151658767772,
            "recall": 0.6583333333333333
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8037344254521258,
            "auditor_fn_violation": 0.018781234513007925,
            "auditor_fp_violation": 0.021072224301506882,
            "ave_precision_score": 0.8040896317959112,
            "fpr": 0.10757409440175632,
            "logloss": 1.1649185800278772,
            "mae": 0.2887138347984487,
            "precision": 0.7699530516431925,
            "recall": 0.6919831223628692
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7863824471101222,
            "auditor_fn_violation": 0.014551352339181298,
            "auditor_fp_violation": 0.013848278102664067,
            "ave_precision_score": 0.742789057779048,
            "fpr": 0.12280701754385964,
            "logloss": 4.487370282009463,
            "mae": 0.28572378820116295,
            "precision": 0.7575757575757576,
            "recall": 0.7291666666666666
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8017808719927675,
            "auditor_fn_violation": 0.01981177080872783,
            "auditor_fp_violation": 0.019082809395464036,
            "ave_precision_score": 0.7654534427011619,
            "fpr": 0.11964873765093303,
            "logloss": 3.743047006869855,
            "mae": 0.2674917484044814,
            "precision": 0.7650862068965517,
            "recall": 0.7489451476793249
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8059825209943183,
            "auditor_fn_violation": 0.025299250730994153,
            "auditor_fp_violation": 0.016264619883040933,
            "ave_precision_score": 0.8065120207254196,
            "fpr": 0.10855263157894737,
            "logloss": 1.218139145751929,
            "mae": 0.2875700504178759,
            "precision": 0.771889400921659,
            "recall": 0.6979166666666666
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8255386215555772,
            "auditor_fn_violation": 0.03146725210391512,
            "auditor_fp_violation": 0.027547870296176658,
            "ave_precision_score": 0.8258184213368415,
            "fpr": 0.10318331503841932,
            "logloss": 1.2081020920315046,
            "mae": 0.2749478510927159,
            "precision": 0.7808857808857809,
            "recall": 0.7067510548523207
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8095459898267445,
            "auditor_fn_violation": 0.01422697368421053,
            "auditor_fp_violation": 0.016873781676413263,
            "ave_precision_score": 0.8101035621346602,
            "fpr": 0.12171052631578948,
            "logloss": 1.0207980820189304,
            "mae": 0.2756834006793332,
            "precision": 0.7623126338329764,
            "recall": 0.7416666666666667
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8402517856076317,
            "auditor_fn_violation": 0.018924814850838564,
            "auditor_fp_violation": 0.019519877821791636,
            "ave_precision_score": 0.8404921107983605,
            "fpr": 0.1119648737650933,
            "logloss": 0.9187097727793838,
            "mae": 0.26199088398543974,
            "precision": 0.7763157894736842,
            "recall": 0.7468354430379747
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7855984101467326,
            "auditor_fn_violation": 0.013477704678362575,
            "auditor_fp_violation": 0.01421377517868746,
            "ave_precision_score": 0.7862971294646297,
            "fpr": 0.1206140350877193,
            "logloss": 1.2373888856225814,
            "mae": 0.3068106639803403,
            "precision": 0.7380952380952381,
            "recall": 0.6458333333333334
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.8036682352404688,
            "auditor_fn_violation": 0.018994289207853378,
            "auditor_fp_violation": 0.019818792435199586,
            "ave_precision_score": 0.8040087164360574,
            "fpr": 0.10976948408342481,
            "logloss": 1.1805561381331298,
            "mae": 0.29069144459963436,
            "precision": 0.7635933806146572,
            "recall": 0.6814345991561181
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7994812110672022,
            "auditor_fn_violation": 0.02284585160818714,
            "auditor_fp_violation": 0.012391366146848609,
            "ave_precision_score": 0.7999043158052328,
            "fpr": 0.1074561403508772,
            "logloss": 1.4493294322476507,
            "mae": 0.2924359269641264,
            "precision": 0.7649880095923262,
            "recall": 0.6645833333333333
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8250720467005557,
            "auditor_fn_violation": 0.021919159638177546,
            "auditor_fp_violation": 0.012715174563622348,
            "ave_precision_score": 0.8253556628596451,
            "fpr": 0.09769484083424808,
            "logloss": 1.3793437761942595,
            "mae": 0.273525879392056,
            "precision": 0.7860576923076923,
            "recall": 0.689873417721519
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7846415344611738,
            "auditor_fn_violation": 0.024198190789473693,
            "auditor_fp_violation": 0.017396645549057833,
            "ave_precision_score": 0.7852860007801634,
            "fpr": 0.1162280701754386,
            "logloss": 1.3163812392848027,
            "mae": 0.30966303252723276,
            "precision": 0.747016706443914,
            "recall": 0.6520833333333333
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8004270207063984,
            "auditor_fn_violation": 0.018839129810520272,
            "auditor_fp_violation": 0.02380013413479291,
            "ave_precision_score": 0.8007891399787359,
            "fpr": 0.10976948408342481,
            "logloss": 1.2866337058810833,
            "mae": 0.29071983179959066,
            "precision": 0.7647058823529411,
            "recall": 0.6856540084388185
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.7957931590579006,
            "auditor_fn_violation": 0.01896016081871345,
            "auditor_fp_violation": 0.016787483755685513,
            "ave_precision_score": 0.796227505373413,
            "fpr": 0.1074561403508772,
            "logloss": 1.5187218747984574,
            "mae": 0.29452331674785387,
            "precision": 0.7632850241545893,
            "recall": 0.6583333333333333
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.820347465876352,
            "auditor_fn_violation": 0.02195389681668497,
            "auditor_fp_violation": 0.013566704428708868,
            "ave_precision_score": 0.8206471246631111,
            "fpr": 0.09440175631174534,
            "logloss": 1.4563706273754737,
            "mae": 0.27720647272093835,
            "precision": 0.7860696517412935,
            "recall": 0.6666666666666666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8133222089449882,
            "auditor_fn_violation": 0.022656250000000006,
            "auditor_fp_violation": 0.017835749675113713,
            "ave_precision_score": 0.8137884965497563,
            "fpr": 0.11074561403508772,
            "logloss": 1.3372141412658396,
            "mae": 0.2799773778080461,
            "precision": 0.7714932126696833,
            "recall": 0.7104166666666667
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8367225353665255,
            "auditor_fn_violation": 0.030124081201628482,
            "auditor_fp_violation": 0.02352885028396889,
            "ave_precision_score": 0.8369599678797147,
            "fpr": 0.10428100987925357,
            "logloss": 1.3074585550138502,
            "mae": 0.26621525935724255,
            "precision": 0.7816091954022989,
            "recall": 0.7172995780590717
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7968649881995962,
            "auditor_fn_violation": 0.02109375000000001,
            "auditor_fp_violation": 0.01481786062378168,
            "ave_precision_score": 0.7972230966040948,
            "fpr": 0.11074561403508772,
            "logloss": 1.497145635234306,
            "mae": 0.2935364917034508,
            "precision": 0.7617924528301887,
            "recall": 0.6729166666666667
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8219953800289317,
            "auditor_fn_violation": 0.023746335227667472,
            "auditor_fp_violation": 0.012607163400794265,
            "ave_precision_score": 0.8222851086593174,
            "fpr": 0.09879253567508232,
            "logloss": 1.4344250831402698,
            "mae": 0.275230158729112,
            "precision": 0.782608695652174,
            "recall": 0.6835443037974683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7690172661793799,
            "auditor_fn_violation": 0.019490131578947373,
            "auditor_fp_violation": 0.016371223196881092,
            "ave_precision_score": 0.7695546168151225,
            "fpr": 0.1118421052631579,
            "logloss": 1.5184723131329723,
            "mae": 0.3135959830703342,
            "precision": 0.7475247524752475,
            "recall": 0.6291666666666667
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7905683024158887,
            "auditor_fn_violation": 0.01756311745334797,
            "auditor_fp_violation": 0.022687367968912884,
            "ave_precision_score": 0.7910120196513756,
            "fpr": 0.10867178924259056,
            "logloss": 1.4514155367956374,
            "mae": 0.2929757091385845,
            "precision": 0.7614457831325301,
            "recall": 0.6666666666666666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.785989352889916,
            "auditor_fn_violation": 0.02147980628654971,
            "auditor_fp_violation": 0.017645386614684865,
            "ave_precision_score": 0.7866077730640056,
            "fpr": 0.1162280701754386,
            "logloss": 1.2526730824468628,
            "mae": 0.30761806736156927,
            "precision": 0.747016706443914,
            "recall": 0.6520833333333333
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8030188205478774,
            "auditor_fn_violation": 0.01567109913064421,
            "auditor_fp_violation": 0.01896977445762069,
            "ave_precision_score": 0.8033743043559797,
            "fpr": 0.10647639956092206,
            "logloss": 1.1979575328452656,
            "mae": 0.2893420522645124,
            "precision": 0.7701421800947867,
            "recall": 0.6856540084388185
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.810788753407774,
            "auditor_fn_violation": 0.02310855263157895,
            "auditor_fp_violation": 0.018640350877192985,
            "ave_precision_score": 0.8112718753697755,
            "fpr": 0.1118421052631579,
            "logloss": 1.445477350611656,
            "mae": 0.2808355586447663,
            "precision": 0.7702702702702703,
            "recall": 0.7125
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8321297909142971,
            "auditor_fn_violation": 0.02830385304784004,
            "auditor_fp_violation": 0.02092653482606435,
            "ave_precision_score": 0.8323714278693796,
            "fpr": 0.10647639956092206,
            "logloss": 1.4184390714071065,
            "mae": 0.2684798366666258,
            "precision": 0.7759815242494227,
            "recall": 0.7088607594936709
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 10197,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.5803569875146739,
            "auditor_fn_violation": 0.014734100877193,
            "auditor_fp_violation": 0.011574074074074073,
            "ave_precision_score": 0.5821641218062555,
            "fpr": 0.02631578947368421,
            "logloss": 3.5883692147480124,
            "mae": 0.5284002040143678,
            "precision": 0.4782608695652174,
            "recall": 0.04583333333333333
        },
        "train": {
            "accuracy": 0.47530186608122943,
            "auc_prc": 0.5645655048854691,
            "auditor_fn_violation": 0.01496014487719251,
            "auditor_fp_violation": 0.017603307653470048,
            "ave_precision_score": 0.5663293241907447,
            "fpr": 0.026344676180021953,
            "logloss": 3.623406259992913,
            "mae": 0.5261573051856797,
            "precision": 0.45454545454545453,
            "recall": 0.04219409282700422
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 10197,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8218850108916861,
            "auditor_fn_violation": 0.023766447368421064,
            "auditor_fp_violation": 0.02150848765432099,
            "ave_precision_score": 0.8223082738748051,
            "fpr": 0.1074561403508772,
            "logloss": 0.5869168294400046,
            "mae": 0.3307185275154169,
            "precision": 0.7726218097447796,
            "recall": 0.69375
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.8063984256716888,
            "auditor_fn_violation": 0.030647454691140174,
            "auditor_fp_violation": 0.019961970023134486,
            "ave_precision_score": 0.8067605197934646,
            "fpr": 0.07903402854006586,
            "logloss": 0.634143768092019,
            "mae": 0.342761473688795,
            "precision": 0.7988826815642458,
            "recall": 0.6033755274261603
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7965445109004151,
            "auditor_fn_violation": 0.021431834795321642,
            "auditor_fp_violation": 0.01481786062378168,
            "ave_precision_score": 0.7969762043463906,
            "fpr": 0.11074561403508772,
            "logloss": 1.4990110247229516,
            "mae": 0.2935267491911846,
            "precision": 0.7612293144208038,
            "recall": 0.6708333333333333
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8219344346024228,
            "auditor_fn_violation": 0.02338506857119037,
            "auditor_fp_violation": 0.012607163400794265,
            "ave_precision_score": 0.8222243286822164,
            "fpr": 0.09879253567508232,
            "logloss": 1.4358151170898903,
            "mae": 0.2752316101899489,
            "precision": 0.7820823244552058,
            "recall": 0.6814345991561181
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7969716607500351,
            "auditor_fn_violation": 0.0206528691520468,
            "auditor_fp_violation": 0.016790021929824567,
            "ave_precision_score": 0.7974031718228923,
            "fpr": 0.10855263157894737,
            "logloss": 1.460696652522791,
            "mae": 0.29458235465938176,
            "precision": 0.7631578947368421,
            "recall": 0.6645833333333333
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8231185576223335,
            "auditor_fn_violation": 0.019531557568768038,
            "auditor_fp_violation": 0.013566704428708868,
            "ave_precision_score": 0.823410664983689,
            "fpr": 0.09440175631174534,
            "logloss": 1.3912157577169624,
            "mae": 0.27629175321319505,
            "precision": 0.7876543209876543,
            "recall": 0.6729957805907173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7860929017041379,
            "auditor_fn_violation": 0.021536915204678365,
            "auditor_fp_violation": 0.017645386614684865,
            "ave_precision_score": 0.7867289880123005,
            "fpr": 0.1162280701754386,
            "logloss": 1.2234997631997049,
            "mae": 0.30670583366188287,
            "precision": 0.7488151658767772,
            "recall": 0.6583333333333333
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.803735527988656,
            "auditor_fn_violation": 0.018781234513007925,
            "auditor_fp_violation": 0.01896977445762069,
            "ave_precision_score": 0.8040924447761464,
            "fpr": 0.10647639956092206,
            "logloss": 1.164079517495089,
            "mae": 0.2887031050407636,
            "precision": 0.7717647058823529,
            "recall": 0.6919831223628692
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7860529269970489,
            "auditor_fn_violation": 0.021536915204678365,
            "auditor_fp_violation": 0.017645386614684865,
            "ave_precision_score": 0.7866707359757547,
            "fpr": 0.1162280701754386,
            "logloss": 1.2246283423983546,
            "mae": 0.30684170918054576,
            "precision": 0.7488151658767772,
            "recall": 0.6583333333333333
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8033926015962064,
            "auditor_fn_violation": 0.017401010620313385,
            "auditor_fp_violation": 0.01896977445762069,
            "ave_precision_score": 0.8037441966030755,
            "fpr": 0.10647639956092206,
            "logloss": 1.1670195737813496,
            "mae": 0.2889646725071801,
            "precision": 0.7706855791962175,
            "recall": 0.6877637130801688
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.8040833164113608,
            "auditor_fn_violation": 0.021671692251461993,
            "auditor_fp_violation": 0.017482943469785583,
            "ave_precision_score": 0.8046493918577391,
            "fpr": 0.11403508771929824,
            "logloss": 1.0754023180735828,
            "mae": 0.3067640325397468,
            "precision": 0.750599520383693,
            "recall": 0.6520833333333333
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8125217014820294,
            "auditor_fn_violation": 0.02232905834456502,
            "auditor_fp_violation": 0.024006108910418558,
            "ave_precision_score": 0.8127986529730227,
            "fpr": 0.10757409440175632,
            "logloss": 1.0656547063297506,
            "mae": 0.29137220701971517,
            "precision": 0.7710280373831776,
            "recall": 0.6962025316455697
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7988579278503745,
            "auditor_fn_violation": 0.01872258771929825,
            "auditor_fp_violation": 0.015838206627680314,
            "ave_precision_score": 0.7992786683408435,
            "fpr": 0.1074561403508772,
            "logloss": 1.467422792770144,
            "mae": 0.2922531481288368,
            "precision": 0.7677725118483413,
            "recall": 0.675
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8234375437487329,
            "auditor_fn_violation": 0.023676860870652648,
            "auditor_fp_violation": 0.012612187175809522,
            "ave_precision_score": 0.8237218880321417,
            "fpr": 0.09659714599341383,
            "logloss": 1.411570762784071,
            "mae": 0.2748852892167701,
            "precision": 0.7864077669902912,
            "recall": 0.6835443037974683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 10197,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7850778429242087,
            "auditor_fn_violation": 0.021648848684210534,
            "auditor_fp_violation": 0.017147904483430804,
            "ave_precision_score": 0.7857134864316424,
            "fpr": 0.11513157894736842,
            "logloss": 1.2527770262488311,
            "mae": 0.30726065267632907,
            "precision": 0.7511848341232228,
            "recall": 0.6604166666666667
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8030436434421194,
            "auditor_fn_violation": 0.016581213207538432,
            "auditor_fp_violation": 0.021072224301506882,
            "ave_precision_score": 0.8033993912089242,
            "fpr": 0.10757409440175632,
            "logloss": 1.187920551771378,
            "mae": 0.28900086477412074,
            "precision": 0.7699530516431925,
            "recall": 0.6919831223628692
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7851767970509497,
            "auditor_fn_violation": 0.021207967836257308,
            "auditor_fp_violation": 0.015135132391163101,
            "ave_precision_score": 0.7857863874922115,
            "fpr": 0.11732456140350878,
            "logloss": 1.2401988573537308,
            "mae": 0.3077829602974555,
            "precision": 0.7470449172576832,
            "recall": 0.6583333333333333
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.8034406297997492,
            "auditor_fn_violation": 0.015228779057649822,
            "auditor_fp_violation": 0.02042918109955364,
            "ave_precision_score": 0.803786140519347,
            "fpr": 0.11086717892425905,
            "logloss": 1.1732463178814851,
            "mae": 0.29003813304308773,
            "precision": 0.7606635071090048,
            "recall": 0.6772151898734177
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7949206082317876,
            "auditor_fn_violation": 0.021340460526315802,
            "auditor_fp_violation": 0.011076591942820013,
            "ave_precision_score": 0.795331545526915,
            "fpr": 0.10964912280701754,
            "logloss": 1.53878929470855,
            "mae": 0.29604594032574383,
            "precision": 0.7601918465227818,
            "recall": 0.6604166666666667
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8228633746741361,
            "auditor_fn_violation": 0.020925676332865545,
            "auditor_fp_violation": 0.014385579756196207,
            "ave_precision_score": 0.8231388792023491,
            "fpr": 0.09989023051591657,
            "logloss": 1.4395811413604367,
            "mae": 0.2757664693450671,
            "precision": 0.7807228915662651,
            "recall": 0.6835443037974683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7859065810539321,
            "auditor_fn_violation": 0.021536915204678365,
            "auditor_fp_violation": 0.017645386614684865,
            "ave_precision_score": 0.7865247029399908,
            "fpr": 0.1162280701754386,
            "logloss": 1.2254265005821032,
            "mae": 0.30692977783591574,
            "precision": 0.7488151658767772,
            "recall": 0.6583333333333333
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.803434183195909,
            "auditor_fn_violation": 0.01733385207519905,
            "auditor_fp_violation": 0.01896977445762069,
            "ave_precision_score": 0.8037894742828419,
            "fpr": 0.10647639956092206,
            "logloss": 1.1669146752590065,
            "mae": 0.2890193541474005,
            "precision": 0.7712264150943396,
            "recall": 0.689873417721519
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8318337494081032,
            "auditor_fn_violation": 0.018174342105263155,
            "auditor_fp_violation": 0.025716780376868108,
            "ave_precision_score": 0.832254459558851,
            "fpr": 0.14692982456140352,
            "logloss": 0.5374403563254629,
            "mae": 0.3385330383045999,
            "precision": 0.7346534653465346,
            "recall": 0.7729166666666667
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8236143759157508,
            "auditor_fn_violation": 0.026548467627265454,
            "auditor_fp_violation": 0.01999713644824131,
            "ave_precision_score": 0.823917370214785,
            "fpr": 0.10428100987925357,
            "logloss": 0.5678912702088446,
            "mae": 0.34727411002378694,
            "precision": 0.7790697674418605,
            "recall": 0.7067510548523207
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7867358516691743,
            "auditor_fn_violation": 0.016890533625730998,
            "auditor_fp_violation": 0.010152696556205326,
            "ave_precision_score": 0.7871938622895945,
            "fpr": 0.11403508771929824,
            "logloss": 1.1500357962829704,
            "mae": 0.31393187676959416,
            "precision": 0.7438423645320197,
            "recall": 0.6291666666666667
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8010080441947205,
            "auditor_fn_violation": 0.012806439809732898,
            "auditor_fp_violation": 0.013142195439919423,
            "ave_precision_score": 0.8013725113823963,
            "fpr": 0.09769484083424808,
            "logloss": 1.094526186150504,
            "mae": 0.29379633581207676,
            "precision": 0.7802469135802469,
            "recall": 0.6666666666666666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7852867782501572,
            "auditor_fn_violation": 0.01976882309941521,
            "auditor_fp_violation": 0.017127599090318392,
            "ave_precision_score": 0.7859134338036138,
            "fpr": 0.1162280701754386,
            "logloss": 1.243330251376619,
            "mae": 0.30797110081909396,
            "precision": 0.7476190476190476,
            "recall": 0.6541666666666667
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8033890919435714,
            "auditor_fn_violation": 0.015312148286067627,
            "auditor_fp_violation": 0.019891637172920848,
            "ave_precision_score": 0.8037433517775902,
            "fpr": 0.10867178924259056,
            "logloss": 1.1904076054151431,
            "mae": 0.2895416281719462,
            "precision": 0.7659574468085106,
            "recall": 0.6835443037974683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8136624142595472,
            "auditor_fn_violation": 0.018704312865497084,
            "auditor_fp_violation": 0.017620004873294345,
            "ave_precision_score": 0.8140890839059742,
            "fpr": 0.11513157894736842,
            "logloss": 1.2253343730983413,
            "mae": 0.27904706057809153,
            "precision": 0.765625,
            "recall": 0.7145833333333333
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8380509504330176,
            "auditor_fn_violation": 0.022509691672803575,
            "auditor_fp_violation": 0.021559530477987077,
            "ave_precision_score": 0.8382938841523979,
            "fpr": 0.10537870472008781,
            "logloss": 1.155898578242656,
            "mae": 0.26288091174501643,
            "precision": 0.7808219178082192,
            "recall": 0.7215189873417721
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.809365718725138,
            "auditor_fn_violation": 0.023291301169590647,
            "auditor_fp_violation": 0.020564286874593895,
            "ave_precision_score": 0.8098574188621421,
            "fpr": 0.1162280701754386,
            "logloss": 1.3643590809134534,
            "mae": 0.2820466150524389,
            "precision": 0.7612612612612613,
            "recall": 0.7041666666666667
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.833746602043151,
            "auditor_fn_violation": 0.028060692798288156,
            "auditor_fp_violation": 0.02519925547654274,
            "ave_precision_score": 0.8339919433926227,
            "fpr": 0.10318331503841932,
            "logloss": 1.342776975157928,
            "mae": 0.2683704710279687,
            "precision": 0.7829099307159353,
            "recall": 0.7151898734177216
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7885391198718195,
            "auditor_fn_violation": 0.011533717105263158,
            "auditor_fp_violation": 0.011218729694606888,
            "ave_precision_score": 0.7895781488413007,
            "fpr": 0.0800438596491228,
            "logloss": 1.056246153004356,
            "mae": 0.3275244589247096,
            "precision": 0.8089005235602095,
            "recall": 0.64375
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.8199974314330386,
            "auditor_fn_violation": 0.014015293621790874,
            "auditor_fp_violation": 0.008608238488647526,
            "ave_precision_score": 0.8202646009426726,
            "fpr": 0.050493962678375415,
            "logloss": 1.1755540545597567,
            "mae": 0.3360798429234712,
            "precision": 0.8535031847133758,
            "recall": 0.5654008438818565
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.7927607297633006,
            "auditor_fn_violation": 0.012870065789473695,
            "auditor_fp_violation": 0.018168250487329438,
            "ave_precision_score": 0.7935388397366356,
            "fpr": 0.08114035087719298,
            "logloss": 1.0034867862117485,
            "mae": 0.3322419001263121,
            "precision": 0.8077922077922078,
            "recall": 0.6479166666666667
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8091278595068696,
            "auditor_fn_violation": 0.011845377871027806,
            "auditor_fp_violation": 0.009185972615402395,
            "ave_precision_score": 0.8094563595947639,
            "fpr": 0.0570801317233809,
            "logloss": 1.1163601980625957,
            "mae": 0.341645812613953,
            "precision": 0.84,
            "recall": 0.5759493670886076
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7839039816859915,
            "auditor_fn_violation": 0.020118329678362582,
            "auditor_fp_violation": 0.01836622807017544,
            "ave_precision_score": 0.7845350543330706,
            "fpr": 0.11842105263157894,
            "logloss": 1.2758063689827694,
            "mae": 0.3054205211374186,
            "precision": 0.7458823529411764,
            "recall": 0.6604166666666667
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8001501731113033,
            "auditor_fn_violation": 0.01949450457836013,
            "auditor_fp_violation": 0.025485610652412546,
            "ave_precision_score": 0.8005064678025458,
            "fpr": 0.11525795828759605,
            "logloss": 1.2292405153269828,
            "mae": 0.28798931133276123,
            "precision": 0.7597254004576659,
            "recall": 0.70042194092827
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.8068720664383253,
            "auditor_fn_violation": 0.02389665570175438,
            "auditor_fp_violation": 0.010279605263157895,
            "ave_precision_score": 0.8071974295759645,
            "fpr": 0.09868421052631579,
            "logloss": 0.9997489771415791,
            "mae": 0.3566262181317132,
            "precision": 0.7744360902255639,
            "recall": 0.64375
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.793093110811684,
            "auditor_fn_violation": 0.019925245591852044,
            "auditor_fp_violation": 0.01225549914972608,
            "ave_precision_score": 0.7934157720813781,
            "fpr": 0.07464324917672886,
            "logloss": 1.0905503731098887,
            "mae": 0.3694562786079882,
            "precision": 0.7875,
            "recall": 0.5316455696202531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7633077419834986,
            "auditor_fn_violation": 0.021712810672514626,
            "auditor_fp_violation": 0.016188474658869405,
            "ave_precision_score": 0.7636423610549156,
            "fpr": 0.12171052631578948,
            "logloss": 1.330782430936951,
            "mae": 0.30627675981115726,
            "precision": 0.7454128440366973,
            "recall": 0.6770833333333334
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7659752792011237,
            "auditor_fn_violation": 0.02104609855169124,
            "auditor_fp_violation": 0.0212756871896249,
            "ave_precision_score": 0.7662510114090055,
            "fpr": 0.11964873765093303,
            "logloss": 1.346114048458038,
            "mae": 0.29111938363469536,
            "precision": 0.7539503386004515,
            "recall": 0.7046413502109705
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8148230000680519,
            "auditor_fn_violation": 0.015254934210526325,
            "auditor_fp_violation": 0.016472750162443144,
            "ave_precision_score": 0.8152536206828528,
            "fpr": 0.12390350877192982,
            "logloss": 1.01770415170645,
            "mae": 0.27868460196241074,
            "precision": 0.7600849256900213,
            "recall": 0.7458333333333333
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8451093645537122,
            "auditor_fn_violation": 0.022942748498195987,
            "auditor_fp_violation": 0.02290841406958431,
            "ave_precision_score": 0.8453411727093146,
            "fpr": 0.10757409440175632,
            "logloss": 0.9152615519650142,
            "mae": 0.25999040152946673,
            "precision": 0.7827050997782705,
            "recall": 0.7447257383966245
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8070377146118114,
            "auditor_fn_violation": 0.013678728070175446,
            "auditor_fp_violation": 0.01737126380766732,
            "ave_precision_score": 0.8075481320558988,
            "fpr": 0.12719298245614036,
            "logloss": 1.0223169496448106,
            "mae": 0.2796606220541741,
            "precision": 0.7547568710359408,
            "recall": 0.74375
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8387491789014373,
            "auditor_fn_violation": 0.016750267476274516,
            "auditor_fp_violation": 0.017640985966084502,
            "ave_precision_score": 0.8389917583009142,
            "fpr": 0.1207464324917673,
            "logloss": 0.8987795679245806,
            "mae": 0.2622336392251336,
            "precision": 0.7644539614561028,
            "recall": 0.7531645569620253
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7857472292225646,
            "auditor_fn_violation": 0.021648848684210534,
            "auditor_fp_violation": 0.017645386614684865,
            "ave_precision_score": 0.7863704849859551,
            "fpr": 0.1162280701754386,
            "logloss": 1.2260806822733492,
            "mae": 0.30703917972245615,
            "precision": 0.7494089834515366,
            "recall": 0.6604166666666667
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8032361110541328,
            "auditor_fn_violation": 0.016581213207538432,
            "auditor_fp_violation": 0.021072224301506882,
            "ave_precision_score": 0.8035908264207068,
            "fpr": 0.10757409440175632,
            "logloss": 1.1661854236458087,
            "mae": 0.2890954344131741,
            "precision": 0.7699530516431925,
            "recall": 0.6919831223628692
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8136259754797486,
            "auditor_fn_violation": 0.02310855263157895,
            "auditor_fp_violation": 0.017985501949317743,
            "ave_precision_score": 0.8141002143410129,
            "fpr": 0.1118421052631579,
            "logloss": 1.2561038565263267,
            "mae": 0.2800429599594177,
            "precision": 0.7702702702702703,
            "recall": 0.7125
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8367018759573486,
            "auditor_fn_violation": 0.028269115869332628,
            "auditor_fp_violation": 0.02429246408628836,
            "ave_precision_score": 0.8369430774830857,
            "fpr": 0.10428100987925357,
            "logloss": 1.2169642837631096,
            "mae": 0.26529246539785806,
            "precision": 0.7821100917431193,
            "recall": 0.7194092827004219
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8054515252980642,
            "auditor_fn_violation": 0.02890625000000001,
            "auditor_fp_violation": 0.01794742933723197,
            "ave_precision_score": 0.8059059370991655,
            "fpr": 0.11513157894736842,
            "logloss": 1.3603861244549618,
            "mae": 0.2852152742039833,
            "precision": 0.7602739726027398,
            "recall": 0.69375
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8252964967388323,
            "auditor_fn_violation": 0.033257374702997125,
            "auditor_fp_violation": 0.02620149859208706,
            "ave_precision_score": 0.8255705174562591,
            "fpr": 0.10757409440175632,
            "logloss": 1.3406243960307884,
            "mae": 0.2754005847880495,
            "precision": 0.7726218097447796,
            "recall": 0.7025316455696202
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.785624526974431,
            "auditor_fn_violation": 0.02066200657894737,
            "auditor_fp_violation": 0.016173245614035086,
            "ave_precision_score": 0.7859761634490067,
            "fpr": 0.11842105263157894,
            "logloss": 1.243959596306004,
            "mae": 0.30770150635130483,
            "precision": 0.7446808510638298,
            "recall": 0.65625
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.803689086255008,
            "auditor_fn_violation": 0.015228779057649822,
            "auditor_fp_violation": 0.02042918109955364,
            "ave_precision_score": 0.804036771978253,
            "fpr": 0.11086717892425905,
            "logloss": 1.1766043885136819,
            "mae": 0.2898248737592886,
            "precision": 0.7606635071090048,
            "recall": 0.6772151898734177
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7924259456584922,
            "auditor_fn_violation": 0.02169225146198831,
            "auditor_fp_violation": 0.014262000487329436,
            "ave_precision_score": 0.7929903313227784,
            "fpr": 0.09100877192982457,
            "logloss": 1.196088536313323,
            "mae": 0.3687674582249367,
            "precision": 0.7768817204301075,
            "recall": 0.6020833333333333
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.7737940078196419,
            "auditor_fn_violation": 0.019457451587952243,
            "auditor_fp_violation": 0.011725490885616181,
            "ave_precision_score": 0.7741492004949331,
            "fpr": 0.06147091108671789,
            "logloss": 1.3603369310278512,
            "mae": 0.3855889175215504,
            "precision": 0.8041958041958042,
            "recall": 0.48523206751054854
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.801153645469093,
            "auditor_fn_violation": 0.018969298245614045,
            "auditor_fp_violation": 0.016079333170890192,
            "ave_precision_score": 0.8015594691211397,
            "fpr": 0.10635964912280702,
            "logloss": 1.4057192269336185,
            "mae": 0.29122611494813394,
            "precision": 0.7695961995249406,
            "recall": 0.675
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8259822820266327,
            "auditor_fn_violation": 0.023676860870652648,
            "auditor_fp_violation": 0.012391141075138095,
            "ave_precision_score": 0.8262595216368724,
            "fpr": 0.09659714599341383,
            "logloss": 1.3602983652460305,
            "mae": 0.27452645126346403,
            "precision": 0.7858880778588808,
            "recall": 0.6814345991561181
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7896938018306998,
            "auditor_fn_violation": 0.02066200657894737,
            "auditor_fp_violation": 0.015685916179337234,
            "ave_precision_score": 0.7900555061077428,
            "fpr": 0.11074561403508772,
            "logloss": 1.5679598449195615,
            "mae": 0.2960473039023258,
            "precision": 0.7572115384615384,
            "recall": 0.65625
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8164901590555702,
            "auditor_fn_violation": 0.02187979083586915,
            "auditor_fp_violation": 0.012157535536928514,
            "ave_precision_score": 0.8168050204050374,
            "fpr": 0.09879253567508232,
            "logloss": 1.492340708602088,
            "mae": 0.27780799948769486,
            "precision": 0.7820823244552058,
            "recall": 0.6814345991561181
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7847899692727904,
            "auditor_fn_violation": 0.01976882309941521,
            "auditor_fp_violation": 0.017127599090318392,
            "ave_precision_score": 0.7854603334377848,
            "fpr": 0.1162280701754386,
            "logloss": 1.2522060966833692,
            "mae": 0.30805136240677977,
            "precision": 0.7476190476190476,
            "recall": 0.6541666666666667
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8030647260839923,
            "auditor_fn_violation": 0.015312148286067627,
            "auditor_fp_violation": 0.023287709083236414,
            "ave_precision_score": 0.8034230999591151,
            "fpr": 0.10867178924259056,
            "logloss": 1.1975238325011441,
            "mae": 0.28961583460775575,
            "precision": 0.7659574468085106,
            "recall": 0.6835443037974683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8107456165777935,
            "auditor_fn_violation": 0.013977978801169593,
            "auditor_fp_violation": 0.018614969135802472,
            "ave_precision_score": 0.811276926793483,
            "fpr": 0.12938596491228072,
            "logloss": 0.9854383675292144,
            "mae": 0.2756592730630988,
            "precision": 0.7536534446764092,
            "recall": 0.7520833333333333
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8422661265224275,
            "auditor_fn_violation": 0.01677110978337896,
            "auditor_fp_violation": 0.019376700233856733,
            "ave_precision_score": 0.8425018893601198,
            "fpr": 0.11855104281009879,
            "logloss": 0.8710200840395604,
            "mae": 0.25836316956044,
            "precision": 0.7692307692307693,
            "recall": 0.759493670886076
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7738864741288584,
            "auditor_fn_violation": 0.023944627192982453,
            "auditor_fp_violation": 0.02267097141000651,
            "ave_precision_score": 0.7744199761731578,
            "fpr": 0.13925438596491227,
            "logloss": 1.0426941084284123,
            "mae": 0.31208494765954364,
            "precision": 0.7214912280701754,
            "recall": 0.6854166666666667
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.78731036818306,
            "auditor_fn_violation": 0.0009031666411927434,
            "auditor_fp_violation": 0.017259179064924756,
            "ave_precision_score": 0.7877195306074565,
            "fpr": 0.13391877058177826,
            "logloss": 0.9554546915582703,
            "mae": 0.2950260201464501,
            "precision": 0.7370689655172413,
            "recall": 0.7215189873417721
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.785011173723283,
            "auditor_fn_violation": 0.019021838450292403,
            "auditor_fp_violation": 0.017850978719948025,
            "ave_precision_score": 0.7857959437566957,
            "fpr": 0.11732456140350878,
            "logloss": 1.235423037654922,
            "mae": 0.3059329815491533,
            "precision": 0.7476415094339622,
            "recall": 0.6604166666666667
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8033573198333213,
            "auditor_fn_violation": 0.016727109357269572,
            "auditor_fp_violation": 0.023385672696033984,
            "ave_precision_score": 0.80370849992237,
            "fpr": 0.11086717892425905,
            "logloss": 1.1828612071431428,
            "mae": 0.2888311632813944,
            "precision": 0.7662037037037037,
            "recall": 0.6983122362869199
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8011140153311959,
            "auditor_fn_violation": 0.021984649122807027,
            "auditor_fp_violation": 0.01442190545808967,
            "ave_precision_score": 0.8016935645953962,
            "fpr": 0.11513157894736842,
            "logloss": 1.078950608283221,
            "mae": 0.29240349926605685,
            "precision": 0.7575057736720554,
            "recall": 0.6833333333333333
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8241720618657791,
            "auditor_fn_violation": 0.019276818259713672,
            "auditor_fp_violation": 0.02255674981851613,
            "ave_precision_score": 0.8244636773747732,
            "fpr": 0.1119648737650933,
            "logloss": 1.0100282661451343,
            "mae": 0.27608882032660853,
            "precision": 0.7649769585253456,
            "recall": 0.70042194092827
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7896465675372376,
            "auditor_fn_violation": 0.024515716374269015,
            "auditor_fp_violation": 0.019330734243014947,
            "ave_precision_score": 0.790260257248853,
            "fpr": 0.11403508771929824,
            "logloss": 1.3025777898713635,
            "mae": 0.30427655531397624,
            "precision": 0.7493975903614458,
            "recall": 0.6479166666666667
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8053047162152096,
            "auditor_fn_violation": 0.020420829338557815,
            "auditor_fp_violation": 0.023393208358556867,
            "ave_precision_score": 0.8056504755125353,
            "fpr": 0.10318331503841932,
            "logloss": 1.3013432364128392,
            "mae": 0.28722344305609254,
            "precision": 0.7761904761904762,
            "recall": 0.6877637130801688
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7730263157894737,
            "auc_prc": 0.8484633056774806,
            "auditor_fn_violation": 0.009375000000000003,
            "auditor_fp_violation": 0.01474932992202729,
            "ave_precision_score": 0.8488123491530932,
            "fpr": 0.08223684210526316,
            "logloss": 0.4994370452736936,
            "mae": 0.2970621932533839,
            "precision": 0.8226950354609929,
            "recall": 0.725
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8771668526449287,
            "auditor_fn_violation": 0.011810640692520394,
            "auditor_fp_violation": 0.01078855684527024,
            "ave_precision_score": 0.8773611134123493,
            "fpr": 0.054884742041712405,
            "logloss": 0.4976202027204147,
            "mae": 0.3002473397804557,
            "precision": 0.8595505617977528,
            "recall": 0.6455696202531646
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8114657814891197,
            "auditor_fn_violation": 0.02310855263157895,
            "auditor_fp_violation": 0.019399264944769335,
            "ave_precision_score": 0.8119322600637423,
            "fpr": 0.11293859649122807,
            "logloss": 1.3442415175945361,
            "mae": 0.28145987408388395,
            "precision": 0.7685393258426966,
            "recall": 0.7125
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8356030855773429,
            "auditor_fn_violation": 0.028213536383720775,
            "auditor_fp_violation": 0.025799596590866277,
            "ave_precision_score": 0.835844412663205,
            "fpr": 0.10647639956092206,
            "logloss": 1.3095505085859565,
            "mae": 0.26713764963385417,
            "precision": 0.7775229357798165,
            "recall": 0.7151898734177216
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 10197,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7857442815229838,
            "auditor_fn_violation": 0.013041392543859656,
            "auditor_fp_violation": 0.019051535087719302,
            "ave_precision_score": 0.7863028351249304,
            "fpr": 0.13486842105263158,
            "logloss": 1.0665858268089843,
            "mae": 0.305060891265844,
            "precision": 0.7266666666666667,
            "recall": 0.68125
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8021364481587494,
            "auditor_fn_violation": 0.008422607882097389,
            "auditor_fp_violation": 0.014059034380204323,
            "ave_precision_score": 0.8024738639736625,
            "fpr": 0.12843029637760703,
            "logloss": 0.9817449331802973,
            "mae": 0.2886143892229049,
            "precision": 0.7445414847161572,
            "recall": 0.7194092827004219
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7872116282613169,
            "auditor_fn_violation": 0.017288011695906432,
            "auditor_fp_violation": 0.02108461257309941,
            "ave_precision_score": 0.7875311127747777,
            "fpr": 0.13486842105263158,
            "logloss": 1.2042631927166958,
            "mae": 0.29778287690075295,
            "precision": 0.7366167023554604,
            "recall": 0.7166666666666667
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8091497901378201,
            "auditor_fn_violation": 0.020168405841403936,
            "auditor_fp_violation": 0.030677682130683462,
            "ave_precision_score": 0.809488015677998,
            "fpr": 0.12733260153677278,
            "logloss": 1.1434909766077972,
            "mae": 0.27878743249774024,
            "precision": 0.7516059957173448,
            "recall": 0.740506329113924
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.6969617930053522,
            "auditor_fn_violation": 0.024616228070175435,
            "auditor_fp_violation": 0.023643092105263157,
            "ave_precision_score": 0.6843320816281266,
            "fpr": 0.14802631578947367,
            "logloss": 2.2598935413097134,
            "mae": 0.30952573962713414,
            "precision": 0.7181628392484343,
            "recall": 0.7166666666666667
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.6760533988790207,
            "auditor_fn_violation": 0.012741597076519055,
            "auditor_fp_violation": 0.039383884232128546,
            "ave_precision_score": 0.6604651660590392,
            "fpr": 0.15806805708013172,
            "logloss": 2.6120047643709654,
            "mae": 0.3083225076845471,
            "precision": 0.7037037037037037,
            "recall": 0.7215189873417721
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8114773974111553,
            "auditor_fn_violation": 0.011476608187134503,
            "auditor_fp_violation": 0.016812865497076026,
            "ave_precision_score": 0.8117213889915359,
            "fpr": 0.13157894736842105,
            "logloss": 0.9968837926534488,
            "mae": 0.2740875441955227,
            "precision": 0.7540983606557377,
            "recall": 0.7666666666666667
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8429591355280951,
            "auditor_fn_violation": 0.020453250705164728,
            "auditor_fp_violation": 0.02462654512480314,
            "ave_precision_score": 0.8431951365055718,
            "fpr": 0.12952799121844127,
            "logloss": 0.8822803471540615,
            "mae": 0.2580881159940264,
            "precision": 0.7531380753138075,
            "recall": 0.759493670886076
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7850662001777318,
            "auditor_fn_violation": 0.019021838450292403,
            "auditor_fp_violation": 0.017850978719948025,
            "ave_precision_score": 0.7857767157082953,
            "fpr": 0.11732456140350878,
            "logloss": 1.2343569131944092,
            "mae": 0.3059138198734122,
            "precision": 0.7476415094339622,
            "recall": 0.6604166666666667
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8034137756603188,
            "auditor_fn_violation": 0.016727109357269572,
            "auditor_fp_violation": 0.023385672696033984,
            "ave_precision_score": 0.8037648570227042,
            "fpr": 0.11086717892425905,
            "logloss": 1.1820069373835205,
            "mae": 0.28882010057391705,
            "precision": 0.7662037037037037,
            "recall": 0.6983122362869199
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7948043923923449,
            "auditor_fn_violation": 0.018640350877192992,
            "auditor_fp_violation": 0.015173205003248867,
            "ave_precision_score": 0.7952375094913633,
            "fpr": 0.11074561403508772,
            "logloss": 1.4891124195109844,
            "mae": 0.2936725147688467,
            "precision": 0.7600950118764845,
            "recall": 0.6666666666666666
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8208985082858689,
            "auditor_fn_violation": 0.02107851991829817,
            "auditor_fp_violation": 0.015392846646755772,
            "ave_precision_score": 0.8211941662345711,
            "fpr": 0.09989023051591657,
            "logloss": 1.4219905795280168,
            "mae": 0.27459002841187663,
            "precision": 0.7796610169491526,
            "recall": 0.679324894514768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5910087719298246,
            "auc_prc": 0.6559077608517554,
            "auditor_fn_violation": 0.08617278874269006,
            "auditor_fp_violation": 0.09605973846653675,
            "ave_precision_score": 0.6397384438487099,
            "fpr": 0.27850877192982454,
            "logloss": 5.077646060261943,
            "mae": 0.41072063703276906,
            "precision": 0.5869918699186992,
            "recall": 0.7520833333333333
        },
        "train": {
            "accuracy": 0.5960482985729967,
            "auc_prc": 0.6222077953152585,
            "auditor_fn_violation": 0.08818611717081892,
            "auditor_fp_violation": 0.09678051378147082,
            "ave_precision_score": 0.6043853060773975,
            "fpr": 0.27332601536772777,
            "logloss": 5.553560175117699,
            "mae": 0.4087859399143753,
            "precision": 0.5877483443708609,
            "recall": 0.7489451476793249
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8125033226638564,
            "auditor_fn_violation": 0.012609649122807024,
            "auditor_fp_violation": 0.015244273879142307,
            "ave_precision_score": 0.8131078927178541,
            "fpr": 0.12171052631578948,
            "logloss": 0.9936877437239137,
            "mae": 0.2732264152949013,
            "precision": 0.7643312101910829,
            "recall": 0.75
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8434462105315919,
            "auditor_fn_violation": 0.025047821515745212,
            "auditor_fp_violation": 0.019904196610458995,
            "ave_precision_score": 0.8436799761872921,
            "fpr": 0.11525795828759605,
            "logloss": 0.8882995837319125,
            "mae": 0.25820054507228757,
            "precision": 0.7717391304347826,
            "recall": 0.7489451476793249
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.698912784481668,
            "auditor_fn_violation": 0.021128015350877195,
            "auditor_fp_violation": 0.023257289636127363,
            "ave_precision_score": 0.6865396518543708,
            "fpr": 0.14364035087719298,
            "logloss": 2.19732371753282,
            "mae": 0.309412399641098,
            "precision": 0.7212765957446808,
            "recall": 0.70625
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.6769552456367071,
            "auditor_fn_violation": 0.012760123571723011,
            "auditor_fp_violation": 0.03978327434584169,
            "ave_precision_score": 0.6617726265914152,
            "fpr": 0.1525795828759605,
            "logloss": 2.5781644219420765,
            "mae": 0.30847304087836086,
            "precision": 0.7098121085594989,
            "recall": 0.7172995780590717
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7831100164709657,
            "auditor_fn_violation": 0.019003563596491236,
            "auditor_fp_violation": 0.017721531838856407,
            "ave_precision_score": 0.7839993700959238,
            "fpr": 0.1206140350877193,
            "logloss": 1.2586793903286329,
            "mae": 0.30516191334280846,
            "precision": 0.745958429561201,
            "recall": 0.6729166666666667
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.804203614909041,
            "auditor_fn_violation": 0.019827981492031296,
            "auditor_fp_violation": 0.025073661101161247,
            "ave_precision_score": 0.8045534706260241,
            "fpr": 0.11086717892425905,
            "logloss": 1.2044285954173322,
            "mae": 0.28701978934777955,
            "precision": 0.766743648960739,
            "recall": 0.70042194092827
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7986826394153066,
            "auditor_fn_violation": 0.023828125,
            "auditor_fp_violation": 0.03564611760883691,
            "ave_precision_score": 0.7990060289286265,
            "fpr": 0.17214912280701755,
            "logloss": 1.529775413144131,
            "mae": 0.28678377105063624,
            "precision": 0.70817843866171,
            "recall": 0.79375
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8215510262902881,
            "auditor_fn_violation": 0.03242368241881921,
            "auditor_fp_violation": 0.034061194603460875,
            "ave_precision_score": 0.8218219325762133,
            "fpr": 0.15806805708013172,
            "logloss": 1.52866843989135,
            "mae": 0.27102957530698135,
            "precision": 0.7246653919694073,
            "recall": 0.79957805907173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7835686509511417,
            "auditor_fn_violation": 0.019003563596491236,
            "auditor_fp_violation": 0.017721531838856407,
            "ave_precision_score": 0.7839649154629682,
            "fpr": 0.1206140350877193,
            "logloss": 1.2600143740700727,
            "mae": 0.3051827668039125,
            "precision": 0.745958429561201,
            "recall": 0.6729166666666667
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8042066526783938,
            "auditor_fn_violation": 0.019827981492031296,
            "auditor_fp_violation": 0.025073661101161247,
            "ave_precision_score": 0.8045564697822134,
            "fpr": 0.11086717892425905,
            "logloss": 1.2055372081042774,
            "mae": 0.28703276236883757,
            "precision": 0.766743648960739,
            "recall": 0.70042194092827
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.785172520819703,
            "auditor_fn_violation": 0.01976882309941521,
            "auditor_fp_violation": 0.017127599090318392,
            "ave_precision_score": 0.7857994731963056,
            "fpr": 0.1162280701754386,
            "logloss": 1.2453504381517433,
            "mae": 0.30796527805404766,
            "precision": 0.7476190476190476,
            "recall": 0.6541666666666667
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8033169821869758,
            "auditor_fn_violation": 0.015312148286067627,
            "auditor_fp_violation": 0.019891637172920848,
            "ave_precision_score": 0.8036737577441518,
            "fpr": 0.10867178924259056,
            "logloss": 1.1922457220566525,
            "mae": 0.2895246135911803,
            "precision": 0.7659574468085106,
            "recall": 0.6835443037974683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 10197,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.786124361295654,
            "auditor_fn_violation": 0.01954495614035088,
            "auditor_fp_violation": 0.0201937134502924,
            "ave_precision_score": 0.7867668147751035,
            "fpr": 0.1206140350877193,
            "logloss": 1.226072555021951,
            "mae": 0.30348145631039264,
            "precision": 0.7453703703703703,
            "recall": 0.6708333333333333
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8038302922616922,
            "auditor_fn_violation": 0.0212591532465367,
            "auditor_fp_violation": 0.024960626163317904,
            "ave_precision_score": 0.8041853305798536,
            "fpr": 0.1141602634467618,
            "logloss": 1.1903170773359306,
            "mae": 0.2856492961226411,
            "precision": 0.7636363636363637,
            "recall": 0.7088607594936709
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8115096616060449,
            "auditor_fn_violation": 0.023056012426900585,
            "auditor_fp_violation": 0.017741837231968816,
            "ave_precision_score": 0.811825190281754,
            "fpr": 0.1118421052631579,
            "logloss": 1.372255814059095,
            "mae": 0.2838517502948525,
            "precision": 0.7633410672853829,
            "recall": 0.6854166666666667
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8326799419576346,
            "auditor_fn_violation": 0.02543687791502823,
            "auditor_fp_violation": 0.02147663819023529,
            "ave_precision_score": 0.8329257380085046,
            "fpr": 0.10428100987925357,
            "logloss": 1.3390881060129174,
            "mae": 0.26852324025557556,
            "precision": 0.7785547785547785,
            "recall": 0.7046413502109705
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 10197,
        "test": {
            "accuracy": 0.5855263157894737,
            "auc_prc": 0.6105410055370445,
            "auditor_fn_violation": 0.015168128654970758,
            "auditor_fp_violation": 0.01258934372969462,
            "ave_precision_score": 0.5755504168930055,
            "fpr": 0.19298245614035087,
            "logloss": 7.2323204747934655,
            "mae": 0.4248138631194688,
            "precision": 0.6123348017621145,
            "recall": 0.5791666666666667
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.6241150845210452,
            "auditor_fn_violation": 0.008385554891689479,
            "auditor_fp_violation": 0.007613531035626105,
            "ave_precision_score": 0.5849564793905792,
            "fpr": 0.1964873765093304,
            "logloss": 7.1494292690427885,
            "mae": 0.42076222717108297,
            "precision": 0.6039823008849557,
            "recall": 0.5759493670886076
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 10197,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7815314272840488,
            "auditor_fn_violation": 0.01797560307017544,
            "auditor_fp_violation": 0.022282630766731645,
            "ave_precision_score": 0.782470332717665,
            "fpr": 0.13706140350877194,
            "logloss": 1.0206550076941683,
            "mae": 0.30181864567732075,
            "precision": 0.7294372294372294,
            "recall": 0.7020833333333333
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8042694640271305,
            "auditor_fn_violation": 0.014867512401172728,
            "auditor_fp_violation": 0.01815341101764099,
            "ave_precision_score": 0.8046195550098663,
            "fpr": 0.13062568605927552,
            "logloss": 0.9365821119779646,
            "mae": 0.2845541278310998,
            "precision": 0.7451820128479657,
            "recall": 0.7341772151898734
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7876146783959976,
            "auditor_fn_violation": 0.022242781432748538,
            "auditor_fp_violation": 0.01903122969460689,
            "ave_precision_score": 0.788240183038021,
            "fpr": 0.1162280701754386,
            "logloss": 1.3467796484274983,
            "mae": 0.3047387381909633,
            "precision": 0.747016706443914,
            "recall": 0.6520833333333333
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8036748089049727,
            "auditor_fn_violation": 0.020061878493981205,
            "auditor_fp_violation": 0.023847859997437874,
            "ave_precision_score": 0.8040241064152502,
            "fpr": 0.10647639956092206,
            "logloss": 1.3399173280778733,
            "mae": 0.2870847925918457,
            "precision": 0.7701421800947867,
            "recall": 0.6856540084388185
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 10197,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7844448160070865,
            "auditor_fn_violation": 0.019024122807017544,
            "auditor_fp_violation": 0.017335729369720602,
            "ave_precision_score": 0.7848058740104308,
            "fpr": 0.1162280701754386,
            "logloss": 1.2687729516679211,
            "mae": 0.30664016276775596,
            "precision": 0.7488151658767772,
            "recall": 0.6583333333333333
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8021424187881055,
            "auditor_fn_violation": 0.016604371326543372,
            "auditor_fp_violation": 0.023654444659350373,
            "ave_precision_score": 0.8025007232004543,
            "fpr": 0.1119648737650933,
            "logloss": 1.2096457714018793,
            "mae": 0.2892464421374049,
            "precision": 0.7638888888888888,
            "recall": 0.6962025316455697
        }
    }
]
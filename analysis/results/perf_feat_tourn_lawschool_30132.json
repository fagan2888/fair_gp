[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7882081156955285,
            "auditor_fn_violation": 0.011815954635542834,
            "auditor_fp_violation": 0.009161007357102431,
            "ave_precision_score": 0.7661734017287734,
            "fpr": 0.2149122807017544,
            "logloss": 1.8316293399602726,
            "mae": 0.3053144955774769,
            "precision": 0.6802610114192496,
            "recall": 0.8723849372384938
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7871243428442656,
            "auditor_fn_violation": 0.0038350136981246978,
            "auditor_fp_violation": 0.021981654617257783,
            "ave_precision_score": 0.7647136315294185,
            "fpr": 0.22941822173435786,
            "logloss": 2.000318694514808,
            "mae": 0.3096314973850749,
            "precision": 0.6671974522292994,
            "recall": 0.8802521008403361
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7337025744379697,
            "auditor_fn_violation": 0.003417932907582769,
            "auditor_fp_violation": 0.015055279327350632,
            "ave_precision_score": 0.6888336651889002,
            "fpr": 0.22478070175438597,
            "logloss": 3.2662974923225434,
            "mae": 0.32502060622209683,
            "precision": 0.6688206785137318,
            "recall": 0.8661087866108786
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.7207180255692911,
            "auditor_fn_violation": 0.003632078517466263,
            "auditor_fp_violation": 0.02501987206177374,
            "ave_precision_score": 0.6703593164711169,
            "fpr": 0.25686059275521406,
            "logloss": 3.7471162342500155,
            "mae": 0.33447318098944345,
            "precision": 0.6427480916030535,
            "recall": 0.884453781512605
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7877859797596933,
            "auditor_fn_violation": 0.01082957131322029,
            "auditor_fp_violation": 0.009479343520090557,
            "ave_precision_score": 0.7628082782553315,
            "fpr": 0.19407894736842105,
            "logloss": 1.9312959974642423,
            "mae": 0.298229347630312,
            "precision": 0.696917808219178,
            "recall": 0.8514644351464435
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7863556081251404,
            "auditor_fn_violation": 0.0031155162394266175,
            "auditor_fp_violation": 0.01707104735228434,
            "ave_precision_score": 0.7599127486468701,
            "fpr": 0.21405049396267836,
            "logloss": 2.164554020672183,
            "mae": 0.3014702271921463,
            "precision": 0.6813725490196079,
            "recall": 0.8760504201680672
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7859797251525941,
            "auditor_fn_violation": 0.009255945826910372,
            "auditor_fp_violation": 0.007215619694397286,
            "ave_precision_score": 0.7609529156348158,
            "fpr": 0.19956140350877194,
            "logloss": 1.9506211828553732,
            "mae": 0.30137863265592485,
            "precision": 0.6910016977928692,
            "recall": 0.8514644351464435
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7843789479020733,
            "auditor_fn_violation": 0.0023798762095397982,
            "auditor_fp_violation": 0.01738900034066392,
            "ave_precision_score": 0.7572482367423587,
            "fpr": 0.21624588364434688,
            "logloss": 2.2039452341114028,
            "mae": 0.30383006374788846,
            "precision": 0.6786296900489397,
            "recall": 0.8739495798319328
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8019421199314208,
            "auditor_fn_violation": 0.013151012992732882,
            "auditor_fp_violation": 0.01198055622928289,
            "ave_precision_score": 0.7798043833766859,
            "fpr": 0.12828947368421054,
            "logloss": 1.705859995828182,
            "mae": 0.2715942073497038,
            "precision": 0.7572614107883817,
            "recall": 0.7635983263598326
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.7928670459823358,
            "auditor_fn_violation": 0.010836277430840615,
            "auditor_fp_violation": 0.02034142094704569,
            "ave_precision_score": 0.7702360916765454,
            "fpr": 0.1437980241492865,
            "logloss": 1.8975481387560769,
            "mae": 0.2770928635753811,
            "precision": 0.744140625,
            "recall": 0.8004201680672269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.7993664550238748,
            "auditor_fn_violation": 0.024519654261175954,
            "auditor_fp_violation": 0.01573995472552349,
            "ave_precision_score": 0.7744834823549089,
            "fpr": 0.13815789473684212,
            "logloss": 1.7898114309753719,
            "mae": 0.27417854550364945,
            "precision": 0.7495029821073559,
            "recall": 0.7887029288702929
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.7934496713390005,
            "auditor_fn_violation": 0.010552629394238488,
            "auditor_fp_violation": 0.022718498050645377,
            "ave_precision_score": 0.7693063183692643,
            "fpr": 0.15587266739846323,
            "logloss": 1.955632344552659,
            "mae": 0.27986338203743233,
            "precision": 0.7310606060606061,
            "recall": 0.8109243697478992
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7884661516427339,
            "auditor_fn_violation": 0.006081167877853634,
            "auditor_fp_violation": 0.014112903225806453,
            "ave_precision_score": 0.7666390248117974,
            "fpr": 0.20833333333333334,
            "logloss": 1.816752797421452,
            "mae": 0.3046973109625182,
            "precision": 0.6859504132231405,
            "recall": 0.8682008368200836
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7880860118272979,
            "auditor_fn_violation": 0.001388261122231552,
            "auditor_fp_violation": 0.02426284113706046,
            "ave_precision_score": 0.7663792342624421,
            "fpr": 0.22941822173435786,
            "logloss": 1.9667647290074082,
            "mae": 0.31074033999553874,
            "precision": 0.6666666666666666,
            "recall": 0.8781512605042017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7847927334513587,
            "auditor_fn_violation": 0.0063449680687073335,
            "auditor_fp_violation": 0.011060918425094993,
            "ave_precision_score": 0.759739436039802,
            "fpr": 0.20175438596491227,
            "logloss": 1.9552899408207103,
            "mae": 0.3040444635880065,
            "precision": 0.688663282571912,
            "recall": 0.8514644351464435
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.781313715674615,
            "auditor_fn_violation": 0.0022415113136363216,
            "auditor_fp_violation": 0.018047617245164467,
            "ave_precision_score": 0.753544178955306,
            "fpr": 0.22063666300768386,
            "logloss": 2.2346054133659923,
            "mae": 0.3089567342851414,
            "precision": 0.6742301458670988,
            "recall": 0.8739495798319328
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7880925412620796,
            "auditor_fn_violation": 0.006622531747779492,
            "auditor_fp_violation": 0.007139825369876315,
            "ave_precision_score": 0.7660009090896103,
            "fpr": 0.23903508771929824,
            "logloss": 1.9132499214014145,
            "mae": 0.3165498868528582,
            "precision": 0.6630602782071098,
            "recall": 0.897489539748954
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7865026448437611,
            "auditor_fn_violation": 0.001831028789122674,
            "auditor_fp_violation": 0.01419685327478961,
            "ave_precision_score": 0.7639974003693655,
            "fpr": 0.25466520307354557,
            "logloss": 2.0846352539289956,
            "mae": 0.3213841798908254,
            "precision": 0.6511278195488722,
            "recall": 0.9096638655462185
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8025255533774762,
            "auditor_fn_violation": 0.021975702855465025,
            "auditor_fp_violation": 0.013276639178591648,
            "ave_precision_score": 0.7796673676799307,
            "fpr": 0.12828947368421054,
            "logloss": 1.7181869029971812,
            "mae": 0.271418434663769,
            "precision": 0.7597535934291582,
            "recall": 0.7740585774058577
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.796063070758181,
            "auditor_fn_violation": 0.011530407991956386,
            "auditor_fp_violation": 0.021916045270449293,
            "ave_precision_score": 0.7742075144500637,
            "fpr": 0.14928649835345773,
            "logloss": 1.864121995862777,
            "mae": 0.2752446831833591,
            "precision": 0.7394636015325671,
            "recall": 0.8109243697478992
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8309693974807495,
            "auditor_fn_violation": 0.0181953314247963,
            "auditor_fp_violation": 0.01563636914867815,
            "ave_precision_score": 0.8312782322253774,
            "fpr": 0.12609649122807018,
            "logloss": 0.8156269721601137,
            "mae": 0.268746602861798,
            "precision": 0.7568710359408034,
            "recall": 0.7489539748953975
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8311657786099168,
            "auditor_fn_violation": 0.017636912064496496,
            "auditor_fp_violation": 0.025496801544343084,
            "ave_precision_score": 0.8316139242422509,
            "fpr": 0.14489571899012074,
            "logloss": 0.8457558188078566,
            "mae": 0.27223614450001493,
            "precision": 0.7401574803149606,
            "recall": 0.7899159663865546
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.7909219088238548,
            "auditor_fn_violation": 0.01787647728106878,
            "auditor_fp_violation": 0.018145161290322585,
            "ave_precision_score": 0.7687441067483923,
            "fpr": 0.17763157894736842,
            "logloss": 1.7582634980587624,
            "mae": 0.3079893462504914,
            "precision": 0.7081081081081081,
            "recall": 0.8221757322175732
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7852690879341049,
            "auditor_fn_violation": 0.009263529780737764,
            "auditor_fp_violation": 0.02854006586169045,
            "ave_precision_score": 0.7626451060228873,
            "fpr": 0.19099890230515917,
            "logloss": 1.9622225689354513,
            "mae": 0.3120299222205411,
            "precision": 0.697391304347826,
            "recall": 0.842436974789916
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 30132,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.7989569488923095,
            "auditor_fn_violation": 0.024512772517066725,
            "auditor_fp_violation": 0.011566213921901528,
            "ave_precision_score": 0.7733924568140758,
            "fpr": 0.12280701754385964,
            "logloss": 1.8279429060712977,
            "mae": 0.2721567551565657,
            "precision": 0.7637130801687764,
            "recall": 0.7573221757322176
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.792829147413788,
            "auditor_fn_violation": 0.012886383971810463,
            "auditor_fp_violation": 0.024005450622657937,
            "ave_precision_score": 0.7672308648510888,
            "fpr": 0.14489571899012074,
            "logloss": 2.0234231163515974,
            "mae": 0.27550302401718024,
            "precision": 0.7421875,
            "recall": 0.7983193277310925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7719298245614035,
            "auc_prc": 0.8451536002520518,
            "auditor_fn_violation": 0.008815514203919853,
            "auditor_fp_violation": 0.0044061767321529645,
            "ave_precision_score": 0.8460341465551439,
            "fpr": 0.08223684210526316,
            "logloss": 0.510744531177208,
            "mae": 0.3099107375078073,
            "precision": 0.8214285714285714,
            "recall": 0.7217573221757322
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8604770980805935,
            "auditor_fn_violation": 0.00828575118301987,
            "auditor_fp_violation": 0.004597701149425288,
            "ave_precision_score": 0.860668991258509,
            "fpr": 0.07354555433589462,
            "logloss": 0.5315139271794488,
            "mae": 0.31719262407075266,
            "precision": 0.8273195876288659,
            "recall": 0.6743697478991597
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7871462215112048,
            "auditor_fn_violation": 0.002211333773764957,
            "auditor_fp_violation": 0.013653084323712508,
            "ave_precision_score": 0.7642987084664195,
            "fpr": 0.2412280701754386,
            "logloss": 1.9386367282687624,
            "mae": 0.3154286857967044,
            "precision": 0.6625766871165644,
            "recall": 0.9037656903765691
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7831935132948389,
            "auditor_fn_violation": 0.012291414919425507,
            "auditor_fp_violation": 0.023904513166029505,
            "ave_precision_score": 0.7606191988993869,
            "fpr": 0.2645444566410538,
            "logloss": 2.120784060672244,
            "mae": 0.32471062679063023,
            "precision": 0.6450662739322534,
            "recall": 0.9201680672268907
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7892462180787609,
            "auditor_fn_violation": 0.004402022315202234,
            "auditor_fp_violation": 0.004527447651386538,
            "ave_precision_score": 0.7670727109402979,
            "fpr": 0.19956140350877194,
            "logloss": 1.8347071792579994,
            "mae": 0.2984868988067702,
            "precision": 0.6930860033726813,
            "recall": 0.8598326359832636
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7860587719977608,
            "auditor_fn_violation": 0.0037635251685745647,
            "auditor_fp_violation": 0.02345534148403296,
            "ave_precision_score": 0.7635261989850382,
            "fpr": 0.21844127332601537,
            "logloss": 2.024871036381298,
            "mae": 0.30339710917500173,
            "precision": 0.6764227642276422,
            "recall": 0.8739495798319328
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7875548416578687,
            "auditor_fn_violation": 0.011992586067679663,
            "auditor_fp_violation": 0.008579917535774919,
            "ave_precision_score": 0.7607730968777838,
            "fpr": 0.23355263157894737,
            "logloss": 2.107905234614653,
            "mae": 0.31464108584904144,
            "precision": 0.6666666666666666,
            "recall": 0.891213389121339
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7849529876120224,
            "auditor_fn_violation": 0.0010976948408342481,
            "auditor_fp_violation": 0.021305373657847262,
            "ave_precision_score": 0.75624385146673,
            "fpr": 0.2557628979143798,
            "logloss": 2.3749583287602274,
            "mae": 0.3200705225024072,
            "precision": 0.6506746626686657,
            "recall": 0.9117647058823529
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7888361890123261,
            "auditor_fn_violation": 0.005308118622917125,
            "auditor_fp_violation": 0.008200945913170029,
            "ave_precision_score": 0.7667276193522434,
            "fpr": 0.23464912280701755,
            "logloss": 1.8887833361482442,
            "mae": 0.31372500073038073,
            "precision": 0.6651017214397497,
            "recall": 0.8891213389121339
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.787243431130491,
            "auditor_fn_violation": 0.0008324954570192506,
            "auditor_fp_violation": 0.010563104836165899,
            "ave_precision_score": 0.7647247268460249,
            "fpr": 0.24807903402854006,
            "logloss": 2.0641102542042473,
            "mae": 0.3182492441591412,
            "precision": 0.6549618320610687,
            "recall": 0.9012605042016807
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 30132,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7879795371099847,
            "auditor_fn_violation": 0.007214361741173016,
            "auditor_fp_violation": 0.00880730050933786,
            "ave_precision_score": 0.7657924762718088,
            "fpr": 0.2532894736842105,
            "logloss": 1.9506011166891442,
            "mae": 0.32223074285062275,
            "precision": 0.6531531531531531,
            "recall": 0.9100418410041841
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7864993083088264,
            "auditor_fn_violation": 0.0059473844422511055,
            "auditor_fp_violation": 0.013550853552367613,
            "ave_precision_score": 0.7639083259326857,
            "fpr": 0.2689352360043908,
            "logloss": 2.1312521574985426,
            "mae": 0.3275748467892991,
            "precision": 0.6407624633431085,
            "recall": 0.9180672268907563
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.7998531737845389,
            "auditor_fn_violation": 0.01697496880276004,
            "auditor_fp_violation": 0.011844126445145129,
            "ave_precision_score": 0.7742512124339935,
            "fpr": 0.16557017543859648,
            "logloss": 1.8368735488410979,
            "mae": 0.2792605233676831,
            "precision": 0.7303571428571428,
            "recall": 0.8556485355648535
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.7922739026451326,
            "auditor_fn_violation": 0.015755149480209212,
            "auditor_fp_violation": 0.028734370465700196,
            "ave_precision_score": 0.7666158554112945,
            "fpr": 0.18990120746432493,
            "logloss": 2.0692856874390078,
            "mae": 0.28857429372598775,
            "precision": 0.7042735042735043,
            "recall": 0.865546218487395
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7917750075165133,
            "auditor_fn_violation": 0.011997173897085812,
            "auditor_fp_violation": 0.009903791737408029,
            "ave_precision_score": 0.770542112860018,
            "fpr": 0.2225877192982456,
            "logloss": 1.811131501964453,
            "mae": 0.3069449449754565,
            "precision": 0.6772655007949125,
            "recall": 0.891213389121339
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7909308635423674,
            "auditor_fn_violation": 0.006706085288121835,
            "auditor_fp_violation": 0.021913521834033597,
            "ave_precision_score": 0.7700142123503804,
            "fpr": 0.24259055982436883,
            "logloss": 1.9724958084056177,
            "mae": 0.3124428752903815,
            "precision": 0.6615620214395099,
            "recall": 0.907563025210084
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7879129892941035,
            "auditor_fn_violation": 0.004463958012185277,
            "auditor_fp_violation": 0.00839295820195651,
            "ave_precision_score": 0.7659280789732101,
            "fpr": 0.22697368421052633,
            "logloss": 1.8645920048480564,
            "mae": 0.31186208381468694,
            "precision": 0.6740157480314961,
            "recall": 0.895397489539749
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7836281448234257,
            "auditor_fn_violation": 0.009367303452665373,
            "auditor_fp_violation": 0.01846146081734105,
            "ave_precision_score": 0.7619065584817635,
            "fpr": 0.24807903402854006,
            "logloss": 2.0419848033468586,
            "mae": 0.3231681936951334,
            "precision": 0.6533742331288344,
            "recall": 0.8949579831932774
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7890775275852051,
            "auditor_fn_violation": 0.005312706452323278,
            "auditor_fp_violation": 0.007981142372059187,
            "ave_precision_score": 0.7670831519943206,
            "fpr": 0.23135964912280702,
            "logloss": 1.8758234108383227,
            "mae": 0.3134063642585381,
            "precision": 0.6687598116169545,
            "recall": 0.891213389121339
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.788488879103997,
            "auditor_fn_violation": 0.002545914084623973,
            "auditor_fp_violation": 0.016831320892791806,
            "ave_precision_score": 0.7667278270452247,
            "fpr": 0.25466520307354557,
            "logloss": 2.0314092035826437,
            "mae": 0.31847975555656305,
            "precision": 0.649546827794562,
            "recall": 0.9033613445378151
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.799426018233414,
            "auditor_fn_violation": 0.008508129633707704,
            "auditor_fp_violation": 0.014236700622524056,
            "ave_precision_score": 0.7784878862287604,
            "fpr": 0.11513157894736842,
            "logloss": 1.6365679114571083,
            "mae": 0.2786040916009306,
            "precision": 0.7676991150442478,
            "recall": 0.7259414225941423
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.799260170392088,
            "auditor_fn_violation": 0.01180713778376334,
            "auditor_fp_violation": 0.014307884477080892,
            "ave_precision_score": 0.783796362476212,
            "fpr": 0.13172338090010977,
            "logloss": 1.6064253353522082,
            "mae": 0.27915915435626815,
            "precision": 0.7540983606557377,
            "recall": 0.773109243697479
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.6723702650138681,
            "auditor_fn_violation": 0.010469426704837411,
            "auditor_fp_violation": 0.0007781550650820583,
            "ave_precision_score": 0.6697714783706438,
            "fpr": 0.24013157894736842,
            "logloss": 1.3736889172718747,
            "mae": 0.33990251331635046,
            "precision": 0.6630769230769231,
            "recall": 0.9016736401673641
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.6586856260632936,
            "auditor_fn_violation": 0.008435646486915296,
            "auditor_fp_violation": 0.015001829491401393,
            "ave_precision_score": 0.6546216408380302,
            "fpr": 0.25466520307354557,
            "logloss": 1.533579286704985,
            "mae": 0.3434832304814252,
            "precision": 0.649016641452345,
            "recall": 0.9012605042016807
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.788081021837965,
            "auditor_fn_violation": 0.007409344490934451,
            "auditor_fp_violation": 0.00409542000161695,
            "ave_precision_score": 0.7660587281735664,
            "fpr": 0.21600877192982457,
            "logloss": 1.8344967770220086,
            "mae": 0.30560434972019784,
            "precision": 0.6786296900489397,
            "recall": 0.8702928870292888
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7869832846038677,
            "auditor_fn_violation": 6.457028475495517e-05,
            "auditor_fp_violation": 0.021981654617257783,
            "ave_precision_score": 0.7645620994998156,
            "fpr": 0.22941822173435786,
            "logloss": 2.0032109416328137,
            "mae": 0.30997383346341284,
            "precision": 0.6677265500794912,
            "recall": 0.8823529411764706
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8493313082916021,
            "auditor_fn_violation": 0.01967949423768627,
            "auditor_fp_violation": 0.0070741369552914535,
            "ave_precision_score": 0.8498853040842136,
            "fpr": 0.1074561403508772,
            "logloss": 0.49719690792064636,
            "mae": 0.3080258243951488,
            "precision": 0.7855579868708972,
            "recall": 0.7510460251046025
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8599043981837635,
            "auditor_fn_violation": 0.013262275272348243,
            "auditor_fp_violation": 0.022723544923476793,
            "ave_precision_score": 0.8602436651727923,
            "fpr": 0.09989023051591657,
            "logloss": 0.47676127104649163,
            "mae": 0.3063788660283765,
            "precision": 0.7995594713656388,
            "recall": 0.7626050420168067
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7887367738663555,
            "auditor_fn_violation": 0.00659729868604566,
            "auditor_fp_violation": 0.01589659632953351,
            "ave_precision_score": 0.7658900552579325,
            "fpr": 0.22149122807017543,
            "logloss": 1.9046494143578185,
            "mae": 0.30549518508466955,
            "precision": 0.6762820512820513,
            "recall": 0.8828451882845189
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7846411322431839,
            "auditor_fn_violation": 0.003440673744799787,
            "auditor_fp_violation": 0.02250652939172565,
            "ave_precision_score": 0.7606194773877504,
            "fpr": 0.24368825466520308,
            "logloss": 2.127528246250867,
            "mae": 0.31163712515639863,
            "precision": 0.6584615384615384,
            "recall": 0.8991596638655462
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8039632316974223,
            "auditor_fn_violation": 0.018144865301328646,
            "auditor_fp_violation": 0.008064516129032265,
            "ave_precision_score": 0.7811571785602638,
            "fpr": 0.1074561403508772,
            "logloss": 1.7204000283657208,
            "mae": 0.26931591357638435,
            "precision": 0.7846153846153846,
            "recall": 0.7468619246861925
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.7943858849137551,
            "auditor_fn_violation": 0.013043197520501065,
            "auditor_fp_violation": 0.02110349874459039,
            "ave_precision_score": 0.771734977889423,
            "fpr": 0.13391877058177826,
            "logloss": 1.8913098750926107,
            "mae": 0.27832637879986216,
            "precision": 0.7510204081632653,
            "recall": 0.773109243697479
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7870419262040478,
            "auditor_fn_violation": 0.004578653747339059,
            "auditor_fp_violation": 0.009888632872503845,
            "ave_precision_score": 0.7620693980696261,
            "fpr": 0.23684210526315788,
            "logloss": 1.9989770020086768,
            "mae": 0.31374257199135364,
            "precision": 0.6645962732919255,
            "recall": 0.895397489539749
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7873863816404884,
            "auditor_fn_violation": 0.0018448652787130227,
            "auditor_fp_violation": 0.024817997148516853,
            "ave_precision_score": 0.7616405255749501,
            "fpr": 0.2524698133918771,
            "logloss": 2.2066695648219667,
            "mae": 0.316902612029607,
            "precision": 0.6525679758308157,
            "recall": 0.907563025210084
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.7865217985490285,
            "auditor_fn_violation": 0.008840747265653674,
            "auditor_fp_violation": 0.008478858436413625,
            "ave_precision_score": 0.7613513740283332,
            "fpr": 0.22807017543859648,
            "logloss": 2.0224152888936837,
            "mae": 0.3129136950914184,
            "precision": 0.6708860759493671,
            "recall": 0.8870292887029289
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7840068700722029,
            "auditor_fn_violation": 0.005131031556420594,
            "auditor_fp_violation": 0.01984682740956636,
            "ave_precision_score": 0.7567757731583455,
            "fpr": 0.24807903402854006,
            "logloss": 2.279904539083582,
            "mae": 0.3172240127664561,
            "precision": 0.6560121765601218,
            "recall": 0.9054621848739496
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7896374037490408,
            "auditor_fn_violation": 0.006762460544667106,
            "auditor_fp_violation": 0.005613832969520577,
            "ave_precision_score": 0.7677124186294385,
            "fpr": 0.21929824561403508,
            "logloss": 1.8200758139101514,
            "mae": 0.3041583424208894,
            "precision": 0.6768982229402262,
            "recall": 0.8765690376569037
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7892837170422791,
            "auditor_fn_violation": 0.002140043723307106,
            "auditor_fp_violation": 0.020909194140580654,
            "ave_precision_score": 0.7675847519140123,
            "fpr": 0.23161361141602635,
            "logloss": 1.968724080334186,
            "mae": 0.30800260099628124,
            "precision": 0.6677165354330709,
            "recall": 0.8907563025210085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.7872421760948802,
            "auditor_fn_violation": 0.009866127137928508,
            "auditor_fp_violation": 0.0056795213841054236,
            "ave_precision_score": 0.7614006374903799,
            "fpr": 0.19517543859649122,
            "logloss": 1.9681190349161097,
            "mae": 0.2998626892303787,
            "precision": 0.6946826758147513,
            "recall": 0.8472803347280334
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7831668568336657,
            "auditor_fn_violation": 0.0031155162394266175,
            "auditor_fp_violation": 0.017141703571924245,
            "ave_precision_score": 0.7553906912883471,
            "fpr": 0.21295279912184412,
            "logloss": 2.2319027764901094,
            "mae": 0.30351154360399835,
            "precision": 0.6824877250409165,
            "recall": 0.8760504201680672
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7880010916671879,
            "auditor_fn_violation": 0.005792134625266095,
            "auditor_fp_violation": 0.0080291454442558,
            "ave_precision_score": 0.7661125041550783,
            "fpr": 0.23026315789473684,
            "logloss": 1.8592259660101333,
            "mae": 0.3126152687197502,
            "precision": 0.6692913385826772,
            "recall": 0.8891213389121339
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7888260878335702,
            "auditor_fn_violation": 0.0008324954570192506,
            "auditor_fp_violation": 0.016841414638454646,
            "ave_precision_score": 0.7678194866641201,
            "fpr": 0.2513721185510428,
            "logloss": 1.9909599101013946,
            "mae": 0.3180603678870469,
            "precision": 0.6519756838905775,
            "recall": 0.9012605042016807
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 30132,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.7990604392058954,
            "auditor_fn_violation": 0.024290262790868385,
            "auditor_fp_violation": 0.01100028296547821,
            "ave_precision_score": 0.7734596453559797,
            "fpr": 0.12390350877192982,
            "logloss": 1.8345604604213184,
            "mae": 0.2721918511215854,
            "precision": 0.7626050420168067,
            "recall": 0.7594142259414226
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.7926631006927598,
            "auditor_fn_violation": 0.0106817699637484,
            "auditor_fp_violation": 0.02286738079917231,
            "ave_precision_score": 0.7670694397444555,
            "fpr": 0.1437980241492865,
            "logloss": 2.033438894907146,
            "mae": 0.2754832573556894,
            "precision": 0.7436399217221135,
            "recall": 0.7983193277310925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.8020687813685314,
            "auditor_fn_violation": 0.02524453130734787,
            "auditor_fp_violation": 0.010287816314981015,
            "ave_precision_score": 0.7892329229793971,
            "fpr": 0.125,
            "logloss": 1.2673775122690962,
            "mae": 0.2730470628522884,
            "precision": 0.7733598409542743,
            "recall": 0.8138075313807531
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.810693232719596,
            "auditor_fn_violation": 0.012969402909352548,
            "auditor_fp_violation": 0.026518793292706008,
            "ave_precision_score": 0.8009810370495649,
            "fpr": 0.15697036223929747,
            "logloss": 1.179765815255652,
            "mae": 0.2777502308761703,
            "precision": 0.7317073170731707,
            "recall": 0.819327731092437
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7858762683145415,
            "auditor_fn_violation": 0.005216362034794099,
            "auditor_fp_violation": 0.011636955291454441,
            "ave_precision_score": 0.7638291695807163,
            "fpr": 0.22587719298245615,
            "logloss": 1.8835496086362113,
            "mae": 0.3115409007602322,
            "precision": 0.671451355661882,
            "recall": 0.8807531380753139
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7857769439011872,
            "auditor_fn_violation": 0.005829774280733149,
            "auditor_fp_violation": 0.018943437172741853,
            "ave_precision_score": 0.7633155121228921,
            "fpr": 0.23929747530186607,
            "logloss": 2.051454245732847,
            "mae": 0.31587295261534104,
            "precision": 0.6614906832298136,
            "recall": 0.8949579831932774
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8008669469190919,
            "auditor_fn_violation": 0.002220509432577265,
            "auditor_fp_violation": 0.02213194276012612,
            "ave_precision_score": 0.7796109279407263,
            "fpr": 0.15899122807017543,
            "logloss": 1.645825591718995,
            "mae": 0.28052811125886173,
            "precision": 0.7358834244080146,
            "recall": 0.8451882845188284
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.7996328934601582,
            "auditor_fn_violation": 0.010778625390880833,
            "auditor_fp_violation": 0.023770771035996827,
            "ave_precision_score": 0.7831250359795365,
            "fpr": 0.18111964873765093,
            "logloss": 1.6825977130229313,
            "mae": 0.2880993005903959,
            "precision": 0.708994708994709,
            "recall": 0.8445378151260504
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7880772054117657,
            "auditor_fn_violation": 0.01077681127504955,
            "auditor_fp_violation": 0.013521707494542812,
            "ave_precision_score": 0.7630585149724904,
            "fpr": 0.20065789473684212,
            "logloss": 1.9255119351924852,
            "mae": 0.29955210324234877,
            "precision": 0.692436974789916,
            "recall": 0.8619246861924686
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7824418847350821,
            "auditor_fn_violation": 0.009846968425130756,
            "auditor_fp_violation": 0.02686450408165841,
            "ave_precision_score": 0.7561445640925765,
            "fpr": 0.21844127332601537,
            "logloss": 2.175877258975614,
            "mae": 0.30861624091062145,
            "precision": 0.6748366013071896,
            "recall": 0.8676470588235294
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.799418777708715,
            "auditor_fn_violation": 0.022718931219261544,
            "auditor_fp_violation": 0.013905732072115781,
            "ave_precision_score": 0.7738549411652905,
            "fpr": 0.12828947368421054,
            "logloss": 1.8257189242802192,
            "mae": 0.27208449188900996,
            "precision": 0.7587628865979381,
            "recall": 0.7698744769874477
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.7936790646085748,
            "auditor_fn_violation": 0.011048436937892618,
            "auditor_fp_violation": 0.018903062190090473,
            "ave_precision_score": 0.768045431765764,
            "fpr": 0.14709110867178923,
            "logloss": 2.025581286435736,
            "mae": 0.2753704613519588,
            "precision": 0.7398058252427184,
            "recall": 0.8004201680672269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.8255462470993922,
            "auditor_fn_violation": 0.009184834471115027,
            "auditor_fp_violation": 0.023258751718004702,
            "ave_precision_score": 0.8259915398186426,
            "fpr": 0.21710526315789475,
            "logloss": 0.918522990835471,
            "mae": 0.2951359705608252,
            "precision": 0.6769983686786297,
            "recall": 0.8682008368200836
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.8191119819273016,
            "auditor_fn_violation": 0.011368982280069002,
            "auditor_fp_violation": 0.020099171051137447,
            "ave_precision_score": 0.8199197643377456,
            "fpr": 0.23380900109769484,
            "logloss": 0.981955071060696,
            "mae": 0.3016578783616495,
            "precision": 0.6661442006269592,
            "recall": 0.8928571428571429
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7888523244806884,
            "auditor_fn_violation": 0.0062784445423181385,
            "auditor_fp_violation": 0.007670385641523168,
            "ave_precision_score": 0.766747078552691,
            "fpr": 0.23574561403508773,
            "logloss": 1.8923286888980433,
            "mae": 0.3140944968612881,
            "precision": 0.6651090342679128,
            "recall": 0.893305439330544
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7873604724318473,
            "auditor_fn_violation": 0.0011991624311634668,
            "auditor_fp_violation": 0.01283167417389002,
            "ave_precision_score": 0.7648106436548655,
            "fpr": 0.2502744237102086,
            "logloss": 2.0677856539199295,
            "mae": 0.3182713632997645,
            "precision": 0.6534954407294833,
            "recall": 0.9033613445378151
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.7998342526846395,
            "auditor_fn_violation": 0.0039409454598840175,
            "auditor_fp_violation": 0.01281429379901367,
            "ave_precision_score": 0.7788482751884717,
            "fpr": 0.125,
            "logloss": 1.6263804307556984,
            "mae": 0.2775336474058573,
            "precision": 0.7625,
            "recall": 0.7656903765690377
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.79859246976338,
            "auditor_fn_violation": 0.011290575505723694,
            "auditor_fp_violation": 0.015996063439191494,
            "ave_precision_score": 0.7824752869578155,
            "fpr": 0.141602634467618,
            "logloss": 1.6290089921759776,
            "mae": 0.2811569396694447,
            "precision": 0.7504835589941973,
            "recall": 0.8151260504201681
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 30132,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.7998103236147269,
            "auditor_fn_violation": 0.006292208030536595,
            "auditor_fp_violation": 0.010788058856819471,
            "ave_precision_score": 0.7786203952476393,
            "fpr": 0.11513157894736842,
            "logloss": 1.6443380358014945,
            "mae": 0.2798820161442244,
            "precision": 0.7687224669603524,
            "recall": 0.7301255230125523
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.7981605179709548,
            "auditor_fn_violation": 0.007568559805920177,
            "auditor_fp_violation": 0.020477686513494076,
            "ave_precision_score": 0.781885889556785,
            "fpr": 0.1350164654226125,
            "logloss": 1.6351108366529268,
            "mae": 0.2791607193673734,
            "precision": 0.7530120481927711,
            "recall": 0.7878151260504201
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7871500684218286,
            "auditor_fn_violation": 0.011815954635542834,
            "auditor_fp_violation": 0.01467883418222976,
            "ave_precision_score": 0.7652043993902184,
            "fpr": 0.2149122807017544,
            "logloss": 1.8568123203420517,
            "mae": 0.30849755267467627,
            "precision": 0.6802610114192496,
            "recall": 0.8723849372384938
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7849809362483162,
            "auditor_fn_violation": 0.0038811353300925214,
            "auditor_fp_violation": 0.024464716050317335,
            "ave_precision_score": 0.762573327003495,
            "fpr": 0.2414928649835346,
            "logloss": 2.0358141674158037,
            "mae": 0.3170795265231721,
            "precision": 0.6557120500782473,
            "recall": 0.8802521008403361
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7895946604083424,
            "auditor_fn_violation": 0.012559182999339355,
            "auditor_fp_violation": 0.0035016977928692757,
            "ave_precision_score": 0.7674064587889217,
            "fpr": 0.22149122807017543,
            "logloss": 1.8727902941815857,
            "mae": 0.308302999568568,
            "precision": 0.6757624398073836,
            "recall": 0.8807531380753139
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7878178833851371,
            "auditor_fn_violation": 0.0017802949939580658,
            "auditor_fp_violation": 0.015983446257112937,
            "ave_precision_score": 0.7652144988335684,
            "fpr": 0.2349066959385291,
            "logloss": 2.0476477291198854,
            "mae": 0.31176908327907815,
            "precision": 0.664576802507837,
            "recall": 0.8907563025210085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7982609815339387,
            "auditor_fn_violation": 0.009588563458856345,
            "auditor_fp_violation": 0.018963739995149165,
            "ave_precision_score": 0.7740113368480125,
            "fpr": 0.12609649122807018,
            "logloss": 1.774842146891178,
            "mae": 0.27785672811845424,
            "precision": 0.7584033613445378,
            "recall": 0.7552301255230126
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.7910570959897951,
            "auditor_fn_violation": 0.006374009537953494,
            "auditor_fp_violation": 0.013767869084118763,
            "ave_precision_score": 0.7647927217441708,
            "fpr": 0.14709110867178923,
            "logloss": 2.01794633346678,
            "mae": 0.2813569752008242,
            "precision": 0.7392996108949417,
            "recall": 0.7983193277310925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7837134909409036,
            "auditor_fn_violation": 0.009925768920208472,
            "auditor_fp_violation": 0.013238742016331156,
            "ave_precision_score": 0.7617707063171403,
            "fpr": 0.2324561403508772,
            "logloss": 1.9162663158260385,
            "mae": 0.31655690812642623,
            "precision": 0.6671899529042387,
            "recall": 0.8891213389121339
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.7841484061243493,
            "auditor_fn_violation": 0.0009547177817339892,
            "auditor_fp_violation": 0.019463265074378285,
            "ave_precision_score": 0.7623673735085462,
            "fpr": 0.25686059275521406,
            "logloss": 2.072144123083191,
            "mae": 0.32308146394448023,
            "precision": 0.6470588235294118,
            "recall": 0.9012605042016807
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 30132,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.7985889337348971,
            "auditor_fn_violation": 0.02120035968582545,
            "auditor_fp_violation": 0.011823914625272861,
            "ave_precision_score": 0.773693611490215,
            "fpr": 0.11842105263157894,
            "logloss": 1.8439417274626957,
            "mae": 0.27073570140767517,
            "precision": 0.7652173913043478,
            "recall": 0.7364016736401674
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.7918039047156783,
            "auditor_fn_violation": 0.012978627235746111,
            "auditor_fp_violation": 0.027298535145160688,
            "ave_precision_score": 0.7662679370361429,
            "fpr": 0.141602634467618,
            "logloss": 2.0393269541547014,
            "mae": 0.2730759612007052,
            "precision": 0.7425149700598802,
            "recall": 0.7815126050420168
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7884294809139409,
            "auditor_fn_violation": 0.013460691477648097,
            "auditor_fp_violation": 0.0007832080200501333,
            "ave_precision_score": 0.7662947563574765,
            "fpr": 0.21820175438596492,
            "logloss": 1.8585899381436575,
            "mae": 0.30667562630505446,
            "precision": 0.678513731825525,
            "recall": 0.8786610878661087
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7875398520137404,
            "auditor_fn_violation": 6.457028475495651e-05,
            "auditor_fp_violation": 0.01800471882609738,
            "ave_precision_score": 0.7649787800689256,
            "fpr": 0.2327113062568606,
            "logloss": 2.031605058678235,
            "mae": 0.31062163204122356,
            "precision": 0.6645569620253164,
            "recall": 0.8823529411764706
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8507485786439171,
            "auditor_fn_violation": 0.030077809586728333,
            "auditor_fp_violation": 0.010783005901851406,
            "ave_precision_score": 0.8509686086293164,
            "fpr": 0.1118421052631579,
            "logloss": 0.5138447350060644,
            "mae": 0.3101023711220286,
            "precision": 0.7796976241900648,
            "recall": 0.7552301255230126
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8426833736458399,
            "auditor_fn_violation": 0.019647815218293684,
            "auditor_fp_violation": 0.023414966501381584,
            "ave_precision_score": 0.8440538621830489,
            "fpr": 0.10867178924259056,
            "logloss": 0.5004661046769627,
            "mae": 0.30846943398827686,
            "precision": 0.7824175824175824,
            "recall": 0.7478991596638656
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7796052631578947,
            "auc_prc": 0.7989149173300394,
            "auditor_fn_violation": 0.02634102253541805,
            "auditor_fp_violation": 0.011601584606677982,
            "ave_precision_score": 0.7847247552205238,
            "fpr": 0.12280701754385964,
            "logloss": 1.3284496985621874,
            "mae": 0.27770462340722907,
            "precision": 0.7764471057884231,
            "recall": 0.8138075313807531
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.803189797560657,
            "auditor_fn_violation": 0.01542307373004087,
            "auditor_fp_violation": 0.02378338821807538,
            "ave_precision_score": 0.7924031803683365,
            "fpr": 0.15916575192096596,
            "logloss": 1.255575130622345,
            "mae": 0.285554858929699,
            "precision": 0.7339449541284404,
            "recall": 0.8403361344537815
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.7985116102562618,
            "auditor_fn_violation": 0.022296850913895626,
            "auditor_fp_violation": 0.012415110356536507,
            "ave_precision_score": 0.772987338436526,
            "fpr": 0.11732456140350878,
            "logloss": 1.832857719375834,
            "mae": 0.2728319364135874,
            "precision": 0.7668845315904139,
            "recall": 0.7364016736401674
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.7921577577379083,
            "auditor_fn_violation": 0.014392255255559965,
            "auditor_fp_violation": 0.02421741928157765,
            "ave_precision_score": 0.7666084727960416,
            "fpr": 0.1394072447859495,
            "logloss": 2.0225822491774346,
            "mae": 0.2757977873177491,
            "precision": 0.7439516129032258,
            "recall": 0.7752100840336135
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 30132,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.7987126357230702,
            "auditor_fn_violation": 0.024551769067019016,
            "auditor_fp_violation": 0.011283248443689872,
            "ave_precision_score": 0.7738129694974584,
            "fpr": 0.13048245614035087,
            "logloss": 1.7998111322974963,
            "mae": 0.27441218645942983,
            "precision": 0.7561475409836066,
            "recall": 0.7719665271966527
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.7926937004770076,
            "auditor_fn_violation": 0.011380512688060954,
            "auditor_fp_violation": 0.02568605927552141,
            "ave_precision_score": 0.7678595421137963,
            "fpr": 0.15148188803512624,
            "logloss": 1.9842484069406305,
            "mae": 0.27856160329268825,
            "precision": 0.7361376673040153,
            "recall": 0.8088235294117647
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 30132,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7741630013929792,
            "auditor_fn_violation": 0.003110548337370622,
            "auditor_fp_violation": 0.010585940658096874,
            "ave_precision_score": 0.7515217105355226,
            "fpr": 0.23903508771929824,
            "logloss": 2.0052842046277006,
            "mae": 0.3233954676405502,
            "precision": 0.659375,
            "recall": 0.8828451882845189
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7740040693866759,
            "auditor_fn_violation": 0.004794343643055467,
            "auditor_fp_violation": 0.021118639363084658,
            "ave_precision_score": 0.7515504833904572,
            "fpr": 0.25905598243688255,
            "logloss": 2.177832459394597,
            "mae": 0.3326111579852886,
            "precision": 0.6440422322775264,
            "recall": 0.8970588235294118
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.7952903874824769,
            "auditor_fn_violation": 0.0021631615650003705,
            "auditor_fp_violation": 0.0226018675721562,
            "ave_precision_score": 0.7810901791407479,
            "fpr": 0.17105263157894737,
            "logloss": 1.409566854485215,
            "mae": 0.29043949125595026,
            "precision": 0.7267950963222417,
            "recall": 0.8682008368200836
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8020935662150628,
            "auditor_fn_violation": 0.009238162883155458,
            "auditor_fp_violation": 0.023278700934933193,
            "ave_precision_score": 0.7921662334211043,
            "fpr": 0.19758507135016465,
            "logloss": 1.3770066357575137,
            "mae": 0.30163352630478374,
            "precision": 0.6954314720812182,
            "recall": 0.8634453781512605
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7882411806752242,
            "auditor_fn_violation": 0.004578653747339059,
            "auditor_fp_violation": 0.013281692133559717,
            "ave_precision_score": 0.7671218807179607,
            "fpr": 0.2225877192982456,
            "logloss": 1.814113267572879,
            "mae": 0.308963181004449,
            "precision": 0.6782884310618067,
            "recall": 0.895397489539749
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7866265600832608,
            "auditor_fn_violation": 0.002587423553395014,
            "auditor_fp_violation": 0.022781583961038156,
            "ave_precision_score": 0.7679838025574094,
            "fpr": 0.24259055982436883,
            "logloss": 1.910204415337237,
            "mae": 0.31998111879123114,
            "precision": 0.6584234930448223,
            "recall": 0.8949579831932774
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 30132,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.8047600132874068,
            "auditor_fn_violation": 0.02407692872348235,
            "auditor_fp_violation": 0.0056946802490096225,
            "ave_precision_score": 0.7836079996195799,
            "fpr": 0.11513157894736842,
            "logloss": 1.6183757063278579,
            "mae": 0.2699353010351475,
            "precision": 0.779874213836478,
            "recall": 0.7782426778242678
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8019473325631452,
            "auditor_fn_violation": 0.008631663422778555,
            "auditor_fp_violation": 0.018915679372169023,
            "ave_precision_score": 0.7855228497641777,
            "fpr": 0.145993413830955,
            "logloss": 1.6293515480295127,
            "mae": 0.27490033050130563,
            "precision": 0.7412451361867705,
            "recall": 0.8004201680672269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7867767547014455,
            "auditor_fn_violation": 0.0034729868604565805,
            "auditor_fp_violation": 0.006119128466327107,
            "ave_precision_score": 0.7648600113360497,
            "fpr": 0.2149122807017544,
            "logloss": 1.8405634900347294,
            "mae": 0.3077060260279962,
            "precision": 0.6813008130081301,
            "recall": 0.8765690376569037
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7846168802969025,
            "auditor_fn_violation": 0.004575265891208294,
            "auditor_fp_violation": 0.018933343427078995,
            "ave_precision_score": 0.7636236378172798,
            "fpr": 0.23710208562019758,
            "logloss": 1.986379045357716,
            "mae": 0.3178313674552685,
            "precision": 0.6593059936908517,
            "recall": 0.8781512605042017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7817470098203076,
            "auditor_fn_violation": 0.0010001468105409982,
            "auditor_fp_violation": 0.005618885924488644,
            "ave_precision_score": 0.7558481810716166,
            "fpr": 0.20285087719298245,
            "logloss": 2.03888273714171,
            "mae": 0.305781957304581,
            "precision": 0.6848381601362862,
            "recall": 0.8410041841004184
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7789913127590994,
            "auditor_fn_violation": 0.0024052431071221033,
            "auditor_fp_violation": 0.026051957555799483,
            "ave_precision_score": 0.7504046860630026,
            "fpr": 0.22283205268935236,
            "logloss": 2.3179782760067376,
            "mae": 0.312060511140333,
            "precision": 0.6693811074918566,
            "recall": 0.8634453781512605
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7884086577531434,
            "auditor_fn_violation": 0.00659729868604566,
            "auditor_fp_violation": 0.0023445711051823075,
            "ave_precision_score": 0.7664344083350778,
            "fpr": 0.20833333333333334,
            "logloss": 1.820839941943203,
            "mae": 0.30418418268974373,
            "precision": 0.6864686468646864,
            "recall": 0.8702928870292888
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7869901694679816,
            "auditor_fn_violation": 0.007575478050715346,
            "auditor_fp_violation": 0.021368459568240024,
            "ave_precision_score": 0.7645810637556762,
            "fpr": 0.22941822173435786,
            "logloss": 1.9925584742475801,
            "mae": 0.3092451875016776,
            "precision": 0.6671974522292994,
            "recall": 0.8802521008403361
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.7874358257103957,
            "auditor_fn_violation": 0.0017387873449313635,
            "auditor_fp_violation": 0.020800489126040905,
            "ave_precision_score": 0.7661579228602489,
            "fpr": 0.18530701754385964,
            "logloss": 1.787171847550819,
            "mae": 0.2919352529588331,
            "precision": 0.7091222030981067,
            "recall": 0.8619246861924686
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7823151870845136,
            "auditor_fn_violation": 0.007690782130634916,
            "auditor_fp_violation": 0.03057143217633773,
            "ave_precision_score": 0.7634154623116427,
            "fpr": 0.21953896816684962,
            "logloss": 1.9338281681737253,
            "mae": 0.30899231345712236,
            "precision": 0.6726677577741408,
            "recall": 0.8634453781512605
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.7883510832012666,
            "auditor_fn_violation": 0.01082957131322029,
            "auditor_fp_violation": 0.010727423397202692,
            "ave_precision_score": 0.7633178621164206,
            "fpr": 0.19298245614035087,
            "logloss": 1.930091943164589,
            "mae": 0.2979977017387507,
            "precision": 0.6981132075471698,
            "recall": 0.8514644351464435
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7864016575114086,
            "auditor_fn_violation": 0.002057024785765023,
            "auditor_fp_violation": 0.02160566259131686,
            "ave_precision_score": 0.7599590892692609,
            "fpr": 0.21295279912184412,
            "logloss": 2.1644874961679235,
            "mae": 0.3011981300469046,
            "precision": 0.6830065359477124,
            "recall": 0.8781512605042017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8054591848560371,
            "auditor_fn_violation": 0.01907848858548044,
            "auditor_fp_violation": 0.005805845258307061,
            "ave_precision_score": 0.7843098686312577,
            "fpr": 0.1118421052631579,
            "logloss": 1.6144854236939343,
            "mae": 0.2686065053645464,
            "precision": 0.7825159914712153,
            "recall": 0.7677824267782427
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8013076424598484,
            "auditor_fn_violation": 0.008237323469453646,
            "auditor_fp_violation": 0.021476967334115606,
            "ave_precision_score": 0.7849542404794605,
            "fpr": 0.13830954994511527,
            "logloss": 1.6503217147710414,
            "mae": 0.2753684673766108,
            "precision": 0.7504950495049505,
            "recall": 0.7962184873949579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 30132,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.7989951350897518,
            "auditor_fn_violation": 0.024512772517066725,
            "auditor_fp_violation": 0.011566213921901528,
            "ave_precision_score": 0.773430626720681,
            "fpr": 0.12280701754385964,
            "logloss": 1.8278295192986609,
            "mae": 0.27213394619442416,
            "precision": 0.7637130801687764,
            "recall": 0.7573221757322176
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.7928457202701408,
            "auditor_fn_violation": 0.012886383971810463,
            "auditor_fp_violation": 0.02286738079917231,
            "ave_precision_score": 0.7672513226295872,
            "fpr": 0.1437980241492865,
            "logloss": 2.0234042009099587,
            "mae": 0.275450113130098,
            "precision": 0.7436399217221135,
            "recall": 0.7983193277310925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8024360453421299,
            "auditor_fn_violation": 0.02502660941055568,
            "auditor_fp_violation": 0.01125798366884955,
            "ave_precision_score": 0.7788722506017722,
            "fpr": 0.11951754385964912,
            "logloss": 1.738942605816054,
            "mae": 0.2721925024241165,
            "precision": 0.7665952890792291,
            "recall": 0.7489539748953975
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.7960822779913863,
            "auditor_fn_violation": 0.015132507448643567,
            "auditor_fp_violation": 0.021986701490089203,
            "ave_precision_score": 0.7726526473751965,
            "fpr": 0.132821075740944,
            "logloss": 1.9104892076041127,
            "mae": 0.2746505846358753,
            "precision": 0.7555555555555555,
            "recall": 0.7857142857142857
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7858878661480612,
            "auditor_fn_violation": 0.009377523306173387,
            "auditor_fp_violation": 0.007576905974613954,
            "ave_precision_score": 0.760806124262374,
            "fpr": 0.20065789473684212,
            "logloss": 1.9595740390548346,
            "mae": 0.301873158283885,
            "precision": 0.6908783783783784,
            "recall": 0.8556485355648535
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7832887430693687,
            "auditor_fn_violation": 0.0031155162394266175,
            "auditor_fp_violation": 0.016402336702120945,
            "ave_precision_score": 0.7554964567906348,
            "fpr": 0.21844127332601537,
            "logloss": 2.231872482874859,
            "mae": 0.30475019816950455,
            "precision": 0.676948051948052,
            "recall": 0.8760504201680672
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8098580516139178,
            "auditor_fn_violation": 0.0019957057916758433,
            "auditor_fp_violation": 0.01881720430107528,
            "ave_precision_score": 0.8103430146770182,
            "fpr": 0.15570175438596492,
            "logloss": 0.7377375938689628,
            "mae": 0.28720225167885555,
            "precision": 0.73992673992674,
            "recall": 0.8451882845188284
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8259597721897868,
            "auditor_fn_violation": 0.012074643249176733,
            "auditor_fp_violation": 0.018663335730597936,
            "ave_precision_score": 0.8262024183298338,
            "fpr": 0.1734357848518112,
            "logloss": 0.7609155474662446,
            "mae": 0.29385012081741435,
            "precision": 0.7208480565371025,
            "recall": 0.8571428571428571
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.8173245971769636,
            "auditor_fn_violation": 0.012169217499816488,
            "auditor_fp_violation": 0.020977342549923197,
            "ave_precision_score": 0.8178032972746101,
            "fpr": 0.19407894736842105,
            "logloss": 0.855199161654152,
            "mae": 0.29034083953537404,
            "precision": 0.6905594405594405,
            "recall": 0.8263598326359832
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.8028728201555022,
            "auditor_fn_violation": 0.011701058030237344,
            "auditor_fp_violation": 0.025814754532722663,
            "ave_precision_score": 0.8036622488388367,
            "fpr": 0.21405049396267836,
            "logloss": 0.9099491859417894,
            "mae": 0.2958400789419643,
            "precision": 0.6760797342192691,
            "recall": 0.8550420168067226
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7892482137576227,
            "auditor_fn_violation": 0.012891800631285328,
            "auditor_fp_violation": 0.00021727706362681311,
            "ave_precision_score": 0.767118167262129,
            "fpr": 0.21271929824561403,
            "logloss": 1.844080919756941,
            "mae": 0.3058832619898838,
            "precision": 0.6830065359477124,
            "recall": 0.8744769874476988
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.787704765239575,
            "auditor_fn_violation": 0.0010976948408342481,
            "auditor_fp_violation": 0.025887934188778283,
            "ave_precision_score": 0.7651732960015485,
            "fpr": 0.22941822173435786,
            "logloss": 2.0200072435752676,
            "mae": 0.30975683966654627,
            "precision": 0.6677265500794912,
            "recall": 0.8823529411764706
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 30132,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.8047546996768606,
            "auditor_fn_violation": 0.02407692872348235,
            "auditor_fp_violation": 0.0056946802490096225,
            "ave_precision_score": 0.783607909916328,
            "fpr": 0.11513157894736842,
            "logloss": 1.6184311062809003,
            "mae": 0.2699466337632354,
            "precision": 0.779874213836478,
            "recall": 0.7782426778242678
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8020071214710454,
            "auditor_fn_violation": 0.008112795063140514,
            "auditor_fp_violation": 0.018915679372169023,
            "ave_precision_score": 0.7855885187349675,
            "fpr": 0.145993413830955,
            "logloss": 1.6292947450775086,
            "mae": 0.27485043601336656,
            "precision": 0.7417475728155339,
            "recall": 0.8025210084033614
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7885148680207414,
            "auditor_fn_violation": 0.011859539014901272,
            "auditor_fp_violation": 0.012609649122807029,
            "ave_precision_score": 0.7635576094188827,
            "fpr": 0.2138157894736842,
            "logloss": 1.945491202070251,
            "mae": 0.3034891719626581,
            "precision": 0.6808510638297872,
            "recall": 0.8702928870292888
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7871586287684035,
            "auditor_fn_violation": 0.0008002103146417749,
            "auditor_fp_violation": 0.018332765560139807,
            "ave_precision_score": 0.7613986945487206,
            "fpr": 0.23819978046103182,
            "logloss": 2.1625788872983995,
            "mae": 0.30811523508625416,
            "precision": 0.6609375,
            "recall": 0.8886554621848739
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7312623519957893,
            "auditor_fn_violation": 0.0021058136974234777,
            "auditor_fp_violation": 0.019709050852938795,
            "ave_precision_score": 0.6846696325691435,
            "fpr": 0.24890350877192982,
            "logloss": 3.413809804155972,
            "mae": 0.3345234604162295,
            "precision": 0.6565809379727685,
            "recall": 0.9079497907949791
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.7194978160788816,
            "auditor_fn_violation": 0.007499377357968435,
            "auditor_fp_violation": 0.023594130486897073,
            "ave_precision_score": 0.6674054405394148,
            "fpr": 0.27771679473106475,
            "logloss": 3.9033052987498045,
            "mae": 0.3444581990565749,
            "precision": 0.634393063583815,
            "recall": 0.9222689075630253
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7957174590076732,
            "auditor_fn_violation": 0.02434761065844528,
            "auditor_fp_violation": 0.009974533106960956,
            "ave_precision_score": 0.769951925006455,
            "fpr": 0.1425438596491228,
            "logloss": 1.8703074509251927,
            "mae": 0.27967082534458215,
            "precision": 0.7430830039525692,
            "recall": 0.7866108786610879
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.7910926390465401,
            "auditor_fn_violation": 0.011235229547362302,
            "auditor_fp_violation": 0.027808269301134288,
            "ave_precision_score": 0.7645626859403392,
            "fpr": 0.15916575192096596,
            "logloss": 2.1081198727504002,
            "mae": 0.2806900693016136,
            "precision": 0.7279549718574109,
            "recall": 0.8151260504201681
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7824747558510559,
            "auditor_fn_violation": 0.007023966820817735,
            "auditor_fp_violation": 0.005977645727221288,
            "ave_precision_score": 0.7603110058839334,
            "fpr": 0.24561403508771928,
            "logloss": 1.9854201010220023,
            "mae": 0.3230850582818803,
            "precision": 0.6574923547400612,
            "recall": 0.899581589958159
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7806693586558172,
            "auditor_fn_violation": 0.001355975979854072,
            "auditor_fp_violation": 0.015342493407522381,
            "ave_precision_score": 0.7573835479188933,
            "fpr": 0.2689352360043908,
            "logloss": 2.194519443307219,
            "mae": 0.33151320315134314,
            "precision": 0.6391752577319587,
            "recall": 0.9117647058823529
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7882744219722647,
            "auditor_fn_violation": 0.008877449900902886,
            "auditor_fp_violation": 0.005391502950925706,
            "ave_precision_score": 0.7663869074946389,
            "fpr": 0.21271929824561403,
            "logloss": 1.8094958747789491,
            "mae": 0.30540259273440923,
            "precision": 0.6824877250409165,
            "recall": 0.8723849372384938
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7894136833746659,
            "auditor_fn_violation": 0.001134592146408511,
            "auditor_fp_violation": 0.019637382187062336,
            "ave_precision_score": 0.7684680983045213,
            "fpr": 0.2283205268935236,
            "logloss": 1.9320376879719499,
            "mae": 0.3094926700633659,
            "precision": 0.6693163751987281,
            "recall": 0.884453781512605
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7882603349161618,
            "auditor_fn_violation": 0.0062761506276150635,
            "auditor_fp_violation": 0.006447570539251367,
            "ave_precision_score": 0.7662784308589714,
            "fpr": 0.23355263157894737,
            "logloss": 1.86868209462036,
            "mae": 0.3131846027135269,
            "precision": 0.6656200941915228,
            "recall": 0.8870292887029289
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7877124718318969,
            "auditor_fn_violation": 0.0007840677434530346,
            "auditor_fp_violation": 0.014628360901876176,
            "ave_precision_score": 0.7660095291436989,
            "fpr": 0.2491767288693743,
            "logloss": 2.01884031073857,
            "mae": 0.3181693081435534,
            "precision": 0.654490106544901,
            "recall": 0.9033613445378151
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 30132,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.6967973885600299,
            "auditor_fn_violation": 0.011797603317918227,
            "auditor_fp_violation": 0.005517826825127346,
            "ave_precision_score": 0.6482326431107395,
            "fpr": 0.2916666666666667,
            "logloss": 3.875289948808439,
            "mae": 0.3555494318859194,
            "precision": 0.6216216216216216,
            "recall": 0.9142259414225942
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.7081502636427461,
            "auditor_fn_violation": 0.004487634790469428,
            "auditor_fp_violation": 0.007489559281830009,
            "ave_precision_score": 0.6606190844843549,
            "fpr": 0.3084522502744237,
            "logloss": 3.8301001297580592,
            "mae": 0.3565273267737335,
            "precision": 0.6150684931506849,
            "recall": 0.9432773109243697
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7819433874174874,
            "auditor_fn_violation": 0.007331351391029878,
            "auditor_fp_violation": 0.010045274476513866,
            "ave_precision_score": 0.7569304151547733,
            "fpr": 0.18421052631578946,
            "logloss": 1.937830247146888,
            "mae": 0.30253652129236763,
            "precision": 0.697841726618705,
            "recall": 0.8117154811715481
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7833095318524386,
            "auditor_fn_violation": 0.0028088073868405773,
            "auditor_fp_violation": 0.01778517985793054,
            "ave_precision_score": 0.7576117143342422,
            "fpr": 0.19538968166849616,
            "logloss": 2.131509618857375,
            "mae": 0.3009688693324303,
            "precision": 0.6915077989601387,
            "recall": 0.8382352941176471
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 30132,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8261256310343814,
            "auditor_fn_violation": 0.010405197093151287,
            "auditor_fp_violation": 0.011765805643140117,
            "ave_precision_score": 0.8265527819537327,
            "fpr": 0.1787280701754386,
            "logloss": 0.6841067037491897,
            "mae": 0.28858011100569375,
            "precision": 0.7084078711985689,
            "recall": 0.8284518828451883
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8104203980375585,
            "auditor_fn_violation": 0.00770231253862687,
            "auditor_fp_violation": 0.021769685958338066,
            "ave_precision_score": 0.8110456546394893,
            "fpr": 0.19978046103183314,
            "logloss": 0.7208875604417438,
            "mae": 0.29198202446189225,
            "precision": 0.6936026936026936,
            "recall": 0.865546218487395
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8130888613104897,
            "auditor_fn_violation": 0.003580800851501138,
            "auditor_fp_violation": 0.016937505052954965,
            "ave_precision_score": 0.8135684730506688,
            "fpr": 0.18201754385964913,
            "logloss": 0.7336709556554987,
            "mae": 0.2990381184499711,
            "precision": 0.7162393162393162,
            "recall": 0.8765690376569037
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8273556367689967,
            "auditor_fn_violation": 0.007056609691077308,
            "auditor_fp_violation": 0.019170546450155826,
            "ave_precision_score": 0.8276544743456056,
            "fpr": 0.19978046103183314,
            "logloss": 0.7664839406592154,
            "mae": 0.3085920849628951,
            "precision": 0.6981757877280266,
            "recall": 0.884453781512605
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7881405889503158,
            "auditor_fn_violation": 0.005936651251559864,
            "auditor_fp_violation": 0.0004598189020939494,
            "ave_precision_score": 0.7662258489338482,
            "fpr": 0.21052631578947367,
            "logloss": 1.8155543142541546,
            "mae": 0.3038301831369162,
            "precision": 0.6836902800658978,
            "recall": 0.8682008368200836
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.787443002549516,
            "auditor_fn_violation": 0.0074163584204263505,
            "auditor_fp_violation": 0.022774013651791002,
            "ave_precision_score": 0.7650238977406103,
            "fpr": 0.22502744237102085,
            "logloss": 1.982524615564575,
            "mae": 0.30770564580081405,
            "precision": 0.6704180064308681,
            "recall": 0.8760504201680672
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8047475512757771,
            "auditor_fn_violation": 0.02407692872348235,
            "auditor_fp_violation": 0.004871048589214977,
            "ave_precision_score": 0.7835955348693459,
            "fpr": 0.1162280701754386,
            "logloss": 1.6189924115487306,
            "mae": 0.26967764136347877,
            "precision": 0.7782426778242678,
            "recall": 0.7782426778242678
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8018236682984903,
            "auditor_fn_violation": 0.007531662500345912,
            "auditor_fp_violation": 0.018915679372169023,
            "ave_precision_score": 0.7853621728956424,
            "fpr": 0.145993413830955,
            "logloss": 1.6317531731490005,
            "mae": 0.2747417836151718,
            "precision": 0.7412451361867705,
            "recall": 0.8004201680672269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7972672501346855,
            "auditor_fn_violation": 0.0026769984584893193,
            "auditor_fp_violation": 0.025502263723825698,
            "ave_precision_score": 0.7743661449993474,
            "fpr": 0.20723684210526316,
            "logloss": 1.823078980669739,
            "mae": 0.2979702813647332,
            "precision": 0.6971153846153846,
            "recall": 0.9100418410041841
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7896413405552357,
            "auditor_fn_violation": 0.007566253724321782,
            "auditor_fp_violation": 0.02113377998157891,
            "ave_precision_score": 0.7676643524172045,
            "fpr": 0.23600439077936333,
            "logloss": 2.025629805660445,
            "mae": 0.3090426179288521,
            "precision": 0.6702453987730062,
            "recall": 0.9180672268907563
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7894773765632424,
            "auditor_fn_violation": 0.010620825075240405,
            "auditor_fp_violation": 0.007162563667232597,
            "ave_precision_score": 0.7675170961938651,
            "fpr": 0.2050438596491228,
            "logloss": 1.7970563961714792,
            "mae": 0.30243730189824036,
            "precision": 0.6893687707641196,
            "recall": 0.8682008368200836
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.787799704397018,
            "auditor_fn_violation": 0.008080509920763037,
            "auditor_fp_violation": 0.023268607189270353,
            "ave_precision_score": 0.7653820824259878,
            "fpr": 0.2217343578485181,
            "logloss": 1.9676936276191235,
            "mae": 0.3060689238703868,
            "precision": 0.6731391585760518,
            "recall": 0.8739495798319328
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8030246453060446,
            "auditor_fn_violation": 0.020420428686779717,
            "auditor_fp_violation": 0.013658137278680578,
            "ave_precision_score": 0.7801483870721665,
            "fpr": 0.12719298245614036,
            "logloss": 1.7186709289817896,
            "mae": 0.271067924076741,
            "precision": 0.7613168724279835,
            "recall": 0.7740585774058577
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.7962346275683311,
            "auditor_fn_violation": 0.011493510686382132,
            "auditor_fp_violation": 0.020931905068322043,
            "ave_precision_score": 0.7735202208679339,
            "fpr": 0.14709110867178923,
            "logloss": 1.8951945991396901,
            "mae": 0.27562073306032586,
            "precision": 0.7398058252427184,
            "recall": 0.8004201680672269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.7966805564847942,
            "auditor_fn_violation": 0.009767488805696249,
            "auditor_fp_violation": 0.020125919637804194,
            "ave_precision_score": 0.7759084497092512,
            "fpr": 0.13815789473684212,
            "logloss": 1.612091224894663,
            "mae": 0.2790884771827239,
            "precision": 0.75,
            "recall": 0.7907949790794979
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.7988720542092849,
            "auditor_fn_violation": 0.009007554723316333,
            "auditor_fp_violation": 0.017752375184526287,
            "ave_precision_score": 0.7836345851269367,
            "fpr": 0.15806805708013172,
            "logloss": 1.5741386924830068,
            "mae": 0.2834261127192482,
            "precision": 0.7323420074349443,
            "recall": 0.8277310924369747
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.799578069369649,
            "auditor_fn_violation": 0.02372366585920869,
            "auditor_fp_violation": 0.009292384186272134,
            "ave_precision_score": 0.7739883928895683,
            "fpr": 0.12609649122807018,
            "logloss": 1.8307465166570411,
            "mae": 0.2712924912695854,
            "precision": 0.760914760914761,
            "recall": 0.7656903765690377
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.7926316404012521,
            "auditor_fn_violation": 0.012503574426477511,
            "auditor_fp_violation": 0.02574662174949847,
            "ave_precision_score": 0.7670270510785062,
            "fpr": 0.14489571899012074,
            "logloss": 2.0334524130959966,
            "mae": 0.2754772847632408,
            "precision": 0.7431906614785992,
            "recall": 0.8025210084033614
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7879773334019384,
            "auditor_fn_violation": 0.015075607428613377,
            "auditor_fp_violation": 0.012223098067750017,
            "ave_precision_score": 0.7660862033891398,
            "fpr": 0.22697368421052633,
            "logloss": 1.8589078472928042,
            "mae": 0.3112824703479929,
            "precision": 0.670906200317965,
            "recall": 0.8828451882845189
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7881585675021204,
            "auditor_fn_violation": 0.00026519938381499707,
            "auditor_fp_violation": 0.02003860857716038,
            "ave_precision_score": 0.7664505184148184,
            "fpr": 0.24368825466520308,
            "logloss": 2.006780880976875,
            "mae": 0.3160795707200142,
            "precision": 0.6589861751152074,
            "recall": 0.9012605042016807
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 30132,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8133458706184875,
            "auditor_fn_violation": 0.009471573808999493,
            "auditor_fp_violation": 0.007096875252647752,
            "ave_precision_score": 0.8128527790144162,
            "fpr": 0.11732456140350878,
            "logloss": 0.8119432197256734,
            "mae": 0.27076552666915754,
            "precision": 0.7693965517241379,
            "recall": 0.7468619246861925
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8282684636644039,
            "auditor_fn_violation": 0.007070446180667658,
            "auditor_fp_violation": 0.015569602684936349,
            "ave_precision_score": 0.8285346364559898,
            "fpr": 0.12623490669593854,
            "logloss": 0.8245515430021062,
            "mae": 0.2677783519133582,
            "precision": 0.7653061224489796,
            "recall": 0.7878151260504201
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7876678266028703,
            "auditor_fn_violation": 0.01112548630991705,
            "auditor_fp_violation": 0.012061403508771934,
            "ave_precision_score": 0.7656263062357692,
            "fpr": 0.23026315789473684,
            "logloss": 1.8882274141922701,
            "mae": 0.31328481402791414,
            "precision": 0.668769716088328,
            "recall": 0.8870292887029289
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7870366380586324,
            "auditor_fn_violation": 0.002545914084623973,
            "auditor_fp_violation": 0.01623831333509975,
            "ave_precision_score": 0.7645290786846297,
            "fpr": 0.2524698133918771,
            "logloss": 2.061564870791218,
            "mae": 0.3184529657120526,
            "precision": 0.6515151515151515,
            "recall": 0.9033613445378151
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.7987180620552888,
            "auditor_fn_violation": 0.020074047566615287,
            "auditor_fp_violation": 0.01454745735306007,
            "ave_precision_score": 0.7738245284035403,
            "fpr": 0.11842105263157894,
            "logloss": 1.8437174501721374,
            "mae": 0.2700425234655987,
            "precision": 0.7667386609071274,
            "recall": 0.7426778242677824
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.7923587144457792,
            "auditor_fn_violation": 0.012937117766975072,
            "auditor_fp_violation": 0.026503652674211745,
            "ave_precision_score": 0.7667893396692212,
            "fpr": 0.1437980241492865,
            "logloss": 2.0425126399162137,
            "mae": 0.2729346301539698,
            "precision": 0.7405940594059406,
            "recall": 0.7857142857142857
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.7930232898667012,
            "auditor_fn_violation": 0.01998917272260149,
            "auditor_fp_violation": 0.012154883175681143,
            "ave_precision_score": 0.7680149645849242,
            "fpr": 0.13925438596491227,
            "logloss": 1.804873949183354,
            "mae": 0.27339974480227075,
            "precision": 0.7475149105367793,
            "recall": 0.7866108786610879
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.7941293559680715,
            "auditor_fn_violation": 0.011364370116872219,
            "auditor_fp_violation": 0.024149286498353458,
            "ave_precision_score": 0.7739100581909023,
            "fpr": 0.14818880351262348,
            "logloss": 1.8317851044828588,
            "mae": 0.27453763353307603,
            "precision": 0.7403846153846154,
            "recall": 0.8088235294117647
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.7885540054343301,
            "auditor_fn_violation": 0.01082957131322029,
            "auditor_fp_violation": 0.007165090144716636,
            "ave_precision_score": 0.76352919274453,
            "fpr": 0.19298245614035087,
            "logloss": 1.9249331855596061,
            "mae": 0.29717205548551745,
            "precision": 0.6981132075471698,
            "recall": 0.8514644351464435
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7864032903143271,
            "auditor_fn_violation": 0.004298536099401342,
            "auditor_fp_violation": 0.02129780334860013,
            "ave_precision_score": 0.759960519319403,
            "fpr": 0.20856201975850713,
            "logloss": 2.159882466498967,
            "mae": 0.3007278763505724,
            "precision": 0.6864686468646864,
            "recall": 0.8739495798319328
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7881033549467469,
            "auditor_fn_violation": 0.011861832929604348,
            "auditor_fp_violation": 0.001230394534723902,
            "ave_precision_score": 0.7661659051531551,
            "fpr": 0.2138157894736842,
            "logloss": 1.832984556333061,
            "mae": 0.3050513025602988,
            "precision": 0.680327868852459,
            "recall": 0.8682008368200836
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7871832813120537,
            "auditor_fn_violation": 0.0009685542713243363,
            "auditor_fp_violation": 0.02411900526136493,
            "ave_precision_score": 0.7647511931213511,
            "fpr": 0.22722283205268934,
            "logloss": 2.0040068688719397,
            "mae": 0.30875357637490103,
            "precision": 0.6698564593301436,
            "recall": 0.8823529411764706
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7865769086644167,
            "auditor_fn_violation": 0.014401196505909127,
            "auditor_fp_violation": 0.0012379739671760084,
            "ave_precision_score": 0.7646870684617342,
            "fpr": 0.21820175438596492,
            "logloss": 1.8394505558391547,
            "mae": 0.3084175825780748,
            "precision": 0.6774716369529984,
            "recall": 0.8744769874476988
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.787463138248021,
            "auditor_fn_violation": 0.008158916695108343,
            "auditor_fp_violation": 0.020023467958666122,
            "ave_precision_score": 0.7657760486902887,
            "fpr": 0.23710208562019758,
            "logloss": 1.977689408791367,
            "mae": 0.3127760922722314,
            "precision": 0.6614420062695925,
            "recall": 0.8865546218487395
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7896970799818963,
            "auditor_fn_violation": 0.006560596050796451,
            "auditor_fp_violation": 0.016518109790605542,
            "ave_precision_score": 0.7678317291375599,
            "fpr": 0.20723684210526316,
            "logloss": 1.8071734193704514,
            "mae": 0.3011789041136804,
            "precision": 0.6865671641791045,
            "recall": 0.8661087866108786
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7901796507906836,
            "auditor_fn_violation": 0.0016096449556771114,
            "auditor_fp_violation": 0.026940207174129732,
            "ave_precision_score": 0.769932271723589,
            "fpr": 0.2283205268935236,
            "logloss": 1.9168834056525488,
            "mae": 0.3076400541424144,
            "precision": 0.6672,
            "recall": 0.8760504201680672
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8009337096398612,
            "auditor_fn_violation": 0.01526370843426559,
            "auditor_fp_violation": 0.01795062252405207,
            "ave_precision_score": 0.7753874734770259,
            "fpr": 0.12609649122807018,
            "logloss": 1.7991431611318502,
            "mae": 0.2709137722939918,
            "precision": 0.7648261758691206,
            "recall": 0.7824267782426778
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.7920958565882659,
            "auditor_fn_violation": 0.015056406755896647,
            "auditor_fp_violation": 0.02586017638820546,
            "ave_precision_score": 0.7666264094374126,
            "fpr": 0.14050493962678376,
            "logloss": 1.9905433333237725,
            "mae": 0.2790087671813342,
            "precision": 0.7455268389662028,
            "recall": 0.7878151260504201
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 30132,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.7990635680753493,
            "auditor_fn_violation": 0.024290262790868385,
            "auditor_fp_violation": 0.011765805643140115,
            "ave_precision_score": 0.773457374330907,
            "fpr": 0.12390350877192982,
            "logloss": 1.832594372253039,
            "mae": 0.27233287372358683,
            "precision": 0.7626050420168067,
            "recall": 0.7594142259414226
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.793304029674728,
            "auditor_fn_violation": 0.012146131778726867,
            "auditor_fp_violation": 0.022612513721185508,
            "ave_precision_score": 0.7676684774935009,
            "fpr": 0.14489571899012074,
            "logloss": 2.030139357377476,
            "mae": 0.27516885024145316,
            "precision": 0.7426900584795322,
            "recall": 0.8004201680672269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 30132,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7859051596106921,
            "auditor_fn_violation": 0.010827277398517214,
            "auditor_fp_violation": 0.007576905974613954,
            "ave_precision_score": 0.7607826281585612,
            "fpr": 0.20065789473684212,
            "logloss": 1.9600827247197286,
            "mae": 0.30213048233979445,
            "precision": 0.6893039049235993,
            "recall": 0.8493723849372385
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7831398377714064,
            "auditor_fn_violation": 0.0031155162394266175,
            "auditor_fp_violation": 0.01684393807487037,
            "ave_precision_score": 0.7553518530705005,
            "fpr": 0.21734357848518113,
            "logloss": 2.2321103040303334,
            "mae": 0.30477161269575415,
            "precision": 0.6780487804878049,
            "recall": 0.8760504201680672
        }
    }
]
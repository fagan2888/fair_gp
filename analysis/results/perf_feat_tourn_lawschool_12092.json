[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5230263157894737,
            "auc_prc": 0.6208680774824946,
            "auditor_fn_violation": 0.051691729323308275,
            "auditor_fp_violation": 0.01667370835345245,
            "ave_precision_score": 0.5349006218723544,
            "fpr": 0.2050438596491228,
            "logloss": 14.269014003983965,
            "mae": 0.4746710772790185,
            "precision": 0.5493975903614458,
            "recall": 0.4789915966386555
        },
        "train": {
            "accuracy": 0.5236004390779363,
            "auc_prc": 0.6248686090052377,
            "auditor_fn_violation": 0.057332739322736075,
            "auditor_fp_violation": 0.005006806722049976,
            "ave_precision_score": 0.5329314436222454,
            "fpr": 0.21075740944017562,
            "logloss": 14.770590299519338,
            "mae": 0.4791043404074622,
            "precision": 0.5514018691588785,
            "recall": 0.49372384937238495
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5635964912280702,
            "auc_prc": 0.7084680854068401,
            "auditor_fn_violation": 0.021736326109391125,
            "auditor_fp_violation": 0.020003420247867373,
            "ave_precision_score": 0.5718113453994552,
            "fpr": 0.3574561403508772,
            "logloss": 10.161026621552121,
            "mae": 0.43210511375196714,
            "precision": 0.5534246575342465,
            "recall": 0.8487394957983193
        },
        "train": {
            "accuracy": 0.5982436882546652,
            "auc_prc": 0.7253161794743316,
            "auditor_fn_violation": 0.017691717685746962,
            "auditor_fp_violation": 0.019299655480995683,
            "ave_precision_score": 0.5934950294966074,
            "fpr": 0.32821075740944017,
            "logloss": 9.328109258246437,
            "mae": 0.3972250430257948,
            "precision": 0.5788732394366197,
            "recall": 0.8598326359832636
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 12092,
        "test": {
            "accuracy": 0.506578947368421,
            "auc_prc": 0.5359867669634315,
            "auditor_fn_violation": 0.07636278195488723,
            "auditor_fp_violation": 0.024344117173668127,
            "ave_precision_score": 0.5042226378084047,
            "fpr": 0.2675438596491228,
            "logloss": 6.312270395381182,
            "mae": 0.49629151728010823,
            "precision": 0.5252918287937743,
            "recall": 0.5672268907563025
        },
        "train": {
            "accuracy": 0.5137211855104281,
            "auc_prc": 0.5509158771277693,
            "auditor_fn_violation": 0.08362230111744416,
            "auditor_fp_violation": 0.0039699540894836734,
            "ave_precision_score": 0.5247965823760418,
            "fpr": 0.2261251372118551,
            "logloss": 5.821916464868378,
            "mae": 0.48742903769113977,
            "precision": 0.5391498881431768,
            "recall": 0.50418410041841
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7787304695919411,
            "auditor_fn_violation": 0.016447368421052634,
            "auditor_fp_violation": 0.01593181635280863,
            "ave_precision_score": 0.7778843682872626,
            "fpr": 0.15021929824561403,
            "logloss": 1.920380352123548,
            "mae": 0.2962347333789327,
            "precision": 0.7226720647773279,
            "recall": 0.75
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7790001579824156,
            "auditor_fn_violation": 0.016515944132384754,
            "auditor_fp_violation": 0.02340650453908225,
            "ave_precision_score": 0.7772104921281,
            "fpr": 0.14050493962678376,
            "logloss": 1.9278374047544369,
            "mae": 0.28130031403241407,
            "precision": 0.7382413087934561,
            "recall": 0.7552301255230126
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.6955843789679743,
            "auditor_fn_violation": 0.057252506265664166,
            "auditor_fp_violation": 0.02189461612747465,
            "ave_precision_score": 0.683254775879189,
            "fpr": 0.15460526315789475,
            "logloss": 3.4322564848886916,
            "mae": 0.36376203168930227,
            "precision": 0.6697892271662763,
            "recall": 0.6008403361344538
        },
        "train": {
            "accuracy": 0.6410537870472008,
            "auc_prc": 0.692123132437588,
            "auditor_fn_violation": 0.05480666332918445,
            "auditor_fp_violation": 0.009374770257286502,
            "ave_precision_score": 0.6806096384163939,
            "fpr": 0.12623490669593854,
            "logloss": 3.5080373595841663,
            "mae": 0.3576809569947697,
            "precision": 0.6981627296587927,
            "recall": 0.5564853556485355
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.5495524329036949,
            "auditor_fn_violation": 0.09038220551378447,
            "auditor_fp_violation": 0.020471189441493638,
            "ave_precision_score": 0.5245622528760641,
            "fpr": 0.22587719298245615,
            "logloss": 5.349213619993842,
            "mae": 0.49348014207972757,
            "precision": 0.5339366515837104,
            "recall": 0.4957983193277311
        },
        "train": {
            "accuracy": 0.5236004390779363,
            "auc_prc": 0.5736628794423616,
            "auditor_fn_violation": 0.09422722742491814,
            "auditor_fp_violation": 0.011889581532361722,
            "ave_precision_score": 0.5539991253702179,
            "fpr": 0.1877058177826564,
            "logloss": 4.953352276982662,
            "mae": 0.4773018395209197,
            "precision": 0.5569948186528497,
            "recall": 0.4497907949790795
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5131578947368421,
            "auc_prc": 0.6449826694181879,
            "auditor_fn_violation": 0.053375626566416055,
            "auditor_fp_violation": 0.03354860775792693,
            "ave_precision_score": 0.5408998047720994,
            "fpr": 0.21600877192982457,
            "logloss": 15.551523508084307,
            "mae": 0.4871800308501619,
            "precision": 0.5375586854460094,
            "recall": 0.4810924369747899
        },
        "train": {
            "accuracy": 0.5225027442371021,
            "auc_prc": 0.6527217073044496,
            "auditor_fn_violation": 0.05369978275746456,
            "auditor_fp_violation": 0.03212215087346596,
            "ave_precision_score": 0.5515727662938126,
            "fpr": 0.20197585071350166,
            "logloss": 15.369903840405701,
            "mae": 0.4775911805499845,
            "precision": 0.5523114355231143,
            "recall": 0.47489539748953974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.7729619362982184,
            "auditor_fn_violation": 0.012664934394810557,
            "auditor_fp_violation": 0.036113793658458085,
            "ave_precision_score": 0.6173642216511483,
            "fpr": 0.2807017543859649,
            "logloss": 11.028244161658526,
            "mae": 0.3471475799181452,
            "precision": 0.6184798807749627,
            "recall": 0.8718487394957983
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.7886780985194066,
            "auditor_fn_violation": 0.0055573671858135595,
            "auditor_fp_violation": 0.03146556204257434,
            "ave_precision_score": 0.6360542148880542,
            "fpr": 0.2711306256860593,
            "logloss": 10.256087941446612,
            "mae": 0.3241109860694839,
            "precision": 0.6346153846153846,
            "recall": 0.897489539748954
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6030701754385965,
            "auc_prc": 0.6720999259214149,
            "auditor_fn_violation": 0.07005795739348372,
            "auditor_fp_violation": 0.025531144374698212,
            "ave_precision_score": 0.6529768359748445,
            "fpr": 0.16337719298245615,
            "logloss": 4.4939963129193545,
            "mae": 0.39364559403988,
            "precision": 0.6383495145631068,
            "recall": 0.5525210084033614
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.6720903468480223,
            "auditor_fn_violation": 0.07109755705487097,
            "auditor_fp_violation": 0.016587107029049617,
            "ave_precision_score": 0.6551563928257426,
            "fpr": 0.132821075740944,
            "logloss": 4.568507854076139,
            "mae": 0.3904463555426218,
            "precision": 0.6666666666666666,
            "recall": 0.5062761506276151
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7740750033270662,
            "auditor_fn_violation": 0.013508034792864514,
            "auditor_fp_violation": 0.03433325285691293,
            "ave_precision_score": 0.7704494437977696,
            "fpr": 0.2149122807017544,
            "logloss": 2.1441151511555887,
            "mae": 0.295977212888185,
            "precision": 0.679214402618658,
            "recall": 0.8718487394957983
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7760176934713165,
            "auditor_fn_violation": 0.016263336533029596,
            "auditor_fp_violation": 0.02474756821298829,
            "ave_precision_score": 0.7728096800580456,
            "fpr": 0.20965971459934138,
            "logloss": 2.014530971145885,
            "mae": 0.28833192629950516,
            "precision": 0.6795302013422819,
            "recall": 0.8472803347280334
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7312948525635634,
            "auditor_fn_violation": 0.040825869821612855,
            "auditor_fp_violation": 0.022100836954772253,
            "ave_precision_score": 0.7268756894642396,
            "fpr": 0.15021929824561403,
            "logloss": 2.61394689335907,
            "mae": 0.327767708346712,
            "precision": 0.6995614035087719,
            "recall": 0.6701680672268907
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.7286057294836301,
            "auditor_fn_violation": 0.03925062807434931,
            "auditor_fp_violation": 0.013139381893865841,
            "ave_precision_score": 0.7266997595855392,
            "fpr": 0.1251372118551043,
            "logloss": 2.590493881275326,
            "mae": 0.3195013598374034,
            "precision": 0.7266187050359713,
            "recall": 0.6338912133891214
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.770939765146022,
            "auditor_fn_violation": 0.013632426654872486,
            "auditor_fp_violation": 0.01750865121519395,
            "ave_precision_score": 0.7677139006767595,
            "fpr": 0.1600877192982456,
            "logloss": 2.0545714765510072,
            "mae": 0.3017370993032466,
            "precision": 0.7108910891089109,
            "recall": 0.7542016806722689
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7694454537366876,
            "auditor_fn_violation": 0.01822678650983562,
            "auditor_fp_violation": 0.023548469691707465,
            "ave_precision_score": 0.7663392150700422,
            "fpr": 0.145993413830955,
            "logloss": 2.0071832506691174,
            "mae": 0.2881673684891992,
            "precision": 0.7318548387096774,
            "recall": 0.7594142259414226
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.714676356682639,
            "auditor_fn_violation": 0.04757988721804512,
            "auditor_fp_violation": 0.019412421535490103,
            "ave_precision_score": 0.7071954576515556,
            "fpr": 0.15460526315789475,
            "logloss": 2.89705053223646,
            "mae": 0.34523014188665035,
            "precision": 0.6838565022421524,
            "recall": 0.6407563025210085
        },
        "train": {
            "accuracy": 0.6641053787047201,
            "auc_prc": 0.7099439372202878,
            "auditor_fn_violation": 0.04619963348933767,
            "auditor_fp_violation": 0.009838692095329608,
            "ave_precision_score": 0.704082959436255,
            "fpr": 0.12952799121844127,
            "logloss": 2.9035741471925753,
            "mae": 0.33711072776516593,
            "precision": 0.7107843137254902,
            "recall": 0.606694560669456
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.6630917138540668,
            "auditor_fn_violation": 0.084312343358396,
            "auditor_fp_violation": 0.02718091099307903,
            "ave_precision_score": 0.6560356005264627,
            "fpr": 0.12938596491228072,
            "logloss": 3.3397836635025806,
            "mae": 0.41501210302247254,
            "precision": 0.6369230769230769,
            "recall": 0.43487394957983194
        },
        "train": {
            "accuracy": 0.5938529088913282,
            "auc_prc": 0.6856538382119021,
            "auditor_fn_violation": 0.0832181289584759,
            "auditor_fp_violation": 0.019946103943842646,
            "ave_precision_score": 0.6791098491133271,
            "fpr": 0.09879253567508232,
            "logloss": 3.4847396829047,
            "mae": 0.4058168362018753,
            "precision": 0.6875,
            "recall": 0.41422594142259417
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.6769980345885019,
            "auditor_fn_violation": 0.06947055137844611,
            "auditor_fp_violation": 0.024583031546756808,
            "ave_precision_score": 0.659067754593943,
            "fpr": 0.15679824561403508,
            "logloss": 4.432900086310654,
            "mae": 0.39142037319224815,
            "precision": 0.6469135802469136,
            "recall": 0.5504201680672269
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.6770226957258858,
            "auditor_fn_violation": 0.06909966058724377,
            "auditor_fp_violation": 0.015826579425700257,
            "ave_precision_score": 0.6607102596386151,
            "fpr": 0.12843029637760703,
            "logloss": 4.541396363000348,
            "mae": 0.38716957660910467,
            "precision": 0.6759002770083102,
            "recall": 0.5104602510460251
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7708333333333334,
            "auc_prc": 0.8259899833589088,
            "auditor_fn_violation": 0.021810039805395847,
            "auditor_fp_violation": 0.020511427651698052,
            "ave_precision_score": 0.8267760009459775,
            "fpr": 0.09539473684210527,
            "logloss": 0.5488874510472009,
            "mae": 0.3087123912961001,
            "precision": 0.8027210884353742,
            "recall": 0.7436974789915967
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.8735292938766225,
            "auditor_fn_violation": 0.015023262863467892,
            "auditor_fp_violation": 0.005620298988751797,
            "ave_precision_score": 0.873739848992086,
            "fpr": 0.0845225027442371,
            "logloss": 0.4699971717569906,
            "mae": 0.28702889524294684,
            "precision": 0.8246013667425968,
            "recall": 0.7573221757322176
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6008771929824561,
            "auc_prc": 0.6693634117331312,
            "auditor_fn_violation": 0.07209429824561404,
            "auditor_fp_violation": 0.025460727506840503,
            "ave_precision_score": 0.6506096460881821,
            "fpr": 0.1611842105263158,
            "logloss": 4.505819605873448,
            "mae": 0.39638821261917534,
            "precision": 0.6379310344827587,
            "recall": 0.5441176470588235
        },
        "train": {
            "accuracy": 0.6004390779363337,
            "auc_prc": 0.6691517674740969,
            "auditor_fn_violation": 0.07424826274864627,
            "auditor_fp_violation": 0.016280360895698714,
            "ave_precision_score": 0.6523250007727833,
            "fpr": 0.132821075740944,
            "logloss": 4.617395046205716,
            "mae": 0.3979852968013245,
            "precision": 0.6601123595505618,
            "recall": 0.4916317991631799
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6019736842105263,
            "auc_prc": 0.6611416602310045,
            "auditor_fn_violation": 0.07072368421052631,
            "auditor_fp_violation": 0.02424855142443265,
            "ave_precision_score": 0.6431942577669345,
            "fpr": 0.16776315789473684,
            "logloss": 4.297911473419185,
            "mae": 0.39530359965342815,
            "precision": 0.6348448687350835,
            "recall": 0.5588235294117647
        },
        "train": {
            "accuracy": 0.6026344676180022,
            "auc_prc": 0.6686361042967129,
            "auditor_fn_violation": 0.07154765786826743,
            "auditor_fp_violation": 0.01424214691872242,
            "ave_precision_score": 0.6517969254024258,
            "fpr": 0.13721185510428102,
            "logloss": 4.319087020044,
            "mae": 0.39714643877981115,
            "precision": 0.6584699453551912,
            "recall": 0.50418410041841
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6206140350877193,
            "auc_prc": 0.6958230670663546,
            "auditor_fn_violation": 0.06328320802005014,
            "auditor_fp_violation": 0.02594358602929342,
            "ave_precision_score": 0.6963100124241824,
            "fpr": 0.125,
            "logloss": 2.4178247074443306,
            "mae": 0.36993393737920444,
            "precision": 0.6815642458100558,
            "recall": 0.5126050420168067
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.7217442471081479,
            "auditor_fn_violation": 0.06687212084747556,
            "auditor_fp_violation": 0.017621424569604752,
            "ave_precision_score": 0.7224558763348854,
            "fpr": 0.09659714599341383,
            "logloss": 2.518445423834096,
            "mae": 0.3636102459083891,
            "precision": 0.7206349206349206,
            "recall": 0.47489539748953974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.6794877561426922,
            "auditor_fn_violation": 0.06300908521303257,
            "auditor_fp_violation": 0.026114598422662158,
            "ave_precision_score": 0.6641997925497671,
            "fpr": 0.16447368421052633,
            "logloss": 3.654993568076399,
            "mae": 0.37586017628079116,
            "precision": 0.6503496503496503,
            "recall": 0.5861344537815126
        },
        "train": {
            "accuracy": 0.6333699231613611,
            "auc_prc": 0.6839091700041272,
            "auditor_fn_violation": 0.06049263074739699,
            "auditor_fp_violation": 0.016313317091843853,
            "ave_precision_score": 0.6697571879004927,
            "fpr": 0.13611416026344675,
            "logloss": 3.6635631307970433,
            "mae": 0.3667422883758794,
            "precision": 0.6836734693877551,
            "recall": 0.5606694560669456
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 12092,
        "test": {
            "accuracy": 0.569078947368421,
            "auc_prc": 0.6441036344743777,
            "auditor_fn_violation": 0.08791510025062658,
            "auditor_fp_violation": 0.02548084661194271,
            "ave_precision_score": 0.6324437648242337,
            "fpr": 0.14473684210526316,
            "logloss": 4.444870059592953,
            "mae": 0.4309149381106042,
            "precision": 0.6195965417867435,
            "recall": 0.45168067226890757
        },
        "train": {
            "accuracy": 0.5729967069154775,
            "auc_prc": 0.6663665494139839,
            "auditor_fn_violation": 0.08566612623949957,
            "auditor_fp_violation": 0.018098021867703686,
            "ave_precision_score": 0.6544285774122116,
            "fpr": 0.1163556531284303,
            "logloss": 4.615194380263515,
            "mae": 0.4237430529089046,
            "precision": 0.6478405315614618,
            "recall": 0.40794979079497906
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.679787024693879,
            "auditor_fn_violation": 0.060581140350877215,
            "auditor_fp_violation": 0.02723120875583454,
            "ave_precision_score": 0.6800598451738897,
            "fpr": 0.13596491228070176,
            "logloss": 2.2566953640895973,
            "mae": 0.3600458859389256,
            "precision": 0.6876574307304786,
            "recall": 0.5735294117647058
        },
        "train": {
            "accuracy": 0.6377607025246982,
            "auc_prc": 0.7062849139102547,
            "auditor_fn_violation": 0.06555396846538589,
            "auditor_fp_violation": 0.016102904454917192,
            "ave_precision_score": 0.7069172448928489,
            "fpr": 0.10867178924259056,
            "logloss": 2.2869077425424917,
            "mae": 0.3578484636900363,
            "precision": 0.7138728323699421,
            "recall": 0.5167364016736402
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.66648895726548,
            "auditor_fn_violation": 0.01187251216275984,
            "auditor_fp_violation": 0.011166103331723817,
            "ave_precision_score": 0.6535656579744917,
            "fpr": 0.22807017543859648,
            "logloss": 2.1752256229696645,
            "mae": 0.3679409355266969,
            "precision": 0.6382608695652174,
            "recall": 0.7710084033613446
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.6667957729941865,
            "auditor_fn_violation": 0.008655254926996406,
            "auditor_fp_violation": 0.015895026910001702,
            "ave_precision_score": 0.6557297969535293,
            "fpr": 0.20636663007683864,
            "logloss": 2.268018088447968,
            "mae": 0.3394684049511023,
            "precision": 0.6719022687609075,
            "recall": 0.805439330543933
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5054824561403509,
            "auc_prc": 0.5358079626549189,
            "auditor_fn_violation": 0.07687186716791981,
            "auditor_fp_violation": 0.024344117173668127,
            "ave_precision_score": 0.5040717619601837,
            "fpr": 0.2675438596491228,
            "logloss": 6.333760168371732,
            "mae": 0.49651840936427816,
            "precision": 0.5243664717348928,
            "recall": 0.5651260504201681
        },
        "train": {
            "accuracy": 0.5126234906695939,
            "auc_prc": 0.5501695915394126,
            "auditor_fn_violation": 0.08407240193084065,
            "auditor_fp_violation": 0.0039699540894836734,
            "ave_precision_score": 0.5245955254867509,
            "fpr": 0.2261251372118551,
            "logloss": 5.82959192248394,
            "mae": 0.48814626895346613,
            "precision": 0.5381165919282511,
            "recall": 0.502092050209205
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.7956647321276269,
            "auditor_fn_violation": 0.010591736694677877,
            "auditor_fp_violation": 0.019153388057299214,
            "ave_precision_score": 0.7937401284573731,
            "fpr": 0.1513157894736842,
            "logloss": 2.0421439075312957,
            "mae": 0.2856958644004015,
            "precision": 0.7267326732673267,
            "recall": 0.7710084033613446
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7849949264365148,
            "auditor_fn_violation": 0.015050820056124821,
            "auditor_fp_violation": 0.01762142456960476,
            "ave_precision_score": 0.7830272025070313,
            "fpr": 0.141602634467618,
            "logloss": 2.0919686273261346,
            "mae": 0.2815493021162954,
            "precision": 0.735655737704918,
            "recall": 0.7510460251046025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6107456140350878,
            "auc_prc": 0.6771404920163927,
            "auditor_fn_violation": 0.06896146616541353,
            "auditor_fp_violation": 0.023997062610655085,
            "ave_precision_score": 0.659224024516142,
            "fpr": 0.15570175438596492,
            "logloss": 4.423531959499722,
            "mae": 0.39118859206497414,
            "precision": 0.6493827160493827,
            "recall": 0.5525210084033614
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.6771635758938883,
            "auditor_fn_violation": 0.06800196574640954,
            "auditor_fp_violation": 0.015826579425700257,
            "ave_precision_score": 0.660790642417781,
            "fpr": 0.12843029637760703,
            "logloss": 4.531995129565537,
            "mae": 0.3868704703428897,
            "precision": 0.6759002770083102,
            "recall": 0.5104602510460251
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7748572634835708,
            "auditor_fn_violation": 0.014701275246940888,
            "auditor_fp_violation": 0.0174835023338162,
            "ave_precision_score": 0.7740363263421445,
            "fpr": 0.14473684210526316,
            "logloss": 1.8898653701219321,
            "mae": 0.3039998184547733,
            "precision": 0.7232704402515723,
            "recall": 0.7247899159663865
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7707341126741498,
            "auditor_fn_violation": 0.01550781016768552,
            "auditor_fp_violation": 0.022351906262437802,
            "ave_precision_score": 0.7689087688606242,
            "fpr": 0.13391877058177826,
            "logloss": 1.901441724381434,
            "mae": 0.29311591658610314,
            "precision": 0.7420718816067653,
            "recall": 0.7343096234309623
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6041666666666666,
            "auc_prc": 0.6720134177237751,
            "auditor_fn_violation": 0.06954887218045114,
            "auditor_fp_violation": 0.025531144374698212,
            "ave_precision_score": 0.6528458890303521,
            "fpr": 0.16337719298245615,
            "logloss": 4.501147310495496,
            "mae": 0.39387319718668296,
            "precision": 0.639225181598063,
            "recall": 0.5546218487394958
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.6720440003717973,
            "auditor_fn_violation": 0.07109755705487097,
            "auditor_fp_violation": 0.016587107029049617,
            "ave_precision_score": 0.65515571634825,
            "fpr": 0.132821075740944,
            "logloss": 4.575419042985929,
            "mae": 0.3907383581067926,
            "precision": 0.6666666666666666,
            "recall": 0.5062761506276151
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7885990301741153,
            "auditor_fn_violation": 0.02344095532950022,
            "auditor_fp_violation": 0.020712618702720103,
            "ave_precision_score": 0.780578879633158,
            "fpr": 0.14364035087719298,
            "logloss": 2.2048121889571948,
            "mae": 0.27674519111344725,
            "precision": 0.7374749498997996,
            "recall": 0.773109243697479
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.7951373812687998,
            "auditor_fn_violation": 0.012896766163441714,
            "auditor_fp_violation": 0.022007133748919418,
            "ave_precision_score": 0.7859427110014496,
            "fpr": 0.12952799121844127,
            "logloss": 2.173264565255717,
            "mae": 0.2603465426733791,
            "precision": 0.7635270541082164,
            "recall": 0.797071129707113
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.688046317748985,
            "auditor_fn_violation": 0.04295895989974938,
            "auditor_fp_violation": 0.016339228231128287,
            "ave_precision_score": 0.6831816472853381,
            "fpr": 0.19188596491228072,
            "logloss": 2.273113700450281,
            "mae": 0.3591488625537097,
            "precision": 0.651394422310757,
            "recall": 0.6869747899159664
        },
        "train": {
            "accuracy": 0.6553238199780461,
            "auc_prc": 0.6862683190116484,
            "auditor_fn_violation": 0.0504250696967331,
            "auditor_fp_violation": 0.006725599105619546,
            "ave_precision_score": 0.6835173945563877,
            "fpr": 0.15477497255762898,
            "logloss": 2.1556881883048495,
            "mae": 0.35011770789166347,
            "precision": 0.6838565022421524,
            "recall": 0.6380753138075314
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7806253700014555,
            "auditor_fn_violation": 0.020807994250331724,
            "auditor_fp_violation": 0.017297400611620793,
            "ave_precision_score": 0.7780774089613542,
            "fpr": 0.14692982456140352,
            "logloss": 2.0474448608251636,
            "mae": 0.29283660323915695,
            "precision": 0.7214137214137214,
            "recall": 0.7289915966386554
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7757846155181523,
            "auditor_fn_violation": 0.012423701022831132,
            "auditor_fp_violation": 0.01808534640764787,
            "ave_precision_score": 0.7736498086460712,
            "fpr": 0.12294182217343579,
            "logloss": 2.058343306269181,
            "mae": 0.2815672888039765,
            "precision": 0.7549234135667396,
            "recall": 0.7217573221757322
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6019736842105263,
            "auc_prc": 0.6694436971198984,
            "auditor_fn_violation": 0.07158521303258146,
            "auditor_fp_violation": 0.025460727506840503,
            "ave_precision_score": 0.6508344956875858,
            "fpr": 0.1611842105263158,
            "logloss": 4.477518624130875,
            "mae": 0.39594272229794575,
            "precision": 0.6388206388206388,
            "recall": 0.5462184873949579
        },
        "train": {
            "accuracy": 0.6037321624588364,
            "auc_prc": 0.6690190203330345,
            "auditor_fn_violation": 0.07334806112185334,
            "auditor_fp_violation": 0.01678991438994279,
            "ave_precision_score": 0.6524416827438435,
            "fpr": 0.13172338090010977,
            "logloss": 4.585914389370959,
            "mae": 0.3971232973874305,
            "precision": 0.6638655462184874,
            "recall": 0.49581589958158995
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5986842105263158,
            "auc_prc": 0.6653860490868861,
            "auditor_fn_violation": 0.07107612781954889,
            "auditor_fp_violation": 0.025264566232094006,
            "ave_precision_score": 0.6469060998339125,
            "fpr": 0.16557017543859648,
            "logloss": 4.600604098135836,
            "mae": 0.39926383429990703,
            "precision": 0.633495145631068,
            "recall": 0.5483193277310925
        },
        "train": {
            "accuracy": 0.5971459934138309,
            "auc_prc": 0.6686404561038702,
            "auditor_fn_violation": 0.07424826274864627,
            "auditor_fp_violation": 0.01773550371010716,
            "ave_precision_score": 0.6513977443216002,
            "fpr": 0.13611416026344675,
            "logloss": 4.696816666104378,
            "mae": 0.39962618643758957,
            "precision": 0.6545961002785515,
            "recall": 0.4916317991631799
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6546052631578947,
            "auc_prc": 0.7183994019228978,
            "auditor_fn_violation": 0.052239974937343364,
            "auditor_fp_violation": 0.021411757605021727,
            "ave_precision_score": 0.7123947886198053,
            "fpr": 0.15021929824561403,
            "logloss": 2.7778866868684218,
            "mae": 0.34569676980644815,
            "precision": 0.6850574712643678,
            "recall": 0.6260504201680672
        },
        "train": {
            "accuracy": 0.6597145993413831,
            "auc_prc": 0.7152101361633729,
            "auditor_fn_violation": 0.050898134837343685,
            "auditor_fp_violation": 0.010713298839181372,
            "ave_precision_score": 0.7114352110947014,
            "fpr": 0.1251372118551043,
            "logloss": 2.780135782502918,
            "mae": 0.3395809166067217,
            "precision": 0.7121212121212122,
            "recall": 0.5899581589958159
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7904485178569882,
            "auditor_fn_violation": 0.01732271856110866,
            "auditor_fp_violation": 0.016698857234830194,
            "ave_precision_score": 0.788752491309713,
            "fpr": 0.1425438596491228,
            "logloss": 1.9880316414489985,
            "mae": 0.2856565292174191,
            "precision": 0.7325102880658436,
            "recall": 0.7478991596638656
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7850462428376197,
            "auditor_fn_violation": 0.014226400709138427,
            "auditor_fp_violation": 0.01885601437904189,
            "ave_precision_score": 0.7830363774406972,
            "fpr": 0.1251372118551043,
            "logloss": 2.0079337232692414,
            "mae": 0.2755550451097279,
            "precision": 0.7537796976241901,
            "recall": 0.7301255230125523
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6030701754385965,
            "auc_prc": 0.6689479213805931,
            "auditor_fn_violation": 0.07005795739348372,
            "auditor_fp_violation": 0.025531144374698212,
            "ave_precision_score": 0.650234311166421,
            "fpr": 0.16337719298245615,
            "logloss": 4.547650252661924,
            "mae": 0.39625791915042596,
            "precision": 0.6383495145631068,
            "recall": 0.5525210084033614
        },
        "train": {
            "accuracy": 0.601536772777168,
            "auc_prc": 0.6703837052165145,
            "auditor_fn_violation": 0.07334806112185334,
            "auditor_fp_violation": 0.01734763463239899,
            "ave_precision_score": 0.6536305447017257,
            "fpr": 0.13391877058177826,
            "logloss": 4.6265085774217845,
            "mae": 0.3955625108109279,
            "precision": 0.6601671309192201,
            "recall": 0.49581589958158995
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8115454565960355,
            "auditor_fn_violation": 0.015899122807017545,
            "auditor_fp_violation": 0.015959480122324166,
            "ave_precision_score": 0.8118117062849626,
            "fpr": 0.12938596491228072,
            "logloss": 1.7738593400352178,
            "mae": 0.2906974608197956,
            "precision": 0.7484008528784648,
            "recall": 0.7373949579831933
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8195895319868207,
            "auditor_fn_violation": 0.005816864083332951,
            "auditor_fp_violation": 0.016161211571173978,
            "ave_precision_score": 0.8198419398147095,
            "fpr": 0.12733260153677278,
            "logloss": 1.8000661961922884,
            "mae": 0.29170290313072544,
            "precision": 0.7483731019522777,
            "recall": 0.7217573221757322
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8093542104246336,
            "auditor_fn_violation": 0.015290984814978627,
            "auditor_fp_violation": 0.02236238532110092,
            "ave_precision_score": 0.8083038241329256,
            "fpr": 0.12719298245614036,
            "logloss": 1.9160434880775328,
            "mae": 0.27572235631464753,
            "precision": 0.7578288100208769,
            "recall": 0.7626050420168067
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.7960653762895978,
            "auditor_fn_violation": 0.010209939879391353,
            "auditor_fp_violation": 0.014335945323135509,
            "ave_precision_score": 0.7945871090211177,
            "fpr": 0.12294182217343579,
            "logloss": 1.9739371352404727,
            "mae": 0.2729800303518988,
            "precision": 0.7617021276595745,
            "recall": 0.7489539748953975
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5866228070175439,
            "auc_prc": 0.6808176629033582,
            "auditor_fn_violation": 0.07855576441102759,
            "auditor_fp_violation": 0.025430548849187194,
            "ave_precision_score": 0.6807059560286997,
            "fpr": 0.12609649122807018,
            "logloss": 2.683848628105802,
            "mae": 0.39625632059674465,
            "precision": 0.6504559270516718,
            "recall": 0.4495798319327731
        },
        "train": {
            "accuracy": 0.5949506037321625,
            "auc_prc": 0.7042365402305142,
            "auditor_fn_violation": 0.07837724878174246,
            "auditor_fp_violation": 0.016450212060446733,
            "ave_precision_score": 0.7047443434432584,
            "fpr": 0.09879253567508232,
            "logloss": 2.810221437856355,
            "mae": 0.3901272458537587,
            "precision": 0.6885813148788927,
            "recall": 0.41631799163179917
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5427631578947368,
            "auc_prc": 0.6616413965691876,
            "auditor_fn_violation": 0.11360432330827068,
            "auditor_fp_violation": 0.0843191694833414,
            "ave_precision_score": 0.6623482504978683,
            "fpr": 0.20833333333333334,
            "logloss": 3.909220641848039,
            "mae": 0.4571723755753192,
            "precision": 0.5671981776765376,
            "recall": 0.523109243697479
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.683710111649291,
            "auditor_fn_violation": 0.11370097690247971,
            "auditor_fp_violation": 0.09067770614734462,
            "ave_precision_score": 0.683749709954254,
            "fpr": 0.2074643249176729,
            "logloss": 3.649302470635546,
            "mae": 0.430522410920792,
            "precision": 0.5944206008583691,
            "recall": 0.5794979079497908
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 12092,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.6733196129639898,
            "auditor_fn_violation": 0.07005795739348372,
            "auditor_fp_violation": 0.02414795589892162,
            "ave_precision_score": 0.6546818734644823,
            "fpr": 0.1600877192982456,
            "logloss": 4.450043178856863,
            "mae": 0.3927676084180703,
            "precision": 0.6430317848410758,
            "recall": 0.5525210084033614
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.6753657604559393,
            "auditor_fn_violation": 0.07309545352249816,
            "auditor_fp_violation": 0.019411199529486927,
            "ave_precision_score": 0.6587007865170955,
            "fpr": 0.13062568605927552,
            "logloss": 4.511549944626946,
            "mae": 0.3896874759255484,
            "precision": 0.6685236768802229,
            "recall": 0.502092050209205
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 12092,
        "test": {
            "accuracy": 0.569078947368421,
            "auc_prc": 0.6440918935527734,
            "auditor_fn_violation": 0.08791510025062658,
            "auditor_fp_violation": 0.02548084661194271,
            "ave_precision_score": 0.632414922851732,
            "fpr": 0.14473684210526316,
            "logloss": 4.446367665286108,
            "mae": 0.4309276230376208,
            "precision": 0.6195965417867435,
            "recall": 0.45168067226890757
        },
        "train": {
            "accuracy": 0.5729967069154775,
            "auc_prc": 0.666357355312061,
            "auditor_fn_violation": 0.08566612623949957,
            "auditor_fp_violation": 0.018098021867703686,
            "ave_precision_score": 0.6544193883580915,
            "fpr": 0.1163556531284303,
            "logloss": 4.6167506949752335,
            "mae": 0.4237530061372492,
            "precision": 0.6478405315614618,
            "recall": 0.40794979079497906
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.7094762168061148,
            "auditor_fn_violation": 0.049107142857142856,
            "auditor_fp_violation": 0.022166224046354422,
            "ave_precision_score": 0.7013352068934597,
            "fpr": 0.14802631578947367,
            "logloss": 3.029817630013342,
            "mae": 0.348029008680175,
            "precision": 0.6910755148741419,
            "recall": 0.634453781512605
        },
        "train": {
            "accuracy": 0.6597145993413831,
            "auc_prc": 0.7022902678929329,
            "auditor_fn_violation": 0.047605050314840934,
            "auditor_fp_violation": 0.010249377001138257,
            "ave_precision_score": 0.6943640468237764,
            "fpr": 0.1251372118551043,
            "logloss": 3.1267388623021644,
            "mae": 0.34220819379997713,
            "precision": 0.7121212121212122,
            "recall": 0.5899581589958159
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5767543859649122,
            "auc_prc": 0.6735090434822197,
            "auditor_fn_violation": 0.08278508771929825,
            "auditor_fp_violation": 0.0294443103170771,
            "ave_precision_score": 0.6702842353813043,
            "fpr": 0.13157894736842105,
            "logloss": 3.0374418731484467,
            "mae": 0.4069501464306948,
            "precision": 0.6363636363636364,
            "recall": 0.4411764705882353
        },
        "train": {
            "accuracy": 0.5905598243688255,
            "auc_prc": 0.7030498787478613,
            "auditor_fn_violation": 0.08586361945354089,
            "auditor_fp_violation": 0.01932500640110733,
            "ave_precision_score": 0.703049633344611,
            "fpr": 0.10098792535675083,
            "logloss": 3.0111680400602485,
            "mae": 0.39852853437184166,
            "precision": 0.6816608996539792,
            "recall": 0.4121338912133891
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6052631578947368,
            "auc_prc": 0.6739510563138276,
            "auditor_fn_violation": 0.07005795739348372,
            "auditor_fp_violation": 0.024733924835023343,
            "ave_precision_score": 0.6557357602507401,
            "fpr": 0.1611842105263158,
            "logloss": 4.481518950563697,
            "mae": 0.39337726823800034,
            "precision": 0.6414634146341464,
            "recall": 0.5525210084033614
        },
        "train": {
            "accuracy": 0.6092206366630076,
            "auc_prc": 0.676545243247515,
            "auditor_fn_violation": 0.07219525189570522,
            "auditor_fp_violation": 0.019461901369710224,
            "ave_precision_score": 0.658971041183076,
            "fpr": 0.13172338090010977,
            "logloss": 4.578181889883251,
            "mae": 0.39068036782591414,
            "precision": 0.6685082872928176,
            "recall": 0.5062761506276151
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7981974021132446,
            "auditor_fn_violation": 0.014371867167919803,
            "auditor_fp_violation": 0.02652452518911958,
            "ave_precision_score": 0.7916720363214089,
            "fpr": 0.20065789473684212,
            "logloss": 2.4188917755263035,
            "mae": 0.28056562937042584,
            "precision": 0.6877133105802048,
            "recall": 0.8466386554621849
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8000018166013954,
            "auditor_fn_violation": 0.0044183365559939194,
            "auditor_fp_violation": 0.030149849288779944,
            "ave_precision_score": 0.793178092982462,
            "fpr": 0.18441273326015367,
            "logloss": 2.2566385193189706,
            "mae": 0.26605679086211637,
            "precision": 0.7083333333333334,
            "recall": 0.8535564853556485
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7739180837187365,
            "auditor_fn_violation": 0.01735957540911101,
            "auditor_fp_violation": 0.01940487687107678,
            "ave_precision_score": 0.7701485069502032,
            "fpr": 0.15789473684210525,
            "logloss": 2.065455681683381,
            "mae": 0.3029141913358417,
            "precision": 0.7073170731707317,
            "recall": 0.7310924369747899
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7684244653636085,
            "auditor_fn_violation": 0.014219511410974195,
            "auditor_fp_violation": 0.01728679242413104,
            "ave_precision_score": 0.7645366405361076,
            "fpr": 0.14270032930845225,
            "logloss": 2.0553845898829124,
            "mae": 0.292009121400948,
            "precision": 0.7325102880658436,
            "recall": 0.7447698744769874
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 12092,
        "test": {
            "accuracy": 0.569078947368421,
            "auc_prc": 0.6368751477999472,
            "auditor_fn_violation": 0.08439066416040102,
            "auditor_fp_violation": 0.025269596008369553,
            "ave_precision_score": 0.6277574787655126,
            "fpr": 0.13815789473684212,
            "logloss": 3.949723155334142,
            "mae": 0.4326876693384854,
            "precision": 0.6238805970149254,
            "recall": 0.43907563025210083
        },
        "train": {
            "accuracy": 0.5817782656421515,
            "auc_prc": 0.6626414175314117,
            "auditor_fn_violation": 0.08501853221206179,
            "auditor_fp_violation": 0.017154967639550482,
            "ave_precision_score": 0.6530762987091205,
            "fpr": 0.10647639956092206,
            "logloss": 4.123331521757969,
            "mae": 0.42306649049933187,
            "precision": 0.6666666666666666,
            "recall": 0.40585774058577406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7469778062778081,
            "auditor_fn_violation": 0.025615509361639397,
            "auditor_fp_violation": 0.01886166103331724,
            "ave_precision_score": 0.7476467396484225,
            "fpr": 0.13815789473684212,
            "logloss": 1.7528787535362214,
            "mae": 0.30997310657650506,
            "precision": 0.7174887892376681,
            "recall": 0.6722689075630253
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7618541901437725,
            "auditor_fn_violation": 0.026413569161664277,
            "auditor_fp_violation": 0.013217969746211939,
            "ave_precision_score": 0.7625192036309061,
            "fpr": 0.1141602634467618,
            "logloss": 1.7488449459967255,
            "mae": 0.2990822512908359,
            "precision": 0.7552941176470588,
            "recall": 0.6715481171548117
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.708600660069977,
            "auditor_fn_violation": 0.03336235810113519,
            "auditor_fp_violation": 0.0225912401416385,
            "ave_precision_score": 0.7093060246339898,
            "fpr": 0.13486842105263158,
            "logloss": 2.2041413214953387,
            "mae": 0.3378362266477982,
            "precision": 0.7043269230769231,
            "recall": 0.615546218487395
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7323596612036414,
            "auditor_fn_violation": 0.0370299776327453,
            "auditor_fp_violation": 0.010107411848513041,
            "ave_precision_score": 0.7330966496814784,
            "fpr": 0.10318331503841932,
            "logloss": 2.2620411623853127,
            "mae": 0.32801001901767235,
            "precision": 0.7519788918205804,
            "recall": 0.5962343096234309
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 12092,
        "test": {
            "accuracy": 0.4956140350877193,
            "auc_prc": 0.5113699516035082,
            "auditor_fn_violation": 0.05874060150375941,
            "auditor_fp_violation": 0.006337518107194595,
            "ave_precision_score": 0.5084506856590254,
            "fpr": 0.16885964912280702,
            "logloss": 4.313027679917835,
            "mae": 0.5109677396026127,
            "precision": 0.5246913580246914,
            "recall": 0.35714285714285715
        },
        "train": {
            "accuracy": 0.5071350164654226,
            "auc_prc": 0.526528429034886,
            "auditor_fn_violation": 0.06650009874660703,
            "auditor_fp_violation": 0.015025490350172262,
            "ave_precision_score": 0.5236992745420326,
            "fpr": 0.14928649835345773,
            "logloss": 4.3835568427096545,
            "mae": 0.4965637370240208,
            "precision": 0.5481727574750831,
            "recall": 0.34518828451882844
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7958854852550579,
            "auditor_fn_violation": 0.02033576588530149,
            "auditor_fp_violation": 0.016542934170288112,
            "ave_precision_score": 0.7961533714377949,
            "fpr": 0.13815789473684212,
            "logloss": 1.5574087148664895,
            "mae": 0.30728004854589336,
            "precision": 0.7301927194860813,
            "recall": 0.7163865546218487
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8158661217413753,
            "auditor_fn_violation": 0.017535560260691044,
            "auditor_fp_violation": 0.0204353767019974,
            "ave_precision_score": 0.8165079696961137,
            "fpr": 0.12733260153677278,
            "logloss": 1.466885113212292,
            "mae": 0.29181832244797795,
            "precision": 0.7531914893617021,
            "recall": 0.7405857740585774
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7495211459344073,
            "auditor_fn_violation": 0.03521671826625387,
            "auditor_fp_violation": 0.023604740061162085,
            "ave_precision_score": 0.7337066866187667,
            "fpr": 0.15021929824561403,
            "logloss": 3.1912761104083525,
            "mae": 0.32271342925087854,
            "precision": 0.700218818380744,
            "recall": 0.6722689075630253
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.755180326021073,
            "auditor_fn_violation": 0.03539721396782239,
            "auditor_fp_violation": 0.00875620780656234,
            "ave_precision_score": 0.7391334455916585,
            "fpr": 0.132821075740944,
            "logloss": 3.2129153918075795,
            "mae": 0.31158119151157876,
            "precision": 0.7218390804597701,
            "recall": 0.6569037656903766
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 12092,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.6723036122406895,
            "auditor_fn_violation": 0.07005795739348372,
            "auditor_fp_violation": 0.02597376468694673,
            "ave_precision_score": 0.6538075458024708,
            "fpr": 0.1600877192982456,
            "logloss": 4.4200902433004785,
            "mae": 0.3924673367568017,
            "precision": 0.6430317848410758,
            "recall": 0.5525210084033614
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.6743865404971792,
            "auditor_fn_violation": 0.07225036628101907,
            "auditor_fp_violation": 0.015441245440003252,
            "ave_precision_score": 0.6577562727033965,
            "fpr": 0.12843029637760703,
            "logloss": 4.494968522350009,
            "mae": 0.39121152927167807,
            "precision": 0.6694915254237288,
            "recall": 0.49581589958158995
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6107456140350878,
            "auc_prc": 0.6811185116067693,
            "auditor_fn_violation": 0.06751253132832082,
            "auditor_fp_violation": 0.025460727506840503,
            "ave_precision_score": 0.6620622453170453,
            "fpr": 0.1611842105263158,
            "logloss": 4.2406491032059215,
            "mae": 0.38579076233080006,
            "precision": 0.6457831325301204,
            "recall": 0.5630252100840336
        },
        "train": {
            "accuracy": 0.6180021953896817,
            "auc_prc": 0.6836395872944201,
            "auditor_fn_violation": 0.06814434457513699,
            "auditor_fp_violation": 0.013955681521460827,
            "ave_precision_score": 0.6659345247899836,
            "fpr": 0.132821075740944,
            "logloss": 4.289695719884197,
            "mae": 0.37959685650248726,
            "precision": 0.6747311827956989,
            "recall": 0.5251046025104602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5054824561403509,
            "auc_prc": 0.5357425289091491,
            "auditor_fn_violation": 0.07687186716791981,
            "auditor_fp_violation": 0.024344117173668127,
            "ave_precision_score": 0.5039892616491869,
            "fpr": 0.2675438596491228,
            "logloss": 6.337033900790223,
            "mae": 0.49653902432488456,
            "precision": 0.5243664717348928,
            "recall": 0.5651260504201681
        },
        "train": {
            "accuracy": 0.5126234906695939,
            "auc_prc": 0.550094381686304,
            "auditor_fn_violation": 0.08407240193084065,
            "auditor_fp_violation": 0.0039699540894836734,
            "ave_precision_score": 0.524538033639824,
            "fpr": 0.2261251372118551,
            "logloss": 5.8330409746930325,
            "mae": 0.48817613977273844,
            "precision": 0.5381165919282511,
            "recall": 0.502092050209205
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.6774324383452293,
            "auditor_fn_violation": 0.06547619047619048,
            "auditor_fp_violation": 0.025460727506840503,
            "ave_precision_score": 0.6629635052589755,
            "fpr": 0.1611842105263158,
            "logloss": 3.692214272966718,
            "mae": 0.37968926541173725,
            "precision": 0.649164677804296,
            "recall": 0.5714285714285714
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6814845066358359,
            "auditor_fn_violation": 0.06589384050815465,
            "auditor_fp_violation": 0.009838692095329608,
            "ave_precision_score": 0.66794671602739,
            "fpr": 0.12952799121844127,
            "logloss": 3.7468740131915617,
            "mae": 0.3734964564944923,
            "precision": 0.6844919786096256,
            "recall": 0.5355648535564853
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5657894736842105,
            "auc_prc": 0.6431499374108626,
            "auditor_fn_violation": 0.08783677944862156,
            "auditor_fp_violation": 0.02310930709802028,
            "ave_precision_score": 0.6304556062385966,
            "fpr": 0.14583333333333334,
            "logloss": 4.882636382289107,
            "mae": 0.43480892882559646,
            "precision": 0.615606936416185,
            "recall": 0.4474789915966387
        },
        "train": {
            "accuracy": 0.5806805708013172,
            "auc_prc": 0.6657194788570571,
            "auditor_fn_violation": 0.0854135186401444,
            "auditor_fp_violation": 0.018478285669378373,
            "ave_precision_score": 0.6531417503273916,
            "fpr": 0.1119648737650933,
            "logloss": 5.088741030884682,
            "mae": 0.42314198309193857,
            "precision": 0.66,
            "recall": 0.41422594142259417
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.7965872081186891,
            "auditor_fn_violation": 0.007129496535456288,
            "auditor_fp_violation": 0.021799050378239183,
            "ave_precision_score": 0.7930723200875216,
            "fpr": 0.14473684210526316,
            "logloss": 1.8616329457731,
            "mae": 0.2766087313648182,
            "precision": 0.7396449704142012,
            "recall": 0.7878151260504201
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.7934535295879014,
            "auditor_fn_violation": 0.01660780144124118,
            "auditor_fp_violation": 0.025809771765666234,
            "ave_precision_score": 0.7897059861263014,
            "fpr": 0.14050493962678376,
            "logloss": 1.753282722361304,
            "mae": 0.26962366301983764,
            "precision": 0.7414141414141414,
            "recall": 0.7677824267782427
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.7709695893056365,
            "auditor_fn_violation": 0.015118218339967565,
            "auditor_fp_violation": 0.021668276195074845,
            "ave_precision_score": 0.7663161358573469,
            "fpr": 0.1787280701754386,
            "logloss": 2.0751498692655557,
            "mae": 0.30020005761691887,
            "precision": 0.6981481481481482,
            "recall": 0.792016806722689
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7699125906947195,
            "auditor_fn_violation": 0.015131195201374192,
            "auditor_fp_violation": 0.02161926467121124,
            "ave_precision_score": 0.7637042026063541,
            "fpr": 0.16136114160263446,
            "logloss": 2.0166994807939758,
            "mae": 0.28579046358468346,
            "precision": 0.720532319391635,
            "recall": 0.7928870292887029
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6052631578947368,
            "auc_prc": 0.6705741136834348,
            "auditor_fn_violation": 0.07056704260651629,
            "auditor_fp_violation": 0.02597376468694673,
            "ave_precision_score": 0.6523327934745534,
            "fpr": 0.1600877192982456,
            "logloss": 4.4096700930020365,
            "mae": 0.3933409845442653,
            "precision": 0.6421568627450981,
            "recall": 0.5504201680672269
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.6733403256006203,
            "auditor_fn_violation": 0.07244785949506038,
            "auditor_fp_violation": 0.016412185680279263,
            "ave_precision_score": 0.6567595444904613,
            "fpr": 0.12952799121844127,
            "logloss": 4.4843420597863854,
            "mae": 0.3917606546199963,
            "precision": 0.6694677871148459,
            "recall": 0.5
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7712157622018393,
            "auditor_fn_violation": 0.015567411174996314,
            "auditor_fp_violation": 0.019435055528730084,
            "ave_precision_score": 0.7679901435696224,
            "fpr": 0.16666666666666666,
            "logloss": 1.9169528143423675,
            "mae": 0.3012591224808184,
            "precision": 0.708253358925144,
            "recall": 0.7752100840336135
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.7723720674299953,
            "auditor_fn_violation": 0.016472311910677953,
            "auditor_fp_violation": 0.020929719644174484,
            "ave_precision_score": 0.769444666533779,
            "fpr": 0.145993413830955,
            "logloss": 1.797252200604643,
            "mae": 0.2864381493996634,
            "precision": 0.7361111111111112,
            "recall": 0.7761506276150628
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 12092,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8248012429068223,
            "auditor_fn_violation": 0.01216275984077842,
            "auditor_fp_violation": 0.02060196362465798,
            "ave_precision_score": 0.8259298703620227,
            "fpr": 0.11842105263157894,
            "logloss": 1.7716748503032544,
            "mae": 0.2793040577739135,
            "precision": 0.7672413793103449,
            "recall": 0.7478991596638656
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8533293876843645,
            "auditor_fn_violation": 0.01252015119713038,
            "auditor_fp_violation": 0.016921739174523342,
            "ave_precision_score": 0.853433768346134,
            "fpr": 0.1119648737650933,
            "logloss": 1.8036195271151516,
            "mae": 0.2769113360017781,
            "precision": 0.7743362831858407,
            "recall": 0.7322175732217573
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 12092,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.6750779862215931,
            "auditor_fn_violation": 0.06954887218045114,
            "auditor_fp_violation": 0.024733924835023343,
            "ave_precision_score": 0.6565975116431055,
            "fpr": 0.1611842105263158,
            "logloss": 4.420584358249109,
            "mae": 0.39218371829683046,
            "precision": 0.6423357664233577,
            "recall": 0.5546218487394958
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.6791675358725187,
            "auditor_fn_violation": 0.07039484864211933,
            "auditor_fp_violation": 0.018037179659435746,
            "ave_precision_score": 0.6622823816515478,
            "fpr": 0.13062568605927552,
            "logloss": 4.4758009021641145,
            "mae": 0.38714009493667817,
            "precision": 0.673972602739726,
            "recall": 0.5146443514644351
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.7040145580439393,
            "auditor_fn_violation": 0.0555686090225564,
            "auditor_fp_violation": 0.020033598905520682,
            "ave_precision_score": 0.6915346204258155,
            "fpr": 0.15021929824561403,
            "logloss": 3.4232628627349313,
            "mae": 0.3580702752412478,
            "precision": 0.6753554502369669,
            "recall": 0.5987394957983193
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.700539928126374,
            "auditor_fn_violation": 0.05145846442136786,
            "auditor_fp_violation": 0.010307684117395043,
            "ave_precision_score": 0.6887596489189687,
            "fpr": 0.1251372118551043,
            "logloss": 3.505962517144102,
            "mae": 0.35135444618674316,
            "precision": 0.7038961038961039,
            "recall": 0.5669456066945606
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6030701754385965,
            "auc_prc": 0.6690899487049825,
            "auditor_fn_violation": 0.07107612781954889,
            "auditor_fp_violation": 0.024376810719459206,
            "ave_precision_score": 0.6504931170834871,
            "fpr": 0.1611842105263158,
            "logloss": 4.459525499984327,
            "mae": 0.395736851806988,
            "precision": 0.6397058823529411,
            "recall": 0.5483193277310925
        },
        "train": {
            "accuracy": 0.5993413830954994,
            "auc_prc": 0.6681628735849834,
            "auditor_fn_violation": 0.07424826274864627,
            "auditor_fp_violation": 0.01450326139587237,
            "ave_precision_score": 0.651641038095176,
            "fpr": 0.13391877058177826,
            "logloss": 4.577087662958466,
            "mae": 0.3987816157097785,
            "precision": 0.6582633053221288,
            "recall": 0.4916317991631799
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.735144473893245,
            "auditor_fn_violation": 0.04511278195488722,
            "auditor_fp_violation": 0.018197730564944475,
            "ave_precision_score": 0.7296749872163122,
            "fpr": 0.1337719298245614,
            "logloss": 2.839735500420618,
            "mae": 0.34559558408762303,
            "precision": 0.6995073891625616,
            "recall": 0.5966386554621849
        },
        "train": {
            "accuracy": 0.6717892425905598,
            "auc_prc": 0.7323110090354469,
            "auditor_fn_violation": 0.04301677773746264,
            "auditor_fp_violation": 0.01015557859672517,
            "ave_precision_score": 0.7288516205327324,
            "fpr": 0.11086717892425905,
            "logloss": 2.8845843273529828,
            "mae": 0.33289378687462506,
            "precision": 0.7349081364829396,
            "recall": 0.5857740585774058
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 12092,
        "test": {
            "accuracy": 0.4868421052631579,
            "auc_prc": 0.56946072533429,
            "auditor_fn_violation": 0.00760172490048651,
            "auditor_fp_violation": 0.0036968855625301787,
            "ave_precision_score": 0.5704543174978978,
            "fpr": 0.007675438596491228,
            "logloss": 4.931272822063067,
            "mae": 0.5197679129133058,
            "precision": 0.6818181818181818,
            "recall": 0.031512605042016806
        },
        "train": {
            "accuracy": 0.4862788144895719,
            "auc_prc": 0.6052650111107365,
            "auditor_fn_violation": 0.00594087145028913,
            "auditor_fp_violation": 0.001543871034799208,
            "ave_precision_score": 0.6061773482535855,
            "fpr": 0.003293084522502744,
            "logloss": 5.084962500343393,
            "mae": 0.5208076843009883,
            "precision": 0.8125,
            "recall": 0.027196652719665274
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.6737147369870446,
            "auditor_fn_violation": 0.06903978696741855,
            "auditor_fp_violation": 0.025460727506840503,
            "ave_precision_score": 0.6548276994383022,
            "fpr": 0.1611842105263158,
            "logloss": 4.362869960100053,
            "mae": 0.39224401948738696,
            "precision": 0.6432038834951457,
            "recall": 0.5567226890756303
        },
        "train": {
            "accuracy": 0.6092206366630076,
            "auc_prc": 0.6731656413088924,
            "auditor_fn_violation": 0.07019735542807803,
            "auditor_fp_violation": 0.016559221016926817,
            "ave_precision_score": 0.6566578614732637,
            "fpr": 0.13391877058177826,
            "logloss": 4.435740239318117,
            "mae": 0.3886427237226529,
            "precision": 0.6666666666666666,
            "recall": 0.5104602510460251
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.816297190992925,
            "auditor_fn_violation": 0.01235165118679051,
            "auditor_fp_violation": 0.0185573595686464,
            "ave_precision_score": 0.8157523292403006,
            "fpr": 0.15679824561403508,
            "logloss": 0.9680297947281307,
            "mae": 0.2705734258286859,
            "precision": 0.7342007434944238,
            "recall": 0.8298319327731093
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8097848196219658,
            "auditor_fn_violation": 0.012880691134391838,
            "auditor_fp_violation": 0.021031123324621065,
            "ave_precision_score": 0.8098641739158795,
            "fpr": 0.15148188803512624,
            "logloss": 0.8465803540206562,
            "mae": 0.2669461445831125,
            "precision": 0.7371428571428571,
            "recall": 0.8096234309623431
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7704620686013988,
            "auditor_fn_violation": 0.013761425622880732,
            "auditor_fp_violation": 0.01920368582005473,
            "ave_precision_score": 0.7671117866565342,
            "fpr": 0.15899122807017543,
            "logloss": 2.0137419332876525,
            "mae": 0.30273435556276385,
            "precision": 0.71,
            "recall": 0.7457983193277311
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7711662112029889,
            "auditor_fn_violation": 0.017880025168902632,
            "auditor_fp_violation": 0.023140319877909966,
            "ave_precision_score": 0.7690039990323208,
            "fpr": 0.14489571899012074,
            "logloss": 1.9356727305380468,
            "mae": 0.28870096231709025,
            "precision": 0.7306122448979592,
            "recall": 0.7489539748953975
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6030701754385965,
            "auc_prc": 0.64439627156422,
            "auditor_fn_violation": 0.05053995282323456,
            "auditor_fp_violation": 0.05486228472557541,
            "ave_precision_score": 0.6367867631444235,
            "fpr": 0.29276315789473684,
            "logloss": 2.1820470859953507,
            "mae": 0.39842825142979854,
            "precision": 0.5879629629629629,
            "recall": 0.8004201680672269
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.6460435356148254,
            "auditor_fn_violation": 0.04259653054944449,
            "auditor_fp_violation": 0.05908031932018973,
            "ave_precision_score": 0.6397853705997041,
            "fpr": 0.2678375411635565,
            "logloss": 2.0537560509485475,
            "mae": 0.3605315891965898,
            "precision": 0.6205287713841369,
            "recall": 0.8347280334728033
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.6198423933966821,
            "auditor_fn_violation": 0.05819235588972432,
            "auditor_fp_violation": 0.017086150008047665,
            "ave_precision_score": 0.6136880081996341,
            "fpr": 0.24342105263157895,
            "logloss": 2.453943066962425,
            "mae": 0.41624186334908875,
            "precision": 0.5827067669172933,
            "recall": 0.6512605042016807
        },
        "train": {
            "accuracy": 0.5971459934138309,
            "auc_prc": 0.6244331380816411,
            "auditor_fn_violation": 0.061567361261017146,
            "auditor_fp_violation": 0.005399745983780479,
            "ave_precision_score": 0.6207732508888675,
            "fpr": 0.1964873765093304,
            "logloss": 2.232726646170941,
            "mae": 0.3981883403080888,
            "precision": 0.6183368869936035,
            "recall": 0.606694560669456
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 12092,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8036806868159423,
            "auditor_fn_violation": 0.012743255196815573,
            "auditor_fp_violation": 0.02010401577337841,
            "ave_precision_score": 0.8015711132060032,
            "fpr": 0.1425438596491228,
            "logloss": 1.98476629218246,
            "mae": 0.2720930882712503,
            "precision": 0.7410358565737052,
            "recall": 0.7815126050420168
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8016242075721194,
            "auditor_fn_violation": 0.005495363502335474,
            "auditor_fp_violation": 0.0179687321751343,
            "ave_precision_score": 0.7993344235039341,
            "fpr": 0.1350164654226125,
            "logloss": 1.9847026307038196,
            "mae": 0.2638558514591166,
            "precision": 0.7489795918367347,
            "recall": 0.7677824267782427
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.6744896990462295,
            "auditor_fn_violation": 0.06860902255639098,
            "auditor_fp_violation": 0.025531144374698212,
            "ave_precision_score": 0.6564587194871013,
            "fpr": 0.16337719298245615,
            "logloss": 4.212734105183779,
            "mae": 0.39030327364289863,
            "precision": 0.6426858513189448,
            "recall": 0.5630252100840336
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.6734654657126891,
            "auditor_fn_violation": 0.06884705298788862,
            "auditor_fp_violation": 0.014310594403023864,
            "ave_precision_score": 0.6570935448393893,
            "fpr": 0.132821075740944,
            "logloss": 4.285241603009578,
            "mae": 0.3870033388615377,
            "precision": 0.6711956521739131,
            "recall": 0.5167364016736402
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7666158476269751,
            "auditor_fn_violation": 0.01635292274804659,
            "auditor_fp_violation": 0.03217296394656365,
            "ave_precision_score": 0.758038676764198,
            "fpr": 0.1787280701754386,
            "logloss": 3.240211788034742,
            "mae": 0.2892240209319611,
            "precision": 0.7014652014652014,
            "recall": 0.8046218487394958
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7831328986619235,
            "auditor_fn_violation": 0.010846051743222082,
            "auditor_fp_violation": 0.015258718815199405,
            "ave_precision_score": 0.7752142115536937,
            "fpr": 0.15587266739846323,
            "logloss": 2.803542891669407,
            "mae": 0.27775794250140046,
            "precision": 0.7253384912959381,
            "recall": 0.7845188284518828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8003804479365766,
            "auditor_fn_violation": 0.015592750257997933,
            "auditor_fp_violation": 0.018585023338161925,
            "ave_precision_score": 0.7978328431096106,
            "fpr": 0.1524122807017544,
            "logloss": 1.212460724610185,
            "mae": 0.27923852827276463,
            "precision": 0.7274509803921568,
            "recall": 0.7794117647058824
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.7965020384901986,
            "auditor_fn_violation": 0.01142245635629613,
            "auditor_fp_violation": 0.02387803165315886,
            "ave_precision_score": 0.7942793659433169,
            "fpr": 0.1394072447859495,
            "logloss": 1.082785832729952,
            "mae": 0.265940405365606,
            "precision": 0.7454909819639278,
            "recall": 0.7782426778242678
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 12092,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.6750627616492553,
            "auditor_fn_violation": 0.06954887218045114,
            "auditor_fp_violation": 0.024733924835023343,
            "ave_precision_score": 0.6565913693010501,
            "fpr": 0.1611842105263158,
            "logloss": 4.420644543703228,
            "mae": 0.39218849533178207,
            "precision": 0.6423357664233577,
            "recall": 0.5546218487394958
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.6791627585775161,
            "auditor_fn_violation": 0.07039484864211933,
            "auditor_fp_violation": 0.018037179659435746,
            "ave_precision_score": 0.6622776077810285,
            "fpr": 0.13062568605927552,
            "logloss": 4.475870851704803,
            "mae": 0.38715459015617026,
            "precision": 0.673972602739726,
            "recall": 0.5146443514644351
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6129385964912281,
            "auc_prc": 0.6799451063965106,
            "auditor_fn_violation": 0.06684680451127821,
            "auditor_fp_violation": 0.02509355383872526,
            "ave_precision_score": 0.6651867163136583,
            "fpr": 0.15570175438596492,
            "logloss": 3.8726108746753813,
            "mae": 0.3809605148020642,
            "precision": 0.6511056511056511,
            "recall": 0.5567226890756303
        },
        "train": {
            "accuracy": 0.6212952799121844,
            "auc_prc": 0.684660689177619,
            "auditor_fn_violation": 0.06375356521179999,
            "auditor_fp_violation": 0.009838692095329608,
            "ave_precision_score": 0.6708032764784352,
            "fpr": 0.12952799121844127,
            "logloss": 3.9121316090202054,
            "mae": 0.3770642361439435,
            "precision": 0.6802168021680217,
            "recall": 0.5251046025104602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.7239350150747688,
            "auditor_fn_violation": 0.03657120743034057,
            "auditor_fp_violation": 0.028760260743602124,
            "ave_precision_score": 0.7244337268885859,
            "fpr": 0.125,
            "logloss": 2.20668857166143,
            "mae": 0.335695868275488,
            "precision": 0.7205882352941176,
            "recall": 0.6176470588235294
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7490543954518255,
            "auditor_fn_violation": 0.03992118642900119,
            "auditor_fp_violation": 0.012508143983085867,
            "ave_precision_score": 0.749607825617212,
            "fpr": 0.09659714599341383,
            "logloss": 2.2333105249601055,
            "mae": 0.3221909757068967,
            "precision": 0.7621621621621621,
            "recall": 0.5899581589958159
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6052631578947368,
            "auc_prc": 0.6732254207102624,
            "auditor_fn_violation": 0.06809993734335841,
            "auditor_fp_violation": 0.023904011749557384,
            "ave_precision_score": 0.6651758360785582,
            "fpr": 0.13706140350877194,
            "logloss": 3.3447816210012067,
            "mae": 0.3843071052533929,
            "precision": 0.6584699453551912,
            "recall": 0.5063025210084033
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.6822576528068014,
            "auditor_fn_violation": 0.07216310183760548,
            "auditor_fp_violation": 0.011232992701470101,
            "ave_precision_score": 0.67383877200305,
            "fpr": 0.1141602634467618,
            "logloss": 3.485191948672148,
            "mae": 0.3856738349867397,
            "precision": 0.6838905775075987,
            "recall": 0.4707112970711297
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 12092,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.6733675773939178,
            "auditor_fn_violation": 0.07005795739348372,
            "auditor_fp_violation": 0.02414795589892162,
            "ave_precision_score": 0.6547239788282674,
            "fpr": 0.1600877192982456,
            "logloss": 4.450079110323867,
            "mae": 0.3927566748770961,
            "precision": 0.6430317848410758,
            "recall": 0.5525210084033614
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.6753834320921868,
            "auditor_fn_violation": 0.07309545352249816,
            "auditor_fp_violation": 0.019411199529486927,
            "ave_precision_score": 0.6587165135896318,
            "fpr": 0.13062568605927552,
            "logloss": 4.511585769699593,
            "mae": 0.3896608056556565,
            "precision": 0.6685236768802229,
            "recall": 0.502092050209205
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7047536062206406,
            "auditor_fn_violation": 0.01869563614919652,
            "auditor_fp_violation": 0.03444139304683728,
            "ave_precision_score": 0.7012881241794691,
            "fpr": 0.16337719298245615,
            "logloss": 1.2205462274648757,
            "mae": 0.31148761632910366,
            "precision": 0.7214953271028037,
            "recall": 0.8109243697478992
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.7031060027893019,
            "auditor_fn_violation": 0.004680129886234724,
            "auditor_fp_violation": 0.029254961808838856,
            "ave_precision_score": 0.6984405319312573,
            "fpr": 0.15367727771679474,
            "logloss": 1.5068834817443808,
            "mae": 0.2965564180842521,
            "precision": 0.7343453510436433,
            "recall": 0.8096234309623431
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.7718126967571042,
            "auditor_fn_violation": 0.013609391124871,
            "auditor_fp_violation": 0.018610172219539686,
            "ave_precision_score": 0.7686577285783295,
            "fpr": 0.1600877192982456,
            "logloss": 1.990100666123784,
            "mae": 0.3012976158212894,
            "precision": 0.7114624505928854,
            "recall": 0.7563025210084033
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.7706982621673758,
            "auditor_fn_violation": 0.017246209737793318,
            "auditor_fp_violation": 0.023140319877909966,
            "ave_precision_score": 0.7675695067910712,
            "fpr": 0.14489571899012074,
            "logloss": 1.9110652404835455,
            "mae": 0.2870804163943296,
            "precision": 0.7311608961303462,
            "recall": 0.7510460251046025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.7040474626677967,
            "auditor_fn_violation": 0.03869047619047619,
            "auditor_fp_violation": 0.026089449541284407,
            "ave_precision_score": 0.7041764413204517,
            "fpr": 0.12828947368421054,
            "logloss": 2.3555957177926006,
            "mae": 0.3429997087721379,
            "precision": 0.7111111111111111,
            "recall": 0.6050420168067226
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.7364011967022033,
            "auditor_fn_violation": 0.04321197451878253,
            "auditor_fp_violation": 0.010999764236442965,
            "ave_precision_score": 0.7370305424428807,
            "fpr": 0.10537870472008781,
            "logloss": 2.373993500120803,
            "mae": 0.33074109228640375,
            "precision": 0.7453580901856764,
            "recall": 0.5878661087866108
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.7044580135727275,
            "auditor_fn_violation": 0.055490288220551374,
            "auditor_fp_violation": 0.022166224046354422,
            "ave_precision_score": 0.6930211491692427,
            "fpr": 0.14802631578947367,
            "logloss": 3.3896529953259815,
            "mae": 0.35860157023598704,
            "precision": 0.6770334928229665,
            "recall": 0.5945378151260504
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.6999479231392784,
            "auditor_fn_violation": 0.0523586660481608,
            "auditor_fp_violation": 0.010307684117395043,
            "ave_precision_score": 0.6883448971908301,
            "fpr": 0.1251372118551043,
            "logloss": 3.5131443517259644,
            "mae": 0.35195695872671134,
            "precision": 0.7023498694516971,
            "recall": 0.5627615062761506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7057228296199497,
            "auditor_fn_violation": 0.015194235588972434,
            "auditor_fp_violation": 0.03329711894414937,
            "ave_precision_score": 0.7000837478886777,
            "fpr": 0.20614035087719298,
            "logloss": 1.4325650704779507,
            "mae": 0.31080410953095733,
            "precision": 0.6845637583892618,
            "recall": 0.8571428571428571
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7037905414254095,
            "auditor_fn_violation": 0.0026523797932292,
            "auditor_fp_violation": 0.029353830397274277,
            "ave_precision_score": 0.6985367884352338,
            "fpr": 0.2052689352360044,
            "logloss": 1.6455962816910892,
            "mae": 0.29746797329733937,
            "precision": 0.6872909698996655,
            "recall": 0.8598326359832636
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6118421052631579,
            "auc_prc": 0.6758435046174256,
            "auditor_fn_violation": 0.06590695488721805,
            "auditor_fp_violation": 0.025460727506840503,
            "ave_precision_score": 0.6617793610889241,
            "fpr": 0.1611842105263158,
            "logloss": 3.7813966354313036,
            "mae": 0.3826297967819241,
            "precision": 0.6466346153846154,
            "recall": 0.5651260504201681
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.6849500775099537,
            "auditor_fn_violation": 0.06589384050815465,
            "auditor_fp_violation": 0.010490210742198892,
            "ave_precision_score": 0.6718085265256948,
            "fpr": 0.12843029637760703,
            "logloss": 3.720572714717559,
            "mae": 0.37256774258921777,
            "precision": 0.6863270777479893,
            "recall": 0.5355648535564853
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.773440454437324,
            "auditor_fn_violation": 0.010107990564646917,
            "auditor_fp_violation": 0.01687992918075004,
            "ave_precision_score": 0.7581420331337151,
            "fpr": 0.2138157894736842,
            "logloss": 2.3382949668039457,
            "mae": 0.31391357456219904,
            "precision": 0.6655231560891939,
            "recall": 0.8151260504201681
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.7896570053151247,
            "auditor_fn_violation": 0.005853607006875523,
            "auditor_fp_violation": 0.02946537444576551,
            "ave_precision_score": 0.7718220421494959,
            "fpr": 0.1942919868276619,
            "logloss": 1.9850166574784582,
            "mae": 0.2852650827920776,
            "precision": 0.705,
            "recall": 0.8849372384937239
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5866228070175439,
            "auc_prc": 0.6894955458360679,
            "auditor_fn_violation": 0.07503132832080202,
            "auditor_fp_violation": 0.025767543859649127,
            "ave_precision_score": 0.6898116777484058,
            "fpr": 0.11951754385964912,
            "logloss": 2.760791494701669,
            "mae": 0.3989065585497243,
            "precision": 0.6561514195583596,
            "recall": 0.4369747899159664
        },
        "train": {
            "accuracy": 0.601536772777168,
            "auc_prc": 0.7099875892721268,
            "auditor_fn_violation": 0.07114348570929917,
            "auditor_fp_violation": 0.015152244950730491,
            "ave_precision_score": 0.7106620884671759,
            "fpr": 0.09110867178924259,
            "logloss": 2.824902229206293,
            "mae": 0.3874050664175015,
            "precision": 0.7046263345195729,
            "recall": 0.41422594142259417
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 12092,
        "test": {
            "accuracy": 0.543859649122807,
            "auc_prc": 0.610073505958513,
            "auditor_fn_violation": 0.024026057791537672,
            "auditor_fp_violation": 0.010949822951875103,
            "ave_precision_score": 0.5761910850024843,
            "fpr": 0.1600877192982456,
            "logloss": 8.822269486371418,
            "mae": 0.4542432332134977,
            "precision": 0.5852272727272727,
            "recall": 0.4327731092436975
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.6180480524122544,
            "auditor_fn_violation": 0.017402367162849243,
            "auditor_fp_violation": 0.01718792383569562,
            "ave_precision_score": 0.5952207302743384,
            "fpr": 0.150384193194292,
            "logloss": 8.233404079355838,
            "mae": 0.4584278397101453,
            "precision": 0.6028985507246377,
            "recall": 0.4351464435146444
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8316620016921477,
            "auditor_fn_violation": 0.015028379772961823,
            "auditor_fp_violation": 0.016593231933043632,
            "ave_precision_score": 0.8321501596194233,
            "fpr": 0.13048245614035087,
            "logloss": 1.736668247949408,
            "mae": 0.26729482041546393,
            "precision": 0.7546391752577319,
            "recall": 0.7689075630252101
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.860904205991586,
            "auditor_fn_violation": 0.00465946199174203,
            "auditor_fp_violation": 0.02307947766964203,
            "ave_precision_score": 0.8610550713117122,
            "fpr": 0.12623490669593854,
            "logloss": 1.7057249373311405,
            "mae": 0.25736962819275555,
            "precision": 0.7633744855967078,
            "recall": 0.7761506276150628
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 12092,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7704881172719801,
            "auditor_fn_violation": 0.012365472504791392,
            "auditor_fp_violation": 0.018972316111379367,
            "ave_precision_score": 0.7673453882393388,
            "fpr": 0.1600877192982456,
            "logloss": 2.049295847696117,
            "mae": 0.30241254173203297,
            "precision": 0.7091633466135459,
            "recall": 0.7478991596638656
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7700825585407197,
            "auditor_fn_violation": 0.017880025168902632,
            "auditor_fp_violation": 0.02319862699416676,
            "ave_precision_score": 0.7670530278851528,
            "fpr": 0.14709110867178923,
            "logloss": 1.9962191297949323,
            "mae": 0.28867178246712644,
            "precision": 0.7276422764227642,
            "recall": 0.7489539748953975
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.6746270569812396,
            "auditor_fn_violation": 0.06860902255639098,
            "auditor_fp_violation": 0.025018107194591986,
            "ave_precision_score": 0.6566116192095774,
            "fpr": 0.16447368421052633,
            "logloss": 4.208248037208704,
            "mae": 0.39020411176172953,
            "precision": 0.6411483253588517,
            "recall": 0.5630252100840336
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.6735711267624865,
            "auditor_fn_violation": 0.06884705298788862,
            "auditor_fp_violation": 0.014310594403023864,
            "ave_precision_score": 0.6572090245304838,
            "fpr": 0.132821075740944,
            "logloss": 4.280314531288453,
            "mae": 0.38687788779798676,
            "precision": 0.6711956521739131,
            "recall": 0.5167364016736402
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5953947368421053,
            "auc_prc": 0.609396056633071,
            "auditor_fn_violation": 0.00954592363261094,
            "auditor_fp_violation": 0.021044583936906502,
            "ave_precision_score": 0.6017925649594927,
            "fpr": 0.3125,
            "logloss": 2.047587237681174,
            "mae": 0.4190194940761023,
            "precision": 0.5790251107828656,
            "recall": 0.8235294117647058
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.6105221620760015,
            "auditor_fn_violation": 0.007729792540267949,
            "auditor_fp_violation": 0.01354753170766334,
            "ave_precision_score": 0.6048628573354473,
            "fpr": 0.3040614709110867,
            "logloss": 1.9171003217411522,
            "mae": 0.3960950152275804,
            "precision": 0.5973837209302325,
            "recall": 0.8598326359832636
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5767543859649122,
            "auc_prc": 0.669509760659777,
            "auditor_fn_violation": 0.08211936090225565,
            "auditor_fp_violation": 0.025500965717044907,
            "ave_precision_score": 0.6658159858650815,
            "fpr": 0.12828947368421054,
            "logloss": 3.0839821259951568,
            "mae": 0.4101553958150166,
            "precision": 0.6388888888888888,
            "recall": 0.43487394957983194
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.689605723291815,
            "auditor_fn_violation": 0.08212043411764167,
            "auditor_fp_violation": 0.01654147537284866,
            "ave_precision_score": 0.6857074879343623,
            "fpr": 0.10098792535675083,
            "logloss": 3.2455304358345787,
            "mae": 0.40253889683764793,
            "precision": 0.6827586206896552,
            "recall": 0.41422594142259417
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7620768012036524,
            "auditor_fn_violation": 0.015134343210968605,
            "auditor_fp_violation": 0.01867052953484629,
            "ave_precision_score": 0.7564091965094832,
            "fpr": 0.16776315789473684,
            "logloss": 4.90126841383788,
            "mae": 0.30824921603486405,
            "precision": 0.6927710843373494,
            "recall": 0.7247899159663865
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.7595913886201088,
            "auditor_fn_violation": 0.007771128329253336,
            "auditor_fp_violation": 0.024661375084608694,
            "ave_precision_score": 0.7542399033780052,
            "fpr": 0.1734357848518112,
            "logloss": 5.2394911443669985,
            "mae": 0.3210990057238015,
            "precision": 0.680161943319838,
            "recall": 0.702928870292887
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5997807017543859,
            "auc_prc": 0.5937779760892814,
            "auditor_fn_violation": 0.013715354562877785,
            "auditor_fp_violation": 0.008226199098664112,
            "ave_precision_score": 0.5832497246338665,
            "fpr": 0.3190789473684211,
            "logloss": 2.315666918180922,
            "mae": 0.4172654400497681,
            "precision": 0.5800865800865801,
            "recall": 0.8445378151260504
        },
        "train": {
            "accuracy": 0.6487376509330406,
            "auc_prc": 0.6036413868241144,
            "auditor_fn_violation": 0.0039016391936765454,
            "auditor_fp_violation": 0.016404580404245788,
            "ave_precision_score": 0.5958823454804181,
            "fpr": 0.28869374313940727,
            "logloss": 2.1475057639420525,
            "mae": 0.379591267471995,
            "precision": 0.6154970760233918,
            "recall": 0.8807531380753139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 12092,
        "test": {
            "accuracy": 0.5932017543859649,
            "auc_prc": 0.7704481283227694,
            "auditor_fn_violation": 0.006514447884416926,
            "auditor_fp_violation": 0.026099509093835526,
            "ave_precision_score": 0.5635930508551666,
            "fpr": 0.38377192982456143,
            "logloss": 13.861932850977638,
            "mae": 0.4068682479555935,
            "precision": 0.5652173913043478,
            "recall": 0.9558823529411765
        },
        "train": {
            "accuracy": 0.5982436882546652,
            "auc_prc": 0.7754643083351902,
            "auditor_fn_violation": 0.006765290797275512,
            "auditor_fp_violation": 0.025254586615221217,
            "ave_precision_score": 0.5705190591453171,
            "fpr": 0.37980241492864986,
            "logloss": 13.679245939559744,
            "mae": 0.40205652357679755,
            "precision": 0.5696517412935324,
            "recall": 0.9581589958158996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 12092,
        "test": {
            "accuracy": 0.6096491228070176,
            "auc_prc": 0.6808767232921665,
            "auditor_fn_violation": 0.06903978696741855,
            "auditor_fp_violation": 0.026486801867052955,
            "ave_precision_score": 0.6608991471089815,
            "fpr": 0.15899122807017543,
            "logloss": 4.404472793629738,
            "mae": 0.3895807410487637,
            "precision": 0.6463414634146342,
            "recall": 0.5567226890756303
        },
        "train": {
            "accuracy": 0.6190998902305159,
            "auc_prc": 0.6825518842888043,
            "auditor_fn_violation": 0.06949464701532639,
            "auditor_fp_violation": 0.017575792913403793,
            "ave_precision_score": 0.6651485895953748,
            "fpr": 0.12843029637760703,
            "logloss": 4.4378278405000575,
            "mae": 0.38241228996342036,
            "precision": 0.6794520547945205,
            "recall": 0.5188284518828452
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 15860,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8234240107087503,
            "auditor_fn_violation": 0.006793177797434767,
            "auditor_fp_violation": 0.014709580717849677,
            "ave_precision_score": 0.821521206872603,
            "fpr": 0.18530701754385964,
            "logloss": 1.52859845690883,
            "mae": 0.2658433858679603,
            "precision": 0.7086206896551724,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8283891287694742,
            "auditor_fn_violation": 0.005869682035925394,
            "auditor_fp_violation": 0.01310135551369837,
            "ave_precision_score": 0.8263624150737228,
            "fpr": 0.16136114160263446,
            "logloss": 1.3607688492782901,
            "mae": 0.238918463773956,
            "precision": 0.7379679144385026,
            "recall": 0.8661087866108786
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8274928459668963,
            "auditor_fn_violation": 0.003395437122217313,
            "auditor_fp_violation": 0.019877675840978597,
            "ave_precision_score": 0.8244804214182837,
            "fpr": 0.18530701754385964,
            "logloss": 1.672821371550423,
            "mae": 0.265190217478157,
            "precision": 0.7065972222222222,
            "recall": 0.8550420168067226
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8292110370960446,
            "auditor_fn_violation": 0.0050659305834317,
            "auditor_fp_violation": 0.020693956087136187,
            "ave_precision_score": 0.8248719906797813,
            "fpr": 0.15587266739846323,
            "logloss": 1.5211151059221089,
            "mae": 0.23709337291498517,
            "precision": 0.7436823104693141,
            "recall": 0.8619246861924686
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8318089730837066,
            "auditor_fn_violation": 0.0033539731682146576,
            "auditor_fp_violation": 0.017025792692741033,
            "ave_precision_score": 0.8289833480645945,
            "fpr": 0.18092105263157895,
            "logloss": 1.5971491787412373,
            "mae": 0.26232967831449083,
            "precision": 0.7135416666666666,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8339760430742966,
            "auditor_fn_violation": 0.0045377510575072685,
            "auditor_fp_violation": 0.016881177702344707,
            "ave_precision_score": 0.8306768158896939,
            "fpr": 0.1602634467618002,
            "logloss": 1.4337381881969964,
            "mae": 0.23485571572985964,
            "precision": 0.7392857142857143,
            "recall": 0.8661087866108786
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7719298245614035,
            "auc_prc": 0.8694072015576055,
            "auditor_fn_violation": 0.0004514963880288972,
            "auditor_fp_violation": 0.012433606953162727,
            "ave_precision_score": 0.8695082834685443,
            "fpr": 0.10526315789473684,
            "logloss": 1.4089738687049251,
            "mae": 0.23585998987441278,
            "precision": 0.7913043478260869,
            "recall": 0.7647058823529411
        },
        "train": {
            "accuracy": 0.7870472008781558,
            "auc_prc": 0.8696357400234214,
            "auditor_fn_violation": 0.01347087434379435,
            "auditor_fp_violation": 0.017667056225805715,
            "ave_precision_score": 0.869800511456756,
            "fpr": 0.09330406147091108,
            "logloss": 1.2052044434231175,
            "mae": 0.21993252753876194,
            "precision": 0.8127753303964758,
            "recall": 0.7719665271966527
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7850877192982456,
            "auc_prc": 0.8771182916800306,
            "auditor_fn_violation": 0.007620153324487692,
            "auditor_fp_violation": 0.006458232737807827,
            "ave_precision_score": 0.8766621280576276,
            "fpr": 0.10087719298245613,
            "logloss": 1.0445068363387682,
            "mae": 0.22054506508513116,
            "precision": 0.8017241379310345,
            "recall": 0.7815126050420168
        },
        "train": {
            "accuracy": 0.7958287596048299,
            "auc_prc": 0.874875731624444,
            "auditor_fn_violation": 0.006815812317146549,
            "auditor_fp_violation": 0.01152199319074286,
            "ave_precision_score": 0.8751079494436756,
            "fpr": 0.0801317233809001,
            "logloss": 1.054796068656654,
            "mae": 0.21167796948759682,
            "precision": 0.8333333333333334,
            "recall": 0.7635983263598326
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.821244294573717,
            "auditor_fn_violation": 0.004653177060297807,
            "auditor_fp_violation": 0.014893167551907298,
            "ave_precision_score": 0.8190229662003278,
            "fpr": 0.19078947368421054,
            "logloss": 1.6013798724533272,
            "mae": 0.2688239627836295,
            "precision": 0.7025641025641025,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8242094314066523,
            "auditor_fn_violation": 0.004236918371002485,
            "auditor_fp_violation": 0.012708416251967872,
            "ave_precision_score": 0.8215358082731814,
            "fpr": 0.16136114160263446,
            "logloss": 1.4365489619548037,
            "mae": 0.23864103926381833,
            "precision": 0.7375,
            "recall": 0.8640167364016736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7916666666666666,
            "auc_prc": 0.8769230685524458,
            "auditor_fn_violation": 0.0017599144921126354,
            "auditor_fp_violation": 0.0038628681796233732,
            "ave_precision_score": 0.8765442659558971,
            "fpr": 0.09868421052631579,
            "logloss": 1.0590842728545862,
            "mae": 0.21975745967324145,
            "precision": 0.8068669527896996,
            "recall": 0.7899159663865546
        },
        "train": {
            "accuracy": 0.7958287596048299,
            "auc_prc": 0.877895630300422,
            "auditor_fn_violation": 0.010522254729503192,
            "auditor_fp_violation": 0.013626119560009439,
            "ave_precision_score": 0.8781060947945276,
            "fpr": 0.08562019758507135,
            "logloss": 1.023789384316581,
            "mae": 0.21372522754608714,
            "precision": 0.8258928571428571,
            "recall": 0.7740585774058577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8225267663714895,
            "auditor_fn_violation": 0.004800604452307243,
            "auditor_fp_violation": 0.015277945436986963,
            "ave_precision_score": 0.8217445523478771,
            "fpr": 0.18092105263157895,
            "logloss": 2.0830809887896224,
            "mae": 0.280187911925009,
            "precision": 0.701627486437613,
            "recall": 0.8151260504201681
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8276214228181547,
            "auditor_fn_violation": 0.0028200193818921694,
            "auditor_fp_violation": 0.02052917510641049,
            "ave_precision_score": 0.8273694539154888,
            "fpr": 0.14709110867178923,
            "logloss": 1.698881762266651,
            "mae": 0.24304300016832966,
            "precision": 0.7462121212121212,
            "recall": 0.8242677824267782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7927631578947368,
            "auc_prc": 0.880470428691781,
            "auditor_fn_violation": 0.006102111897390535,
            "auditor_fp_violation": 0.007876629647513282,
            "ave_precision_score": 0.8801093298106735,
            "fpr": 0.09429824561403509,
            "logloss": 0.9369540227291672,
            "mae": 0.21663831397878885,
            "precision": 0.8126361655773421,
            "recall": 0.7836134453781513
        },
        "train": {
            "accuracy": 0.7958287596048299,
            "auc_prc": 0.8785693709465087,
            "auditor_fn_violation": 0.012871505403506197,
            "auditor_fp_violation": 0.008109759343715384,
            "ave_precision_score": 0.8787823009471579,
            "fpr": 0.0845225027442371,
            "logloss": 0.8848555915449015,
            "mae": 0.21470689010144295,
            "precision": 0.827354260089686,
            "recall": 0.7719665271966527
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8674166449619143,
            "auditor_fn_violation": 0.007479636591478696,
            "auditor_fp_violation": 0.006910912602607444,
            "ave_precision_score": 0.865426020749274,
            "fpr": 0.14473684210526316,
            "logloss": 1.4073234805563921,
            "mae": 0.23979664146168886,
            "precision": 0.7476099426386233,
            "recall": 0.8214285714285714
        },
        "train": {
            "accuracy": 0.7848518111964874,
            "auc_prc": 0.8606681253231833,
            "auditor_fn_violation": 0.004498711701243288,
            "auditor_fp_violation": 0.014049479925873912,
            "ave_precision_score": 0.8570954781378599,
            "fpr": 0.12403951701427003,
            "logloss": 1.3276789057096525,
            "mae": 0.2204478425664659,
            "precision": 0.7775590551181102,
            "recall": 0.8263598326359832
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 15860,
        "test": {
            "accuracy": 0.49890350877192985,
            "auc_prc": 0.5331375159399036,
            "auditor_fn_violation": 0.00681160622143595,
            "auditor_fp_violation": 0.006055850635763722,
            "ave_precision_score": 0.5498434202839587,
            "fpr": 0.02850877192982456,
            "logloss": 14.341009889240082,
            "mae": 0.5030472360437476,
            "precision": 0.6338028169014085,
            "recall": 0.09453781512605042
        },
        "train": {
            "accuracy": 0.5093304061470911,
            "auc_prc": 0.5456319641826116,
            "auditor_fn_violation": 0.008896380362744531,
            "auditor_fp_violation": 0.0028240925004373043,
            "ave_precision_score": 0.5509527080127139,
            "fpr": 0.01756311745334797,
            "logloss": 14.45454420721696,
            "mae": 0.5007824196843899,
            "precision": 0.746031746031746,
            "recall": 0.09832635983263599
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 15860,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8234162410395107,
            "auditor_fn_violation": 0.004975674480318446,
            "auditor_fp_violation": 0.015199983904715924,
            "ave_precision_score": 0.8210264921937553,
            "fpr": 0.18421052631578946,
            "logloss": 1.5648040034633726,
            "mae": 0.26447579768755436,
            "precision": 0.7093425605536332,
            "recall": 0.8613445378151261
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8293631342212754,
            "auditor_fn_violation": 0.005869682035925394,
            "auditor_fp_violation": 0.011418054418285112,
            "ave_precision_score": 0.8267607063894159,
            "fpr": 0.1602634467618002,
            "logloss": 1.402180595885278,
            "mae": 0.23743643466678496,
            "precision": 0.7392857142857143,
            "recall": 0.8661087866108786
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7916666666666666,
            "auc_prc": 0.8324734231181619,
            "auditor_fn_violation": 0.005468634822349999,
            "auditor_fp_violation": 0.002864457588926445,
            "ave_precision_score": 0.8123262246880867,
            "fpr": 0.09978070175438597,
            "logloss": 1.8673870891173412,
            "mae": 0.21746380232536358,
            "precision": 0.8055555555555556,
            "recall": 0.792016806722689
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8371253608397646,
            "auditor_fn_violation": 0.018449540483812445,
            "auditor_fp_violation": 0.01435369096721366,
            "ave_precision_score": 0.8176449101190213,
            "fpr": 0.09989023051591657,
            "logloss": 1.8512023062107645,
            "mae": 0.21901326359230183,
            "precision": 0.8013100436681223,
            "recall": 0.7677824267782427
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8297916784066428,
            "auditor_fn_violation": 0.003059118384195785,
            "auditor_fp_violation": 0.0174835023338162,
            "ave_precision_score": 0.8271587329726386,
            "fpr": 0.18421052631578946,
            "logloss": 1.5967062442315183,
            "mae": 0.26498584401945646,
            "precision": 0.7098445595854922,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8299059449565113,
            "auditor_fn_violation": 0.0034905777365440535,
            "auditor_fp_violation": 0.02196910736875195,
            "ave_precision_score": 0.826756415579399,
            "fpr": 0.15697036223929747,
            "logloss": 1.4382792068867387,
            "mae": 0.23628752438583225,
            "precision": 0.7432675044883303,
            "recall": 0.8661087866108786
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.8671729719457174,
            "auditor_fn_violation": 0.008143059855521162,
            "auditor_fp_violation": 0.00883228713986802,
            "ave_precision_score": 0.8665492259450076,
            "fpr": 0.12280701754385964,
            "logloss": 1.1546808592231235,
            "mae": 0.23469381288691657,
            "precision": 0.7746478873239436,
            "recall": 0.8088235294117647
        },
        "train": {
            "accuracy": 0.7859495060373216,
            "auc_prc": 0.8723297709488683,
            "auditor_fn_violation": 0.013149373762796873,
            "auditor_fp_violation": 0.01565419316894107,
            "ave_precision_score": 0.8721164251243854,
            "fpr": 0.10757409440175632,
            "logloss": 1.1156566503272465,
            "mae": 0.22017650200760677,
            "precision": 0.7954070981210856,
            "recall": 0.797071129707113
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8297217412600607,
            "auditor_fn_violation": 0.004482714138286899,
            "auditor_fp_violation": 0.015018911958796082,
            "ave_precision_score": 0.8278864322035641,
            "fpr": 0.18421052631578946,
            "logloss": 1.487951933771931,
            "mae": 0.26350615677204325,
            "precision": 0.7073170731707317,
            "recall": 0.8529411764705882
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8339161862378321,
            "auditor_fn_violation": 0.004487229537636241,
            "auditor_fp_violation": 0.021454483690485547,
            "ave_precision_score": 0.8329287762638528,
            "fpr": 0.150384193194292,
            "logloss": 1.292862687539902,
            "mae": 0.23415850580086117,
            "precision": 0.7490842490842491,
            "recall": 0.8556485355648535
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7883771929824561,
            "auc_prc": 0.8761476444536562,
            "auditor_fn_violation": 0.006420002211410882,
            "auditor_fp_violation": 0.00905359729599228,
            "ave_precision_score": 0.8756259788105082,
            "fpr": 0.10526315789473684,
            "logloss": 1.0257811419455507,
            "mae": 0.22114074996672628,
            "precision": 0.7978947368421052,
            "recall": 0.7962184873949579
        },
        "train": {
            "accuracy": 0.7947310647639956,
            "auc_prc": 0.8767319103986602,
            "auditor_fn_violation": 0.011847296409757084,
            "auditor_fp_violation": 0.012525889627164024,
            "ave_precision_score": 0.8768986080308847,
            "fpr": 0.09220636663007684,
            "logloss": 0.9241881681293986,
            "mae": 0.21144739210048558,
            "precision": 0.8169934640522876,
            "recall": 0.7845188284518828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7927631578947368,
            "auc_prc": 0.8790704004951408,
            "auditor_fn_violation": 0.005574598260356778,
            "auditor_fp_violation": 0.004836129888942543,
            "ave_precision_score": 0.878721872723476,
            "fpr": 0.10416666666666667,
            "logloss": 1.0940472848720564,
            "mae": 0.21808143637858746,
            "precision": 0.80083857442348,
            "recall": 0.8025210084033614
        },
        "train": {
            "accuracy": 0.8057080131723381,
            "auc_prc": 0.8789532126928629,
            "auditor_fn_violation": 0.011004505600999413,
            "auditor_fp_violation": 0.013050653673475084,
            "ave_precision_score": 0.8791318066679297,
            "fpr": 0.0867178924259056,
            "logloss": 1.0476798495841975,
            "mae": 0.20614416544645559,
            "precision": 0.8278867102396514,
            "recall": 0.7949790794979079
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7763157894736842,
            "auc_prc": 0.8688533965020918,
            "auditor_fn_violation": 0.00876271561256082,
            "auditor_fp_violation": 0.008198535329148562,
            "ave_precision_score": 0.8689006474269823,
            "fpr": 0.08991228070175439,
            "logloss": 1.7490677693462136,
            "mae": 0.23310720682109673,
            "precision": 0.8119266055045872,
            "recall": 0.7436974789915967
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8661892114725454,
            "auditor_fn_violation": 0.021411938694432076,
            "auditor_fp_violation": 0.0179687321751343,
            "ave_precision_score": 0.8662662749347472,
            "fpr": 0.0801317233809001,
            "logloss": 1.5016377255114775,
            "mae": 0.22365015579149972,
            "precision": 0.8282352941176471,
            "recall": 0.7364016736401674
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 15860,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8237394420760517,
            "auditor_fn_violation": 0.0033539731682146576,
            "auditor_fp_violation": 0.013230826492837607,
            "ave_precision_score": 0.8211480462179077,
            "fpr": 0.18530701754385964,
            "logloss": 1.6206211647061122,
            "mae": 0.2655502704650188,
            "precision": 0.7086206896551724,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8293146731413534,
            "auditor_fn_violation": 0.004021053695189897,
            "auditor_fp_violation": 0.02184235276819373,
            "ave_precision_score": 0.8261807288674319,
            "fpr": 0.16465422612513722,
            "logloss": 1.4456488679002175,
            "mae": 0.23705752251338102,
            "precision": 0.7345132743362832,
            "recall": 0.8682008368200836
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8342464383422169,
            "auditor_fn_violation": 0.00360275689223058,
            "auditor_fp_violation": 0.01741811524223403,
            "ave_precision_score": 0.8321159473506539,
            "fpr": 0.17653508771929824,
            "logloss": 1.4884765808895155,
            "mae": 0.2606770894988209,
            "precision": 0.7170474516695958,
            "recall": 0.8571428571428571
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.838392138812925,
            "auditor_fn_violation": 0.004372407901565709,
            "auditor_fp_violation": 0.013134311709843511,
            "ave_precision_score": 0.8372178791018027,
            "fpr": 0.14928649835345773,
            "logloss": 1.290921048229207,
            "mae": 0.23203486425070427,
            "precision": 0.7509157509157509,
            "recall": 0.8577405857740585
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7927631578947368,
            "auc_prc": 0.832358398974906,
            "auditor_fn_violation": 0.005056298835323606,
            "auditor_fp_violation": 0.0023992032834379527,
            "ave_precision_score": 0.8122145237273707,
            "fpr": 0.09868421052631579,
            "logloss": 1.869632483883178,
            "mae": 0.21753320504928828,
            "precision": 0.8072805139186295,
            "recall": 0.792016806722689
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8369598284268128,
            "auditor_fn_violation": 0.017778982129160563,
            "auditor_fp_violation": 0.011732405827669518,
            "ave_precision_score": 0.8174710329299052,
            "fpr": 0.09879253567508232,
            "logloss": 1.855030524743344,
            "mae": 0.2189789081463917,
            "precision": 0.8034934497816594,
            "recall": 0.7698744769874477
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8232027127617286,
            "auditor_fn_violation": 0.00030176544301931254,
            "auditor_fp_violation": 0.01619587960727507,
            "ave_precision_score": 0.8226047953177933,
            "fpr": 0.18092105263157895,
            "logloss": 1.9517234437382431,
            "mae": 0.2746101723419676,
            "precision": 0.7043010752688172,
            "recall": 0.8256302521008403
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8273803769628051,
            "auditor_fn_violation": 0.005426470520693158,
            "auditor_fp_violation": 0.01692680935854567,
            "ave_precision_score": 0.8263604038455756,
            "fpr": 0.14818880351262348,
            "logloss": 1.6196861765393202,
            "mae": 0.2390924641126081,
            "precision": 0.7481343283582089,
            "recall": 0.8389121338912134
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7817982456140351,
            "auc_prc": 0.8644355234676988,
            "auditor_fn_violation": 0.0024671052631578985,
            "auditor_fp_violation": 0.008731691614356997,
            "ave_precision_score": 0.8645760368169944,
            "fpr": 0.08771929824561403,
            "logloss": 1.4458783235456643,
            "mae": 0.2314224405037434,
            "precision": 0.816933638443936,
            "recall": 0.75
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8614799657806372,
            "auditor_fn_violation": 0.01433892591248755,
            "auditor_fp_violation": 0.013846672564980747,
            "ave_precision_score": 0.86158801752532,
            "fpr": 0.08342480790340286,
            "logloss": 1.5622705183930106,
            "mae": 0.23535570775189302,
            "precision": 0.8207547169811321,
            "recall": 0.7280334728033473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7905701754385965,
            "auc_prc": 0.865955484013263,
            "auditor_fn_violation": 0.010918841220698807,
            "auditor_fp_violation": 0.013811765652663767,
            "ave_precision_score": 0.860933344947091,
            "fpr": 0.10855263157894737,
            "logloss": 1.2649150354676812,
            "mae": 0.21650014732577735,
            "precision": 0.7950310559006211,
            "recall": 0.8067226890756303
        },
        "train": {
            "accuracy": 0.7947310647639956,
            "auc_prc": 0.8548974975573477,
            "auditor_fn_violation": 0.011890928631463887,
            "auditor_fp_violation": 0.018387022356976444,
            "ave_precision_score": 0.8497143298115015,
            "fpr": 0.09989023051591657,
            "logloss": 1.3152232356474027,
            "mae": 0.21068865983356044,
            "precision": 0.8076109936575053,
            "recall": 0.799163179916318
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.82320538407051,
            "auditor_fn_violation": 0.0033539731682146576,
            "auditor_fp_violation": 0.01448072589731209,
            "ave_precision_score": 0.8203567022786848,
            "fpr": 0.18640350877192982,
            "logloss": 1.6224714421457453,
            "mae": 0.26561693133447395,
            "precision": 0.7074010327022375,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8291738979432175,
            "auditor_fn_violation": 0.004771987195091146,
            "auditor_fp_violation": 0.024367304411313608,
            "ave_precision_score": 0.8260329527650934,
            "fpr": 0.16355653128430298,
            "logloss": 1.4472030218746217,
            "mae": 0.2370738223860375,
            "precision": 0.7353463587921847,
            "recall": 0.8661087866108786
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8330446542944294,
            "auditor_fn_violation": 0.005830292643373142,
            "auditor_fp_violation": 0.018129828585224536,
            "ave_precision_score": 0.8324869657579195,
            "fpr": 0.19188596491228072,
            "logloss": 1.7728314362943611,
            "mae": 0.28009155523190044,
            "precision": 0.6961805555555556,
            "recall": 0.842436974789916
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.835077808664025,
            "auditor_fn_violation": 0.006604540506776777,
            "auditor_fp_violation": 0.014622410720397095,
            "ave_precision_score": 0.8337402863260773,
            "fpr": 0.1602634467618002,
            "logloss": 1.4216751792096538,
            "mae": 0.24818740613729184,
            "precision": 0.736936936936937,
            "recall": 0.8556485355648535
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8555056878056466,
            "auditor_fn_violation": 0.00550549167035235,
            "auditor_fp_violation": 0.015999718332528577,
            "ave_precision_score": 0.8536608993978326,
            "fpr": 0.16447368421052633,
            "logloss": 1.249996887746415,
            "mae": 0.25609891078844055,
            "precision": 0.7277676950998185,
            "recall": 0.842436974789916
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8511972278893045,
            "auditor_fn_violation": 0.0016258743667586773,
            "auditor_fp_violation": 0.021391106390206437,
            "ave_precision_score": 0.8491949271187713,
            "fpr": 0.1525795828759605,
            "logloss": 1.1392205879863457,
            "mae": 0.23717433059802848,
            "precision": 0.7454212454212454,
            "recall": 0.8514644351464435
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8318503845921421,
            "auditor_fn_violation": 0.0033539731682146576,
            "auditor_fp_violation": 0.017025792692741033,
            "ave_precision_score": 0.8289910380894043,
            "fpr": 0.18092105263157895,
            "logloss": 1.5965993307427193,
            "mae": 0.2623208007500191,
            "precision": 0.7135416666666666,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8340200612158193,
            "auditor_fn_violation": 0.0045377510575072685,
            "auditor_fp_violation": 0.016881177702344707,
            "ave_precision_score": 0.8307637090494018,
            "fpr": 0.1602634467618002,
            "logloss": 1.4333489265667834,
            "mae": 0.23484711496961652,
            "precision": 0.7392857142857143,
            "recall": 0.8661087866108786
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 15860,
        "test": {
            "accuracy": 0.8015350877192983,
            "auc_prc": 0.8829929714818687,
            "auditor_fn_violation": 0.00841027200353826,
            "auditor_fp_violation": 0.008198535329148564,
            "ave_precision_score": 0.8826697052610919,
            "fpr": 0.09429824561403509,
            "logloss": 0.9289769024760356,
            "mae": 0.21333048238106067,
            "precision": 0.815845824411135,
            "recall": 0.8004201680672269
        },
        "train": {
            "accuracy": 0.7980241492864983,
            "auc_prc": 0.8806836795279311,
            "auditor_fn_violation": 0.011220370276812002,
            "auditor_fp_violation": 0.013973427165538973,
            "ave_precision_score": 0.8808616018289902,
            "fpr": 0.09440175631174534,
            "logloss": 0.8536220357600833,
            "mae": 0.21248866800283064,
            "precision": 0.8154506437768241,
            "recall": 0.7949790794979079
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.7555872403545714,
            "auditor_fn_violation": 0.00733681630546956,
            "auditor_fp_violation": 0.009254788347014329,
            "ave_precision_score": 0.7460075789460949,
            "fpr": 0.16666666666666666,
            "logloss": 2.025112269733664,
            "mae": 0.26193758540065876,
            "precision": 0.7241379310344828,
            "recall": 0.8382352941176471
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.7726926675638937,
            "auditor_fn_violation": 0.012187168452525847,
            "auditor_fp_violation": 0.012563916007331487,
            "ave_precision_score": 0.7662111902964462,
            "fpr": 0.14928649835345773,
            "logloss": 1.7259354243716927,
            "mae": 0.24578440530879828,
            "precision": 0.7457943925233644,
            "recall": 0.8347280334728033
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8457301308775057,
            "auditor_fn_violation": 0.0037041132242370644,
            "auditor_fp_violation": 0.014930890873973927,
            "ave_precision_score": 0.8426889404076044,
            "fpr": 0.16557017543859648,
            "logloss": 1.5590194409516909,
            "mae": 0.2553731093573254,
            "precision": 0.7259528130671506,
            "recall": 0.8403361344537815
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8432294341163806,
            "auditor_fn_violation": 0.0052197915757662096,
            "auditor_fp_violation": 0.018919391679321006,
            "ave_precision_score": 0.8385403462274093,
            "fpr": 0.14050493962678376,
            "logloss": 1.4322640259339707,
            "mae": 0.22992330670079383,
            "precision": 0.7589453860640302,
            "recall": 0.8430962343096234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8298281889514625,
            "auditor_fn_violation": 0.0033539731682146576,
            "auditor_fp_violation": 0.014847899565427331,
            "ave_precision_score": 0.8274073516869678,
            "fpr": 0.18421052631578946,
            "logloss": 1.5926399435112337,
            "mae": 0.26346861642641695,
            "precision": 0.7098445595854922,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8321617768965504,
            "auditor_fn_violation": 0.0047375407042699914,
            "auditor_fp_violation": 0.016412185680279263,
            "ave_precision_score": 0.8290034067864624,
            "fpr": 0.15916575192096596,
            "logloss": 1.4288906103559027,
            "mae": 0.23532033260932972,
            "precision": 0.7401433691756273,
            "recall": 0.8640167364016736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8257294594733373,
            "auditor_fn_violation": 0.005853328173374612,
            "auditor_fp_violation": 0.012966763238371159,
            "ave_precision_score": 0.8235442459112602,
            "fpr": 0.18092105263157895,
            "logloss": 1.5838260310874503,
            "mae": 0.2638371267862605,
            "precision": 0.7125435540069687,
            "recall": 0.8592436974789915
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8282069236298887,
            "auditor_fn_violation": 0.003954457146268988,
            "auditor_fp_violation": 0.01225463478196942,
            "ave_precision_score": 0.8256190458640111,
            "fpr": 0.15587266739846323,
            "logloss": 1.421204543684329,
            "mae": 0.23559925463789233,
            "precision": 0.7432188065099458,
            "recall": 0.8598326359832636
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8297677729361527,
            "auditor_fn_violation": 0.003059118384195785,
            "auditor_fp_violation": 0.0174835023338162,
            "ave_precision_score": 0.8271369558428664,
            "fpr": 0.18421052631578946,
            "logloss": 1.5967843178851304,
            "mae": 0.2649924239203715,
            "precision": 0.7098445595854922,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8299359598020698,
            "auditor_fn_violation": 0.0034905777365440535,
            "auditor_fp_violation": 0.02196910736875195,
            "ave_precision_score": 0.8268293740378575,
            "fpr": 0.15697036223929747,
            "logloss": 1.4383044258334,
            "mae": 0.2362930516656323,
            "precision": 0.7432675044883303,
            "recall": 0.8661087866108786
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.836071603706189,
            "auditor_fn_violation": 0.006871498599439784,
            "auditor_fp_violation": 0.00928999678094318,
            "ave_precision_score": 0.836317271819979,
            "fpr": 0.15899122807017543,
            "logloss": 0.8512190783279789,
            "mae": 0.28355793449838784,
            "precision": 0.7222222222222222,
            "recall": 0.792016806722689
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8404731223048236,
            "auditor_fn_violation": 0.012935805519705695,
            "auditor_fp_violation": 0.022549643439308632,
            "ave_precision_score": 0.8407401252443109,
            "fpr": 0.14489571899012074,
            "logloss": 0.856272983080463,
            "mae": 0.2720036628604472,
            "precision": 0.7446808510638298,
            "recall": 0.805439330543933
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8349541912414827,
            "auditor_fn_violation": 0.0033539731682146576,
            "auditor_fp_violation": 0.017025792692741033,
            "ave_precision_score": 0.8321732352851099,
            "fpr": 0.18092105263157895,
            "logloss": 1.5787241283546127,
            "mae": 0.26228440796165964,
            "precision": 0.7135416666666666,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8363745361606264,
            "auditor_fn_violation": 0.0045377510575072685,
            "auditor_fp_violation": 0.019591191062279606,
            "ave_precision_score": 0.833529207583704,
            "fpr": 0.15806805708013172,
            "logloss": 1.4074004336833872,
            "mae": 0.2343734428588306,
            "precision": 0.7419354838709677,
            "recall": 0.8661087866108786
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8526990118938024,
            "auditor_fn_violation": 0.005328118089340999,
            "auditor_fp_violation": 0.015039031063898286,
            "ave_precision_score": 0.850151239293182,
            "fpr": 0.16228070175438597,
            "logloss": 1.483998342333421,
            "mae": 0.2511611105228665,
            "precision": 0.7274401473296501,
            "recall": 0.8298319327731093
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8508613160518051,
            "auditor_fn_violation": 0.007936471485194902,
            "auditor_fp_violation": 0.016353878564022484,
            "ave_precision_score": 0.8483567902066778,
            "fpr": 0.13391877058177826,
            "logloss": 1.3312377306604835,
            "mae": 0.2272485345621131,
            "precision": 0.7635658914728682,
            "recall": 0.8242677824267782
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.786821210857652,
            "auditor_fn_violation": 0.0046001953412944125,
            "auditor_fp_violation": 0.0022935779816513767,
            "ave_precision_score": 0.7730017255303759,
            "fpr": 0.15789473684210525,
            "logloss": 1.8169096604440056,
            "mae": 0.24769442933307678,
            "precision": 0.7377049180327869,
            "recall": 0.8508403361344538
        },
        "train": {
            "accuracy": 0.7837541163556532,
            "auc_prc": 0.7949705854924419,
            "auditor_fn_violation": 0.0076149709041974205,
            "auditor_fp_violation": 0.006543072480815695,
            "ave_precision_score": 0.782349709356968,
            "fpr": 0.1437980241492865,
            "logloss": 1.665921959993069,
            "mae": 0.22909050355652844,
            "precision": 0.7587476979742173,
            "recall": 0.8619246861924686
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8422580079364925,
            "auditor_fn_violation": 0.006422305764411027,
            "auditor_fp_violation": 0.016424734427812657,
            "ave_precision_score": 0.8396485021748028,
            "fpr": 0.17214912280701755,
            "logloss": 1.5184733469394018,
            "mae": 0.25745857276506545,
            "precision": 0.7221238938053097,
            "recall": 0.8571428571428571
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8446568316072355,
            "auditor_fn_violation": 0.008048996688544018,
            "auditor_fp_violation": 0.018239987020328904,
            "ave_precision_score": 0.8426685931456468,
            "fpr": 0.1437980241492865,
            "logloss": 1.3353620396331316,
            "mae": 0.23108228034840075,
            "precision": 0.7565055762081785,
            "recall": 0.8514644351464435
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8200061154698939,
            "auditor_fn_violation": 0.009456085065605193,
            "auditor_fp_violation": 0.015448957830355714,
            "ave_precision_score": 0.8193671888892826,
            "fpr": 0.18530701754385964,
            "logloss": 1.9283284019521696,
            "mae": 0.2767476285779756,
            "precision": 0.699288256227758,
            "recall": 0.8256302521008403
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8232728111400907,
            "auditor_fn_violation": 0.0037661496631133237,
            "auditor_fp_violation": 0.019644427994514067,
            "ave_precision_score": 0.8222551363975833,
            "fpr": 0.14489571899012074,
            "logloss": 1.6047985652553562,
            "mae": 0.24095829022740242,
            "precision": 0.75,
            "recall": 0.8284518828451883
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8350150659566559,
            "auditor_fn_violation": 0.0033539731682146576,
            "auditor_fp_violation": 0.017025792692741033,
            "ave_precision_score": 0.8321992965568727,
            "fpr": 0.18092105263157895,
            "logloss": 1.5807643559035078,
            "mae": 0.2622600059776872,
            "precision": 0.7135416666666666,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8361815549641863,
            "auditor_fn_violation": 0.0045377510575072685,
            "auditor_fp_violation": 0.019591191062279606,
            "ave_precision_score": 0.8332661179481746,
            "fpr": 0.15806805708013172,
            "logloss": 1.409332217238267,
            "mae": 0.23433037745846172,
            "precision": 0.7419354838709677,
            "recall": 0.8661087866108786
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8298281889514625,
            "auditor_fn_violation": 0.0033539731682146576,
            "auditor_fp_violation": 0.014847899565427331,
            "ave_precision_score": 0.8274073516869678,
            "fpr": 0.18421052631578946,
            "logloss": 1.592639960641918,
            "mae": 0.26346861671676747,
            "precision": 0.7098445595854922,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8321617768965504,
            "auditor_fn_violation": 0.0047375407042699914,
            "auditor_fp_violation": 0.016412185680279263,
            "ave_precision_score": 0.8290034067864624,
            "fpr": 0.15916575192096596,
            "logloss": 1.4288906217003512,
            "mae": 0.2353203344968772,
            "precision": 0.7401433691756273,
            "recall": 0.8640167364016736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 15860,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.69821996012604,
            "auditor_fn_violation": 0.0037686127082411968,
            "auditor_fp_violation": 0.017071060679220994,
            "ave_precision_score": 0.6987960025750171,
            "fpr": 0.22807017543859648,
            "logloss": 1.7668742552349794,
            "mae": 0.3214453495847758,
            "precision": 0.6556291390728477,
            "recall": 0.8319327731092437
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7336148636123936,
            "auditor_fn_violation": 0.0133284955150669,
            "auditor_fp_violation": 0.030446455054086195,
            "ave_precision_score": 0.7354871169120794,
            "fpr": 0.20417124039517015,
            "logloss": 1.4596823137585253,
            "mae": 0.29420123125926345,
            "precision": 0.6879194630872483,
            "recall": 0.8577405857740585
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 15860,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7105582243238691,
            "auditor_fn_violation": 0.007518796992481204,
            "auditor_fp_violation": 0.006800257524545329,
            "ave_precision_score": 0.7064308555461996,
            "fpr": 0.2576754385964912,
            "logloss": 1.551917359570869,
            "mae": 0.3296100540083308,
            "precision": 0.6476761619190404,
            "recall": 0.907563025210084
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.71772527118914,
            "auditor_fn_violation": 0.009677167488024106,
            "auditor_fp_violation": 0.029870989167551838,
            "ave_precision_score": 0.7149433009478041,
            "fpr": 0.24807903402854006,
            "logloss": 1.513189607998729,
            "mae": 0.31654912143372477,
            "precision": 0.65284178187404,
            "recall": 0.8891213389121339
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7894736842105263,
            "auc_prc": 0.8801718236238716,
            "auditor_fn_violation": 0.00435371517027864,
            "auditor_fp_violation": 0.013706140350877203,
            "ave_precision_score": 0.8798832263505013,
            "fpr": 0.12609649122807018,
            "logloss": 0.9023535771067156,
            "mae": 0.2255720615317968,
            "precision": 0.7762645914396887,
            "recall": 0.8382352941176471
        },
        "train": {
            "accuracy": 0.8068057080131723,
            "auc_prc": 0.8748676550537664,
            "auditor_fn_violation": 0.007890542830766692,
            "auditor_fp_violation": 0.014325804955090851,
            "ave_precision_score": 0.8750822667447357,
            "fpr": 0.10208562019758508,
            "logloss": 0.7989431884802106,
            "mae": 0.21115974546197888,
            "precision": 0.8094262295081968,
            "recall": 0.8263598326359832
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7741228070175439,
            "auc_prc": 0.8682175549694039,
            "auditor_fn_violation": 0.008578431372549024,
            "auditor_fp_violation": 0.01767463383228714,
            "ave_precision_score": 0.8678660809754835,
            "fpr": 0.1337719298245614,
            "logloss": 1.144010492636965,
            "mae": 0.23580906324439685,
            "precision": 0.7626459143968871,
            "recall": 0.8235294117647058
        },
        "train": {
            "accuracy": 0.8024149286498353,
            "auc_prc": 0.8695923997131618,
            "auditor_fn_violation": 0.008427908087576761,
            "auditor_fp_violation": 0.015377868139724134,
            "ave_precision_score": 0.8697510425764258,
            "fpr": 0.10318331503841932,
            "logloss": 1.0498908108653955,
            "mae": 0.2104952533238207,
            "precision": 0.8065843621399177,
            "recall": 0.8200836820083682
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8457735019549841,
            "auditor_fn_violation": 0.0037041132242370644,
            "auditor_fp_violation": 0.014930890873973927,
            "ave_precision_score": 0.8427142086301799,
            "fpr": 0.16557017543859648,
            "logloss": 1.5594518044210792,
            "mae": 0.2554134412029094,
            "precision": 0.7259528130671506,
            "recall": 0.8403361344537815
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8437544985461157,
            "auditor_fn_violation": 0.0052197915757662096,
            "auditor_fp_violation": 0.018919391679321006,
            "ave_precision_score": 0.8390267343953264,
            "fpr": 0.14050493962678376,
            "logloss": 1.4326098546009733,
            "mae": 0.22994601566624887,
            "precision": 0.7589453860640302,
            "recall": 0.8430962343096234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.826699399336805,
            "auditor_fn_violation": 0.001011259767064726,
            "auditor_fp_violation": 0.01312268630291325,
            "ave_precision_score": 0.8246464361722885,
            "fpr": 0.18640350877192982,
            "logloss": 1.5069933983942136,
            "mae": 0.26505051056938334,
            "precision": 0.7053726169844021,
            "recall": 0.8550420168067226
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8322174177013586,
            "auditor_fn_violation": 0.004487229537636241,
            "auditor_fp_violation": 0.02045565743808672,
            "ave_precision_score": 0.8313383256730129,
            "fpr": 0.15367727771679474,
            "logloss": 1.3036513452358278,
            "mae": 0.23526122048687484,
            "precision": 0.7449908925318761,
            "recall": 0.8556485355648535
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7916666666666666,
            "auc_prc": 0.8745704477261573,
            "auditor_fn_violation": 0.006433823529411767,
            "auditor_fp_violation": 0.004486560437791729,
            "ave_precision_score": 0.8741361045453551,
            "fpr": 0.09758771929824561,
            "logloss": 1.0653623907420873,
            "mae": 0.22206614221426607,
            "precision": 0.8081896551724138,
            "recall": 0.7878151260504201
        },
        "train": {
            "accuracy": 0.7947310647639956,
            "auc_prc": 0.8774489781436154,
            "auditor_fn_violation": 0.01142245635629613,
            "auditor_fp_violation": 0.013927795509338012,
            "ave_precision_score": 0.8776727373515574,
            "fpr": 0.08232711306256861,
            "logloss": 1.017474946240922,
            "mae": 0.2119249098067778,
            "precision": 0.8299319727891157,
            "recall": 0.7656903765690377
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8282170162076038,
            "auditor_fn_violation": 0.0012623470440807916,
            "auditor_fp_violation": 0.013298728472557546,
            "ave_precision_score": 0.8258088653211328,
            "fpr": 0.1875,
            "logloss": 1.5919200639946949,
            "mae": 0.2647375549995042,
            "precision": 0.7056798623063684,
            "recall": 0.8613445378151261
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8302489068364253,
            "auditor_fn_violation": 0.0047375407042699914,
            "auditor_fp_violation": 0.010449649270020258,
            "ave_precision_score": 0.8273484924896403,
            "fpr": 0.15806805708013172,
            "logloss": 1.426925558368466,
            "mae": 0.23601067339975523,
            "precision": 0.7414721723518851,
            "recall": 0.8640167364016736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 15860,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8079494729287731,
            "auditor_fn_violation": 0.01609722836503023,
            "auditor_fp_violation": 0.016087739417350717,
            "ave_precision_score": 0.8084327637412588,
            "fpr": 0.13267543859649122,
            "logloss": 0.8945215565447273,
            "mae": 0.2851242025953123,
            "precision": 0.7441860465116279,
            "recall": 0.7394957983193278
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8209516026116431,
            "auditor_fn_violation": 0.02251881926615196,
            "auditor_fp_violation": 0.025977087838403096,
            "ave_precision_score": 0.8212612308907268,
            "fpr": 0.1350164654226125,
            "logloss": 0.8463355782745295,
            "mae": 0.2739640087370254,
            "precision": 0.7458677685950413,
            "recall": 0.7552301255230126
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 15860,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8108203576217725,
            "auditor_fn_violation": 0.023950040542532802,
            "auditor_fp_violation": 0.026441533880572996,
            "ave_precision_score": 0.7906538278515381,
            "fpr": 0.14802631578947367,
            "logloss": 1.5972743048836269,
            "mae": 0.2916026128676497,
            "precision": 0.7423664122137404,
            "recall": 0.8172268907563025
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8093051669474804,
            "auditor_fn_violation": 0.02716220622884411,
            "auditor_fp_violation": 0.03147570241061899,
            "ave_precision_score": 0.789919547207901,
            "fpr": 0.145993413830955,
            "logloss": 1.6107982555364726,
            "mae": 0.2911339685464729,
            "precision": 0.744721689059501,
            "recall": 0.8117154811715481
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8457301308775057,
            "auditor_fn_violation": 0.0037041132242370644,
            "auditor_fp_violation": 0.014930890873973927,
            "ave_precision_score": 0.8426889404076044,
            "fpr": 0.16557017543859648,
            "logloss": 1.5590193684317368,
            "mae": 0.25537309791371254,
            "precision": 0.7259528130671506,
            "recall": 0.8403361344537815
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8432294341163806,
            "auditor_fn_violation": 0.0052197915757662096,
            "auditor_fp_violation": 0.018919391679321006,
            "ave_precision_score": 0.8385403462274093,
            "fpr": 0.14050493962678376,
            "logloss": 1.4322639582245826,
            "mae": 0.22992329683415036,
            "precision": 0.7589453860640302,
            "recall": 0.8430962343096234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7877402226346957,
            "auditor_fn_violation": 0.029370300751879703,
            "auditor_fp_violation": 0.03192901979719943,
            "ave_precision_score": 0.7881228425127735,
            "fpr": 0.16666666666666666,
            "logloss": 1.0392303088550345,
            "mae": 0.29994624484608884,
            "precision": 0.7088122605363985,
            "recall": 0.7773109243697479
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8025253636774116,
            "auditor_fn_violation": 0.02935759591051261,
            "auditor_fp_violation": 0.03786159918674248,
            "ave_precision_score": 0.8029756148802112,
            "fpr": 0.1437980241492865,
            "logloss": 1.0098807752742893,
            "mae": 0.2778856372335163,
            "precision": 0.7385229540918163,
            "recall": 0.7740585774058577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 15860,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.610645400896882,
            "auditor_fn_violation": 0.004238537520271268,
            "auditor_fp_violation": 0.009003299533236775,
            "ave_precision_score": 0.5430962677534569,
            "fpr": 0.3256578947368421,
            "logloss": 6.805168030780724,
            "mae": 0.38865811914892023,
            "precision": 0.5903448275862069,
            "recall": 0.8991596638655462
        },
        "train": {
            "accuracy": 0.6201975850713501,
            "auc_prc": 0.5961305096469274,
            "auditor_fn_violation": 0.0038993427609551336,
            "auditor_fp_violation": 0.013443592935205579,
            "ave_precision_score": 0.5374055038727394,
            "fpr": 0.32930845225027444,
            "logloss": 6.4727183048904,
            "mae": 0.3767341183239944,
            "precision": 0.5901639344262295,
            "recall": 0.9037656903765691
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7792114447578802,
            "auditor_fn_violation": 0.02400762936753649,
            "auditor_fp_violation": 0.017850676001931437,
            "ave_precision_score": 0.7800351847384197,
            "fpr": 0.12171052631578948,
            "logloss": 2.7185069066229097,
            "mae": 0.30706036210404725,
            "precision": 0.7406542056074766,
            "recall": 0.6659663865546218
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.786908130069385,
            "auditor_fn_violation": 0.03150246407231008,
            "auditor_fp_violation": 0.03439105822345822,
            "ave_precision_score": 0.7868015916338088,
            "fpr": 0.132821075740944,
            "logloss": 3.1319138162301288,
            "mae": 0.31055827502107874,
            "precision": 0.7268623024830699,
            "recall": 0.6736401673640168
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 15860,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8479497166079711,
            "auditor_fn_violation": 0.010619379330679643,
            "auditor_fp_violation": 0.01576080395943989,
            "ave_precision_score": 0.8459462031080205,
            "fpr": 0.1962719298245614,
            "logloss": 1.6129367126598142,
            "mae": 0.27027299250077413,
            "precision": 0.6960950764006791,
            "recall": 0.8613445378151261
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8635891958240991,
            "auditor_fn_violation": 0.005685967418212549,
            "auditor_fp_violation": 0.02576414010946528,
            "ave_precision_score": 0.8615733916272373,
            "fpr": 0.1602634467618002,
            "logloss": 1.4223188133617812,
            "mae": 0.2402363733391553,
            "precision": 0.738819320214669,
            "recall": 0.8640167364016736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7763157894736842,
            "auc_prc": 0.870652002057938,
            "auditor_fn_violation": 0.007094943240454084,
            "auditor_fp_violation": 0.013047239658779982,
            "ave_precision_score": 0.8701447093903694,
            "fpr": 0.13157894736842105,
            "logloss": 1.1623996111697092,
            "mae": 0.23277444351553653,
            "precision": 0.765625,
            "recall": 0.8235294117647058
        },
        "train": {
            "accuracy": 0.7947310647639956,
            "auc_prc": 0.8688788203292523,
            "auditor_fn_violation": 0.008492208203776256,
            "auditor_fp_violation": 0.010424298349908616,
            "ave_precision_score": 0.8689383271241488,
            "fpr": 0.10867178924259056,
            "logloss": 1.0826987390446572,
            "mae": 0.21184757084543152,
            "precision": 0.7975460122699386,
            "recall": 0.8158995815899581
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7817982456140351,
            "auc_prc": 0.8727479478198363,
            "auditor_fn_violation": 0.005593026684357957,
            "auditor_fp_violation": 0.008892644455174632,
            "ave_precision_score": 0.8717481920188622,
            "fpr": 0.12828947368421054,
            "logloss": 1.100918316000702,
            "mae": 0.23320349592666992,
            "precision": 0.7710371819960861,
            "recall": 0.8277310924369747
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8680374513552639,
            "auditor_fn_violation": 0.0022665790960322276,
            "auditor_fp_violation": 0.01762902984563825,
            "ave_precision_score": 0.8678144141246671,
            "fpr": 0.12294182217343579,
            "logloss": 1.0082529674406346,
            "mae": 0.222473889021339,
            "precision": 0.7764471057884231,
            "recall": 0.8138075313807531
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8202702251209246,
            "auditor_fn_violation": 0.004653177060297807,
            "auditor_fp_violation": 0.012423547400611627,
            "ave_precision_score": 0.8180168352929627,
            "fpr": 0.19188596491228072,
            "logloss": 1.589319255349694,
            "mae": 0.268953575333844,
            "precision": 0.7013651877133106,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8236937432347134,
            "auditor_fn_violation": 0.0025674117825370104,
            "auditor_fp_violation": 0.012708416251967872,
            "ave_precision_score": 0.8212125042983086,
            "fpr": 0.16136114160263446,
            "logloss": 1.4265880987211375,
            "mae": 0.23886687503554227,
            "precision": 0.7379679144385026,
            "recall": 0.8661087866108786
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7872807017543859,
            "auc_prc": 0.8807015081447941,
            "auditor_fn_violation": 0.0006357806280406944,
            "auditor_fp_violation": 0.007212699179140515,
            "ave_precision_score": 0.8800032450818801,
            "fpr": 0.08771929824561403,
            "logloss": 1.1151419012717265,
            "mae": 0.2192388508509769,
            "precision": 0.8190045248868778,
            "recall": 0.7605042016806722
        },
        "train": {
            "accuracy": 0.7936333699231614,
            "auc_prc": 0.8776654110444696,
            "auditor_fn_violation": 0.009015794864257859,
            "auditor_fp_violation": 0.006198299967297315,
            "ave_precision_score": 0.8777325708237833,
            "fpr": 0.07793633369923161,
            "logloss": 1.0287288973080708,
            "mae": 0.21626713088493896,
            "precision": 0.8356481481481481,
            "recall": 0.7552301255230126
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 15860,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7661010576093339,
            "auditor_fn_violation": 0.023268188854489166,
            "auditor_fp_violation": 0.02643901899243523,
            "ave_precision_score": 0.6712809391848528,
            "fpr": 0.24890350877192982,
            "logloss": 7.8277708564439274,
            "mae": 0.33371192534368327,
            "precision": 0.6373801916932907,
            "recall": 0.8382352941176471
        },
        "train": {
            "accuracy": 0.6717892425905598,
            "auc_prc": 0.7730185703749929,
            "auditor_fn_violation": 0.030423140693247117,
            "auditor_fp_violation": 0.04218139597376687,
            "ave_precision_score": 0.6870912669348046,
            "fpr": 0.22502744237102085,
            "logloss": 7.7586365601728735,
            "mae": 0.32960998703464783,
            "precision": 0.6519524617996605,
            "recall": 0.803347280334728
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7916666666666666,
            "auc_prc": 0.8754214778414835,
            "auditor_fn_violation": 0.007979507592510692,
            "auditor_fp_violation": 0.006410449863190084,
            "ave_precision_score": 0.8749268451180542,
            "fpr": 0.10416666666666667,
            "logloss": 1.0856441229245706,
            "mae": 0.221605105537905,
            "precision": 0.8004201680672269,
            "recall": 0.8004201680672269
        },
        "train": {
            "accuracy": 0.7980241492864983,
            "auc_prc": 0.8760698939016534,
            "auditor_fn_violation": 0.009624349535431662,
            "auditor_fp_violation": 0.013927795509338012,
            "ave_precision_score": 0.8762748651682825,
            "fpr": 0.09110867178924259,
            "logloss": 1.027886086412968,
            "mae": 0.20929784491283657,
            "precision": 0.8195652173913044,
            "recall": 0.7887029288702929
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 15860,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.7231548746474943,
            "auditor_fn_violation": 0.0045794633642930885,
            "auditor_fp_violation": 0.014873048446805086,
            "ave_precision_score": 0.7239726129770281,
            "fpr": 0.26973684210526316,
            "logloss": 1.4133092421416384,
            "mae": 0.32090554127632476,
            "precision": 0.638235294117647,
            "recall": 0.9117647058823529
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7412010744059363,
            "auditor_fn_violation": 0.008000771601394393,
            "auditor_fp_violation": 0.03573972717339777,
            "ave_precision_score": 0.7417581958955607,
            "fpr": 0.2535675082327113,
            "logloss": 1.3128884995596393,
            "mae": 0.30285726430199944,
            "precision": 0.6515837104072398,
            "recall": 0.9037656903765691
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8271799068330727,
            "auditor_fn_violation": 0.016025818222025656,
            "auditor_fp_violation": 0.017141477547078707,
            "ave_precision_score": 0.827581846514122,
            "fpr": 0.12719298245614036,
            "logloss": 0.9033374498433345,
            "mae": 0.2760339267525684,
            "precision": 0.7537154989384289,
            "recall": 0.7457983193277311
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8269118110060528,
            "auditor_fn_violation": 0.02472109824598469,
            "auditor_fp_violation": 0.02598976329845892,
            "ave_precision_score": 0.8272278175780352,
            "fpr": 0.1350164654226125,
            "logloss": 0.9446829103101253,
            "mae": 0.2741735215408663,
            "precision": 0.7479508196721312,
            "recall": 0.7635983263598326
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7894736842105263,
            "auc_prc": 0.8788462446759924,
            "auditor_fn_violation": 0.00579343579537078,
            "auditor_fp_violation": 0.011998531305327544,
            "ave_precision_score": 0.8783157147639491,
            "fpr": 0.10635964912280702,
            "logloss": 1.0510951589030075,
            "mae": 0.22056727181403973,
            "precision": 0.797071129707113,
            "recall": 0.8004201680672269
        },
        "train": {
            "accuracy": 0.7991218441273326,
            "auc_prc": 0.8769666756536578,
            "auditor_fn_violation": 0.011907003660513754,
            "auditor_fp_violation": 0.016100369362906028,
            "ave_precision_score": 0.8772181316635173,
            "fpr": 0.08562019758507135,
            "logloss": 1.0082408357808488,
            "mae": 0.20607312619109366,
            "precision": 0.8270509977827051,
            "recall": 0.7803347280334728
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 15860,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.8218712497712846,
            "auditor_fn_violation": 0.024664141972578494,
            "auditor_fp_violation": 0.0037119748913568337,
            "ave_precision_score": 0.8222949695503907,
            "fpr": 0.027412280701754384,
            "logloss": 1.1275367686054079,
            "mae": 0.3621051141532961,
            "precision": 0.8842592592592593,
            "recall": 0.4012605042016807
        },
        "train": {
            "accuracy": 0.6520307354555434,
            "auc_prc": 0.8085898835014672,
            "auditor_fn_violation": 0.02749748540617006,
            "auditor_fp_violation": 0.008495093329412392,
            "ave_precision_score": 0.8101789071537876,
            "fpr": 0.031833150384193196,
            "logloss": 1.183900816429136,
            "mae": 0.3593053353217063,
            "precision": 0.867579908675799,
            "recall": 0.39748953974895396
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 15860,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.8412591247088379,
            "auditor_fn_violation": 0.0063301636444051314,
            "auditor_fp_violation": 0.021296072750684066,
            "ave_precision_score": 0.837519451746123,
            "fpr": 0.3333333333333333,
            "logloss": 1.764730330269519,
            "mae": 0.34639575141173945,
            "precision": 0.5989445910290238,
            "recall": 0.9537815126050421
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.82801361853216,
            "auditor_fn_violation": 0.004223139774674023,
            "auditor_fp_violation": 0.028925399847387484,
            "ave_precision_score": 0.8248842011039247,
            "fpr": 0.30954994511525796,
            "logloss": 1.6865069477515287,
            "mae": 0.3318836315914768,
            "precision": 0.6173677069199457,
            "recall": 0.9518828451882845
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8358933211859053,
            "auditor_fn_violation": 0.006030701754385966,
            "auditor_fp_violation": 0.017307460164171902,
            "ave_precision_score": 0.8337745706334572,
            "fpr": 0.1962719298245614,
            "logloss": 1.696235194669503,
            "mae": 0.2702310558270617,
            "precision": 0.6976351351351351,
            "recall": 0.8676470588235294
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8477999245658233,
            "auditor_fn_violation": 0.004794951522305251,
            "auditor_fp_violation": 0.013192618826100294,
            "ave_precision_score": 0.8450498556444671,
            "fpr": 0.1712403951701427,
            "logloss": 1.4748600747013283,
            "mae": 0.2403318776653984,
            "precision": 0.7291666666666666,
            "recall": 0.8786610878661087
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 15860,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8444376388955144,
            "auditor_fn_violation": 0.004459678608285425,
            "auditor_fp_violation": 0.016432279092225985,
            "ave_precision_score": 0.841757259070076,
            "fpr": 0.18092105263157895,
            "logloss": 1.3923676833211749,
            "mae": 0.26220872085486246,
            "precision": 0.7115384615384616,
            "recall": 0.8550420168067226
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.843233992450741,
            "auditor_fn_violation": 0.0028406872763848683,
            "auditor_fp_violation": 0.011159475033146333,
            "ave_precision_score": 0.840614076158037,
            "fpr": 0.16465422612513722,
            "logloss": 1.191908152563575,
            "mae": 0.23824050784924916,
            "precision": 0.7335701598579041,
            "recall": 0.8640167364016736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7883771929824561,
            "auc_prc": 0.8797897879730643,
            "auditor_fn_violation": 0.0028011204481792735,
            "auditor_fp_violation": 0.008050156929019796,
            "ave_precision_score": 0.8790780013790518,
            "fpr": 0.10197368421052631,
            "logloss": 1.000408255362612,
            "mae": 0.2180164637105845,
            "precision": 0.8017057569296375,
            "recall": 0.7899159663865546
        },
        "train": {
            "accuracy": 0.7980241492864983,
            "auc_prc": 0.8781246946434464,
            "auditor_fn_violation": 0.010797826656072453,
            "auditor_fp_violation": 0.012409275394650456,
            "ave_precision_score": 0.8781396433713751,
            "fpr": 0.09220636663007684,
            "logloss": 0.9075816433138135,
            "mae": 0.2112448937211918,
            "precision": 0.8181818181818182,
            "recall": 0.7907949790794979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8265077465151028,
            "auditor_fn_violation": 0.000668030370042755,
            "auditor_fp_violation": 0.015431353613391277,
            "ave_precision_score": 0.8259495487156842,
            "fpr": 0.17543859649122806,
            "logloss": 1.8892644644356975,
            "mae": 0.27196028394445715,
            "precision": 0.7111913357400722,
            "recall": 0.8277310924369747
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8297267436586726,
            "auditor_fn_violation": 0.008896380362744514,
            "auditor_fp_violation": 0.018868689839097716,
            "ave_precision_score": 0.8289611588085781,
            "fpr": 0.14489571899012074,
            "logloss": 1.555165393692564,
            "mae": 0.23729226799929687,
            "precision": 0.7523452157598499,
            "recall": 0.8389121338912134
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8324751075810292,
            "auditor_fn_violation": 0.006772445820433438,
            "auditor_fp_violation": 0.019424995976178982,
            "ave_precision_score": 0.830881464166922,
            "fpr": 0.17653508771929824,
            "logloss": 1.4813680282253217,
            "mae": 0.2607659616676692,
            "precision": 0.7160493827160493,
            "recall": 0.8529411764705882
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8368384616887017,
            "auditor_fn_violation": 0.005146305728681067,
            "auditor_fp_violation": 0.017106800891338356,
            "ave_precision_score": 0.8358126417332263,
            "fpr": 0.14709110867178923,
            "logloss": 1.2899462700411273,
            "mae": 0.23233311684252497,
            "precision": 0.7523105360443623,
            "recall": 0.8514644351464435
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 15860,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8510049447826162,
            "auditor_fn_violation": 0.007716902550493883,
            "auditor_fp_violation": 0.013409383550619668,
            "ave_precision_score": 0.8484277873763507,
            "fpr": 0.17434210526315788,
            "logloss": 1.3511454583490374,
            "mae": 0.2583319447463395,
            "precision": 0.7190812720848057,
            "recall": 0.8550420168067226
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.845605674733884,
            "auditor_fn_violation": 0.0021724253544543933,
            "auditor_fp_violation": 0.014090041398052554,
            "ave_precision_score": 0.8421340769130825,
            "fpr": 0.15806805708013172,
            "logloss": 1.2322204686307245,
            "mae": 0.2382842786665916,
            "precision": 0.740072202166065,
            "recall": 0.8577405857740585
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8223198152094792,
            "auditor_fn_violation": 0.004127966976264191,
            "auditor_fp_violation": 0.01412361178174796,
            "ave_precision_score": 0.8191845355985248,
            "fpr": 0.19298245614035087,
            "logloss": 1.6525033569553893,
            "mae": 0.2689554168452353,
            "precision": 0.7011884550084889,
            "recall": 0.8676470588235294
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.825024964917164,
            "auditor_fn_violation": 0.0022987291541319726,
            "auditor_fp_violation": 0.011192431229291467,
            "ave_precision_score": 0.8211208631843032,
            "fpr": 0.16794731064763996,
            "logloss": 1.5076185650200382,
            "mae": 0.24083755077188618,
            "precision": 0.7315789473684211,
            "recall": 0.8723849372384938
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7916666666666666,
            "auc_prc": 0.8754214778414835,
            "auditor_fn_violation": 0.007979507592510692,
            "auditor_fp_violation": 0.006410449863190084,
            "ave_precision_score": 0.8749268451180542,
            "fpr": 0.10416666666666667,
            "logloss": 1.0856441100566507,
            "mae": 0.22160510857517857,
            "precision": 0.8004201680672269,
            "recall": 0.8004201680672269
        },
        "train": {
            "accuracy": 0.7980241492864983,
            "auc_prc": 0.8760698939016534,
            "auditor_fn_violation": 0.009624349535431662,
            "auditor_fp_violation": 0.013927795509338012,
            "ave_precision_score": 0.8762748651682825,
            "fpr": 0.09110867178924259,
            "logloss": 1.0278861132515118,
            "mae": 0.2092978458028388,
            "precision": 0.8195652173913044,
            "recall": 0.7887029288702929
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8355575602771776,
            "auditor_fn_violation": 0.015514429455992922,
            "auditor_fp_violation": 0.01705345646225656,
            "ave_precision_score": 0.8368515849549041,
            "fpr": 0.14144736842105263,
            "logloss": 0.6178701983521631,
            "mae": 0.2729496530854815,
            "precision": 0.75,
            "recall": 0.8130252100840336
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8482035963378298,
            "auditor_fn_violation": 0.028379315571191714,
            "auditor_fp_violation": 0.023875496561147687,
            "ave_precision_score": 0.8484259983244028,
            "fpr": 0.14270032930845225,
            "logloss": 0.6206602050270824,
            "mae": 0.2727996936016999,
            "precision": 0.748062015503876,
            "recall": 0.8075313807531381
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8317517597236888,
            "auditor_fn_violation": 0.0033539731682146576,
            "auditor_fp_violation": 0.01936966843714792,
            "ave_precision_score": 0.8290413486983088,
            "fpr": 0.18201754385964913,
            "logloss": 1.5867349842424607,
            "mae": 0.26283030317218625,
            "precision": 0.7123050259965338,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.83397964472694,
            "auditor_fn_violation": 0.0034676134093299515,
            "auditor_fp_violation": 0.016412185680279263,
            "ave_precision_score": 0.8305479990225724,
            "fpr": 0.15916575192096596,
            "logloss": 1.40947349687663,
            "mae": 0.23546773361362192,
            "precision": 0.7410714285714286,
            "recall": 0.8682008368200836
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7839912280701754,
            "auc_prc": 0.8672576672832409,
            "auditor_fn_violation": 0.004994102904319625,
            "auditor_fp_violation": 0.006757504426203125,
            "ave_precision_score": 0.8660229170864192,
            "fpr": 0.10635964912280702,
            "logloss": 1.2096669081942075,
            "mae": 0.22448415601754942,
            "precision": 0.7949260042283298,
            "recall": 0.7899159663865546
        },
        "train": {
            "accuracy": 0.7947310647639956,
            "auc_prc": 0.8757947945402218,
            "auditor_fn_violation": 0.01125940963307598,
            "auditor_fp_violation": 0.013717382872411352,
            "ave_precision_score": 0.8757687741363651,
            "fpr": 0.09001097694840834,
            "logloss": 1.1218240278582037,
            "mae": 0.21242862418396502,
            "precision": 0.8197802197802198,
            "recall": 0.7803347280334728
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8297916784066428,
            "auditor_fn_violation": 0.003059118384195785,
            "auditor_fp_violation": 0.0174835023338162,
            "ave_precision_score": 0.8271587329726386,
            "fpr": 0.18421052631578946,
            "logloss": 1.596706174032943,
            "mae": 0.2649858418673126,
            "precision": 0.7098445595854922,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8299059449565113,
            "auditor_fn_violation": 0.0034905777365440535,
            "auditor_fp_violation": 0.02196910736875195,
            "ave_precision_score": 0.826756415579399,
            "fpr": 0.15697036223929747,
            "logloss": 1.4382792824629191,
            "mae": 0.2362875231841224,
            "precision": 0.7432675044883303,
            "recall": 0.8661087866108786
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8318599013067656,
            "auditor_fn_violation": 0.0033539731682146576,
            "auditor_fp_violation": 0.017025792692741033,
            "ave_precision_score": 0.8289945015111939,
            "fpr": 0.18092105263157895,
            "logloss": 1.5984275634937577,
            "mae": 0.26240696242257,
            "precision": 0.7135416666666666,
            "recall": 0.8634453781512605
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8339014132614132,
            "auditor_fn_violation": 0.0045377510575072685,
            "auditor_fp_violation": 0.016881177702344707,
            "ave_precision_score": 0.8306196454289017,
            "fpr": 0.1602634467618002,
            "logloss": 1.4347379386738395,
            "mae": 0.23494525221323279,
            "precision": 0.7392857142857143,
            "recall": 0.8661087866108786
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8491619302177403,
            "auditor_fn_violation": 0.010918841220698807,
            "auditor_fp_violation": 0.016459942861741513,
            "ave_precision_score": 0.8465047231988714,
            "fpr": 0.1699561403508772,
            "logloss": 1.4765881389325621,
            "mae": 0.2535926804824278,
            "precision": 0.7227191413237924,
            "recall": 0.8487394957983193
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8475722832309031,
            "auditor_fn_violation": 0.006627504833990881,
            "auditor_fp_violation": 0.019256558916805888,
            "ave_precision_score": 0.8454765301576057,
            "fpr": 0.14270032930845225,
            "logloss": 1.347962521407687,
            "mae": 0.23047834885038468,
            "precision": 0.7565543071161048,
            "recall": 0.8451882845188284
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8300308899841848,
            "auditor_fn_violation": 0.002909387439186204,
            "auditor_fp_violation": 0.015220103009818126,
            "ave_precision_score": 0.7958962026700791,
            "fpr": 0.14473684210526316,
            "logloss": 3.4797460693761324,
            "mae": 0.24519330240349307,
            "precision": 0.7456647398843931,
            "recall": 0.8130252100840336
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8310396410511457,
            "auditor_fn_violation": 0.0033160488497168497,
            "auditor_fp_violation": 0.019755972043005304,
            "ave_precision_score": 0.8002416655921172,
            "fpr": 0.1141602634467618,
            "logloss": 3.2627538456240828,
            "mae": 0.2251424520635545,
            "precision": 0.7860082304526749,
            "recall": 0.799163179916318
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.7818256881247003,
            "auditor_fn_violation": 0.005139226743328912,
            "auditor_fp_violation": 0.01499879285369387,
            "ave_precision_score": 0.779179695340857,
            "fpr": 0.20614035087719298,
            "logloss": 1.7095133776933618,
            "mae": 0.28784991307540464,
            "precision": 0.6829679595278246,
            "recall": 0.8508403361344538
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.7960365067190833,
            "auditor_fn_violation": 0.0016350600976443206,
            "auditor_fp_violation": 0.011821134048060275,
            "ave_precision_score": 0.7941968062246256,
            "fpr": 0.16355653128430298,
            "logloss": 1.4818555150118993,
            "mae": 0.25293550281309923,
            "precision": 0.7285974499089253,
            "recall": 0.8368200836820083
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 15860,
        "test": {
            "accuracy": 0.4375,
            "auc_prc": 0.5039320188964147,
            "auditor_fn_violation": 0.04936053368715907,
            "auditor_fp_violation": 0.06783407774022211,
            "ave_precision_score": 0.5239589173373502,
            "fpr": 0.20723684210526316,
            "logloss": 9.661463486192792,
            "mae": 0.5560015285724089,
            "precision": 0.44574780058651026,
            "recall": 0.31932773109243695
        },
        "train": {
            "accuracy": 0.4621295279912184,
            "auc_prc": 0.5310364194977168,
            "auditor_fn_violation": 0.04547855361481475,
            "auditor_fp_violation": 0.0699229078519405,
            "ave_precision_score": 0.5528028159034502,
            "fpr": 0.18660812294182216,
            "logloss": 9.714567091862252,
            "mae": 0.5297267546000678,
            "precision": 0.4817073170731707,
            "recall": 0.3305439330543933
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 15860,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.758782603184329,
            "auditor_fn_violation": 0.0221233230134159,
            "auditor_fp_violation": 0.012322951875100602,
            "ave_precision_score": 0.7595705871423536,
            "fpr": 0.11403508771929824,
            "logloss": 1.1691520824089479,
            "mae": 0.32408577471540345,
            "precision": 0.7386934673366834,
            "recall": 0.6176470588235294
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7804420853553382,
            "auditor_fn_violation": 0.024406486963151446,
            "auditor_fp_violation": 0.021882914240372354,
            "ave_precision_score": 0.7809041187031283,
            "fpr": 0.1119648737650933,
            "logloss": 1.2090366141542492,
            "mae": 0.31674069780533626,
            "precision": 0.7493857493857494,
            "recall": 0.6380753138075314
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7894172852227497,
            "auditor_fn_violation": 0.006795481350434911,
            "auditor_fp_violation": 0.016236117817479485,
            "ave_precision_score": 0.7826295222507167,
            "fpr": 0.18859649122807018,
            "logloss": 1.8252111717070265,
            "mae": 0.26825193304582445,
            "precision": 0.7039586919104991,
            "recall": 0.8592436974789915
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.7985310081188834,
            "auditor_fn_violation": 0.0034262776203445567,
            "auditor_fp_violation": 0.01673414236569717,
            "ave_precision_score": 0.7939899392115659,
            "fpr": 0.16465422612513722,
            "logloss": 1.624985516733457,
            "mae": 0.24352144664707948,
            "precision": 0.7321428571428571,
            "recall": 0.8577405857740585
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8372536798440562,
            "auditor_fn_violation": 0.0074911543564794335,
            "auditor_fp_violation": 0.015899122807017555,
            "ave_precision_score": 0.8306039722890675,
            "fpr": 0.1787280701754386,
            "logloss": 1.6797022464985663,
            "mae": 0.26068002781326066,
            "precision": 0.7125220458553791,
            "recall": 0.8487394957983193
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8461520147533405,
            "auditor_fn_violation": 0.0065402403905772824,
            "auditor_fp_violation": 0.01864306665010407,
            "ave_precision_score": 0.8417009843227539,
            "fpr": 0.16794731064763996,
            "logloss": 1.3945557139745777,
            "mae": 0.24005496986885527,
            "precision": 0.7306338028169014,
            "recall": 0.8682008368200836
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 15860,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.7550052660042753,
            "auditor_fn_violation": 0.007293048798466754,
            "auditor_fp_violation": 0.01790600354096252,
            "ave_precision_score": 0.7497778081760925,
            "fpr": 0.26973684210526316,
            "logloss": 2.49123690371654,
            "mae": 0.3329998287123105,
            "precision": 0.6317365269461078,
            "recall": 0.8865546218487395
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7786446247537278,
            "auditor_fn_violation": 0.011684249686536934,
            "auditor_fp_violation": 0.03391446092535929,
            "ave_precision_score": 0.7735647350828252,
            "fpr": 0.25686059275521406,
            "logloss": 2.2163392380202627,
            "mae": 0.3187460645203663,
            "precision": 0.6411042944785276,
            "recall": 0.8744769874476988
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7905701754385965,
            "auc_prc": 0.8779759087804215,
            "auditor_fn_violation": 0.0074358690844759006,
            "auditor_fp_violation": 0.005019716723000163,
            "ave_precision_score": 0.8776155011841105,
            "fpr": 0.09649122807017543,
            "logloss": 1.0062201046436217,
            "mae": 0.21958190953097248,
            "precision": 0.8091106290672451,
            "recall": 0.7836134453781513
        },
        "train": {
            "accuracy": 0.7969264544456641,
            "auc_prc": 0.8791507664835942,
            "auditor_fn_violation": 0.013617846037964624,
            "auditor_fp_violation": 0.01200112558085296,
            "ave_precision_score": 0.8793644887167724,
            "fpr": 0.0801317233809001,
            "logloss": 0.9806704040691017,
            "mae": 0.2116275912100773,
            "precision": 0.8337129840546698,
            "recall": 0.7656903765690377
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8475775833443933,
            "auditor_fn_violation": 0.0007993328910511598,
            "auditor_fp_violation": 0.014862988894253984,
            "ave_precision_score": 0.8469458889751968,
            "fpr": 0.15460526315789475,
            "logloss": 1.8102025331509202,
            "mae": 0.2620737850616856,
            "precision": 0.7329545454545454,
            "recall": 0.8130252100840336
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8463055185769558,
            "auditor_fn_violation": 0.010995319870113767,
            "auditor_fp_violation": 0.019854840631440718,
            "ave_precision_score": 0.8451502931785777,
            "fpr": 0.13062568605927552,
            "logloss": 1.565932826610494,
            "mae": 0.2324364528697057,
            "precision": 0.7671232876712328,
            "recall": 0.8200836820083682
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8376304190140433,
            "auditor_fn_violation": 0.0074911543564794335,
            "auditor_fp_violation": 0.015899122807017555,
            "ave_precision_score": 0.8308504300013663,
            "fpr": 0.1787280701754386,
            "logloss": 1.6806055852191095,
            "mae": 0.26066240985036426,
            "precision": 0.7125220458553791,
            "recall": 0.8487394957983193
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8460970510360382,
            "auditor_fn_violation": 0.0065402403905772824,
            "auditor_fp_violation": 0.01864306665010407,
            "ave_precision_score": 0.8415934045937251,
            "fpr": 0.16794731064763996,
            "logloss": 1.3955522722335811,
            "mae": 0.24005012612568924,
            "precision": 0.7306338028169014,
            "recall": 0.8682008368200836
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8429195827642426,
            "auditor_fn_violation": 0.0012669541500810882,
            "auditor_fp_violation": 0.012167028810558513,
            "ave_precision_score": 0.8426070109044772,
            "fpr": 0.16776315789473684,
            "logloss": 1.7072426785818664,
            "mae": 0.26668853552612665,
            "precision": 0.7197802197802198,
            "recall": 0.8256302521008403
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8417232096891952,
            "auditor_fn_violation": 0.008793040890281044,
            "auditor_fp_violation": 0.012601942387498953,
            "ave_precision_score": 0.8407788422678356,
            "fpr": 0.14050493962678376,
            "logloss": 1.373273224696495,
            "mae": 0.23757612937822983,
            "precision": 0.7580340264650284,
            "recall": 0.8389121338912134
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7905701754385965,
            "auc_prc": 0.881690382963779,
            "auditor_fn_violation": 0.006855373728438747,
            "auditor_fp_violation": 0.005142946241751168,
            "ave_precision_score": 0.8815381578406402,
            "fpr": 0.09100877192982457,
            "logloss": 0.8773571473575881,
            "mae": 0.21735824262355377,
            "precision": 0.8159645232815964,
            "recall": 0.773109243697479
        },
        "train": {
            "accuracy": 0.7947310647639956,
            "auc_prc": 0.8800050419187604,
            "auditor_fn_violation": 0.010733526539872966,
            "auditor_fp_violation": 0.009856437739407751,
            "ave_precision_score": 0.8802013117252654,
            "fpr": 0.0801317233809001,
            "logloss": 0.8001431110692148,
            "mae": 0.21221112709764872,
            "precision": 0.8329519450800915,
            "recall": 0.7615062761506276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7916666666666666,
            "auc_prc": 0.87543782873539,
            "auditor_fn_violation": 0.007979507592510692,
            "auditor_fp_violation": 0.006410449863190084,
            "ave_precision_score": 0.8749354968303782,
            "fpr": 0.10416666666666667,
            "logloss": 1.0857142920937923,
            "mae": 0.22162809888162824,
            "precision": 0.8004201680672269,
            "recall": 0.8004201680672269
        },
        "train": {
            "accuracy": 0.7980241492864983,
            "auc_prc": 0.8760538710484679,
            "auditor_fn_violation": 0.009624349535431662,
            "auditor_fp_violation": 0.013927795509338012,
            "ave_precision_score": 0.8762591634862009,
            "fpr": 0.09110867178924259,
            "logloss": 1.0278548262543334,
            "mae": 0.20930125348466005,
            "precision": 0.8195652173913044,
            "recall": 0.7887029288702929
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8435672362201028,
            "auditor_fn_violation": 0.001990269792127378,
            "auditor_fp_violation": 0.019113149847094803,
            "ave_precision_score": 0.8409539375742534,
            "fpr": 0.18201754385964913,
            "logloss": 1.3856033612061076,
            "mae": 0.2625430229269993,
            "precision": 0.71280276816609,
            "recall": 0.865546218487395
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8417564224333618,
            "auditor_fn_violation": 0.003559470718186369,
            "auditor_fp_violation": 0.020625508602834745,
            "ave_precision_score": 0.8390409008179476,
            "fpr": 0.16794731064763996,
            "logloss": 1.1885641703454985,
            "mae": 0.23809645416678157,
            "precision": 0.7311072056239016,
            "recall": 0.8702928870292888
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8502151575278858,
            "auditor_fn_violation": 0.009591994692613888,
            "auditor_fp_violation": 0.01551685981007565,
            "ave_precision_score": 0.8476921972301514,
            "fpr": 0.16447368421052633,
            "logloss": 1.4438428216045702,
            "mae": 0.2555911940288443,
            "precision": 0.7277676950998185,
            "recall": 0.842436974789916
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8463611861233455,
            "auditor_fn_violation": 0.004579086846492663,
            "auditor_fp_violation": 0.020488613634231868,
            "ave_precision_score": 0.8443284476371837,
            "fpr": 0.14489571899012074,
            "logloss": 1.3555092702732028,
            "mae": 0.23431379607819922,
            "precision": 0.7509433962264151,
            "recall": 0.8326359832635983
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 15860,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.675378468518504,
            "auditor_fn_violation": 0.0016217013121037965,
            "auditor_fp_violation": 0.008339369064864,
            "ave_precision_score": 0.6724876586279595,
            "fpr": 0.15350877192982457,
            "logloss": 2.484932926195658,
            "mae": 0.3230364246587741,
            "precision": 0.7021276595744681,
            "recall": 0.6932773109243697
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7088955124090421,
            "auditor_fn_violation": 0.015177123855802397,
            "auditor_fp_violation": 0.00671292364556372,
            "ave_precision_score": 0.705101607261731,
            "fpr": 0.11855104281009879,
            "logloss": 2.4364053674666777,
            "mae": 0.3036773343075598,
            "precision": 0.747072599531616,
            "recall": 0.6673640167364017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 15860,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8101075959891875,
            "auditor_fn_violation": 0.003740970072239424,
            "auditor_fp_violation": 0.015064179945276035,
            "ave_precision_score": 0.8047436795771317,
            "fpr": 0.19407894736842105,
            "logloss": 1.7925814636300432,
            "mae": 0.2696742254537805,
            "precision": 0.6994906621392191,
            "recall": 0.865546218487395
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8227590600868938,
            "auditor_fn_violation": 0.003559470718186369,
            "auditor_fp_violation": 0.011397773682195796,
            "ave_precision_score": 0.8183824673098716,
            "fpr": 0.17014270032930845,
            "logloss": 1.5413479350244137,
            "mae": 0.24030996589881784,
            "precision": 0.7285464098073555,
            "recall": 0.8702928870292888
        }
    }
]
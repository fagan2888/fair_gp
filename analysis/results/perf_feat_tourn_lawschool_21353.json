[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8097760802710408,
            "auditor_fn_violation": 0.01871284616499928,
            "auditor_fp_violation": 0.037957042138055425,
            "ave_precision_score": 0.8104891985318378,
            "fpr": 0.17434210526315788,
            "logloss": 1.365816107136736,
            "mae": 0.2883781301835582,
            "precision": 0.7150537634408602,
            "recall": 0.8243801652892562
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8080126805986133,
            "auditor_fn_violation": 0.02238596819020483,
            "auditor_fp_violation": 0.05091213214155036,
            "ave_precision_score": 0.8083469873321596,
            "fpr": 0.18441273326015367,
            "logloss": 1.2637970899328879,
            "mae": 0.30176298087029413,
            "precision": 0.6962025316455697,
            "recall": 0.8191489361702128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 21353,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8024181907085015,
            "auditor_fn_violation": 0.01064321444106134,
            "auditor_fp_violation": 0.010042629939334321,
            "ave_precision_score": 0.802375363984856,
            "fpr": 0.1524122807017544,
            "logloss": 1.4225126157872408,
            "mae": 0.29134162761231985,
            "precision": 0.7367424242424242,
            "recall": 0.8037190082644629
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8155001784440166,
            "auditor_fn_violation": 0.012698227339608101,
            "auditor_fp_violation": 0.024584879689160698,
            "ave_precision_score": 0.8157680833707104,
            "fpr": 0.15587266739846323,
            "logloss": 1.2457763512703048,
            "mae": 0.2936444299762839,
            "precision": 0.7325800376647834,
            "recall": 0.8276595744680851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6293859649122807,
            "auc_prc": 0.622992160909613,
            "auditor_fn_violation": 0.041177323473974195,
            "auditor_fp_violation": 0.017815420560747666,
            "ave_precision_score": 0.5984378619807845,
            "fpr": 0.1600877192982456,
            "logloss": 4.765469759719781,
            "mae": 0.39203691371358707,
            "precision": 0.6666666666666666,
            "recall": 0.6033057851239669
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.6308844785899368,
            "auditor_fn_violation": 0.032907490015647994,
            "auditor_fp_violation": 0.019765974446858882,
            "ave_precision_score": 0.6027658237186326,
            "fpr": 0.15916575192096596,
            "logloss": 4.428350646577438,
            "mae": 0.37730636291152087,
            "precision": 0.6697038724373576,
            "recall": 0.625531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.6428201607585802,
            "auditor_fn_violation": 0.03824806075105119,
            "auditor_fp_violation": 0.006227967699622892,
            "ave_precision_score": 0.6166721729731157,
            "fpr": 0.1524122807017544,
            "logloss": 4.808150082039668,
            "mae": 0.36360891182581856,
            "precision": 0.682648401826484,
            "recall": 0.6177685950413223
        },
        "train": {
            "accuracy": 0.6465422612513722,
            "auc_prc": 0.6464779259385314,
            "auditor_fn_violation": 0.026134479295606895,
            "auditor_fp_violation": 0.02322831803778958,
            "ave_precision_score": 0.6169634053332884,
            "fpr": 0.16245883644346873,
            "logloss": 4.460140157313402,
            "mae": 0.36003116377255917,
            "precision": 0.6666666666666666,
            "recall": 0.6297872340425532
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8426468889860544,
            "auditor_fn_violation": 0.010810859794113382,
            "auditor_fp_violation": 0.008518302180685364,
            "ave_precision_score": 0.843044280204039,
            "fpr": 0.08223684210526316,
            "logloss": 1.2533978553245126,
            "mae": 0.2782235686029272,
            "precision": 0.8192771084337349,
            "recall": 0.7024793388429752
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8351936665143623,
            "auditor_fn_violation": 0.02034705841137866,
            "auditor_fp_violation": 0.013799592284773404,
            "ave_precision_score": 0.835506232489746,
            "fpr": 0.09220636663007684,
            "logloss": 1.152284115020293,
            "mae": 0.28991910206795785,
            "precision": 0.7884130982367759,
            "recall": 0.6659574468085107
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6397041702067663,
            "auditor_fn_violation": 0.04047955632883862,
            "auditor_fp_violation": 0.008313350549270376,
            "ave_precision_score": 0.6117402842521492,
            "fpr": 0.14364035087719298,
            "logloss": 4.917884064956867,
            "mae": 0.3741791168146917,
            "precision": 0.6880952380952381,
            "recall": 0.5971074380165289
        },
        "train": {
            "accuracy": 0.6399560922063666,
            "auc_prc": 0.6426939818676225,
            "auditor_fn_violation": 0.030198285727631557,
            "auditor_fp_violation": 0.018354652508643418,
            "ave_precision_score": 0.6124301751779566,
            "fpr": 0.14709110867178923,
            "logloss": 4.583610531383808,
            "mae": 0.3699069154524613,
            "precision": 0.6731707317073171,
            "recall": 0.5872340425531914
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6129385964912281,
            "auc_prc": 0.6197615325318642,
            "auditor_fn_violation": 0.037552559083659565,
            "auditor_fp_violation": 0.01438760452533202,
            "ave_precision_score": 0.5961874114194479,
            "fpr": 0.1875,
            "logloss": 4.463057461962798,
            "mae": 0.3915307197592533,
            "precision": 0.638477801268499,
            "recall": 0.6239669421487604
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.6247344663216552,
            "auditor_fn_violation": 0.026905201205128806,
            "auditor_fp_violation": 0.014180425188736308,
            "ave_precision_score": 0.597556709811971,
            "fpr": 0.1877058177826564,
            "logloss": 4.2232321182356065,
            "mae": 0.3797187429579608,
            "precision": 0.638477801268499,
            "recall": 0.6425531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.6671810895283331,
            "auditor_fn_violation": 0.027756633318834274,
            "auditor_fp_violation": 0.013424331857681585,
            "ave_precision_score": 0.6258993488580784,
            "fpr": 0.21710526315789475,
            "logloss": 3.4122528357764077,
            "mae": 0.3651214708311352,
            "precision": 0.6514084507042254,
            "recall": 0.7644628099173554
        },
        "train": {
            "accuracy": 0.6597145993413831,
            "auc_prc": 0.6740688974621751,
            "auditor_fn_violation": 0.018516009996029613,
            "auditor_fp_violation": 0.0277087051432355,
            "ave_precision_score": 0.6366946268984996,
            "fpr": 0.2217343578485181,
            "logloss": 3.073274574389364,
            "mae": 0.36251750945651395,
            "precision": 0.6418439716312057,
            "recall": 0.7702127659574468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6030701754385965,
            "auc_prc": 0.6177137148157084,
            "auditor_fn_violation": 0.04082390894591852,
            "auditor_fp_violation": 0.022209071159206437,
            "ave_precision_score": 0.5874205055454148,
            "fpr": 0.20285087719298245,
            "logloss": 5.21352623739153,
            "mae": 0.3997911187302426,
            "precision": 0.6239837398373984,
            "recall": 0.6342975206611571
        },
        "train": {
            "accuracy": 0.6212952799121844,
            "auc_prc": 0.6250727289177608,
            "auditor_fn_violation": 0.032136768106126076,
            "auditor_fp_violation": 0.010506507762270674,
            "ave_precision_score": 0.5916187341390363,
            "fpr": 0.2074643249176729,
            "logloss": 4.963892047105218,
            "mae": 0.3843643679598093,
            "precision": 0.6242544731610338,
            "recall": 0.6680851063829787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8522814298515564,
            "auditor_fn_violation": 0.011798608090474123,
            "auditor_fp_violation": 0.011149368748975241,
            "ave_precision_score": 0.8525785883440276,
            "fpr": 0.07017543859649122,
            "logloss": 1.134953166906816,
            "mae": 0.3073646819183388,
            "precision": 0.8341968911917098,
            "recall": 0.6652892561983471
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8345404609995349,
            "auditor_fn_violation": 0.021164490739659484,
            "auditor_fp_violation": 0.011213413283352127,
            "ave_precision_score": 0.8348801294716275,
            "fpr": 0.07135016465422613,
            "logloss": 1.0657708807841104,
            "mae": 0.3124655055275222,
            "precision": 0.8329048843187661,
            "recall": 0.6893617021276596
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8448324949136814,
            "auditor_fn_violation": 0.009397201681890685,
            "auditor_fp_violation": 0.005321056730611577,
            "ave_precision_score": 0.8453544520764918,
            "fpr": 0.051535087719298246,
            "logloss": 1.2151697798210024,
            "mae": 0.29226364013370665,
            "precision": 0.8653295128939829,
            "recall": 0.6239669421487604
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8380657907841044,
            "auditor_fn_violation": 0.02589392063899852,
            "auditor_fp_violation": 0.007362769476616114,
            "ave_precision_score": 0.8383510553900849,
            "fpr": 0.05598243688254665,
            "logloss": 1.1527697449207956,
            "mae": 0.3042673075915933,
            "precision": 0.8416149068322981,
            "recall": 0.5765957446808511
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.6241854044409886,
            "auditor_fn_violation": 0.01146331738437002,
            "auditor_fp_violation": 0.0032126168224299112,
            "ave_precision_score": 0.6215378241084488,
            "fpr": 0.16776315789473684,
            "logloss": 2.1073815628512667,
            "mae": 0.3788646748983275,
            "precision": 0.6785714285714286,
            "recall": 0.6673553719008265
        },
        "train": {
            "accuracy": 0.6410537870472008,
            "auc_prc": 0.6270993724279441,
            "auditor_fn_violation": 0.016119765513697835,
            "auditor_fp_violation": 0.025184753740500963,
            "ave_precision_score": 0.6228331903966624,
            "fpr": 0.17014270032930845,
            "logloss": 1.906985061699938,
            "mae": 0.38259648174150673,
            "precision": 0.6578366445916115,
            "recall": 0.6340425531914894
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.6208100520447701,
            "auditor_fn_violation": 0.03899340292880963,
            "auditor_fp_violation": 0.027750450893589122,
            "ave_precision_score": 0.5876391590717424,
            "fpr": 0.18969298245614036,
            "logloss": 5.5882411623414034,
            "mae": 0.3970448006821724,
            "precision": 0.6342494714587738,
            "recall": 0.6198347107438017
        },
        "train": {
            "accuracy": 0.6300768386388584,
            "auc_prc": 0.6297029495762754,
            "auditor_fn_violation": 0.03162295349977814,
            "auditor_fp_violation": 0.029936453176221095,
            "ave_precision_score": 0.5939920962841672,
            "fpr": 0.17892425905598244,
            "logloss": 5.255953791949073,
            "mae": 0.3831144142985924,
            "precision": 0.644880174291939,
            "recall": 0.6297872340425532
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.7803197142010363,
            "auditor_fn_violation": 0.033064647672901265,
            "auditor_fp_violation": 0.02879058042302017,
            "ave_precision_score": 0.7813906757572975,
            "fpr": 0.13486842105263158,
            "logloss": 0.6502611706308918,
            "mae": 0.32882264311487425,
            "precision": 0.740506329113924,
            "recall": 0.7252066115702479
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.7806177056420398,
            "auditor_fn_violation": 0.04572015788121541,
            "auditor_fp_violation": 0.03514116953038076,
            "ave_precision_score": 0.7816778208308575,
            "fpr": 0.12184412733260154,
            "logloss": 0.6091880408624716,
            "mae": 0.32138537399003225,
            "precision": 0.75764192139738,
            "recall": 0.7382978723404255
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8313936506870495,
            "auditor_fn_violation": 0.0012958532695374794,
            "auditor_fp_violation": 0.008064846696179703,
            "ave_precision_score": 0.8321474970397766,
            "fpr": 0.043859649122807015,
            "logloss": 1.2412746318346344,
            "mae": 0.30598489637388815,
            "precision": 0.8773006134969326,
            "recall": 0.5909090909090909
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.8247142473827214,
            "auditor_fn_violation": 0.02175537753695962,
            "auditor_fp_violation": 0.008537626539821931,
            "ave_precision_score": 0.8250653780543346,
            "fpr": 0.05378704720087816,
            "logloss": 1.1854818765358073,
            "mae": 0.3140359422859679,
            "precision": 0.8439490445859873,
            "recall": 0.5638297872340425
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8465350465908271,
            "auditor_fn_violation": 0.012795418297810647,
            "auditor_fp_violation": 0.0038479668798163647,
            "ave_precision_score": 0.8469101254557179,
            "fpr": 0.0581140350877193,
            "logloss": 1.1989781243473543,
            "mae": 0.28686658214956107,
            "precision": 0.8555858310626703,
            "recall": 0.6487603305785123
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8359835537975666,
            "auditor_fn_violation": 0.023119321764719633,
            "auditor_fp_violation": 0.01049406224253331,
            "ave_precision_score": 0.8362587521335947,
            "fpr": 0.06476399560922064,
            "logloss": 1.0945202266605676,
            "mae": 0.3018913099637994,
            "precision": 0.8264705882352941,
            "recall": 0.597872340425532
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8432820107102279,
            "auditor_fn_violation": 0.009979429462084972,
            "auditor_fp_violation": 0.00949950811608461,
            "ave_precision_score": 0.8436374867957951,
            "fpr": 0.06140350877192982,
            "logloss": 1.5878043886829258,
            "mae": 0.28571254081112524,
            "precision": 0.8490566037735849,
            "recall": 0.6508264462809917
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8395032561378706,
            "auditor_fn_violation": 0.02607609127215826,
            "auditor_fp_violation": 0.012388270346557942,
            "ave_precision_score": 0.8397807458796869,
            "fpr": 0.06915477497255763,
            "logloss": 1.2596375883152666,
            "mae": 0.2973159868517087,
            "precision": 0.8210227272727273,
            "recall": 0.6148936170212767
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8402322923100783,
            "auditor_fn_violation": 0.007965419747716397,
            "auditor_fp_violation": 0.011892318412854567,
            "ave_precision_score": 0.8405758604328355,
            "fpr": 0.07785087719298246,
            "logloss": 1.1948108802732438,
            "mae": 0.2818269531347125,
            "precision": 0.8268292682926829,
            "recall": 0.7004132231404959
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8334235157141895,
            "auditor_fn_violation": 0.021087418548707288,
            "auditor_fp_violation": 0.014421868271640891,
            "ave_precision_score": 0.8337308716808799,
            "fpr": 0.09330406147091108,
            "logloss": 0.9810893019552865,
            "mae": 0.2932271199433867,
            "precision": 0.7831632653061225,
            "recall": 0.6531914893617021
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8424789581739804,
            "auditor_fn_violation": 0.013493185442946205,
            "auditor_fp_violation": 0.005779636005902608,
            "ave_precision_score": 0.8428408160970873,
            "fpr": 0.0581140350877193,
            "logloss": 1.2468742657362808,
            "mae": 0.28938556502598584,
            "precision": 0.8523676880222841,
            "recall": 0.6322314049586777
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.8361545696439288,
            "auditor_fn_violation": 0.02904220286334868,
            "auditor_fp_violation": 0.008415660446395905,
            "ave_precision_score": 0.8364586074848321,
            "fpr": 0.06256860592755215,
            "logloss": 1.132408225892881,
            "mae": 0.3027068070223407,
            "precision": 0.8272727272727273,
            "recall": 0.5808510638297872
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.6363479528069456,
            "auditor_fn_violation": 0.03812345947513412,
            "auditor_fp_violation": 0.015525086079685205,
            "ave_precision_score": 0.6064835813553913,
            "fpr": 0.18421052631578946,
            "logloss": 4.999107046559829,
            "mae": 0.3764470650168633,
            "precision": 0.6463157894736842,
            "recall": 0.6342975206611571
        },
        "train": {
            "accuracy": 0.6509330406147091,
            "auc_prc": 0.6410029360229436,
            "auditor_fn_violation": 0.029941378424457574,
            "auditor_fp_violation": 0.02771119424718296,
            "ave_precision_score": 0.6100713725448524,
            "fpr": 0.1778265642151482,
            "logloss": 4.623720508094108,
            "mae": 0.36087397525835996,
            "precision": 0.6596638655462185,
            "recall": 0.6680851063829787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6359649122807017,
            "auc_prc": 0.6357596129200312,
            "auditor_fn_violation": 0.03698165869218501,
            "auditor_fp_violation": 0.008572101983931792,
            "ave_precision_score": 0.6058901255063895,
            "fpr": 0.14692982456140352,
            "logloss": 5.214155835904796,
            "mae": 0.3785893199942296,
            "precision": 0.680952380952381,
            "recall": 0.5909090909090909
        },
        "train": {
            "accuracy": 0.6388583973655324,
            "auc_prc": 0.6383864986884005,
            "auditor_fn_violation": 0.02883200597893361,
            "auditor_fp_violation": 0.016629703473046736,
            "ave_precision_score": 0.6074404302791323,
            "fpr": 0.14489571899012074,
            "logloss": 4.81963648170444,
            "mae": 0.37308724312004315,
            "precision": 0.674074074074074,
            "recall": 0.5808510638297872
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 21353,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.845892599977321,
            "auditor_fn_violation": 0.010688523995940268,
            "auditor_fp_violation": 0.00756271519921299,
            "ave_precision_score": 0.8462077811700316,
            "fpr": 0.06578947368421052,
            "logloss": 1.2766898093191732,
            "mae": 0.2817932045862859,
            "precision": 0.8429319371727748,
            "recall": 0.6652892561983471
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8397150570442443,
            "auditor_fn_violation": 0.021346661372819216,
            "auditor_fp_violation": 0.011006817655712122,
            "ave_precision_score": 0.8400026767354739,
            "fpr": 0.07244785949506037,
            "logloss": 1.1624176881971675,
            "mae": 0.29334792477862137,
            "precision": 0.819672131147541,
            "recall": 0.6382978723404256
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 21353,
        "test": {
            "accuracy": 0.631578947368421,
            "auc_prc": 0.6599946081890165,
            "auditor_fn_violation": 0.016248006379585327,
            "auditor_fp_violation": 0.013260370552549599,
            "ave_precision_score": 0.6384510680327389,
            "fpr": 0.30153508771929827,
            "logloss": 2.75938373635016,
            "mae": 0.3881335216599739,
            "precision": 0.6060171919770774,
            "recall": 0.8739669421487604
        },
        "train": {
            "accuracy": 0.6388583973655324,
            "auc_prc": 0.6746172857753912,
            "auditor_fn_violation": 0.011275895088399468,
            "auditor_fp_violation": 0.019564357027113805,
            "ave_precision_score": 0.6539706418239876,
            "fpr": 0.3018660812294182,
            "logloss": 2.3798690816806465,
            "mae": 0.391984502582062,
            "precision": 0.6020260492040521,
            "recall": 0.8851063829787233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.6394674400269628,
            "auditor_fn_violation": 0.03656934174278672,
            "auditor_fp_violation": 0.01614250286932284,
            "ave_precision_score": 0.6088777573920157,
            "fpr": 0.1875,
            "logloss": 5.0605674714832976,
            "mae": 0.3738152368132356,
            "precision": 0.6474226804123712,
            "recall": 0.6487603305785123
        },
        "train": {
            "accuracy": 0.6487376509330406,
            "auc_prc": 0.642803510967671,
            "auditor_fn_violation": 0.029380853399350725,
            "auditor_fp_violation": 0.028408143352474546,
            "ave_precision_score": 0.6111426480269506,
            "fpr": 0.18660812294182216,
            "logloss": 4.6822485797093565,
            "mae": 0.36049391959476274,
            "precision": 0.6530612244897959,
            "recall": 0.6808510638297872
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.639420661726811,
            "auditor_fn_violation": 0.03847460852544585,
            "auditor_fp_violation": 0.004875286932283982,
            "ave_precision_score": 0.6117642322943428,
            "fpr": 0.14802631578947367,
            "logloss": 4.903742295451317,
            "mae": 0.3745059995336122,
            "precision": 0.6830985915492958,
            "recall": 0.6012396694214877
        },
        "train": {
            "accuracy": 0.6410537870472008,
            "auc_prc": 0.6424662300737873,
            "auditor_fn_violation": 0.028458322628862366,
            "auditor_fp_violation": 0.021981276960107137,
            "ave_precision_score": 0.6125486211550815,
            "fpr": 0.14709110867178923,
            "logloss": 4.578723690512108,
            "mae": 0.3678883190081633,
            "precision": 0.6739659367396593,
            "recall": 0.5893617021276596
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6074561403508771,
            "auc_prc": 0.6142487425095544,
            "auditor_fn_violation": 0.038984341017833844,
            "auditor_fp_violation": 0.014244138383341536,
            "ave_precision_score": 0.5925516411743676,
            "fpr": 0.18311403508771928,
            "logloss": 4.06119696813098,
            "mae": 0.40103866928898413,
            "precision": 0.6369565217391304,
            "recall": 0.6053719008264463
        },
        "train": {
            "accuracy": 0.6223929747530187,
            "auc_prc": 0.6219937866492707,
            "auditor_fn_violation": 0.025527243851741134,
            "auditor_fp_violation": 0.007982556359536128,
            "ave_precision_score": 0.5972337838192676,
            "fpr": 0.18221734357848518,
            "logloss": 3.7484731702210077,
            "mae": 0.38948885407047035,
            "precision": 0.6375545851528385,
            "recall": 0.6212765957446809
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7752192982456141,
            "auc_prc": 0.8634355952431745,
            "auditor_fn_violation": 0.004907024793388436,
            "auditor_fp_violation": 0.013549864731923274,
            "ave_precision_score": 0.8636523224025421,
            "fpr": 0.09978070175438597,
            "logloss": 0.713562008563231,
            "mae": 0.26774890867983286,
            "precision": 0.8026030368763557,
            "recall": 0.7644628099173554
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8523837634640039,
            "auditor_fn_violation": 0.027106056005792103,
            "auditor_fp_violation": 0.020594846061366374,
            "ave_precision_score": 0.8525882480619191,
            "fpr": 0.10428100987925357,
            "logloss": 0.7551903500218273,
            "mae": 0.27860015922070847,
            "precision": 0.7806004618937644,
            "recall": 0.7191489361702128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 21353,
        "test": {
            "accuracy": 0.5833333333333334,
            "auc_prc": 0.6510298238061087,
            "auditor_fn_violation": 0.022550565463244892,
            "auditor_fp_violation": 0.009835116412526646,
            "ave_precision_score": 0.6531724674977812,
            "fpr": 0.07346491228070176,
            "logloss": 9.02283175349695,
            "mae": 0.4252560107117807,
            "precision": 0.7184873949579832,
            "recall": 0.35330578512396693
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.6375336119012471,
            "auditor_fn_violation": 0.023574748347618944,
            "auditor_fp_violation": 0.009722440018817629,
            "ave_precision_score": 0.6372651321943178,
            "fpr": 0.06915477497255763,
            "logloss": 8.827558285020686,
            "mae": 0.4166382157214179,
            "precision": 0.72,
            "recall": 0.3446808510638298
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 21353,
        "test": {
            "accuracy": 0.643640350877193,
            "auc_prc": 0.6409551576510941,
            "auditor_fn_violation": 0.04042518486298391,
            "auditor_fp_violation": 0.010068248893261201,
            "ave_precision_score": 0.6124846452154405,
            "fpr": 0.14692982456140352,
            "logloss": 5.081358691060634,
            "mae": 0.37116731315543,
            "precision": 0.6861826697892272,
            "recall": 0.6053719008264463
        },
        "train": {
            "accuracy": 0.6388583973655324,
            "auc_prc": 0.6403796921316733,
            "auditor_fn_violation": 0.027461055188359767,
            "auditor_fp_violation": 0.01887487523366463,
            "ave_precision_score": 0.6097823292961724,
            "fpr": 0.15587266739846323,
            "logloss": 4.731754781434849,
            "mae": 0.3672455400503819,
            "precision": 0.6658823529411765,
            "recall": 0.6021276595744681
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8465221054702206,
            "auditor_fn_violation": 0.012795418297810647,
            "auditor_fp_violation": 0.0038479668798163647,
            "ave_precision_score": 0.8468972128478647,
            "fpr": 0.0581140350877193,
            "logloss": 1.198791146529382,
            "mae": 0.2868422671731129,
            "precision": 0.8555858310626703,
            "recall": 0.6487603305785123
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8359679798612156,
            "auditor_fn_violation": 0.022350935376135653,
            "auditor_fp_violation": 0.01049406224253331,
            "ave_precision_score": 0.8362432358530679,
            "fpr": 0.06476399560922064,
            "logloss": 1.0942342006822396,
            "mae": 0.3018612047208221,
            "precision": 0.827485380116959,
            "recall": 0.6021276595744681
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 21353,
        "test": {
            "accuracy": 0.631578947368421,
            "auc_prc": 0.6355874337445138,
            "auditor_fn_violation": 0.03835227272727272,
            "auditor_fp_violation": 0.009348356287916054,
            "ave_precision_score": 0.605702925853529,
            "fpr": 0.1524122807017544,
            "logloss": 5.160702474942124,
            "mae": 0.3787053627161271,
            "precision": 0.6737089201877934,
            "recall": 0.5929752066115702
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.6387550517168104,
            "auditor_fn_violation": 0.028913749211761677,
            "auditor_fp_violation": 0.021901625633788096,
            "ave_precision_score": 0.6078234224751755,
            "fpr": 0.14489571899012074,
            "logloss": 4.7646044511295855,
            "mae": 0.37218251411182035,
            "precision": 0.6780487804878049,
            "recall": 0.5914893617021276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.6439892325242549,
            "auditor_fn_violation": 0.03540715166014209,
            "auditor_fp_violation": 0.002469667158550586,
            "ave_precision_score": 0.6217483895824012,
            "fpr": 0.13596491228070176,
            "logloss": 4.2368568861050795,
            "mae": 0.3653910779691257,
            "precision": 0.7012048192771084,
            "recall": 0.6012396694214877
        },
        "train": {
            "accuracy": 0.6531284302963776,
            "auc_prc": 0.6498160696289987,
            "auditor_fn_violation": 0.021654950136627982,
            "auditor_fp_violation": 0.001075292905307029,
            "ave_precision_score": 0.6245424002414363,
            "fpr": 0.1394072447859495,
            "logloss": 3.913348012514784,
            "mae": 0.36188752914738026,
            "precision": 0.6887254901960784,
            "recall": 0.597872340425532
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.800889368704102,
            "auditor_fn_violation": 0.013040089894156887,
            "auditor_fp_violation": 9.22282341367483e-05,
            "ave_precision_score": 0.8020767779655796,
            "fpr": 0.06140350877192982,
            "logloss": 1.2634176576095597,
            "mae": 0.31744337131559824,
            "precision": 0.8486486486486486,
            "recall": 0.6487603305785123
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8075020396122568,
            "auditor_fn_violation": 0.02016021673634304,
            "auditor_fp_violation": 0.010912231705708264,
            "ave_precision_score": 0.8087541638878619,
            "fpr": 0.06695938529088913,
            "logloss": 1.1626603311381194,
            "mae": 0.32054778669818007,
            "precision": 0.8291316526610645,
            "recall": 0.6297872340425532
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.620383696627324,
            "auditor_fn_violation": 0.04100061620994636,
            "auditor_fp_violation": 0.02238071815051648,
            "ave_precision_score": 0.5944804786985594,
            "fpr": 0.18859649122807018,
            "logloss": 4.5709438047318285,
            "mae": 0.3948620416436429,
            "precision": 0.6424116424116424,
            "recall": 0.6384297520661157
        },
        "train": {
            "accuracy": 0.6487376509330406,
            "auc_prc": 0.6319560417262239,
            "auditor_fn_violation": 0.03031506177452881,
            "auditor_fp_violation": 0.023444870081219465,
            "ave_precision_score": 0.6027408582675238,
            "fpr": 0.1756311745334797,
            "logloss": 4.372825870866752,
            "mae": 0.3731359368785486,
            "precision": 0.6595744680851063,
            "recall": 0.6595744680851063
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8420142737765838,
            "auditor_fn_violation": 0.0066491771784834034,
            "auditor_fp_violation": 0.0025414002295458273,
            "ave_precision_score": 0.8423125805284987,
            "fpr": 0.07456140350877193,
            "logloss": 0.881764656480715,
            "mae": 0.29180252670624407,
            "precision": 0.8251928020565553,
            "recall": 0.6632231404958677
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8367286148370114,
            "auditor_fn_violation": 0.013134969755003863,
            "auditor_fp_violation": 0.004726808396245438,
            "ave_precision_score": 0.8369526900469721,
            "fpr": 0.07464324917672886,
            "logloss": 0.7826513211037445,
            "mae": 0.30236001158427106,
            "precision": 0.8136986301369863,
            "recall": 0.6319148936170212
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.7482689048838871,
            "auditor_fn_violation": 0.01765939901406409,
            "auditor_fp_violation": 0.018199704869650765,
            "ave_precision_score": 0.7488570502210857,
            "fpr": 0.13596491228070176,
            "logloss": 1.0367468195466436,
            "mae": 0.34474195455757556,
            "precision": 0.7213483146067415,
            "recall": 0.6632231404958677
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.7327107397275016,
            "auditor_fn_violation": 0.018744891047948242,
            "auditor_fp_violation": 0.027755998118237413,
            "ave_precision_score": 0.7337244229399031,
            "fpr": 0.14489571899012074,
            "logloss": 1.0375505713754916,
            "mae": 0.3554615631185465,
            "precision": 0.695852534562212,
            "recall": 0.6425531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8405128121440298,
            "auditor_fn_violation": 0.005593464549804267,
            "auditor_fp_violation": 0.006384243318576816,
            "ave_precision_score": 0.8408964942747846,
            "fpr": 0.025219298245614034,
            "logloss": 1.2847171928681311,
            "mae": 0.313469131260817,
            "precision": 0.9178571428571428,
            "recall": 0.53099173553719
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.8417163221030403,
            "auditor_fn_violation": 0.022280869747997287,
            "auditor_fp_violation": 0.00607092452787921,
            "ave_precision_score": 0.8419522133283346,
            "fpr": 0.03293084522502744,
            "logloss": 1.2380811347880065,
            "mae": 0.3246411797695618,
            "precision": 0.8837209302325582,
            "recall": 0.4851063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6381578947368421,
            "auc_prc": 0.6391683014706839,
            "auditor_fn_violation": 0.0388755980861244,
            "auditor_fp_violation": 0.00595896868339072,
            "ave_precision_score": 0.6106636271683697,
            "fpr": 0.14692982456140352,
            "logloss": 4.98005504955879,
            "mae": 0.3750779022947116,
            "precision": 0.6824644549763034,
            "recall": 0.5950413223140496
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.6418504453190241,
            "auditor_fn_violation": 0.02827148095382675,
            "auditor_fp_violation": 0.016629703473046736,
            "ave_precision_score": 0.6112506582814261,
            "fpr": 0.14489571899012074,
            "logloss": 4.640768411713458,
            "mae": 0.36904518520922747,
            "precision": 0.6788321167883211,
            "recall": 0.5936170212765958
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 21353,
        "test": {
            "accuracy": 0.4901315789473684,
            "auc_prc": 0.7608393276244825,
            "auditor_fn_violation": 0.005978595766275194,
            "auditor_fp_violation": 0.0015063944909001474,
            "ave_precision_score": 0.761652951476158,
            "fpr": 0.0043859649122807015,
            "logloss": 1.7320430605353367,
            "mae": 0.47173013504116473,
            "precision": 0.8518518518518519,
            "recall": 0.047520661157024795
        },
        "train": {
            "accuracy": 0.4994511525795829,
            "auc_prc": 0.758139009745895,
            "auditor_fn_violation": 0.003218347852488494,
            "auditor_fp_violation": 0.0015830701105908885,
            "ave_precision_score": 0.7590552852800148,
            "fpr": 0.003293084522502744,
            "logloss": 1.6643689282984702,
            "mae": 0.4642800698609452,
            "precision": 0.85,
            "recall": 0.036170212765957444
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.848542490241196,
            "auditor_fn_violation": 0.016125670581412215,
            "auditor_fp_violation": 0.0138265494343335,
            "ave_precision_score": 0.8488533548737983,
            "fpr": 0.12171052631578948,
            "logloss": 1.101876439277222,
            "mae": 0.27475941506945006,
            "precision": 0.76875,
            "recall": 0.762396694214876
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8415177639757188,
            "auditor_fn_violation": 0.016252890207160708,
            "auditor_fp_violation": 0.02163529151140881,
            "ave_precision_score": 0.8418226764155197,
            "fpr": 0.12733260153677278,
            "logloss": 1.0244064552795198,
            "mae": 0.28489372771504595,
            "precision": 0.7557894736842106,
            "recall": 0.7638297872340426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6400058068918973,
            "auditor_fn_violation": 0.03847460852544585,
            "auditor_fp_violation": 0.004875286932283982,
            "ave_precision_score": 0.6123456456861383,
            "fpr": 0.14802631578947367,
            "logloss": 4.89431652996172,
            "mae": 0.3739379495115493,
            "precision": 0.6830985915492958,
            "recall": 0.6012396694214877
        },
        "train": {
            "accuracy": 0.6410537870472008,
            "auc_prc": 0.6428547701303863,
            "auditor_fn_violation": 0.028458322628862366,
            "auditor_fp_violation": 0.021981276960107137,
            "ave_precision_score": 0.612927806378188,
            "fpr": 0.14709110867178923,
            "logloss": 4.569682691179465,
            "mae": 0.3674144835509227,
            "precision": 0.6739659367396593,
            "recall": 0.5893617021276596
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.6174415969687775,
            "auditor_fn_violation": 0.043084855734377266,
            "auditor_fp_violation": 0.01469503197245451,
            "ave_precision_score": 0.596540031258882,
            "fpr": 0.13157894736842105,
            "logloss": 4.162774000835803,
            "mae": 0.41320681833181216,
            "precision": 0.6730245231607629,
            "recall": 0.5103305785123967
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6264900508823512,
            "auditor_fn_violation": 0.03028703552327347,
            "auditor_fp_violation": 0.01942247810210802,
            "ave_precision_score": 0.6018491879272194,
            "fpr": 0.14050493962678376,
            "logloss": 3.894487329713502,
            "mae": 0.39641513961057706,
            "precision": 0.6683937823834197,
            "recall": 0.548936170212766
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8364640288257308,
            "auditor_fn_violation": 0.011730643758155721,
            "auditor_fp_violation": 0.020733419413018532,
            "ave_precision_score": 0.8368061514678006,
            "fpr": 0.14364035087719298,
            "logloss": 0.9490089704168995,
            "mae": 0.29441469925206404,
            "precision": 0.7446393762183235,
            "recall": 0.7892561983471075
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8402231664512371,
            "auditor_fn_violation": 0.016203844267463862,
            "auditor_fp_violation": 0.020251349716615515,
            "ave_precision_score": 0.840482160895559,
            "fpr": 0.141602634467618,
            "logloss": 0.906412506490185,
            "mae": 0.3036264612035658,
            "precision": 0.7435387673956262,
            "recall": 0.7957446808510639
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.7547056283621036,
            "auditor_fn_violation": 0.046251993620414676,
            "auditor_fp_violation": 0.03238235776356781,
            "ave_precision_score": 0.7551553881573335,
            "fpr": 0.11074561403508772,
            "logloss": 1.689347979469874,
            "mae": 0.3391722036038268,
            "precision": 0.7416879795396419,
            "recall": 0.5991735537190083
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.7363101232274575,
            "auditor_fn_violation": 0.046915944601443355,
            "auditor_fp_violation": 0.03303538759082118,
            "ave_precision_score": 0.736860030421289,
            "fpr": 0.1119648737650933,
            "logloss": 1.6082463269880265,
            "mae": 0.34896063232319635,
            "precision": 0.7287234042553191,
            "recall": 0.5829787234042553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8139346343093807,
            "auditor_fn_violation": 0.01545055821371611,
            "auditor_fp_violation": 0.036296933923594045,
            "ave_precision_score": 0.8146235172334485,
            "fpr": 0.16228070175438597,
            "logloss": 1.3591616874560608,
            "mae": 0.2862677916928054,
            "precision": 0.7279411764705882,
            "recall": 0.8181818181818182
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8126748633430395,
            "auditor_fn_violation": 0.017796669547142492,
            "auditor_fp_violation": 0.04614051987425048,
            "ave_precision_score": 0.813000750406027,
            "fpr": 0.18111964873765093,
            "logloss": 1.2454557404330269,
            "mae": 0.29957356722191036,
            "precision": 0.7,
            "recall": 0.8191489361702128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.6556190712671843,
            "auditor_fn_violation": 0.0328086486878353,
            "auditor_fp_violation": 0.005313371044433523,
            "ave_precision_score": 0.6280223369983963,
            "fpr": 0.16447368421052633,
            "logloss": 4.562673654445053,
            "mae": 0.35557417229348476,
            "precision": 0.6739130434782609,
            "recall": 0.640495867768595
        },
        "train": {
            "accuracy": 0.6586169045005489,
            "auc_prc": 0.6596761352128074,
            "auditor_fn_violation": 0.020972978022747977,
            "auditor_fp_violation": 0.008072164101645057,
            "ave_precision_score": 0.6306399224687327,
            "fpr": 0.17233809001097694,
            "logloss": 4.2058375441894436,
            "mae": 0.34765796353879236,
            "precision": 0.6680761099365751,
            "recall": 0.6723404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.6410239495269304,
            "auditor_fn_violation": 0.03835227272727272,
            "auditor_fp_violation": 0.006789022790621424,
            "ave_precision_score": 0.6133561713833069,
            "fpr": 0.14692982456140352,
            "logloss": 4.939712757605125,
            "mae": 0.37649213311170493,
            "precision": 0.6817102137767221,
            "recall": 0.5929752066115702
        },
        "train": {
            "accuracy": 0.6465422612513722,
            "auc_prc": 0.6463658386815861,
            "auditor_fn_violation": 0.02671835953009319,
            "auditor_fp_violation": 0.017655214299404365,
            "ave_precision_score": 0.6160907915374442,
            "fpr": 0.14270032930845225,
            "logloss": 4.627618616306935,
            "mae": 0.36605857374878603,
            "precision": 0.6813725490196079,
            "recall": 0.5914893617021276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 21353,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.6670338296567739,
            "auditor_fn_violation": 0.03542301000434972,
            "auditor_fp_violation": 0.014039186751926551,
            "ave_precision_score": 0.6677808767162277,
            "fpr": 0.07456140350877193,
            "logloss": 3.422044885897445,
            "mae": 0.42892989322441366,
            "precision": 0.7068965517241379,
            "recall": 0.33884297520661155
        },
        "train": {
            "accuracy": 0.6114160263446762,
            "auc_prc": 0.6862679554532509,
            "auditor_fn_violation": 0.029707826330663066,
            "auditor_fp_violation": 0.017894168278361475,
            "ave_precision_score": 0.6871259029422082,
            "fpr": 0.07683863885839737,
            "logloss": 2.839500834979807,
            "mae": 0.39926043237239156,
            "precision": 0.7265625,
            "recall": 0.39574468085106385
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7350001378393206,
            "auditor_fn_violation": 0.01015387124836886,
            "auditor_fp_violation": 0.018302180685358254,
            "ave_precision_score": 0.7201575732794845,
            "fpr": 0.14912280701754385,
            "logloss": 1.705379442632635,
            "mae": 0.3366416059216236,
            "precision": 0.7130801687763713,
            "recall": 0.6983471074380165
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7496487935711779,
            "auditor_fn_violation": 0.020832846766471264,
            "auditor_fp_violation": 0.03007584299727941,
            "ave_precision_score": 0.7398942720393928,
            "fpr": 0.15587266739846323,
            "logloss": 1.3328480220461378,
            "mae": 0.340166813681889,
            "precision": 0.69593147751606,
            "recall": 0.6914893617021277
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 21353,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.8579351672589267,
            "auditor_fn_violation": 0.010566188197767145,
            "auditor_fp_violation": 0.008794986883095593,
            "ave_precision_score": 0.858104043528938,
            "fpr": 0.12171052631578948,
            "logloss": 1.3135864555865833,
            "mae": 0.30338347437388424,
            "precision": 0.7757575757575758,
            "recall": 0.7933884297520661
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.837956549698571,
            "auditor_fn_violation": 0.017170749935773173,
            "auditor_fp_violation": 0.01164402826626444,
            "ave_precision_score": 0.8382155135642717,
            "fpr": 0.12733260153677278,
            "logloss": 1.1993181844245555,
            "mae": 0.31562250350914917,
            "precision": 0.763265306122449,
            "recall": 0.7957446808510639
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8422344504206498,
            "auditor_fn_violation": 0.007122662026968253,
            "auditor_fp_violation": 0.0045038121003443225,
            "ave_precision_score": 0.8426119285105531,
            "fpr": 0.05921052631578947,
            "logloss": 1.1349164185567227,
            "mae": 0.28916868457426925,
            "precision": 0.8524590163934426,
            "recall": 0.6446280991735537
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.8354419551434819,
            "auditor_fn_violation": 0.02182077212322209,
            "auditor_fp_violation": 0.008004958295063366,
            "ave_precision_score": 0.8357717321035524,
            "fpr": 0.06586169045005488,
            "logloss": 0.9556706859853618,
            "mae": 0.3021506519467774,
            "precision": 0.8230088495575221,
            "recall": 0.5936170212765958
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8441019116461617,
            "auditor_fn_violation": 0.011431600695954764,
            "auditor_fp_violation": 0.006891498606328909,
            "ave_precision_score": 0.8444554091982994,
            "fpr": 0.05482456140350877,
            "logloss": 1.262384876001202,
            "mae": 0.2900229476654211,
            "precision": 0.8607242339832869,
            "recall": 0.6384297520661157
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8371722634684649,
            "auditor_fn_violation": 0.026379708994091144,
            "auditor_fp_violation": 0.009269423100378097,
            "ave_precision_score": 0.8373874596045393,
            "fpr": 0.06147091108671789,
            "logloss": 1.1657310046743752,
            "mae": 0.30321146118247455,
            "precision": 0.8318318318318318,
            "recall": 0.5893617021276596
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8443627001679419,
            "auditor_fn_violation": 0.00873568218065826,
            "auditor_fp_violation": 0.009302242170847684,
            "ave_precision_score": 0.8447152521347561,
            "fpr": 0.06030701754385965,
            "logloss": 1.1365631883626637,
            "mae": 0.28578655993575075,
            "precision": 0.8501362397820164,
            "recall": 0.6446280991735537
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.840358469527682,
            "auditor_fn_violation": 0.024978396431324015,
            "auditor_fp_violation": 0.010790265612282237,
            "ave_precision_score": 0.8406337657884337,
            "fpr": 0.06366630076838639,
            "logloss": 0.9402208851454793,
            "mae": 0.29769793237670283,
            "precision": 0.8328530259365994,
            "recall": 0.6148936170212767
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8288790876786785,
            "auditor_fn_violation": 0.014879657822241555,
            "auditor_fp_violation": 0.03284349893425152,
            "ave_precision_score": 0.8295300527480984,
            "fpr": 0.13048245614035087,
            "logloss": 1.3922361756448887,
            "mae": 0.27555719060594613,
            "precision": 0.7571428571428571,
            "recall": 0.7665289256198347
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8251394435620805,
            "auditor_fn_violation": 0.023542051054487707,
            "auditor_fp_violation": 0.039380113552922084,
            "ave_precision_score": 0.8254527041772406,
            "fpr": 0.14709110867178923,
            "logloss": 1.2576334756705827,
            "mae": 0.29038201560445803,
            "precision": 0.7298387096774194,
            "recall": 0.7702127659574468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8175264323161914,
            "auditor_fn_violation": 0.014467340872843265,
            "auditor_fp_violation": 0.010421790457452043,
            "ave_precision_score": 0.8179946509530815,
            "fpr": 0.0625,
            "logloss": 1.5907002556221939,
            "mae": 0.34053773786468605,
            "precision": 0.8357348703170029,
            "recall": 0.5991735537190083
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.8105640463305516,
            "auditor_fn_violation": 0.023658827101384967,
            "auditor_fp_violation": 0.0037635251685745673,
            "ave_precision_score": 0.810749427718593,
            "fpr": 0.06915477497255763,
            "logloss": 1.429619128773674,
            "mae": 0.3521911608364983,
            "precision": 0.811377245508982,
            "recall": 0.5765957446808511
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 21353,
        "test": {
            "accuracy": 0.618421052631579,
            "auc_prc": 0.6306060309244108,
            "auditor_fn_violation": 0.04101420907641004,
            "auditor_fp_violation": 0.022109157238891626,
            "ave_precision_score": 0.6066793029300401,
            "fpr": 0.1425438596491228,
            "logloss": 4.819633772749823,
            "mae": 0.3863295818609143,
            "precision": 0.6717171717171717,
            "recall": 0.5495867768595041
        },
        "train": {
            "accuracy": 0.6366630076838639,
            "auc_prc": 0.6377334889204654,
            "auditor_fn_violation": 0.026043393979027038,
            "auditor_fp_violation": 0.009493442455650395,
            "ave_precision_score": 0.6094752214910346,
            "fpr": 0.13391877058177826,
            "logloss": 4.487341878673023,
            "mae": 0.37887797945798635,
            "precision": 0.6814621409921671,
            "recall": 0.5553191489361702
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.6388484790410824,
            "auditor_fn_violation": 0.036012034217775844,
            "auditor_fp_violation": 0.012202307755369733,
            "ave_precision_score": 0.6108608868793588,
            "fpr": 0.13706140350877194,
            "logloss": 4.980367337915451,
            "mae": 0.3774383517481553,
            "precision": 0.6928746928746928,
            "recall": 0.5826446280991735
        },
        "train": {
            "accuracy": 0.6366630076838639,
            "auc_prc": 0.6420109164721294,
            "auditor_fn_violation": 0.03221150477614033,
            "auditor_fp_violation": 0.014782788344024033,
            "ave_precision_score": 0.611766340156181,
            "fpr": 0.14050493962678376,
            "logloss": 4.6542941440118755,
            "mae": 0.3717208281990071,
            "precision": 0.6759493670886076,
            "recall": 0.5680851063829787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 21353,
        "test": {
            "accuracy": 0.5997807017543859,
            "auc_prc": 0.6171516920446299,
            "auditor_fn_violation": 0.038726076555023935,
            "auditor_fp_violation": 0.020830771437940654,
            "ave_precision_score": 0.5859700916785856,
            "fpr": 0.19188596491228072,
            "logloss": 5.312128443553653,
            "mae": 0.4050664501967506,
            "precision": 0.6268656716417911,
            "recall": 0.6074380165289256
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.6224358295049326,
            "auditor_fn_violation": 0.033047621271924704,
            "auditor_fp_violation": 0.021299262478500364,
            "ave_precision_score": 0.589184239351761,
            "fpr": 0.18990120746432493,
            "logloss": 5.0332986443426755,
            "mae": 0.38554891081948456,
            "precision": 0.6342494714587738,
            "recall": 0.6382978723404256
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6491228070175439,
            "auc_prc": 0.6411463729489824,
            "auditor_fn_violation": 0.03716516238944469,
            "auditor_fp_violation": 0.008884653221839645,
            "ave_precision_score": 0.6184181405963096,
            "fpr": 0.14144736842105263,
            "logloss": 4.4352421781023965,
            "mae": 0.3645250441720452,
            "precision": 0.6943127962085308,
            "recall": 0.6053719008264463
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.6459458579868255,
            "auditor_fn_violation": 0.02165027909475209,
            "auditor_fp_violation": 0.0111412292688755,
            "ave_precision_score": 0.6203539523820637,
            "fpr": 0.14489571899012074,
            "logloss": 4.076811292228963,
            "mae": 0.3626040999186231,
            "precision": 0.6788321167883211,
            "recall": 0.5936170212765958
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.7846436701316314,
            "auditor_fn_violation": 0.009569377990430622,
            "auditor_fp_violation": 0.011415805869814725,
            "ave_precision_score": 0.7841641735589275,
            "fpr": 0.10526315789473684,
            "logloss": 1.3302599891535045,
            "mae": 0.2953493253722034,
            "precision": 0.7813211845102506,
            "recall": 0.7086776859504132
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7766784649931927,
            "auditor_fn_violation": 0.016537823761590027,
            "auditor_fp_violation": 0.015733626051957557,
            "ave_precision_score": 0.7755238226906005,
            "fpr": 0.10757409440175632,
            "logloss": 1.111297616822425,
            "mae": 0.30488026137814944,
            "precision": 0.7649880095923262,
            "recall": 0.6787234042553192
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 21353,
        "test": {
            "accuracy": 0.631578947368421,
            "auc_prc": 0.6356333100835372,
            "auditor_fn_violation": 0.03835227272727272,
            "auditor_fp_violation": 0.009348356287916054,
            "ave_precision_score": 0.6057487686243808,
            "fpr": 0.1524122807017544,
            "logloss": 5.158713373872607,
            "mae": 0.37863647392886457,
            "precision": 0.6737089201877934,
            "recall": 0.5929752066115702
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.6388025179464745,
            "auditor_fn_violation": 0.028913749211761677,
            "auditor_fp_violation": 0.021901625633788096,
            "ave_precision_score": 0.6078611588834428,
            "fpr": 0.14489571899012074,
            "logloss": 4.7625542336101345,
            "mae": 0.3721421768447955,
            "precision": 0.6780487804878049,
            "recall": 0.5914893617021276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8419997227723253,
            "auditor_fn_violation": 0.010926399159054669,
            "auditor_fp_violation": 0.013321856041974096,
            "ave_precision_score": 0.8423544993816008,
            "fpr": 0.08223684210526316,
            "logloss": 1.1537237316879367,
            "mae": 0.28058745229757376,
            "precision": 0.8188405797101449,
            "recall": 0.7004132231404959
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8406804427027066,
            "auditor_fn_violation": 0.02096363593899619,
            "auditor_fp_violation": 0.01604725314933877,
            "ave_precision_score": 0.840965069352486,
            "fpr": 0.09220636663007684,
            "logloss": 0.9415789457637859,
            "mae": 0.2910259452547362,
            "precision": 0.7857142857142857,
            "recall": 0.6553191489361702
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.83828740872039,
            "auditor_fn_violation": 0.010294330868493549,
            "auditor_fp_violation": 0.004129775373011971,
            "ave_precision_score": 0.838658560154392,
            "fpr": 0.04824561403508772,
            "logloss": 1.3144904868310998,
            "mae": 0.2988449763698985,
            "precision": 0.8662613981762918,
            "recall": 0.5888429752066116
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.836086868175513,
            "auditor_fn_violation": 0.02675339234416237,
            "auditor_fp_violation": 0.006650885747639708,
            "ave_precision_score": 0.8363671397881922,
            "fpr": 0.048298572996706916,
            "logloss": 1.095239819254912,
            "mae": 0.3092834398279074,
            "precision": 0.8528428093645485,
            "recall": 0.5425531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6337719298245614,
            "auc_prc": 0.6371414464441063,
            "auditor_fn_violation": 0.03712664926779761,
            "auditor_fp_violation": 0.0032177406132152912,
            "ave_precision_score": 0.6080704672843015,
            "fpr": 0.1513157894736842,
            "logloss": 5.076058847752904,
            "mae": 0.3768600217119141,
            "precision": 0.676056338028169,
            "recall": 0.5950413223140496
        },
        "train": {
            "accuracy": 0.6421514818880352,
            "auc_prc": 0.640437807656535,
            "auditor_fn_violation": 0.027360627788028117,
            "auditor_fp_violation": 0.01958426985869357,
            "ave_precision_score": 0.6095040017959865,
            "fpr": 0.145993413830955,
            "logloss": 4.714341882410897,
            "mae": 0.3695374275558396,
            "precision": 0.675609756097561,
            "recall": 0.5893617021276596
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8416215224787171,
            "auditor_fn_violation": 0.010375888067275626,
            "auditor_fp_violation": 0.011139121167404498,
            "ave_precision_score": 0.8422348669256048,
            "fpr": 0.07456140350877193,
            "logloss": 1.2592989498999132,
            "mae": 0.281049902530951,
            "precision": 0.8251928020565553,
            "recall": 0.6632231404958677
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8358889095186359,
            "auditor_fn_violation": 0.02182310764416004,
            "auditor_fp_violation": 0.014966982036136816,
            "ave_precision_score": 0.8360715056197773,
            "fpr": 0.07683863885839737,
            "logloss": 1.1805044305517753,
            "mae": 0.2918838288161612,
            "precision": 0.8087431693989071,
            "recall": 0.6297872340425532
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8400347859080246,
            "auditor_fn_violation": 0.01040307380020299,
            "auditor_fp_violation": 0.013739444990982126,
            "ave_precision_score": 0.8407292420288579,
            "fpr": 0.09100877192982457,
            "logloss": 1.2075604364523218,
            "mae": 0.2799800674396811,
            "precision": 0.801909307875895,
            "recall": 0.6942148760330579
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8312478878400322,
            "auditor_fn_violation": 0.02034705841137866,
            "auditor_fp_violation": 0.013306749703174357,
            "ave_precision_score": 0.8315308335829334,
            "fpr": 0.0889132821075741,
            "logloss": 1.0416389483050723,
            "mae": 0.2907290549913284,
            "precision": 0.7944162436548223,
            "recall": 0.6659574468085107
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 21353,
        "test": {
            "accuracy": 0.5997807017543859,
            "auc_prc": 0.6172021197198208,
            "auditor_fn_violation": 0.038726076555023935,
            "auditor_fp_violation": 0.020830771437940654,
            "ave_precision_score": 0.5860296040124804,
            "fpr": 0.19188596491228072,
            "logloss": 5.311431878222632,
            "mae": 0.4050610074803299,
            "precision": 0.6268656716417911,
            "recall": 0.6074380165289256
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.6224664369398155,
            "auditor_fn_violation": 0.033047621271924704,
            "auditor_fp_violation": 0.021299262478500364,
            "ave_precision_score": 0.5892147402825839,
            "fpr": 0.18990120746432493,
            "logloss": 5.032848961136172,
            "mae": 0.38554781256578324,
            "precision": 0.6342494714587738,
            "recall": 0.6382978723404256
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6337719298245614,
            "auc_prc": 0.6367593315265908,
            "auditor_fn_violation": 0.034716180948238376,
            "auditor_fp_violation": 0.010954664699131006,
            "ave_precision_score": 0.6066370833254593,
            "fpr": 0.14035087719298245,
            "logloss": 5.181050307096844,
            "mae": 0.38113564532372535,
            "precision": 0.6847290640394089,
            "recall": 0.5743801652892562
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.6391899347658041,
            "auditor_fn_violation": 0.03157157203914334,
            "auditor_fp_violation": 0.015768473507222137,
            "ave_precision_score": 0.6082530557726892,
            "fpr": 0.145993413830955,
            "logloss": 4.781110407375744,
            "mae": 0.3751597104221345,
            "precision": 0.6683291770573566,
            "recall": 0.5702127659574469
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6395789163340202,
            "auditor_fn_violation": 0.03743928519646224,
            "auditor_fp_violation": 0.007898323495655026,
            "ave_precision_score": 0.6153618897851043,
            "fpr": 0.13925438596491227,
            "logloss": 4.686259889544029,
            "mae": 0.36951645188070037,
            "precision": 0.691747572815534,
            "recall": 0.5888429752066116
        },
        "train": {
            "accuracy": 0.6465422612513722,
            "auc_prc": 0.6424330786365982,
            "auditor_fn_violation": 0.02118317490716305,
            "auditor_fp_violation": 0.010491573138585845,
            "ave_precision_score": 0.6146433324309417,
            "fpr": 0.14270032930845225,
            "logloss": 4.373980531075164,
            "mae": 0.36705010038594027,
            "precision": 0.6813725490196079,
            "recall": 0.5914893617021276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.841090721753157,
            "auditor_fn_violation": 0.011252627954182986,
            "auditor_fp_violation": 0.006358624364649944,
            "ave_precision_score": 0.8414349443529958,
            "fpr": 0.027412280701754384,
            "logloss": 1.3050495265418407,
            "mae": 0.31088162056532814,
            "precision": 0.9119718309859155,
            "recall": 0.5351239669421488
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.8334814257882219,
            "auditor_fn_violation": 0.02093560968774086,
            "auditor_fp_violation": 0.00308399979091527,
            "ave_precision_score": 0.8337566470905782,
            "fpr": 0.038419319429198684,
            "logloss": 1.2545689004435407,
            "mae": 0.3242741924654243,
            "precision": 0.8694029850746269,
            "recall": 0.4957446808510638
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6402880127496648,
            "auditor_fn_violation": 0.037378117297375675,
            "auditor_fp_violation": 0.004073413674372852,
            "ave_precision_score": 0.6120794464056551,
            "fpr": 0.14802631578947367,
            "logloss": 5.001321991611562,
            "mae": 0.37411059831015897,
            "precision": 0.6830985915492958,
            "recall": 0.6012396694214877
        },
        "train": {
            "accuracy": 0.6421514818880352,
            "auc_prc": 0.6426501413441486,
            "auditor_fn_violation": 0.027173786112992506,
            "auditor_fp_violation": 0.019556889715271402,
            "ave_precision_score": 0.6120475186462468,
            "fpr": 0.14818880351262348,
            "logloss": 4.6663597048406515,
            "mae": 0.3658716926035406,
            "precision": 0.6739130434782609,
            "recall": 0.5936170212765958
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7730263157894737,
            "auc_prc": 0.8698890637366843,
            "auditor_fn_violation": 0.0048481223720458246,
            "auditor_fp_violation": 0.010357743072634857,
            "ave_precision_score": 0.8701148911198102,
            "fpr": 0.049342105263157895,
            "logloss": 0.6524142605534522,
            "mae": 0.3043999770230393,
            "precision": 0.8773841961852861,
            "recall": 0.6652892561983471
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8515304535052558,
            "auditor_fn_violation": 0.0206693603008151,
            "auditor_fp_violation": 0.010471660307006081,
            "ave_precision_score": 0.8518576774965553,
            "fpr": 0.0570801317233809,
            "logloss": 0.7302972793997439,
            "mae": 0.3148208711431201,
            "precision": 0.8539325842696629,
            "recall": 0.6468085106382979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 21353,
        "test": {
            "accuracy": 0.5866228070175439,
            "auc_prc": 0.7451820814308228,
            "auditor_fn_violation": 0.003740303755255909,
            "auditor_fp_violation": 0.014664289227742246,
            "ave_precision_score": 0.7457437059293375,
            "fpr": 0.40350877192982454,
            "logloss": 0.8893204262577857,
            "mae": 0.41361053378255874,
            "precision": 0.5634638196915777,
            "recall": 0.981404958677686
        },
        "train": {
            "accuracy": 0.557628979143798,
            "auc_prc": 0.7416576822947982,
            "auditor_fn_violation": 0.00581544713548357,
            "auditor_fp_violation": 0.014954536516399473,
            "ave_precision_score": 0.742101431735429,
            "fpr": 0.429198682766191,
            "logloss": 0.9754550856799293,
            "mae": 0.42488460564979735,
            "precision": 0.5394581861012956,
            "recall": 0.9744680851063829
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6491228070175439,
            "auc_prc": 0.850380285909263,
            "auditor_fn_violation": 0.00525817384370016,
            "auditor_fp_violation": 0.009148528447286463,
            "ave_precision_score": 0.8505221159896764,
            "fpr": 0.2883771929824561,
            "logloss": 1.1976172151157434,
            "mae": 0.3479973343445749,
            "precision": 0.618840579710145,
            "recall": 0.8822314049586777
        },
        "train": {
            "accuracy": 0.6311745334796927,
            "auc_prc": 0.8385384009625951,
            "auditor_fn_violation": 0.0017049302847000018,
            "auditor_fp_violation": 0.02218538348379969,
            "ave_precision_score": 0.8386979298672785,
            "fpr": 0.3194291986827662,
            "logloss": 1.1859239062461648,
            "mae": 0.36338160110508233,
            "precision": 0.5935754189944135,
            "recall": 0.9042553191489362
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7071857940801708,
            "auditor_fn_violation": 0.02971400608960418,
            "auditor_fp_violation": 0.012299659780291858,
            "ave_precision_score": 0.6586658609434108,
            "fpr": 0.1611842105263158,
            "logloss": 5.102781079928398,
            "mae": 0.3335663756663761,
            "precision": 0.6975308641975309,
            "recall": 0.7004132231404959
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.7122343797964745,
            "auditor_fn_violation": 0.018593082186981812,
            "auditor_fp_violation": 0.009789645825399318,
            "ave_precision_score": 0.6660962059489623,
            "fpr": 0.1712403951701427,
            "logloss": 4.516269735386691,
            "mae": 0.3424928177134279,
            "precision": 0.6729559748427673,
            "recall": 0.6829787234042554
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.6392198825899363,
            "auditor_fn_violation": 0.037647709148905334,
            "auditor_fp_violation": 0.006158796524020332,
            "ave_precision_score": 0.611033398498422,
            "fpr": 0.14473684210526316,
            "logloss": 4.973273183711567,
            "mae": 0.37541176323710923,
            "precision": 0.6864608076009501,
            "recall": 0.5971074380165289
        },
        "train": {
            "accuracy": 0.6399560922063666,
            "auc_prc": 0.6415823904562883,
            "auditor_fn_violation": 0.02901884765396922,
            "auditor_fp_violation": 0.016211534009871784,
            "ave_precision_score": 0.6113117732657538,
            "fpr": 0.141602634467618,
            "logloss": 4.65212058652217,
            "mae": 0.36969832452179646,
            "precision": 0.6775,
            "recall": 0.5765957446808511
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6381578947368421,
            "auc_prc": 0.6378590292509223,
            "auditor_fn_violation": 0.03652856314339568,
            "auditor_fp_violation": 0.008134017871782266,
            "ave_precision_score": 0.6090660582217768,
            "fpr": 0.14144736842105263,
            "logloss": 5.037049276944384,
            "mae": 0.3780253411073256,
            "precision": 0.6868932038834952,
            "recall": 0.5847107438016529
        },
        "train": {
            "accuracy": 0.6366630076838639,
            "auc_prc": 0.6399939093287093,
            "auditor_fn_violation": 0.03057196907770278,
            "auditor_fp_violation": 0.01958426985869357,
            "ave_precision_score": 0.6090486265205148,
            "fpr": 0.145993413830955,
            "logloss": 4.707266734556607,
            "mae": 0.3727653499339762,
            "precision": 0.671604938271605,
            "recall": 0.5787234042553191
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6282894736842105,
            "auc_prc": 0.6228617786799446,
            "auditor_fn_violation": 0.041177323473974195,
            "auditor_fp_violation": 0.01836878996556813,
            "ave_precision_score": 0.5983071017017602,
            "fpr": 0.1611842105263158,
            "logloss": 4.766309988941268,
            "mae": 0.3920488743320073,
            "precision": 0.6651480637813212,
            "recall": 0.6033057851239669
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.6307960758635601,
            "auditor_fn_violation": 0.032907490015647994,
            "auditor_fp_violation": 0.019765974446858882,
            "ave_precision_score": 0.602677475430755,
            "fpr": 0.15916575192096596,
            "logloss": 4.42936221743216,
            "mae": 0.37733734499061955,
            "precision": 0.6697038724373576,
            "recall": 0.625531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6386897402056735,
            "auditor_fn_violation": 0.04002646078004931,
            "auditor_fp_violation": 0.008597720937858669,
            "ave_precision_score": 0.6146408086153309,
            "fpr": 0.13596491228070176,
            "logloss": 4.7747019143772516,
            "mae": 0.37041398093325567,
            "precision": 0.6945812807881774,
            "recall": 0.5826446280991735
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.6415027970902286,
            "auditor_fn_violation": 0.027092042880164435,
            "auditor_fp_violation": 0.011365248624147799,
            "ave_precision_score": 0.6142574424735979,
            "fpr": 0.141602634467618,
            "logloss": 4.422819176268923,
            "mae": 0.3669269402400359,
            "precision": 0.6806930693069307,
            "recall": 0.5851063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6282894736842105,
            "auc_prc": 0.623030406781504,
            "auditor_fn_violation": 0.041177323473974195,
            "auditor_fp_violation": 0.01836878996556813,
            "ave_precision_score": 0.5984760601043146,
            "fpr": 0.1611842105263158,
            "logloss": 4.767563006531935,
            "mae": 0.39199000818860086,
            "precision": 0.6651480637813212,
            "recall": 0.6033057851239669
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.6308435936893848,
            "auditor_fn_violation": 0.032907490015647994,
            "auditor_fp_violation": 0.019765974446858882,
            "ave_precision_score": 0.6027249631295747,
            "fpr": 0.15916575192096596,
            "logloss": 4.429612604594328,
            "mae": 0.3772902768800415,
            "precision": 0.6697038724373576,
            "recall": 0.625531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6337719298245614,
            "auc_prc": 0.6363390858122938,
            "auditor_fn_violation": 0.0383024322169059,
            "auditor_fp_violation": 0.004165641908509596,
            "ave_precision_score": 0.606977454488172,
            "fpr": 0.1524122807017544,
            "logloss": 5.105090220083461,
            "mae": 0.3773289805334693,
            "precision": 0.6752336448598131,
            "recall": 0.5971074380165289
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.6399292903416935,
            "auditor_fn_violation": 0.027816054370927434,
            "auditor_fp_violation": 0.01958426985869357,
            "ave_precision_score": 0.6089967168567814,
            "fpr": 0.145993413830955,
            "logloss": 4.724628709185339,
            "mae": 0.37010004453938183,
            "precision": 0.6763990267639902,
            "recall": 0.5914893617021276
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6337719298245614,
            "auc_prc": 0.6407798596820868,
            "auditor_fn_violation": 0.03728070175438597,
            "auditor_fp_violation": 0.0163551401869159,
            "ave_precision_score": 0.6114760883178914,
            "fpr": 0.18859649122807018,
            "logloss": 4.609228040009255,
            "mae": 0.36435034158178475,
            "precision": 0.6518218623481782,
            "recall": 0.6652892561983471
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.6454973925482296,
            "auditor_fn_violation": 0.025270336548567158,
            "auditor_fp_violation": 0.0297199011327912,
            "ave_precision_score": 0.6138116005786306,
            "fpr": 0.1942919868276619,
            "logloss": 4.367317709479665,
            "mae": 0.3600173227594732,
            "precision": 0.6452905811623246,
            "recall": 0.6851063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8473412491828692,
            "auditor_fn_violation": 0.010167464114832544,
            "auditor_fp_violation": 0.013718949827840636,
            "ave_precision_score": 0.8475548322607813,
            "fpr": 0.10416666666666667,
            "logloss": 1.2071008044161897,
            "mae": 0.2776085360055324,
            "precision": 0.7865168539325843,
            "recall": 0.7231404958677686
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8345294972358948,
            "auditor_fn_violation": 0.016315949272485232,
            "auditor_fp_violation": 0.01858116096786318,
            "ave_precision_score": 0.8347715500066799,
            "fpr": 0.10976948408342481,
            "logloss": 1.1117595236055435,
            "mae": 0.28989624827682975,
            "precision": 0.771689497716895,
            "recall": 0.7191489361702128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.828081369394117,
            "auditor_fn_violation": 0.006678628389154712,
            "auditor_fp_violation": 0.006115244302344648,
            "ave_precision_score": 0.8284065214685197,
            "fpr": 0.0712719298245614,
            "logloss": 1.0614475640623893,
            "mae": 0.3031449624975892,
            "precision": 0.8257372654155496,
            "recall": 0.6363636363636364
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.8265202752080276,
            "auditor_fn_violation": 0.01804657028750263,
            "auditor_fp_violation": 0.010663321310961265,
            "ave_precision_score": 0.8268417397978247,
            "fpr": 0.06915477497255763,
            "logloss": 0.9050098896608674,
            "mae": 0.3102997157161744,
            "precision": 0.8157894736842105,
            "recall": 0.5936170212765958
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.6392411194496574,
            "auditor_fn_violation": 0.037647709148905334,
            "auditor_fp_violation": 0.006158796524020332,
            "ave_precision_score": 0.6110619374560387,
            "fpr": 0.14473684210526316,
            "logloss": 4.973475345552705,
            "mae": 0.3753359850239436,
            "precision": 0.6864608076009501,
            "recall": 0.5971074380165289
        },
        "train": {
            "accuracy": 0.6410537870472008,
            "auc_prc": 0.6415787951480211,
            "auditor_fn_violation": 0.02947427423686854,
            "auditor_fp_violation": 0.016211534009871784,
            "ave_precision_score": 0.6113139229135909,
            "fpr": 0.141602634467618,
            "logloss": 4.652102342769829,
            "mae": 0.3696206987400334,
            "precision": 0.6783042394014963,
            "recall": 0.5787234042553191
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.810434602638846,
            "auditor_fn_violation": 0.021886780484268526,
            "auditor_fp_violation": 0.044602598786686355,
            "ave_precision_score": 0.8109952642381362,
            "fpr": 0.17324561403508773,
            "logloss": 1.3739856542275233,
            "mae": 0.28923561020974536,
            "precision": 0.7132486388384754,
            "recall": 0.8119834710743802
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8066421360039123,
            "auditor_fn_violation": 0.023637807412943457,
            "auditor_fp_violation": 0.047875425325637024,
            "ave_precision_score": 0.8069816075493711,
            "fpr": 0.1756311745334797,
            "logloss": 1.2653182819242743,
            "mae": 0.29888293168908375,
            "precision": 0.7053406998158379,
            "recall": 0.8148936170212766
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6546052631578947,
            "auc_prc": 0.6579647422676458,
            "auditor_fn_violation": 0.0348249238799478,
            "auditor_fp_violation": 0.0069709173635022145,
            "ave_precision_score": 0.6308839760080915,
            "fpr": 0.14364035087719298,
            "logloss": 4.613927008343323,
            "mae": 0.35169170077152945,
            "precision": 0.6960556844547564,
            "recall": 0.6198347107438017
        },
        "train": {
            "accuracy": 0.6597145993413831,
            "auc_prc": 0.6650059394660403,
            "auditor_fn_violation": 0.018777588341079485,
            "auditor_fp_violation": 0.016395727701984567,
            "ave_precision_score": 0.6360136571735688,
            "fpr": 0.14709110867178923,
            "logloss": 4.254080782146551,
            "mae": 0.3494626769307803,
            "precision": 0.6869158878504673,
            "recall": 0.625531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8415035095801314,
            "auditor_fn_violation": 0.010378153545019569,
            "auditor_fp_violation": 0.007111821610100017,
            "ave_precision_score": 0.8419011096898947,
            "fpr": 0.05921052631578947,
            "logloss": 1.263811018835914,
            "mae": 0.2893932282664931,
            "precision": 0.850415512465374,
            "recall": 0.6342975206611571
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.8332814473253151,
            "auditor_fn_violation": 0.02509283695728332,
            "auditor_fp_violation": 0.01270189744393916,
            "ave_precision_score": 0.8335954577801056,
            "fpr": 0.06256860592755215,
            "logloss": 1.1926729806398189,
            "mae": 0.3031823358593632,
            "precision": 0.8288288288288288,
            "recall": 0.5872340425531914
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6019736842105263,
            "auc_prc": 0.6159738899395244,
            "auditor_fn_violation": 0.04010348702334348,
            "auditor_fp_violation": 0.016726615018855556,
            "ave_precision_score": 0.5853463202515989,
            "fpr": 0.20285087719298245,
            "logloss": 5.194138693820177,
            "mae": 0.4041819737649895,
            "precision": 0.6232179226069247,
            "recall": 0.6322314049586777
        },
        "train": {
            "accuracy": 0.6300768386388584,
            "auc_prc": 0.6226021528257235,
            "auditor_fn_violation": 0.031494499848191145,
            "auditor_fp_violation": 0.023156134023312944,
            "ave_precision_score": 0.5898491692066439,
            "fpr": 0.19978046103183314,
            "logloss": 4.9245632315238925,
            "mae": 0.38337364636902466,
            "precision": 0.6338028169014085,
            "recall": 0.6702127659574468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.7171529606363187,
            "auditor_fn_violation": 0.011871103378280415,
            "auditor_fp_violation": 0.013188637481554355,
            "ave_precision_score": 0.7187199768306053,
            "fpr": 0.16666666666666666,
            "logloss": 1.5103779437687208,
            "mae": 0.322235956352214,
            "precision": 0.6947791164658634,
            "recall": 0.7148760330578512
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.7362197759064677,
            "auditor_fn_violation": 0.013303127262535913,
            "auditor_fp_violation": 0.029237014966982045,
            "ave_precision_score": 0.7378571206876131,
            "fpr": 0.17672886937431395,
            "logloss": 1.4365485717949782,
            "mae": 0.336628046310648,
            "precision": 0.676056338028169,
            "recall": 0.7148936170212766
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8325070369687382,
            "auditor_fn_violation": 0.00990919965202262,
            "auditor_fp_violation": 0.017292793900639453,
            "ave_precision_score": 0.8328445137969704,
            "fpr": 0.17214912280701755,
            "logloss": 1.0287225986735802,
            "mae": 0.30626802099973865,
            "precision": 0.7171171171171171,
            "recall": 0.8223140495867769
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8396704462349625,
            "auditor_fn_violation": 0.011439381554055633,
            "auditor_fp_violation": 0.023452337393061882,
            "ave_precision_score": 0.8398540555765488,
            "fpr": 0.19319429198682767,
            "logloss": 0.9916154240364653,
            "mae": 0.31632810576006026,
            "precision": 0.6928446771378709,
            "recall": 0.8446808510638298
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.6409420996292352,
            "auditor_fn_violation": 0.03762505437146587,
            "auditor_fp_violation": 0.008597720937858669,
            "ave_precision_score": 0.6179110089986486,
            "fpr": 0.13596491228070176,
            "logloss": 4.591483151584701,
            "mae": 0.3651013817694936,
            "precision": 0.6982968369829684,
            "recall": 0.5929752066115702
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.6425078235638941,
            "auditor_fn_violation": 0.02197725202606442,
            "auditor_fp_violation": 0.010499040450428257,
            "ave_precision_score": 0.6165131955643359,
            "fpr": 0.14050493962678376,
            "logloss": 4.256386844137308,
            "mae": 0.3656876851863461,
            "precision": 0.6823821339950372,
            "recall": 0.5851063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7479757662428002,
            "auditor_fn_violation": 0.012059138031027988,
            "auditor_fp_violation": 0.005029000655845222,
            "ave_precision_score": 0.745809828325018,
            "fpr": 0.0581140350877193,
            "logloss": 1.9689578089258997,
            "mae": 0.3281643784969792,
            "precision": 0.8301282051282052,
            "recall": 0.5351239669421488
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7195429130983567,
            "auditor_fn_violation": 0.01762851203961044,
            "auditor_fp_violation": 0.00403234839490132,
            "ave_precision_score": 0.7163853707763851,
            "fpr": 0.059275521405049394,
            "logloss": 2.0699162017985766,
            "mae": 0.34085684358759294,
            "precision": 0.8187919463087249,
            "recall": 0.5191489361702127
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8456166532404781,
            "auditor_fn_violation": 0.011089513556618824,
            "auditor_fp_violation": 0.015394429414658142,
            "ave_precision_score": 0.8458323662898358,
            "fpr": 0.1524122807017544,
            "logloss": 1.1951966561364655,
            "mae": 0.29041092000756685,
            "precision": 0.728515625,
            "recall": 0.7706611570247934
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8400802952172154,
            "auditor_fn_violation": 0.011093724455239744,
            "auditor_fp_violation": 0.013568105617658704,
            "ave_precision_score": 0.840247184647974,
            "fpr": 0.141602634467618,
            "logloss": 1.1070109355150675,
            "mae": 0.2931219483720111,
            "precision": 0.7414829659318637,
            "recall": 0.7872340425531915
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.741692367523927,
            "auditor_fn_violation": 0.025738092648977813,
            "auditor_fp_violation": 0.01683421462534842,
            "ave_precision_score": 0.6628211006750598,
            "fpr": 0.2138157894736842,
            "logloss": 6.327547087636968,
            "mae": 0.34123570855327756,
            "precision": 0.6578947368421053,
            "recall": 0.7747933884297521
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7446952511275522,
            "auditor_fn_violation": 0.017936800803419202,
            "auditor_fp_violation": 0.016480357236198544,
            "ave_precision_score": 0.6702135403799813,
            "fpr": 0.20417124039517015,
            "logloss": 5.570228724413087,
            "mae": 0.32779109975692733,
            "precision": 0.6642599277978339,
            "recall": 0.7829787234042553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.6496509252922614,
            "auditor_fn_violation": 0.03342032767870089,
            "auditor_fp_violation": 0.01608357927529104,
            "ave_precision_score": 0.6266991713052967,
            "fpr": 0.15570175438596492,
            "logloss": 4.376944669491308,
            "mae": 0.3515991122116172,
            "precision": 0.693304535637149,
            "recall": 0.6632231404958677
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.6542353012802484,
            "auditor_fn_violation": 0.02426606254525072,
            "auditor_fp_violation": 0.010802711132019591,
            "ave_precision_score": 0.6286906201653358,
            "fpr": 0.1690450054884742,
            "logloss": 4.008314604213483,
            "mae": 0.3485518306125813,
            "precision": 0.6723404255319149,
            "recall": 0.6723404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 21353,
        "test": {
            "accuracy": 0.5570175438596491,
            "auc_prc": 0.7574685861984121,
            "auditor_fn_violation": 0.013411628244164139,
            "auditor_fp_violation": 0.0007480734546646989,
            "ave_precision_score": 0.7585749050155852,
            "fpr": 0.017543859649122806,
            "logloss": 1.3274371034234367,
            "mae": 0.43589727606357737,
            "precision": 0.8571428571428571,
            "recall": 0.19834710743801653
        },
        "train": {
            "accuracy": 0.5510428100987925,
            "auc_prc": 0.7673369982014411,
            "auditor_fn_violation": 0.011728986150360849,
            "auditor_fp_violation": 0.0033976268882964823,
            "ave_precision_score": 0.7677375815548579,
            "fpr": 0.013172338090010977,
            "logloss": 1.2379603732293163,
            "mae": 0.431630817906434,
            "precision": 0.8588235294117647,
            "recall": 0.15531914893617021
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6566614735621314,
            "auditor_fn_violation": 0.03807814992025519,
            "auditor_fp_violation": 0.018220200032792268,
            "ave_precision_score": 0.6210237259136031,
            "fpr": 0.20285087719298245,
            "logloss": 5.2959677718464135,
            "mae": 0.37010013294986693,
            "precision": 0.6482889733840305,
            "recall": 0.7045454545454546
        },
        "train": {
            "accuracy": 0.6465422612513722,
            "auc_prc": 0.6683255980265087,
            "auditor_fn_violation": 0.024674778709391134,
            "auditor_fp_violation": 0.02060729158110372,
            "ave_precision_score": 0.6283258252785383,
            "fpr": 0.21185510428100987,
            "logloss": 5.045451949628989,
            "mae": 0.35101510309451306,
            "precision": 0.6385767790262172,
            "recall": 0.725531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 21353,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6432955539788636,
            "auditor_fn_violation": 0.03825032622879512,
            "auditor_fp_violation": 0.0035610345958353904,
            "ave_precision_score": 0.6169023834590848,
            "fpr": 0.1600877192982456,
            "logloss": 4.683318218746328,
            "mae": 0.366533079720293,
            "precision": 0.6755555555555556,
            "recall": 0.628099173553719
        },
        "train": {
            "accuracy": 0.6498353457738749,
            "auc_prc": 0.644838291464392,
            "auditor_fn_violation": 0.025387112595464417,
            "auditor_fp_violation": 0.020788996169269026,
            "ave_precision_score": 0.6155475360501388,
            "fpr": 0.16794731064763996,
            "logloss": 4.380075619407045,
            "mae": 0.35874673871025364,
            "precision": 0.6652078774617067,
            "recall": 0.6468085106382979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 21353,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.8667972640881434,
            "auditor_fn_violation": 0.014666702914310572,
            "auditor_fp_violation": 0.01981882275782915,
            "ave_precision_score": 0.8670141516440459,
            "fpr": 0.13596491228070176,
            "logloss": 0.6304214827952671,
            "mae": 0.2972331154803549,
            "precision": 0.7655954631379962,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8587166228403038,
            "auditor_fn_violation": 0.03391176401896443,
            "auditor_fp_violation": 0.01732416347439086,
            "ave_precision_score": 0.8590161966531922,
            "fpr": 0.1350164654226125,
            "logloss": 0.6975495630391126,
            "mae": 0.2998002167634204,
            "precision": 0.7620889748549323,
            "recall": 0.8382978723404255
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(0)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8330549735885615,
            "auditor_fn_violation": 0.018481738938541326,
            "auditor_fp_violation": 0.016661625327687033,
            "ave_precision_score": 0.8334749386126779,
            "fpr": 0.11732456140350878,
            "logloss": 0.7450456223505445,
            "mae": 0.2668224923692178,
            "precision": 0.7703862660944206,
            "recall": 0.7526205450733753
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8395995882427445,
            "auditor_fn_violation": 0.015126096831873192,
            "auditor_fp_violation": 0.023795191388406932,
            "ave_precision_score": 0.8398439993800642,
            "fpr": 0.14709110867178923,
            "logloss": 0.7635696063312807,
            "mae": 0.2729100285478741,
            "precision": 0.7367387033398821,
            "recall": 0.7861635220125787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(1)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5427631578947368,
            "auc_prc": 0.5862903016381272,
            "auditor_fn_violation": 0.07204430100408253,
            "auditor_fp_violation": 0.06228070175438597,
            "ave_precision_score": 0.5882378903418757,
            "fpr": 0.13486842105263158,
            "logloss": 0.6899414848648278,
            "mae": 0.49217296586159553,
            "precision": 0.5980392156862745,
            "recall": 0.3836477987421384
        },
        "train": {
            "accuracy": 0.5631174533479693,
            "auc_prc": 0.5978853670129893,
            "auditor_fn_violation": 0.07182652279270137,
            "auditor_fp_violation": 0.07312569870552944,
            "ave_precision_score": 0.5988438312349146,
            "fpr": 0.14050493962678376,
            "logloss": 0.6856324690402443,
            "mae": 0.4901245386226248,
            "precision": 0.6179104477611941,
            "recall": 0.4339622641509434
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(2)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.8129475500596511,
            "auditor_fn_violation": 0.016982971054470563,
            "auditor_fp_violation": 0.01986287557975399,
            "ave_precision_score": 0.8133661853030365,
            "fpr": 0.18640350877192982,
            "logloss": 0.6510980824249444,
            "mae": 0.40463031498935986,
            "precision": 0.6816479400749064,
            "recall": 0.7631027253668763
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.8186411261382661,
            "auditor_fn_violation": 0.015754337275369526,
            "auditor_fp_violation": 0.025712363483688865,
            "ave_precision_score": 0.8188936682553092,
            "fpr": 0.18660812294182216,
            "logloss": 0.5966257973613188,
            "mae": 0.40951392304921025,
            "precision": 0.6880733944954128,
            "recall": 0.7861635220125787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(3)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.8129734642382603,
            "auditor_fn_violation": 0.0017746147338997395,
            "auditor_fp_violation": 0.014743395845936699,
            "ave_precision_score": 0.7895341422497514,
            "fpr": 0.27521929824561403,
            "logloss": 0.5751861308037476,
            "mae": 0.4028307420708108,
            "precision": 0.6335766423357664,
            "recall": 0.909853249475891
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.7983221041866392,
            "auditor_fn_violation": 0.010935525961518547,
            "auditor_fp_violation": 0.01750494468528533,
            "ave_precision_score": 0.7776170110641282,
            "fpr": 0.2711306256860593,
            "logloss": 0.5806769458959294,
            "mae": 0.40450442563558386,
            "precision": 0.6356932153392331,
            "recall": 0.9035639412997903
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(4)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5986842105263158,
            "auc_prc": 0.6499825689613672,
            "auditor_fn_violation": 0.011452241715399616,
            "auditor_fp_violation": 0.008645896350070579,
            "ave_precision_score": 0.6493946149253143,
            "fpr": 0.10416666666666667,
            "logloss": 2.622739685643766,
            "mae": 0.47351176517480553,
            "precision": 0.6843853820598007,
            "recall": 0.43186582809224316
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.6535779360813458,
            "auditor_fn_violation": 0.011727154945264842,
            "auditor_fp_violation": 0.007471406819871822,
            "ave_precision_score": 0.6522382899306929,
            "fpr": 0.10537870472008781,
            "logloss": 2.6659034566323596,
            "mae": 0.47411376191752563,
            "precision": 0.6723549488054608,
            "recall": 0.4129979035639413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(5)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8524961695610176,
            "auditor_fn_violation": 0.015957740262606206,
            "auditor_fp_violation": 0.016863278886872356,
            "ave_precision_score": 0.8183908969206406,
            "fpr": 0.12609649122807018,
            "logloss": 0.5116440052833336,
            "mae": 0.3183469916407934,
            "precision": 0.7638603696098563,
            "recall": 0.779874213836478
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8586309844458638,
            "auditor_fn_violation": 0.012152885648733049,
            "auditor_fp_violation": 0.009530216959132368,
            "ave_precision_score": 0.8281477895449767,
            "fpr": 0.10976948408342481,
            "logloss": 0.4979575825282007,
            "mae": 0.3177888370257047,
            "precision": 0.7863247863247863,
            "recall": 0.7714884696016772
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(6)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.7138785786944553,
            "auditor_fn_violation": 0.0033515392254220558,
            "auditor_fp_violation": 0.010420447670901391,
            "ave_precision_score": 0.7155776389385398,
            "fpr": 0.14035087719298245,
            "logloss": 0.6419819519067521,
            "mae": 0.4303576467554146,
            "precision": 0.6945107398568019,
            "recall": 0.610062893081761
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.7167952830816676,
            "auditor_fn_violation": 0.0166104011763975,
            "auditor_fp_violation": 0.006146079408357665,
            "ave_precision_score": 0.7176296813303112,
            "fpr": 0.14928649835345773,
            "logloss": 0.6409462009056428,
            "mae": 0.43525448503150604,
            "precision": 0.6792452830188679,
            "recall": 0.6037735849056604
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(7)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8016009816229852,
            "auditor_fn_violation": 0.007323733862959292,
            "auditor_fp_violation": 0.011400988102440009,
            "ave_precision_score": 0.8030403815583508,
            "fpr": 0.08991228070175439,
            "logloss": 0.5576714038861325,
            "mae": 0.37906116099982956,
            "precision": 0.7918781725888325,
            "recall": 0.6540880503144654
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8433976187286185,
            "auditor_fn_violation": 0.007960013531332633,
            "auditor_fp_violation": 0.00434272359841568,
            "ave_precision_score": 0.8437729551398273,
            "fpr": 0.07793633369923161,
            "logloss": 0.5242435282502632,
            "mae": 0.36536048233398755,
            "precision": 0.8188775510204082,
            "recall": 0.6729559748427673
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(8)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5021929824561403,
            "auc_prc": 0.5925502901390555,
            "auditor_fn_violation": 0.01020633344367207,
            "auditor_fp_violation": 0.0022711232103246625,
            "ave_precision_score": 0.5522285227802591,
            "fpr": 0.05043859649122807,
            "logloss": 1.0028134021533968,
            "mae": 0.49647725366986906,
            "precision": 0.6,
            "recall": 0.14465408805031446
        },
        "train": {
            "accuracy": 0.5005488474204172,
            "auc_prc": 0.6087370889545397,
            "auditor_fn_violation": 0.010489084034638385,
            "auditor_fp_violation": 0.006879562136104046,
            "ave_precision_score": 0.5657317872831717,
            "fpr": 0.06366630076838639,
            "logloss": 0.9169298497406789,
            "mae": 0.4906541630250346,
            "precision": 0.5797101449275363,
            "recall": 0.16771488469601678
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(9)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.8502193592693704,
            "auditor_fn_violation": 0.0025010114384493754,
            "auditor_fp_violation": 0.008784533172010483,
            "ave_precision_score": 0.8232426091627143,
            "fpr": 0.20942982456140352,
            "logloss": 2.0727195328927372,
            "mae": 0.3236893868546631,
            "precision": 0.6789915966386555,
            "recall": 0.8469601677148847
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8427485054890983,
            "auditor_fn_violation": 0.006503324151357621,
            "auditor_fp_violation": 0.0014517899507807811,
            "ave_precision_score": 0.8177816536137914,
            "fpr": 0.18551042810098792,
            "logloss": 1.907868421607538,
            "mae": 0.31267797917551354,
            "precision": 0.699288256227758,
            "recall": 0.8238993710691824
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(10)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5537280701754386,
            "auc_prc": 0.771495771206025,
            "auditor_fn_violation": 0.002379179079774909,
            "auditor_fp_violation": 0.005217785843920145,
            "ave_precision_score": 0.5763340468198846,
            "fpr": 0.43640350877192985,
            "logloss": 0.6877105392860358,
            "mae": 0.4941875458155808,
            "precision": 0.5404157043879908,
            "recall": 0.9811320754716981
        },
        "train": {
            "accuracy": 0.5301866081229418,
            "auc_prc": 0.7758325313351734,
            "auditor_fn_violation": 0.00468073649110453,
            "auditor_fp_violation": 0.002321852220935111,
            "ave_precision_score": 0.5790952464048138,
            "fpr": 0.4500548847420417,
            "logloss": 0.6910930717480097,
            "mae": 0.49585584548642425,
            "precision": 0.5281933256616801,
            "recall": 0.9622641509433962
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(11)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.7989053219225719,
            "auditor_fn_violation": 0.003779101842656958,
            "auditor_fp_violation": 0.012615950796531572,
            "ave_precision_score": 0.6212188835455184,
            "fpr": 0.2905701754385965,
            "logloss": 0.6081632750375262,
            "mae": 0.4184890458066213,
            "precision": 0.6278089887640449,
            "recall": 0.9371069182389937
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.8010112100874225,
            "auditor_fn_violation": 0.0043355494342384145,
            "auditor_fp_violation": 0.006955439659664037,
            "ave_precision_score": 0.6244572387297804,
            "fpr": 0.287596048298573,
            "logloss": 0.6042202079367954,
            "mae": 0.41663656510519015,
            "precision": 0.6309859154929578,
            "recall": 0.939203354297694
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(12)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7312383339961637,
            "auditor_fn_violation": 0.011895895398874546,
            "auditor_fp_violation": 0.0179093567251462,
            "ave_precision_score": 0.6438086470376023,
            "fpr": 0.22478070175438597,
            "logloss": 5.368633877747228,
            "mae": 0.3564428410803278,
            "precision": 0.6644844517184942,
            "recall": 0.8511530398322851
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7688473832776684,
            "auditor_fn_violation": 0.009515656534275926,
            "auditor_fp_violation": 0.007542225841861125,
            "ave_precision_score": 0.6893725901870762,
            "fpr": 0.18880351262349068,
            "logloss": 4.540941870222893,
            "mae": 0.3357897660604031,
            "precision": 0.7013888888888888,
            "recall": 0.8469601677148847
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(13)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5230263157894737,
            "auc_prc": 0.7615131578947368,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5230263157894737,
            "fpr": 0.4769736842105263,
            "logloss": 0.6926933645634265,
            "mae": 0.49813924729824066,
            "precision": 0.5230263157894737,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5236004390779363,
            "auc_prc": 0.7618002195389681,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5236004390779363,
            "fpr": 0.47639956092206365,
            "logloss": 0.6926003721766595,
            "mae": 0.49809285249197127,
            "precision": 0.5236004390779363,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(14)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8373416773097582,
            "auditor_fn_violation": 0.013383169664202436,
            "auditor_fp_violation": 0.017443032869530157,
            "ave_precision_score": 0.8052175892855178,
            "fpr": 0.2412280701754386,
            "logloss": 0.5947715019333198,
            "mae": 0.3824756395463881,
            "precision": 0.6620583717357911,
            "recall": 0.9035639412997903
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.8374506074573631,
            "auditor_fn_violation": 0.009754986227036433,
            "auditor_fp_violation": 0.019156545448107368,
            "ave_precision_score": 0.8013682117909303,
            "fpr": 0.24588364434687157,
            "logloss": 0.5894596526392435,
            "mae": 0.3818709151543325,
            "precision": 0.6585365853658537,
            "recall": 0.9056603773584906
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(15)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8161897555203098,
            "auditor_fn_violation": 0.047939883776527276,
            "auditor_fp_violation": 0.040660919540229885,
            "ave_precision_score": 0.8171613309755108,
            "fpr": 0.11842105263157894,
            "logloss": 0.5285727548622494,
            "mae": 0.34346485378122643,
            "precision": 0.7562076749435666,
            "recall": 0.7023060796645703
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7845259140075742,
            "auditor_fn_violation": 0.03701785997832226,
            "auditor_fp_violation": 0.047977358146969705,
            "ave_precision_score": 0.7995594987275194,
            "fpr": 0.12184412733260154,
            "logloss": 0.539033822592324,
            "mae": 0.3504467307130932,
            "precision": 0.7459954233409611,
            "recall": 0.6834381551362684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(16)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.636353623814268,
            "auditor_fn_violation": 0.03380733017028946,
            "auditor_fp_violation": 0.035980036297640655,
            "ave_precision_score": 0.6372068206713006,
            "fpr": 0.32456140350877194,
            "logloss": 0.6826943020111619,
            "mae": 0.47679917006408895,
            "precision": 0.5653450807635829,
            "recall": 0.8071278825995807
        },
        "train": {
            "accuracy": 0.5587266739846323,
            "auc_prc": 0.6040269729310217,
            "auditor_fn_violation": 0.03438753460500245,
            "auditor_fp_violation": 0.03600894343077695,
            "ave_precision_score": 0.6054544622102046,
            "fpr": 0.3216245883644347,
            "logloss": 0.6777742287219576,
            "mae": 0.47518089207954517,
            "precision": 0.556732223903177,
            "recall": 0.7714884696016772
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(17)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5723684210526315,
            "auc_prc": 0.498711035013419,
            "auditor_fn_violation": 0.008103001213726138,
            "auditor_fp_violation": 0.014503932244404124,
            "ave_precision_score": 0.6299069407623654,
            "fpr": 0.32456140350877194,
            "logloss": 0.6829578935625419,
            "mae": 0.49393431561296447,
            "precision": 0.5640648011782032,
            "recall": 0.8029350104821803
        },
        "train": {
            "accuracy": 0.5499451152579583,
            "auc_prc": 0.4472747338361629,
            "auditor_fn_violation": 0.008988670960793654,
            "auditor_fp_violation": 0.013516316196816184,
            "ave_precision_score": 0.60718581107565,
            "fpr": 0.3545554335894621,
            "logloss": 0.6856532863136738,
            "mae": 0.49518818732935826,
            "precision": 0.5469845722300141,
            "recall": 0.8176100628930818
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(18)",
        "seed": 6832,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.7715629066580715,
            "auditor_fn_violation": 0.008654694913384088,
            "auditor_fp_violation": 0.020992135511191775,
            "ave_precision_score": 0.7650773058419734,
            "fpr": 0.14364035087719298,
            "logloss": 0.635372210039839,
            "mae": 0.440136203991674,
            "precision": 0.7405940594059406,
            "recall": 0.7840670859538784
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.7483085259264063,
            "auditor_fn_violation": 0.007994532237019243,
            "auditor_fp_violation": 0.014528016510949133,
            "ave_precision_score": 0.7655270458941511,
            "fpr": 0.12623490669593854,
            "logloss": 0.6387997973705141,
            "mae": 0.4408621752407692,
            "precision": 0.7584033613445378,
            "recall": 0.7568134171907757
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(19)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5296052631578947,
            "auc_prc": 0.7627749548535705,
            "auditor_fn_violation": 0.0005976681746294458,
            "auditor_fp_violation": 0.003632284734825571,
            "ave_precision_score": 0.526544906953544,
            "fpr": 0.4692982456140351,
            "logloss": 0.6901287379346729,
            "mae": 0.4954577738300717,
            "precision": 0.5265486725663717,
            "recall": 0.9979035639412998
        },
        "train": {
            "accuracy": 0.5290889132821076,
            "auc_prc": 0.7632450331125828,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0017730048005179834,
            "ave_precision_score": 0.5264900662251656,
            "fpr": 0.47091108671789245,
            "logloss": 0.6880613438456948,
            "mae": 0.4959689317236355,
            "precision": 0.5264900662251656,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(20)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.7864819270415838,
            "auditor_fn_violation": 0.005031906285630212,
            "auditor_fp_violation": 0.011947973381730191,
            "ave_precision_score": 0.7731893493592834,
            "fpr": 0.08223684210526316,
            "logloss": 0.5563971195481482,
            "mae": 0.3641804085325515,
            "precision": 0.8138957816377171,
            "recall": 0.6876310272536688
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8281274379548238,
            "auditor_fn_violation": 0.009607706416106894,
            "auditor_fp_violation": 0.009590918977980346,
            "ave_precision_score": 0.80689285696924,
            "fpr": 0.07025246981339188,
            "logloss": 0.5165670230439652,
            "mae": 0.352408577378279,
            "precision": 0.8354755784061697,
            "recall": 0.6813417190775681
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(21)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.7771469508730005,
            "auditor_fn_violation": 0.02110458273566516,
            "auditor_fp_violation": 0.016447368421052634,
            "ave_precision_score": 0.645044635936291,
            "fpr": 0.15899122807017543,
            "logloss": 0.6277845235842251,
            "mae": 0.42934782254068476,
            "precision": 0.697286012526096,
            "recall": 0.70020964360587
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7752703385355306,
            "auditor_fn_violation": 0.011225483089286085,
            "auditor_fp_violation": 0.025186279320339736,
            "ave_precision_score": 0.6395866936324859,
            "fpr": 0.17233809001097694,
            "logloss": 0.6305800159525148,
            "mae": 0.43123245383198777,
            "precision": 0.685370741482966,
            "recall": 0.7169811320754716
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(22)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.7820127853703707,
            "auditor_fn_violation": 0.008116793556217588,
            "auditor_fp_violation": 0.025068058076225048,
            "ave_precision_score": 0.7829424917313277,
            "fpr": 0.29385964912280704,
            "logloss": 0.69574121048295,
            "mae": 0.3441860760611139,
            "precision": 0.6235955056179775,
            "recall": 0.9308176100628931
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.7160434686059174,
            "auditor_fn_violation": 0.008551434022096576,
            "auditor_fp_violation": 0.028355430554361183,
            "ave_precision_score": 0.7173116459667923,
            "fpr": 0.27771679473106475,
            "logloss": 0.7645969436246929,
            "mae": 0.349376342115202,
            "precision": 0.6396011396011396,
            "recall": 0.9412997903563941
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(23)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.7670568879665006,
            "auditor_fn_violation": 0.004682500275846849,
            "auditor_fp_violation": 0.009502923976608197,
            "ave_precision_score": 0.7674398418366772,
            "fpr": 0.15899122807017543,
            "logloss": 0.6020413826441047,
            "mae": 0.40576377975051864,
            "precision": 0.6927966101694916,
            "recall": 0.6855345911949685
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.7726816206435494,
            "auditor_fn_violation": 0.007170685794632112,
            "auditor_fp_violation": 0.008022783491074277,
            "ave_precision_score": 0.7731112167555343,
            "fpr": 0.14489571899012074,
            "logloss": 0.597977060348428,
            "mae": 0.4063121487686061,
            "precision": 0.7053571428571429,
            "recall": 0.6624737945492662
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(24)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0.6298191115708942,
            "auditor_fn_violation": 0.013521093089116937,
            "auditor_fp_violation": 0.009419741883444244,
            "ave_precision_score": 0.6312945330211487,
            "fpr": 0.01864035087719298,
            "logloss": 1.0469588239070697,
            "mae": 0.48396237936532616,
            "precision": 0.6909090909090909,
            "recall": 0.07966457023060797
        },
        "train": {
            "accuracy": 0.49286498353457736,
            "auc_prc": 0.6343099656593153,
            "auditor_fn_violation": 0.015277979136894283,
            "auditor_fp_violation": 0.008994015792641905,
            "ave_precision_score": 0.635321034957008,
            "fpr": 0.01646542261251372,
            "logloss": 1.0528454326031493,
            "mae": 0.48481452049854346,
            "precision": 0.6666666666666666,
            "recall": 0.06289308176100629
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(25)",
        "seed": 6832,
        "test": {
            "accuracy": 0.4769736842105263,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.064688969706573,
            "mae": 0.5230263157894737,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.47639956092206365,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.084518485589896,
            "mae": 0.5236004390779363,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(26)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8195412284889771,
            "auditor_fn_violation": 0.01199933796756041,
            "auditor_fp_violation": 0.016805303488606572,
            "ave_precision_score": 0.8148704762544092,
            "fpr": 0.09649122807017543,
            "logloss": 0.5339680655356703,
            "mae": 0.34878822048947195,
            "precision": 0.7977011494252874,
            "recall": 0.7274633123689728
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8144892129694434,
            "auditor_fn_violation": 0.007021104736656801,
            "auditor_fp_violation": 0.010936480395777168,
            "ave_precision_score": 0.8138453790628944,
            "fpr": 0.09330406147091108,
            "logloss": 0.5224155173473546,
            "mae": 0.3496972951691679,
            "precision": 0.7995283018867925,
            "recall": 0.710691823899371
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(27)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7991909187675527,
            "auditor_fn_violation": 0.02052990179852146,
            "auditor_fp_violation": 0.01732456140350877,
            "ave_precision_score": 0.7969474250760504,
            "fpr": 0.12719298245614036,
            "logloss": 1.01156221287768,
            "mae": 0.2767275141277184,
            "precision": 0.7531914893617021,
            "recall": 0.7421383647798742
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7955862228610892,
            "auditor_fn_violation": 0.010859584809008008,
            "auditor_fp_violation": 0.02072973943658411,
            "ave_precision_score": 0.7948026075681485,
            "fpr": 0.14928649835345773,
            "logloss": 1.0200739117739774,
            "mae": 0.28536854834659703,
            "precision": 0.7285429141716567,
            "recall": 0.7651991614255765
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(28)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8324864325810006,
            "auditor_fn_violation": 0.010806300342050096,
            "auditor_fp_violation": 0.0007032667876588021,
            "ave_precision_score": 0.8258619522676904,
            "fpr": 0.08881578947368421,
            "logloss": 0.5425943091235989,
            "mae": 0.3326631087050038,
            "precision": 0.7980049875311721,
            "recall": 0.6708595387840671
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8418319317107174,
            "auditor_fn_violation": 0.00890582606714579,
            "auditor_fp_violation": 0.0032930845225027433,
            "ave_precision_score": 0.8334161371309865,
            "fpr": 0.07683863885839737,
            "logloss": 0.5064093492728468,
            "mae": 0.32492574825923803,
            "precision": 0.8177083333333334,
            "recall": 0.6582809224318659
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(29)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6206140350877193,
            "auc_prc": 0.6711824528508868,
            "auditor_fn_violation": 0.0523327448600537,
            "auditor_fp_violation": 0.03153357531760436,
            "ave_precision_score": 0.6705856332063945,
            "fpr": 0.13706140350877194,
            "logloss": 0.6550082602317308,
            "mae": 0.46987901784871755,
            "precision": 0.6719160104986877,
            "recall": 0.5366876310272537
        },
        "train": {
            "accuracy": 0.6289791437980241,
            "auc_prc": 0.6987030765417367,
            "auditor_fn_violation": 0.05610440297597269,
            "auditor_fp_violation": 0.049467086859530475,
            "ave_precision_score": 0.6984866448577073,
            "fpr": 0.13830954994511527,
            "logloss": 0.6432064484953715,
            "mae": 0.46322992162699234,
            "precision": 0.6777493606138107,
            "recall": 0.5555555555555556
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(30)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5230263157894737,
            "auc_prc": 0.762087912087912,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5241758241758242,
            "fpr": 0.4769736842105263,
            "logloss": 16.42409916789696,
            "mae": 0.47697286516950843,
            "precision": 0.5230263157894737,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5236004390779363,
            "auc_prc": 0.7626651982378855,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.525330396475771,
            "fpr": 0.47639956092206365,
            "logloss": 16.347578121029485,
            "mae": 0.4758629993209462,
            "precision": 0.5236004390779363,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(31)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6008771929824561,
            "auc_prc": 0.5985602523969158,
            "auditor_fn_violation": 0.03390847401522676,
            "auditor_fp_violation": 0.018267291792700143,
            "ave_precision_score": 0.6069049678044177,
            "fpr": 0.1162280701754386,
            "logloss": 0.8603167960562226,
            "mae": 0.45975011545376393,
            "precision": 0.6738461538461539,
            "recall": 0.4591194968553459
        },
        "train": {
            "accuracy": 0.6004390779363337,
            "auc_prc": 0.5935636920863221,
            "auditor_fn_violation": 0.023481924855078967,
            "auditor_fp_violation": 0.02332727999312044,
            "ave_precision_score": 0.6019825002472462,
            "fpr": 0.12403951701427003,
            "logloss": 0.8512050582936906,
            "mae": 0.4579001527666048,
            "precision": 0.6666666666666666,
            "recall": 0.47379454926624737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(32)",
        "seed": 6832,
        "test": {
            "accuracy": 0.4769736842105263,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.064688969706573,
            "mae": 0.5230263157894737,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.47639956092206365,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.084518485589896,
            "mae": 0.5236004390779363,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(33)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.5680422905106199,
            "auditor_fn_violation": 0.0073444223766964654,
            "auditor_fp_violation": 0.018201754385964913,
            "ave_precision_score": 0.5714515508151208,
            "fpr": 0.13267543859649122,
            "logloss": 0.9650989935112565,
            "mae": 0.4840013869553788,
            "precision": 0.578397212543554,
            "recall": 0.3480083857442348
        },
        "train": {
            "accuracy": 0.5159165751920965,
            "auc_prc": 0.5552876432479024,
            "auditor_fn_violation": 0.0067725700557132,
            "auditor_fp_violation": 0.022388927951762137,
            "ave_precision_score": 0.5582008243840771,
            "fpr": 0.12843029637760703,
            "logloss": 0.9894329084903991,
            "mae": 0.48525409564824373,
            "precision": 0.5666666666666667,
            "recall": 0.32075471698113206
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(34)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8313981076476553,
            "auditor_fn_violation": 0.011220070616793556,
            "auditor_fp_violation": 0.011257309941520474,
            "ave_precision_score": 0.732285270930283,
            "fpr": 0.09100877192982457,
            "logloss": 0.5517569354610923,
            "mae": 0.3675624535321013,
            "precision": 0.7995169082125604,
            "recall": 0.6939203354297694
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8401156150611404,
            "auditor_fn_violation": 0.005921108648776771,
            "auditor_fp_violation": 0.002086631897899205,
            "ave_precision_score": 0.7458175595688803,
            "fpr": 0.07135016465422613,
            "logloss": 0.5408130695339318,
            "mae": 0.36344755887494523,
            "precision": 0.8293963254593176,
            "recall": 0.6624737945492662
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(35)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7455453604748848,
            "auditor_fn_violation": 0.023840063996469166,
            "auditor_fp_violation": 0.01485934664246824,
            "ave_precision_score": 0.749936665672283,
            "fpr": 0.08552631578947369,
            "logloss": 0.6076676407282512,
            "mae": 0.3767414155207868,
            "precision": 0.7947368421052632,
            "recall": 0.6331236897274634
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.7516942759588321,
            "auditor_fn_violation": 0.015906219580390613,
            "auditor_fp_violation": 0.004196027052866399,
            "ave_precision_score": 0.7525507213869087,
            "fpr": 0.07354555433589462,
            "logloss": 0.6173458699849153,
            "mae": 0.37477953880452824,
            "precision": 0.8203753351206434,
            "recall": 0.6415094339622641
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(36)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8012408870014214,
            "auditor_fn_violation": 0.010339659421089406,
            "auditor_fp_violation": 0.002850877192982463,
            "ave_precision_score": 0.6955268920071931,
            "fpr": 0.09539473684210527,
            "logloss": 0.6307639443870172,
            "mae": 0.4573746321461441,
            "precision": 0.7808564231738035,
            "recall": 0.649895178197065
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7876200990800474,
            "auditor_fn_violation": 0.009722768768395597,
            "auditor_fp_violation": 0.008604511171700722,
            "ave_precision_score": 0.678696300386668,
            "fpr": 0.09989023051591657,
            "logloss": 0.6316238614161814,
            "mae": 0.45781463243958714,
            "precision": 0.762402088772846,
            "recall": 0.6121593291404612
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(37)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.819939891944644,
            "auditor_fn_violation": 0.013534885431608372,
            "auditor_fp_violation": 0.03137729380923574,
            "ave_precision_score": 0.7381496695038225,
            "fpr": 0.2883771929824561,
            "logloss": 0.5815982057075539,
            "mae": 0.36440150201189936,
            "precision": 0.6143695014662757,
            "recall": 0.8784067085953878
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.8413659083177231,
            "auditor_fn_violation": 0.005281361970051573,
            "auditor_fp_violation": 0.02952141516639942,
            "ave_precision_score": 0.7608585749351243,
            "fpr": 0.26344676180021953,
            "logloss": 0.5313250041810577,
            "mae": 0.3481805924690123,
            "precision": 0.6460176991150443,
            "recall": 0.9182389937106918
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(38)",
        "seed": 6832,
        "test": {
            "accuracy": 0.4769736842105263,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 2.123423142000118,
            "mae": 0.5107700471590968,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.47639956092206365,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 2.2016332697136414,
            "mae": 0.5115228453207326,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(39)",
        "seed": 6832,
        "test": {
            "accuracy": 0.4057017543859649,
            "auc_prc": 0.5801584273931675,
            "auditor_fn_violation": 0.01455781749972415,
            "auditor_fp_violation": 0.013077233313167978,
            "ave_precision_score": 0.581562932278449,
            "fpr": 0.33114035087719296,
            "logloss": 9.285660329058912,
            "mae": 0.5624797048692509,
            "precision": 0.4397031539888683,
            "recall": 0.4968553459119497
        },
        "train": {
            "accuracy": 0.41822173435784854,
            "auc_prc": 0.5701091259661321,
            "auditor_fn_violation": 0.010258959330060975,
            "auditor_fp_violation": 0.02297318488317391,
            "ave_precision_score": 0.5715935217300108,
            "fpr": 0.3062568605927552,
            "logloss": 9.843876872154057,
            "mae": 0.5648053677276704,
            "precision": 0.44752475247524753,
            "recall": 0.47379454926624737
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(40)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7331681379867256,
            "auditor_fn_violation": 0.0075857883702968145,
            "auditor_fp_violation": 0.01599364791288567,
            "ave_precision_score": 0.6314183424486232,
            "fpr": 0.1699561403508772,
            "logloss": 0.6096121385464578,
            "mae": 0.42125762616725343,
            "precision": 0.7024952015355086,
            "recall": 0.7672955974842768
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7223783971954031,
            "auditor_fn_violation": 0.010125487001406072,
            "auditor_fp_violation": 0.015451193047595445,
            "ave_precision_score": 0.6209577547407925,
            "fpr": 0.16794731064763996,
            "logloss": 0.6242923497343676,
            "mae": 0.42785217419152205,
            "precision": 0.694,
            "recall": 0.7274633123689728
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(41)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.6368526642873467,
            "auditor_fn_violation": 0.027308838133068523,
            "auditor_fp_violation": 0.016593567251461997,
            "ave_precision_score": 0.6386724921321024,
            "fpr": 0.16885964912280702,
            "logloss": 0.8359949915551782,
            "mae": 0.3882687654704888,
            "precision": 0.6798336798336798,
            "recall": 0.6855345911949685
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.6438207706529941,
            "auditor_fn_violation": 0.01949156247770667,
            "auditor_fp_violation": 0.01823589816224638,
            "ave_precision_score": 0.6458569716616638,
            "fpr": 0.18441273326015367,
            "logloss": 0.8301122269341648,
            "mae": 0.38055870903462574,
            "precision": 0.6737864077669903,
            "recall": 0.7274633123689728
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(42)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7867073234273665,
            "auditor_fn_violation": 0.020132222590018022,
            "auditor_fp_violation": 0.027034180278281916,
            "ave_precision_score": 0.6547680127992938,
            "fpr": 0.16447368421052633,
            "logloss": 0.6243679221771267,
            "mae": 0.41484670265855494,
            "precision": 0.7,
            "recall": 0.7337526205450734
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.7802739356877948,
            "auditor_fn_violation": 0.01413656060219033,
            "auditor_fp_violation": 0.024037999463798834,
            "ave_precision_score": 0.6396403300836363,
            "fpr": 0.18990120746432493,
            "logloss": 0.632919542430695,
            "mae": 0.42088269627971,
            "precision": 0.6760299625468165,
            "recall": 0.7568134171907757
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(43)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5230263157894737,
            "auc_prc": 0.7160129096325721,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5301390268123138,
            "fpr": 0.4769736842105263,
            "logloss": 0.6885009824437996,
            "mae": 0.49592541419623193,
            "precision": 0.5230263157894737,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5236004390779363,
            "auc_prc": 0.765795184410432,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.531590368820864,
            "fpr": 0.47639956092206365,
            "logloss": 0.6876001742810403,
            "mae": 0.49589564025598354,
            "precision": 0.5236004390779363,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(44)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.6199583994959073,
            "auditor_fn_violation": 0.0019769024237743208,
            "auditor_fp_violation": 0.008628251663641883,
            "ave_precision_score": 0.5890227082752433,
            "fpr": 0.39035087719298245,
            "logloss": 0.8998692092231169,
            "mae": 0.4508024599207075,
            "precision": 0.5538847117794486,
            "recall": 0.9266247379454927
        },
        "train": {
            "accuracy": 0.6048298572996706,
            "auc_prc": 0.6692878648338961,
            "auditor_fn_violation": 0.004314838210826447,
            "auditor_fp_violation": 0.023906478422961563,
            "ave_precision_score": 0.6337503231331306,
            "fpr": 0.37102085620197583,
            "logloss": 0.847108074375111,
            "mae": 0.42656963901710826,
            "precision": 0.5737704918032787,
            "recall": 0.9538784067085954
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(45)",
        "seed": 6832,
        "test": {
            "accuracy": 0.46600877192982454,
            "auc_prc": 0.4764994353455358,
            "auditor_fn_violation": 0.026403140976130068,
            "auditor_fp_violation": 0.011501814882032689,
            "ave_precision_score": 0.4781314341076473,
            "fpr": 0.26535087719298245,
            "logloss": 0.907060519903608,
            "mae": 0.5209138916372403,
            "precision": 0.48945147679324896,
            "recall": 0.4863731656184486
        },
        "train": {
            "accuracy": 0.4665203073545554,
            "auc_prc": 0.4717040950992556,
            "auditor_fn_violation": 0.0042181858349039396,
            "auditor_fp_violation": 0.023608026830292336,
            "ave_precision_score": 0.4731984249148273,
            "fpr": 0.2601536772777168,
            "logloss": 0.9277324547882931,
            "mae": 0.5239329989140794,
            "precision": 0.49032258064516127,
            "recall": 0.4779874213836478
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(46)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7719298245614035,
            "auc_prc": 0.8241403972754748,
            "auditor_fn_violation": 0.005402000809150762,
            "auditor_fp_violation": 0.0092911877394636,
            "ave_precision_score": 0.8061401128625988,
            "fpr": 0.08442982456140351,
            "logloss": 0.5071550025853653,
            "mae": 0.35137061524744095,
            "precision": 0.817966903073286,
            "recall": 0.7253668763102725
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8212194084436057,
            "auditor_fn_violation": 0.010470674058272183,
            "auditor_fp_violation": 0.005311426649197978,
            "ave_precision_score": 0.802065671795829,
            "fpr": 0.07683863885839737,
            "logloss": 0.5198624203781366,
            "mae": 0.3593681053913778,
            "precision": 0.8214285714285714,
            "recall": 0.6750524109014675
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(47)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8353766778943144,
            "auditor_fn_violation": 0.01894378241200486,
            "auditor_fp_violation": 0.008650937689050215,
            "ave_precision_score": 0.8360944050317699,
            "fpr": 0.08442982456140351,
            "logloss": 0.6181138938386537,
            "mae": 0.297315319760257,
            "precision": 0.8094059405940595,
            "recall": 0.6855345911949685
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8460729965733946,
            "auditor_fn_violation": 0.009547873992916761,
            "auditor_fp_violation": 0.006985790669088004,
            "ave_precision_score": 0.846279688013543,
            "fpr": 0.09110867178924259,
            "logloss": 0.5900493348070477,
            "mae": 0.30002869519284114,
            "precision": 0.7925,
            "recall": 0.6645702306079665
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(48)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.5374517909673298,
            "auditor_fn_violation": 0.0029860421493986544,
            "auditor_fp_violation": 0.0018123613631780705,
            "ave_precision_score": 0.5384815785923279,
            "fpr": 0.4649122807017544,
            "logloss": 0.9109297244981022,
            "mae": 0.48508165215893667,
            "precision": 0.5251959686450168,
            "recall": 0.9832285115303984
        },
        "train": {
            "accuracy": 0.5334796926454446,
            "auc_prc": 0.5323965858383805,
            "auditor_fn_violation": 0.001882420083443218,
            "auditor_fp_violation": 0.006621578556000159,
            "ave_precision_score": 0.5339044783457062,
            "fpr": 0.4610318331503842,
            "logloss": 0.8819472960481554,
            "mae": 0.4828158170679987,
            "precision": 0.5291479820627802,
            "recall": 0.989517819706499
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(49)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5317982456140351,
            "auc_prc": 0.6540371175775357,
            "auditor_fn_violation": 0.0011838427305160177,
            "auditor_fp_violation": 0.002608892921960096,
            "ave_precision_score": 0.5456488942101246,
            "fpr": 0.46381578947368424,
            "logloss": 7.997755938383899,
            "mae": 0.4602981775737645,
            "precision": 0.5279017857142857,
            "recall": 0.9916142557651991
        },
        "train": {
            "accuracy": 0.5422612513721186,
            "auc_prc": 0.6662454273882651,
            "auditor_fn_violation": 0.00042803195051398354,
            "auditor_fp_violation": 0.004051859758102467,
            "ave_precision_score": 0.5604467388083346,
            "fpr": 0.4544456641053787,
            "logloss": 7.740079600352502,
            "mae": 0.4454145828010484,
            "precision": 0.5337837837837838,
            "recall": 0.9937106918238994
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(50)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8084444511566019,
            "auditor_fn_violation": 0.01636461436610394,
            "auditor_fp_violation": 0.01664398064125832,
            "ave_precision_score": 0.753851377160023,
            "fpr": 0.17105263157894737,
            "logloss": 2.9552086308723076,
            "mae": 0.2996249824696988,
            "precision": 0.7243816254416962,
            "recall": 0.859538784067086
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.7974660493264143,
            "auditor_fn_violation": 0.010160005707092674,
            "auditor_fp_violation": 0.01546383930152211,
            "ave_precision_score": 0.7339816261305308,
            "fpr": 0.17892425905598244,
            "logloss": 3.456911520339764,
            "mae": 0.3096395434012266,
            "precision": 0.7155322862129145,
            "recall": 0.859538784067086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(51)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.7803967689265601,
            "auditor_fn_violation": 0.004231950421126191,
            "auditor_fp_violation": 0.004824561403508777,
            "ave_precision_score": 0.698671819165187,
            "fpr": 0.09539473684210527,
            "logloss": 0.6367799867145635,
            "mae": 0.37145118837922875,
            "precision": 0.7635869565217391,
            "recall": 0.589098532494759
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.7726549351808758,
            "auditor_fn_violation": 0.013310412912757435,
            "auditor_fp_violation": 0.01693080475701488,
            "ave_precision_score": 0.6902313669659196,
            "fpr": 0.10098792535675083,
            "logloss": 0.6395361659484577,
            "mae": 0.3766872443250584,
            "precision": 0.7451523545706371,
            "recall": 0.5639412997903563
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(52)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8483942700524076,
            "auditor_fn_violation": 0.015219849939313699,
            "auditor_fp_violation": 0.01703972575115951,
            "ave_precision_score": 0.8433976987735591,
            "fpr": 0.12609649122807018,
            "logloss": 0.5124267612878983,
            "mae": 0.31022181344349264,
            "precision": 0.7653061224489796,
            "recall": 0.7861635220125787
        },
        "train": {
            "accuracy": 0.7991218441273326,
            "auc_prc": 0.8557927044540456,
            "auditor_fn_violation": 0.01203091955530702,
            "auditor_fp_violation": 0.016182146524556497,
            "ave_precision_score": 0.8492753243410682,
            "fpr": 0.09989023051591657,
            "logloss": 0.47757860389623746,
            "mae": 0.2988477116931152,
            "precision": 0.8088235294117647,
            "recall": 0.8071278825995807
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(53)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.6836594816764289,
            "auditor_fn_violation": 0.08000248262164845,
            "auditor_fp_violation": 0.053682698124621904,
            "ave_precision_score": 0.6843535602264252,
            "fpr": 0.15789473684210525,
            "logloss": 0.6772371260765747,
            "mae": 0.4430879521706517,
            "precision": 0.6521739130434783,
            "recall": 0.5660377358490566
        },
        "train": {
            "accuracy": 0.6717892425905598,
            "auc_prc": 0.7260692846892879,
            "auditor_fn_violation": 0.08154008657291387,
            "auditor_fp_violation": 0.049097816244871945,
            "ave_precision_score": 0.7271469266426668,
            "fpr": 0.14050493962678376,
            "logloss": 0.6355015061406489,
            "mae": 0.42516626022617066,
            "precision": 0.7050691244239631,
            "recall": 0.6415094339622641
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(54)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5493421052631579,
            "auc_prc": 0.6851408460622739,
            "auditor_fn_violation": 0.0009286843944242157,
            "auditor_fp_violation": 0.0038818310143174098,
            "ave_precision_score": 0.6860772602108132,
            "fpr": 0.44846491228070173,
            "logloss": 1.6469984310780952,
            "mae": 0.43083666277265076,
            "precision": 0.5373303167420814,
            "recall": 0.9958071278825996
        },
        "train": {
            "accuracy": 0.5609220636663008,
            "auc_prc": 0.6587231935967711,
            "auditor_fn_violation": 0.000524684326436496,
            "auditor_fp_violation": 0.005526412965951235,
            "ave_precision_score": 0.6600393557010461,
            "fpr": 0.43578485181119647,
            "logloss": 1.6559098655089328,
            "mae": 0.42244991414355265,
            "precision": 0.5442020665901263,
            "recall": 0.9937106918238994
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(55)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8065053789068806,
            "auditor_fn_violation": 0.0009953473831328839,
            "auditor_fp_violation": 0.007083081266384352,
            "ave_precision_score": 0.7971515901947445,
            "fpr": 0.08771929824561403,
            "logloss": 0.5494540972302367,
            "mae": 0.34839402535815905,
            "precision": 0.805352798053528,
            "recall": 0.6939203354297694
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8214291135004833,
            "auditor_fn_violation": 0.014270032930845234,
            "auditor_fp_violation": 0.013969052087390674,
            "ave_precision_score": 0.8101746328052172,
            "fpr": 0.07574094401756312,
            "logloss": 0.55305279826313,
            "mae": 0.3494675685799835,
            "precision": 0.8217054263565892,
            "recall": 0.6666666666666666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(56)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5296052631578947,
            "auc_prc": 0.7627773847574914,
            "auditor_fn_violation": 0.0005976681746294458,
            "auditor_fp_violation": 0.003632284734825571,
            "ave_precision_score": 0.5265485497019248,
            "fpr": 0.4692982456140351,
            "logloss": 16.212746901178008,
            "mae": 0.4707662484020387,
            "precision": 0.5265486725663717,
            "recall": 0.9979035639412998
        },
        "train": {
            "accuracy": 0.5290889132821076,
            "auc_prc": 0.7632450331125828,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0017730048005179834,
            "ave_precision_score": 0.5264900662251656,
            "fpr": 0.47091108671789245,
            "logloss": 16.26552338320753,
            "mae": 0.47134384440930827,
            "precision": 0.5264900662251656,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(57)",
        "seed": 6832,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.8555068928317382,
            "auditor_fn_violation": 0.00638585457354077,
            "auditor_fp_violation": 0.012648719499899177,
            "ave_precision_score": 0.8558121911002924,
            "fpr": 0.14912280701754385,
            "logloss": 0.49713221191860807,
            "mae": 0.32654098968282996,
            "precision": 0.7472118959107806,
            "recall": 0.8427672955974843
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8580505490348758,
            "auditor_fn_violation": 0.00780122748517422,
            "auditor_fp_violation": 0.005483415702600579,
            "ave_precision_score": 0.8582914275709255,
            "fpr": 0.13391877058177826,
            "logloss": 0.49082790198869664,
            "mae": 0.328853726317507,
            "precision": 0.7635658914728682,
            "recall": 0.8259958071278826
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(58)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8303968949104293,
            "auditor_fn_violation": 0.010626999889661261,
            "auditor_fp_violation": 0.018141258318209324,
            "ave_precision_score": 0.8311766696438228,
            "fpr": 0.17324561403508773,
            "logloss": 0.590882204559467,
            "mae": 0.34002235167141054,
            "precision": 0.7163375224416517,
            "recall": 0.8364779874213837
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8324340833432637,
            "auditor_fn_violation": 0.017984245662724635,
            "auditor_fp_violation": 0.019055375416694078,
            "ave_precision_score": 0.8326755584614575,
            "fpr": 0.18111964873765093,
            "logloss": 0.540908316091613,
            "mae": 0.34543690011191514,
            "precision": 0.7053571428571429,
            "recall": 0.8280922431865828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(59)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5230263157894737,
            "auc_prc": 0.7615131578947368,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5230263157894737,
            "fpr": 0.4769736842105263,
            "logloss": 0.6920866952430973,
            "mae": 0.4989577559264083,
            "precision": 0.5230263157894737,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5246981339187706,
            "auc_prc": 0.762087912087912,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0004805576492131596,
            "ave_precision_score": 0.5241758241758242,
            "fpr": 0.47530186608122943,
            "logloss": 0.6912229798908217,
            "mae": 0.4983580815702115,
            "precision": 0.5241758241758242,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(60)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7840919571391494,
            "auditor_fn_violation": 0.022028669682592223,
            "auditor_fp_violation": 0.017120387174833634,
            "ave_precision_score": 0.6596770212880636,
            "fpr": 0.16228070175438597,
            "logloss": 0.6184474127108845,
            "mae": 0.42442018385126923,
            "precision": 0.6967213114754098,
            "recall": 0.7127882599580713
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7738170596976215,
            "auditor_fn_violation": 0.009713563780212504,
            "auditor_fp_violation": 0.02600828582557275,
            "ave_precision_score": 0.6441399391820493,
            "fpr": 0.17672886937431395,
            "logloss": 0.6280199948382935,
            "mae": 0.4298728853627696,
            "precision": 0.6830708661417323,
            "recall": 0.7274633123689728
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(61)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.7887199584184605,
            "auditor_fn_violation": 0.017989812056346324,
            "auditor_fp_violation": 0.016447368421052634,
            "ave_precision_score": 0.7505565299958431,
            "fpr": 0.1425438596491228,
            "logloss": 3.946096877056493,
            "mae": 0.2883315396948448,
            "precision": 0.7319587628865979,
            "recall": 0.7442348008385744
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7833608308855771,
            "auditor_fn_violation": 0.015487392618059728,
            "auditor_fp_violation": 0.02143540040569182,
            "ave_precision_score": 0.7402787467990732,
            "fpr": 0.15916575192096596,
            "logloss": 4.34777513073738,
            "mae": 0.2920154214868176,
            "precision": 0.716796875,
            "recall": 0.7693920335429769
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(62)",
        "seed": 6832,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.854897930919765,
            "auditor_fn_violation": 0.016840450182058923,
            "auditor_fp_violation": 0.016001209921355116,
            "ave_precision_score": 0.8420189041213446,
            "fpr": 0.1162280701754386,
            "logloss": 0.510506302917293,
            "mae": 0.32775192573797285,
            "precision": 0.7700650759219089,
            "recall": 0.7442348008385744
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8581584953923753,
            "auditor_fn_violation": 0.02514342522212787,
            "auditor_fp_violation": 0.020927020997840027,
            "ave_precision_score": 0.845009443172829,
            "fpr": 0.1119648737650933,
            "logloss": 0.5061261871856149,
            "mae": 0.3290383233131614,
            "precision": 0.7792207792207793,
            "recall": 0.7547169811320755
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(63)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7793331741766439,
            "auditor_fn_violation": 0.011187888484313511,
            "auditor_fp_violation": 0.016359144988909058,
            "ave_precision_score": 0.7480540268434231,
            "fpr": 0.14035087719298245,
            "logloss": 0.5800746963160534,
            "mae": 0.3972389823512027,
            "precision": 0.7382413087934561,
            "recall": 0.7568134171907757
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7750762635380878,
            "auditor_fn_violation": 0.00625248822336825,
            "auditor_fp_violation": 0.009874195065937572,
            "ave_precision_score": 0.7393529705015266,
            "fpr": 0.13391877058177826,
            "logloss": 0.592972592610742,
            "mae": 0.4030136507245764,
            "precision": 0.7365010799136069,
            "recall": 0.7148846960167715
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(64)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.47551218441584636,
            "auditor_fn_violation": 0.014723325609621543,
            "auditor_fp_violation": 0.030406836055656385,
            "ave_precision_score": 0.46907674424237666,
            "fpr": 0.2883771929824561,
            "logloss": 3.3274845087918377,
            "mae": 0.5089939253356479,
            "precision": 0.5286738351254481,
            "recall": 0.6184486373165619
        },
        "train": {
            "accuracy": 0.5060373216245884,
            "auc_prc": 0.4532526320244411,
            "auditor_fn_violation": 0.014603713752482478,
            "auditor_fp_violation": 0.01942464603135259,
            "ave_precision_score": 0.45017571631104575,
            "fpr": 0.2810098792535675,
            "logloss": 3.8113259366702588,
            "mae": 0.5220244039723226,
            "precision": 0.5250463821892394,
            "recall": 0.5932914046121593
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(65)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8274339278708811,
            "auditor_fn_violation": 0.006698481003346947,
            "auditor_fp_violation": 0.008895442629562413,
            "ave_precision_score": 0.8274711846580838,
            "fpr": 0.07456140350877193,
            "logloss": 0.5438085462179244,
            "mae": 0.36174346743511004,
            "precision": 0.8242894056847545,
            "recall": 0.6687631027253669
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8515150154301796,
            "auditor_fn_violation": 0.011554561416831779,
            "auditor_fp_violation": 0.005847627815688439,
            "ave_precision_score": 0.8513408482089353,
            "fpr": 0.0570801317233809,
            "logloss": 0.5435915468864837,
            "mae": 0.3610144742978195,
            "precision": 0.8567493112947658,
            "recall": 0.6519916142557652
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(66)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5383771929824561,
            "auc_prc": 0.752632471769843,
            "auditor_fn_violation": 0.01013737173121483,
            "auditor_fp_violation": 0.01640199637023595,
            "ave_precision_score": 0.5627092666903207,
            "fpr": 0.44627192982456143,
            "logloss": 0.8740554302051274,
            "mae": 0.4728065678163579,
            "precision": 0.532183908045977,
            "recall": 0.9706498951781971
        },
        "train": {
            "accuracy": 0.544456641053787,
            "auc_prc": 0.7555899068237357,
            "auditor_fn_violation": 0.003566932920949863,
            "auditor_fp_violation": 0.01620238053083916,
            "ave_precision_score": 0.565611298705915,
            "fpr": 0.4500548847420417,
            "logloss": 0.8643195075334617,
            "mae": 0.46868237825243453,
            "precision": 0.5351473922902494,
            "recall": 0.989517819706499
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(67)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8273690178515432,
            "auditor_fn_violation": 0.008597226819669726,
            "auditor_fp_violation": 0.012497479330510184,
            "ave_precision_score": 0.7921455616885702,
            "fpr": 0.09758771929824561,
            "logloss": 0.5280689478908999,
            "mae": 0.34534243861899566,
            "precision": 0.7972665148063781,
            "recall": 0.7337526205450734
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8392053274082565,
            "auditor_fn_violation": 0.004349356916513058,
            "auditor_fp_violation": 0.002663301076954987,
            "ave_precision_score": 0.8008478608501,
            "fpr": 0.08232711306256861,
            "logloss": 0.4985768211406026,
            "mae": 0.33529036628689646,
            "precision": 0.8179611650485437,
            "recall": 0.7064989517819706
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(68)",
        "seed": 6832,
        "test": {
            "accuracy": 0.4956140350877193,
            "auc_prc": 0.49061701727842116,
            "auditor_fn_violation": 0.006344477546066427,
            "auditor_fp_violation": 0.007803992740471879,
            "ave_precision_score": 0.4948976604459533,
            "fpr": 0.375,
            "logloss": 3.4086462363441346,
            "mae": 0.5124965060089731,
            "precision": 0.5121255349500713,
            "recall": 0.7526205450733753
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.4878452284608128,
            "auditor_fn_violation": 0.004572577879953158,
            "auditor_fp_violation": 0.0069807321675173445,
            "ave_precision_score": 0.4920169225861554,
            "fpr": 0.3699231613611416,
            "logloss": 3.9379569652577695,
            "mae": 0.5156284710504809,
            "precision": 0.5094614264919942,
            "recall": 0.7337526205450734
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(69)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5997807017543859,
            "auc_prc": 0.6877128711097198,
            "auditor_fn_violation": 0.017456508146676965,
            "auditor_fp_violation": 0.016447368421052648,
            "ave_precision_score": 0.5577705162988883,
            "fpr": 0.34978070175438597,
            "logloss": 9.958836183458311,
            "mae": 0.44932802992958676,
            "precision": 0.5746666666666667,
            "recall": 0.9035639412997903
        },
        "train": {
            "accuracy": 0.5609220636663008,
            "auc_prc": 0.6792715398938834,
            "auditor_fn_violation": 0.017303076537175496,
            "auditor_fp_violation": 0.008462873127722112,
            "ave_precision_score": 0.5382246971354924,
            "fpr": 0.3885839736553238,
            "logloss": 10.728443993803756,
            "mae": 0.4684651732929337,
            "precision": 0.5490445859872611,
            "recall": 0.9035639412997903
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(70)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.6699200658599602,
            "auditor_fn_violation": 0.02114366103939094,
            "auditor_fp_violation": 0.03808983666061706,
            "ave_precision_score": 0.6641361899029066,
            "fpr": 0.13486842105263158,
            "logloss": 1.5560810860323944,
            "mae": 0.3381579458753613,
            "precision": 0.7421383647798742,
            "recall": 0.7421383647798742
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.6866693427140875,
            "auditor_fn_violation": 0.013441583994366547,
            "auditor_fp_violation": 0.037129401528679176,
            "ave_precision_score": 0.6806729224615752,
            "fpr": 0.14050493962678376,
            "logloss": 1.4524578944537243,
            "mae": 0.3371400079088931,
            "precision": 0.7360824742268042,
            "recall": 0.7484276729559748
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(71)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.8399318822065553,
            "auditor_fn_violation": 0.0031837323917760845,
            "auditor_fp_violation": 0.01600373059084494,
            "ave_precision_score": 0.8406846887261282,
            "fpr": 0.22916666666666666,
            "logloss": 0.5648570662670037,
            "mae": 0.33944556561886874,
            "precision": 0.6687797147385103,
            "recall": 0.8846960167714885
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8683897420370641,
            "auditor_fn_violation": 0.005108768441618515,
            "auditor_fp_violation": 0.02475124818526257,
            "ave_precision_score": 0.8685823787798663,
            "fpr": 0.20087815587266739,
            "logloss": 0.5122047298233144,
            "mae": 0.3242816423688546,
            "precision": 0.7038834951456311,
            "recall": 0.9119496855345912
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(72)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.7071172169719286,
            "auditor_fn_violation": 0.014316451506123802,
            "auditor_fp_violation": 0.01733716475095786,
            "ave_precision_score": 0.6619570156407665,
            "fpr": 0.17982456140350878,
            "logloss": 0.6263135683182249,
            "mae": 0.44663444223503274,
            "precision": 0.6815533980582524,
            "recall": 0.7358490566037735
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7461625374890167,
            "auditor_fn_violation": 0.0025336729973972905,
            "auditor_fp_violation": 0.022325696682128825,
            "ave_precision_score": 0.6761394635824213,
            "fpr": 0.18331503841931943,
            "logloss": 0.6187513615420758,
            "mae": 0.44377343960179977,
            "precision": 0.6860902255639098,
            "recall": 0.7651991614255765
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(73)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6162280701754386,
            "auc_prc": 0.656243638605627,
            "auditor_fn_violation": 0.0097029129427342,
            "auditor_fp_violation": 0.005772333131679777,
            "ave_precision_score": 0.6521591377315739,
            "fpr": 0.10416666666666667,
            "logloss": 3.667647181825534,
            "mae": 0.4446552849019241,
            "precision": 0.7003154574132492,
            "recall": 0.46540880503144655
        },
        "train": {
            "accuracy": 0.5960482985729967,
            "auc_prc": 0.6553404114089849,
            "auditor_fn_violation": 0.006358345587473868,
            "auditor_fp_violation": 0.015013632661732942,
            "ave_precision_score": 0.6496443622097159,
            "fpr": 0.10757409440175632,
            "logloss": 3.6797851864750424,
            "mae": 0.4457889604625113,
            "precision": 0.6786885245901639,
            "recall": 0.4339622641509434
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(74)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5230263157894737,
            "auc_prc": 0.7159740761554507,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.6384888649359648,
            "fpr": 0.4769736842105263,
            "logloss": 6.36954769772262,
            "mae": 0.47644665196799396,
            "precision": 0.5230263157894737,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5236004390779363,
            "auc_prc": 0.7016876886716255,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.6174860418137769,
            "fpr": 0.47639956092206365,
            "logloss": 6.877555665687092,
            "mae": 0.47591945074261477,
            "precision": 0.5236004390779363,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(75)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8218510161822647,
            "auditor_fn_violation": 0.004723877303321203,
            "auditor_fp_violation": 0.01449637023593467,
            "ave_precision_score": 0.7611838514047669,
            "fpr": 0.09100877192982457,
            "logloss": 0.5733342356517636,
            "mae": 0.3744177116987933,
            "precision": 0.7855297157622739,
            "recall": 0.6373165618448637
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8338633947738183,
            "auditor_fn_violation": 0.004229692070132801,
            "auditor_fp_violation": 0.0051343790942247084,
            "ave_precision_score": 0.7726651839497995,
            "fpr": 0.07683863885839737,
            "logloss": 0.539166724205119,
            "mae": 0.3629399214541022,
            "precision": 0.8113207547169812,
            "recall": 0.6310272536687631
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(76)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5427631578947368,
            "auc_prc": 0.7622501033147195,
            "auditor_fn_violation": 0.001820589208871235,
            "auditor_fp_violation": 0.004053236539624926,
            "ave_precision_score": 0.7083563821694323,
            "fpr": 0.4506578947368421,
            "logloss": 5.7844291531761804,
            "mae": 0.4570305939869356,
            "precision": 0.5340136054421769,
            "recall": 0.9874213836477987
        },
        "train": {
            "accuracy": 0.5587266739846323,
            "auc_prc": 0.7632624216886725,
            "auditor_fn_violation": 0.0017742614722918348,
            "auditor_fp_violation": 0.0075245210863638,
            "ave_precision_score": 0.699803838076851,
            "fpr": 0.43798024149286496,
            "logloss": 6.039984525641476,
            "mae": 0.44107612055027595,
            "precision": 0.5429553264604811,
            "recall": 0.9937106918238994
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(77)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.7939200206938308,
            "auditor_fn_violation": 0.017916252896391916,
            "auditor_fp_violation": 0.0032188949384956635,
            "ave_precision_score": 0.7709890092808466,
            "fpr": 0.06798245614035088,
            "logloss": 1.9922954122395646,
            "mae": 0.3357448083319176,
            "precision": 0.7769784172661871,
            "recall": 0.4528301886792453
        },
        "train": {
            "accuracy": 0.6608122941822173,
            "auc_prc": 0.7969546125584883,
            "auditor_fn_violation": 0.010953935937884763,
            "auditor_fp_violation": 0.004772696231922182,
            "ave_precision_score": 0.7680970603287016,
            "fpr": 0.06695938529088913,
            "logloss": 2.1793531548421634,
            "mae": 0.3312902759092074,
            "precision": 0.7896551724137931,
            "recall": 0.480083857442348
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(78)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.6194039762115118,
            "auditor_fn_violation": 0.008955827724447389,
            "auditor_fp_violation": 0.04295220810647309,
            "ave_precision_score": 0.5591471382484928,
            "fpr": 0.33881578947368424,
            "logloss": 5.3658057245941615,
            "mae": 0.39089616590195636,
            "precision": 0.590728476821192,
            "recall": 0.9350104821802935
        },
        "train": {
            "accuracy": 0.6542261251372119,
            "auc_prc": 0.653157844109185,
            "auditor_fn_violation": 0.0078472524260897,
            "auditor_fp_violation": 0.04335894621295281,
            "ave_precision_score": 0.5945722467844188,
            "fpr": 0.3150384193194292,
            "logloss": 4.712180723842989,
            "mae": 0.3722634404755596,
            "precision": 0.6100543478260869,
            "recall": 0.9412997903563941
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(79)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5657894736842105,
            "auc_prc": 0.5243574686237009,
            "auditor_fn_violation": 0.04001158556769282,
            "auditor_fp_violation": 0.017952208106473095,
            "ave_precision_score": 0.5261122910973228,
            "fpr": 0.35526315789473684,
            "logloss": 1.0389908678640567,
            "mae": 0.4668300973676276,
            "precision": 0.5555555555555556,
            "recall": 0.8490566037735849
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.5233231883863118,
            "auditor_fn_violation": 0.030176252511235836,
            "auditor_fp_violation": 0.008149246030340897,
            "ave_precision_score": 0.5250094192540524,
            "fpr": 0.36553238199780463,
            "logloss": 1.0615902336399723,
            "mae": 0.464778113803539,
            "precision": 0.5542168674698795,
            "recall": 0.8679245283018868
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(80)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.7989332592102734,
            "auditor_fn_violation": 0.016378406708595392,
            "auditor_fp_violation": 0.018942831215970967,
            "ave_precision_score": 0.7865846419942701,
            "fpr": 0.14802631578947367,
            "logloss": 1.359121191358487,
            "mae": 0.2763902330272941,
            "precision": 0.7383720930232558,
            "recall": 0.7987421383647799
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.7924276132967508,
            "auditor_fn_violation": 0.009485740322680868,
            "auditor_fp_violation": 0.020603276897317484,
            "ave_precision_score": 0.780202142315751,
            "fpr": 0.16245883644346873,
            "logloss": 1.454221096305146,
            "mae": 0.2836125499555334,
            "precision": 0.7233644859813084,
            "recall": 0.8113207547169812
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(81)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7752192982456141,
            "auc_prc": 0.8467188538211196,
            "auditor_fn_violation": 0.011757971973960064,
            "auditor_fp_violation": 0.013004133897963297,
            "ave_precision_score": 0.8394735562030355,
            "fpr": 0.11403508771929824,
            "logloss": 0.5010107894164398,
            "mae": 0.33102510741650404,
            "precision": 0.7833333333333333,
            "recall": 0.7882599580712788
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8494006307706254,
            "auditor_fn_violation": 0.016608099929351715,
            "auditor_fp_violation": 0.021794554017209023,
            "ave_precision_score": 0.8495363335747019,
            "fpr": 0.11306256860592755,
            "logloss": 0.488412341479564,
            "mae": 0.3215082538867232,
            "precision": 0.7876288659793814,
            "recall": 0.80083857442348
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(82)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5822368421052632,
            "auc_prc": 0.6562886637504011,
            "auditor_fn_violation": 0.05531189083820663,
            "auditor_fp_violation": 0.017077535793506755,
            "ave_precision_score": 0.6620836296266157,
            "fpr": 0.08114035087719298,
            "logloss": 9.05533926554871,
            "mae": 0.4203938688505401,
            "precision": 0.6967213114754098,
            "recall": 0.35639412997903563
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.6210506470233453,
            "auditor_fn_violation": 0.047449412836816265,
            "auditor_fp_violation": 0.01819543014968106,
            "ave_precision_score": 0.6335223803868467,
            "fpr": 0.08781558726673985,
            "logloss": 10.113384443306616,
            "mae": 0.44248047118606076,
            "precision": 0.6521739130434783,
            "recall": 0.31446540880503143
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(83)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5241228070175439,
            "auc_prc": 0.5793941014441625,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0005595886267392721,
            "ave_precision_score": 0.5814867997979036,
            "fpr": 0.4758771929824561,
            "logloss": 1.0133004048377943,
            "mae": 0.47631742419642314,
            "precision": 0.5236004390779363,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5268935236004391,
            "auc_prc": 0.5405517830484106,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0013733831764354814,
            "ave_precision_score": 0.5421249606365016,
            "fpr": 0.47310647639956094,
            "logloss": 1.023526441075544,
            "mae": 0.4774691986919889,
            "precision": 0.525330396475771,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(84)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.7691665936741882,
            "auditor_fn_violation": 0.028520265548567438,
            "auditor_fp_violation": 0.023253176043557172,
            "ave_precision_score": 0.7840439219675376,
            "fpr": 0.14364035087719298,
            "logloss": 0.5472203957733067,
            "mae": 0.3689827273697837,
            "precision": 0.7400793650793651,
            "recall": 0.7819706498951782
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8120948074580012,
            "auditor_fn_violation": 0.029538807079556414,
            "auditor_fp_violation": 0.030416769944407073,
            "ave_precision_score": 0.8052169772328104,
            "fpr": 0.13830954994511527,
            "logloss": 0.5345258416075811,
            "mae": 0.363345815357471,
            "precision": 0.749003984063745,
            "recall": 0.7882599580712788
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(85)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7371923872250465,
            "auditor_fn_violation": 0.03911508330574865,
            "auditor_fp_violation": 0.034928917120387176,
            "ave_precision_score": 0.7375012707210542,
            "fpr": 0.15789473684210525,
            "logloss": 0.6075788131224706,
            "mae": 0.4260951345949842,
            "precision": 0.6955602536997886,
            "recall": 0.689727463312369
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.7807285081867947,
            "auditor_fn_violation": 0.043297963166239795,
            "auditor_fp_violation": 0.040260614000920646,
            "ave_precision_score": 0.7807452439420388,
            "fpr": 0.15367727771679474,
            "logloss": 0.5922430104649106,
            "mae": 0.4207241982058034,
            "precision": 0.6916299559471366,
            "recall": 0.6582809224318659
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(86)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.7864429495536215,
            "auditor_fn_violation": 0.0037078414064511457,
            "auditor_fp_violation": 0.0038188142770719945,
            "ave_precision_score": 0.7766293397562407,
            "fpr": 0.15789473684210525,
            "logloss": 0.5971334351582873,
            "mae": 0.4311453642879139,
            "precision": 0.6916488222698073,
            "recall": 0.6771488469601677
        },
        "train": {
            "accuracy": 0.6344676180021954,
            "auc_prc": 0.7515320979042208,
            "auditor_fn_violation": 0.007131564594853953,
            "auditor_fp_violation": 0.009945014087926876,
            "ave_precision_score": 0.7412082138948889,
            "fpr": 0.1712403951701427,
            "logloss": 0.6186022192651757,
            "mae": 0.44289251795584494,
            "precision": 0.6578947368421053,
            "recall": 0.6289308176100629
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(87)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7937429075895723,
            "auditor_fn_violation": 0.003882544411342822,
            "auditor_fp_violation": 0.007108287961282527,
            "ave_precision_score": 0.7817281735497241,
            "fpr": 0.24671052631578946,
            "logloss": 0.5930443285839789,
            "mae": 0.38409799843204256,
            "precision": 0.6495327102803738,
            "recall": 0.8742138364779874
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7931171871324546,
            "auditor_fn_violation": 0.0016568978729573603,
            "auditor_fp_violation": 0.005632641498935199,
            "ave_precision_score": 0.7780503403481636,
            "fpr": 0.24259055982436883,
            "logloss": 0.5993967521866792,
            "mae": 0.39392743539495595,
            "precision": 0.6497622820919176,
            "recall": 0.859538784067086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(88)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5230263157894737,
            "auc_prc": 0.7615131578947368,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5230263157894737,
            "fpr": 0.4769736842105263,
            "logloss": 0.69227814303087,
            "mae": 0.49939026683568954,
            "precision": 0.5230263157894737,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5236004390779363,
            "auc_prc": 0.7618002195389681,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5236004390779363,
            "fpr": 0.47639956092206365,
            "logloss": 0.692247730535352,
            "mae": 0.4993750641427108,
            "precision": 0.5236004390779363,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(89)",
        "seed": 6832,
        "test": {
            "accuracy": 0.4868421052631579,
            "auc_prc": 0.4956662702566377,
            "auditor_fn_violation": 0.014337140019860972,
            "auditor_fp_violation": 0.039322444041137335,
            "ave_precision_score": 0.5114232866715489,
            "fpr": 0.17763157894736842,
            "logloss": 0.8653014600725591,
            "mae": 0.512507782399393,
            "precision": 0.5135135135135135,
            "recall": 0.3584905660377358
        },
        "train": {
            "accuracy": 0.5005488474204172,
            "auc_prc": 0.5028439407216951,
            "auditor_fn_violation": 0.015740529793094885,
            "auditor_fp_violation": 0.032986488742304754,
            "ave_precision_score": 0.5180962056663028,
            "fpr": 0.18331503841931943,
            "logloss": 0.8451483512490139,
            "mae": 0.5099342050916146,
            "precision": 0.5308988764044944,
            "recall": 0.39622641509433965
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(90)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.7888302102654444,
            "auditor_fn_violation": 0.036894516164625406,
            "auditor_fp_violation": 0.005769812462189957,
            "ave_precision_score": 0.7718513092616542,
            "fpr": 0.05592105263157895,
            "logloss": 0.6422411425606388,
            "mae": 0.3782353655500501,
            "precision": 0.8449848024316109,
            "recall": 0.5828092243186582
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7848053305194261,
            "auditor_fn_violation": 0.03907977733133585,
            "auditor_fp_violation": 0.005660463257573843,
            "ave_precision_score": 0.767744575180712,
            "fpr": 0.054884742041712405,
            "logloss": 0.6628475231797576,
            "mae": 0.37973333406134313,
            "precision": 0.8402555910543131,
            "recall": 0.5513626834381551
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(91)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5307017543859649,
            "auc_prc": 0.6643141802981412,
            "auditor_fn_violation": 0.003714737577696863,
            "auditor_fp_violation": 0.004932950191570886,
            "ave_precision_score": 0.6090193954763617,
            "fpr": 0.4616228070175439,
            "logloss": 7.881399260446615,
            "mae": 0.4689729086392801,
            "precision": 0.5274971941638609,
            "recall": 0.9853249475890985
        },
        "train": {
            "accuracy": 0.535675082327113,
            "auc_prc": 0.6450682383341045,
            "auditor_fn_violation": 0.0013255182983658847,
            "auditor_fp_violation": 0.001173572364394219,
            "ave_precision_score": 0.600301567362012,
            "fpr": 0.4588364434687157,
            "logloss": 7.942739015483763,
            "mae": 0.4630291565410776,
            "precision": 0.5303370786516854,
            "recall": 0.989517819706499
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(92)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8462275284923191,
            "auditor_fn_violation": 0.0022550479973518726,
            "auditor_fp_violation": 0.021856725146198835,
            "ave_precision_score": 0.8465636877850156,
            "fpr": 0.16228070175438597,
            "logloss": 0.5202859283456934,
            "mae": 0.3214567927633445,
            "precision": 0.7323688969258589,
            "recall": 0.8490566037735849
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8412914398378638,
            "auditor_fn_violation": 0.008974863478519012,
            "auditor_fp_violation": 0.012631078421949857,
            "ave_precision_score": 0.8416534186346306,
            "fpr": 0.15587266739846323,
            "logloss": 0.5107286017900435,
            "mae": 0.3216003084736217,
            "precision": 0.7375231053604436,
            "recall": 0.8364779874213837
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(93)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.6514149661102716,
            "auditor_fn_violation": 0.0040411563499944845,
            "auditor_fp_violation": 0.006208408953418031,
            "ave_precision_score": 0.72762895305194,
            "fpr": 0.08881578947368421,
            "logloss": 0.5828932737417594,
            "mae": 0.37547829171250524,
            "precision": 0.7990074441687345,
            "recall": 0.6750524109014675
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8146925134997839,
            "auditor_fn_violation": 0.010615652622155945,
            "auditor_fp_violation": 0.01026875818844942,
            "ave_precision_score": 0.7253138522698335,
            "fpr": 0.06915477497255763,
            "logloss": 0.5938823565001653,
            "mae": 0.3756430834494981,
            "precision": 0.8288043478260869,
            "recall": 0.639412997903564
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(94)",
        "seed": 6832,
        "test": {
            "accuracy": 0.36403508771929827,
            "auc_prc": 0.385917076054625,
            "auditor_fn_violation": 0.01072814373459857,
            "auditor_fp_violation": 0.024856321839080465,
            "ave_precision_score": 0.3870608765299035,
            "fpr": 0.1513157894736842,
            "logloss": 1.2437179024131584,
            "mae": 0.5871232322290445,
            "precision": 0.2023121387283237,
            "recall": 0.07337526205450734
        },
        "train": {
            "accuracy": 0.3567508232711306,
            "auc_prc": 0.38774631560270234,
            "auditor_fn_violation": 0.0057255026498859805,
            "auditor_fp_violation": 0.019257715479520657,
            "ave_precision_score": 0.3890596130100973,
            "fpr": 0.16465422612513722,
            "logloss": 1.307586580727262,
            "mae": 0.5956305054425463,
            "precision": 0.21465968586387435,
            "recall": 0.0859538784067086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(95)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.824226156760211,
            "auditor_fn_violation": 0.008521368935966755,
            "auditor_fp_violation": 0.02067201048598508,
            "ave_precision_score": 0.8259891027979979,
            "fpr": 0.1162280701754386,
            "logloss": 0.5037305648139856,
            "mae": 0.33841365327437717,
            "precision": 0.7720430107526882,
            "recall": 0.7526205450733753
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.829162981551296,
            "auditor_fn_violation": 0.00312739473520701,
            "auditor_fp_violation": 0.013809709287914739,
            "ave_precision_score": 0.8297718571105681,
            "fpr": 0.10318331503841932,
            "logloss": 0.5069947637021988,
            "mae": 0.344241585463907,
            "precision": 0.7887640449438202,
            "recall": 0.7358490566037735
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(96)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8503075927069419,
            "auditor_fn_violation": 0.01649104417227555,
            "auditor_fp_violation": 0.01710526315789474,
            "ave_precision_score": 0.8309344837520946,
            "fpr": 0.14912280701754385,
            "logloss": 0.5445567646170918,
            "mae": 0.32520078736757696,
            "precision": 0.7263581488933601,
            "recall": 0.7568134171907757
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8449907184380645,
            "auditor_fn_violation": 0.01963423979454467,
            "auditor_fp_violation": 0.023415803770607074,
            "ave_precision_score": 0.8217987507775757,
            "fpr": 0.15148188803512624,
            "logloss": 0.5041254417155161,
            "mae": 0.31763451981013713,
            "precision": 0.7309941520467836,
            "recall": 0.7861635220125787
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(97)",
        "seed": 6832,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8375830781564504,
            "auditor_fn_violation": 0.003126264298061713,
            "auditor_fp_violation": 0.010806110102843311,
            "ave_precision_score": 0.797155059638649,
            "fpr": 0.11842105263157894,
            "logloss": 0.5426457991490268,
            "mae": 0.36545628088673476,
            "precision": 0.7697228144989339,
            "recall": 0.7568134171907757
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8345798665353096,
            "auditor_fn_violation": 0.002029699894372771,
            "auditor_fp_violation": 0.0072715960078305625,
            "ave_precision_score": 0.7936935612270108,
            "fpr": 0.10867178924259056,
            "logloss": 0.5471477987885492,
            "mae": 0.36885201693824304,
            "precision": 0.777027027027027,
            "recall": 0.7232704402515723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(98)",
        "seed": 6832,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.7410632532082386,
            "auditor_fn_violation": 0.0058433557688771206,
            "auditor_fp_violation": 0.009959165154264997,
            "ave_precision_score": 0.650749483207505,
            "fpr": 0.2532894736842105,
            "logloss": 0.6408454484865701,
            "mae": 0.4502454186021759,
            "precision": 0.6304,
            "recall": 0.8259958071278826
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.7413539556907782,
            "auditor_fn_violation": 0.008602061457103611,
            "auditor_fp_violation": 0.00998548210049221,
            "ave_precision_score": 0.6517605257094905,
            "fpr": 0.2667398463227223,
            "logloss": 0.6486070922124549,
            "mae": 0.45532125865981293,
            "precision": 0.6203125,
            "recall": 0.8322851153039832
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(99)",
        "seed": 6832,
        "test": {
            "accuracy": 0.5975877192982456,
            "auc_prc": 0.7676100890337786,
            "auditor_fn_violation": 0.0076156717790282855,
            "auditor_fp_violation": 0.019736842105263164,
            "ave_precision_score": 0.7682654464079663,
            "fpr": 0.38377192982456143,
            "logloss": 2.039873534937516,
            "mae": 0.3926591791564573,
            "precision": 0.5679012345679012,
            "recall": 0.9643605870020965
        },
        "train": {
            "accuracy": 0.6289791437980241,
            "auc_prc": 0.7957341435687044,
            "auditor_fn_violation": 0.0028995712776753724,
            "auditor_fp_violation": 0.026349734681592633,
            "ave_precision_score": 0.7959672284091178,
            "fpr": 0.3578485181119649,
            "logloss": 2.017531237887925,
            "mae": 0.36880149497917497,
            "precision": 0.5878634639696586,
            "recall": 0.9748427672955975
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7892020774847004,
            "auditor_fn_violation": 0.027628020355147515,
            "auditor_fp_violation": 0.023332495917598294,
            "ave_precision_score": 0.789285189858539,
            "fpr": 0.10307017543859649,
            "logloss": 3.066095027070159,
            "mae": 0.3097189274400573,
            "precision": 0.7740384615384616,
            "recall": 0.6531440162271805
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7928653570207742,
            "auditor_fn_violation": 0.01924418590807461,
            "auditor_fp_violation": 0.02599829247469204,
            "ave_precision_score": 0.7928602354179515,
            "fpr": 0.11086717892425905,
            "logloss": 2.86955236469494,
            "mae": 0.29270118270263745,
            "precision": 0.7506172839506173,
            "recall": 0.6594360086767896
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7923308135946295,
            "auditor_fn_violation": 0.025039144514430085,
            "auditor_fp_violation": 0.019017187957961736,
            "ave_precision_score": 0.7925562159647046,
            "fpr": 0.08881578947368421,
            "logloss": 3.1773575379797028,
            "mae": 0.3101441860975742,
            "precision": 0.7928388746803069,
            "recall": 0.6288032454361054
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8006197180855275,
            "auditor_fn_violation": 0.01716308983239319,
            "auditor_fp_violation": 0.010523234540797661,
            "ave_precision_score": 0.800503057581855,
            "fpr": 0.08562019758507135,
            "logloss": 2.9890079561876677,
            "mae": 0.29108885441720694,
            "precision": 0.7886178861788617,
            "recall": 0.631236442516269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7951347432733166,
            "auditor_fn_violation": 0.026925198391516317,
            "auditor_fp_violation": 0.02132793200184232,
            "ave_precision_score": 0.7952115060649481,
            "fpr": 0.09539473684210527,
            "logloss": 3.013107613455791,
            "mae": 0.3043562041169044,
            "precision": 0.7841191066997518,
            "recall": 0.640973630831643
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7999361691536493,
            "auditor_fn_violation": 0.019951377595119668,
            "auditor_fp_violation": 0.01866081229418222,
            "ave_precision_score": 0.7998963505171661,
            "fpr": 0.09879253567508232,
            "logloss": 2.7883580605727443,
            "mae": 0.2883803280864623,
            "precision": 0.7668393782383419,
            "recall": 0.6420824295010846
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7989854129506795,
            "auditor_fn_violation": 0.02760800327390485,
            "auditor_fp_violation": 0.017473202696478674,
            "ave_precision_score": 0.7990356679526465,
            "fpr": 0.09320175438596491,
            "logloss": 3.0327301706571044,
            "mae": 0.3104129763109571,
            "precision": 0.7842639593908629,
            "recall": 0.6267748478701826
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8045016530908252,
            "auditor_fn_violation": 0.017696459993666234,
            "auditor_fp_violation": 0.01535309184046835,
            "ave_precision_score": 0.8043732667030671,
            "fpr": 0.09001097694840834,
            "logloss": 2.871882606009659,
            "mae": 0.28949737076292287,
            "precision": 0.7859007832898173,
            "recall": 0.6529284164859002
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 22571,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7828804070427275,
            "auditor_fn_violation": 0.024349667271627354,
            "auditor_fp_violation": 0.023332495917598294,
            "ave_precision_score": 0.7829633478915891,
            "fpr": 0.10307017543859649,
            "logloss": 3.230729839996878,
            "mae": 0.3136780911951347,
            "precision": 0.7661691542288557,
            "recall": 0.6247464503042597
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7849903019802295,
            "auditor_fn_violation": 0.022472980277209627,
            "auditor_fp_violation": 0.02128064398097329,
            "ave_precision_score": 0.7848323600442348,
            "fpr": 0.10537870472008781,
            "logloss": 3.0581125640493534,
            "mae": 0.2961273851386691,
            "precision": 0.7538461538461538,
            "recall": 0.6377440347071583
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7900720648058781,
            "auditor_fn_violation": 0.024645475249991104,
            "auditor_fp_violation": 0.02364914374241092,
            "ave_precision_score": 0.790200709325336,
            "fpr": 0.1074561403508772,
            "logloss": 3.0360132994814184,
            "mae": 0.3094942854215588,
            "precision": 0.7683215130023641,
            "recall": 0.6592292089249493
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7994503492751452,
            "auditor_fn_violation": 0.020251398310835746,
            "auditor_fp_violation": 0.020182949140139045,
            "ave_precision_score": 0.7993372595708648,
            "fpr": 0.10537870472008781,
            "logloss": 2.8695391327928403,
            "mae": 0.28986351063549465,
            "precision": 0.7652811735941321,
            "recall": 0.6789587852494577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7868171118681795,
            "auditor_fn_violation": 0.02366686238923882,
            "auditor_fp_violation": 0.022123476950131893,
            "ave_precision_score": 0.786939273408497,
            "fpr": 0.10307017543859649,
            "logloss": 3.1002846647722873,
            "mae": 0.30565133580050935,
            "precision": 0.7740384615384616,
            "recall": 0.6531440162271805
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7875022611110594,
            "auditor_fn_violation": 0.017877424869812444,
            "auditor_fp_violation": 0.023746798390047567,
            "ave_precision_score": 0.7875256245105736,
            "fpr": 0.11306256860592755,
            "logloss": 2.921899948724762,
            "mae": 0.2925023951978153,
            "precision": 0.745049504950495,
            "recall": 0.6529284164859002
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7869326829083354,
            "auditor_fn_violation": 0.02662716629301449,
            "auditor_fp_violation": 0.02346072520202655,
            "ave_precision_score": 0.7869841913458895,
            "fpr": 0.09978070175438597,
            "logloss": 3.1486594291205865,
            "mae": 0.311739582649726,
            "precision": 0.7725,
            "recall": 0.6267748478701826
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7883804781674018,
            "auditor_fn_violation": 0.020182345923885233,
            "auditor_fp_violation": 0.022832052689352365,
            "ave_precision_score": 0.7883460099544244,
            "fpr": 0.10537870472008781,
            "logloss": 2.971301373813649,
            "mae": 0.29497152189285025,
            "precision": 0.7551020408163265,
            "recall": 0.6420824295010846
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 22571,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7723980109471035,
            "auditor_fn_violation": 0.02301964342905947,
            "auditor_fp_violation": 0.022267407779592182,
            "ave_precision_score": 0.7725770301922428,
            "fpr": 0.1162280701754386,
            "logloss": 3.0216357078276888,
            "mae": 0.3170203827562301,
            "precision": 0.7511737089201878,
            "recall": 0.6490872210953347
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7583418137214931,
            "auditor_fn_violation": 0.018306025892263995,
            "auditor_fp_violation": 0.019792657641175753,
            "ave_precision_score": 0.7595157540811779,
            "fpr": 0.11306256860592755,
            "logloss": 2.827322668829807,
            "mae": 0.3050893434085518,
            "precision": 0.7444168734491315,
            "recall": 0.6507592190889371
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7938717619789417,
            "auditor_fn_violation": 0.02639808191879293,
            "auditor_fp_violation": 0.02513032282376586,
            "ave_precision_score": 0.7941177720973768,
            "fpr": 0.11403508771929824,
            "logloss": 2.287685025779588,
            "mae": 0.30937582919303863,
            "precision": 0.7587006960556845,
            "recall": 0.6632860040567952
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7971828961205498,
            "auditor_fn_violation": 0.020189489274259426,
            "auditor_fp_violation": 0.020475667764361508,
            "ave_precision_score": 0.7973111723512185,
            "fpr": 0.11745334796926454,
            "logloss": 2.1148130434402965,
            "mae": 0.2934264699246943,
            "precision": 0.7470449172576832,
            "recall": 0.6854663774403471
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8014742427338566,
            "auditor_fn_violation": 0.02717874808725669,
            "auditor_fp_violation": 0.018993635640413686,
            "ave_precision_score": 0.8014274836822379,
            "fpr": 0.08771929824561403,
            "logloss": 2.9913225435213135,
            "mae": 0.3028011700618322,
            "precision": 0.797979797979798,
            "recall": 0.640973630831643
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.808607594797757,
            "auditor_fn_violation": 0.020153772522388456,
            "auditor_fp_violation": 0.024920112208805956,
            "ave_precision_score": 0.8084831342054363,
            "fpr": 0.10318331503841932,
            "logloss": 2.7999732114689664,
            "mae": 0.2888232087768056,
            "precision": 0.7614213197969543,
            "recall": 0.6507592190889371
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7872375027795868,
            "auditor_fn_violation": 0.02662716629301449,
            "auditor_fp_violation": 0.02418823012184399,
            "ave_precision_score": 0.7872882323912214,
            "fpr": 0.10087719298245613,
            "logloss": 3.1508441246843613,
            "mae": 0.3113745637252279,
            "precision": 0.770573566084788,
            "recall": 0.6267748478701826
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7888183154767245,
            "auditor_fn_violation": 0.017855994818689866,
            "auditor_fp_violation": 0.023417489937797294,
            "ave_precision_score": 0.7887847467114877,
            "fpr": 0.10428100987925357,
            "logloss": 2.974956166494797,
            "mae": 0.2944864825062401,
            "precision": 0.7564102564102564,
            "recall": 0.6399132321041214
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7942377870693418,
            "auditor_fn_violation": 0.02835753176043558,
            "auditor_fp_violation": 0.024324310178788267,
            "ave_precision_score": 0.7943050639304359,
            "fpr": 0.10855263157894737,
            "logloss": 3.0756777316640753,
            "mae": 0.3039329773855044,
            "precision": 0.7654028436018957,
            "recall": 0.6551724137931034
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8004555306076644,
            "auditor_fn_violation": 0.02153482026139901,
            "auditor_fp_violation": 0.021405049396267837,
            "ave_precision_score": 0.8003687320301882,
            "fpr": 0.11525795828759605,
            "logloss": 2.9017495593745,
            "mae": 0.2890619108360009,
            "precision": 0.7505938242280285,
            "recall": 0.6854663774403471
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7742592967695219,
            "auditor_fn_violation": 0.031173267855236466,
            "auditor_fp_violation": 0.024347862496336307,
            "ave_precision_score": 0.7743209501752548,
            "fpr": 0.1074561403508772,
            "logloss": 3.1155529393370207,
            "mae": 0.32414485050438135,
            "precision": 0.7644230769230769,
            "recall": 0.6450304259634888
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7592083162710286,
            "auditor_fn_violation": 0.01844889289974785,
            "auditor_fp_violation": 0.023988291255031102,
            "ave_precision_score": 0.7595509358287498,
            "fpr": 0.12184412733260154,
            "logloss": 2.963519979897846,
            "mae": 0.3119297188775151,
            "precision": 0.7272727272727273,
            "recall": 0.6420824295010846
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7878834166730377,
            "auditor_fn_violation": 0.025034696274153948,
            "auditor_fp_violation": 0.02199524766570364,
            "ave_precision_score": 0.7880089132516404,
            "fpr": 0.10087719298245613,
            "logloss": 3.0168040134778753,
            "mae": 0.3105381887524805,
            "precision": 0.77,
            "recall": 0.6247464503042597
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7853402695176765,
            "auditor_fn_violation": 0.018358410461674742,
            "auditor_fp_violation": 0.016099524332235642,
            "ave_precision_score": 0.785394483063776,
            "fpr": 0.10537870472008781,
            "logloss": 2.804115897050834,
            "mae": 0.29645901390338114,
            "precision": 0.7557251908396947,
            "recall": 0.6442516268980477
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7895210357353258,
            "auditor_fn_violation": 0.030588324258923185,
            "auditor_fp_violation": 0.025999141648871592,
            "ave_precision_score": 0.789579482753195,
            "fpr": 0.10855263157894737,
            "logloss": 3.1506928124066915,
            "mae": 0.3098888653163177,
            "precision": 0.7654028436018957,
            "recall": 0.6551724137931034
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7950253558013682,
            "auditor_fn_violation": 0.02368734984082235,
            "auditor_fp_violation": 0.02742285644590804,
            "ave_precision_score": 0.7948339838020668,
            "fpr": 0.11306256860592755,
            "logloss": 2.98104262640271,
            "mae": 0.29097799225494503,
            "precision": 0.7475490196078431,
            "recall": 0.6616052060737527
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7944906410622873,
            "auditor_fn_violation": 0.030559410697128225,
            "auditor_fp_violation": 0.024661893396976928,
            "ave_precision_score": 0.794501701783181,
            "fpr": 0.10416666666666667,
            "logloss": 3.0900303131277194,
            "mae": 0.30937230334327614,
            "precision": 0.7682926829268293,
            "recall": 0.6389452332657201
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8100671571735293,
            "auditor_fn_violation": 0.020365691916822835,
            "auditor_fp_violation": 0.022724722527137458,
            "ave_precision_score": 0.8098322820133534,
            "fpr": 0.10318331503841932,
            "logloss": 2.900897235688525,
            "mae": 0.28501955329676515,
            "precision": 0.7661691542288557,
            "recall": 0.6681127982646421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7910006421926987,
            "auditor_fn_violation": 0.028141792107042457,
            "auditor_fp_violation": 0.022615458694468873,
            "ave_precision_score": 0.7910669179960392,
            "fpr": 0.10307017543859649,
            "logloss": 3.06311037181355,
            "mae": 0.3093818538588462,
            "precision": 0.7712895377128953,
            "recall": 0.6430020283975659
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7956067043187135,
            "auditor_fn_violation": 0.021980089101390335,
            "auditor_fp_violation": 0.02065617758263203,
            "ave_precision_score": 0.7955647420750835,
            "fpr": 0.10647639956092206,
            "logloss": 2.8683330535367406,
            "mae": 0.29290520822776195,
            "precision": 0.7544303797468355,
            "recall": 0.6464208242950108
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8228844195688827,
            "auditor_fn_violation": 0.010613501298886168,
            "auditor_fp_violation": 0.01402409663777583,
            "ave_precision_score": 0.8233425101041386,
            "fpr": 0.13048245614035087,
            "logloss": 0.846218250343049,
            "mae": 0.28113703544608004,
            "precision": 0.7531120331950207,
            "recall": 0.7363083164300203
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8267327811183305,
            "auditor_fn_violation": 0.00815056277695365,
            "auditor_fp_violation": 0.0251152579582876,
            "ave_precision_score": 0.8269976951164193,
            "fpr": 0.13830954994511527,
            "logloss": 0.7788485515654471,
            "mae": 0.26866767067198305,
            "precision": 0.738045738045738,
            "recall": 0.7700650759219089
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.787639762161568,
            "auditor_fn_violation": 0.022841713818013604,
            "auditor_fp_violation": 0.023465959050370556,
            "ave_precision_score": 0.7877233535552639,
            "fpr": 0.10087719298245613,
            "logloss": 3.0697964196384344,
            "mae": 0.3119463219242303,
            "precision": 0.77,
            "recall": 0.6247464503042597
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7903159757861571,
            "auditor_fn_violation": 0.01650828271475888,
            "auditor_fp_violation": 0.025466520307354558,
            "ave_precision_score": 0.7903216702063662,
            "fpr": 0.09879253567508232,
            "logloss": 2.8680910644184516,
            "mae": 0.2949380819599996,
            "precision": 0.765625,
            "recall": 0.6377440347071583
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8233640910302411,
            "auditor_fn_violation": 0.0076287320735916875,
            "auditor_fp_violation": 0.017203659506762132,
            "ave_precision_score": 0.823722958875237,
            "fpr": 0.1337719298245614,
            "logloss": 0.869401438160809,
            "mae": 0.2845544748625124,
            "precision": 0.7494866529774127,
            "recall": 0.7403651115618661
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8202962393099865,
            "auditor_fn_violation": 0.008531541463577244,
            "auditor_fp_violation": 0.02689352360043908,
            "ave_precision_score": 0.8206262768055337,
            "fpr": 0.13721185510428102,
            "logloss": 0.798246205024116,
            "mae": 0.26954138571769987,
            "precision": 0.7395833333333334,
            "recall": 0.7700650759219089
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7854595517985214,
            "auditor_fn_violation": 0.02393375680580764,
            "auditor_fp_violation": 0.021388121257798435,
            "ave_precision_score": 0.7855669920281221,
            "fpr": 0.10197368421052631,
            "logloss": 3.013921824346791,
            "mae": 0.308330865115384,
            "precision": 0.7764423076923077,
            "recall": 0.6551724137931034
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7838014589623449,
            "auditor_fn_violation": 0.01863938224305964,
            "auditor_fp_violation": 0.0216977680204903,
            "ave_precision_score": 0.7838416481950488,
            "fpr": 0.11086717892425905,
            "logloss": 2.815748099408308,
            "mae": 0.29506860323282835,
            "precision": 0.7487562189054726,
            "recall": 0.6529284164859002
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7865648632540015,
            "auditor_fn_violation": 0.023838119639870474,
            "auditor_fp_violation": 0.022021416907423687,
            "ave_precision_score": 0.786616919716905,
            "fpr": 0.09758771929824561,
            "logloss": 3.1432839676094773,
            "mae": 0.3123223794264568,
            "precision": 0.7741116751269036,
            "recall": 0.6186612576064908
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.786747358367452,
            "auditor_fn_violation": 0.019463248652883184,
            "auditor_fp_violation": 0.024300524454201732,
            "ave_precision_score": 0.7867686454246032,
            "fpr": 0.09769484083424808,
            "logloss": 2.9589444913560747,
            "mae": 0.2953767859482877,
            "precision": 0.7651715039577837,
            "recall": 0.6290672451193059
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7914799228517697,
            "auditor_fn_violation": 0.024830077221451197,
            "auditor_fp_violation": 0.023565402168906756,
            "ave_precision_score": 0.7916053516167237,
            "fpr": 0.10416666666666667,
            "logloss": 3.0592884602657064,
            "mae": 0.310531063858667,
            "precision": 0.7671568627450981,
            "recall": 0.6348884381338742
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.7921328560082648,
            "auditor_fn_violation": 0.020027573332444384,
            "auditor_fp_violation": 0.024559092572264914,
            "ave_precision_score": 0.7921123198588501,
            "fpr": 0.10537870472008781,
            "logloss": 2.8613671592959067,
            "mae": 0.29354686807908453,
            "precision": 0.7581863979848866,
            "recall": 0.6529284164859002
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7943703872203041,
            "auditor_fn_violation": 0.030559410697128225,
            "auditor_fp_violation": 0.025140790520453885,
            "ave_precision_score": 0.7943822114476139,
            "fpr": 0.10526315789473684,
            "logloss": 3.089608352231406,
            "mae": 0.3093937231751639,
            "precision": 0.7664233576642335,
            "recall": 0.6389452332657201
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8096892213351623,
            "auditor_fn_violation": 0.02072762166911527,
            "auditor_fp_violation": 0.022724722527137458,
            "ave_precision_score": 0.809496075012862,
            "fpr": 0.10318331503841932,
            "logloss": 2.900959312909555,
            "mae": 0.2850473242826691,
            "precision": 0.7667493796526055,
            "recall": 0.6702819956616052
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7931769104785908,
            "auditor_fn_violation": 0.02544171025942138,
            "auditor_fp_violation": 0.02597035548297953,
            "ave_precision_score": 0.79330986077699,
            "fpr": 0.11842105263157894,
            "logloss": 3.008875758775129,
            "mae": 0.30793339584686874,
            "precision": 0.7505773672055427,
            "recall": 0.6592292089249493
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8003974369629641,
            "auditor_fn_violation": 0.0191632279371671,
            "auditor_fp_violation": 0.027013050371996587,
            "ave_precision_score": 0.800304895850361,
            "fpr": 0.11306256860592755,
            "logloss": 2.8090292038483167,
            "mae": 0.2879803522402933,
            "precision": 0.7547619047619047,
            "recall": 0.6876355748373102
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.8153229392858131,
            "auditor_fn_violation": 0.017608359133126937,
            "auditor_fp_violation": 0.0173868441988025,
            "ave_precision_score": 0.815829860792793,
            "fpr": 0.08442982456140351,
            "logloss": 1.1398908853354008,
            "mae": 0.3005341668176191,
            "precision": 0.8015463917525774,
            "recall": 0.6308316430020284
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8058913439750532,
            "auditor_fn_violation": 0.018475085184453215,
            "auditor_fp_violation": 0.02226856933772411,
            "ave_precision_score": 0.80621755775178,
            "fpr": 0.09110867178924259,
            "logloss": 1.020522820514998,
            "mae": 0.2907560660543433,
            "precision": 0.7821522309711286,
            "recall": 0.6464208242950108
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7921210325214966,
            "auditor_fn_violation": 0.026653855734671376,
            "auditor_fp_violation": 0.012919754637189633,
            "ave_precision_score": 0.7921523131546644,
            "fpr": 0.07346491228070176,
            "logloss": 3.1100921416832144,
            "mae": 0.3213567390486152,
            "precision": 0.8085714285714286,
            "recall": 0.5740365111561866
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.7947019045434227,
            "auditor_fn_violation": 0.012774691585847603,
            "auditor_fp_violation": 0.01419441395292109,
            "ave_precision_score": 0.7946618137434056,
            "fpr": 0.0801317233809001,
            "logloss": 2.8899792105123425,
            "mae": 0.29980769866250495,
            "precision": 0.7871720116618076,
            "recall": 0.5856832971800434
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7801287580821235,
            "auditor_fn_violation": 0.02078217857015764,
            "auditor_fp_violation": 0.0217230875518151,
            "ave_precision_score": 0.780297520428406,
            "fpr": 0.09868421052631579,
            "logloss": 3.129178454903263,
            "mae": 0.30983912444159234,
            "precision": 0.7788697788697788,
            "recall": 0.6430020283975659
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7778577956140195,
            "auditor_fn_violation": 0.01939657738272406,
            "auditor_fp_violation": 0.01717282595438468,
            "ave_precision_score": 0.7779115082394686,
            "fpr": 0.10318331503841932,
            "logloss": 2.9808345534228127,
            "mae": 0.2983528414294725,
            "precision": 0.7589743589743589,
            "recall": 0.6420824295010846
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.787789697626816,
            "auditor_fn_violation": 0.02848653072844383,
            "auditor_fp_violation": 0.023094355817945823,
            "ave_precision_score": 0.7878753120612033,
            "fpr": 0.10197368421052631,
            "logloss": 3.116817292823053,
            "mae": 0.31158437040876724,
            "precision": 0.7692307692307693,
            "recall": 0.6288032454361054
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7841544953617623,
            "auditor_fn_violation": 0.018941784075567125,
            "auditor_fp_violation": 0.023193072325893408,
            "ave_precision_score": 0.7842922994875148,
            "fpr": 0.10647639956092206,
            "logloss": 2.937890037380122,
            "mae": 0.2958132998245235,
            "precision": 0.7544303797468355,
            "recall": 0.6464208242950108
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7866741489661757,
            "auditor_fn_violation": 0.02701193907690119,
            "auditor_fp_violation": 0.023465959050370556,
            "ave_precision_score": 0.7867103363792751,
            "fpr": 0.10087719298245613,
            "logloss": 3.190363988992582,
            "mae": 0.3119323274653166,
            "precision": 0.7694235588972431,
            "recall": 0.6227180527383367
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7894343692424362,
            "auditor_fn_violation": 0.01650828271475888,
            "auditor_fp_violation": 0.025466520307354558,
            "ave_precision_score": 0.7890279954935098,
            "fpr": 0.09879253567508232,
            "logloss": 3.0173685140091804,
            "mae": 0.2952764953622585,
            "precision": 0.765625,
            "recall": 0.6377440347071583
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 22571,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8299017070665148,
            "auditor_fn_violation": 0.004726255293405934,
            "auditor_fp_violation": 0.01865866934639702,
            "ave_precision_score": 0.8302620218446111,
            "fpr": 0.13815789473684212,
            "logloss": 0.8245247604226705,
            "mae": 0.28045705263412946,
            "precision": 0.748,
            "recall": 0.7586206896551724
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8312576643707095,
            "auditor_fn_violation": 0.008517254762828863,
            "auditor_fp_violation": 0.02641053787047201,
            "ave_precision_score": 0.8315096892613486,
            "fpr": 0.150384193194292,
            "logloss": 0.7666294346798501,
            "mae": 0.26790146959756306,
            "precision": 0.7248995983935743,
            "recall": 0.7830802603036876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7700503160368891,
            "auditor_fn_violation": 0.031086527169851615,
            "auditor_fp_violation": 0.025439119876062476,
            "ave_precision_score": 0.7702090799555716,
            "fpr": 0.11842105263157894,
            "logloss": 2.8827611448853454,
            "mae": 0.3312588542832578,
            "precision": 0.7452830188679245,
            "recall": 0.640973630831643
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7749714792864848,
            "auditor_fn_violation": 0.01989184967533473,
            "auditor_fp_violation": 0.030086595926332484,
            "ave_precision_score": 0.7750328598017573,
            "fpr": 0.1163556531284303,
            "logloss": 2.661348839853739,
            "mae": 0.31090791805728557,
            "precision": 0.7389162561576355,
            "recall": 0.6507592190889371
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7875842483302079,
            "auditor_fn_violation": 0.029776520408526384,
            "auditor_fp_violation": 0.022359000125612358,
            "ave_precision_score": 0.7876829523561009,
            "fpr": 0.10964912280701754,
            "logloss": 2.97459043169338,
            "mae": 0.31247747882644156,
            "precision": 0.7630331753554502,
            "recall": 0.6531440162271805
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7894978272403264,
            "auditor_fn_violation": 0.02000376216453041,
            "auditor_fp_violation": 0.02621783144285889,
            "ave_precision_score": 0.7895210911212454,
            "fpr": 0.11745334796926454,
            "logloss": 2.745688865244806,
            "mae": 0.2942731804645044,
            "precision": 0.7415458937198067,
            "recall": 0.665943600867679
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7925819952402834,
            "auditor_fn_violation": 0.02824410163339384,
            "auditor_fp_violation": 0.024551982581752712,
            "ave_precision_score": 0.7926102565621254,
            "fpr": 0.10964912280701754,
            "logloss": 3.1061829574869866,
            "mae": 0.30975649474115163,
            "precision": 0.7590361445783133,
            "recall": 0.6389452332657201
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.8023037717903544,
            "auditor_fn_violation": 0.022915868000409557,
            "auditor_fp_violation": 0.0242371020856202,
            "ave_precision_score": 0.8021484603705439,
            "fpr": 0.10867178924259056,
            "logloss": 2.9244962793409637,
            "mae": 0.28859304013761333,
            "precision": 0.7567567567567568,
            "recall": 0.6681127982646421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7869872765446932,
            "auditor_fn_violation": 0.02662716629301449,
            "auditor_fp_violation": 0.02346072520202655,
            "ave_precision_score": 0.7870389399190263,
            "fpr": 0.09978070175438597,
            "logloss": 3.1438847390868685,
            "mae": 0.3117137476855291,
            "precision": 0.7725,
            "recall": 0.6267748478701826
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7884813781959541,
            "auditor_fn_violation": 0.020182345923885233,
            "auditor_fp_violation": 0.02396877668008294,
            "ave_precision_score": 0.7884411561578006,
            "fpr": 0.10647639956092206,
            "logloss": 2.965976585072131,
            "mae": 0.2949100126849755,
            "precision": 0.7531806615776081,
            "recall": 0.6420824295010846
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7823891898262877,
            "auditor_fn_violation": 0.024467545638945237,
            "auditor_fp_violation": 0.020647531717120967,
            "ave_precision_score": 0.7825609650771144,
            "fpr": 0.09978070175438597,
            "logloss": 3.1394534231685576,
            "mae": 0.31143770059275816,
            "precision": 0.7758620689655172,
            "recall": 0.6389452332657201
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7818210146893142,
            "auditor_fn_violation": 0.020075195668272337,
            "auditor_fp_violation": 0.02008781558726674,
            "ave_precision_score": 0.7819011530724332,
            "fpr": 0.10208562019758508,
            "logloss": 2.9553450157093324,
            "mae": 0.29553030313999307,
            "precision": 0.7627551020408163,
            "recall": 0.648590021691974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7878025468125063,
            "auditor_fn_violation": 0.02848653072844383,
            "auditor_fp_violation": 0.023332495917598294,
            "ave_precision_score": 0.7878881936468128,
            "fpr": 0.10307017543859649,
            "logloss": 3.1132155128249064,
            "mae": 0.3116836952485932,
            "precision": 0.7673267326732673,
            "recall": 0.6288032454361054
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7846145315096066,
            "auditor_fn_violation": 0.01930371382785955,
            "auditor_fp_violation": 0.0242371020856202,
            "ave_precision_score": 0.7847492040202753,
            "fpr": 0.10867178924259056,
            "logloss": 2.9318191019878688,
            "mae": 0.2958387778662819,
            "precision": 0.7512562814070352,
            "recall": 0.648590021691974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 22571,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7925425738627204,
            "auditor_fn_violation": 0.02750569374755347,
            "auditor_fp_violation": 0.022021416907423687,
            "ave_precision_score": 0.7925832804955946,
            "fpr": 0.09758771929824561,
            "logloss": 3.1358905926694773,
            "mae": 0.3062386422871414,
            "precision": 0.7791563275434243,
            "recall": 0.6369168356997972
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7959862884282293,
            "auditor_fn_violation": 0.021203845027394756,
            "auditor_fp_violation": 0.025905598243688256,
            "ave_precision_score": 0.7958914296600998,
            "fpr": 0.10428100987925357,
            "logloss": 2.956969116384737,
            "mae": 0.29067233096868794,
            "precision": 0.7582697201017812,
            "recall": 0.6464208242950108
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 22571,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7915976666546536,
            "auditor_fn_violation": 0.022319045585566345,
            "auditor_fp_violation": 0.01790761210903153,
            "ave_precision_score": 0.7916459267548688,
            "fpr": 0.08662280701754387,
            "logloss": 3.006904597661731,
            "mae": 0.31799938246734827,
            "precision": 0.7876344086021505,
            "recall": 0.5943204868154158
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7934764847587992,
            "auditor_fn_violation": 0.013689040433744235,
            "auditor_fp_violation": 0.021153799243810226,
            "ave_precision_score": 0.7934307989396832,
            "fpr": 0.0867178924259056,
            "logloss": 2.7633903377937648,
            "mae": 0.29808270593303987,
            "precision": 0.7787114845938375,
            "recall": 0.6030368763557483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8236260274261368,
            "auditor_fn_violation": 0.0072773210917760995,
            "auditor_fp_violation": 0.01562042038269899,
            "ave_precision_score": 0.8242755162122622,
            "fpr": 0.15460526315789475,
            "logloss": 0.8684968200842108,
            "mae": 0.28240325344399064,
            "precision": 0.7293666026871402,
            "recall": 0.77079107505071
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8172714056861486,
            "auditor_fn_violation": 0.007724342871293494,
            "auditor_fp_violation": 0.02401756311745335,
            "ave_precision_score": 0.8175725236576227,
            "fpr": 0.15806805708013172,
            "logloss": 0.837854447439469,
            "mae": 0.27419465152528655,
            "precision": 0.7209302325581395,
            "recall": 0.806941431670282
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7927317059718308,
            "auditor_fn_violation": 0.03145795523290986,
            "auditor_fp_violation": 0.02445777331156053,
            "ave_precision_score": 0.7927555777503064,
            "fpr": 0.10416666666666667,
            "logloss": 3.181985102381784,
            "mae": 0.3097214107042743,
            "precision": 0.7699757869249395,
            "recall": 0.6450304259634888
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8023741418957937,
            "auditor_fn_violation": 0.02219677072940751,
            "auditor_fp_violation": 0.0189901207464325,
            "ave_precision_score": 0.802118636327804,
            "fpr": 0.10208562019758508,
            "logloss": 3.0150692525609206,
            "mae": 0.28750067346930364,
            "precision": 0.7669172932330827,
            "recall": 0.6637744034707158
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7944363864640054,
            "auditor_fn_violation": 0.029834347532116307,
            "auditor_fp_violation": 0.025648473809822894,
            "ave_precision_score": 0.7944916100267083,
            "fpr": 0.10964912280701754,
            "logloss": 3.0210515775476576,
            "mae": 0.30816068551464754,
            "precision": 0.7641509433962265,
            "recall": 0.6572008113590264
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8051137195767131,
            "auditor_fn_violation": 0.0211609849251496,
            "auditor_fp_violation": 0.02349554823758995,
            "ave_precision_score": 0.8050127124454759,
            "fpr": 0.11306256860592755,
            "logloss": 2.819173070484845,
            "mae": 0.2864039190272592,
            "precision": 0.7535885167464115,
            "recall": 0.6832971800433839
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 22571,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7918776480390073,
            "auditor_fn_violation": 0.028602184975623644,
            "auditor_fp_violation": 0.024551982581752712,
            "ave_precision_score": 0.7919050197464144,
            "fpr": 0.10964912280701754,
            "logloss": 3.112419707344,
            "mae": 0.31000454397132704,
            "precision": 0.7584541062801933,
            "recall": 0.6369168356997972
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.8012531690925235,
            "auditor_fn_violation": 0.022915868000409557,
            "auditor_fp_violation": 0.0242371020856202,
            "ave_precision_score": 0.8011073327218032,
            "fpr": 0.10867178924259056,
            "logloss": 2.931599658189568,
            "mae": 0.28911815592960366,
            "precision": 0.7567567567567568,
            "recall": 0.6681127982646421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 22571,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.785397681489617,
            "auditor_fn_violation": 0.023348813209494338,
            "auditor_fp_violation": 0.022481995561696607,
            "ave_precision_score": 0.785532613055502,
            "fpr": 0.10307017543859649,
            "logloss": 3.008206745376873,
            "mae": 0.3103210795725932,
            "precision": 0.7723970944309927,
            "recall": 0.6470588235294118
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7853667584689069,
            "auditor_fn_violation": 0.015636793969107398,
            "auditor_fp_violation": 0.025232345407976583,
            "ave_precision_score": 0.7854064539760361,
            "fpr": 0.10208562019758508,
            "logloss": 2.7759916507110756,
            "mae": 0.2938660389030216,
            "precision": 0.7639593908629442,
            "recall": 0.6529284164859002
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7877032664482284,
            "auditor_fn_violation": 0.022950695704779193,
            "auditor_fp_violation": 0.021293911987606245,
            "ave_precision_score": 0.7877928760740996,
            "fpr": 0.10855263157894737,
            "logloss": 3.133011870464089,
            "mae": 0.3086669046883596,
            "precision": 0.7654028436018957,
            "recall": 0.6551724137931034
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.7870204163670703,
            "auditor_fn_violation": 0.01964183241223799,
            "auditor_fp_violation": 0.015943407732650326,
            "ave_precision_score": 0.7870363568876038,
            "fpr": 0.10647639956092206,
            "logloss": 2.9609817018677944,
            "mae": 0.2925778334245061,
            "precision": 0.7599009900990099,
            "recall": 0.665943600867679
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7942480121652444,
            "auditor_fn_violation": 0.026911853670687876,
            "auditor_fp_violation": 0.026258217141900103,
            "ave_precision_score": 0.7943106188674496,
            "fpr": 0.10855263157894737,
            "logloss": 2.928682461926677,
            "mae": 0.30869387784466645,
            "precision": 0.7602905569007264,
            "recall": 0.6369168356997972
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8049567560701002,
            "auditor_fn_violation": 0.020327594048160473,
            "auditor_fp_violation": 0.016977680204903037,
            "ave_precision_score": 0.8048417783232275,
            "fpr": 0.10537870472008781,
            "logloss": 2.6942050334659653,
            "mae": 0.2873658921651367,
            "precision": 0.7647058823529411,
            "recall": 0.6767895878524945
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.8154325654359607,
            "auditor_fn_violation": 0.01833787053841502,
            "auditor_fp_violation": 0.019684503621823057,
            "ave_precision_score": 0.8158569037048375,
            "fpr": 0.09868421052631579,
            "logloss": 1.2542609029737015,
            "mae": 0.29440128132293664,
            "precision": 0.7820823244552058,
            "recall": 0.6551724137931034
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8120864391253221,
            "auditor_fn_violation": 0.015927290217657895,
            "auditor_fp_violation": 0.022280765947066718,
            "ave_precision_score": 0.8123818590650624,
            "fpr": 0.10757409440175632,
            "logloss": 1.138308388534401,
            "mae": 0.2848445466680558,
            "precision": 0.7655502392344498,
            "recall": 0.6941431670281996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 22571,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7338070954290592,
            "auditor_fn_violation": 0.0366179139532401,
            "auditor_fp_violation": 0.03409852196122765,
            "ave_precision_score": 0.7244943539105152,
            "fpr": 0.1524122807017544,
            "logloss": 2.9612384684267865,
            "mae": 0.30118067865394,
            "precision": 0.7236580516898609,
            "recall": 0.7383367139959433
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7163595388163124,
            "auditor_fn_violation": 0.03148788844944056,
            "auditor_fp_violation": 0.032882058787657036,
            "ave_precision_score": 0.70409082815878,
            "fpr": 0.15367727771679474,
            "logloss": 2.9958118991403637,
            "mae": 0.28824033196644544,
            "precision": 0.714867617107943,
            "recall": 0.7613882863340564
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7944208864006594,
            "auditor_fn_violation": 0.030559410697128225,
            "auditor_fp_violation": 0.025030879705229662,
            "ave_precision_score": 0.7944324918861914,
            "fpr": 0.10635964912280702,
            "logloss": 3.0895084502254764,
            "mae": 0.30939833017587676,
            "precision": 0.7645631067961165,
            "recall": 0.6389452332657201
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8096518608012573,
            "auditor_fn_violation": 0.020365691916822835,
            "auditor_fp_violation": 0.022724722527137458,
            "ave_precision_score": 0.8094587486897586,
            "fpr": 0.10318331503841932,
            "logloss": 2.900702045693277,
            "mae": 0.28506095058273473,
            "precision": 0.7661691542288557,
            "recall": 0.6681127982646421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7849814965821567,
            "auditor_fn_violation": 0.02356232874274938,
            "auditor_fp_violation": 0.01107744002009798,
            "ave_precision_score": 0.7850676765709352,
            "fpr": 0.07346491228070176,
            "logloss": 3.117167810666575,
            "mae": 0.32594731683714223,
            "precision": 0.8046647230320699,
            "recall": 0.5598377281947262
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7815129731747235,
            "auditor_fn_violation": 0.013391400834819555,
            "auditor_fp_violation": 0.01113794365166484,
            "ave_precision_score": 0.7815323438111422,
            "fpr": 0.0801317233809001,
            "logloss": 2.894261598365412,
            "mae": 0.3056536075037496,
            "precision": 0.7833827893175074,
            "recall": 0.5726681127982647
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.81000882900618,
            "auditor_fn_violation": 0.02679397530336999,
            "auditor_fp_violation": 0.021754490641879162,
            "ave_precision_score": 0.8098158727554147,
            "fpr": 0.11293859649122807,
            "logloss": 2.343589464115303,
            "mae": 0.2898746590668541,
            "precision": 0.7680180180180181,
            "recall": 0.691683569979716
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.803512736631232,
            "auditor_fn_violation": 0.02693281202749714,
            "auditor_fp_violation": 0.027976582510062204,
            "ave_precision_score": 0.8033214148470808,
            "fpr": 0.12403951701427003,
            "logloss": 2.1721766390381223,
            "mae": 0.2728907157524647,
            "precision": 0.7488888888888889,
            "recall": 0.7310195227765727
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8337848028852574,
            "auditor_fn_violation": 0.00575824703747198,
            "auditor_fp_violation": 0.01172643721475527,
            "ave_precision_score": 0.8341319641741687,
            "fpr": 0.09978070175438597,
            "logloss": 0.9601246179192419,
            "mae": 0.2740190314597012,
            "precision": 0.7912844036697247,
            "recall": 0.6997971602434077
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8191341548041525,
            "auditor_fn_violation": 0.003881220369977935,
            "auditor_fp_violation": 0.01541163556531285,
            "ave_precision_score": 0.8194791494409898,
            "fpr": 0.11855104281009879,
            "logloss": 0.8887566844448647,
            "mae": 0.2657820061955027,
            "precision": 0.755656108597285,
            "recall": 0.7245119305856833
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7958237335742088,
            "auditor_fn_violation": 0.02582425892317001,
            "auditor_fp_violation": 0.02090922413432149,
            "ave_precision_score": 0.7958219310692658,
            "fpr": 0.08662280701754387,
            "logloss": 3.0440395558914055,
            "mae": 0.3161520930359034,
            "precision": 0.7887700534759359,
            "recall": 0.5983772819472617
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8021087251662888,
            "auditor_fn_violation": 0.020087101252229324,
            "auditor_fp_violation": 0.013860226856933772,
            "ave_precision_score": 0.8020312680130037,
            "fpr": 0.08562019758507135,
            "logloss": 2.8282708784280857,
            "mae": 0.2938617962473339,
            "precision": 0.7857142857142857,
            "recall": 0.6203904555314533
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7857255282437103,
            "auditor_fn_violation": 0.022292356143909474,
            "auditor_fp_violation": 0.023599422183142822,
            "ave_precision_score": 0.785820523054555,
            "fpr": 0.09868421052631579,
            "logloss": 3.0831598904384165,
            "mae": 0.31105555513096816,
            "precision": 0.7738693467336684,
            "recall": 0.6247464503042597
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7853744499883777,
            "auditor_fn_violation": 0.01684402018234593,
            "auditor_fp_violation": 0.023354067569215757,
            "ave_precision_score": 0.7854326714147862,
            "fpr": 0.09989023051591657,
            "logloss": 2.8852545669980256,
            "mae": 0.2955773859007263,
            "precision": 0.762402088772846,
            "recall": 0.6334056399132321
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 22571,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7868175239173316,
            "auditor_fn_violation": 0.02154282765737875,
            "auditor_fp_violation": 0.022481995561696607,
            "ave_precision_score": 0.7869268100756319,
            "fpr": 0.10307017543859649,
            "logloss": 2.9265519649277323,
            "mae": 0.30814053615951326,
            "precision": 0.7723970944309927,
            "recall": 0.6470588235294118
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7851157491382643,
            "auditor_fn_violation": 0.016070157225141744,
            "auditor_fp_violation": 0.023139407244785953,
            "ave_precision_score": 0.7851685875138477,
            "fpr": 0.10867178924259056,
            "logloss": 2.6993933172802413,
            "mae": 0.29503968918437734,
            "precision": 0.7512562814070352,
            "recall": 0.648590021691974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7851257101148512,
            "auditor_fn_violation": 0.025971050852282837,
            "auditor_fp_violation": 0.022861449566637358,
            "ave_precision_score": 0.7852528920174067,
            "fpr": 0.10307017543859649,
            "logloss": 3.085595471660549,
            "mae": 0.31373071833123806,
            "precision": 0.7667493796526055,
            "recall": 0.6267748478701826
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7824360081442103,
            "auditor_fn_violation": 0.01879891706808328,
            "auditor_fp_violation": 0.025495792169776807,
            "ave_precision_score": 0.7825441112044973,
            "fpr": 0.10208562019758508,
            "logloss": 2.8952367883701133,
            "mae": 0.29467571236658874,
            "precision": 0.7596899224806202,
            "recall": 0.6377440347071583
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.792584830438186,
            "auditor_fn_violation": 0.02824410163339384,
            "auditor_fp_violation": 0.024551982581752712,
            "ave_precision_score": 0.7926122423577937,
            "fpr": 0.10964912280701754,
            "logloss": 3.1060793813368313,
            "mae": 0.30975565310100683,
            "precision": 0.7590361445783133,
            "recall": 0.6389452332657201
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.8023235999474856,
            "auditor_fn_violation": 0.022915868000409557,
            "auditor_fp_violation": 0.0242371020856202,
            "ave_precision_score": 0.8021681749801304,
            "fpr": 0.10867178924259056,
            "logloss": 2.9243591578874892,
            "mae": 0.28858963526487197,
            "precision": 0.7567567567567568,
            "recall": 0.6681127982646421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7850377356120416,
            "auditor_fn_violation": 0.02476780185758515,
            "auditor_fp_violation": 0.02137241971276641,
            "ave_precision_score": 0.7851778803160492,
            "fpr": 0.10416666666666667,
            "logloss": 3.031179079149017,
            "mae": 0.31027603052154934,
            "precision": 0.7705314009661836,
            "recall": 0.6470588235294118
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7849991386711213,
            "auditor_fn_violation": 0.018917972907653152,
            "auditor_fp_violation": 0.019941456275155506,
            "ave_precision_score": 0.7850679574053077,
            "fpr": 0.10428100987925357,
            "logloss": 2.8146126491630734,
            "mae": 0.2944650700281153,
            "precision": 0.7619047619047619,
            "recall": 0.6594360086767896
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8298860104614879,
            "auditor_fn_violation": 0.007054909077968755,
            "auditor_fp_violation": 0.016852991667713438,
            "ave_precision_score": 0.8302367749571222,
            "fpr": 0.12938596491228072,
            "logloss": 0.876559734892076,
            "mae": 0.28047563604100784,
            "precision": 0.7536534446764092,
            "recall": 0.7322515212981744
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8254480120877696,
            "auditor_fn_violation": 0.010103078545899605,
            "auditor_fp_violation": 0.025949506037321626,
            "ave_precision_score": 0.825745183980333,
            "fpr": 0.14050493962678376,
            "logloss": 0.8082038457873439,
            "mae": 0.2678226298034407,
            "precision": 0.7344398340248963,
            "recall": 0.7678958785249458
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.7236031626053986,
            "auditor_fn_violation": 0.030830753353973173,
            "auditor_fp_violation": 0.034224134321483905,
            "ave_precision_score": 0.7048607368268118,
            "fpr": 0.14364035087719298,
            "logloss": 4.113646127864924,
            "mae": 0.33393949238301357,
            "precision": 0.7088888888888889,
            "recall": 0.6470588235294118
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7082158297517881,
            "auditor_fn_violation": 0.03560483938176685,
            "auditor_fp_violation": 0.03396511769728015,
            "ave_precision_score": 0.686555133901507,
            "fpr": 0.14050493962678376,
            "logloss": 4.300826646318377,
            "mae": 0.32438618474044445,
            "precision": 0.7050691244239631,
            "recall": 0.6637744034707158
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.7519534077634408,
            "auditor_fn_violation": 0.028293032276431457,
            "auditor_fp_violation": 0.018239961478876195,
            "ave_precision_score": 0.7525157055297989,
            "fpr": 0.06798245614035088,
            "logloss": 3.3903396795891454,
            "mae": 0.39088198951215236,
            "precision": 0.7720588235294118,
            "recall": 0.4259634888438134
        },
        "train": {
            "accuracy": 0.6695938529088913,
            "auc_prc": 0.7727758783008414,
            "auditor_fn_violation": 0.02985444233054188,
            "auditor_fp_violation": 0.019041346505671426,
            "ave_precision_score": 0.7732998193957705,
            "fpr": 0.06256860592755215,
            "logloss": 3.437518353733917,
            "mae": 0.3509808097414499,
            "precision": 0.791970802919708,
            "recall": 0.47071583514099785
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 22571,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.792863487896196,
            "auditor_fn_violation": 0.028204067470908516,
            "auditor_fp_violation": 0.024551982581752712,
            "ave_precision_score": 0.7928861338984976,
            "fpr": 0.10964912280701754,
            "logloss": 3.106657949442706,
            "mae": 0.3096346192630761,
            "precision": 0.7584541062801933,
            "recall": 0.6369168356997972
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.80217053818804,
            "auditor_fn_violation": 0.022915868000409557,
            "auditor_fp_violation": 0.0242371020856202,
            "ave_precision_score": 0.8020157270174093,
            "fpr": 0.10867178924259056,
            "logloss": 2.926037540465771,
            "mae": 0.2888753065629647,
            "precision": 0.7567567567567568,
            "recall": 0.6681127982646421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7828142589529854,
            "auditor_fn_violation": 0.023382175011565417,
            "auditor_fp_violation": 0.022864066490809364,
            "ave_precision_score": 0.782942527506453,
            "fpr": 0.10416666666666667,
            "logloss": 3.078652336677643,
            "mae": 0.3083958540444781,
            "precision": 0.7688564476885644,
            "recall": 0.640973630831643
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7794606949528302,
            "auditor_fn_violation": 0.016651149722242728,
            "auditor_fp_violation": 0.016099524332235642,
            "ave_precision_score": 0.7795197966441844,
            "fpr": 0.10976948408342481,
            "logloss": 2.8995677335819527,
            "mae": 0.29714276111788773,
            "precision": 0.75,
            "recall": 0.6507592190889371
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.8045908352306281,
            "auditor_fn_violation": 0.026088929219600734,
            "auditor_fp_violation": 0.01775583050705523,
            "ave_precision_score": 0.804569723567708,
            "fpr": 0.08333333333333333,
            "logloss": 3.046975961288352,
            "mae": 0.3083541846448387,
            "precision": 0.8010471204188482,
            "recall": 0.6206896551724138
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8104135487892573,
            "auditor_fn_violation": 0.019179895754706883,
            "auditor_fp_violation": 0.0201536772777168,
            "ave_precision_score": 0.8103073299497145,
            "fpr": 0.09001097694840834,
            "logloss": 2.8194140674047086,
            "mae": 0.28892678343766415,
            "precision": 0.7795698924731183,
            "recall": 0.6290672451193059
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.7839605055746668,
            "auditor_fn_violation": 0.023631276467029644,
            "auditor_fp_violation": 0.021388121257798435,
            "ave_precision_score": 0.7840741129881199,
            "fpr": 0.10197368421052631,
            "logloss": 3.0068116078501004,
            "mae": 0.30957120543687155,
            "precision": 0.7769784172661871,
            "recall": 0.6572008113590264
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7822232920147689,
            "auditor_fn_violation": 0.017958382840719968,
            "auditor_fp_violation": 0.0216977680204903,
            "ave_precision_score": 0.7822716918905133,
            "fpr": 0.11086717892425905,
            "logloss": 2.810420999886618,
            "mae": 0.2962400004613928,
            "precision": 0.7481296758104738,
            "recall": 0.6507592190889371
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7999696013459001,
            "auditor_fn_violation": 0.02543058965873101,
            "auditor_fp_violation": 0.018792132479169284,
            "ave_precision_score": 0.8000530572669939,
            "fpr": 0.09320175438596491,
            "logloss": 2.9284469418533265,
            "mae": 0.3064824036887382,
            "precision": 0.7842639593908629,
            "recall": 0.6267748478701826
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.8040938757714033,
            "auditor_fn_violation": 0.01855604315536073,
            "auditor_fp_violation": 0.024759116965483594,
            "ave_precision_score": 0.8039649585858823,
            "fpr": 0.10976948408342481,
            "logloss": 2.714217401114869,
            "mae": 0.2931799524777661,
            "precision": 0.7518610421836228,
            "recall": 0.6572668112798264
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8088878150023225,
            "auditor_fn_violation": 0.025864293085655323,
            "auditor_fp_violation": 0.03248387974710045,
            "ave_precision_score": 0.8101756092133177,
            "fpr": 0.12171052631578948,
            "logloss": 1.8537500851268909,
            "mae": 0.2910686615956787,
            "precision": 0.7581699346405228,
            "recall": 0.7058823529411765
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.809795922829458,
            "auditor_fn_violation": 0.02246583692683542,
            "auditor_fp_violation": 0.027198438834004152,
            "ave_precision_score": 0.8100557062115343,
            "fpr": 0.132821075740944,
            "logloss": 1.729573190475949,
            "mae": 0.27886003785007957,
            "precision": 0.737527114967462,
            "recall": 0.737527114967462
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.762180793632091,
            "auditor_fn_violation": 0.025172591722714497,
            "auditor_fp_violation": 0.025203596700582007,
            "ave_precision_score": 0.7623831614112143,
            "fpr": 0.11293859649122807,
            "logloss": 3.0309965162130474,
            "mae": 0.33072578237922323,
            "precision": 0.7493917274939172,
            "recall": 0.6247464503042597
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7562573726747974,
            "auditor_fn_violation": 0.01827745249076722,
            "auditor_fp_violation": 0.026344676180021957,
            "ave_precision_score": 0.7567227170151696,
            "fpr": 0.11525795828759605,
            "logloss": 2.8147156767989325,
            "mae": 0.3127995842543445,
            "precision": 0.7341772151898734,
            "recall": 0.6290672451193059
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7869130546949529,
            "auditor_fn_violation": 0.02662716629301449,
            "auditor_fp_violation": 0.02346072520202655,
            "ave_precision_score": 0.7869644257908635,
            "fpr": 0.09978070175438597,
            "logloss": 3.14997354746112,
            "mae": 0.31168038735048215,
            "precision": 0.7725,
            "recall": 0.6267748478701826
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7884096525601412,
            "auditor_fn_violation": 0.020182345923885233,
            "auditor_fp_violation": 0.022832052689352365,
            "ave_precision_score": 0.7883753555930572,
            "fpr": 0.10537870472008781,
            "logloss": 2.972331230095181,
            "mae": 0.29489557070930217,
            "precision": 0.7551020408163265,
            "recall": 0.6420824295010846
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7823191162216054,
            "auditor_fn_violation": 0.021854204476709015,
            "auditor_fp_violation": 0.019804882133735293,
            "ave_precision_score": 0.7824483017330653,
            "fpr": 0.10197368421052631,
            "logloss": 3.122491446849568,
            "mae": 0.3109372909746275,
            "precision": 0.770935960591133,
            "recall": 0.6348884381338742
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7740887189016105,
            "auditor_fn_violation": 0.017896473804143626,
            "auditor_fp_violation": 0.021844127332601534,
            "ave_precision_score": 0.7742319785631095,
            "fpr": 0.10867178924259056,
            "logloss": 2.9666475318552443,
            "mae": 0.29763336080495095,
            "precision": 0.753731343283582,
            "recall": 0.6572668112798264
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8101235967221525,
            "auditor_fn_violation": 0.007566456709725634,
            "auditor_fp_violation": 0.017426098061382574,
            "ave_precision_score": 0.8105891931324287,
            "fpr": 0.1337719298245614,
            "logloss": 1.0415685986265524,
            "mae": 0.2917764632585196,
            "precision": 0.7489711934156379,
            "recall": 0.7383367139959433
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8064384308890089,
            "auditor_fn_violation": 0.008807751011379358,
            "auditor_fp_violation": 0.024690815953165024,
            "ave_precision_score": 0.8067675276950691,
            "fpr": 0.1394072447859495,
            "logloss": 0.9347067202307647,
            "mae": 0.27015838415028004,
            "precision": 0.7365145228215768,
            "recall": 0.7700650759219089
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7907934330112898,
            "auditor_fn_violation": 0.02661159745204797,
            "auditor_fp_violation": 0.022481995561696607,
            "ave_precision_score": 0.7909448524189079,
            "fpr": 0.10307017543859649,
            "logloss": 2.976393524557477,
            "mae": 0.30893379131754234,
            "precision": 0.7696078431372549,
            "recall": 0.6369168356997972
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.792267861777548,
            "auditor_fn_violation": 0.020227587142921777,
            "auditor_fp_violation": 0.018075375045737287,
            "ave_precision_score": 0.7922391907592483,
            "fpr": 0.10757409440175632,
            "logloss": 2.744518852576502,
            "mae": 0.2944377031130718,
            "precision": 0.7525252525252525,
            "recall": 0.6464208242950108
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7852573361418322,
            "auditor_fn_violation": 0.0272699370129177,
            "auditor_fp_violation": 0.021631495205794923,
            "ave_precision_score": 0.7853758429553258,
            "fpr": 0.10307017543859649,
            "logloss": 3.1521412935060575,
            "mae": 0.3101076434563724,
            "precision": 0.7701711491442543,
            "recall": 0.6389452332657201
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.7834322248883339,
            "auditor_fn_violation": 0.017748844563076988,
            "auditor_fp_violation": 0.01694109037687523,
            "ave_precision_score": 0.7834263030574518,
            "fpr": 0.10428100987925357,
            "logloss": 2.9899959263418925,
            "mae": 0.29549320851685795,
            "precision": 0.759493670886076,
            "recall": 0.6507592190889371
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7822330254321085,
            "auditor_fn_violation": 0.021627344222625543,
            "auditor_fp_violation": 0.022727986433865092,
            "ave_precision_score": 0.7823235102832691,
            "fpr": 0.10307017543859649,
            "logloss": 3.173018412954883,
            "mae": 0.3093684365566768,
            "precision": 0.7745803357314148,
            "recall": 0.6551724137931034
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.7790505133452783,
            "auditor_fn_violation": 0.016424943627059974,
            "auditor_fp_violation": 0.022502744237102086,
            "ave_precision_score": 0.7789939395244132,
            "fpr": 0.11525795828759605,
            "logloss": 3.0284951086611422,
            "mae": 0.29618357813622004,
            "precision": 0.7407407407407407,
            "recall": 0.6507592190889371
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7793526092029919,
            "auditor_fn_violation": 0.022924006263122316,
            "auditor_fp_violation": 0.01955104048905079,
            "ave_precision_score": 0.7794964328110566,
            "fpr": 0.09978070175438597,
            "logloss": 3.2013784302242545,
            "mae": 0.3104327367912835,
            "precision": 0.7753086419753087,
            "recall": 0.6369168356997972
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7760897521183789,
            "auditor_fn_violation": 0.01808220091387263,
            "auditor_fp_violation": 0.017138675448225397,
            "ave_precision_score": 0.7760796126106534,
            "fpr": 0.10318331503841932,
            "logloss": 3.0528580049572036,
            "mae": 0.29694437070427904,
            "precision": 0.7577319587628866,
            "recall": 0.6377440347071583
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7861617801496908,
            "auditor_fn_violation": 0.025079178676915417,
            "auditor_fp_violation": 0.02297136038186158,
            "ave_precision_score": 0.7862570256135291,
            "fpr": 0.10197368421052631,
            "logloss": 3.1167202218533103,
            "mae": 0.30995420186660183,
            "precision": 0.7698019801980198,
            "recall": 0.6308316430020284
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7864377935857438,
            "auditor_fn_violation": 0.020332356281743274,
            "auditor_fp_violation": 0.018455909257226493,
            "ave_precision_score": 0.786455594527983,
            "fpr": 0.10537870472008781,
            "logloss": 2.9339273209211085,
            "mae": 0.2965103423166728,
            "precision": 0.7551020408163265,
            "recall": 0.6420824295010846
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7940776073408284,
            "auditor_fn_violation": 0.024556510444468178,
            "auditor_fp_violation": 0.019530105095674752,
            "ave_precision_score": 0.7940704829905298,
            "fpr": 0.09868421052631579,
            "logloss": 2.913174106761348,
            "mae": 0.3086613135757765,
            "precision": 0.7755610972568578,
            "recall": 0.6308316430020284
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8042709610996503,
            "auditor_fn_violation": 0.021411002188246344,
            "auditor_fp_violation": 0.018592511281863643,
            "ave_precision_score": 0.8041927850738361,
            "fpr": 0.09659714599341383,
            "logloss": 2.659868210458976,
            "mae": 0.28486163323757413,
            "precision": 0.7788944723618091,
            "recall": 0.6724511930585684
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.797384679533242,
            "auditor_fn_violation": 0.030926390519910325,
            "auditor_fp_violation": 0.02416729472846795,
            "ave_precision_score": 0.797467290635502,
            "fpr": 0.11074561403508772,
            "logloss": 3.0216724156165893,
            "mae": 0.3066287051075769,
            "precision": 0.7589498806682577,
            "recall": 0.6450304259634888
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8065912481012372,
            "auditor_fn_violation": 0.020644282581416342,
            "auditor_fp_violation": 0.02366142212464935,
            "ave_precision_score": 0.8064744096552162,
            "fpr": 0.10976948408342481,
            "logloss": 2.8266897168651637,
            "mae": 0.28734131790235756,
            "precision": 0.7578692493946732,
            "recall": 0.6789587852494577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7806356353413977,
            "auditor_fn_violation": 0.026220152307747054,
            "auditor_fp_violation": 0.024745634970481097,
            "ave_precision_score": 0.7807584850305804,
            "fpr": 0.09978070175438597,
            "logloss": 3.216659043754418,
            "mae": 0.31212079594848263,
            "precision": 0.7753086419753087,
            "recall": 0.6369168356997972
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7808050728210367,
            "auditor_fn_violation": 0.02159911041476674,
            "auditor_fp_violation": 0.017836321502622276,
            "ave_precision_score": 0.7806696960362948,
            "fpr": 0.10757409440175632,
            "logloss": 3.04852693313356,
            "mae": 0.2979463000791214,
            "precision": 0.7518987341772152,
            "recall": 0.6442516268980477
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7845256899422363,
            "auditor_fn_violation": 0.024187306501547993,
            "auditor_fp_violation": 0.022021416907423687,
            "ave_precision_score": 0.7845985478596698,
            "fpr": 0.09758771929824561,
            "logloss": 3.1584049328363935,
            "mae": 0.31189517570438136,
            "precision": 0.7763819095477387,
            "recall": 0.6267748478701826
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7852225371214244,
            "auditor_fn_violation": 0.018136966600074786,
            "auditor_fp_violation": 0.022319795096963045,
            "ave_precision_score": 0.7852172970320107,
            "fpr": 0.10428100987925357,
            "logloss": 2.9834104371939176,
            "mae": 0.29591803167118197,
            "precision": 0.7570332480818415,
            "recall": 0.6420824295010846
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7863744578405052,
            "auditor_fn_violation": 0.028946923597025016,
            "auditor_fp_violation": 0.02164196290248294,
            "ave_precision_score": 0.7864413027881562,
            "fpr": 0.09868421052631579,
            "logloss": 3.2214322864789566,
            "mae": 0.3139191218704452,
            "precision": 0.7704081632653061,
            "recall": 0.6125760649087221
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.789175045733678,
            "auditor_fn_violation": 0.019756126018225073,
            "auditor_fp_violation": 0.018002195389681667,
            "ave_precision_score": 0.7889940850888377,
            "fpr": 0.09989023051591657,
            "logloss": 3.0557186880973286,
            "mae": 0.29803724135872933,
            "precision": 0.7617801047120419,
            "recall": 0.631236442516269
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7796168265657474,
            "auditor_fn_violation": 0.023755827194761752,
            "auditor_fp_violation": 0.01955104048905079,
            "ave_precision_score": 0.7797694444823251,
            "fpr": 0.09978070175438597,
            "logloss": 3.192087452966248,
            "mae": 0.3102019927237056,
            "precision": 0.7769607843137255,
            "recall": 0.6430020283975659
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7766887463185304,
            "auditor_fn_violation": 0.017329768007791013,
            "auditor_fp_violation": 0.017138675448225397,
            "ave_precision_score": 0.7766911528616041,
            "fpr": 0.10318331503841932,
            "logloss": 3.043810199829605,
            "mae": 0.2967667799606507,
            "precision": 0.7583547557840618,
            "recall": 0.6399132321041214
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7789249063188612,
            "auditor_fn_violation": 0.024073876374506253,
            "auditor_fp_violation": 0.0198074990579073,
            "ave_precision_score": 0.7789902171575999,
            "fpr": 0.09649122807017543,
            "logloss": 3.2124596256766753,
            "mae": 0.31172478332762166,
            "precision": 0.7777777777777778,
            "recall": 0.6247464503042597
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.775551121847535,
            "auditor_fn_violation": 0.017798848015696332,
            "auditor_fp_violation": 0.014979875594584705,
            "ave_precision_score": 0.7755499811699205,
            "fpr": 0.09989023051591657,
            "logloss": 3.061035513045175,
            "mae": 0.2977714943413299,
            "precision": 0.7611548556430446,
            "recall": 0.6290672451193059
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.763645875497633,
            "auditor_fn_violation": 0.02056199067648839,
            "auditor_fp_violation": 0.01587949587572751,
            "ave_precision_score": 0.7636901070967823,
            "fpr": 0.08333333333333333,
            "logloss": 3.043904924984348,
            "mae": 0.3577723058917229,
            "precision": 0.7834757834757835,
            "recall": 0.5578093306288032
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7598004207457576,
            "auditor_fn_violation": 0.023908793702422323,
            "auditor_fp_violation": 0.018880351262349066,
            "ave_precision_score": 0.7600952947097768,
            "fpr": 0.07683863885839737,
            "logloss": 2.851799355334641,
            "mae": 0.32571791422373014,
            "precision": 0.7916666666666666,
            "recall": 0.5770065075921909
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7957087883579488,
            "auditor_fn_violation": 0.02870004626169887,
            "auditor_fp_violation": 0.023895134614579407,
            "ave_precision_score": 0.7957997104539496,
            "fpr": 0.1118421052631579,
            "logloss": 3.0047912858980528,
            "mae": 0.31004693528647037,
            "precision": 0.7548076923076923,
            "recall": 0.6369168356997972
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.8031207422884752,
            "auditor_fn_violation": 0.018917972907653163,
            "auditor_fp_violation": 0.02549091352603976,
            "ave_precision_score": 0.8030221946753058,
            "fpr": 0.10976948408342481,
            "logloss": 2.8026400409826087,
            "mae": 0.2894633305045813,
            "precision": 0.7555012224938875,
            "recall": 0.6702819956616052
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7852781500724568,
            "auditor_fn_violation": 0.027757019323155774,
            "auditor_fp_violation": 0.02222030314449609,
            "ave_precision_score": 0.7853511694378842,
            "fpr": 0.09978070175438597,
            "logloss": 3.223029992961388,
            "mae": 0.3114942041083536,
            "precision": 0.7758620689655172,
            "recall": 0.6389452332657201
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.787335579661228,
            "auditor_fn_violation": 0.019870419624212158,
            "auditor_fp_violation": 0.017563117453347973,
            "ave_precision_score": 0.7872139600137793,
            "fpr": 0.09879253567508232,
            "logloss": 3.0544635181402042,
            "mae": 0.2935463421386539,
            "precision": 0.7668393782383419,
            "recall": 0.6420824295010846
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7914637558785396,
            "auditor_fn_violation": 0.02607336037863421,
            "auditor_fp_violation": 0.02105838881212578,
            "ave_precision_score": 0.7915087017371012,
            "fpr": 0.09539473684210527,
            "logloss": 3.1175315301475215,
            "mae": 0.3100127359026324,
            "precision": 0.7814070351758794,
            "recall": 0.6308316430020284
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7950027844743288,
            "auditor_fn_violation": 0.01844889289974785,
            "auditor_fp_violation": 0.023710208562019757,
            "ave_precision_score": 0.7949449527571065,
            "fpr": 0.09879253567508232,
            "logloss": 2.933744504841665,
            "mae": 0.29250622647285146,
            "precision": 0.7668393782383419,
            "recall": 0.6420824295010846
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.8128297550581873,
            "auditor_fn_violation": 0.026927422511654392,
            "auditor_fp_violation": 0.02332726206925428,
            "ave_precision_score": 0.8130685266617526,
            "fpr": 0.12171052631578948,
            "logloss": 1.9089239286936366,
            "mae": 0.29055271904484264,
            "precision": 0.758695652173913,
            "recall": 0.7079107505070994
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8113196854848252,
            "auditor_fn_violation": 0.02061332806312817,
            "auditor_fp_violation": 0.02542749115745823,
            "ave_precision_score": 0.8114563596715996,
            "fpr": 0.12952799121844127,
            "logloss": 1.7978473251541,
            "mae": 0.2770592776512277,
            "precision": 0.7412280701754386,
            "recall": 0.7331887201735358
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7902770033636265,
            "auditor_fn_violation": 0.025072506316501198,
            "auditor_fp_violation": 0.028184273332495922,
            "ave_precision_score": 0.7906012693702249,
            "fpr": 0.12828947368421054,
            "logloss": 3.166955435266325,
            "mae": 0.30782217163939607,
            "precision": 0.7428571428571429,
            "recall": 0.6855983772819473
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7929483313909707,
            "auditor_fn_violation": 0.016774967795395397,
            "auditor_fp_violation": 0.022675936089767052,
            "ave_precision_score": 0.7927519270603628,
            "fpr": 0.13062568605927552,
            "logloss": 3.0280549520447395,
            "mae": 0.28904766940508664,
            "precision": 0.7337807606263982,
            "recall": 0.7114967462039046
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.7603804907109668,
            "auditor_fn_violation": 0.029745382726593366,
            "auditor_fp_violation": 0.023322028220910278,
            "ave_precision_score": 0.7605816161394474,
            "fpr": 0.1074561403508772,
            "logloss": 3.12237893707439,
            "mae": 0.33188731751500966,
            "precision": 0.7562189054726368,
            "recall": 0.6166328600405679
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7514175400341929,
            "auditor_fn_violation": 0.018501277469158587,
            "auditor_fp_violation": 0.02566654470057325,
            "ave_precision_score": 0.7526494265734194,
            "fpr": 0.1141602634467618,
            "logloss": 2.9330024272998094,
            "mae": 0.313514271675598,
            "precision": 0.7340153452685422,
            "recall": 0.6225596529284165
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7845329579431912,
            "auditor_fn_violation": 0.026271307070922748,
            "auditor_fp_violation": 0.02418823012184399,
            "ave_precision_score": 0.7845970547688509,
            "fpr": 0.10087719298245613,
            "logloss": 3.162447282928651,
            "mae": 0.31317054071884554,
            "precision": 0.77,
            "recall": 0.6247464503042597
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7857229453859571,
            "auditor_fn_violation": 0.018687004578887587,
            "auditor_fp_violation": 0.02481278204659105,
            "ave_precision_score": 0.7856634670821037,
            "fpr": 0.10757409440175632,
            "logloss": 2.983953859059583,
            "mae": 0.29583835624001176,
            "precision": 0.75,
            "recall": 0.6377440347071583
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.8053379698964342,
            "auditor_fn_violation": 0.02607336037863421,
            "auditor_fp_violation": 0.022322363187204288,
            "ave_precision_score": 0.8052925570152769,
            "fpr": 0.08991228070175439,
            "logloss": 2.997933533198494,
            "mae": 0.3039742996455674,
            "precision": 0.7913486005089059,
            "recall": 0.6308316430020284
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8169060963144232,
            "auditor_fn_violation": 0.023380185774732064,
            "auditor_fp_violation": 0.01959263324795707,
            "ave_precision_score": 0.8167687499144735,
            "fpr": 0.09659714599341383,
            "logloss": 2.7836163516082695,
            "mae": 0.2817781273287954,
            "precision": 0.7743589743589744,
            "recall": 0.6550976138828634
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 22571,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.7066788311971463,
            "auditor_fn_violation": 0.04411987117896161,
            "auditor_fp_violation": 0.031756374827283,
            "ave_precision_score": 0.6972101735865599,
            "fpr": 0.14364035087719298,
            "logloss": 3.1790476800904255,
            "mae": 0.33315619004976477,
            "precision": 0.7108167770419426,
            "recall": 0.6531440162271805
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.692148926698877,
            "auditor_fn_violation": 0.03947177305099638,
            "auditor_fp_violation": 0.033733382119770706,
            "ave_precision_score": 0.6793148670651314,
            "fpr": 0.1525795828759605,
            "logloss": 3.121491813664269,
            "mae": 0.31712818574523627,
            "precision": 0.6951754385964912,
            "recall": 0.6876355748373102
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6491228070175439,
            "auc_prc": 0.7253524249535781,
            "auditor_fn_violation": 0.01747491192484253,
            "auditor_fp_violation": 0.018229493782188164,
            "ave_precision_score": 0.7250788915574544,
            "fpr": 0.10416666666666667,
            "logloss": 2.9199166858162227,
            "mae": 0.4022012378114462,
            "precision": 0.7382920110192838,
            "recall": 0.5436105476673428
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.7062637242173435,
            "auditor_fn_violation": 0.02132766310054742,
            "auditor_fp_violation": 0.01698743749237712,
            "ave_precision_score": 0.7060756550624327,
            "fpr": 0.10098792535675083,
            "logloss": 2.7791770681988446,
            "mae": 0.38324929774496996,
            "precision": 0.7309941520467836,
            "recall": 0.5422993492407809
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7940912488604017,
            "auditor_fn_violation": 0.02568858759474752,
            "auditor_fp_violation": 0.021406439727002468,
            "ave_precision_score": 0.7941447331846668,
            "fpr": 0.09868421052631579,
            "logloss": 3.1000827186017226,
            "mae": 0.30581453673722775,
            "precision": 0.7783251231527094,
            "recall": 0.640973630831643
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7923470483434867,
            "auditor_fn_violation": 0.020668093749330315,
            "auditor_fp_violation": 0.023715087205756807,
            "ave_precision_score": 0.7923354505353672,
            "fpr": 0.10757409440175632,
            "logloss": 2.9183430849054988,
            "mae": 0.29137849095602736,
            "precision": 0.7531486146095718,
            "recall": 0.648590021691974
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7914084283522309,
            "auditor_fn_violation": 0.02607336037863421,
            "auditor_fp_violation": 0.02105838881212578,
            "ave_precision_score": 0.7914535808592558,
            "fpr": 0.09539473684210527,
            "logloss": 3.1170631728330616,
            "mae": 0.3100736950155075,
            "precision": 0.7814070351758794,
            "recall": 0.6308316430020284
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7949601603111818,
            "auditor_fn_violation": 0.01844889289974785,
            "auditor_fp_violation": 0.023710208562019757,
            "ave_precision_score": 0.7949023912200696,
            "fpr": 0.09879253567508232,
            "logloss": 2.933209108011921,
            "mae": 0.2925582841873551,
            "precision": 0.7668393782383419,
            "recall": 0.6420824295010846
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7798333912873636,
            "auditor_fn_violation": 0.02327986548521405,
            "auditor_fp_violation": 0.01955104048905079,
            "ave_precision_score": 0.7798070771763963,
            "fpr": 0.09978070175438597,
            "logloss": 3.203495745011915,
            "mae": 0.31012959490880115,
            "precision": 0.7758620689655172,
            "recall": 0.6389452332657201
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7767997472702821,
            "auditor_fn_violation": 0.01808220091387263,
            "auditor_fp_violation": 0.01986095865349433,
            "ave_precision_score": 0.7767788372741441,
            "fpr": 0.10208562019758508,
            "logloss": 3.0551093351278844,
            "mae": 0.29665847636490933,
            "precision": 0.7596899224806202,
            "recall": 0.6377440347071583
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 22571,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.7877738813200278,
            "auditor_fn_violation": 0.02848653072844383,
            "auditor_fp_violation": 0.023094355817945823,
            "ave_precision_score": 0.7878667517376575,
            "fpr": 0.10197368421052631,
            "logloss": 3.1173221010137793,
            "mae": 0.311591736012617,
            "precision": 0.7692307692307693,
            "recall": 0.6288032454361054
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7840843394560699,
            "auditor_fn_violation": 0.018941784075567125,
            "auditor_fp_violation": 0.023193072325893408,
            "ave_precision_score": 0.7842223754233923,
            "fpr": 0.10647639956092206,
            "logloss": 2.9385516567696692,
            "mae": 0.29582635909432875,
            "precision": 0.7544303797468355,
            "recall": 0.6464208242950108
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 22571,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7823240929882607,
            "auditor_fn_violation": 0.020417422867513618,
            "auditor_fp_violation": 0.020901373361805465,
            "ave_precision_score": 0.782465788211124,
            "fpr": 0.10197368421052631,
            "logloss": 3.168161084481786,
            "mae": 0.31053430522641484,
            "precision": 0.7720588235294118,
            "recall": 0.6389452332657201
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.7793025006522604,
            "auditor_fn_violation": 0.017877424869812444,
            "auditor_fp_violation": 0.01813391877058178,
            "ave_precision_score": 0.7793781809314981,
            "fpr": 0.10537870472008781,
            "logloss": 2.993504287117941,
            "mae": 0.2956051889400861,
            "precision": 0.7581863979848866,
            "recall": 0.6529284164859002
        }
    }
]
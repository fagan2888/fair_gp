[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473455718685381,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044980301178998,
            "fpr": 0.48355263157894735,
            "logloss": 16.673682077455354,
            "mae": 0.48920642210152804,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.42590559824368823,
            "logloss": 14.579854001390768,
            "mae": 0.4305027825015919,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 19863,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.7240740395733993,
            "auditor_fn_violation": 0.006203831948291784,
            "auditor_fp_violation": 0.026818347953216387,
            "ave_precision_score": 0.6553614865556515,
            "fpr": 0.3541666666666667,
            "logloss": 5.392000659535571,
            "mae": 0.3814896831868232,
            "precision": 0.5744400527009222,
            "recall": 0.956140350877193
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.7596903903102505,
            "auditor_fn_violation": 0.00742817593094662,
            "auditor_fp_violation": 0.02704369250723602,
            "ave_precision_score": 0.7010241977489686,
            "fpr": 0.3029637760702525,
            "logloss": 4.6874237169976265,
            "mae": 0.3373052899603096,
            "precision": 0.6329787234042553,
            "recall": 0.9558232931726908
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473517107640107,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004414819944598345,
            "ave_precision_score": 0.5045041689741859,
            "fpr": 0.48355263157894735,
            "logloss": 16.663255507114368,
            "mae": 0.4895558231999438,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.775377470818498,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.007572233902026087,
            "ave_precision_score": 0.5596203605548424,
            "fpr": 0.424807903402854,
            "logloss": 14.584947780225528,
            "mae": 0.42990002462728133,
            "precision": 0.5612244897959183,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 19863,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8048314715513359,
            "auditor_fn_violation": 0.012025334718374883,
            "auditor_fp_violation": 0.014427516158818099,
            "ave_precision_score": 0.8051585557016128,
            "fpr": 0.14802631578947367,
            "logloss": 0.8899106862196298,
            "mae": 0.2789485415003888,
            "precision": 0.725609756097561,
            "recall": 0.7828947368421053
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.829618622173424,
            "auditor_fn_violation": 0.006614823729605582,
            "auditor_fp_violation": 0.020167817075666533,
            "ave_precision_score": 0.8300447923521154,
            "fpr": 0.12403951701427003,
            "logloss": 0.8513089156180098,
            "mae": 0.2792029201470658,
            "precision": 0.7674897119341564,
            "recall": 0.748995983935743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.7974905499844974,
            "auditor_fn_violation": 0.01426159972299169,
            "auditor_fp_violation": 0.01413896583564174,
            "ave_precision_score": 0.798866601930188,
            "fpr": 0.1513157894736842,
            "logloss": 0.8358686087595469,
            "mae": 0.2749645102611753,
            "precision": 0.7245508982035929,
            "recall": 0.7960526315789473
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8390314527259489,
            "auditor_fn_violation": 0.0069498631187758795,
            "auditor_fp_violation": 0.025470241306815015,
            "ave_precision_score": 0.8393386828022549,
            "fpr": 0.12294182217343579,
            "logloss": 0.7648308111045575,
            "mae": 0.2743381645269293,
            "precision": 0.7709611451942741,
            "recall": 0.7570281124497992
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7473455609667239,
            "auditor_fn_violation": 0.003347183748845799,
            "auditor_fp_violation": 0.004737034472145281,
            "ave_precision_score": 0.5044980192192143,
            "fpr": 0.48135964912280704,
            "logloss": 16.68257293050799,
            "mae": 0.48827732811840535,
            "precision": 0.5061867266591676,
            "recall": 0.9868421052631579
        },
        "train": {
            "accuracy": 0.5729967069154775,
            "auc_prc": 0.7753723729407862,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.006905111855901646,
            "ave_precision_score": 0.5596152632969602,
            "fpr": 0.42371020856201974,
            "logloss": 14.582639680888867,
            "mae": 0.4286779107486004,
            "precision": 0.5618615209988649,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7473419230636441,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.003943521083410276,
            "ave_precision_score": 0.5044943813283527,
            "fpr": 0.4824561403508772,
            "logloss": 16.678528382878223,
            "mae": 0.4894849507307798,
            "precision": 0.5061728395061729,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.775356278911926,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5595991709883553,
            "fpr": 0.42590559824368823,
            "logloss": 14.591339885925152,
            "mae": 0.4301534300262815,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 19863,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8110789938323265,
            "auditor_fn_violation": 0.017654470606340414,
            "auditor_fp_violation": 0.0180440135426285,
            "ave_precision_score": 0.8113825611396713,
            "fpr": 0.14912280701754385,
            "logloss": 0.8736736956369608,
            "mae": 0.27796979152735324,
            "precision": 0.7246963562753036,
            "recall": 0.7850877192982456
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8310306462891153,
            "auditor_fn_violation": 0.009284117810429426,
            "auditor_fp_violation": 0.01650794832063321,
            "ave_precision_score": 0.8315259279667614,
            "fpr": 0.1163556531284303,
            "logloss": 0.8906556812867418,
            "mae": 0.27874546952837187,
            "precision": 0.7768421052631579,
            "recall": 0.7409638554216867
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5032894736842105,
            "auc_prc": 0.7473319305327292,
            "auditor_fn_violation": 0.0016808056325023084,
            "auditor_fp_violation": 0.0024238227146814624,
            "ave_precision_score": 0.5044843889134393,
            "fpr": 0.49122807017543857,
            "logloss": 16.66420675690564,
            "mae": 0.4955050133313355,
            "precision": 0.5016685205784205,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.557628979143798,
            "auc_prc": 0.775364547101955,
            "auditor_fn_violation": 0.001013934993541675,
            "auditor_fp_violation": 0.004119677974075269,
            "ave_precision_score": 0.5596074384984353,
            "fpr": 0.44017563117453345,
            "logloss": 14.596206210562723,
            "mae": 0.4403028351461267,
            "precision": 0.5529542920847269,
            "recall": 0.9959839357429718
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5032894736842105,
            "auc_prc": 0.4259719760314314,
            "auditor_fn_violation": 0.00020919898430288712,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5016487377499477,
            "fpr": 0.0,
            "logloss": 17.12855074422105,
            "mae": 0.49672046519316787,
            "precision": 1.0,
            "recall": 0.006578947368421052
        },
        "train": {
            "accuracy": 0.45773874862788144,
            "auc_prc": 0.4054566982324191,
            "auditor_fn_violation": 0.0008442110924488442,
            "auditor_fp_violation": 0.0007202791812738047,
            "ave_precision_score": 0.5480176663121493,
            "fpr": 0.003293084522502744,
            "logloss": 18.62623276357331,
            "mae": 0.5427106081622983,
            "precision": 0.7,
            "recall": 0.014056224899598393
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473467834551674,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.50449924170159,
            "fpr": 0.48355263157894735,
            "logloss": 16.672329568218593,
            "mae": 0.4893564968463042,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753887359962034,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596316244651987,
            "fpr": 0.42590559824368823,
            "logloss": 14.579954949117665,
            "mae": 0.43076070234460595,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7473455226228191,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.003943521083410276,
            "ave_precision_score": 0.504497980888044,
            "fpr": 0.4824561403508772,
            "logloss": 16.67034555178702,
            "mae": 0.4891294034705871,
            "precision": 0.5061728395061729,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753825896100216,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596254787156453,
            "fpr": 0.42590559824368823,
            "logloss": 14.581536318330114,
            "mae": 0.4302555253670697,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7473455664897376,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.003943521083410276,
            "ave_precision_score": 0.5044980247391386,
            "fpr": 0.4824561403508772,
            "logloss": 16.67055981991611,
            "mae": 0.4892130360040385,
            "precision": 0.5061728395061729,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753862377679356,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596291265177036,
            "fpr": 0.42590559824368823,
            "logloss": 14.580128428046013,
            "mae": 0.4304014843110323,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7473443278231162,
            "auditor_fn_violation": 0.003347183748845799,
            "auditor_fp_violation": 0.004737034472145281,
            "ave_precision_score": 0.5044967860817108,
            "fpr": 0.48135964912280704,
            "logloss": 16.674195504826454,
            "mae": 0.48857377689723375,
            "precision": 0.5061867266591676,
            "recall": 0.9868421052631579
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753762277732715,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596191176501788,
            "fpr": 0.42590559824368823,
            "logloss": 14.583651134385752,
            "mae": 0.42925005185435683,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473455718685381,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044980301178998,
            "fpr": 0.48355263157894735,
            "logloss": 16.670799461476616,
            "mae": 0.49000706899942625,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.005355581366297853,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.4270032930845225,
            "logloss": 14.583122224149962,
            "mae": 0.4326331723441938,
            "precision": 0.5599547511312217,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5131578947368421,
            "auc_prc": 0.7473431189272639,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004737034472145281,
            "ave_precision_score": 0.504495577190233,
            "fpr": 0.48135964912280704,
            "logloss": 16.67556200272646,
            "mae": 0.4888008723458061,
            "precision": 0.5067415730337078,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7753836847805418,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.007572233902026087,
            "ave_precision_score": 0.5596265738435758,
            "fpr": 0.424807903402854,
            "logloss": 14.58001896963082,
            "mae": 0.42963519600494604,
            "precision": 0.5612244897959183,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7715974139993692,
            "auditor_fn_violation": 0.014254385964912278,
            "auditor_fp_violation": 0.01540858725761774,
            "ave_precision_score": 0.7720708203179814,
            "fpr": 0.15789473684210525,
            "logloss": 0.9891092558446443,
            "mae": 0.2956681045557342,
            "precision": 0.7037037037037037,
            "recall": 0.75
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8102561128874873,
            "auditor_fn_violation": 0.01071244362741857,
            "auditor_fp_violation": 0.017408961761414835,
            "ave_precision_score": 0.810885742492095,
            "fpr": 0.12843029637760703,
            "logloss": 0.9672744987748693,
            "mae": 0.29213563136717485,
            "precision": 0.7547169811320755,
            "recall": 0.7228915662650602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7473406850038745,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.003943521083410276,
            "ave_precision_score": 0.5044931432713258,
            "fpr": 0.4824561403508772,
            "logloss": 16.666975751470066,
            "mae": 0.48929144763099464,
            "precision": 0.5061728395061729,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753799611716408,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.55962285063867,
            "fpr": 0.42590559824368823,
            "logloss": 14.582492933172583,
            "mae": 0.4298814368243856,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473468024433727,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.504499260683619,
            "fpr": 0.48355263157894735,
            "logloss": 16.670633197103747,
            "mae": 0.4891870547941492,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7753848848379116,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.007572233902026087,
            "ave_precision_score": 0.559627773792342,
            "fpr": 0.424807903402854,
            "logloss": 14.578781002972846,
            "mae": 0.43060143914291593,
            "precision": 0.5612244897959183,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473443549031084,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044968131554485,
            "fpr": 0.48355263157894735,
            "logloss": 16.67414279237426,
            "mae": 0.48914587412229066,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.42590559824368823,
            "logloss": 14.579969086566887,
            "mae": 0.43038531963352133,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473468024433727,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.504499260683619,
            "fpr": 0.48355263157894735,
            "logloss": 16.673938306918107,
            "mae": 0.4892651401121389,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753873858167528,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596302744898431,
            "fpr": 0.42590559824368823,
            "logloss": 14.58005640417507,
            "mae": 0.43080291311188956,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.796930837178684,
            "auditor_fn_violation": 0.014802631578947373,
            "auditor_fp_violation": 0.016216528162511547,
            "ave_precision_score": 0.7983066372026081,
            "fpr": 0.15679824561403508,
            "logloss": 0.8450262770627414,
            "mae": 0.27629640443395687,
            "precision": 0.720703125,
            "recall": 0.8092105263157895
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8385121749180511,
            "auditor_fn_violation": 0.010782978235664945,
            "auditor_fp_violation": 0.028048362361558894,
            "ave_precision_score": 0.8388696037649293,
            "fpr": 0.12403951701427003,
            "logloss": 0.7522222587650282,
            "mae": 0.2727983636436274,
            "precision": 0.7717171717171717,
            "recall": 0.7670682730923695
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8003693697677934,
            "auditor_fn_violation": 0.012349953831948294,
            "auditor_fp_violation": 0.019493978916589726,
            "ave_precision_score": 0.8007160519868293,
            "fpr": 0.17214912280701755,
            "logloss": 0.9138189813402724,
            "mae": 0.28140527460626724,
            "precision": 0.6998087954110899,
            "recall": 0.8026315789473685
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8263017404283238,
            "auditor_fn_violation": 0.008808009204766381,
            "auditor_fp_violation": 0.019553852164691434,
            "ave_precision_score": 0.8269430475978498,
            "fpr": 0.1394072447859495,
            "logloss": 0.8825434739521463,
            "mae": 0.27993862167627526,
            "precision": 0.7485148514851485,
            "recall": 0.7590361445783133
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5131578947368421,
            "auc_prc": 0.7473467671988058,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004737034472145281,
            "ave_precision_score": 0.5044992254484052,
            "fpr": 0.48135964912280704,
            "logloss": 16.676359151379472,
            "mae": 0.48863072957661435,
            "precision": 0.5067415730337078,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7753685589432213,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.007572233902026087,
            "ave_precision_score": 0.5596114497607899,
            "fpr": 0.424807903402854,
            "logloss": 14.58101304246972,
            "mae": 0.43008600949971987,
            "precision": 0.5612244897959183,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5087719298245614,
            "auc_prc": 0.7473492310045236,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004619209756848265,
            "ave_precision_score": 0.5045016892359129,
            "fpr": 0.4857456140350877,
            "logloss": 16.66988713995717,
            "mae": 0.49064156883192345,
            "precision": 0.5044742729306487,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.566410537870472,
            "auc_prc": 0.7753886340153998,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.006883849001841913,
            "ave_precision_score": 0.5596315225484935,
            "fpr": 0.43029637760702527,
            "logloss": 14.582942934885391,
            "mae": 0.43356886166644876,
            "precision": 0.5580608793686584,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5032894736842105,
            "auc_prc": 0.7473283092297885,
            "auditor_fn_violation": 0.0018010349338257927,
            "auditor_fp_violation": 0.0024238227146814624,
            "ave_precision_score": 0.5044807676192073,
            "fpr": 0.49122807017543857,
            "logloss": 16.665517217780693,
            "mae": 0.49514644744781117,
            "precision": 0.5016685205784205,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.557628979143798,
            "auc_prc": 0.7753696115984566,
            "auditor_fn_violation": 0.001013934993541675,
            "auditor_fp_violation": 0.004119677974075269,
            "ave_precision_score": 0.5596125023960195,
            "fpr": 0.44017563117453345,
            "logloss": 14.594634588941044,
            "mae": 0.4398009151399673,
            "precision": 0.5529542920847269,
            "recall": 0.9959839357429718
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473443549031084,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044968131554485,
            "fpr": 0.48355263157894735,
            "logloss": 16.673581197750142,
            "mae": 0.48919661219516053,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.42590559824368823,
            "logloss": 14.579910635803758,
            "mae": 0.43048210084889055,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8135942593013805,
            "auditor_fn_violation": 0.020393294090489383,
            "auditor_fp_violation": 0.015536030317020628,
            "ave_precision_score": 0.8138947119049253,
            "fpr": 0.1524122807017544,
            "logloss": 0.8182675567408354,
            "mae": 0.27755667140207635,
            "precision": 0.717479674796748,
            "recall": 0.7741228070175439
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8392521841608904,
            "auditor_fn_violation": 0.0073047403665154565,
            "auditor_fp_violation": 0.023482164452229018,
            "ave_precision_score": 0.8395809519840813,
            "fpr": 0.11855104281009879,
            "logloss": 0.7865951047920758,
            "mae": 0.2721805587962925,
            "precision": 0.7768595041322314,
            "recall": 0.7550200803212851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473443440328804,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.504496802289813,
            "fpr": 0.48355263157894735,
            "logloss": 16.67435937489171,
            "mae": 0.489095336510717,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.42590559824368823,
            "logloss": 14.580063339131891,
            "mae": 0.4302009165420275,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 19863,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8032970061988476,
            "auditor_fn_violation": 0.018423938134810712,
            "auditor_fp_violation": 0.012349953831948292,
            "ave_precision_score": 0.8046513274546353,
            "fpr": 0.1425438596491228,
            "logloss": 0.786105415063512,
            "mae": 0.27726276406942735,
            "precision": 0.7336065573770492,
            "recall": 0.7850877192982456
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8438943613087135,
            "auditor_fn_violation": 0.008040945340087024,
            "auditor_fp_violation": 0.018562471594155906,
            "ave_precision_score": 0.8442282635267232,
            "fpr": 0.10976948408342481,
            "logloss": 0.6986822425905984,
            "mae": 0.2738742949329805,
            "precision": 0.788135593220339,
            "recall": 0.7469879518072289
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 19863,
        "test": {
            "accuracy": 0.6567982456140351,
            "auc_prc": 0.8213182160367737,
            "auditor_fn_violation": 0.01144823407202216,
            "auditor_fp_violation": 0.024757617728531855,
            "ave_precision_score": 0.821544283469728,
            "fpr": 0.31359649122807015,
            "logloss": 1.5938333448043882,
            "mae": 0.3409285788461505,
            "precision": 0.6,
            "recall": 0.9407894736842105
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.8405440426765161,
            "auditor_fn_violation": 0.004582545329506832,
            "auditor_fp_violation": 0.015117889236477495,
            "ave_precision_score": 0.8407328497305527,
            "fpr": 0.278814489571899,
            "logloss": 1.3741245996147085,
            "mae": 0.30926999841121855,
            "precision": 0.6496551724137931,
            "recall": 0.9457831325301205
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7982778771523865,
            "auditor_fn_violation": 0.014037973222530007,
            "auditor_fp_violation": 0.013946598953524167,
            "ave_precision_score": 0.7996704959907609,
            "fpr": 0.15570175438596492,
            "logloss": 0.8029368259543374,
            "mae": 0.27605503910284096,
            "precision": 0.7204724409448819,
            "recall": 0.8026315789473685
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.83915886935429,
            "auditor_fn_violation": 0.007882242471532675,
            "auditor_fp_violation": 0.016441501901696513,
            "ave_precision_score": 0.8394899723107996,
            "fpr": 0.11855104281009879,
            "logloss": 0.7219652756938266,
            "mae": 0.2742461559436296,
            "precision": 0.7804878048780488,
            "recall": 0.7710843373493976
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8088129951058721,
            "auditor_fn_violation": 0.011570867959372115,
            "auditor_fp_violation": 0.0180440135426285,
            "ave_precision_score": 0.8091247963076719,
            "fpr": 0.14912280701754385,
            "logloss": 0.8942240712386538,
            "mae": 0.27842744665207353,
            "precision": 0.7235772357723578,
            "recall": 0.7807017543859649
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.830056986668269,
            "auditor_fn_violation": 0.009178315898059864,
            "auditor_fp_violation": 0.01650794832063321,
            "ave_precision_score": 0.8305481496073636,
            "fpr": 0.1163556531284303,
            "logloss": 0.9055914297888892,
            "mae": 0.27951018246008874,
            "precision": 0.7754237288135594,
            "recall": 0.7349397590361446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7857453176432884,
            "auditor_fn_violation": 0.007040627885503238,
            "auditor_fp_violation": 0.010459949215143123,
            "ave_precision_score": 0.7586143212907583,
            "fpr": 0.13815789473684212,
            "logloss": 3.6912474793566905,
            "mae": 0.2863125737731076,
            "precision": 0.72,
            "recall": 0.7105263157894737
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.8130952706930744,
            "auditor_fn_violation": 0.024889899884940423,
            "auditor_fp_violation": 0.01886546726450725,
            "ave_precision_score": 0.7933480103320105,
            "fpr": 0.13172338090010977,
            "logloss": 3.7219459930373775,
            "mae": 0.3008497665176671,
            "precision": 0.7424892703862661,
            "recall": 0.6947791164658634
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473455306372727,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004414819944598345,
            "ave_precision_score": 0.5044979889024399,
            "fpr": 0.48355263157894735,
            "logloss": 16.665175717105907,
            "mae": 0.4895474968636586,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753800637171996,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.559622953122036,
            "fpr": 0.42590559824368823,
            "logloss": 14.583517753317832,
            "mae": 0.42985353622588335,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473455718685381,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044980301178998,
            "fpr": 0.48355263157894735,
            "logloss": 16.674828319733823,
            "mae": 0.48910016332863915,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.42590559824368823,
            "logloss": 14.579987803465006,
            "mae": 0.4302191652494204,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.747345555550561,
            "auditor_fn_violation": 0.003347183748845799,
            "auditor_fp_violation": 0.004737034472145281,
            "ave_precision_score": 0.504498013804595,
            "fpr": 0.48135964912280704,
            "logloss": 16.677631614162276,
            "mae": 0.48857521617551425,
            "precision": 0.5061867266591676,
            "recall": 0.9868421052631579
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.775387508130929,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.007572233902026087,
            "ave_precision_score": 0.5596303967271585,
            "fpr": 0.424807903402854,
            "logloss": 14.579196957188824,
            "mae": 0.42870893368145785,
            "precision": 0.5612244897959183,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7473467450919676,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.003943521083410276,
            "ave_precision_score": 0.5044992033496604,
            "fpr": 0.4824561403508772,
            "logloss": 16.662565050936315,
            "mae": 0.48956754437477573,
            "precision": 0.5061728395061729,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753837451237083,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596266341505006,
            "fpr": 0.42590559824368823,
            "logloss": 14.582317683317537,
            "mae": 0.43096439221258637,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473430611393065,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004414819944598345,
            "ave_precision_score": 0.5044955194245069,
            "fpr": 0.48355263157894735,
            "logloss": 16.66389073688135,
            "mae": 0.4898221205708136,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753825770021069,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.55962546611804,
            "fpr": 0.42590559824368823,
            "logloss": 14.582714967221362,
            "mae": 0.42999197442414344,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473467834551674,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.50449924170159,
            "fpr": 0.48355263157894735,
            "logloss": 16.6732547105764,
            "mae": 0.48920072002402093,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753899695854573,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596328579245582,
            "fpr": 0.42590559824368823,
            "logloss": 14.579891794423167,
            "mae": 0.43044460572612314,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.747341866260835,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004414819944598345,
            "ave_precision_score": 0.5044943245409729,
            "fpr": 0.48355263157894735,
            "logloss": 16.666386255133215,
            "mae": 0.48957059762623556,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753812861554482,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596241754366109,
            "fpr": 0.42590559824368823,
            "logloss": 14.583676038360355,
            "mae": 0.42985413296253866,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.747344335963303,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044967942188394,
            "fpr": 0.48355263157894735,
            "logloss": 16.672333609316432,
            "mae": 0.48923644915141773,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753812258122816,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.559624115129686,
            "fpr": 0.42590559824368823,
            "logloss": 14.582244395106366,
            "mae": 0.43057534916041135,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473394295989282,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004414819944598345,
            "ave_precision_score": 0.5044918878850527,
            "fpr": 0.48355263157894735,
            "logloss": 16.664466188522912,
            "mae": 0.4894249233660595,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7753787866073553,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.007572233902026087,
            "ave_precision_score": 0.5596216761695059,
            "fpr": 0.424807903402854,
            "logloss": 14.583073770897087,
            "mae": 0.4297233764343953,
            "precision": 0.5612244897959183,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473455718685381,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044980301178998,
            "fpr": 0.48355263157894735,
            "logloss": 16.673104434349167,
            "mae": 0.4891530683411949,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.42590559824368823,
            "logloss": 14.57952541258385,
            "mae": 0.4303943317174502,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5098684210526315,
            "auc_prc": 0.7473394545211118,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.005025584795321653,
            "ave_precision_score": 0.5044919127931249,
            "fpr": 0.48464912280701755,
            "logloss": 16.661812433797643,
            "mae": 0.49010046778649147,
            "precision": 0.5050391937290034,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753738751000631,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596167651687745,
            "fpr": 0.42590559824368823,
            "logloss": 14.583968010394916,
            "mae": 0.43135713763548933,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5098684210526315,
            "auc_prc": 0.7473418684088553,
            "auditor_fn_violation": 0.003347183748845799,
            "auditor_fp_violation": 0.004414819944598345,
            "ave_precision_score": 0.504494326693888,
            "fpr": 0.48355263157894735,
            "logloss": 16.665673587711748,
            "mae": 0.4895470862108275,
            "precision": 0.5050505050505051,
            "recall": 0.9868421052631579
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.775385030289821,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.007572233902026087,
            "ave_precision_score": 0.5596279191539959,
            "fpr": 0.424807903402854,
            "logloss": 14.581784205479272,
            "mae": 0.42946147301357174,
            "precision": 0.5612244897959183,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473479950344635,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5045004532764921,
            "fpr": 0.48355263157894735,
            "logloss": 16.671852934778023,
            "mae": 0.48912015121005753,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753785450359211,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596214347406254,
            "fpr": 0.42590559824368823,
            "logloss": 14.58333264660384,
            "mae": 0.4306623982572149,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473468024433727,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.504499260683619,
            "fpr": 0.48355263157894735,
            "logloss": 16.673817485201095,
            "mae": 0.4892679918830577,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753873858167528,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596302744898431,
            "fpr": 0.42590559824368823,
            "logloss": 14.58006481480822,
            "mae": 0.43080970332546226,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473455718685381,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044980301178998,
            "fpr": 0.48355263157894735,
            "logloss": 16.674163699078385,
            "mae": 0.4892006590452154,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.42590559824368823,
            "logloss": 14.579821151472622,
            "mae": 0.4304973297861945,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473443192499803,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004414819944598345,
            "ave_precision_score": 0.5044967775164906,
            "fpr": 0.48355263157894735,
            "logloss": 16.66742917412396,
            "mae": 0.4894244057685571,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753837451237083,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596266341505006,
            "fpr": 0.42590559824368823,
            "logloss": 14.58336325693175,
            "mae": 0.43004980958230693,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8153947342130693,
            "auditor_fn_violation": 0.02087661588180979,
            "auditor_fp_violation": 0.018106532779316713,
            "ave_precision_score": 0.8156903028749811,
            "fpr": 0.1513157894736842,
            "logloss": 0.8061444132961043,
            "mae": 0.27701545385172094,
            "precision": 0.7195121951219512,
            "recall": 0.7763157894736842
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8403028940462144,
            "auditor_fn_violation": 0.009601523547538123,
            "auditor_fp_violation": 0.024866907822869797,
            "ave_precision_score": 0.8406264700190662,
            "fpr": 0.11855104281009879,
            "logloss": 0.7745493780102867,
            "mae": 0.2717152108844028,
            "precision": 0.7777777777777778,
            "recall": 0.7590361445783133
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473467834551674,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.50449924170159,
            "fpr": 0.48355263157894735,
            "logloss": 16.673354358808382,
            "mae": 0.4891990749867554,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753899695854573,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596328579245582,
            "fpr": 0.42590559824368823,
            "logloss": 14.579886262242225,
            "mae": 0.43044107896303474,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5032894736842105,
            "auc_prc": 0.7473283092297885,
            "auditor_fn_violation": 0.0018010349338257927,
            "auditor_fp_violation": 0.0024238227146814624,
            "ave_precision_score": 0.5044807676192073,
            "fpr": 0.49122807017543857,
            "logloss": 16.665620114266492,
            "mae": 0.49513100899369483,
            "precision": 0.5016685205784205,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.557628979143798,
            "auc_prc": 0.7753696115984566,
            "auditor_fn_violation": 0.001013934993541675,
            "auditor_fp_violation": 0.004119677974075269,
            "ave_precision_score": 0.5596125023960195,
            "fpr": 0.44017563117453345,
            "logloss": 14.594500133796066,
            "mae": 0.43975724359949603,
            "precision": 0.5529542920847269,
            "recall": 0.9959839357429718
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5076754385964912,
            "auc_prc": 0.7473442754557468,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.005049630655586351,
            "ave_precision_score": 0.5044967337394524,
            "fpr": 0.4868421052631579,
            "logloss": 16.659393180756187,
            "mae": 0.4917175314362605,
            "precision": 0.5039106145251396,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5686059275521405,
            "auc_prc": 0.7753825770021069,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.005881837004276502,
            "ave_precision_score": 0.55962546611804,
            "fpr": 0.4281009879253567,
            "logloss": 14.59068959968256,
            "mae": 0.43460890005836694,
            "precision": 0.559322033898305,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8162137985839666,
            "auditor_fn_violation": 0.017769890735610954,
            "auditor_fp_violation": 0.021237303785780252,
            "ave_precision_score": 0.8165082041469601,
            "fpr": 0.15789473684210525,
            "logloss": 0.7811344710506828,
            "mae": 0.2780420458547885,
            "precision": 0.7154150197628458,
            "recall": 0.793859649122807
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8407360405858013,
            "auditor_fn_violation": 0.006467141893589732,
            "auditor_fp_violation": 0.026490858301682692,
            "ave_precision_score": 0.841066883946844,
            "fpr": 0.12623490669593854,
            "logloss": 0.7474322754774695,
            "mae": 0.27226648960498284,
            "precision": 0.7686116700201208,
            "recall": 0.7670682730923695
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473455718685381,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044980301178998,
            "fpr": 0.48355263157894735,
            "logloss": 16.67357780521509,
            "mae": 0.48920769693181165,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.42590559824368823,
            "logloss": 14.579858968027866,
            "mae": 0.43050566672219487,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5098684210526315,
            "auc_prc": 0.7473504035320534,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.005025584795321653,
            "ave_precision_score": 0.5045028617767229,
            "fpr": 0.48464912280701755,
            "logloss": 16.663322599726644,
            "mae": 0.4900204511700152,
            "precision": 0.5050391937290034,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753825713215609,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596254604401061,
            "fpr": 0.42590559824368823,
            "logloss": 14.582811958474192,
            "mae": 0.4303264274363425,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473455718685381,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044980301178998,
            "fpr": 0.48355263157894735,
            "logloss": 16.67322973181665,
            "mae": 0.48915087620989944,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.42590559824368823,
            "logloss": 14.579520353383812,
            "mae": 0.4303881167648471,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473443549031084,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044968131554485,
            "fpr": 0.48355263157894735,
            "logloss": 16.673583075029676,
            "mae": 0.4891959561933965,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.42590559824368823,
            "logloss": 14.579900752376474,
            "mae": 0.43047902833356083,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 19863,
        "test": {
            "accuracy": 0.6217105263157895,
            "auc_prc": 0.7193264769461084,
            "auditor_fn_violation": 0.006001846722068328,
            "auditor_fp_violation": 0.02542849722991691,
            "ave_precision_score": 0.647288388535156,
            "fpr": 0.35855263157894735,
            "logloss": 5.618314293101848,
            "mae": 0.37732430465537264,
            "precision": 0.5725490196078431,
            "recall": 0.9605263157894737
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.7594021241053972,
            "auditor_fn_violation": 0.006088018374265449,
            "auditor_fp_violation": 0.025398479174363383,
            "ave_precision_score": 0.6985881817634395,
            "fpr": 0.3106476399560922,
            "logloss": 4.822941203036393,
            "mae": 0.3385157718150743,
            "precision": 0.6271409749670619,
            "recall": 0.9558232931726908
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473455718685381,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044980301178998,
            "fpr": 0.48355263157894735,
            "logloss": 16.673203841340285,
            "mae": 0.48915067648462335,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.42590559824368823,
            "logloss": 14.579521794128059,
            "mae": 0.43038595615924413,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473553427269432,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004414819944598345,
            "ave_precision_score": 0.5045078009269283,
            "fpr": 0.48355263157894735,
            "logloss": 16.661400247777625,
            "mae": 0.48957950296876723,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.775371194622841,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596140850798034,
            "fpr": 0.42590559824368823,
            "logloss": 14.585966714248746,
            "mae": 0.43074098948090905,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7473431236580416,
            "auditor_fn_violation": 0.003347183748845799,
            "auditor_fp_violation": 0.004737034472145281,
            "ave_precision_score": 0.5044955819259793,
            "fpr": 0.48135964912280704,
            "logloss": 16.689344897637685,
            "mae": 0.4884093379801643,
            "precision": 0.5061867266591676,
            "recall": 0.9868421052631579
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7753660976122257,
            "auditor_fn_violation": 0.002468711288623209,
            "auditor_fp_violation": 0.006905111855901646,
            "ave_precision_score": 0.5596089886816897,
            "fpr": 0.42371020856201974,
            "logloss": 14.587162806217094,
            "mae": 0.42890326567966514,
            "precision": 0.5613636363636364,
            "recall": 0.9919678714859438
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473467834551674,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.50449924170159,
            "fpr": 0.48355263157894735,
            "logloss": 16.673326802844255,
            "mae": 0.4892049834739395,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753861494785524,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.559629038283358,
            "fpr": 0.42590559824368823,
            "logloss": 14.58003431078716,
            "mae": 0.43052966560373435,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7473443789363934,
            "auditor_fn_violation": 0.003347183748845799,
            "auditor_fp_violation": 0.004737034472145281,
            "ave_precision_score": 0.504496837184234,
            "fpr": 0.48135964912280704,
            "logloss": 16.68638469335161,
            "mae": 0.48844853541533895,
            "precision": 0.5061867266591676,
            "recall": 0.9868421052631579
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753660306078674,
            "auditor_fn_violation": 0.0031299732409329965,
            "auditor_fp_violation": 0.006905111855901646,
            "ave_precision_score": 0.5596089217197153,
            "fpr": 0.42371020856201974,
            "logloss": 14.5857969632502,
            "mae": 0.429220033055305,
            "precision": 0.5608646188850968,
            "recall": 0.9899598393574297
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.747345555550561,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.003943521083410276,
            "ave_precision_score": 0.504498013804595,
            "fpr": 0.4824561403508772,
            "logloss": 16.674561826491725,
            "mae": 0.4889048093620202,
            "precision": 0.5061728395061729,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.775389920433237,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.007572233902026087,
            "ave_precision_score": 0.5596328088023131,
            "fpr": 0.424807903402854,
            "logloss": 14.578634082276736,
            "mae": 0.429780416556299,
            "precision": 0.5612244897959183,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7643666618899148,
            "auditor_fn_violation": 0.015050303939673746,
            "auditor_fp_violation": 0.013100184672206836,
            "ave_precision_score": 0.7646951545812428,
            "fpr": 0.15899122807017543,
            "logloss": 1.1083251896392396,
            "mae": 0.2849830925579065,
            "precision": 0.7088353413654619,
            "recall": 0.7741228070175439
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8209129215185031,
            "auditor_fn_violation": 0.0077852573851939135,
            "auditor_fp_violation": 0.0239632365253307,
            "ave_precision_score": 0.821243345055334,
            "fpr": 0.1251372118551043,
            "logloss": 0.9745254245514862,
            "mae": 0.2812722987594477,
            "precision": 0.7634854771784232,
            "recall": 0.7389558232931727
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5076754385964912,
            "auc_prc": 0.7473430887235457,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.005049630655586351,
            "ave_precision_score": 0.5044955469961913,
            "fpr": 0.4868421052631579,
            "logloss": 16.660002620395343,
            "mae": 0.49180336228027793,
            "precision": 0.5039106145251396,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.7753826163802855,
            "auditor_fn_violation": 0.0012519892963731986,
            "auditor_fp_violation": 0.005881837004276502,
            "ave_precision_score": 0.5596255054709288,
            "fpr": 0.4281009879253567,
            "logloss": 14.59124700995446,
            "mae": 0.4350380390475322,
            "precision": 0.5598194130925508,
            "recall": 0.9959839357429718
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473431189272639,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.504495577190233,
            "fpr": 0.48355263157894735,
            "logloss": 16.67193340437479,
            "mae": 0.489751812774216,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.7753836480051752,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.005355581366297853,
            "ave_precision_score": 0.5596265370905997,
            "fpr": 0.4270032930845225,
            "logloss": 14.583046778644775,
            "mae": 0.43206730163173057,
            "precision": 0.5599547511312217,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7473431189272639,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.003943521083410276,
            "ave_precision_score": 0.504495577190233,
            "fpr": 0.4824561403508772,
            "logloss": 16.6754173066526,
            "mae": 0.4888227288941311,
            "precision": 0.5061728395061729,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7753836847805418,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.007572233902026087,
            "ave_precision_score": 0.5596265738435758,
            "fpr": 0.424807903402854,
            "logloss": 14.57985249070941,
            "mae": 0.42968455744446177,
            "precision": 0.5612244897959183,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5098684210526315,
            "auc_prc": 0.7473455199590485,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.005126577408433363,
            "ave_precision_score": 0.504497978225757,
            "fpr": 0.48464912280701755,
            "logloss": 16.668150735551826,
            "mae": 0.49006400873574324,
            "precision": 0.5050391937290034,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.7753799848681083,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.005355581366297853,
            "ave_precision_score": 0.5596228743211332,
            "fpr": 0.4270032930845225,
            "logloss": 14.585779416621675,
            "mae": 0.4325378752372712,
            "precision": 0.5599547511312217,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473443549031084,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044968131554485,
            "fpr": 0.48355263157894735,
            "logloss": 16.673605300457368,
            "mae": 0.48919612177434657,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.42590559824368823,
            "logloss": 14.579907505031,
            "mae": 0.4304800651017582,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473443029936188,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044967612633057,
            "fpr": 0.48355263157894735,
            "logloss": 16.67295471183984,
            "mae": 0.4895772644465914,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.7753861989299353,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.005355581366297853,
            "ave_precision_score": 0.5596290877043631,
            "fpr": 0.4270032930845225,
            "logloss": 14.582705464858368,
            "mae": 0.4312929481991145,
            "precision": 0.5599547511312217,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7473345342950795,
            "auditor_fn_violation": 0.003347183748845799,
            "auditor_fp_violation": 0.004737034472145281,
            "ave_precision_score": 0.504486992604146,
            "fpr": 0.48135964912280704,
            "logloss": 16.668782068644028,
            "mae": 0.48893878658631246,
            "precision": 0.5061867266591676,
            "recall": 0.9868421052631579
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7753574848328014,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.007572233902026087,
            "ave_precision_score": 0.5596003768029469,
            "fpr": 0.424807903402854,
            "logloss": 14.587132652587565,
            "mae": 0.4294125202887693,
            "precision": 0.5612244897959183,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5098684210526315,
            "auc_prc": 0.7473540869861919,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.005025584795321653,
            "ave_precision_score": 0.5045065452037069,
            "fpr": 0.48464912280701755,
            "logloss": 16.66342642006823,
            "mae": 0.49004540146380743,
            "precision": 0.5050391937290034,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753837451237083,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596266341505006,
            "fpr": 0.42590559824368823,
            "logloss": 14.582553535952206,
            "mae": 0.4303320857338487,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5098684210526315,
            "auc_prc": 0.7473505639192426,
            "auditor_fn_violation": 0.003347183748845799,
            "auditor_fp_violation": 0.005583448753462614,
            "ave_precision_score": 0.5045030221021283,
            "fpr": 0.48355263157894735,
            "logloss": 16.658536826511217,
            "mae": 0.48966202824987737,
            "precision": 0.5050505050505051,
            "recall": 0.9868421052631579
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753611597969126,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.005873863434004083,
            "ave_precision_score": 0.5596040513925583,
            "fpr": 0.42590559824368823,
            "logloss": 14.59274585616216,
            "mae": 0.4307455531040275,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8137015616571247,
            "auditor_fn_violation": 0.018258021698984304,
            "auditor_fp_violation": 0.019794552169898432,
            "ave_precision_score": 0.8140076643250598,
            "fpr": 0.14364035087719298,
            "logloss": 0.8092652173322277,
            "mae": 0.27705139640166254,
            "precision": 0.7270833333333333,
            "recall": 0.7653508771929824
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8371059945829418,
            "auditor_fn_violation": 0.010022526990508691,
            "auditor_fp_violation": 0.0229452773872205,
            "ave_precision_score": 0.8374897495921357,
            "fpr": 0.1141602634467618,
            "logloss": 0.7882595778929451,
            "mae": 0.27502708100237416,
            "precision": 0.7810526315789473,
            "recall": 0.7449799196787149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5131578947368421,
            "auc_prc": 0.7473431377161371,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004737034472145281,
            "ave_precision_score": 0.5044955959745556,
            "fpr": 0.48135964912280704,
            "logloss": 16.678649436459608,
            "mae": 0.48886756592561614,
            "precision": 0.5067415730337078,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7753886840950367,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.007572233902026087,
            "ave_precision_score": 0.559631572595828,
            "fpr": 0.424807903402854,
            "logloss": 14.578622017979226,
            "mae": 0.42971597438796455,
            "precision": 0.5612244897959183,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7952008713957552,
            "auditor_fn_violation": 0.019352108341028004,
            "auditor_fp_violation": 0.01650507848568791,
            "ave_precision_score": 0.7965730781491479,
            "fpr": 0.15350877192982457,
            "logloss": 0.8676996649030218,
            "mae": 0.27635698595019154,
            "precision": 0.7222222222222222,
            "recall": 0.7982456140350878
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.834145534793406,
            "auditor_fn_violation": 0.007042439792099249,
            "auditor_fp_violation": 0.024776540693115887,
            "ave_precision_score": 0.834534108881736,
            "fpr": 0.12952799121844127,
            "logloss": 0.7928034576483854,
            "mae": 0.27764534020813664,
            "precision": 0.7606490872210954,
            "recall": 0.7530120481927711
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5328947368421053,
            "auc_prc": 0.7953868547405165,
            "auditor_fn_violation": 0.003347183748845799,
            "auditor_fp_violation": 0.012927054478301019,
            "ave_precision_score": 0.6744824785119853,
            "fpr": 0.4605263157894737,
            "logloss": 9.312044900781537,
            "mae": 0.4669699091231721,
            "precision": 0.5172413793103449,
            "recall": 0.9868421052631579
        },
        "train": {
            "accuracy": 0.5905598243688255,
            "auc_prc": 0.8221563755708811,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.012268666792471898,
            "ave_precision_score": 0.7183628746280845,
            "fpr": 0.4061470911086718,
            "logloss": 8.073968515710574,
            "mae": 0.4112319153640165,
            "precision": 0.5722543352601156,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8168546831700463,
            "auditor_fn_violation": 0.010758117882425365,
            "auditor_fp_violation": 0.012802016004924597,
            "ave_precision_score": 0.8171622248769423,
            "fpr": 0.15899122807017543,
            "logloss": 0.786508109822085,
            "mae": 0.2738620143810555,
            "precision": 0.7184466019417476,
            "recall": 0.8114035087719298
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8345301697767483,
            "auditor_fn_violation": 0.004086598865274492,
            "auditor_fp_violation": 0.026732723266612267,
            "ave_precision_score": 0.8349240869646382,
            "fpr": 0.132821075740944,
            "logloss": 0.7704323373810967,
            "mae": 0.2739659075440001,
            "precision": 0.7613412228796844,
            "recall": 0.7751004016064257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473480438095398,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5045005020375827,
            "fpr": 0.48355263157894735,
            "logloss": 16.665837727636386,
            "mae": 0.4894789997731211,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753886045683775,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.559631493119437,
            "fpr": 0.42590559824368823,
            "logloss": 14.577761764897334,
            "mae": 0.4315429315320855,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8154108380292455,
            "auditor_fn_violation": 0.02148978531855956,
            "auditor_fp_violation": 0.019736842105263167,
            "ave_precision_score": 0.8157064385196567,
            "fpr": 0.14912280701754385,
            "logloss": 0.8065570546302631,
            "mae": 0.27697569388415305,
            "precision": 0.721881390593047,
            "recall": 0.7741228070175439
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8403168200633785,
            "auditor_fn_violation": 0.010575782823941211,
            "auditor_fp_violation": 0.024866907822869797,
            "ave_precision_score": 0.8406403434301305,
            "fpr": 0.11855104281009879,
            "logloss": 0.7764290217171036,
            "mae": 0.2718608890636404,
            "precision": 0.7768595041322314,
            "recall": 0.7550200803212851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.7815758981460861,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.007735553247152978,
            "ave_precision_score": 0.5986195308173582,
            "fpr": 0.4682017543859649,
            "logloss": 12.356686674807651,
            "mae": 0.4744547385924553,
            "precision": 0.5136674259681093,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.575192096597146,
            "auc_prc": 0.8136729075763846,
            "auditor_fn_violation": 0.0031299732409329965,
            "auditor_fp_violation": 0.008135699534609275,
            "ave_precision_score": 0.6591319076041682,
            "fpr": 0.41931942919868276,
            "logloss": 10.539328886370617,
            "mae": 0.4260813684953705,
            "precision": 0.5634285714285714,
            "recall": 0.9899598393574297
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8066743002706267,
            "auditor_fn_violation": 0.019299207448445682,
            "auditor_fp_violation": 0.02247326100338566,
            "ave_precision_score": 0.8070287783541692,
            "fpr": 0.15021929824561403,
            "logloss": 0.8136017580984738,
            "mae": 0.2760491682272191,
            "precision": 0.7204081632653061,
            "recall": 0.7741228070175439
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8310392316315998,
            "auditor_fn_violation": 0.010205476130647732,
            "auditor_fp_violation": 0.020577127016316588,
            "ave_precision_score": 0.8314369284515416,
            "fpr": 0.11855104281009879,
            "logloss": 0.8045148594958585,
            "mae": 0.27680175752137365,
            "precision": 0.7754677754677755,
            "recall": 0.748995983935743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7473455718685381,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.003943521083410276,
            "ave_precision_score": 0.5044980301178998,
            "fpr": 0.4824561403508772,
            "logloss": 16.673664267099927,
            "mae": 0.48906959220539786,
            "precision": 0.5061728395061729,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.007572233902026087,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.424807903402854,
            "logloss": 14.579558989786072,
            "mae": 0.4302004539393769,
            "precision": 0.5612244897959183,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473480194013715,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5045004776371651,
            "fpr": 0.48355263157894735,
            "logloss": 16.672888235925587,
            "mae": 0.4891900623106096,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7753848848379116,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.007572233902026087,
            "ave_precision_score": 0.559627773792342,
            "fpr": 0.424807903402854,
            "logloss": 14.578805773080816,
            "mae": 0.4306211803644123,
            "precision": 0.5612244897959183,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.747351688752266,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004414819944598345,
            "ave_precision_score": 0.5045041469704352,
            "fpr": 0.48355263157894735,
            "logloss": 16.661407443950203,
            "mae": 0.48986795775450676,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7753749168694131,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.007572233902026087,
            "ave_precision_score": 0.5596178069161962,
            "fpr": 0.424807903402854,
            "logloss": 14.58457051494258,
            "mae": 0.43024947992928136,
            "precision": 0.5612244897959183,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7481516290914122,
            "auditor_fn_violation": 0.003347183748845799,
            "auditor_fp_violation": 0.004737034472145281,
            "ave_precision_score": 0.5050345835252132,
            "fpr": 0.48135964912280704,
            "logloss": 16.804573978328946,
            "mae": 0.4882872299619798,
            "precision": 0.5061867266591676,
            "recall": 0.9868421052631579
        },
        "train": {
            "accuracy": 0.5729967069154775,
            "auc_prc": 0.7760394298895683,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.006905111855901646,
            "ave_precision_score": 0.5600873372865264,
            "fpr": 0.42371020856201974,
            "logloss": 14.639025590204211,
            "mae": 0.4285119212616937,
            "precision": 0.5618615209988649,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473504147404366,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004414819944598345,
            "ave_precision_score": 0.5045028729816771,
            "fpr": 0.48355263157894735,
            "logloss": 16.666337335127984,
            "mae": 0.4894030334046753,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.775371161909532,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596140523893537,
            "fpr": 0.42590559824368823,
            "logloss": 14.584394940481385,
            "mae": 0.430086525281459,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 19863,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8159626051692149,
            "auditor_fn_violation": 0.013910530163127115,
            "auditor_fp_violation": 0.020727531548168678,
            "ave_precision_score": 0.8162636580372116,
            "fpr": 0.16557017543859648,
            "logloss": 0.7673382330217302,
            "mae": 0.2769620148003147,
            "precision": 0.710727969348659,
            "recall": 0.8135964912280702
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8372978115738546,
            "auditor_fn_violation": 0.00837157631624192,
            "auditor_fp_violation": 0.027997863083166995,
            "ave_precision_score": 0.8375664246943275,
            "fpr": 0.13721185510428102,
            "logloss": 0.7596581450750169,
            "mae": 0.27627593594360994,
            "precision": 0.755859375,
            "recall": 0.7771084337349398
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7473455609908791,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.003943521083410276,
            "ave_precision_score": 0.504498019243359,
            "fpr": 0.4824561403508772,
            "logloss": 16.674703166999937,
            "mae": 0.48913896170339677,
            "precision": 0.5061728395061729,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753899695854573,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596328579245582,
            "fpr": 0.42590559824368823,
            "logloss": 14.579739846685195,
            "mae": 0.4304034105625361,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 19863,
        "test": {
            "accuracy": 0.506578947368421,
            "auc_prc": 0.7487038692918904,
            "auditor_fn_violation": 0.0027412280701754384,
            "auditor_fp_violation": 0.0060980301631271255,
            "ave_precision_score": 0.5050379106222039,
            "fpr": 0.48793859649122806,
            "logloss": 16.699560457059125,
            "mae": 0.4930687494727306,
            "precision": 0.5033482142857143,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5598243688254665,
            "auc_prc": 0.7754670421676483,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.005876521290761559,
            "ave_precision_score": 0.5580638408160411,
            "fpr": 0.43688254665203075,
            "logloss": 14.746404115765888,
            "mae": 0.43972934492983573,
            "precision": 0.5543113101903695,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.747345585477943,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044980437211677,
            "fpr": 0.48355263157894735,
            "logloss": 16.673606546010074,
            "mae": 0.4892174908822964,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753836512486577,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596265403329823,
            "fpr": 0.42590559824368823,
            "logloss": 14.579863949579336,
            "mae": 0.43058277944343415,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473455664897376,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044980247391386,
            "fpr": 0.48355263157894735,
            "logloss": 16.6701282731559,
            "mae": 0.48940555377583805,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753887359962034,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596316244651987,
            "fpr": 0.42590559824368823,
            "logloss": 14.580110183545813,
            "mae": 0.43087165654012927,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473467834551674,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.50449924170159,
            "fpr": 0.48355263157894735,
            "logloss": 16.67251960551771,
            "mae": 0.4893238151239945,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753887359962034,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596316244651987,
            "fpr": 0.42590559824368823,
            "logloss": 14.579896359869334,
            "mae": 0.430677743691526,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.747348014030002,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5045004722673092,
            "fpr": 0.48355263157894735,
            "logloss": 16.671639140722483,
            "mae": 0.4895696090105615,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.5697036223929748,
            "auc_prc": 0.7753887359962034,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.005355581366297853,
            "ave_precision_score": 0.5596316244651987,
            "fpr": 0.4270032930845225,
            "logloss": 14.580255522141465,
            "mae": 0.4313627881636413,
            "precision": 0.5599547511312217,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.7473455718685381,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004486957525392447,
            "ave_precision_score": 0.5044980301178998,
            "fpr": 0.48355263157894735,
            "logloss": 16.674200560031117,
            "mae": 0.4891005505892229,
            "precision": 0.5056053811659192,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753874317254905,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596303203697056,
            "fpr": 0.42590559824368823,
            "logloss": 14.580090193632044,
            "mae": 0.4302152563701637,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5131578947368421,
            "auc_prc": 0.7473504343303252,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.004737034472145281,
            "ave_precision_score": 0.5045028925619254,
            "fpr": 0.48135964912280704,
            "logloss": 16.67595842618058,
            "mae": 0.4884526682900366,
            "precision": 0.5067415730337078,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753699554595886,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596128460513998,
            "fpr": 0.42590559824368823,
            "logloss": 14.58125063916215,
            "mae": 0.42962452567492154,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 19863,
        "test": {
            "accuracy": 0.5120614035087719,
            "auc_prc": 0.7473467834551674,
            "auditor_fn_violation": 0.002777296860572484,
            "auditor_fp_violation": 0.003943521083410276,
            "ave_precision_score": 0.50449924170159,
            "fpr": 0.4824561403508772,
            "logloss": 16.67289208097566,
            "mae": 0.48914880260733995,
            "precision": 0.5061728395061729,
            "recall": 0.9890350877192983
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7753899695854573,
            "auditor_fn_violation": 0.0017920198907595255,
            "auditor_fp_violation": 0.0071363453938013644,
            "ave_precision_score": 0.5596328579245582,
            "fpr": 0.42590559824368823,
            "logloss": 14.579524888634559,
            "mae": 0.43033960284289907,
            "precision": 0.5605889014722537,
            "recall": 0.9939759036144579
        }
    }
]
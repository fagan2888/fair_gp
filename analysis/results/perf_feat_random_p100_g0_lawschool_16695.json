[
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(0)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8234220404949394,
            "auditor_fn_violation": 0.018111842105263162,
            "auditor_fp_violation": 0.024122807017543862,
            "ave_precision_score": 0.8236723778594932,
            "fpr": 0.13157894736842105,
            "logloss": 1.0234244841935491,
            "mae": 0.28294272895777417,
            "precision": 0.7565922920892495,
            "recall": 0.746
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8389570043237073,
            "auditor_fn_violation": 0.010817371625313713,
            "auditor_fp_violation": 0.025597667218316376,
            "ave_precision_score": 0.8395438611884969,
            "fpr": 0.12733260153677278,
            "logloss": 0.7985132775507755,
            "mae": 0.25036811497084893,
            "precision": 0.7537154989384289,
            "recall": 0.7819383259911894
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(1)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6491228070175439,
            "auc_prc": 0.7558108349143055,
            "auditor_fn_violation": 0.018750000000000003,
            "auditor_fp_violation": 0.036178887753363996,
            "ave_precision_score": 0.755506264203387,
            "fpr": 0.29605263157894735,
            "logloss": 1.9352372740964057,
            "mae": 0.3549738657146588,
            "precision": 0.625,
            "recall": 0.9
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.7507097528802942,
            "auditor_fn_violation": 0.012826588393448649,
            "auditor_fp_violation": 0.038013388514316876,
            "ave_precision_score": 0.7502430970035364,
            "fpr": 0.31284302963776073,
            "logloss": 1.857518266406705,
            "mae": 0.35662682485873254,
            "precision": 0.5928571428571429,
            "recall": 0.9140969162995595
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(2)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.7837905179615705,
            "auditor_fn_violation": 0.017057017543859656,
            "auditor_fp_violation": 0.021708929483903936,
            "ave_precision_score": 0.6556643692564745,
            "fpr": 0.16557017543859648,
            "logloss": 0.6371806107534407,
            "mae": 0.43857955612372934,
            "precision": 0.7003968253968254,
            "recall": 0.706
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7681606456951362,
            "auditor_fn_violation": 0.006271851139039743,
            "auditor_fp_violation": 0.02586668652285343,
            "ave_precision_score": 0.6271745981026097,
            "fpr": 0.17014270032930845,
            "logloss": 0.6197738111597813,
            "mae": 0.4300568697219623,
            "precision": 0.6777546777546778,
            "recall": 0.7180616740088106
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(3)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8430883004115794,
            "auditor_fn_violation": 0.013710526315789475,
            "auditor_fp_violation": 0.010600302333503661,
            "ave_precision_score": 0.8329584643236385,
            "fpr": 0.049342105263157895,
            "logloss": 0.612300089307638,
            "mae": 0.3437043303288232,
            "precision": 0.8648648648648649,
            "recall": 0.576
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8464937256845912,
            "auditor_fn_violation": 0.005739928528943851,
            "auditor_fp_violation": 0.0016861745695090648,
            "ave_precision_score": 0.8362241655809229,
            "fpr": 0.052689352360043906,
            "logloss": 0.5302582702051288,
            "mae": 0.315456160706971,
            "precision": 0.8518518518518519,
            "recall": 0.6079295154185022
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(4)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.7921001561010277,
            "auditor_fn_violation": 0.014807017543859656,
            "auditor_fp_violation": 0.026049650826094367,
            "ave_precision_score": 0.7324187290743983,
            "fpr": 0.17105263157894737,
            "logloss": 0.6273192644082131,
            "mae": 0.39474505369775525,
            "precision": 0.7022900763358778,
            "recall": 0.736
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7985726179572744,
            "auditor_fn_violation": 0.008235129136302755,
            "auditor_fp_violation": 0.024836246508153453,
            "ave_precision_score": 0.7339885290020329,
            "fpr": 0.17453347969264543,
            "logloss": 0.5711726255711495,
            "mae": 0.3706118548393688,
            "precision": 0.689453125,
            "recall": 0.7775330396475771
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(5)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.7346247276853998,
            "auditor_fn_violation": 0.015394736842105274,
            "auditor_fp_violation": 0.021708929483903936,
            "ave_precision_score": 0.6905711928952016,
            "fpr": 0.16557017543859648,
            "logloss": 0.6270579442243626,
            "mae": 0.4291344580747056,
            "precision": 0.7015810276679841,
            "recall": 0.71
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.704664633472057,
            "auditor_fn_violation": 0.006271851139039743,
            "auditor_fp_violation": 0.02586668652285343,
            "ave_precision_score": 0.6544881845876058,
            "fpr": 0.17014270032930845,
            "logloss": 0.6237411382677863,
            "mae": 0.42664929501442433,
            "precision": 0.6777546777546778,
            "recall": 0.7180616740088106
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(6)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8595684767858639,
            "auditor_fn_violation": 0.0021140350877193034,
            "auditor_fp_violation": 0.008771929824561406,
            "ave_precision_score": 0.8448491741840548,
            "fpr": 0.07675438596491228,
            "logloss": 0.5383302574781215,
            "mae": 0.31570488546174347,
            "precision": 0.8309178743961353,
            "recall": 0.688
        },
        "train": {
            "accuracy": 0.7870472008781558,
            "auc_prc": 0.8682200172204932,
            "auditor_fn_violation": 0.008090059333549336,
            "auditor_fp_violation": 0.0011913712058069744,
            "ave_precision_score": 0.8514404109653144,
            "fpr": 0.06915477497255763,
            "logloss": 0.45874219847608777,
            "mae": 0.2866450592662841,
            "precision": 0.8367875647668394,
            "recall": 0.711453744493392
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(7)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6129385964912281,
            "auc_prc": 0.6957658312071191,
            "auditor_fn_violation": 0.021447368421052646,
            "auditor_fp_violation": 0.01412930080054506,
            "ave_precision_score": 0.6234145473439189,
            "fpr": 0.15899122807017543,
            "logloss": 0.7142852208666146,
            "mae": 0.46808963529602216,
            "precision": 0.6681922196796338,
            "recall": 0.584
        },
        "train": {
            "accuracy": 0.6300768386388584,
            "auc_prc": 0.673045050517537,
            "auditor_fn_violation": 0.006204151897754801,
            "auditor_fp_violation": 0.020796153023945118,
            "ave_precision_score": 0.592147903959373,
            "fpr": 0.16794731064763996,
            "logloss": 0.7007641063341548,
            "mae": 0.4646927105134065,
            "precision": 0.6382978723404256,
            "recall": 0.5947136563876652
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(8)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6513157894736842,
            "auc_prc": 0.7417221282572419,
            "auditor_fn_violation": 0.08961403508771931,
            "auditor_fp_violation": 0.08191481434167945,
            "ave_precision_score": 0.7423185290666288,
            "fpr": 0.15899122807017543,
            "logloss": 0.6354068487274602,
            "mae": 0.4433320907369387,
            "precision": 0.6927966101694916,
            "recall": 0.654
        },
        "train": {
            "accuracy": 0.6564215148188803,
            "auc_prc": 0.7274469903190632,
            "auditor_fn_violation": 0.08405827937542615,
            "auditor_fp_violation": 0.09067631933552232,
            "ave_precision_score": 0.7283135589577792,
            "fpr": 0.16465422612513722,
            "logloss": 0.6233886406316548,
            "mae": 0.4379424258461375,
            "precision": 0.6598639455782312,
            "recall": 0.6409691629955947
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(9)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8318027570949393,
            "auditor_fn_violation": 0.002605263157894737,
            "auditor_fp_violation": 0.009000809061488674,
            "ave_precision_score": 0.832313804405935,
            "fpr": 0.10197368421052631,
            "logloss": 0.5310180739642484,
            "mae": 0.33205343501871093,
            "precision": 0.7910112359550562,
            "recall": 0.704
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8517740844330671,
            "auditor_fn_violation": 0.00500974385508494,
            "auditor_fp_violation": 0.010201115949722216,
            "ave_precision_score": 0.8521527565540699,
            "fpr": 0.09001097694840834,
            "logloss": 0.47991759534310824,
            "mae": 0.30767816494900885,
            "precision": 0.8038277511961722,
            "recall": 0.7400881057268722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(10)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.814684527469654,
            "auditor_fn_violation": 0.01797368421052632,
            "auditor_fp_violation": 0.01769289729177313,
            "ave_precision_score": 0.8149257722823864,
            "fpr": 0.15570175438596492,
            "logloss": 0.8061996885248165,
            "mae": 0.3194299972224184,
            "precision": 0.7215686274509804,
            "recall": 0.736
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8064953415884908,
            "auditor_fn_violation": 0.009673738013607552,
            "auditor_fp_violation": 0.019431840836650043,
            "ave_precision_score": 0.8067634481904993,
            "fpr": 0.14489571899012074,
            "logloss": 0.6920731488019547,
            "mae": 0.292708422009957,
            "precision": 0.7221052631578947,
            "recall": 0.7555066079295154
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(11)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8146976253845684,
            "auditor_fn_violation": 0.012188596491228072,
            "auditor_fp_violation": 0.015393459376596832,
            "ave_precision_score": 0.8152213712748833,
            "fpr": 0.14473684210526316,
            "logloss": 0.6291039051645341,
            "mae": 0.329227785442721,
            "precision": 0.751412429378531,
            "recall": 0.798
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8626077291927426,
            "auditor_fn_violation": 0.003210878300942476,
            "auditor_fp_violation": 0.01133003624554737,
            "ave_precision_score": 0.8628261431715447,
            "fpr": 0.12952799121844127,
            "logloss": 0.48421097276491104,
            "mae": 0.30450917617006457,
            "precision": 0.756198347107438,
            "recall": 0.8061674008810573
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(12)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.776021706815208,
            "auditor_fn_violation": 0.008122807017543872,
            "auditor_fp_violation": 0.010272951796968155,
            "ave_precision_score": 0.7597861379731068,
            "fpr": 0.08991228070175439,
            "logloss": 0.5952335249478723,
            "mae": 0.3991817372867413,
            "precision": 0.8,
            "recall": 0.656
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7517939986514452,
            "auditor_fn_violation": 0.0035783884679178157,
            "auditor_fp_violation": 0.008281951446819449,
            "ave_precision_score": 0.7291393032417823,
            "fpr": 0.09769484083424808,
            "logloss": 0.5799168617402798,
            "mae": 0.391992554379347,
            "precision": 0.7700258397932817,
            "recall": 0.6563876651982379
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(13)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5899122807017544,
            "auc_prc": 0.7110569391698631,
            "auditor_fn_violation": 0.005236842105263164,
            "auditor_fp_violation": 0.01832630727303697,
            "ave_precision_score": 0.7107696435634763,
            "fpr": 0.12938596491228072,
            "logloss": 6.408703970158571,
            "mae": 0.4134936723574846,
            "precision": 0.6740331491712708,
            "recall": 0.488
        },
        "train": {
            "accuracy": 0.6377607025246982,
            "auc_prc": 0.7111369524942666,
            "auditor_fn_violation": 0.0034236473449808344,
            "auditor_fp_violation": 0.019085958873673817,
            "ave_precision_score": 0.7113608613687887,
            "fpr": 0.1251372118551043,
            "logloss": 5.2678485299406335,
            "mae": 0.37444960649545117,
            "precision": 0.6761363636363636,
            "recall": 0.5242290748898678
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(14)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5570175438596491,
            "auc_prc": 0.6249897542633178,
            "auditor_fn_violation": 0.12107236842105264,
            "auditor_fp_violation": 0.07078755748594788,
            "ave_precision_score": 0.6385209765448059,
            "fpr": 0.15679824561403508,
            "logloss": 0.9691547242283728,
            "mae": 0.46685968327163435,
            "precision": 0.6256544502617801,
            "recall": 0.478
        },
        "train": {
            "accuracy": 0.5960482985729967,
            "auc_prc": 0.6435484044172257,
            "auditor_fn_violation": 0.1101805151912262,
            "auditor_fp_violation": 0.08440240484042592,
            "ave_precision_score": 0.6428197129523675,
            "fpr": 0.17453347969264543,
            "logloss": 0.8449778446349455,
            "mae": 0.4269282123573333,
            "precision": 0.6064356435643564,
            "recall": 0.539647577092511
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(15)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8498636915107489,
            "auditor_fn_violation": 0.008004385964912281,
            "auditor_fp_violation": 0.017722172543008008,
            "ave_precision_score": 0.8500682528890606,
            "fpr": 0.20065789473684212,
            "logloss": 0.5653516786238344,
            "mae": 0.32668821780386853,
            "precision": 0.6990131578947368,
            "recall": 0.85
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8668989927835697,
            "auditor_fn_violation": 0.008198861685614397,
            "auditor_fp_violation": 0.012972975569684415,
            "ave_precision_score": 0.8676115217638873,
            "fpr": 0.18660812294182216,
            "logloss": 0.4953719030484153,
            "mae": 0.30234159961849877,
            "precision": 0.7053726169844021,
            "recall": 0.8964757709251101
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(16)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5646929824561403,
            "auc_prc": 0.8439942110588715,
            "auditor_fn_violation": 0.0019210526315789475,
            "auditor_fp_violation": 0.00563681655595299,
            "ave_precision_score": 0.8442659146403554,
            "fpr": 0.43201754385964913,
            "logloss": 0.9667198075997387,
            "mae": 0.396364247719599,
            "precision": 0.5578002244668911,
            "recall": 0.994
        },
        "train": {
            "accuracy": 0.535675082327113,
            "auc_prc": 0.8635731559413614,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.015627139243911643,
            "ave_precision_score": 0.8639159676322468,
            "fpr": 0.4643249176728869,
            "logloss": 1.0100042193158913,
            "mae": 0.42562302976985217,
            "precision": 0.5176738882554162,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(17)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7877834463418361,
            "auditor_fn_violation": 0.014548245614035088,
            "auditor_fp_violation": 0.01414526911940045,
            "ave_precision_score": 0.7508622743263602,
            "fpr": 0.13048245614035087,
            "logloss": 4.414684722804973,
            "mae": 0.30263784821447354,
            "precision": 0.7510460251046025,
            "recall": 0.718
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.7938971853211693,
            "auditor_fn_violation": 0.007456587861526044,
            "auditor_fp_violation": 0.01808674431396475,
            "ave_precision_score": 0.7557094442028175,
            "fpr": 0.12184412733260154,
            "logloss": 3.6958034419676533,
            "mae": 0.26481983259248487,
            "precision": 0.753880266075388,
            "recall": 0.748898678414097
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(18)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5570175438596491,
            "auc_prc": 0.7741470370126378,
            "auditor_fn_violation": 0.0036074561403508783,
            "auditor_fp_violation": 0.00849514563106798,
            "ave_precision_score": 0.5598605025356194,
            "fpr": 0.42872807017543857,
            "logloss": 0.6924408447781726,
            "mae": 0.4933619394310211,
            "precision": 0.5546697038724373,
            "recall": 0.974
        },
        "train": {
            "accuracy": 0.5203073545554336,
            "auc_prc": 0.6205616606719075,
            "auditor_fn_violation": 0.001092859180742468,
            "auditor_fp_violation": 0.008601411870957205,
            "ave_precision_score": 0.5133678638117076,
            "fpr": 0.4698133918770582,
            "logloss": 0.6868560242068787,
            "mae": 0.4906561972346447,
            "precision": 0.5097365406643757,
            "recall": 0.9801762114537445
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(19)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8523293383533552,
            "auditor_fn_violation": 0.010241228070175438,
            "auditor_fp_violation": 0.011119272696303868,
            "ave_precision_score": 0.8525623890024181,
            "fpr": 0.08223684210526316,
            "logloss": 0.5366850031950536,
            "mae": 0.31996906749171966,
            "precision": 0.8255813953488372,
            "recall": 0.71
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.8700119644901613,
            "auditor_fn_violation": 0.018588277392805504,
            "auditor_fp_violation": 0.0014796061749538212,
            "ave_precision_score": 0.8703078227980405,
            "fpr": 0.07354555433589462,
            "logloss": 0.4720790558366227,
            "mae": 0.2914389964963926,
            "precision": 0.830379746835443,
            "recall": 0.7224669603524229
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(20)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0.3924613970198191,
            "auditor_fn_violation": 0.0007543859649122937,
            "auditor_fp_violation": 0.0007877703968659514,
            "ave_precision_score": 0.5463809267486899,
            "fpr": 0.0043859649122807015,
            "logloss": 18.619427851833482,
            "mae": 0.5520352251593514,
            "precision": 0.5,
            "recall": 0.008
        },
        "train": {
            "accuracy": 0.5038419319429198,
            "auc_prc": 0.3738019614534499,
            "auditor_fn_violation": 0.0008244800456486465,
            "auditor_fp_violation": 0.0005884797286748157,
            "ave_precision_score": 0.5014144955941244,
            "fpr": 0.0010976948408342481,
            "logloss": 16.99678658280288,
            "mae": 0.49956305913471755,
            "precision": 0.75,
            "recall": 0.006607929515418502
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(21)",
        "seed": 16695,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.7975418628080124,
            "auditor_fn_violation": 0.002688596491228083,
            "auditor_fp_violation": 0.007207034576733095,
            "ave_precision_score": 0.7832382553327809,
            "fpr": 0.09649122807017543,
            "logloss": 0.5518731506741473,
            "mae": 0.3520715761390564,
            "precision": 0.8009049773755657,
            "recall": 0.708
        },
        "train": {
            "accuracy": 0.7848518111964874,
            "auc_prc": 0.8200052798039205,
            "auditor_fn_violation": 0.0035034357364952134,
            "auditor_fp_violation": 0.010049792590920117,
            "ave_precision_score": 0.8027719738801897,
            "fpr": 0.0867178924259056,
            "logloss": 0.5047299517366889,
            "mae": 0.3355970512646221,
            "precision": 0.8100961538461539,
            "recall": 0.7422907488986784
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(22)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.7628252627409571,
            "auditor_fn_violation": 0.005403508771929826,
            "auditor_fp_violation": 0.011007494464316132,
            "ave_precision_score": 0.7245623029553305,
            "fpr": 0.10964912280701754,
            "logloss": 0.5905626384422064,
            "mae": 0.3888023876536049,
            "precision": 0.7787610619469026,
            "recall": 0.704
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.7121057917711462,
            "auditor_fn_violation": 0.003346276783512329,
            "auditor_fp_violation": 0.011044203234476748,
            "ave_precision_score": 0.6922043334141487,
            "fpr": 0.11525795828759605,
            "logloss": 0.5756608792509949,
            "mae": 0.3825377301117722,
            "precision": 0.7552447552447552,
            "recall": 0.7136563876651982
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(23)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4451754385964912,
            "auc_prc": 0.7867330287339808,
            "auditor_fn_violation": 0.0006206140350877199,
            "auditor_fp_violation": 0.003169711292795095,
            "ave_precision_score": 0.7879515504572073,
            "fpr": 0.007675438596491228,
            "logloss": 0.6933699551337976,
            "mae": 0.5001103060091274,
            "precision": 0.125,
            "recall": 0.002
        },
        "train": {
            "accuracy": 0.4972557628979144,
            "auc_prc": 0.8130730495122034,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0018831351317594102,
            "ave_precision_score": 0.8140504641776295,
            "fpr": 0.0043907793633369925,
            "logloss": 0.6931331233200311,
            "mae": 0.4999919636123921,
            "precision": 0.0,
            "recall": 0.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(24)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5328947368421053,
            "auc_prc": 0.7151282837561194,
            "auditor_fn_violation": 0.0049122807017543905,
            "auditor_fp_violation": 0.01248722534491569,
            "ave_precision_score": 0.5559395890814309,
            "fpr": 0.31140350877192985,
            "logloss": 0.6933359886356215,
            "mae": 0.4971297293117172,
            "precision": 0.557632398753894,
            "recall": 0.716
        },
        "train": {
            "accuracy": 0.5411635565312843,
            "auc_prc": 0.7036100764164055,
            "auditor_fn_violation": 0.009115219273006861,
            "auditor_fp_violation": 0.011354055826309602,
            "ave_precision_score": 0.520779953770396,
            "fpr": 0.3391877058177827,
            "logloss": 0.6902554230409983,
            "mae": 0.4967746916508177,
            "precision": 0.5275229357798165,
            "recall": 0.7599118942731278
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(25)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.8259831106879306,
            "auditor_fn_violation": 0.03133333333333334,
            "auditor_fp_violation": 0.030156170158405726,
            "ave_precision_score": 0.8260723715373715,
            "fpr": 0.16776315789473684,
            "logloss": 1.7258371136577746,
            "mae": 0.328493382837886,
            "precision": 0.7150837988826816,
            "recall": 0.768
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8379496054686096,
            "auditor_fn_violation": 0.018022505162067146,
            "auditor_fp_violation": 0.022600023539189155,
            "ave_precision_score": 0.8381658932104529,
            "fpr": 0.16245883644346873,
            "logloss": 1.4242007988776941,
            "mae": 0.3016767746150461,
            "precision": 0.7051792828685259,
            "recall": 0.7797356828193832
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(26)",
        "seed": 16695,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.812829757673651,
            "auditor_fn_violation": 0.008850877192982458,
            "auditor_fp_violation": 0.015377491057741449,
            "ave_precision_score": 0.7649490949415721,
            "fpr": 0.2050438596491228,
            "logloss": 0.5542352416666434,
            "mae": 0.364455844541162,
            "precision": 0.7027027027027027,
            "recall": 0.884
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7787432990168456,
            "auditor_fn_violation": 0.004564863126641102,
            "auditor_fp_violation": 0.013114691096181616,
            "ave_precision_score": 0.7239624655084803,
            "fpr": 0.21295279912184412,
            "logloss": 0.5529286470474502,
            "mae": 0.36369512639285506,
            "precision": 0.674496644295302,
            "recall": 0.8854625550660793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(27)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8379286951039925,
            "auditor_fn_violation": 0.012644736842105264,
            "auditor_fp_violation": 0.014020183955033214,
            "ave_precision_score": 0.831455762306391,
            "fpr": 0.20285087719298245,
            "logloss": 1.0118427635021598,
            "mae": 0.3322780446002358,
            "precision": 0.702572347266881,
            "recall": 0.874
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8553486564850228,
            "auditor_fn_violation": 0.004922701973432885,
            "auditor_fp_violation": 0.014327679924674597,
            "ave_precision_score": 0.8523183714714205,
            "fpr": 0.20417124039517015,
            "logloss": 0.8046319619630579,
            "mae": 0.32725052694196105,
            "precision": 0.6868686868686869,
            "recall": 0.8986784140969163
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(28)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.9357326726484,
            "mae": 0.5482456140350878,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.21251864246921,
            "mae": 0.4983534577387486,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(29)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6008771929824561,
            "auc_prc": 0.6943800523624979,
            "auditor_fn_violation": 0.005096491228070173,
            "auditor_fp_violation": 0.015183209845000857,
            "ave_precision_score": 0.6943926482923877,
            "fpr": 0.20285087719298245,
            "logloss": 1.8652986800174325,
            "mae": 0.4291046972027288,
            "precision": 0.6343873517786561,
            "recall": 0.642
        },
        "train": {
            "accuracy": 0.5927552140504939,
            "auc_prc": 0.6303216031843185,
            "auditor_fn_violation": 0.009066862672089058,
            "auditor_fp_violation": 0.022146053462782856,
            "ave_precision_score": 0.631558806629285,
            "fpr": 0.2305159165751921,
            "logloss": 1.9132109437999274,
            "mae": 0.4354259066594185,
            "precision": 0.5825049701789264,
            "recall": 0.6453744493392071
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(30)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7454854812459679,
            "auditor_fn_violation": 0.00035087719298246376,
            "auditor_fp_violation": 0.011183145971725431,
            "ave_precision_score": 0.7388248792484791,
            "fpr": 0.12171052631578948,
            "logloss": 1.089941520846549,
            "mae": 0.36397978037661105,
            "precision": 0.7701863354037267,
            "recall": 0.744
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.7137189012250996,
            "auditor_fn_violation": 0.007238983157395903,
            "auditor_fp_violation": 0.008531755086746718,
            "ave_precision_score": 0.7072064050692923,
            "fpr": 0.12733260153677278,
            "logloss": 1.0770417430261836,
            "mae": 0.348322994017444,
            "precision": 0.7531914893617021,
            "recall": 0.7797356828193832
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(31)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.8397667936740504,
            "auditor_fn_violation": 0.021875000000000002,
            "auditor_fp_violation": 0.039207545562936465,
            "ave_precision_score": 0.8377398809510777,
            "fpr": 0.21052631578947367,
            "logloss": 0.5685885261977307,
            "mae": 0.33738832324824963,
            "precision": 0.6947535771065183,
            "recall": 0.874
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8542207344472852,
            "auditor_fn_violation": 0.019502217150152084,
            "auditor_fp_violation": 0.027516831721219156,
            "ave_precision_score": 0.8519737362835638,
            "fpr": 0.21075740944017562,
            "logloss": 0.5367882515919892,
            "mae": 0.33020964672157194,
            "precision": 0.6789297658862876,
            "recall": 0.8942731277533039
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(32)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8461411952518508,
            "auditor_fn_violation": 0.013407894736842106,
            "auditor_fp_violation": 0.007270907852154661,
            "ave_precision_score": 0.8426039070812943,
            "fpr": 0.07017543859649122,
            "logloss": 0.557797879839578,
            "mae": 0.3403191058079532,
            "precision": 0.8358974358974359,
            "recall": 0.652
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8535598449528691,
            "auditor_fn_violation": 0.016888542870544542,
            "auditor_fp_violation": 0.008719107816692169,
            "ave_precision_score": 0.8492829790125213,
            "fpr": 0.06256860592755215,
            "logloss": 0.4996419912036405,
            "mae": 0.3203119937571749,
            "precision": 0.8442622950819673,
            "recall": 0.6806167400881057
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(33)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8244632918918194,
            "auditor_fn_violation": 0.005000000000000004,
            "auditor_fp_violation": 0.012944983818770227,
            "ave_precision_score": 0.8257795117449916,
            "fpr": 0.1206140350877193,
            "logloss": 0.548176670417266,
            "mae": 0.34046812554445577,
            "precision": 0.7808764940239044,
            "recall": 0.784
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8665973523022192,
            "auditor_fn_violation": 0.009593949622093164,
            "auditor_fp_violation": 0.01702267688619763,
            "ave_precision_score": 0.8668866823845475,
            "fpr": 0.11525795828759605,
            "logloss": 0.47651710464272634,
            "mae": 0.31847238240375797,
            "precision": 0.7727272727272727,
            "recall": 0.7863436123348018
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(34)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5098684210526315,
            "auc_prc": 0.6451764039344593,
            "auditor_fn_violation": 0.0015043859649122857,
            "auditor_fp_violation": 0.014193174075966616,
            "ave_precision_score": 0.647509378218897,
            "fpr": 0.0668859649122807,
            "logloss": 1.8179661861301053,
            "mae": 0.4973271871798458,
            "precision": 0.6514285714285715,
            "recall": 0.228
        },
        "train": {
            "accuracy": 0.5214050493962679,
            "auc_prc": 0.5722111644587511,
            "auditor_fn_violation": 0.0039023776940671287,
            "auditor_fp_violation": 0.013748808028304676,
            "ave_precision_score": 0.5750315211417953,
            "fpr": 0.08562019758507135,
            "logloss": 1.7493811321559898,
            "mae": 0.47488123356013423,
            "precision": 0.5517241379310345,
            "recall": 0.21145374449339208
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(35)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.7842776227828925,
            "auditor_fn_violation": 0.011184210526315798,
            "auditor_fp_violation": 0.005706012604326354,
            "ave_precision_score": 0.7851098348980488,
            "fpr": 0.09210526315789473,
            "logloss": 0.5816177838793614,
            "mae": 0.3779281307044521,
            "precision": 0.7941176470588235,
            "recall": 0.648
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.7817428706855544,
            "auditor_fn_violation": 0.01161525554045755,
            "auditor_fp_violation": 0.00649729659618521,
            "ave_precision_score": 0.7824652573019801,
            "fpr": 0.0889132821075741,
            "logloss": 0.5400005150043348,
            "mae": 0.3610351290288008,
            "precision": 0.7949367088607595,
            "recall": 0.6916299559471366
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(36)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8301362770458838,
            "auditor_fn_violation": 0.0069473684210526404,
            "auditor_fp_violation": 0.012463272866632609,
            "ave_precision_score": 0.8007289527597313,
            "fpr": 0.13486842105263158,
            "logloss": 0.5645465200039785,
            "mae": 0.33161567651928125,
            "precision": 0.7564356435643564,
            "recall": 0.764
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8397906180535701,
            "auditor_fn_violation": 0.003491346586265762,
            "auditor_fp_violation": 0.008538960960975384,
            "ave_precision_score": 0.8083092743864119,
            "fpr": 0.12294182217343579,
            "logloss": 0.49631241347164873,
            "mae": 0.3054378673896341,
            "precision": 0.7647058823529411,
            "recall": 0.801762114537445
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(37)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6162280701754386,
            "auc_prc": 0.6672266568958927,
            "auditor_fn_violation": 0.0012719298245614037,
            "auditor_fp_violation": 0.007510432634985534,
            "ave_precision_score": 0.669012833478925,
            "fpr": 0.3618421052631579,
            "logloss": 1.4103292049131115,
            "mae": 0.391349362311558,
            "precision": 0.5925925925925926,
            "recall": 0.96
        },
        "train": {
            "accuracy": 0.5982436882546652,
            "auc_prc": 0.6629960359266946,
            "auditor_fn_violation": 0.0025217967378637026,
            "auditor_fp_violation": 0.008771950894369104,
            "ave_precision_score": 0.6651014410634333,
            "fpr": 0.38309549945115257,
            "logloss": 1.2949284186092094,
            "mae": 0.4027477827569231,
            "precision": 0.5559796437659033,
            "recall": 0.9625550660792952
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(38)",
        "seed": 16695,
        "test": {
            "accuracy": 0.45394736842105265,
            "auc_prc": 0.577002113789557,
            "auditor_fn_violation": 0.009421052631578958,
            "auditor_fp_violation": 0.016857221938340997,
            "ave_precision_score": 0.5724202121151105,
            "fpr": 0.05482456140350877,
            "logloss": 1.9755752679258358,
            "mae": 0.5427612024359405,
            "precision": 0.5098039215686274,
            "recall": 0.104
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.501379381288406,
            "auditor_fn_violation": 0.003239892261493173,
            "auditor_fp_violation": 0.02116125065153113,
            "ave_precision_score": 0.530656637636573,
            "fpr": 0.07135016465422613,
            "logloss": 1.8177723789953055,
            "mae": 0.5144687376662876,
            "precision": 0.41964285714285715,
            "recall": 0.10352422907488987
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(39)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7860245558730492,
            "auditor_fn_violation": 0.00803508771929825,
            "auditor_fp_violation": 0.01394034236075626,
            "ave_precision_score": 0.7278480897755686,
            "fpr": 0.24780701754385964,
            "logloss": 3.4233930504691332,
            "mae": 0.34345956898310726,
            "precision": 0.6646884272997032,
            "recall": 0.896
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.808433346604964,
            "auditor_fn_violation": 0.0024710223069000043,
            "auditor_fp_violation": 0.012951357946998388,
            "ave_precision_score": 0.7533450969473873,
            "fpr": 0.2327113062568606,
            "logloss": 2.9138701011695476,
            "mae": 0.32702003802969215,
            "precision": 0.6661417322834645,
            "recall": 0.9317180616740088
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(40)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5471491228070176,
            "auc_prc": 0.7326694077852794,
            "auditor_fn_violation": 0.018945175438596494,
            "auditor_fp_violation": 0.01769289729177314,
            "ave_precision_score": 0.7332282555957617,
            "fpr": 0.41228070175438597,
            "logloss": 0.7306245074985803,
            "mae": 0.44490896121255663,
            "precision": 0.5518474374255066,
            "recall": 0.926
        },
        "train": {
            "accuracy": 0.5181119648737651,
            "auc_prc": 0.7300668163043975,
            "auditor_fn_violation": 0.014888997422593172,
            "auditor_fp_violation": 0.018262087253529073,
            "ave_precision_score": 0.7305715445033834,
            "fpr": 0.4500548847420417,
            "logloss": 0.7571897589520218,
            "mae": 0.45837910694165235,
            "precision": 0.5089820359281437,
            "recall": 0.9361233480176211
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(41)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.8175273350738284,
            "auditor_fn_violation": 0.021956140350877194,
            "auditor_fp_violation": 0.017314980412195542,
            "ave_precision_score": 0.7927829827349218,
            "fpr": 0.13815789473684212,
            "logloss": 0.5969324239950158,
            "mae": 0.34957612638271185,
            "precision": 0.7364016736401674,
            "recall": 0.704
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8204775837181641,
            "auditor_fn_violation": 0.012930555085421938,
            "auditor_fp_violation": 0.0150770908444564,
            "ave_precision_score": 0.7984208483688805,
            "fpr": 0.12952799121844127,
            "logloss": 0.5353722404798624,
            "mae": 0.3218855713339658,
            "precision": 0.7417943107221007,
            "recall": 0.7466960352422908
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(42)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8560531163806724,
            "auditor_fn_violation": 0.002228070175438594,
            "auditor_fp_violation": 0.008803866462272188,
            "ave_precision_score": 0.8502677229366473,
            "fpr": 0.08881578947368421,
            "logloss": 0.5479187726593591,
            "mae": 0.32028172527881044,
            "precision": 0.8111888111888111,
            "recall": 0.696
        },
        "train": {
            "accuracy": 0.7947310647639956,
            "auc_prc": 0.8683768251727977,
            "auditor_fn_violation": 0.006736074507850697,
            "auditor_fp_violation": 0.00976155762177327,
            "ave_precision_score": 0.8603532195484305,
            "fpr": 0.07354555433589462,
            "logloss": 0.4639855499879491,
            "mae": 0.28813187110224486,
            "precision": 0.8329177057356608,
            "recall": 0.73568281938326
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(43)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8301897904901758,
            "auditor_fn_violation": 0.02992982456140352,
            "auditor_fp_violation": 0.029488162152955202,
            "ave_precision_score": 0.8304940044064508,
            "fpr": 0.11842105263157894,
            "logloss": 0.5505022645735766,
            "mae": 0.3435833126876101,
            "precision": 0.7697228144989339,
            "recall": 0.722
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8453904132699057,
            "auditor_fn_violation": 0.028044410702282915,
            "auditor_fp_violation": 0.020728898197810857,
            "ave_precision_score": 0.8457790666873436,
            "fpr": 0.10757409440175632,
            "logloss": 0.5037328262887789,
            "mae": 0.32583405167311913,
            "precision": 0.7715617715617715,
            "recall": 0.7290748898678414
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(44)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8355472695051933,
            "auditor_fn_violation": 0.004802631578947371,
            "auditor_fp_violation": 0.007366717765287004,
            "ave_precision_score": 0.8367809844454992,
            "fpr": 0.08114035087719298,
            "logloss": 0.5463036947027438,
            "mae": 0.33271253514185284,
            "precision": 0.8186274509803921,
            "recall": 0.668
        },
        "train": {
            "accuracy": 0.7870472008781558,
            "auc_prc": 0.8711972558401007,
            "auditor_fn_violation": 0.010043666010628779,
            "auditor_fp_violation": 0.007609403185476804,
            "ave_precision_score": 0.8713823570013524,
            "fpr": 0.06805708013172337,
            "logloss": 0.46697828342196096,
            "mae": 0.2989968841774829,
            "precision": 0.8385416666666666,
            "recall": 0.7092511013215859
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(45)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5383771929824561,
            "auc_prc": 0.43616357215262863,
            "auditor_fn_violation": 0.012609649122807024,
            "auditor_fp_violation": 0.025128811105433493,
            "ave_precision_score": 0.5755788937986774,
            "fpr": 0.1875,
            "logloss": 0.72695182903882,
            "mae": 0.4927678951634127,
            "precision": 0.5938242280285035,
            "recall": 0.5
        },
        "train": {
            "accuracy": 0.5631174533479693,
            "auc_prc": 0.6649797768084494,
            "auditor_fn_violation": 0.01307562488817536,
            "auditor_fp_violation": 0.02001551664917241,
            "ave_precision_score": 0.5428382988713646,
            "fpr": 0.19319429198682767,
            "logloss": 0.7004071209589787,
            "mae": 0.4802838927451787,
            "precision": 0.5686274509803921,
            "recall": 0.5110132158590308
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(46)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.721521566456408,
            "auditor_fn_violation": 0.00630482456140351,
            "auditor_fp_violation": 0.014134623573496855,
            "ave_precision_score": 0.5775437108438586,
            "fpr": 0.3475877192982456,
            "logloss": 0.696858783889003,
            "mae": 0.486558233959633,
            "precision": 0.5727762803234502,
            "recall": 0.85
        },
        "train": {
            "accuracy": 0.5378704720087816,
            "auc_prc": 0.7330997848252406,
            "auditor_fn_violation": 0.009129726253282207,
            "auditor_fp_violation": 0.006675041493825769,
            "ave_precision_score": 0.5305275899510207,
            "fpr": 0.39626783754116357,
            "logloss": 0.6875935675451573,
            "mae": 0.4846389807768894,
            "precision": 0.5218543046357615,
            "recall": 0.8678414096916299
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(47)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.7174078111521831,
            "auditor_fn_violation": 0.012500000000000013,
            "auditor_fp_violation": 0.010975557826605352,
            "ave_precision_score": 0.6828913492252345,
            "fpr": 0.12280701754385964,
            "logloss": 0.662823001880635,
            "mae": 0.43346174074369564,
            "precision": 0.6989247311827957,
            "recall": 0.52
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.7528332750711813,
            "auditor_fn_violation": 0.006890815630787686,
            "auditor_fp_violation": 0.00575028763447963,
            "ave_precision_score": 0.7073836137436156,
            "fpr": 0.10428100987925357,
            "logloss": 0.607292849891394,
            "mae": 0.4107763479360503,
            "precision": 0.7277936962750716,
            "recall": 0.5594713656387665
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(48)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8462247920797382,
            "auditor_fn_violation": 0.002061403508771934,
            "auditor_fp_violation": 0.013253704649974456,
            "ave_precision_score": 0.8464940700869235,
            "fpr": 0.125,
            "logloss": 0.5513903222131231,
            "mae": 0.3182635925980764,
            "precision": 0.7790697674418605,
            "recall": 0.804
        },
        "train": {
            "accuracy": 0.7848518111964874,
            "auc_prc": 0.8677558392656142,
            "auditor_fn_violation": 0.013472149015701387,
            "auditor_fp_violation": 0.015968217290735415,
            "ave_precision_score": 0.8681034775012932,
            "fpr": 0.12403951701427003,
            "logloss": 0.4615370986632324,
            "mae": 0.29712051364800734,
            "precision": 0.7665289256198347,
            "recall": 0.8171806167400881
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(49)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.718886895092786,
            "auditor_fn_violation": 0.011622807017543863,
            "auditor_fp_violation": 0.017554505195026406,
            "ave_precision_score": 0.7205612560856286,
            "fpr": 0.2149122807017544,
            "logloss": 0.6980821454317948,
            "mae": 0.39669028292164993,
            "precision": 0.6655290102389079,
            "recall": 0.78
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7408615602521806,
            "auditor_fn_violation": 0.007618582474600698,
            "auditor_fp_violation": 0.023286983548989143,
            "ave_precision_score": 0.7413797629491717,
            "fpr": 0.20417124039517015,
            "logloss": 0.5925329242967413,
            "mae": 0.35759776270857363,
            "precision": 0.6684491978609626,
            "recall": 0.8259911894273128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(50)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.7489562289807105,
            "auditor_fn_violation": 0.012576754385964924,
            "auditor_fp_violation": 0.017110053653551355,
            "ave_precision_score": 0.7497352895041407,
            "fpr": 0.11732456140350878,
            "logloss": 0.9377955359742483,
            "mae": 0.37185888360499786,
            "precision": 0.7377450980392157,
            "recall": 0.602
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7609989471016559,
            "auditor_fn_violation": 0.011218731412931523,
            "auditor_fp_violation": 0.014872924407977387,
            "ave_precision_score": 0.7619928054203431,
            "fpr": 0.09549945115257959,
            "logloss": 0.7108487705341494,
            "mae": 0.334887963143653,
            "precision": 0.7616438356164383,
            "recall": 0.6123348017621145
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(51)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.7164779463440327,
            "auditor_fn_violation": 0.04241447368421054,
            "auditor_fp_violation": 0.08107913898824731,
            "ave_precision_score": 0.7046737554502113,
            "fpr": 0.2817982456140351,
            "logloss": 0.6751799142638286,
            "mae": 0.4017669933913439,
            "precision": 0.6164179104477612,
            "recall": 0.826
        },
        "train": {
            "accuracy": 0.5960482985729967,
            "auc_prc": 0.6729443026473332,
            "auditor_fn_violation": 0.03316779256952471,
            "auditor_fp_violation": 0.08258172061864834,
            "ave_precision_score": 0.662051838674567,
            "fpr": 0.3205268935236004,
            "logloss": 0.7020341100551609,
            "mae": 0.4140554008888884,
            "precision": 0.564179104477612,
            "recall": 0.8325991189427313
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(52)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5921052631578947,
            "auc_prc": 0.7256817259272369,
            "auditor_fn_violation": 0.00853947368421055,
            "auditor_fp_violation": 0.007733989098960995,
            "ave_precision_score": 0.7237295866057321,
            "fpr": 0.025219298245614034,
            "logloss": 0.7663369165145338,
            "mae": 0.45714251413581924,
            "precision": 0.867816091954023,
            "recall": 0.302
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.7490155684512659,
            "auditor_fn_violation": 0.008602639303278103,
            "auditor_fp_violation": 0.0022938699627936703,
            "ave_precision_score": 0.7381321930719765,
            "fpr": 0.030735455543358946,
            "logloss": 0.6937417131137826,
            "mae": 0.42567143271933794,
            "precision": 0.8486486486486486,
            "recall": 0.3458149779735683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(53)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8093042864325197,
            "auditor_fn_violation": 0.02707017543859649,
            "auditor_fp_violation": 0.02345213762561744,
            "ave_precision_score": 0.8095765676150385,
            "fpr": 0.12828947368421054,
            "logloss": 0.7757148446510139,
            "mae": 0.31760899170847584,
            "precision": 0.7505330490405118,
            "recall": 0.704
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8362332742228372,
            "auditor_fn_violation": 0.01873092936551305,
            "auditor_fp_violation": 0.02813173298873242,
            "ave_precision_score": 0.8366777563984895,
            "fpr": 0.11855104281009879,
            "logloss": 0.6245444907981021,
            "mae": 0.28404088905444014,
            "precision": 0.7528604118993135,
            "recall": 0.724669603524229
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(54)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8574822792633718,
            "auditor_fn_violation": 0.0141140350877193,
            "auditor_fp_violation": 0.01779935275080906,
            "ave_precision_score": 0.8342452224789481,
            "fpr": 0.13596491228070176,
            "logloss": 0.5252654694279584,
            "mae": 0.3068686144246736,
            "precision": 0.7651515151515151,
            "recall": 0.808
        },
        "train": {
            "accuracy": 0.7870472008781558,
            "auc_prc": 0.8771727902297832,
            "auditor_fn_violation": 0.009231275115209605,
            "auditor_fp_violation": 0.0152188063709536,
            "ave_precision_score": 0.8592894035502268,
            "fpr": 0.1350164654226125,
            "logloss": 0.46459480646182805,
            "mae": 0.2868246681309427,
            "precision": 0.7569169960474308,
            "recall": 0.8436123348017621
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(55)",
        "seed": 16695,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7921468634099026,
            "auditor_fn_violation": 0.018552631578947373,
            "auditor_fp_violation": 0.01871486969851814,
            "ave_precision_score": 0.742045996852175,
            "fpr": 0.1337719298245614,
            "logloss": 0.6634290165874211,
            "mae": 0.40124058352554576,
            "precision": 0.7376344086021506,
            "recall": 0.686
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8244925084914062,
            "auditor_fn_violation": 0.011864292035184265,
            "auditor_fp_violation": 0.016366942331388547,
            "ave_precision_score": 0.7623702097016941,
            "fpr": 0.11745334796926454,
            "logloss": 0.5749642238240266,
            "mae": 0.36285838305065515,
            "precision": 0.7534562211981567,
            "recall": 0.7202643171806168
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(56)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5614035087719298,
            "auc_prc": 0.6081305350445314,
            "auditor_fn_violation": 0.028061403508771936,
            "auditor_fp_violation": 0.023646418838358038,
            "ave_precision_score": 0.5989797832050455,
            "fpr": 0.15679824561403508,
            "logloss": 0.8361549102421778,
            "mae": 0.49317420378588794,
            "precision": 0.6295336787564767,
            "recall": 0.486
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.5433195233390137,
            "auditor_fn_violation": 0.03900443430030417,
            "auditor_fp_violation": 0.01200018254881379,
            "ave_precision_score": 0.544269691365042,
            "fpr": 0.18990120746432493,
            "logloss": 0.7698300037001876,
            "mae": 0.47779347456819926,
            "precision": 0.5770171149144254,
            "recall": 0.5198237885462555
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(57)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5482456140350878,
            "auc_prc": 0.7741228070175439,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5482456140350878,
            "fpr": 0.4517543859649123,
            "logloss": 0.691866578963284,
            "mae": 0.49930836286461144,
            "precision": 0.5482456140350878,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.4983534577387486,
            "auc_prc": 0.7491767288693743,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.4983534577387486,
            "fpr": 0.5016465422612514,
            "logloss": 0.6932971600822998,
            "mae": 0.5000236044207467,
            "precision": 0.4983534577387486,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(58)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.6919666299394136,
            "auditor_fn_violation": 0.009758771929824564,
            "auditor_fp_violation": 0.008516436722875149,
            "ave_precision_score": 0.6736116105503898,
            "fpr": 0.24342105263157895,
            "logloss": 2.0556663284380217,
            "mae": 0.36519708745222407,
            "precision": 0.653125,
            "recall": 0.836
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.6793354833882504,
            "auditor_fn_violation": 0.006987528832623301,
            "auditor_fp_violation": 0.022696101862238104,
            "ave_precision_score": 0.6576555587376129,
            "fpr": 0.2535675082327113,
            "logloss": 2.130591929940184,
            "mae": 0.36073423109084274,
            "precision": 0.6268174474959612,
            "recall": 0.8546255506607929
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(59)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7552561163570541,
            "auditor_fn_violation": 0.01243421052631579,
            "auditor_fp_violation": 0.028987821495486288,
            "ave_precision_score": 0.7559962487585323,
            "fpr": 0.2543859649122807,
            "logloss": 1.350657552012483,
            "mae": 0.3282776647823402,
            "precision": 0.6537313432835821,
            "recall": 0.876
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7421074211198697,
            "auditor_fn_violation": 0.012284994463169194,
            "auditor_fp_violation": 0.022710513610695454,
            "ave_precision_score": 0.7423584966576968,
            "fpr": 0.2711306256860593,
            "logloss": 1.3194710017460092,
            "mae": 0.331970849577914,
            "precision": 0.6223241590214067,
            "recall": 0.8964757709251101
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(60)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.8088319094861192,
            "auditor_fn_violation": 0.004399122807017555,
            "auditor_fp_violation": 0.011443961846363484,
            "ave_precision_score": 0.8067138120903102,
            "fpr": 0.07675438596491228,
            "logloss": 0.5841710016696144,
            "mae": 0.39303852848120424,
            "precision": 0.8097826086956522,
            "recall": 0.596
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7787288269784574,
            "auditor_fn_violation": 0.005244273369536313,
            "auditor_fp_violation": 0.008795970475131329,
            "ave_precision_score": 0.7767752640453276,
            "fpr": 0.08781558726673985,
            "logloss": 0.5628476454547987,
            "mae": 0.3789889671963473,
            "precision": 0.778393351800554,
            "recall": 0.6189427312775331
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(61)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6567982456140351,
            "auc_prc": 0.7975701789762799,
            "auditor_fn_violation": 0.0006228070175438641,
            "auditor_fp_violation": 0.007664793050587639,
            "ave_precision_score": 0.756395819123403,
            "fpr": 0.09978070175438597,
            "logloss": 0.6203544219661898,
            "mae": 0.38319091829716373,
            "precision": 0.7533875338753387,
            "recall": 0.556
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.8130856972226905,
            "auditor_fn_violation": 0.00037234582706713293,
            "auditor_fp_violation": 0.011906506183841066,
            "ave_precision_score": 0.7671285324683818,
            "fpr": 0.09330406147091108,
            "logloss": 0.5623760914906344,
            "mae": 0.3608153987140216,
            "precision": 0.7632311977715878,
            "recall": 0.6035242290748899
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(62)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.7687715077380773,
            "auditor_fn_violation": 0.03021052631578948,
            "auditor_fp_violation": 0.007952222789984672,
            "ave_precision_score": 0.7683722942715268,
            "fpr": 0.03837719298245614,
            "logloss": 1.5205604034532552,
            "mae": 0.38538397738257635,
            "precision": 0.8553719008264463,
            "recall": 0.414
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.7711848930720537,
            "auditor_fn_violation": 0.025805500079788397,
            "auditor_fp_violation": 0.011147487431754366,
            "ave_precision_score": 0.7707917521617318,
            "fpr": 0.03951701427003293,
            "logloss": 1.2624109535591532,
            "mae": 0.3507726857666691,
            "precision": 0.8481012658227848,
            "recall": 0.44273127753303965
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(63)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8440344222255256,
            "auditor_fn_violation": 0.014491228070175449,
            "auditor_fp_violation": 0.011111288536876174,
            "ave_precision_score": 0.8420796399735269,
            "fpr": 0.08223684210526316,
            "logloss": 0.5675886505971877,
            "mae": 0.33501036416865854,
            "precision": 0.8106060606060606,
            "recall": 0.642
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.87342721430817,
            "auditor_fn_violation": 0.009676155843653438,
            "auditor_fp_violation": 0.009161068102717338,
            "ave_precision_score": 0.8713793139011669,
            "fpr": 0.06147091108671789,
            "logloss": 0.4655765635758079,
            "mae": 0.2960115883317413,
            "precision": 0.8478260869565217,
            "recall": 0.6872246696035242
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(64)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8249756858459081,
            "auditor_fn_violation": 0.0105921052631579,
            "auditor_fp_violation": 0.023319068301822524,
            "ave_precision_score": 0.7864175356035388,
            "fpr": 0.1513157894736842,
            "logloss": 0.5717179430745363,
            "mae": 0.33780751760901984,
            "precision": 0.7386363636363636,
            "recall": 0.78
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8090233260353312,
            "auditor_fn_violation": 0.013943625874650025,
            "auditor_fp_violation": 0.021581593314870282,
            "ave_precision_score": 0.7642580765841598,
            "fpr": 0.14928649835345773,
            "logloss": 0.5303711107596356,
            "mae": 0.3225010336345035,
            "precision": 0.7274549098196392,
            "recall": 0.7995594713656388
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(65)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5668859649122807,
            "auc_prc": 0.6044827432556055,
            "auditor_fn_violation": 0.015991228070175445,
            "auditor_fp_violation": 0.022148058252427195,
            "ave_precision_score": 0.6059514992668208,
            "fpr": 0.18311403508771928,
            "logloss": 0.8415491503396385,
            "mae": 0.4636631938268485,
            "precision": 0.6195899772209568,
            "recall": 0.544
        },
        "train": {
            "accuracy": 0.5642151481888035,
            "auc_prc": 0.5503095347470572,
            "auditor_fn_violation": 0.021102820640531542,
            "auditor_fp_violation": 0.01879772390452698,
            "ave_precision_score": 0.5521198748856504,
            "fpr": 0.19319429198682767,
            "logloss": 0.8211359387348371,
            "mae": 0.4626466582234489,
            "precision": 0.5696821515892421,
            "recall": 0.513215859030837
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(66)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.693501055649985,
            "mae": 0.5001697316112226,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.6928757487306401,
            "mae": 0.49985700646163866,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(67)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.6361970899975997,
            "auditor_fn_violation": 0.006162280701754393,
            "auditor_fp_violation": 0.004468467893033555,
            "ave_precision_score": 0.6311801756116748,
            "fpr": 0.027412280701754384,
            "logloss": 0.7387957717818999,
            "mae": 0.49053766614381683,
            "precision": 0.6527777777777778,
            "recall": 0.094
        },
        "train": {
            "accuracy": 0.5301866081229418,
            "auc_prc": 0.6167016960488055,
            "auditor_fn_violation": 0.008148087254650709,
            "auditor_fp_violation": 0.007032933247183104,
            "ave_precision_score": 0.6125504312782795,
            "fpr": 0.03293084522502744,
            "logloss": 0.6914539659549286,
            "mae": 0.46885350426714195,
            "precision": 0.6511627906976745,
            "recall": 0.12334801762114538
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(68)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4550438596491228,
            "auc_prc": 0.5387298519761193,
            "auditor_fn_violation": 0.008910087719298243,
            "auditor_fp_violation": 0.007521078180889116,
            "ave_precision_score": 0.540819503273436,
            "fpr": 0.05043859649122807,
            "logloss": 0.7046935978522084,
            "mae": 0.5035835556163076,
            "precision": 0.5157894736842106,
            "recall": 0.098
        },
        "train": {
            "accuracy": 0.4983534577387486,
            "auc_prc": 0.4963341524503197,
            "auditor_fn_violation": 0.005691571928026038,
            "auditor_fp_violation": 0.014402140625037531,
            "ave_precision_score": 0.4983324130087293,
            "fpr": 0.052689352360043906,
            "logloss": 0.6957712730946584,
            "mae": 0.49916244635990237,
            "precision": 0.4838709677419355,
            "recall": 0.09911894273127753
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(69)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7046777874944563,
            "auditor_fn_violation": 0.011278508771929827,
            "auditor_fp_violation": 0.0033427014137284985,
            "ave_precision_score": 0.6976328520763801,
            "fpr": 0.13157894736842105,
            "logloss": 1.9276775426475625,
            "mae": 0.3683643588491684,
            "precision": 0.7430406852248393,
            "recall": 0.694
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.6689597146484597,
            "auditor_fn_violation": 0.01943210007882126,
            "auditor_fp_violation": 0.014476601325400464,
            "ave_precision_score": 0.6583429719779761,
            "fpr": 0.12733260153677278,
            "logloss": 2.182194945092726,
            "mae": 0.355908803874577,
            "precision": 0.7381489841986456,
            "recall": 0.7202643171806168
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(70)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6096491228070176,
            "auc_prc": 0.6950670912493818,
            "auditor_fn_violation": 0.0111622807017544,
            "auditor_fp_violation": 0.018358243910747755,
            "ave_precision_score": 0.6941409564107712,
            "fpr": 0.13815789473684212,
            "logloss": 6.532010401055753,
            "mae": 0.39452773810206293,
            "precision": 0.6818181818181818,
            "recall": 0.54
        },
        "train": {
            "accuracy": 0.6553238199780461,
            "auc_prc": 0.6870546685410295,
            "auditor_fn_violation": 0.01767191980541304,
            "auditor_fp_violation": 0.024725756436647157,
            "ave_precision_score": 0.684852373100368,
            "fpr": 0.1394072447859495,
            "logloss": 5.465700810846337,
            "mae": 0.34870419579696016,
            "precision": 0.6776649746192893,
            "recall": 0.5881057268722467
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(71)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.7444981143711793,
            "auditor_fn_violation": 0.015394736842105274,
            "auditor_fp_violation": 0.021708929483903936,
            "ave_precision_score": 0.7375886708980558,
            "fpr": 0.16557017543859648,
            "logloss": 0.8133683570778439,
            "mae": 0.35493903503785923,
            "precision": 0.7015810276679841,
            "recall": 0.71
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7428236834016049,
            "auditor_fn_violation": 0.006271851139039743,
            "auditor_fp_violation": 0.02586668652285343,
            "ave_precision_score": 0.7347363776955332,
            "fpr": 0.17014270032930845,
            "logloss": 0.7389106041610456,
            "mae": 0.3333602052480473,
            "precision": 0.6777546777546778,
            "recall": 0.7180616740088106
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(72)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7695978216303512,
            "auditor_fn_violation": 0.020964912280701763,
            "auditor_fp_violation": 0.014446005791176973,
            "ave_precision_score": 0.7701850917042373,
            "fpr": 0.15460526315789475,
            "logloss": 1.0973275086280234,
            "mae": 0.34198017083182375,
            "precision": 0.718562874251497,
            "recall": 0.72
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.7825724693983833,
            "auditor_fn_violation": 0.011610419880365775,
            "auditor_fp_violation": 0.012240378356436167,
            "ave_precision_score": 0.7830774079180385,
            "fpr": 0.14818880351262348,
            "logloss": 0.973979577222486,
            "mae": 0.3064562304235774,
            "precision": 0.7204968944099379,
            "recall": 0.7665198237885462
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(73)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.7910501414204664,
            "auditor_fn_violation": 0.0395,
            "auditor_fp_violation": 0.048069962527678416,
            "ave_precision_score": 0.780052089021104,
            "fpr": 0.1337719298245614,
            "logloss": 0.6082635833698842,
            "mae": 0.3862121833259599,
            "precision": 0.7220956719817767,
            "recall": 0.634
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.7842751365436303,
            "auditor_fn_violation": 0.025421065102491817,
            "auditor_fp_violation": 0.05863900251485011,
            "ave_precision_score": 0.7707026974043616,
            "fpr": 0.13062568605927552,
            "logloss": 0.5831613840369754,
            "mae": 0.37455309482555094,
            "precision": 0.7104622871046229,
            "recall": 0.6431718061674009
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(74)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8062399321109699,
            "auditor_fn_violation": 0.028885964912280713,
            "auditor_fp_violation": 0.017267075455629367,
            "ave_precision_score": 0.8075803626578955,
            "fpr": 0.07456140350877193,
            "logloss": 1.2247397454663203,
            "mae": 0.33345509672581053,
            "precision": 0.8210526315789474,
            "recall": 0.624
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.7968449017807868,
            "auditor_fn_violation": 0.022074788318979487,
            "auditor_fp_violation": 0.016350128624854982,
            "ave_precision_score": 0.7973897945771377,
            "fpr": 0.07574094401756312,
            "logloss": 1.0455306817944703,
            "mae": 0.30636178498391303,
            "precision": 0.8140161725067385,
            "recall": 0.6651982378854625
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(75)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4517543859649123,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.9357326726484,
            "mae": 0.5482456140350878,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.5016465422612514,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.21251864246921,
            "mae": 0.4983534577387486,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(76)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6151315789473685,
            "auc_prc": 0.7535657292629631,
            "auditor_fn_violation": 0.016600877192982456,
            "auditor_fp_violation": 0.048860394311020273,
            "ave_precision_score": 0.6136264774188361,
            "fpr": 0.2817982456140351,
            "logloss": 0.669552246473883,
            "mae": 0.4698375861993746,
            "precision": 0.6123680241327301,
            "recall": 0.812
        },
        "train": {
            "accuracy": 0.6037321624588364,
            "auc_prc": 0.7360279437624305,
            "auditor_fn_violation": 0.01849881768110756,
            "auditor_fp_violation": 0.05080141331213208,
            "ave_precision_score": 0.5862592680379036,
            "fpr": 0.300768386388584,
            "logloss": 0.6654999139690176,
            "mae": 0.4671414373636769,
            "precision": 0.5725429017160687,
            "recall": 0.8083700440528634
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(77)",
        "seed": 16695,
        "test": {
            "accuracy": 0.4868421052631579,
            "auc_prc": 0.604330078536792,
            "auditor_fn_violation": 0.011414473684210556,
            "auditor_fp_violation": 0.006610884006131839,
            "ave_precision_score": 0.5586058707236445,
            "fpr": 0.06907894736842106,
            "logloss": 0.8077096172252447,
            "mae": 0.5093416701348727,
            "precision": 0.6012658227848101,
            "recall": 0.19
        },
        "train": {
            "accuracy": 0.5192096597145993,
            "auc_prc": 0.5752523397785888,
            "auditor_fn_violation": 0.0016489600912972672,
            "auditor_fp_violation": 0.008702294110158602,
            "ave_precision_score": 0.5353502285825151,
            "fpr": 0.08342480790340286,
            "logloss": 0.733267089499175,
            "mae": 0.4840590343631714,
            "precision": 0.5476190476190477,
            "recall": 0.2026431718061674
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(78)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5592105263157895,
            "auc_prc": 0.645614521331588,
            "auditor_fn_violation": 0.007234649122807022,
            "auditor_fp_violation": 0.002256855731561916,
            "ave_precision_score": 0.6377407368893302,
            "fpr": 0.03399122807017544,
            "logloss": 12.049270754661492,
            "mae": 0.4754789670692305,
            "precision": 0.80625,
            "recall": 0.258
        },
        "train": {
            "accuracy": 0.605927552140505,
            "auc_prc": 0.6148565352576292,
            "auditor_fn_violation": 0.00595753323307398,
            "auditor_fp_violation": 0.0015396551268594154,
            "ave_precision_score": 0.6176435310714131,
            "fpr": 0.02854006586169045,
            "logloss": 11.031685440604498,
            "mae": 0.4371586131297285,
            "precision": 0.8231292517006803,
            "recall": 0.2665198237885463
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(79)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7997657390277817,
            "auditor_fn_violation": 0.0168421052631579,
            "auditor_fp_violation": 0.012732072900698357,
            "ave_precision_score": 0.8001240339269515,
            "fpr": 0.13048245614035087,
            "logloss": 0.9869478211287772,
            "mae": 0.3093980181335535,
            "precision": 0.7429805615550756,
            "recall": 0.688
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.7820994545993887,
            "auditor_fn_violation": 0.009811554326223305,
            "auditor_fp_violation": 0.013792043273676701,
            "ave_precision_score": 0.7824786054424937,
            "fpr": 0.12294182217343579,
            "logloss": 0.8859312960701086,
            "mae": 0.2732096735486231,
            "precision": 0.7471783295711061,
            "recall": 0.7290748898678414
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(80)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.8363738251578354,
            "auditor_fn_violation": 0.011820175438596493,
            "auditor_fp_violation": 0.022973088059955727,
            "ave_precision_score": 0.8366056894935113,
            "fpr": 0.18640350877192982,
            "logloss": 0.7189679868267631,
            "mae": 0.3056172238561486,
            "precision": 0.7043478260869566,
            "recall": 0.81
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.858193707266966,
            "auditor_fn_violation": 0.007161612595927407,
            "auditor_fp_violation": 0.017700029063692724,
            "ave_precision_score": 0.8586268732585434,
            "fpr": 0.17892425905598244,
            "logloss": 0.5926116945320657,
            "mae": 0.2750295251660288,
            "precision": 0.7041742286751361,
            "recall": 0.8546255506607929
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(81)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.786721860384517,
            "auditor_fn_violation": 0.030146929824561415,
            "auditor_fp_violation": 0.021259155169477094,
            "ave_precision_score": 0.7871034961570128,
            "fpr": 0.07894736842105263,
            "logloss": 1.3649952341873877,
            "mae": 0.3742144080024487,
            "precision": 0.806970509383378,
            "recall": 0.602
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7950511340890205,
            "auditor_fn_violation": 0.03693960744111375,
            "auditor_fp_violation": 0.020762525610877988,
            "ave_precision_score": 0.7945141110417294,
            "fpr": 0.08232711306256861,
            "logloss": 1.153200301847088,
            "mae": 0.3358313937965709,
            "precision": 0.7916666666666666,
            "recall": 0.6277533039647577
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(82)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.809334740555946,
            "auditor_fn_violation": 0.007313596491228073,
            "auditor_fp_violation": 0.014078734457502984,
            "ave_precision_score": 0.8047438844235619,
            "fpr": 0.15021929824561403,
            "logloss": 0.5677299384294512,
            "mae": 0.3440849363156431,
            "precision": 0.740530303030303,
            "recall": 0.782
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8434096941380994,
            "auditor_fn_violation": 0.008041702732631521,
            "auditor_fp_violation": 0.019030713837920668,
            "ave_precision_score": 0.8326782185595666,
            "fpr": 0.1437980241492865,
            "logloss": 0.4920157199028365,
            "mae": 0.3154627633353636,
            "precision": 0.741106719367589,
            "recall": 0.8259911894273128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(83)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.7607787342260527,
            "auditor_fn_violation": 0.004269736842105269,
            "auditor_fp_violation": 0.009751320047692049,
            "ave_precision_score": 0.7622791316850529,
            "fpr": 0.10087719298245613,
            "logloss": 0.6062492013777541,
            "mae": 0.379600545170864,
            "precision": 0.7969094922737306,
            "recall": 0.722
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.7998765955536039,
            "auditor_fn_violation": 0.002538721548184943,
            "auditor_fp_violation": 0.013285230119593492,
            "ave_precision_score": 0.800489400270632,
            "fpr": 0.09659714599341383,
            "logloss": 0.5429436250932163,
            "mae": 0.35244295539172893,
            "precision": 0.7939110070257611,
            "recall": 0.7466960352422908
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(84)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8586813624578133,
            "auditor_fn_violation": 0.01251535087719298,
            "auditor_fp_violation": 0.010054718105944476,
            "ave_precision_score": 0.8363462982813501,
            "fpr": 0.06907894736842106,
            "logloss": 0.5461677635618252,
            "mae": 0.3191691628134434,
            "precision": 0.8342105263157895,
            "recall": 0.634
        },
        "train": {
            "accuracy": 0.7859495060373216,
            "auc_prc": 0.8682243314194078,
            "auditor_fn_violation": 0.018820389077210987,
            "auditor_fp_violation": 0.011575035969322193,
            "ave_precision_score": 0.8456136317160812,
            "fpr": 0.048298572996706916,
            "logloss": 0.46995778856089426,
            "mae": 0.29134629549853236,
            "precision": 0.8731988472622478,
            "recall": 0.6674008810572687
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(85)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5668859649122807,
            "auc_prc": 0.627090271147893,
            "auditor_fn_violation": 0.034596491228070174,
            "auditor_fp_violation": 0.04718105944472833,
            "ave_precision_score": 0.628084691566165,
            "fpr": 0.15679824561403508,
            "logloss": 5.386314216530981,
            "mae": 0.44900820888617016,
            "precision": 0.6342710997442456,
            "recall": 0.496
        },
        "train": {
            "accuracy": 0.5938529088913282,
            "auc_prc": 0.6163494390910287,
            "auditor_fn_violation": 0.030793483464460324,
            "auditor_fp_violation": 0.05206003934407329,
            "ave_precision_score": 0.6173182883270982,
            "fpr": 0.15587266739846323,
            "logloss": 4.758017912401676,
            "mae": 0.42873633414460915,
            "precision": 0.6141304347826086,
            "recall": 0.4977973568281938
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(86)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.838384376926931,
            "auditor_fn_violation": 0.01222587719298246,
            "auditor_fp_violation": 0.002307422074603994,
            "ave_precision_score": 0.8284928837044242,
            "fpr": 0.1787280701754386,
            "logloss": 0.5514209144875536,
            "mae": 0.35482544456394044,
            "precision": 0.7227891156462585,
            "recall": 0.85
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8581141141124604,
            "auditor_fn_violation": 0.0055078168445383624,
            "auditor_fp_violation": 0.007928863609614561,
            "ave_precision_score": 0.8488193287133634,
            "fpr": 0.1756311745334797,
            "logloss": 0.4794209504633676,
            "mae": 0.32771539372455205,
            "precision": 0.7137745974955277,
            "recall": 0.8788546255506607
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(87)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8504334063974048,
            "auditor_fn_violation": 0.008574561403508775,
            "auditor_fp_violation": 0.023505365355135418,
            "ave_precision_score": 0.8196856623096591,
            "fpr": 0.16228070175438597,
            "logloss": 0.549763652140518,
            "mae": 0.31518617913682473,
            "precision": 0.7328519855595668,
            "recall": 0.812
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8631022922955881,
            "auditor_fn_violation": 0.006083260395460286,
            "auditor_fp_violation": 0.02109879974154931,
            "ave_precision_score": 0.8286806724811433,
            "fpr": 0.15697036223929747,
            "logloss": 0.4700207766785285,
            "mae": 0.28551225597703367,
            "precision": 0.7306967984934086,
            "recall": 0.8546255506607929
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(88)",
        "seed": 16695,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.713347727890057,
            "auditor_fn_violation": 0.016732456140350883,
            "auditor_fp_violation": 0.02810424118548799,
            "ave_precision_score": 0.6918846098445736,
            "fpr": 0.22916666666666666,
            "logloss": 2.180921221913155,
            "mae": 0.35836690587549175,
            "precision": 0.6623586429725363,
            "recall": 0.82
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.6883439733405293,
            "auditor_fn_violation": 0.01761147405426578,
            "auditor_fp_violation": 0.018190028511242365,
            "ave_precision_score": 0.6653164086601262,
            "fpr": 0.2491767288693743,
            "logloss": 2.175627762713249,
            "mae": 0.36394606251729616,
            "precision": 0.6296900489396411,
            "recall": 0.8502202643171806
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(89)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5482456140350878,
            "auc_prc": 0.4923154614646219,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5383372325753927,
            "fpr": 0.4517543859649123,
            "logloss": 0.7140325424163482,
            "mae": 0.48638186989384785,
            "precision": 0.5482456140350878,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.4983534577387486,
            "auc_prc": 0.4755944195592067,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.514084835569215,
            "fpr": 0.5016465422612514,
            "logloss": 0.7419608720884011,
            "mae": 0.5002555963629295,
            "precision": 0.4983534577387486,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(90)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5844298245614035,
            "auc_prc": 0.6395388104805754,
            "auditor_fn_violation": 0.0002280701754385973,
            "auditor_fp_violation": 0.01041134389371489,
            "ave_precision_score": 0.6195666234912414,
            "fpr": 0.28289473684210525,
            "logloss": 4.477912621502188,
            "mae": 0.41615277013027,
            "precision": 0.5949764521193093,
            "recall": 0.758
        },
        "train": {
            "accuracy": 0.575192096597146,
            "auc_prc": 0.5927937741056275,
            "auditor_fn_violation": 0.0019777849775383612,
            "auditor_fp_violation": 0.012944152072769714,
            "ave_precision_score": 0.5749059300230588,
            "fpr": 0.3018660812294182,
            "logloss": 3.876353741824423,
            "mae": 0.4346578729637667,
            "precision": 0.5542949756888168,
            "recall": 0.7533039647577092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(91)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5021929824561403,
            "auc_prc": 0.6052096779172893,
            "auditor_fn_violation": 0.014855263157894765,
            "auditor_fp_violation": 0.016583099131323456,
            "ave_precision_score": 0.595105226968926,
            "fpr": 0.06907894736842106,
            "logloss": 12.153813082536567,
            "mae": 0.5142268586686546,
            "precision": 0.6337209302325582,
            "recall": 0.218
        },
        "train": {
            "accuracy": 0.5422612513721186,
            "auc_prc": 0.55710314577323,
            "auditor_fn_violation": 0.009579442641817839,
            "auditor_fp_violation": 0.022758552772219918,
            "ave_precision_score": 0.5507888130526377,
            "fpr": 0.07793633369923161,
            "logloss": 11.360727082831783,
            "mae": 0.4793451600631682,
            "precision": 0.6033519553072626,
            "recall": 0.23788546255506607
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(92)",
        "seed": 16695,
        "test": {
            "accuracy": 0.43969298245614036,
            "auc_prc": 0.4799909166020648,
            "auditor_fn_violation": 0.014583333333333334,
            "auditor_fp_violation": 0.004489758984840754,
            "ave_precision_score": 0.48150793329995634,
            "fpr": 0.4133771929824561,
            "logloss": 2.896147357053917,
            "mae": 0.5584929549115366,
            "precision": 0.49259757738896365,
            "recall": 0.732
        },
        "train": {
            "accuracy": 0.3940724478594951,
            "auc_prc": 0.4314466819456996,
            "auditor_fn_violation": 0.004898523672973986,
            "auditor_fp_violation": 0.005301121474225797,
            "ave_precision_score": 0.43326734619836654,
            "fpr": 0.47200878155872666,
            "logloss": 2.7144253492525636,
            "mae": 0.5822133452110354,
            "precision": 0.4356955380577428,
            "recall": 0.7312775330396476
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(93)",
        "seed": 16695,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8326803402795231,
            "auditor_fn_violation": 0.006280701754385971,
            "auditor_fp_violation": 0.008596278317152107,
            "ave_precision_score": 0.8329283702834234,
            "fpr": 0.08991228070175439,
            "logloss": 0.5480207880051092,
            "mae": 0.35081644739400136,
            "precision": 0.8093023255813954,
            "recall": 0.696
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.8465480358593309,
            "auditor_fn_violation": 0.0028578751142424716,
            "auditor_fp_violation": 0.008762343062064195,
            "ave_precision_score": 0.8468990724165182,
            "fpr": 0.08232711306256861,
            "logloss": 0.4878007066664911,
            "mae": 0.31944460436112926,
            "precision": 0.8175182481751825,
            "recall": 0.7400881057268722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(94)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8424925753081439,
            "auditor_fn_violation": 0.011052631578947375,
            "auditor_fp_violation": 0.018959717254300804,
            "ave_precision_score": 0.8432075006620758,
            "fpr": 0.15350877192982457,
            "logloss": 0.5944808861543156,
            "mae": 0.2971015581327568,
            "precision": 0.7397769516728625,
            "recall": 0.796
        },
        "train": {
            "accuracy": 0.7925356750823271,
            "auc_prc": 0.8666568665498602,
            "auditor_fn_violation": 0.003278577542227407,
            "auditor_fp_violation": 0.008937686001628533,
            "ave_precision_score": 0.8671799661598052,
            "fpr": 0.14270032930845225,
            "logloss": 0.4943974313145274,
            "mae": 0.26493756087312476,
            "precision": 0.7523809523809524,
            "recall": 0.8700440528634361
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(95)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.857362436604778,
            "auditor_fn_violation": 0.008223684210526315,
            "auditor_fp_violation": 0.010485862715040031,
            "ave_precision_score": 0.8294197708484281,
            "fpr": 0.10087719298245613,
            "logloss": 0.5473258022806168,
            "mae": 0.3075199614426023,
            "precision": 0.8029978586723768,
            "recall": 0.75
        },
        "train": {
            "accuracy": 0.7914379802414928,
            "auc_prc": 0.8684144472822564,
            "auditor_fn_violation": 0.008261725266807548,
            "auditor_fp_violation": 0.0037686722215950477,
            "ave_precision_score": 0.8359032660838543,
            "fpr": 0.10208562019758508,
            "logloss": 0.47328983942771974,
            "mae": 0.28092877229043867,
            "precision": 0.7933333333333333,
            "recall": 0.7863436123348018
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(96)",
        "seed": 16695,
        "test": {
            "accuracy": 0.5482456140350878,
            "auc_prc": 0.7741228070175439,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5482456140350878,
            "fpr": 0.4517543859649123,
            "logloss": 0.691866578963284,
            "mae": 0.49930836286461144,
            "precision": 0.5482456140350878,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.4983534577387486,
            "auc_prc": 0.7491767288693743,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.4983534577387486,
            "fpr": 0.5016465422612514,
            "logloss": 0.6932971600822998,
            "mae": 0.5000236044207467,
            "precision": 0.4983534577387486,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(97)",
        "seed": 16695,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8305457992904622,
            "auditor_fn_violation": 0.002543859649122807,
            "auditor_fp_violation": 0.0037898143416794454,
            "ave_precision_score": 0.825859069338362,
            "fpr": 0.06140350877192982,
            "logloss": 0.5679078014634847,
            "mae": 0.3364017603911666,
            "precision": 0.8440111420612814,
            "recall": 0.606
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8516964620125316,
            "auditor_fn_violation": 0.009124890593190426,
            "auditor_fp_violation": 0.010169890494731307,
            "ave_precision_score": 0.8430374821910187,
            "fpr": 0.04720087815587267,
            "logloss": 0.4842613581393725,
            "mae": 0.30257750273577133,
            "precision": 0.8672839506172839,
            "recall": 0.6189427312775331
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(98)",
        "seed": 16695,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.8232372157077976,
            "auditor_fn_violation": 0.01582456140350878,
            "auditor_fp_violation": 0.017160619996593424,
            "ave_precision_score": 0.8234632701875884,
            "fpr": 0.15899122807017543,
            "logloss": 0.8200951196010601,
            "mae": 0.31216109825573657,
            "precision": 0.7162426614481409,
            "recall": 0.732
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.828024329678164,
            "auditor_fn_violation": 0.010285449015217822,
            "auditor_fp_violation": 0.01834615578619692,
            "ave_precision_score": 0.8283487948459336,
            "fpr": 0.145993413830955,
            "logloss": 0.6786004564168,
            "mae": 0.27965945566718264,
            "precision": 0.7240663900414938,
            "recall": 0.7687224669603524
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(99)",
        "seed": 16695,
        "test": {
            "accuracy": 0.46600877192982454,
            "auc_prc": 0.741240240191916,
            "auditor_fn_violation": 0.0033903508771929805,
            "auditor_fp_violation": 0.0009580991313234543,
            "ave_precision_score": 0.72523795655062,
            "fpr": 0.0043859649122807015,
            "logloss": 0.8872574658602157,
            "mae": 0.4489905667513149,
            "precision": 0.8095238095238095,
            "recall": 0.034
        },
        "train": {
            "accuracy": 0.5126234906695939,
            "auc_prc": 0.689421941114903,
            "auditor_fn_violation": 0.0022485819426780918,
            "auditor_fp_violation": 0.001191371205806974,
            "ave_precision_score": 0.7035027097090969,
            "fpr": 0.006586169045005488,
            "logloss": 0.8549105440452494,
            "mae": 0.42526829970921837,
            "precision": 0.7272727272727273,
            "recall": 0.03524229074889868
        }
    }
]
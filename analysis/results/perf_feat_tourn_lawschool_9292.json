[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.7696659477948816,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.017579810181190692,
            "ave_precision_score": 0.5595878892138977,
            "fpr": 0.42214912280701755,
            "logloss": 13.918695099134487,
            "mae": 0.4307045853680361,
            "precision": 0.5543981481481481,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7518543543990348,
            "auditor_fn_violation": 0.0032415935065147838,
            "auditor_fp_violation": 0.012228618543592402,
            "ave_precision_score": 0.5335018380740952,
            "fpr": 0.43907793633369924,
            "logloss": 14.676740898245482,
            "mae": 0.44844790736961915,
            "precision": 0.5370370370370371,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 9292,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.711774124270488,
            "auditor_fn_violation": 0.01759585820220655,
            "auditor_fp_violation": 0.015017050823780775,
            "ave_precision_score": 0.7005006792592303,
            "fpr": 0.20394736842105263,
            "logloss": 2.105641833120599,
            "mae": 0.3269740067778497,
            "precision": 0.6820512820512821,
            "recall": 0.822680412371134
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.7523649584652499,
            "auditor_fn_violation": 0.011447389054414303,
            "auditor_fp_violation": 0.018640944514257615,
            "ave_precision_score": 0.7419274877109245,
            "fpr": 0.18990120746432493,
            "logloss": 1.8508028369945708,
            "mae": 0.30138709351849197,
            "precision": 0.6991304347826087,
            "recall": 0.8571428571428571
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5625,
            "auc_prc": 0.76784208026625,
            "auditor_fn_violation": 0.0035155543497920063,
            "auditor_fp_violation": 0.019595607872139358,
            "ave_precision_score": 0.5595345029127075,
            "fpr": 0.43201754385964913,
            "logloss": 13.826207797140107,
            "mae": 0.4393489890357989,
            "precision": 0.5491990846681922,
            "recall": 0.9896907216494846
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.7513609552051423,
            "auditor_fn_violation": 0.003311808528316548,
            "auditor_fp_violation": 0.012119345754007093,
            "ave_precision_score": 0.5335444814777335,
            "fpr": 0.4544456641053787,
            "logloss": 14.634103529984474,
            "mae": 0.45873259356284435,
            "precision": 0.5284738041002278,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5723684210526315,
            "auc_prc": 0.7671442369128076,
            "auditor_fn_violation": 0.004205100379815519,
            "auditor_fp_violation": 0.01644223263075723,
            "ave_precision_score": 0.5590372642307097,
            "fpr": 0.42105263157894735,
            "logloss": 13.795141953584332,
            "mae": 0.4317944197267807,
            "precision": 0.5550405561993047,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5499451152579583,
            "auc_prc": 0.7517065317479646,
            "auditor_fn_violation": 0.004184815299385151,
            "auditor_fp_violation": 0.010634229204643108,
            "ave_precision_score": 0.5341938560715841,
            "fpr": 0.4434687156970362,
            "logloss": 14.582607403062882,
            "mae": 0.45321321692928035,
            "precision": 0.5340253748558247,
            "recall": 0.9872068230277186
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5789473684210527,
            "auc_prc": 0.7694506700754503,
            "auditor_fn_violation": 0.0033120817507686744,
            "auditor_fp_violation": 0.014256953860059995,
            "ave_precision_score": 0.5598960138340027,
            "fpr": 0.4100877192982456,
            "logloss": 13.837211802071339,
            "mae": 0.42282397908235925,
            "precision": 0.5594817432273262,
            "recall": 0.979381443298969
        },
        "train": {
            "accuracy": 0.5521405049396267,
            "auc_prc": 0.7521332718748802,
            "auditor_fn_violation": 0.0028811563945990615,
            "auditor_fp_violation": 0.005945433142437095,
            "ave_precision_score": 0.534084464389697,
            "fpr": 0.433589462129528,
            "logloss": 14.645534655553277,
            "mae": 0.446277268146986,
            "precision": 0.535840188014101,
            "recall": 0.9722814498933902
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5646929824561403,
            "auc_prc": 0.7696055938176621,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.017574674390895273,
            "ave_precision_score": 0.5595275417824129,
            "fpr": 0.42872807017543857,
            "logloss": 13.937969221825245,
            "mae": 0.4400797195520314,
            "precision": 0.5505747126436782,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5455543358946213,
            "auc_prc": 0.7517496314249662,
            "auditor_fn_violation": 0.003370321046484685,
            "auditor_fp_violation": 0.012116862281516512,
            "ave_precision_score": 0.5333971229141163,
            "fpr": 0.4489571899012075,
            "logloss": 14.70438449947956,
            "mae": 0.45598213286473566,
            "precision": 0.5315005727376861,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5526315789473685,
            "auc_prc": 0.769354284664047,
            "auditor_fn_violation": 0.0038885874480014478,
            "auditor_fp_violation": 0.015556308804798892,
            "ave_precision_score": 0.5589534834061831,
            "fpr": 0.4418859649122807,
            "logloss": 13.987591978724783,
            "mae": 0.44423216449026687,
            "precision": 0.5436013590033975,
            "recall": 0.9896907216494846
        },
        "train": {
            "accuracy": 0.5455543358946213,
            "auc_prc": 0.7518643221093051,
            "auditor_fn_violation": 0.0017483540428639302,
            "auditor_fp_violation": 0.012621007197103285,
            "ave_precision_score": 0.5335118050696497,
            "fpr": 0.45115257958287597,
            "logloss": 14.717342288420774,
            "mae": 0.4555802894982532,
            "precision": 0.5313568985176739,
            "recall": 0.9936034115138592
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5625,
            "auc_prc": 0.767046331896176,
            "auditor_fn_violation": 0.00528576596129499,
            "auditor_fp_violation": 0.02040192694851884,
            "ave_precision_score": 0.5589393702992986,
            "fpr": 0.4298245614035088,
            "logloss": 13.812025973595306,
            "mae": 0.4391341492432902,
            "precision": 0.5494252873563218,
            "recall": 0.9855670103092784
        },
        "train": {
            "accuracy": 0.5510428100987925,
            "auc_prc": 0.7514034502117182,
            "auditor_fn_violation": 0.004339288347349032,
            "auditor_fp_violation": 0.01937108542648675,
            "ave_precision_score": 0.5335869735283609,
            "fpr": 0.442371020856202,
            "logloss": 14.590933757468214,
            "mae": 0.4536096863972014,
            "precision": 0.5346420323325635,
            "recall": 0.9872068230277186
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 9292,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7388062025349426,
            "auditor_fn_violation": 0.017702116115029844,
            "auditor_fp_violation": 0.024949669255104986,
            "ave_precision_score": 0.7254255104520584,
            "fpr": 0.26864035087719296,
            "logloss": 2.1419127351085603,
            "mae": 0.3385463559655468,
            "precision": 0.6407624633431085,
            "recall": 0.9010309278350516
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7793473243490696,
            "auditor_fn_violation": 0.016891393744777758,
            "auditor_fp_violation": 0.03223547292766639,
            "ave_precision_score": 0.7681548603038872,
            "fpr": 0.2513721185510428,
            "logloss": 1.8197268236293933,
            "mae": 0.3065344457985076,
            "precision": 0.6530303030303031,
            "recall": 0.9189765458422174
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 9292,
        "test": {
            "accuracy": 0.6381578947368421,
            "auc_prc": 0.658820116429429,
            "auditor_fn_violation": 0.017218303490685476,
            "auditor_fp_violation": 0.03225789884547435,
            "ave_precision_score": 0.6134892693950542,
            "fpr": 0.2642543859649123,
            "logloss": 5.631139947260787,
            "mae": 0.3747153358672536,
            "precision": 0.6216640502354788,
            "recall": 0.8164948453608247
        },
        "train": {
            "accuracy": 0.6542261251372119,
            "auc_prc": 0.6834106476862112,
            "auditor_fn_violation": 0.019351260008566237,
            "auditor_fp_violation": 0.031699042869702154,
            "ave_precision_score": 0.6418126493562455,
            "fpr": 0.265642151481888,
            "logloss": 4.898451040438806,
            "mae": 0.36237878663114886,
            "precision": 0.6206896551724138,
            "recall": 0.8443496801705757
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.7696660199381335,
            "auditor_fn_violation": 0.0038885874480014478,
            "auditor_fp_violation": 0.019097436213484534,
            "ave_precision_score": 0.5595879613107893,
            "fpr": 0.4243421052631579,
            "logloss": 13.9269164601557,
            "mae": 0.4335619266992485,
            "precision": 0.5536332179930796,
            "recall": 0.9896907216494846
        },
        "train": {
            "accuracy": 0.5543358946212953,
            "auc_prc": 0.7518503936127243,
            "auditor_fn_violation": 0.0032415935065147838,
            "auditor_fp_violation": 0.01234037480566828,
            "ave_precision_score": 0.5334978775787519,
            "fpr": 0.44017563117453345,
            "logloss": 14.683447329476941,
            "mae": 0.44989755856086283,
            "precision": 0.5364161849710982,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.7671031427807589,
            "auditor_fn_violation": 0.0056610598661602455,
            "auditor_fp_violation": 0.02015284111919144,
            "ave_precision_score": 0.5589961741249057,
            "fpr": 0.41776315789473684,
            "logloss": 13.797002525599558,
            "mae": 0.42873531353784367,
            "precision": 0.5559440559440559,
            "recall": 0.9835051546391752
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7514559677248616,
            "auditor_fn_violation": 0.004439929878598228,
            "auditor_fp_violation": 0.010743501994228412,
            "ave_precision_score": 0.5336394875823658,
            "fpr": 0.43688254665203075,
            "logloss": 14.589307427685597,
            "mae": 0.4487736708696294,
            "precision": 0.5372093023255814,
            "recall": 0.9850746268656716
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5679824561403509,
            "auc_prc": 0.7696607891877231,
            "auditor_fn_violation": 0.0038885874480014478,
            "auditor_fp_violation": 0.017453983318953124,
            "ave_precision_score": 0.5595827311519423,
            "fpr": 0.42653508771929827,
            "logloss": 13.932833008016019,
            "mae": 0.435655735362552,
            "precision": 0.5523590333716916,
            "recall": 0.9896907216494846
        },
        "train": {
            "accuracy": 0.5543358946212953,
            "auc_prc": 0.7518435164037292,
            "auditor_fn_violation": 0.002349862729632378,
            "auditor_fp_violation": 0.013068032245406817,
            "ave_precision_score": 0.5334910009055779,
            "fpr": 0.44127332601536773,
            "logloss": 14.688395226875294,
            "mae": 0.4503860708222683,
            "precision": 0.5363321799307958,
            "recall": 0.9914712153518124
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.7696681305631907,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.016285591026747197,
            "ave_precision_score": 0.5595900719638491,
            "fpr": 0.4199561403508772,
            "logloss": 13.909288718656203,
            "mae": 0.42778679914844714,
            "precision": 0.5556844547563805,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7518617314278301,
            "auditor_fn_violation": 0.004737173470892363,
            "auditor_fp_violation": 0.012119345754007079,
            "ave_precision_score": 0.533509214403216,
            "fpr": 0.43468715697036225,
            "logloss": 14.66691575558641,
            "mae": 0.4470620241464751,
            "precision": 0.5373831775700935,
            "recall": 0.9808102345415778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5679824561403509,
            "auc_prc": 0.7685882420734496,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.015183964008381616,
            "ave_precision_score": 0.5592339791928906,
            "fpr": 0.42543859649122806,
            "logloss": 13.885699762606535,
            "mae": 0.4309870710901438,
            "precision": 0.552479815455594,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5565312843029637,
            "auc_prc": 0.7521038631491751,
            "auditor_fn_violation": 0.0066517030653538015,
            "auditor_fp_violation": 0.013597011885899346,
            "ave_precision_score": 0.534055056968039,
            "fpr": 0.4313940724478595,
            "logloss": 14.637989211200352,
            "mae": 0.44910613852471887,
            "precision": 0.5381903642773208,
            "recall": 0.976545842217484
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 9292,
        "test": {
            "accuracy": 0.581140350877193,
            "auc_prc": 0.7671057248970795,
            "auditor_fn_violation": 0.006327997829625611,
            "auditor_fp_violation": 0.019102572003779954,
            "ave_precision_score": 0.5589987559889289,
            "fpr": 0.40899122807017546,
            "logloss": 13.781490119845449,
            "mae": 0.42205497805811054,
            "precision": 0.5606595995288575,
            "recall": 0.9814432989690721
        },
        "train": {
            "accuracy": 0.5598243688254665,
            "auc_prc": 0.7514573971759954,
            "auditor_fn_violation": 0.006857667129305644,
            "auditor_fp_violation": 0.00886599679135356,
            "ave_precision_score": 0.5336409168876726,
            "fpr": 0.429198682766191,
            "logloss": 14.582836109309323,
            "mae": 0.4453214023261101,
            "precision": 0.54,
            "recall": 0.9786780383795309
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5723684210526315,
            "auc_prc": 0.7671207270824575,
            "auditor_fn_violation": 0.0056610598661602455,
            "auditor_fp_violation": 0.018332203459468344,
            "ave_precision_score": 0.5590137575819147,
            "fpr": 0.41885964912280704,
            "logloss": 13.794668005753854,
            "mae": 0.4296820450402249,
            "precision": 0.5552968568102444,
            "recall": 0.9835051546391752
        },
        "train": {
            "accuracy": 0.5521405049396267,
            "auc_prc": 0.7514379681265126,
            "auditor_fn_violation": 0.005369108667108241,
            "auditor_fp_violation": 0.011893349757364743,
            "ave_precision_score": 0.5336214891287455,
            "fpr": 0.43798024149286496,
            "logloss": 14.590073905411607,
            "mae": 0.4497365437901882,
            "precision": 0.5355064027939465,
            "recall": 0.9808102345415778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 9292,
        "test": {
            "accuracy": 0.581140350877193,
            "auc_prc": 0.7680440731042719,
            "auditor_fn_violation": 0.006490775908844277,
            "auditor_fp_violation": 0.021963207198323686,
            "ave_precision_score": 0.5592017777494641,
            "fpr": 0.40899122807017546,
            "logloss": 14.061569659445945,
            "mae": 0.4229607816452655,
            "precision": 0.5606595995288575,
            "recall": 0.9814432989690721
        },
        "train": {
            "accuracy": 0.5543358946212953,
            "auc_prc": 0.751122127765824,
            "auditor_fn_violation": 0.010337991709946428,
            "auditor_fp_violation": 0.015211269004773233,
            "ave_precision_score": 0.5333031781953037,
            "fpr": 0.4270032930845225,
            "logloss": 14.828723721422781,
            "mae": 0.4469829972738076,
            "precision": 0.5374554102259215,
            "recall": 0.9637526652452025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 9292,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7121859784808877,
            "auditor_fn_violation": 0.031040875384337134,
            "auditor_fp_violation": 0.016455072106495754,
            "ave_precision_score": 0.7051287571366599,
            "fpr": 0.16557017543859648,
            "logloss": 1.8905484969555812,
            "mae": 0.30616059901050197,
            "precision": 0.7129277566539924,
            "recall": 0.7731958762886598
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.7764634046798391,
            "auditor_fn_violation": 0.025300812855902393,
            "auditor_fp_violation": 0.015094545797716203,
            "ave_precision_score": 0.7692907951435458,
            "fpr": 0.15148188803512624,
            "logloss": 1.4295696935087996,
            "mae": 0.2574429495976056,
            "precision": 0.7381404174573055,
            "recall": 0.8294243070362474
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5833333333333334,
            "auc_prc": 0.7685999697147954,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.017274230658613762,
            "ave_precision_score": 0.5600910618827967,
            "fpr": 0.4100877192982456,
            "logloss": 13.884200027583638,
            "mae": 0.42157363377999096,
            "precision": 0.5615474794841735,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5587266739846323,
            "auc_prc": 0.750222246498792,
            "auditor_fn_violation": 0.006780430605323704,
            "auditor_fp_violation": 0.013467871316389437,
            "ave_precision_score": 0.5334779048773841,
            "fpr": 0.429198682766191,
            "logloss": 14.736670686219282,
            "mae": 0.44676566151855335,
            "precision": 0.5394581861012956,
            "recall": 0.976545842217484
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 9292,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7027201416858794,
            "auditor_fn_violation": 0.02965047929101103,
            "auditor_fp_violation": 0.02938699207033979,
            "ave_precision_score": 0.6955884646886417,
            "fpr": 0.21710526315789475,
            "logloss": 1.7154221312095475,
            "mae": 0.3346549074688464,
            "precision": 0.67,
            "recall": 0.8288659793814434
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7787039882374237,
            "auditor_fn_violation": 0.024542490620443343,
            "auditor_fp_violation": 0.03213116708306222,
            "ave_precision_score": 0.7734028025528119,
            "fpr": 0.21514818880351264,
            "logloss": 1.350172428286208,
            "mae": 0.30630963864217553,
            "precision": 0.672787979966611,
            "recall": 0.8592750533049041
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.7696616318891142,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.017579810181190692,
            "ave_precision_score": 0.5595835739589909,
            "fpr": 0.42214912280701755,
            "logloss": 13.91534590591374,
            "mae": 0.4301399986450737,
            "precision": 0.5543981481481481,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5521405049396267,
            "auc_prc": 0.7518577716728618,
            "auditor_fn_violation": 0.004737173470892363,
            "auditor_fp_violation": 0.012320507025743678,
            "ave_precision_score": 0.5335052549514676,
            "fpr": 0.43798024149286496,
            "logloss": 14.671070477253776,
            "mae": 0.44854864549671336,
            "precision": 0.5355064027939465,
            "recall": 0.9808102345415778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 9292,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7902095762924629,
            "auditor_fn_violation": 0.020442213781877376,
            "auditor_fp_violation": 0.030077755865072546,
            "ave_precision_score": 0.788580232789278,
            "fpr": 0.25877192982456143,
            "logloss": 1.3579953586918485,
            "mae": 0.32887462338795453,
            "precision": 0.6488095238095238,
            "recall": 0.8989690721649485
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.8224102420555239,
            "auditor_fn_violation": 0.017038845290561464,
            "auditor_fp_violation": 0.03240683252951608,
            "ave_precision_score": 0.8199972656596008,
            "fpr": 0.2557628979143798,
            "logloss": 1.165008106515289,
            "mae": 0.3149773285184542,
            "precision": 0.6490963855421686,
            "recall": 0.9189765458422174
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5723684210526315,
            "auc_prc": 0.7696535475658479,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.017215169070216533,
            "ave_precision_score": 0.5595754899867053,
            "fpr": 0.42105263157894735,
            "logloss": 13.906832851004836,
            "mae": 0.43022024644971935,
            "precision": 0.5550405561993047,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5543358946212953,
            "auc_prc": 0.7518581529529103,
            "auditor_fn_violation": 0.0036207546242443107,
            "auditor_fp_violation": 0.011495994158872697,
            "ave_precision_score": 0.5335056364420925,
            "fpr": 0.43798024149286496,
            "logloss": 14.669434087727597,
            "mae": 0.4479226459230956,
            "precision": 0.5365853658536586,
            "recall": 0.9850746268656716
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.7696694903893135,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.016285591026747197,
            "ave_precision_score": 0.5595914316170663,
            "fpr": 0.4199561403508772,
            "logloss": 13.909266834186907,
            "mae": 0.42778176007903856,
            "precision": 0.5556844547563805,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7518617314278301,
            "auditor_fn_violation": 0.004737173470892363,
            "auditor_fp_violation": 0.012119345754007079,
            "ave_precision_score": 0.533509214403216,
            "fpr": 0.43468715697036225,
            "logloss": 14.666893656931828,
            "mae": 0.44705698088742535,
            "precision": 0.5373831775700935,
            "recall": 0.9808102345415778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5635964912280702,
            "auc_prc": 0.7498608675064327,
            "auditor_fn_violation": 0.024789744981009224,
            "auditor_fp_violation": 0.009437014667817092,
            "ave_precision_score": 0.567130445315155,
            "fpr": 0.37609649122807015,
            "logloss": 13.75788988956928,
            "mae": 0.4381689580532537,
            "precision": 0.55627425614489,
            "recall": 0.8865979381443299
        },
        "train": {
            "accuracy": 0.5488474204171241,
            "auc_prc": 0.7334170112683022,
            "auditor_fn_violation": 0.023426071773795288,
            "auditor_fp_violation": 0.01705648906527062,
            "ave_precision_score": 0.5376506741661553,
            "fpr": 0.39846322722283206,
            "logloss": 14.611575018288194,
            "mae": 0.4537675594192452,
            "precision": 0.5369897959183674,
            "recall": 0.8976545842217484
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 9292,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7544342339437057,
            "auditor_fn_violation": 0.013949177066377286,
            "auditor_fp_violation": 0.028054254488680725,
            "ave_precision_score": 0.7419251856169952,
            "fpr": 0.2543859649122807,
            "logloss": 2.1710774140961266,
            "mae": 0.3240198515341977,
            "precision": 0.65424739195231,
            "recall": 0.9051546391752577
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.8123905399791062,
            "auditor_fn_violation": 0.0111899339744745,
            "auditor_fp_violation": 0.024305745265259703,
            "ave_precision_score": 0.8027387599781444,
            "fpr": 0.2557628979143798,
            "logloss": 1.7446097233089126,
            "mae": 0.2985726210217831,
            "precision": 0.6517189835575485,
            "recall": 0.929637526652452
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 9292,
        "test": {
            "accuracy": 0.569078947368421,
            "auc_prc": 0.769666060960078,
            "auditor_fn_violation": 0.0038885874480014478,
            "auditor_fp_violation": 0.019469781009901804,
            "ave_precision_score": 0.5595880023085941,
            "fpr": 0.42543859649122806,
            "logloss": 13.927914937681972,
            "mae": 0.4340253545252748,
            "precision": 0.5529953917050692,
            "recall": 0.9896907216494846
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.7518463617051081,
            "auditor_fn_violation": 0.0032415935065147838,
            "auditor_fp_violation": 0.012462064957706473,
            "ave_precision_score": 0.5334938460203648,
            "fpr": 0.44127332601536773,
            "logloss": 14.683233093948791,
            "mae": 0.45033181166742003,
            "precision": 0.535796766743649,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 9292,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7327060503996529,
            "auditor_fn_violation": 0.00856845722553807,
            "auditor_fp_violation": 0.003523152142651709,
            "ave_precision_score": 0.7271712342304584,
            "fpr": 0.16557017543859648,
            "logloss": 1.5801697750153967,
            "mae": 0.2956714337510704,
            "precision": 0.7129277566539924,
            "recall": 0.7731958762886598
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.7703026603153987,
            "auditor_fn_violation": 0.011796123662696396,
            "auditor_fp_violation": 0.006312987071042219,
            "ave_precision_score": 0.7655315477580525,
            "fpr": 0.1602634467618002,
            "logloss": 1.3245204676906912,
            "mae": 0.2663917795836979,
            "precision": 0.7245283018867924,
            "recall": 0.8187633262260128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5723684210526315,
            "auc_prc": 0.7693376551465042,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.017215169070216533,
            "ave_precision_score": 0.5589368561003604,
            "fpr": 0.42105263157894735,
            "logloss": 13.933470561638988,
            "mae": 0.4291263765863321,
            "precision": 0.5550405561993047,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5521405049396267,
            "auc_prc": 0.7518629757172627,
            "auditor_fn_violation": 0.004737173470892363,
            "auditor_fp_violation": 0.011495994158872697,
            "ave_precision_score": 0.5335104586314164,
            "fpr": 0.43798024149286496,
            "logloss": 14.671651088112467,
            "mae": 0.4478601131857924,
            "precision": 0.5355064027939465,
            "recall": 0.9808102345415778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.769681937531582,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.01804973499322076,
            "ave_precision_score": 0.559603876573773,
            "fpr": 0.4232456140350877,
            "logloss": 13.905606129916073,
            "mae": 0.4305655206129764,
            "precision": 0.553757225433526,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7517773509846516,
            "auditor_fn_violation": 0.0061297714032940214,
            "auditor_fp_violation": 0.014314735435675598,
            "ave_precision_score": 0.5334248401330719,
            "fpr": 0.43468715697036225,
            "logloss": 14.666129547816043,
            "mae": 0.44942039448968424,
            "precision": 0.5373831775700935,
            "recall": 0.9808102345415778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 9292,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7228243227410031,
            "auditor_fn_violation": 0.015703563031289565,
            "auditor_fp_violation": 0.01869170878014712,
            "ave_precision_score": 0.7059304556708488,
            "fpr": 0.2642543859649123,
            "logloss": 2.259920413708073,
            "mae": 0.33468543035490983,
            "precision": 0.6471449487554904,
            "recall": 0.911340206185567
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7525304496122913,
            "auditor_fn_violation": 0.017120762815996858,
            "auditor_fp_violation": 0.020195598293357722,
            "ave_precision_score": 0.7377921467870079,
            "fpr": 0.24807903402854006,
            "logloss": 2.045746983094487,
            "mae": 0.3057736568213785,
            "precision": 0.6533742331288344,
            "recall": 0.908315565031983
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5679824561403509,
            "auc_prc": 0.7704390240204264,
            "auditor_fn_violation": 0.0010761439681678422,
            "auditor_fp_violation": 0.0014380212827150018,
            "ave_precision_score": 0.5610080848763005,
            "fpr": 0.4298245614035088,
            "logloss": 13.85315241016205,
            "mae": 0.4353179025666067,
            "precision": 0.552,
            "recall": 0.9958762886597938
        },
        "train": {
            "accuracy": 0.557628979143798,
            "auc_prc": 0.7514518873620643,
            "auditor_fn_violation": 0.0011585478597291104,
            "auditor_fp_violation": 0.005503375039114689,
            "ave_precision_score": 0.5356217346341174,
            "fpr": 0.43798024149286496,
            "logloss": 14.531410509638498,
            "mae": 0.44773760909032284,
            "precision": 0.5381944444444444,
            "recall": 0.9914712153518124
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5657894736842105,
            "auc_prc": 0.7696348557074651,
            "auditor_fn_violation": 0.0038885874480014478,
            "auditor_fp_violation": 0.01820894449237849,
            "ave_precision_score": 0.5595568003714511,
            "fpr": 0.42872807017543857,
            "logloss": 13.926592303095932,
            "mae": 0.43636604885201896,
            "precision": 0.5510907003444316,
            "recall": 0.9896907216494846
        },
        "train": {
            "accuracy": 0.5499451152579583,
            "auc_prc": 0.7517595105360029,
            "auditor_fn_violation": 0.003370321046484685,
            "auditor_fp_violation": 0.011898316702345904,
            "ave_precision_score": 0.5334070015209282,
            "fpr": 0.4445664105378705,
            "logloss": 14.689106975053903,
            "mae": 0.4529149181217761,
            "precision": 0.5339470655926352,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.7696674570226542,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.01775442705123465,
            "ave_precision_score": 0.5595893981860536,
            "fpr": 0.41885964912280704,
            "logloss": 13.908002388117826,
            "mae": 0.4273971772209702,
            "precision": 0.5563298490127758,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5543358946212953,
            "auc_prc": 0.7518464209794403,
            "auditor_fn_violation": 0.004737173470892363,
            "auditor_fp_violation": 0.010986882298304787,
            "ave_precision_score": 0.5334939052791157,
            "fpr": 0.43578485181119647,
            "logloss": 14.66955141165758,
            "mae": 0.4474721865797595,
            "precision": 0.5367561260210035,
            "recall": 0.9808102345415778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 9292,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.7622484915274995,
            "auditor_fn_violation": 0.021665310182673175,
            "auditor_fp_violation": 0.03524692879740334,
            "ave_precision_score": 0.7592118978981768,
            "fpr": 0.18092105263157895,
            "logloss": 1.2263645568809802,
            "mae": 0.30347690189695364,
            "precision": 0.7058823529411765,
            "recall": 0.8164948453608247
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8698220312289524,
            "auditor_fn_violation": 0.0228760541030148,
            "auditor_fp_violation": 0.030521876909169483,
            "ave_precision_score": 0.8683504956850095,
            "fpr": 0.1690450054884742,
            "logloss": 0.6780112290190747,
            "mae": 0.2611221707011092,
            "precision": 0.7259786476868327,
            "recall": 0.8699360341151386
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.7696659477948816,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.017579810181190692,
            "ave_precision_score": 0.5595878892138977,
            "fpr": 0.42214912280701755,
            "logloss": 13.918697519681858,
            "mae": 0.4307054332091132,
            "precision": 0.5543981481481481,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7518543543990348,
            "auditor_fn_violation": 0.0032415935065147838,
            "auditor_fp_violation": 0.012228618543592402,
            "ave_precision_score": 0.5335018380740952,
            "fpr": 0.43907793633369924,
            "logloss": 14.67674246774618,
            "mae": 0.4484482689102035,
            "precision": 0.5370370370370371,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5537280701754386,
            "auc_prc": 0.762387376583036,
            "auditor_fn_violation": 0.008206728160607706,
            "auditor_fp_violation": 0.012834339948231244,
            "ave_precision_score": 0.548489550646629,
            "fpr": 0.4298245614035088,
            "logloss": 14.561597016661244,
            "mae": 0.447869287697845,
            "precision": 0.5452436194895591,
            "recall": 0.9690721649484536
        },
        "train": {
            "accuracy": 0.5345773874862788,
            "auc_prc": 0.7486261961394747,
            "auditor_fn_violation": 0.01071481232694923,
            "auditor_fp_violation": 0.013000978488161294,
            "ave_precision_score": 0.5267415568399043,
            "fpr": 0.4445664105378705,
            "logloss": 15.135870713333617,
            "mae": 0.46692273289946695,
            "precision": 0.5263157894736842,
            "recall": 0.9594882729211087
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5679824561403509,
            "auc_prc": 0.7696785312708001,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.01707650273224044,
            "ave_precision_score": 0.5596004722669504,
            "fpr": 0.42543859649122806,
            "logloss": 13.922398810533261,
            "mae": 0.43250828814363657,
            "precision": 0.552479815455594,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.7518370915243253,
            "auditor_fn_violation": 0.0036535216344184673,
            "auditor_fp_violation": 0.011493510686382134,
            "ave_precision_score": 0.5334845765426298,
            "fpr": 0.43907793633369924,
            "logloss": 14.678820828169997,
            "mae": 0.4493299572075465,
            "precision": 0.5359628770301624,
            "recall": 0.9850746268656716
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.7650630983850343,
            "auditor_fn_violation": 0.006364170736118647,
            "auditor_fp_violation": 0.009832470520563708,
            "ave_precision_score": 0.5553432219778015,
            "fpr": 0.4144736842105263,
            "logloss": 14.29613719755162,
            "mae": 0.4297202864599441,
            "precision": 0.5547703180212014,
            "recall": 0.9711340206185567
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7499627499850018,
            "auditor_fn_violation": 0.0031549949796259415,
            "auditor_fp_violation": 0.011409072621702567,
            "ave_precision_score": 0.5320715069527857,
            "fpr": 0.43029637760702527,
            "logloss": 14.872592617489234,
            "mae": 0.4507871962286451,
            "precision": 0.5377358490566038,
            "recall": 0.9722814498933902
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.7696699388413416,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.01804973499322076,
            "ave_precision_score": 0.5595918798011046,
            "fpr": 0.4232456140350877,
            "logloss": 13.916750501384984,
            "mae": 0.4306336018431195,
            "precision": 0.553757225433526,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5565312843029637,
            "auc_prc": 0.7518611193103654,
            "auditor_fn_violation": 0.002349862729632378,
            "auditor_fp_violation": 0.012228618543592402,
            "ave_precision_score": 0.5335086024505423,
            "fpr": 0.43907793633369924,
            "logloss": 14.67755562168814,
            "mae": 0.44836202140221587,
            "precision": 0.5375722543352601,
            "recall": 0.9914712153518124
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5789473684210527,
            "auc_prc": 0.7696610152196371,
            "auditor_fn_violation": 0.004189274733224815,
            "auditor_fp_violation": 0.017112453264308304,
            "ave_precision_score": 0.5595829569998478,
            "fpr": 0.4133771929824561,
            "logloss": 13.90532834372819,
            "mae": 0.4256551111499839,
            "precision": 0.5590643274853802,
            "recall": 0.9855670103092784
        },
        "train": {
            "accuracy": 0.5543358946212953,
            "auc_prc": 0.7518518502550892,
            "auditor_fn_violation": 0.004737173470892363,
            "auditor_fp_violation": 0.010986882298304787,
            "ave_precision_score": 0.5334993340489338,
            "fpr": 0.43578485181119647,
            "logloss": 14.668592716020452,
            "mae": 0.44672258884326665,
            "precision": 0.5367561260210035,
            "recall": 0.9808102345415778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5921052631578947,
            "auc_prc": 0.7509116085508722,
            "auditor_fn_violation": 0.026734038705009948,
            "auditor_fp_violation": 0.01689675007190106,
            "ave_precision_score": 0.585306345416499,
            "fpr": 0.3300438596491228,
            "logloss": 12.802220966088083,
            "mae": 0.409986330900004,
            "precision": 0.579020979020979,
            "recall": 0.8536082474226804
        },
        "train": {
            "accuracy": 0.570801317233809,
            "auc_prc": 0.7404332567244252,
            "auditor_fn_violation": 0.0268268193297274,
            "auditor_fp_violation": 0.020307354555433598,
            "ave_precision_score": 0.5646685678818324,
            "fpr": 0.3589462129527991,
            "logloss": 13.348746167390319,
            "mae": 0.4301261486520896,
            "precision": 0.5532786885245902,
            "recall": 0.8635394456289979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.7696740482524205,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.017579810181190692,
            "ave_precision_score": 0.5595959886715846,
            "fpr": 0.42214912280701755,
            "logloss": 13.915481746472443,
            "mae": 0.4304385620395981,
            "precision": 0.5543981481481481,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7518505046682024,
            "auditor_fn_violation": 0.002946690414947374,
            "auditor_fp_violation": 0.011495994158872697,
            "ave_precision_score": 0.5334979886142158,
            "fpr": 0.43798024149286496,
            "logloss": 14.67693534750648,
            "mae": 0.4483044003769811,
            "precision": 0.537122969837587,
            "recall": 0.9872068230277186
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5723684210526315,
            "auc_prc": 0.7672739885748407,
            "auditor_fn_violation": 0.0026496654006149395,
            "auditor_fp_violation": 0.006057664653436871,
            "ave_precision_score": 0.5591670037163096,
            "fpr": 0.4232456140350877,
            "logloss": 13.782114860229628,
            "mae": 0.43248566145816686,
            "precision": 0.5547866205305652,
            "recall": 0.9917525773195877
        },
        "train": {
            "accuracy": 0.5521405049396267,
            "auc_prc": 0.7514031911554218,
            "auditor_fn_violation": 0.003288403521049293,
            "auditor_fp_violation": 0.0023568153935558944,
            "ave_precision_score": 0.5335867155176282,
            "fpr": 0.442371020856202,
            "logloss": 14.622590972705428,
            "mae": 0.4535496389726177,
            "precision": 0.5351787773933102,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5855263157894737,
            "auc_prc": 0.7590046004164805,
            "auditor_fn_violation": 0.011993579309097486,
            "auditor_fp_violation": 0.02764852705534329,
            "ave_precision_score": 0.737808047480462,
            "fpr": 0.3925438596491228,
            "logloss": 3.626750924048221,
            "mae": 0.41255788771740953,
            "precision": 0.5650060753341434,
            "recall": 0.9587628865979382
        },
        "train": {
            "accuracy": 0.5784851811196488,
            "auc_prc": 0.7831228519948384,
            "auditor_fn_violation": 0.006766387600963349,
            "auditor_fp_violation": 0.026275138950285853,
            "ave_precision_score": 0.7662591322667194,
            "fpr": 0.4061470911086718,
            "logloss": 3.4991137986115883,
            "mae": 0.4192572528516626,
            "precision": 0.5515151515151515,
            "recall": 0.9701492537313433
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5427631578947368,
            "auc_prc": 0.7694729736457062,
            "auditor_fn_violation": 0.0007279797431723639,
            "auditor_fp_violation": 0.002252044044537575,
            "ave_precision_score": 0.559918314344751,
            "fpr": 0.45614035087719296,
            "logloss": 13.952280585815608,
            "mae": 0.45304316436492326,
            "precision": 0.5377777777777778,
            "recall": 0.9979381443298969
        },
        "train": {
            "accuracy": 0.5290889132821076,
            "auc_prc": 0.7521786560765917,
            "auditor_fn_violation": 0.0011328023517351304,
            "auditor_fp_violation": 0.0029677496262374005,
            "ave_precision_score": 0.5341298441691618,
            "fpr": 0.46871569703622395,
            "logloss": 14.708002280783553,
            "mae": 0.4662405446488613,
            "precision": 0.5223713646532439,
            "recall": 0.9957356076759062
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5767543859649122,
            "auc_prc": 0.7670136210208216,
            "auditor_fn_violation": 0.005769578585639357,
            "auditor_fp_violation": 0.01376905378199598,
            "ave_precision_score": 0.5589066620664725,
            "fpr": 0.4144736842105263,
            "logloss": 13.858897478543975,
            "mae": 0.42856741958458267,
            "precision": 0.5578947368421052,
            "recall": 0.9835051546391752
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.7513051215693026,
            "auditor_fn_violation": 0.008212817050079695,
            "auditor_fp_violation": 0.01107132036298434,
            "ave_precision_score": 0.5334886513719174,
            "fpr": 0.43249176728869376,
            "logloss": 14.622728865842207,
            "mae": 0.4485617608469737,
            "precision": 0.5364705882352941,
            "recall": 0.9722814498933902
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5537280701754386,
            "auc_prc": 0.767153658979981,
            "auditor_fn_violation": 0.0035155543497920063,
            "auditor_fp_violation": 0.015006779243189952,
            "ave_precision_score": 0.5590477921505468,
            "fpr": 0.4407894736842105,
            "logloss": 13.839339879905305,
            "mae": 0.44414273293672557,
            "precision": 0.54421768707483,
            "recall": 0.9896907216494846
        },
        "train": {
            "accuracy": 0.535675082327113,
            "auc_prc": 0.7516780981380954,
            "auditor_fn_violation": 0.002085386147512399,
            "auditor_fp_violation": 0.012611073307140987,
            "ave_precision_score": 0.5341654243545955,
            "fpr": 0.4610318331503842,
            "logloss": 14.622967239507766,
            "mae": 0.46306197807870336,
            "precision": 0.5259593679458239,
            "recall": 0.9936034115138592
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.7490985630510788,
            "auditor_fn_violation": 0.026917164044130946,
            "auditor_fp_violation": 0.011047084925428339,
            "ave_precision_score": 0.5717392095552322,
            "fpr": 0.36622807017543857,
            "logloss": 13.533888999743258,
            "mae": 0.4305409541608082,
            "precision": 0.5616797900262467,
            "recall": 0.8824742268041237
        },
        "train": {
            "accuracy": 0.5565312843029637,
            "auc_prc": 0.7357635258259785,
            "auditor_fn_violation": 0.023330111243999543,
            "auditor_fp_violation": 0.017250199919535508,
            "ave_precision_score": 0.54422907931312,
            "fpr": 0.38748627881448955,
            "logloss": 14.36497582394125,
            "mae": 0.44506306730766804,
            "precision": 0.5421530479896238,
            "recall": 0.8912579957356077
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.7664186979071403,
            "auditor_fn_violation": 0.004921776089708808,
            "auditor_fp_violation": 0.016773491104811213,
            "ave_precision_score": 0.5585128257903824,
            "fpr": 0.41885964912280704,
            "logloss": 13.780113970903214,
            "mae": 0.43017758097220193,
            "precision": 0.5558139534883721,
            "recall": 0.9855670103092784
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.7514094185349433,
            "auditor_fn_violation": 0.005369108667108241,
            "auditor_fp_violation": 0.008667318992107535,
            "ave_precision_score": 0.5335929419174411,
            "fpr": 0.43688254665203075,
            "logloss": 14.595238243078937,
            "mae": 0.4500133326300574,
            "precision": 0.5361305361305362,
            "recall": 0.9808102345415778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.7690851061066066,
            "auditor_fn_violation": 0.002136462289744981,
            "auditor_fp_violation": 0.008435535560212008,
            "ave_precision_score": 0.5592075466776468,
            "fpr": 0.4232456140350877,
            "logloss": 13.890746116634766,
            "mae": 0.432994589822189,
            "precision": 0.5552995391705069,
            "recall": 0.9938144329896907
        },
        "train": {
            "accuracy": 0.5565312843029637,
            "auc_prc": 0.7521858595382236,
            "auditor_fn_violation": 0.001921551096641616,
            "auditor_fp_violation": 0.004495085207941155,
            "ave_precision_score": 0.5341370482224069,
            "fpr": 0.43798024149286496,
            "logloss": 14.659629179628675,
            "mae": 0.44790512383842146,
            "precision": 0.5376593279258401,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5778508771929824,
            "auc_prc": 0.7673045543010455,
            "auditor_fn_violation": 0.007492313257370231,
            "auditor_fp_violation": 0.017289638029499992,
            "ave_precision_score": 0.5591975640132896,
            "fpr": 0.3980263157894737,
            "logloss": 13.744732207167868,
            "mae": 0.4221962281476195,
            "precision": 0.5605326876513317,
            "recall": 0.954639175257732
        },
        "train": {
            "accuracy": 0.5477497255762898,
            "auc_prc": 0.7515170265990158,
            "auditor_fn_violation": 0.013483624686665464,
            "auditor_fp_violation": 0.012308089663290799,
            "ave_precision_score": 0.5337005411603136,
            "fpr": 0.42151481888035125,
            "logloss": 14.592768893940745,
            "mae": 0.4495039232304047,
            "precision": 0.5345454545454545,
            "recall": 0.9402985074626866
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 9292,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7020359248686215,
            "auditor_fn_violation": 0.018821215409658164,
            "auditor_fp_violation": 0.005800875138666338,
            "ave_precision_score": 0.6947277571931584,
            "fpr": 0.12171052631578948,
            "logloss": 2.02958408317422,
            "mae": 0.3366831910472752,
            "precision": 0.7279411764705882,
            "recall": 0.6123711340206186
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.7667890961738865,
            "auditor_fn_violation": 0.01625243704638172,
            "auditor_fp_violation": 0.0004122564334354913,
            "ave_precision_score": 0.7611764165663372,
            "fpr": 0.09659714599341383,
            "logloss": 1.4604469411765977,
            "mae": 0.26831628670891444,
            "precision": 0.7884615384615384,
            "recall": 0.6993603411513859
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.7671442369128076,
            "auditor_fn_violation": 0.0035155543497920063,
            "auditor_fp_violation": 0.01644223263075723,
            "ave_precision_score": 0.5590372642307097,
            "fpr": 0.42105263157894735,
            "logloss": 13.79654321827214,
            "mae": 0.4321309499772676,
            "precision": 0.5555555555555556,
            "recall": 0.9896907216494846
        },
        "train": {
            "accuracy": 0.5510428100987925,
            "auc_prc": 0.7517078655219931,
            "auditor_fn_violation": 0.003288403521049293,
            "auditor_fp_violation": 0.010634229204643108,
            "ave_precision_score": 0.5341951897392774,
            "fpr": 0.4434687156970362,
            "logloss": 14.583450084560116,
            "mae": 0.45338295764493236,
            "precision": 0.5345622119815668,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 9292,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.6845792139965645,
            "auditor_fn_violation": 0.007679960209802857,
            "auditor_fp_violation": 0.03081474177246395,
            "ave_precision_score": 0.6590763769146603,
            "fpr": 0.2719298245614035,
            "logloss": 2.869330207481867,
            "mae": 0.3412034264364127,
            "precision": 0.6374269005847953,
            "recall": 0.8989690721649485
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.6824151284932939,
            "auditor_fn_violation": 0.011578457095110928,
            "auditor_fp_violation": 0.03460470568367516,
            "ave_precision_score": 0.6625467856065385,
            "fpr": 0.28210757409440174,
            "logloss": 2.6496751860406484,
            "mae": 0.3396373791207996,
            "precision": 0.6259097525473072,
            "recall": 0.9168443496801706
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5668859649122807,
            "auc_prc": 0.7671220900033091,
            "auditor_fn_violation": 0.004354313619099295,
            "auditor_fp_violation": 0.01974454579070627,
            "ave_precision_score": 0.5590151203758947,
            "fpr": 0.42653508771929827,
            "logloss": 13.792192988880728,
            "mae": 0.4358194841461581,
            "precision": 0.5518433179723502,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5499451152579583,
            "auc_prc": 0.7514617484480052,
            "auditor_fn_violation": 0.004353331351709385,
            "auditor_fp_violation": 0.012427296342838427,
            "ave_precision_score": 0.5336452673230134,
            "fpr": 0.4434687156970362,
            "logloss": 14.58280426751639,
            "mae": 0.4527006429731598,
            "precision": 0.5340253748558247,
            "recall": 0.9872068230277186
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.7671031427807589,
            "auditor_fn_violation": 0.0056610598661602455,
            "auditor_fp_violation": 0.02015284111919144,
            "ave_precision_score": 0.5589961741249057,
            "fpr": 0.41776315789473684,
            "logloss": 13.796841584342681,
            "mae": 0.4287224293416529,
            "precision": 0.5559440559440559,
            "recall": 0.9835051546391752
        },
        "train": {
            "accuracy": 0.5565312843029637,
            "auc_prc": 0.75145730482885,
            "auditor_fn_violation": 0.004439929878598228,
            "auditor_fp_violation": 0.008436356050484034,
            "ave_precision_score": 0.5336408245792722,
            "fpr": 0.43578485181119647,
            "logloss": 14.589334681359514,
            "mae": 0.4487776943605152,
            "precision": 0.5378346915017462,
            "recall": 0.9850746268656716
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5800438596491229,
            "auc_prc": 0.7696964454321908,
            "auditor_fn_violation": 0.005161421595225177,
            "auditor_fp_violation": 0.018719955626771853,
            "ave_precision_score": 0.5596183840522639,
            "fpr": 0.41118421052631576,
            "logloss": 13.89238571014723,
            "mae": 0.42341303781671014,
            "precision": 0.5598591549295775,
            "recall": 0.9835051546391752
        },
        "train": {
            "accuracy": 0.557628979143798,
            "auc_prc": 0.7518527926065626,
            "auditor_fn_violation": 0.006033810873498277,
            "auditor_fp_violation": 0.011761725715364248,
            "ave_precision_score": 0.5335002761405172,
            "fpr": 0.43029637760702527,
            "logloss": 14.661680773193213,
            "mae": 0.4456302046207985,
            "precision": 0.5388235294117647,
            "recall": 0.976545842217484
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5635964912280702,
            "auc_prc": 0.7668307684701086,
            "auditor_fn_violation": 0.004449267498643516,
            "auditor_fp_violation": 0.010407679033649695,
            "ave_precision_score": 0.5567146980627892,
            "fpr": 0.4298245614035088,
            "logloss": 13.953586796616486,
            "mae": 0.43932571829761174,
            "precision": 0.5499425947187141,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5521405049396267,
            "auc_prc": 0.7527522400734291,
            "auditor_fn_violation": 0.00412864328194374,
            "auditor_fp_violation": 0.011965370459591432,
            "ave_precision_score": 0.5333276843662351,
            "fpr": 0.44127332601536773,
            "logloss": 14.711541725331093,
            "mae": 0.4549558012625446,
            "precision": 0.5352601156069364,
            "recall": 0.9872068230277186
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 9292,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7735209813907836,
            "auditor_fn_violation": 0.03947368421052632,
            "auditor_fp_violation": 0.0394197584124245,
            "ave_precision_score": 0.7751957151682229,
            "fpr": 0.17543859649122806,
            "logloss": 0.838203141197668,
            "mae": 0.31886121286154473,
            "precision": 0.708029197080292,
            "recall": 0.8
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8521030604745377,
            "auditor_fn_violation": 0.030119903852230148,
            "auditor_fp_violation": 0.03331329998857603,
            "ave_precision_score": 0.8523315336651414,
            "fpr": 0.18221734357848518,
            "logloss": 0.6040340803019643,
            "mae": 0.29955261148687734,
            "precision": 0.7040998217468806,
            "recall": 0.8422174840085288
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.7696725780859888,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.017579810181190692,
            "ave_precision_score": 0.5595945187461184,
            "fpr": 0.42214912280701755,
            "logloss": 13.916526092195454,
            "mae": 0.43052199003382713,
            "precision": 0.5543981481481481,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.751848926504979,
            "auditor_fn_violation": 0.0032415935065147838,
            "auditor_fp_violation": 0.012228618543592402,
            "ave_precision_score": 0.5334964106441965,
            "fpr": 0.43907793633369924,
            "logloss": 14.676880434388924,
            "mae": 0.44863499925062816,
            "precision": 0.5370370370370371,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 9292,
        "test": {
            "accuracy": 0.555921052631579,
            "auc_prc": 0.758904501595304,
            "auditor_fn_violation": 0.015486525592331345,
            "auditor_fp_violation": 0.008114548666748855,
            "ave_precision_score": 0.5627450582670754,
            "fpr": 0.41885964912280704,
            "logloss": 13.715857710002627,
            "mae": 0.4450781695290319,
            "precision": 0.54739336492891,
            "recall": 0.9525773195876288
        },
        "train": {
            "accuracy": 0.5367727771679474,
            "auc_prc": 0.740018734139152,
            "auditor_fn_violation": 0.01992234218588725,
            "auditor_fp_violation": 0.00954646825377116,
            "ave_precision_score": 0.5320382704097971,
            "fpr": 0.43029637760702527,
            "logloss": 14.86423191874049,
            "mae": 0.4657446510969364,
            "precision": 0.5282791817087846,
            "recall": 0.9360341151385928
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5789473684210527,
            "auc_prc": 0.7683689608114279,
            "auditor_fn_violation": 0.0050529028757460666,
            "auditor_fp_violation": 0.01978049632277417,
            "ave_precision_score": 0.5607040225851675,
            "fpr": 0.41228070175438597,
            "logloss": 13.969831697750267,
            "mae": 0.42616858937744073,
            "precision": 0.5592028135990621,
            "recall": 0.9835051546391752
        },
        "train": {
            "accuracy": 0.5587266739846323,
            "auc_prc": 0.7519270144671578,
            "auditor_fn_violation": 0.006497230017389922,
            "auditor_fp_violation": 0.015705480030397713,
            "ave_precision_score": 0.5347188835403093,
            "fpr": 0.429198682766191,
            "logloss": 14.676286364450762,
            "mae": 0.44523647645963454,
            "precision": 0.5394581861012956,
            "recall": 0.976545842217484
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 9292,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.6705805436657148,
            "auditor_fn_violation": 0.02620274914089347,
            "auditor_fp_violation": 0.025039545585274665,
            "ave_precision_score": 0.6072552465231449,
            "fpr": 0.23793859649122806,
            "logloss": 7.4467474982618596,
            "mae": 0.3694550010136577,
            "precision": 0.627147766323024,
            "recall": 0.7525773195876289
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.6879551434074473,
            "auditor_fn_violation": 0.02317563819603566,
            "auditor_fp_violation": 0.02305159165751921,
            "ave_precision_score": 0.6264277931171435,
            "fpr": 0.24588364434687157,
            "logloss": 6.693131151858483,
            "mae": 0.36021153915894505,
            "precision": 0.6216216216216216,
            "recall": 0.7846481876332623
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 9292,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.7251875400927563,
            "auditor_fn_violation": 0.026449177066377286,
            "auditor_fp_violation": 0.02482384239286742,
            "ave_precision_score": 0.7139597679504519,
            "fpr": 0.21710526315789475,
            "logloss": 2.0478697822911034,
            "mae": 0.31673735605457287,
            "precision": 0.6754098360655738,
            "recall": 0.8494845360824742
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.7827515540127592,
            "auditor_fn_violation": 0.016816497721522543,
            "auditor_fp_violation": 0.024099617048541955,
            "ave_precision_score": 0.7744913572404536,
            "fpr": 0.21295279912184412,
            "logloss": 1.579940870541939,
            "mae": 0.2774490405287551,
            "precision": 0.686084142394822,
            "recall": 0.9040511727078892
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5668859649122807,
            "auc_prc": 0.7671106040725425,
            "auditor_fn_violation": 0.004218665219750408,
            "auditor_fp_violation": 0.01974454579070627,
            "ave_precision_score": 0.5590036360990046,
            "fpr": 0.42653508771929827,
            "logloss": 13.810510655691719,
            "mae": 0.43478394790427527,
            "precision": 0.5518433179723502,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5477497255762898,
            "auc_prc": 0.7514076503733182,
            "auditor_fn_violation": 0.004439929878598228,
            "auditor_fp_violation": 0.011811395165175773,
            "ave_precision_score": 0.5335911735613335,
            "fpr": 0.4445664105378705,
            "logloss": 14.604210033307709,
            "mae": 0.45293880500401706,
            "precision": 0.532871972318339,
            "recall": 0.9850746268656716
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.7696681305631907,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.016285591026747197,
            "ave_precision_score": 0.5595900719638491,
            "fpr": 0.4199561403508772,
            "logloss": 13.909287789621864,
            "mae": 0.42778642488007645,
            "precision": 0.5556844547563805,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7518617314278301,
            "auditor_fn_violation": 0.004737173470892363,
            "auditor_fp_violation": 0.012119345754007079,
            "ave_precision_score": 0.533509214403216,
            "fpr": 0.43468715697036225,
            "logloss": 14.666915601176049,
            "mae": 0.44706187059582486,
            "precision": 0.5373831775700935,
            "recall": 0.9808102345415778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5668859649122807,
            "auc_prc": 0.7688759531698729,
            "auditor_fn_violation": 0.0014017001266051728,
            "auditor_fp_violation": 0.0047762849747319085,
            "ave_precision_score": 0.5544660792425926,
            "fpr": 0.42872807017543857,
            "logloss": 14.327451647254847,
            "mae": 0.43698207739352773,
            "precision": 0.551605504587156,
            "recall": 0.9917525773195877
        },
        "train": {
            "accuracy": 0.5477497255762898,
            "auc_prc": 0.755647054423486,
            "auditor_fn_violation": 0.0005172506606063301,
            "auditor_fp_violation": 0.004726048149564653,
            "ave_precision_score": 0.5333096618497604,
            "fpr": 0.4489571899012075,
            "logloss": 14.961965089296669,
            "mae": 0.45669381655491625,
            "precision": 0.5325714285714286,
            "recall": 0.9936034115138592
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.7693547492111442,
            "auditor_fn_violation": 0.003581117742810636,
            "auditor_fp_violation": 0.01457794075352316,
            "ave_precision_score": 0.5598001036009131,
            "fpr": 0.40899122807017546,
            "logloss": 13.836029435325372,
            "mae": 0.42457229588274853,
            "precision": 0.5580568720379147,
            "recall": 0.9711340206185567
        },
        "train": {
            "accuracy": 0.5499451152579583,
            "auc_prc": 0.7520151769922035,
            "auditor_fn_violation": 0.00550953871071177,
            "auditor_fp_violation": 0.007872607795123474,
            "ave_precision_score": 0.5339663779400108,
            "fpr": 0.4313940724478595,
            "logloss": 14.649502849695443,
            "mae": 0.45114539211079124,
            "precision": 0.5349112426035503,
            "recall": 0.9637526652452025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5701754385964912,
            "auc_prc": 0.7696725520599004,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.01847086979744444,
            "ave_precision_score": 0.5595944927642902,
            "fpr": 0.4232456140350877,
            "logloss": 13.919021345951622,
            "mae": 0.43103526310549795,
            "precision": 0.553757225433526,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7518502436195512,
            "auditor_fn_violation": 0.002946690414947374,
            "auditor_fp_violation": 0.011495994158872697,
            "ave_precision_score": 0.533497727682083,
            "fpr": 0.43798024149286496,
            "logloss": 14.67587807515827,
            "mae": 0.4486516046800679,
            "precision": 0.537122969837587,
            "recall": 0.9872068230277186
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5668859649122807,
            "auc_prc": 0.7690770131927538,
            "auditor_fn_violation": 0.002136462289744981,
            "auditor_fp_violation": 0.001959303997699173,
            "ave_precision_score": 0.559199454756605,
            "fpr": 0.4298245614035088,
            "logloss": 13.906528898448231,
            "mae": 0.43713804335265305,
            "precision": 0.551487414187643,
            "recall": 0.9938144329896907
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7521807263152194,
            "auditor_fn_violation": 0.0018466550733864005,
            "auditor_fp_violation": 0.00580139173798373,
            "ave_precision_score": 0.5341319153117251,
            "fpr": 0.44127332601536773,
            "logloss": 14.671575691443413,
            "mae": 0.45072726330801244,
            "precision": 0.5368663594470046,
            "recall": 0.9936034115138592
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5789473684210527,
            "auc_prc": 0.7671240148891846,
            "auditor_fn_violation": 0.006327997829625611,
            "auditor_fp_violation": 0.0179187723406878,
            "ave_precision_score": 0.5590170453528374,
            "fpr": 0.41118421052631576,
            "logloss": 13.770981340417377,
            "mae": 0.4242106658523306,
            "precision": 0.5593419506462984,
            "recall": 0.9814432989690721
        },
        "train": {
            "accuracy": 0.5565312843029637,
            "auc_prc": 0.7514677967067249,
            "auditor_fn_violation": 0.006857667129305644,
            "auditor_fp_violation": 0.008185525328935944,
            "ave_precision_score": 0.5336513161380914,
            "fpr": 0.43249176728869376,
            "logloss": 14.5811148365803,
            "mae": 0.446559189693443,
            "precision": 0.5381008206330598,
            "recall": 0.9786780383795309
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5570175438596491,
            "auc_prc": 0.7586217137598897,
            "auditor_fn_violation": 0.00808238379453789,
            "auditor_fp_violation": 0.013948806442335348,
            "ave_precision_score": 0.5748733554773297,
            "fpr": 0.41228070175438597,
            "logloss": 13.274268177031109,
            "mae": 0.44397492121019144,
            "precision": 0.5486194477791116,
            "recall": 0.9422680412371134
        },
        "train": {
            "accuracy": 0.535675082327113,
            "auc_prc": 0.74371029598027,
            "auditor_fn_violation": 0.012175284780425927,
            "auditor_fp_violation": 0.012583755109744642,
            "ave_precision_score": 0.550042045315323,
            "fpr": 0.4270032930845225,
            "logloss": 14.205956754397455,
            "mae": 0.46633008695112826,
            "precision": 0.5279126213592233,
            "recall": 0.9275053304904051
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.7696713402311972,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.01775442705123465,
            "ave_precision_score": 0.5595932809914445,
            "fpr": 0.41885964912280704,
            "logloss": 13.907943757966418,
            "mae": 0.4275635203523866,
            "precision": 0.5563298490127758,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.7518545660749941,
            "auditor_fn_violation": 0.0045007828974930894,
            "auditor_fp_violation": 0.011495994158872697,
            "ave_precision_score": 0.53350204968761,
            "fpr": 0.43798024149286496,
            "logloss": 14.671210998276106,
            "mae": 0.44731230270421546,
            "precision": 0.536046511627907,
            "recall": 0.9829424307036247
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5844298245614035,
            "auc_prc": 0.7706999240903688,
            "auditor_fn_violation": 0.0035494664496292278,
            "auditor_fp_violation": 0.022484489913307863,
            "ave_precision_score": 0.5641433663064034,
            "fpr": 0.4100877192982456,
            "logloss": 13.671421313612184,
            "mae": 0.4217850091530439,
            "precision": 0.5620608899297423,
            "recall": 0.9896907216494846
        },
        "train": {
            "accuracy": 0.557628979143798,
            "auc_prc": 0.7531830620574176,
            "auditor_fn_violation": 0.00674064209296937,
            "auditor_fp_violation": 0.026754449140966894,
            "ave_precision_score": 0.5380431803593148,
            "fpr": 0.4313940724478595,
            "logloss": 14.433564549999494,
            "mae": 0.4441371664895597,
            "precision": 0.5387323943661971,
            "recall": 0.9786780383795309
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5778508771929824,
            "auc_prc": 0.7693883296875471,
            "auditor_fn_violation": 0.0013293543136190995,
            "auditor_fp_violation": 0.010859628579645854,
            "ave_precision_score": 0.5598336805601568,
            "fpr": 0.4133771929824561,
            "logloss": 13.844909713741721,
            "mae": 0.42604418413403883,
            "precision": 0.5585480093676815,
            "recall": 0.9835051546391752
        },
        "train": {
            "accuracy": 0.5587266739846323,
            "auc_prc": 0.7521531338282984,
            "auditor_fn_violation": 0.00228666921001079,
            "auditor_fp_violation": 0.006496764035344787,
            "ave_precision_score": 0.5341043249024886,
            "fpr": 0.43468715697036225,
            "logloss": 14.645613012065898,
            "mae": 0.44566807190340707,
            "precision": 0.5389988358556461,
            "recall": 0.9872068230277186
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.7696681305631907,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.016285591026747197,
            "ave_precision_score": 0.5595900719638491,
            "fpr": 0.4199561403508772,
            "logloss": 13.909171322028056,
            "mae": 0.4277379841622573,
            "precision": 0.5556844547563805,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7518617314278301,
            "auditor_fn_violation": 0.004737173470892363,
            "auditor_fp_violation": 0.012119345754007079,
            "ave_precision_score": 0.533509214403216,
            "fpr": 0.43468715697036225,
            "logloss": 14.666842284851194,
            "mae": 0.44703558544430994,
            "precision": 0.5373831775700935,
            "recall": 0.9808102345415778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 9292,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.7257854168849837,
            "auditor_fn_violation": 0.023724905046120456,
            "auditor_fp_violation": 0.028755289864004274,
            "ave_precision_score": 0.7158753345076216,
            "fpr": 0.22587719298245615,
            "logloss": 2.0205116895405624,
            "mae": 0.31534407014530846,
            "precision": 0.6730158730158731,
            "recall": 0.8742268041237113
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.7842634518886196,
            "auditor_fn_violation": 0.014848136610346417,
            "auditor_fp_violation": 0.02548787817077351,
            "ave_precision_score": 0.7765396948027028,
            "fpr": 0.22063666300768386,
            "logloss": 1.60064994245584,
            "mae": 0.2827929302972968,
            "precision": 0.6804451510333863,
            "recall": 0.9125799573560768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5679824561403509,
            "auc_prc": 0.7696695675536412,
            "auditor_fn_violation": 0.0038885874480014478,
            "auditor_fp_violation": 0.018234623443855548,
            "ave_precision_score": 0.5595915081591275,
            "fpr": 0.42653508771929827,
            "logloss": 13.921516674986941,
            "mae": 0.4345226141689509,
            "precision": 0.5523590333716916,
            "recall": 0.9896907216494846
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.7518483629434762,
            "auditor_fn_violation": 0.004030342251421269,
            "auditor_fp_violation": 0.01307548266287855,
            "ave_precision_score": 0.5334958469576079,
            "fpr": 0.44017563117453345,
            "logloss": 14.674275623862798,
            "mae": 0.45049807395086006,
            "precision": 0.5358796296296297,
            "recall": 0.9872068230277186
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 9292,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.709704098520545,
            "auditor_fn_violation": 0.008765147404593962,
            "auditor_fp_violation": 0.017767266526973174,
            "ave_precision_score": 0.7043728627470387,
            "fpr": 0.23355263157894737,
            "logloss": 1.3925374564632613,
            "mae": 0.34464404415824723,
            "precision": 0.6592,
            "recall": 0.8494845360824742
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7563325916119644,
            "auditor_fn_violation": 0.005113994087895168,
            "auditor_fp_violation": 0.020014304801545722,
            "ave_precision_score": 0.7516604572468006,
            "fpr": 0.22722283205268934,
            "logloss": 1.2666837335761896,
            "mae": 0.3255662958755861,
            "precision": 0.6617647058823529,
            "recall": 0.8635394456289979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5614035087719298,
            "auc_prc": 0.7557365977953839,
            "auditor_fn_violation": 0.02153870500994755,
            "auditor_fp_violation": 0.008088869715271797,
            "ave_precision_score": 0.5540326242208278,
            "fpr": 0.3991228070175439,
            "logloss": 14.515057788645139,
            "mae": 0.4395992387229653,
            "precision": 0.5522755227552275,
            "recall": 0.9257731958762887
        },
        "train": {
            "accuracy": 0.5422612513721186,
            "auc_prc": 0.738154422006194,
            "auditor_fn_violation": 0.025078465286863475,
            "auditor_fp_violation": 0.009171463907694296,
            "ave_precision_score": 0.5263220464882362,
            "fpr": 0.41602634467618005,
            "logloss": 15.475941495170611,
            "mae": 0.45955746994380486,
            "precision": 0.5320987654320988,
            "recall": 0.9189765458422174
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5537280701754386,
            "auc_prc": 0.7678746873315923,
            "auditor_fn_violation": 0.004679869777536626,
            "auditor_fp_violation": 0.014002732240437158,
            "ave_precision_score": 0.5632872942170185,
            "fpr": 0.43969298245614036,
            "logloss": 13.669291548924065,
            "mae": 0.44512570495048004,
            "precision": 0.5443181818181818,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5455543358946213,
            "auc_prc": 0.7516240849320353,
            "auditor_fn_violation": 0.0028507298851516297,
            "auditor_fp_violation": 0.015469550143793058,
            "ave_precision_score": 0.5380963013828796,
            "fpr": 0.446761800219539,
            "logloss": 14.47145696074523,
            "mae": 0.4561260037318813,
            "precision": 0.5316455696202531,
            "recall": 0.9850746268656716
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.7487533634784589,
            "auditor_fn_violation": 0.026917164044130946,
            "auditor_fp_violation": 0.012236020378815896,
            "ave_precision_score": 0.5710398305295524,
            "fpr": 0.36403508771929827,
            "logloss": 13.56042829205891,
            "mae": 0.4301253955872341,
            "precision": 0.5631578947368421,
            "recall": 0.8824742268041237
        },
        "train": {
            "accuracy": 0.5587266739846323,
            "auc_prc": 0.7357929151056462,
            "auditor_fn_violation": 0.023330111243999543,
            "auditor_fp_violation": 0.01888680829082457,
            "ave_precision_score": 0.544236770660963,
            "fpr": 0.38529088913282106,
            "logloss": 14.371087320362427,
            "mae": 0.44448649567062753,
            "precision": 0.5435630689206762,
            "recall": 0.8912579957356077
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 9292,
        "test": {
            "accuracy": 0.643640350877193,
            "auc_prc": 0.7222872272965838,
            "auditor_fn_violation": 0.019748146138542234,
            "auditor_fp_violation": 0.041271210813920045,
            "ave_precision_score": 0.7126201730651808,
            "fpr": 0.30701754385964913,
            "logloss": 2.1877012407887553,
            "mae": 0.34960568383814,
            "precision": 0.6111111111111112,
            "recall": 0.9072164948453608
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.779579870169989,
            "auditor_fn_violation": 0.009495411448325256,
            "auditor_fp_violation": 0.03769911240693188,
            "ave_precision_score": 0.772026123050715,
            "fpr": 0.300768386388584,
            "logloss": 1.805578419961968,
            "mae": 0.3270163282027708,
            "precision": 0.6199722607489597,
            "recall": 0.9530916844349681
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5723684210526315,
            "auc_prc": 0.7684342020105625,
            "auditor_fn_violation": 0.003542684029661784,
            "auditor_fp_violation": 0.017325588561567865,
            "ave_precision_score": 0.561621733185746,
            "fpr": 0.42105263157894735,
            "logloss": 13.717651201676919,
            "mae": 0.4296158343888879,
            "precision": 0.5550405561993047,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7506241270734318,
            "auditor_fn_violation": 0.005099951083534812,
            "auditor_fp_violation": 0.01779656386746204,
            "ave_precision_score": 0.535024764672851,
            "fpr": 0.43249176728869376,
            "logloss": 14.51603002774633,
            "mae": 0.44615963489423255,
            "precision": 0.5375586854460094,
            "recall": 0.976545842217484
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.7696659477948816,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.017579810181190692,
            "ave_precision_score": 0.5595878892138977,
            "fpr": 0.42214912280701755,
            "logloss": 13.919029952745323,
            "mae": 0.43080872170681334,
            "precision": 0.5543981481481481,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7518529928509836,
            "auditor_fn_violation": 0.0032415935065147838,
            "auditor_fp_violation": 0.012228618543592402,
            "ave_precision_score": 0.5335004766343527,
            "fpr": 0.43907793633369924,
            "logloss": 14.676963333262716,
            "mae": 0.44850506114558925,
            "precision": 0.5370370370370371,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.7696784482491329,
            "auditor_fn_violation": 0.0038885874480014478,
            "auditor_fp_violation": 0.01847086979744444,
            "ave_precision_score": 0.5596003879767208,
            "fpr": 0.4232456140350877,
            "logloss": 13.913488482188765,
            "mae": 0.43154524740241057,
            "precision": 0.5542725173210161,
            "recall": 0.9896907216494846
        },
        "train": {
            "accuracy": 0.5543358946212953,
            "auc_prc": 0.7518463107003525,
            "auditor_fn_violation": 0.0032415935065147838,
            "auditor_fp_violation": 0.01234037480566828,
            "ave_precision_score": 0.533493795047803,
            "fpr": 0.44017563117453345,
            "logloss": 14.678107110008552,
            "mae": 0.4494569055739486,
            "precision": 0.5364161849710982,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 9292,
        "test": {
            "accuracy": 0.569078947368421,
            "auc_prc": 0.7677803329079362,
            "auditor_fn_violation": 0.0056610598661602455,
            "auditor_fp_violation": 0.019341386252516538,
            "ave_precision_score": 0.5594727621603155,
            "fpr": 0.42214912280701755,
            "logloss": 13.80153348123325,
            "mae": 0.43314219633117046,
            "precision": 0.5533642691415314,
            "recall": 0.9835051546391752
        },
        "train": {
            "accuracy": 0.5477497255762898,
            "auc_prc": 0.7513554216323426,
            "auditor_fn_violation": 0.005032076562459774,
            "auditor_fp_violation": 0.01275263123910376,
            "ave_precision_score": 0.533538948441455,
            "fpr": 0.442371020856202,
            "logloss": 14.60799776644144,
            "mae": 0.4531816867257944,
            "precision": 0.5330243337195828,
            "recall": 0.9808102345415778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.7691350492621118,
            "auditor_fn_violation": 0.0011462289744981011,
            "auditor_fp_violation": 0.010677308024158763,
            "ave_precision_score": 0.5592567874392261,
            "fpr": 0.42214912280701755,
            "logloss": 13.878608662341025,
            "mae": 0.43059384364479275,
            "precision": 0.5554272517321016,
            "recall": 0.9917525773195877
        },
        "train": {
            "accuracy": 0.557628979143798,
            "auc_prc": 0.7521681431672698,
            "auditor_fn_violation": 0.001921551096641616,
            "auditor_fp_violation": 0.004291440463713988,
            "ave_precision_score": 0.5341193329358771,
            "fpr": 0.43688254665203075,
            "logloss": 14.650732546674698,
            "mae": 0.44729233997662377,
            "precision": 0.5382830626450116,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5932017543859649,
            "auc_prc": 0.7719617925440713,
            "auditor_fn_violation": 0.006637728341472237,
            "auditor_fp_violation": 0.015106927153950457,
            "ave_precision_score": 0.5646853224746603,
            "fpr": 0.39473684210526316,
            "logloss": 13.617190317252327,
            "mae": 0.40850788345974476,
            "precision": 0.5683453237410072,
            "recall": 0.977319587628866
        },
        "train": {
            "accuracy": 0.5609220636663008,
            "auc_prc": 0.7538277954187034,
            "auditor_fn_violation": 0.009160719844403512,
            "auditor_fp_violation": 0.01160278347596744,
            "ave_precision_score": 0.5381535736934684,
            "fpr": 0.42041712403951703,
            "logloss": 14.417822774392508,
            "mae": 0.4381754078653276,
            "precision": 0.5413173652694611,
            "recall": 0.9637526652452025
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5679824561403509,
            "auc_prc": 0.7696623274421285,
            "auditor_fn_violation": 0.0038885874480014478,
            "auditor_fp_violation": 0.018234623443855548,
            "ave_precision_score": 0.5595842690788857,
            "fpr": 0.42653508771929827,
            "logloss": 13.935641774310804,
            "mae": 0.4361456976979146,
            "precision": 0.5523590333716916,
            "recall": 0.9896907216494846
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.7518501780299182,
            "auditor_fn_violation": 0.002349862729632378,
            "auditor_fp_violation": 0.012946342093368632,
            "ave_precision_score": 0.5334976620825724,
            "fpr": 0.442371020856202,
            "logloss": 14.690768167390315,
            "mae": 0.45116541534394444,
            "precision": 0.5357142857142857,
            "recall": 0.9914712153518124
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5888157894736842,
            "auc_prc": 0.7547710760851155,
            "auditor_fn_violation": 0.02470157352143245,
            "auditor_fp_violation": 0.01706366325650192,
            "ave_precision_score": 0.6376757928127887,
            "fpr": 0.3333333333333333,
            "logloss": 10.51359393314078,
            "mae": 0.41556502707812737,
            "precision": 0.5766016713091922,
            "recall": 0.8536082474226804
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.7533011034436009,
            "auditor_fn_violation": 0.023545437310858286,
            "auditor_fp_violation": 0.016783307091307362,
            "ave_precision_score": 0.6305528760801558,
            "fpr": 0.3611416026344676,
            "logloss": 10.596982523327597,
            "mae": 0.4282144329077407,
            "precision": 0.55359565807327,
            "recall": 0.8699360341151386
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 9292,
        "test": {
            "accuracy": 0.569078947368421,
            "auc_prc": 0.7696754198476401,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.019097436213484534,
            "ave_precision_score": 0.5595973600934874,
            "fpr": 0.4243421052631579,
            "logloss": 13.91747175415752,
            "mae": 0.43104813836244876,
            "precision": 0.5531177829099307,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5565312843029637,
            "auc_prc": 0.7518463697516655,
            "auditor_fn_violation": 0.0032415935065147838,
            "auditor_fp_violation": 0.011495994158872697,
            "ave_precision_score": 0.5334938540184891,
            "fpr": 0.43798024149286496,
            "logloss": 14.679050336453527,
            "mae": 0.4484385920233001,
            "precision": 0.5376593279258401,
            "recall": 0.9893390191897654
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5778508771929824,
            "auc_prc": 0.7671184034502356,
            "auditor_fn_violation": 0.005064206909025141,
            "auditor_fp_violation": 0.016806873741731378,
            "ave_precision_score": 0.5590114345006123,
            "fpr": 0.4144736842105263,
            "logloss": 13.786563732879806,
            "mae": 0.4279966252374113,
            "precision": 0.5584112149532711,
            "recall": 0.9855670103092784
        },
        "train": {
            "accuracy": 0.5609220636663008,
            "auc_prc": 0.7514551266545075,
            "auditor_fn_violation": 0.006274882448351,
            "auditor_fp_violation": 0.015690579195454267,
            "ave_precision_score": 0.5336386447284542,
            "fpr": 0.4281009879253567,
            "logloss": 14.626227472630468,
            "mae": 0.44596680489876056,
            "precision": 0.5406360424028268,
            "recall": 0.9786780383795309
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5723684210526315,
            "auc_prc": 0.769664485283699,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.017215169070216533,
            "ave_precision_score": 0.5595864269551984,
            "fpr": 0.42105263157894735,
            "logloss": 13.923650398333217,
            "mae": 0.42971929783147234,
            "precision": 0.5550405561993047,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.751873423933293,
            "auditor_fn_violation": 0.0036535216344184673,
            "auditor_fp_violation": 0.011841196835062661,
            "ave_precision_score": 0.5335209055720356,
            "fpr": 0.43688254665203075,
            "logloss": 14.671582254360144,
            "mae": 0.447723053038072,
            "precision": 0.5372093023255814,
            "recall": 0.9850746268656716
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5800438596491229,
            "auc_prc": 0.7694923158232072,
            "auditor_fn_violation": 0.004521613311629589,
            "auditor_fp_violation": 0.014973396606269776,
            "ave_precision_score": 0.5599376544967527,
            "fpr": 0.40350877192982454,
            "logloss": 13.839080086922628,
            "mae": 0.42034842416714613,
            "precision": 0.5608591885441527,
            "recall": 0.9690721649484536
        },
        "train": {
            "accuracy": 0.5510428100987925,
            "auc_prc": 0.7521050141224612,
            "auditor_fn_violation": 0.009010927797893082,
            "auditor_fp_violation": 0.011404105676721427,
            "ave_precision_score": 0.5340562082604405,
            "fpr": 0.42590559824368823,
            "logloss": 14.665955729373563,
            "mae": 0.449872529869053,
            "precision": 0.5358851674641149,
            "recall": 0.9552238805970149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5668859649122807,
            "auc_prc": 0.748740741464549,
            "auditor_fn_violation": 0.026917164044130946,
            "auditor_fp_violation": 0.008928571428571438,
            "ave_precision_score": 0.5710577096960053,
            "fpr": 0.3706140350877193,
            "logloss": 13.562531844345436,
            "mae": 0.4339147707899841,
            "precision": 0.5587467362924282,
            "recall": 0.8824742268041237
        },
        "train": {
            "accuracy": 0.5521405049396267,
            "auc_prc": 0.7337367998156129,
            "auditor_fn_violation": 0.023330111243999543,
            "auditor_fp_violation": 0.01730980325930931,
            "ave_precision_score": 0.5411338512578918,
            "fpr": 0.3918770581778266,
            "logloss": 14.448341510759294,
            "mae": 0.4484477763418129,
            "precision": 0.5393548387096774,
            "recall": 0.8912579957356077
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5679824561403509,
            "auc_prc": 0.7696646447250294,
            "auditor_fn_violation": 0.0038885874480014478,
            "auditor_fp_violation": 0.018234623443855548,
            "ave_precision_score": 0.5595865862722443,
            "fpr": 0.42653508771929827,
            "logloss": 13.929856570718616,
            "mae": 0.43481592284800635,
            "precision": 0.5523590333716916,
            "recall": 0.9896907216494846
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.7518476986295661,
            "auditor_fn_violation": 0.002349862729632378,
            "auditor_fp_violation": 0.013107767805256012,
            "ave_precision_score": 0.533495182837876,
            "fpr": 0.442371020856202,
            "logloss": 14.685663212084425,
            "mae": 0.4506417080003754,
            "precision": 0.5357142857142857,
            "recall": 0.9914712153518124
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 9292,
        "test": {
            "accuracy": 0.5668859649122807,
            "auc_prc": 0.7696643461167265,
            "auditor_fn_violation": 0.003515554349792006,
            "auditor_fp_violation": 0.017941883397017133,
            "ave_precision_score": 0.5595862879233932,
            "fpr": 0.42653508771929827,
            "logloss": 13.921915177957661,
            "mae": 0.4336082306926185,
            "precision": 0.5518433179723502,
            "recall": 0.9876288659793815
        },
        "train": {
            "accuracy": 0.5532381997804611,
            "auc_prc": 0.7521418842432867,
            "auditor_fn_violation": 0.004636531939643167,
            "auditor_fp_violation": 0.012591205527216378,
            "ave_precision_score": 0.5340930757346535,
            "fpr": 0.43907793633369924,
            "logloss": 14.652879972574034,
            "mae": 0.44990876692961806,
            "precision": 0.5359628770301624,
            "recall": 0.9850746268656716
        }
    }
]
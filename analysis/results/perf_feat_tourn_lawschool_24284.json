[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.8797349616262494,
            "auditor_fn_violation": 0.014007731496430234,
            "auditor_fp_violation": 0.013476732611671025,
            "ave_precision_score": 0.8799563112569411,
            "fpr": 0.10197368421052631,
            "logloss": 0.7237425304450013,
            "mae": 0.23722476879862475,
            "precision": 0.8033826638477801,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.847619547436872,
            "auditor_fn_violation": 0.012131298466768175,
            "auditor_fp_violation": 0.01879617824989048,
            "ave_precision_score": 0.8479641109355652,
            "fpr": 0.11964873765093303,
            "logloss": 0.7946772439482271,
            "mae": 0.2379585927661553,
            "precision": 0.7775510204081633,
            "recall": 0.8193548387096774
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.8822402011237694,
            "auditor_fn_violation": 0.01604599433143185,
            "auditor_fp_violation": 0.013608933681722038,
            "ave_precision_score": 0.8824365963098768,
            "fpr": 0.10197368421052631,
            "logloss": 0.7159311074520879,
            "mae": 0.23365163599668057,
            "precision": 0.8029661016949152,
            "recall": 0.7750511247443763
        },
        "train": {
            "accuracy": 0.7892425905598244,
            "auc_prc": 0.849743433737062,
            "auditor_fn_violation": 0.012959881024043059,
            "auditor_fp_violation": 0.016928128061116494,
            "ave_precision_score": 0.8500865550494645,
            "fpr": 0.11745334796926454,
            "logloss": 0.7920856593522566,
            "mae": 0.23575974743650432,
            "precision": 0.7802874743326489,
            "recall": 0.8172043010752689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8508053404047231,
            "auditor_fn_violation": 0.023194489290711447,
            "auditor_fp_violation": 0.019817199618431425,
            "ave_precision_score": 0.8510560016622288,
            "fpr": 0.11732456140350878,
            "logloss": 0.743775170102396,
            "mae": 0.26695291858284104,
            "precision": 0.7698924731182796,
            "recall": 0.7321063394683026
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8102684272710612,
            "auditor_fn_violation": 0.02068623632307638,
            "auditor_fp_violation": 0.032724104492673024,
            "ave_precision_score": 0.8106762899571708,
            "fpr": 0.1394072447859495,
            "logloss": 0.8308711075770785,
            "mae": 0.2742051242947888,
            "precision": 0.7381443298969073,
            "recall": 0.7698924731182796
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.88175568530608,
            "auditor_fn_violation": 0.016438399167653293,
            "auditor_fp_violation": 0.014985380116959065,
            "ave_precision_score": 0.881960199842114,
            "fpr": 0.10307017543859649,
            "logloss": 0.7178484145368196,
            "mae": 0.23376213124215123,
            "precision": 0.8016877637130801,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7859495060373216,
            "auc_prc": 0.849894473866746,
            "auditor_fn_violation": 0.013387155790045211,
            "auditor_fp_violation": 0.017942142129331096,
            "ave_precision_score": 0.850239014529077,
            "fpr": 0.11964873765093303,
            "logloss": 0.7936130581021552,
            "mae": 0.23619378196421878,
            "precision": 0.7766393442622951,
            "recall": 0.8150537634408602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7741228070175439,
            "auc_prc": 0.8760962407796469,
            "auditor_fn_violation": 0.01886233989882682,
            "auditor_fp_violation": 0.01518238563311352,
            "ave_precision_score": 0.8762934470143466,
            "fpr": 0.10087719298245613,
            "logloss": 0.7736818861285354,
            "mae": 0.2396376614039409,
            "precision": 0.8029978586723768,
            "recall": 0.7668711656441718
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8408354621610665,
            "auditor_fn_violation": 0.015610873080509426,
            "auditor_fp_violation": 0.022692256575093647,
            "ave_precision_score": 0.8411842921003171,
            "fpr": 0.1163556531284303,
            "logloss": 0.8606942155444953,
            "mae": 0.2451754701642457,
            "precision": 0.7758985200845666,
            "recall": 0.789247311827957
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8366500956244434,
            "auditor_fn_violation": 0.020113550748035735,
            "auditor_fp_violation": 0.02557701878810502,
            "ave_precision_score": 0.8370144138745433,
            "fpr": 0.11732456140350878,
            "logloss": 0.8508221707866668,
            "mae": 0.2758371216707204,
            "precision": 0.76431718061674,
            "recall": 0.7096114519427403
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.7932577580368418,
            "auditor_fn_violation": 0.02117488757480259,
            "auditor_fp_violation": 0.02732669465870551,
            "ave_precision_score": 0.7936977589175549,
            "fpr": 0.13721185510428102,
            "logloss": 0.9496431663850833,
            "mae": 0.28511232934069786,
            "precision": 0.7368421052631579,
            "recall": 0.7526881720430108
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7763157894736842,
            "auc_prc": 0.8817977336971212,
            "auditor_fn_violation": 0.01707297384565709,
            "auditor_fp_violation": 0.012240263780017423,
            "ave_precision_score": 0.8819941925976317,
            "fpr": 0.10526315789473684,
            "logloss": 0.7154373430789788,
            "mae": 0.23506153652662135,
            "precision": 0.7987421383647799,
            "recall": 0.7791411042944786
        },
        "train": {
            "accuracy": 0.7848518111964874,
            "auc_prc": 0.84778098717734,
            "auditor_fn_violation": 0.012759227128406693,
            "auditor_fp_violation": 0.018673118290155697,
            "ave_precision_score": 0.8481480661839751,
            "fpr": 0.12184412733260154,
            "logloss": 0.7941926889244744,
            "mae": 0.23823804264191384,
            "precision": 0.7739307535641547,
            "recall": 0.8172043010752689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7839912280701754,
            "auc_prc": 0.8946399931276289,
            "auditor_fn_violation": 0.013956158289383996,
            "auditor_fp_violation": 0.010176890216083947,
            "ave_precision_score": 0.894764610875166,
            "fpr": 0.08991228070175439,
            "logloss": 0.5419922876405259,
            "mae": 0.2395798141349332,
            "precision": 0.8201754385964912,
            "recall": 0.7648261758691206
        },
        "train": {
            "accuracy": 0.8046103183315039,
            "auc_prc": 0.8598280396204406,
            "auditor_fn_violation": 0.00658380841093918,
            "auditor_fp_violation": 0.015914113992901907,
            "ave_precision_score": 0.8601468163280952,
            "fpr": 0.09769484083424808,
            "logloss": 0.5710978609696152,
            "mae": 0.22853594752078804,
            "precision": 0.8086021505376344,
            "recall": 0.8086021505376344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7817982456140351,
            "auc_prc": 0.8822357470259248,
            "auditor_fn_violation": 0.015097495784450908,
            "auditor_fp_violation": 0.015794139604329977,
            "ave_precision_score": 0.8824319427236416,
            "fpr": 0.09868421052631579,
            "logloss": 0.7272936889783685,
            "mae": 0.23225669975536245,
            "precision": 0.8085106382978723,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7914379802414928,
            "auc_prc": 0.8505970044264681,
            "auditor_fn_violation": 0.01218087178216069,
            "auditor_fp_violation": 0.0159141139929019,
            "ave_precision_score": 0.850945397040869,
            "fpr": 0.11525795828759605,
            "logloss": 0.8025707553741603,
            "mae": 0.23397918934928846,
            "precision": 0.7835051546391752,
            "recall": 0.8172043010752689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7752192982456141,
            "auc_prc": 0.8803459436518578,
            "auditor_fn_violation": 0.014483101926595628,
            "auditor_fp_violation": 0.013671145949981344,
            "ave_precision_score": 0.8804817367296738,
            "fpr": 0.10416666666666667,
            "logloss": 0.7229486176771257,
            "mae": 0.2371329116459459,
            "precision": 0.79957805907173,
            "recall": 0.7750511247443763
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.8482269046551819,
            "auditor_fn_violation": 0.012131298466768175,
            "auditor_fp_violation": 0.01879617824989048,
            "ave_precision_score": 0.8485645654134927,
            "fpr": 0.11964873765093303,
            "logloss": 0.7943771568732098,
            "mae": 0.2378340803448444,
            "precision": 0.7775510204081633,
            "recall": 0.8193548387096774
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7828947368421053,
            "auc_prc": 0.8943826753375475,
            "auditor_fn_violation": 0.012231819323359525,
            "auditor_fp_violation": 0.005298411513417115,
            "ave_precision_score": 0.8915378680586827,
            "fpr": 0.07017543859649122,
            "logloss": 0.9509422932989987,
            "mae": 0.22520483507368122,
            "precision": 0.847255369928401,
            "recall": 0.7259713701431493
        },
        "train": {
            "accuracy": 0.7925356750823271,
            "auc_prc": 0.8623686420509475,
            "auditor_fn_violation": 0.011857464915076192,
            "auditor_fp_violation": 0.011080318774519699,
            "ave_precision_score": 0.8565846393089854,
            "fpr": 0.09001097694840834,
            "logloss": 1.1534510524340313,
            "mae": 0.21725756481076822,
            "precision": 0.8136363636363636,
            "recall": 0.7698924731182796
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7741228070175439,
            "auc_prc": 0.879325641387347,
            "auditor_fn_violation": 0.014007731496430234,
            "auditor_fp_violation": 0.01216249844469329,
            "ave_precision_score": 0.8795319864119959,
            "fpr": 0.10635964912280702,
            "logloss": 0.7345270064806588,
            "mae": 0.2371742941012219,
            "precision": 0.7966457023060797,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7859495060373216,
            "auc_prc": 0.8466491846065544,
            "auditor_fn_violation": 0.01103360362593393,
            "auditor_fp_violation": 0.019780657927768733,
            "ave_precision_score": 0.8470002884540345,
            "fpr": 0.12184412733260154,
            "logloss": 0.8129043718260093,
            "mae": 0.2387240159632452,
            "precision": 0.774390243902439,
            "recall": 0.8193548387096774
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7817982456140351,
            "auc_prc": 0.8753014728152735,
            "auditor_fn_violation": 0.009935690453126686,
            "auditor_fp_violation": 0.010135415370577753,
            "ave_precision_score": 0.8754762928330655,
            "fpr": 0.10416666666666667,
            "logloss": 0.7318043840712122,
            "mae": 0.23821955310644272,
            "precision": 0.8020833333333334,
            "recall": 0.787321063394683
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8466949879138184,
            "auditor_fn_violation": 0.008392054105732806,
            "auditor_fp_violation": 0.01874941546519126,
            "ave_precision_score": 0.8470032966544883,
            "fpr": 0.1207464324917673,
            "logloss": 0.7909740968702021,
            "mae": 0.23853186895412207,
            "precision": 0.7708333333333334,
            "recall": 0.7956989247311828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.841549269536496,
            "auditor_fn_violation": 0.018147041940228894,
            "auditor_fp_violation": 0.009534030110737843,
            "ave_precision_score": 0.8428618958823413,
            "fpr": 0.11074561403508772,
            "logloss": 0.6805568054246148,
            "mae": 0.2529955897532533,
            "precision": 0.7873684210526316,
            "recall": 0.7648261758691206
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.7912168825300592,
            "auditor_fn_violation": 0.011196487376509334,
            "auditor_fp_violation": 0.009062135434869292,
            "ave_precision_score": 0.7926022864673227,
            "fpr": 0.12623490669593854,
            "logloss": 0.8065509655523357,
            "mae": 0.25551046000686795,
            "precision": 0.7638603696098563,
            "recall": 0.8
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.8796743324040199,
            "auditor_fn_violation": 0.016983281311663618,
            "auditor_fp_violation": 0.010498320268757003,
            "ave_precision_score": 0.8798942403392347,
            "fpr": 0.10526315789473684,
            "logloss": 0.7098364961166869,
            "mae": 0.23809931794159966,
            "precision": 0.799163179916318,
            "recall": 0.7811860940695297
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8478090222081709,
            "auditor_fn_violation": 0.016252965546545806,
            "auditor_fp_violation": 0.022391990273340785,
            "ave_precision_score": 0.8481662618159926,
            "fpr": 0.1251372118551043,
            "logloss": 0.785531191018357,
            "mae": 0.23952490865955675,
            "precision": 0.7668711656441718,
            "recall": 0.8064516129032258
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7741228070175439,
            "auc_prc": 0.8787723244895005,
            "auditor_fn_violation": 0.018212069027374166,
            "auditor_fp_violation": 0.012940151797934556,
            "ave_precision_score": 0.8790025404298871,
            "fpr": 0.10526315789473684,
            "logloss": 0.699187956519209,
            "mae": 0.2396878000968668,
            "precision": 0.7978947368421052,
            "recall": 0.7750511247443763
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.845730627398041,
            "auditor_fn_violation": 0.014978223150738292,
            "auditor_fp_violation": 0.021220459456665668,
            "ave_precision_score": 0.8460803447766618,
            "fpr": 0.1251372118551043,
            "logloss": 0.7733964232806717,
            "mae": 0.24298818574064637,
            "precision": 0.7692307692307693,
            "recall": 0.8172043010752689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8770485480228916,
            "auditor_fn_violation": 0.012828274674416105,
            "auditor_fp_violation": 0.01215472191116088,
            "ave_precision_score": 0.8773009534822385,
            "fpr": 0.1118421052631579,
            "logloss": 0.7130746794169236,
            "mae": 0.23820719597544726,
            "precision": 0.7888198757763976,
            "recall": 0.7791411042944786
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8490215171499449,
            "auditor_fn_violation": 0.00914981764101838,
            "auditor_fp_violation": 0.022337843891057477,
            "ave_precision_score": 0.8493548493737313,
            "fpr": 0.13172338090010977,
            "logloss": 0.8023647503449091,
            "mae": 0.24100461125784514,
            "precision": 0.7619047619047619,
            "recall": 0.8258064516129032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8762862729099602,
            "auditor_fn_violation": 0.025672245542281067,
            "auditor_fp_violation": 0.02657241508025383,
            "ave_precision_score": 0.8764721754293733,
            "fpr": 0.14802631578947367,
            "logloss": 0.5666099776324025,
            "mae": 0.284183655395167,
            "precision": 0.7448015122873346,
            "recall": 0.8057259713701431
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8456007073543821,
            "auditor_fn_violation": 0.02321919667622724,
            "auditor_fp_violation": 0.027580198175759164,
            "ave_precision_score": 0.8459553353228255,
            "fpr": 0.15148188803512624,
            "logloss": 0.574973664689815,
            "mae": 0.2843496176252485,
            "precision": 0.7366412213740458,
            "recall": 0.8301075268817204
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.8820311893967704,
            "auditor_fn_violation": 0.016438399167653293,
            "auditor_fp_violation": 0.014464352370287424,
            "ave_precision_score": 0.8822298254190306,
            "fpr": 0.10197368421052631,
            "logloss": 0.7171724303075881,
            "mae": 0.233702816930665,
            "precision": 0.8033826638477801,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.8494396847078018,
            "auditor_fn_violation": 0.012959881024043059,
            "auditor_fp_violation": 0.0174351350952238,
            "ave_precision_score": 0.8497946278882881,
            "fpr": 0.11855104281009879,
            "logloss": 0.7933082047295665,
            "mae": 0.2361140002469129,
            "precision": 0.7786885245901639,
            "recall": 0.8172043010752689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7719298245614035,
            "auc_prc": 0.8763387633207784,
            "auditor_fn_violation": 0.004345603271983643,
            "auditor_fp_violation": 0.007543237526440218,
            "ave_precision_score": 0.8765397731602633,
            "fpr": 0.08223684210526316,
            "logloss": 0.7508743307737517,
            "mae": 0.24336119914481932,
            "precision": 0.8259860788863109,
            "recall": 0.7280163599182005
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8485373262887057,
            "auditor_fn_violation": 0.006376072613103882,
            "auditor_fp_violation": 0.015106840657041742,
            "ave_precision_score": 0.8488427801053869,
            "fpr": 0.10098792535675083,
            "logloss": 0.7872171258049583,
            "mae": 0.24052804214158469,
            "precision": 0.7927927927927928,
            "recall": 0.7569892473118279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7796052631578947,
            "auc_prc": 0.8838310355239163,
            "auditor_fn_violation": 0.014377713199153307,
            "auditor_fp_violation": 0.01685174816473809,
            "ave_precision_score": 0.884018502429735,
            "fpr": 0.10087719298245613,
            "logloss": 0.6996650023834493,
            "mae": 0.23343351013647867,
            "precision": 0.8050847457627118,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7914379802414928,
            "auc_prc": 0.8520012856371,
            "auditor_fn_violation": 0.0106346564687275,
            "auditor_fp_violation": 0.017622186234020665,
            "ave_precision_score": 0.8523325058452067,
            "fpr": 0.1207464324917673,
            "logloss": 0.7856320571331277,
            "mae": 0.23542812752810416,
            "precision": 0.7777777777777778,
            "recall": 0.8279569892473119
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7807017543859649,
            "auc_prc": 0.8858021530704894,
            "auditor_fn_violation": 0.017032612205360023,
            "auditor_fp_violation": 0.013297872340425532,
            "ave_precision_score": 0.8859653859879693,
            "fpr": 0.09868421052631579,
            "logloss": 0.7009162041323799,
            "mae": 0.23311952323350096,
            "precision": 0.8081023454157783,
            "recall": 0.7750511247443763
        },
        "train": {
            "accuracy": 0.7892425905598244,
            "auc_prc": 0.8506059774126882,
            "auditor_fn_violation": 0.013387155790045211,
            "auditor_fp_violation": 0.020344272543354025,
            "ave_precision_score": 0.8509771614495676,
            "fpr": 0.1163556531284303,
            "logloss": 0.7924260348282,
            "mae": 0.23704969070728069,
            "precision": 0.7814432989690722,
            "recall": 0.8150537634408602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7807017543859649,
            "auc_prc": 0.8858098223814259,
            "auditor_fn_violation": 0.017032612205360023,
            "auditor_fp_violation": 0.013297872340425532,
            "ave_precision_score": 0.8859722374099418,
            "fpr": 0.09868421052631579,
            "logloss": 0.7009698925081177,
            "mae": 0.2331188426519869,
            "precision": 0.8081023454157783,
            "recall": 0.7750511247443763
        },
        "train": {
            "accuracy": 0.7892425905598244,
            "auc_prc": 0.8505718077212598,
            "auditor_fn_violation": 0.013387155790045211,
            "auditor_fp_violation": 0.020344272543354025,
            "ave_precision_score": 0.850944339161706,
            "fpr": 0.1163556531284303,
            "logloss": 0.792456610703084,
            "mae": 0.23704728068359932,
            "precision": 0.7814432989690722,
            "recall": 0.8150537634408602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.8846835345842139,
            "auditor_fn_violation": 0.014377713199153307,
            "auditor_fp_violation": 0.014596553440338436,
            "ave_precision_score": 0.8848634479468044,
            "fpr": 0.10197368421052631,
            "logloss": 0.7029573378459683,
            "mae": 0.2342286130132081,
            "precision": 0.8033826638477801,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7837541163556532,
            "auc_prc": 0.8488563935287812,
            "auditor_fn_violation": 0.018271307673241033,
            "auditor_fp_violation": 0.021087554700152108,
            "ave_precision_score": 0.8492265300708403,
            "fpr": 0.12294182217343579,
            "logloss": 0.7793785876449322,
            "mae": 0.23799644134047326,
            "precision": 0.7723577235772358,
            "recall": 0.8172043010752689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7796052631578947,
            "auc_prc": 0.8798535448063749,
            "auditor_fn_violation": 0.018673985577440533,
            "auditor_fp_violation": 0.011970677284227122,
            "ave_precision_score": 0.8800620182327434,
            "fpr": 0.10416666666666667,
            "logloss": 0.7169980190550943,
            "mae": 0.23689827194551713,
            "precision": 0.801255230125523,
            "recall": 0.7832310838445807
        },
        "train": {
            "accuracy": 0.7837541163556532,
            "auc_prc": 0.8495868485243978,
            "auditor_fn_violation": 0.013387155790045211,
            "auditor_fp_violation": 0.01978558032615812,
            "ave_precision_score": 0.8499311799286329,
            "fpr": 0.12184412733260154,
            "logloss": 0.7913291497448367,
            "mae": 0.23794011626491648,
            "precision": 0.773469387755102,
            "recall": 0.8150537634408602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8431772195308741,
            "auditor_fn_violation": 0.017543859649122806,
            "auditor_fp_violation": 0.01491798349301149,
            "ave_precision_score": 0.8434341250322689,
            "fpr": 0.08662280701754387,
            "logloss": 1.0051832141227346,
            "mae": 0.2860799342578016,
            "precision": 0.8025,
            "recall": 0.656441717791411
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7996857925923718,
            "auditor_fn_violation": 0.016486668319110515,
            "auditor_fp_violation": 0.022519972631464956,
            "ave_precision_score": 0.8000957813178649,
            "fpr": 0.10976948408342481,
            "logloss": 1.0474575058566944,
            "mae": 0.2914497118512535,
            "precision": 0.7624703087885986,
            "recall": 0.6903225806451613
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7796052631578947,
            "auc_prc": 0.8828992731042877,
            "auditor_fn_violation": 0.01610429447852761,
            "auditor_fp_violation": 0.013391190742814484,
            "ave_precision_score": 0.883107299124067,
            "fpr": 0.10197368421052631,
            "logloss": 0.7084505706942352,
            "mae": 0.23358872624686455,
            "precision": 0.8037974683544303,
            "recall": 0.7791411042944786
        },
        "train": {
            "accuracy": 0.7837541163556532,
            "auc_prc": 0.8522827326927477,
            "auditor_fn_violation": 0.017327054046716955,
            "auditor_fp_violation": 0.0197954251229369,
            "ave_precision_score": 0.8526475155630535,
            "fpr": 0.12184412733260154,
            "logloss": 0.7776115499035889,
            "mae": 0.23563923664271907,
            "precision": 0.773469387755102,
            "recall": 0.8150537634408602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7730263157894737,
            "auc_prc": 0.8703624467153035,
            "auditor_fn_violation": 0.005971280450615294,
            "auditor_fp_violation": 0.011076375927999671,
            "ave_precision_score": 0.8705776326073431,
            "fpr": 0.15350877192982457,
            "logloss": 0.48934899224200645,
            "mae": 0.31355902482653364,
            "precision": 0.7508896797153025,
            "recall": 0.8629856850715747
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.849581220098949,
            "auditor_fn_violation": 0.012702571910815249,
            "auditor_fp_violation": 0.01429464492279219,
            "ave_precision_score": 0.8499171533951846,
            "fpr": 0.1668496158068057,
            "logloss": 0.5204392979613428,
            "mae": 0.32250303442106387,
            "precision": 0.7280858676207513,
            "recall": 0.875268817204301
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7719298245614035,
            "auc_prc": 0.8762919904757314,
            "auditor_fn_violation": 0.019925196426649444,
            "auditor_fp_violation": 0.012305068226120857,
            "ave_precision_score": 0.876492960148209,
            "fpr": 0.10307017543859649,
            "logloss": 0.7738092789426594,
            "mae": 0.2398037734844716,
            "precision": 0.7995735607675906,
            "recall": 0.7668711656441718
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8441334976080773,
            "auditor_fn_violation": 0.016045229748710508,
            "auditor_fp_violation": 0.018434381968270228,
            "ave_precision_score": 0.84449694473928,
            "fpr": 0.12184412733260154,
            "logloss": 0.8550762923266401,
            "mae": 0.24368248306430174,
            "precision": 0.7682672233820459,
            "recall": 0.7913978494623656
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8653803710467395,
            "auditor_fn_violation": 0.03467737595522549,
            "auditor_fp_violation": 0.027287856165235786,
            "ave_precision_score": 0.8655741070389749,
            "fpr": 0.09978070175438597,
            "logloss": 0.5585892472944795,
            "mae": 0.2864807037323562,
            "precision": 0.7982261640798226,
            "recall": 0.7361963190184049
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8367509937444577,
            "auditor_fn_violation": 0.028766686732056228,
            "auditor_fp_violation": 0.01862143310706709,
            "ave_precision_score": 0.8370983105607147,
            "fpr": 0.11525795828759605,
            "logloss": 0.5462977710242094,
            "mae": 0.2909172200195995,
            "precision": 0.7697368421052632,
            "recall": 0.7548387096774194
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8433986624915524,
            "auditor_fn_violation": 0.01727702436049224,
            "auditor_fp_violation": 0.01740128986769525,
            "ave_precision_score": 0.8438666075598369,
            "fpr": 0.15350877192982457,
            "logloss": 0.7130845219111152,
            "mae": 0.28337891529316583,
            "precision": 0.7482014388489209,
            "recall": 0.8507157464212679
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.7968512831361205,
            "auditor_fn_violation": 0.01866081229418222,
            "auditor_fp_violation": 0.030361353265765226,
            "ave_precision_score": 0.7974339462167508,
            "fpr": 0.18551042810098792,
            "logloss": 0.8596270802067476,
            "mae": 0.29782414008932534,
            "precision": 0.7045454545454546,
            "recall": 0.8666666666666667
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8619437165492543,
            "auditor_fn_violation": 0.015741039715854065,
            "auditor_fp_violation": 0.014114408361328861,
            "ave_precision_score": 0.8622890615932661,
            "fpr": 0.09539473684210527,
            "logloss": 0.8199107931951449,
            "mae": 0.2523473191283196,
            "precision": 0.7986111111111112,
            "recall": 0.7055214723926381
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8238960139988192,
            "auditor_fn_violation": 0.016614142558691262,
            "auditor_fp_violation": 0.01219278081052212,
            "ave_precision_score": 0.824264262816465,
            "fpr": 0.11306256860592755,
            "logloss": 0.8867613511363482,
            "mae": 0.25911073818156743,
            "precision": 0.774617067833698,
            "recall": 0.7612903225806451
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8278568145892649,
            "auditor_fn_violation": 0.008280863200947156,
            "auditor_fp_violation": 0.017606071917382112,
            "ave_precision_score": 0.8280461934619294,
            "fpr": 0.12719298245614036,
            "logloss": 0.7309772033238909,
            "mae": 0.27422177811319526,
            "precision": 0.7665995975855131,
            "recall": 0.7791411042944786
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.792456270633182,
            "auditor_fn_violation": 0.009928826882900746,
            "auditor_fp_violation": 0.02150103616486097,
            "ave_precision_score": 0.7927759154282603,
            "fpr": 0.14270032930845225,
            "logloss": 0.8295940649972535,
            "mae": 0.2821375877867605,
            "precision": 0.7445972495088409,
            "recall": 0.8150537634408602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8427634375844095,
            "auditor_fn_violation": 0.015350877192982467,
            "auditor_fp_violation": 0.015122765542698355,
            "ave_precision_score": 0.8430283688016736,
            "fpr": 0.08662280701754387,
            "logloss": 0.9996628344942601,
            "mae": 0.28523180181285873,
            "precision": 0.8025,
            "recall": 0.656441717791411
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8005134338143202,
            "auditor_fn_violation": 0.01985293249766888,
            "auditor_fp_violation": 0.0218234532593661,
            "ave_precision_score": 0.800913194041279,
            "fpr": 0.11306256860592755,
            "logloss": 1.04023037692924,
            "mae": 0.2888909904752658,
            "precision": 0.7593457943925234,
            "recall": 0.6989247311827957
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7719298245614035,
            "auc_prc": 0.8761636507200301,
            "auditor_fn_violation": 0.013151167796792599,
            "auditor_fp_violation": 0.014181804985276433,
            "ave_precision_score": 0.8764011232790783,
            "fpr": 0.10087719298245613,
            "logloss": 0.7529744748861312,
            "mae": 0.23978131242237846,
            "precision": 0.8021505376344086,
            "recall": 0.7627811860940695
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8476237913992747,
            "auditor_fn_violation": 0.013151092383414185,
            "auditor_fp_violation": 0.021624096124595753,
            "ave_precision_score": 0.8479461297765933,
            "fpr": 0.12403951701427003,
            "logloss": 0.8172269359552722,
            "mae": 0.2412172204644629,
            "precision": 0.7674897119341564,
            "recall": 0.8021505376344086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.881957461082373,
            "auditor_fn_violation": 0.016438399167653293,
            "auditor_fp_violation": 0.014464352370287424,
            "ave_precision_score": 0.8821566335373356,
            "fpr": 0.10197368421052631,
            "logloss": 0.719529192936624,
            "mae": 0.23361733066025997,
            "precision": 0.8033826638477801,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.8494291628065225,
            "auditor_fn_violation": 0.012959881024043059,
            "auditor_fp_violation": 0.0174351350952238,
            "ave_precision_score": 0.8497799981417722,
            "fpr": 0.11855104281009879,
            "logloss": 0.795970161714829,
            "mae": 0.23594744547431723,
            "precision": 0.7786885245901639,
            "recall": 0.8172043010752689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.8820311893967704,
            "auditor_fn_violation": 0.016438399167653293,
            "auditor_fp_violation": 0.014464352370287424,
            "ave_precision_score": 0.8822298254190306,
            "fpr": 0.10197368421052631,
            "logloss": 0.7171722554713804,
            "mae": 0.2337029690537797,
            "precision": 0.8033826638477801,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.8494396847078018,
            "auditor_fn_violation": 0.012959881024043059,
            "auditor_fp_violation": 0.0174351350952238,
            "ave_precision_score": 0.8497946278882881,
            "fpr": 0.11855104281009879,
            "logloss": 0.7933081685598549,
            "mae": 0.23611417430144532,
            "precision": 0.7786885245901639,
            "recall": 0.8172043010752689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7796052631578947,
            "auc_prc": 0.8838240766191487,
            "auditor_fn_violation": 0.014377713199153307,
            "auditor_fp_violation": 0.01685174816473809,
            "ave_precision_score": 0.8839585423527785,
            "fpr": 0.10087719298245613,
            "logloss": 0.6997210941293294,
            "mae": 0.23342467906331735,
            "precision": 0.8050847457627118,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7914379802414928,
            "auc_prc": 0.8519671666004595,
            "auditor_fn_violation": 0.0106346564687275,
            "auditor_fp_violation": 0.017622186234020665,
            "ave_precision_score": 0.8522891055031627,
            "fpr": 0.1207464324917673,
            "logloss": 0.7857473804909955,
            "mae": 0.23543281926967685,
            "precision": 0.7777777777777778,
            "recall": 0.8279569892473119
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8709272948206965,
            "auditor_fn_violation": 0.02543007570049869,
            "auditor_fp_violation": 0.017414250756915932,
            "ave_precision_score": 0.8710945660385252,
            "fpr": 0.10964912280701754,
            "logloss": 0.5667560409068713,
            "mae": 0.2770418594276475,
            "precision": 0.7863247863247863,
            "recall": 0.7525562372188139
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8486695720887957,
            "auditor_fn_violation": 0.017895966856697712,
            "auditor_fp_violation": 0.012355219957372028,
            "ave_precision_score": 0.8489930049284649,
            "fpr": 0.11306256860592755,
            "logloss": 0.5560816815490117,
            "mae": 0.2673177443001324,
            "precision": 0.7760869565217391,
            "recall": 0.7677419354838709
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8229822524055419,
            "auditor_fn_violation": 0.010597172891328525,
            "auditor_fp_violation": 0.022326427771556554,
            "ave_precision_score": 0.8203057828505919,
            "fpr": 0.125,
            "logloss": 1.106932341316396,
            "mae": 0.2732381761589889,
            "precision": 0.758985200845666,
            "recall": 0.7341513292433538
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8007122048962139,
            "auditor_fn_violation": 0.01120356927870826,
            "auditor_fp_violation": 0.0318897579656712,
            "ave_precision_score": 0.7985554029907829,
            "fpr": 0.14818880351262348,
            "logloss": 1.0570189742830491,
            "mae": 0.2729753162915257,
            "precision": 0.7289156626506024,
            "recall": 0.7806451612903226
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7719298245614035,
            "auc_prc": 0.8754772560672069,
            "auditor_fn_violation": 0.018469935062605393,
            "auditor_fp_violation": 0.014596553440338436,
            "ave_precision_score": 0.8756740908577603,
            "fpr": 0.10197368421052631,
            "logloss": 0.7781151897575671,
            "mae": 0.2403715928711304,
            "precision": 0.8008565310492506,
            "recall": 0.7648261758691206
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8421229850487526,
            "auditor_fn_violation": 0.014772847986969304,
            "auditor_fp_violation": 0.019325336076750035,
            "ave_precision_score": 0.8424682998287223,
            "fpr": 0.12403951701427003,
            "logloss": 0.8627555893115042,
            "mae": 0.24490785129199824,
            "precision": 0.7655601659751037,
            "recall": 0.7935483870967742
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7730263157894737,
            "auc_prc": 0.8815772486673292,
            "auditor_fn_violation": 0.014483101926595628,
            "auditor_fp_violation": 0.011659615942930617,
            "ave_precision_score": 0.881744406625812,
            "fpr": 0.10635964912280702,
            "logloss": 0.7052595884631436,
            "mae": 0.2379876845395985,
            "precision": 0.7962184873949579,
            "recall": 0.7750511247443763
        },
        "train": {
            "accuracy": 0.7870472008781558,
            "auc_prc": 0.8471235618621509,
            "auditor_fn_violation": 0.01150336980512966,
            "auditor_fp_violation": 0.020327044148991162,
            "ave_precision_score": 0.8475016683026079,
            "fpr": 0.12184412733260154,
            "logloss": 0.7823716273261344,
            "mae": 0.24003618727839374,
            "precision": 0.7748478701825557,
            "recall": 0.821505376344086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8721178657175201,
            "auditor_fn_violation": 0.02479101639579522,
            "auditor_fp_violation": 0.016779167185102237,
            "ave_precision_score": 0.8722821331020467,
            "fpr": 0.10635964912280702,
            "logloss": 0.5644977357493929,
            "mae": 0.2758895051598681,
            "precision": 0.79004329004329,
            "recall": 0.7464212678936605
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8481448728935055,
            "auditor_fn_violation": 0.017258595658793955,
            "auditor_fp_violation": 0.013068967723833771,
            "ave_precision_score": 0.8484873938318818,
            "fpr": 0.1119648737650933,
            "logloss": 0.5560389185606178,
            "mae": 0.26657675942198145,
            "precision": 0.7782608695652173,
            "recall": 0.7698924731182796
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7708333333333334,
            "auc_prc": 0.8754189513503924,
            "auditor_fn_violation": 0.01790038747174685,
            "auditor_fp_violation": 0.015659346356434827,
            "ave_precision_score": 0.8756139005568018,
            "fpr": 0.10087719298245613,
            "logloss": 0.7602035804053822,
            "mae": 0.2417570852615667,
            "precision": 0.8017241379310345,
            "recall": 0.7607361963190185
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8392003992954614,
            "auditor_fn_violation": 0.01757728125774584,
            "auditor_fp_violation": 0.02461199194695624,
            "ave_precision_score": 0.8395870423641847,
            "fpr": 0.1251372118551043,
            "logloss": 0.857813643078538,
            "mae": 0.2484940856719783,
            "precision": 0.7639751552795031,
            "recall": 0.7935483870967742
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7741228070175439,
            "auc_prc": 0.8755841645242423,
            "auditor_fn_violation": 0.01611774835862663,
            "auditor_fp_violation": 0.012108062709966406,
            "ave_precision_score": 0.8757495210631869,
            "fpr": 0.1074561403508772,
            "logloss": 0.7300332914977726,
            "mae": 0.23754280550884685,
            "precision": 0.7954070981210856,
            "recall": 0.7791411042944786
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8473551794336194,
            "auditor_fn_violation": 0.01569585590689659,
            "auditor_fp_violation": 0.01929580168641369,
            "ave_precision_score": 0.8477049208408388,
            "fpr": 0.12733260153677278,
            "logloss": 0.807132931577652,
            "mae": 0.24105965248488542,
            "precision": 0.7656565656565657,
            "recall": 0.8150537634408602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8776141226141785,
            "auditor_fn_violation": 0.014521221253542854,
            "auditor_fp_violation": 0.015159056032516278,
            "ave_precision_score": 0.8777695735260092,
            "fpr": 0.12390350877192982,
            "logloss": 0.7169034071402854,
            "mae": 0.25183192148679556,
            "precision": 0.7735470941883767,
            "recall": 0.7893660531697342
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8413734209154503,
            "auditor_fn_violation": 0.013037781948231297,
            "auditor_fp_violation": 0.031931598351981026,
            "ave_precision_score": 0.8419564484196947,
            "fpr": 0.14709110867178923,
            "logloss": 0.8096149118251729,
            "mae": 0.24969055086553602,
            "precision": 0.7447619047619047,
            "recall": 0.8408602150537634
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8679551333413369,
            "auditor_fn_violation": 0.011991891794926997,
            "auditor_fp_violation": 0.004619260918253087,
            "ave_precision_score": 0.8680827536423112,
            "fpr": 0.11293859649122807,
            "logloss": 0.7854306938247709,
            "mae": 0.2447892942030735,
            "precision": 0.7863070539419087,
            "recall": 0.7750511247443763
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8400024506220793,
            "auditor_fn_violation": 0.00938352041358309,
            "auditor_fp_violation": 0.014688436793943492,
            "ave_precision_score": 0.840300110774971,
            "fpr": 0.12733260153677278,
            "logloss": 0.8602792542462538,
            "mae": 0.24835823575426927,
            "precision": 0.7637474541751528,
            "recall": 0.8064516129032258
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.8803645139233001,
            "auditor_fn_violation": 0.016844257883973743,
            "auditor_fp_violation": 0.011589627141138906,
            "ave_precision_score": 0.8804985141705893,
            "fpr": 0.10416666666666667,
            "logloss": 0.7156595748474648,
            "mae": 0.23680858816830483,
            "precision": 0.8004201680672269,
            "recall": 0.7791411042944786
        },
        "train": {
            "accuracy": 0.7892425905598244,
            "auc_prc": 0.8481870990040448,
            "auditor_fn_violation": 0.014262951028646297,
            "auditor_fp_violation": 0.019507464817157507,
            "ave_precision_score": 0.848544604175012,
            "fpr": 0.12184412733260154,
            "logloss": 0.7897572845805043,
            "mae": 0.237912857468461,
            "precision": 0.7757575757575758,
            "recall": 0.8258064516129032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.8821370823161231,
            "auditor_fn_violation": 0.016438399167653293,
            "auditor_fp_violation": 0.014464352370287424,
            "ave_precision_score": 0.8823355676422642,
            "fpr": 0.10197368421052631,
            "logloss": 0.7161488614718932,
            "mae": 0.2336734541243355,
            "precision": 0.8033826638477801,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.8496367660794477,
            "auditor_fn_violation": 0.012959881024043059,
            "auditor_fp_violation": 0.0174351350952238,
            "ave_precision_score": 0.8499119995390481,
            "fpr": 0.11855104281009879,
            "logloss": 0.7922701665255893,
            "mae": 0.23597984881753192,
            "precision": 0.7786885245901639,
            "recall": 0.8172043010752689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.8796529064686834,
            "auditor_fn_violation": 0.017615613676317584,
            "auditor_fp_violation": 0.014516195927170177,
            "ave_precision_score": 0.8798664972792146,
            "fpr": 0.10635964912280702,
            "logloss": 0.7213991748674985,
            "mae": 0.23671617820416055,
            "precision": 0.7979166666666667,
            "recall": 0.7832310838445807
        },
        "train": {
            "accuracy": 0.7859495060373216,
            "auc_prc": 0.8480749266605667,
            "auditor_fn_violation": 0.01150336980512966,
            "auditor_fp_violation": 0.017267773549984492,
            "ave_precision_score": 0.8484149013509587,
            "fpr": 0.12294182217343579,
            "logloss": 0.7980850521588345,
            "mae": 0.23838555484412102,
            "precision": 0.7732793522267206,
            "recall": 0.821505376344086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7763157894736842,
            "auc_prc": 0.875105949018114,
            "auditor_fn_violation": 0.017216481900046644,
            "auditor_fp_violation": 0.014329559122392276,
            "ave_precision_score": 0.8753206781073547,
            "fpr": 0.09429824561403509,
            "logloss": 0.7826775000544168,
            "mae": 0.23951094778683008,
            "precision": 0.811816192560175,
            "recall": 0.7586912065439673
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8438193320650083,
            "auditor_fn_violation": 0.01101943982153607,
            "auditor_fp_violation": 0.018778949855527617,
            "ave_precision_score": 0.8441739520726734,
            "fpr": 0.1163556531284303,
            "logloss": 0.874131471913975,
            "mae": 0.24175235682479343,
            "precision": 0.7754237288135594,
            "recall": 0.7870967741935484
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.8820456180835075,
            "auditor_fn_violation": 0.016438399167653293,
            "auditor_fp_violation": 0.014464352370287424,
            "ave_precision_score": 0.882244193590879,
            "fpr": 0.10197368421052631,
            "logloss": 0.7175968202077554,
            "mae": 0.23365783619776356,
            "precision": 0.8033826638477801,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.8493838891079907,
            "auditor_fn_violation": 0.012959881024043059,
            "auditor_fp_violation": 0.0174351350952238,
            "ave_precision_score": 0.8497213753999309,
            "fpr": 0.11855104281009879,
            "logloss": 0.7937863113387055,
            "mae": 0.23609961767919932,
            "precision": 0.7786885245901639,
            "recall": 0.8172043010752689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7763157894736842,
            "auc_prc": 0.8671519218560757,
            "auditor_fn_violation": 0.007516234348652823,
            "auditor_fp_violation": 0.009661046825100581,
            "ave_precision_score": 0.8673426795644428,
            "fpr": 0.09978070175438597,
            "logloss": 1.025771385668508,
            "mae": 0.24497395260563903,
            "precision": 0.8051391862955032,
            "recall": 0.7689161554192229
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.840676162381725,
            "auditor_fn_violation": 0.006276925982318853,
            "auditor_fp_violation": 0.019876644696361863,
            "ave_precision_score": 0.8409757171326827,
            "fpr": 0.1163556531284303,
            "logloss": 1.029378584959951,
            "mae": 0.24314058845010228,
            "precision": 0.7735042735042735,
            "recall": 0.7784946236559139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.881926198165411,
            "auditor_fn_violation": 0.016438399167653293,
            "auditor_fp_violation": 0.014464352370287424,
            "ave_precision_score": 0.8821193354090925,
            "fpr": 0.10197368421052631,
            "logloss": 0.7169067838011912,
            "mae": 0.23436632317330494,
            "precision": 0.8033826638477801,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7892425905598244,
            "auc_prc": 0.848854043939155,
            "auditor_fn_violation": 0.01150336980512966,
            "auditor_fp_violation": 0.017942142129331096,
            "ave_precision_score": 0.8491956394391873,
            "fpr": 0.11964873765093303,
            "logloss": 0.7933213326733006,
            "mae": 0.2364453414463311,
            "precision": 0.7780040733197556,
            "recall": 0.821505376344086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 24284,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8705069084319058,
            "auditor_fn_violation": 0.02280208445449001,
            "auditor_fp_violation": 0.021989444651818676,
            "ave_precision_score": 0.870668733560148,
            "fpr": 0.13267543859649122,
            "logloss": 0.6196658522091069,
            "mae": 0.27431069362582494,
            "precision": 0.7565392354124748,
            "recall": 0.7689161554192229
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8473861825585822,
            "auditor_fn_violation": 0.020471418623042158,
            "auditor_fp_violation": 0.01838269678518161,
            "ave_precision_score": 0.8476951811560473,
            "fpr": 0.1350164654226125,
            "logloss": 0.6262657301308452,
            "mae": 0.2640396447561934,
            "precision": 0.7458677685950413,
            "recall": 0.7763440860215054
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8537090024834837,
            "auditor_fn_violation": 0.019461037563233235,
            "auditor_fp_violation": 0.02064928870639958,
            "ave_precision_score": 0.8540423816413792,
            "fpr": 0.11732456140350878,
            "logloss": 0.7293394237698385,
            "mae": 0.2640735799391149,
            "precision": 0.773784355179704,
            "recall": 0.7484662576687117
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8167852471788322,
            "auditor_fn_violation": 0.015993295799251683,
            "auditor_fp_violation": 0.03186268477452955,
            "ave_precision_score": 0.8171720358922875,
            "fpr": 0.145993413830955,
            "logloss": 0.8106853965840709,
            "mae": 0.2690241355921826,
            "precision": 0.73558648111332,
            "recall": 0.7956989247311828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.863587527425647,
            "auditor_fn_violation": 0.019373587342589607,
            "auditor_fp_violation": 0.014111816183484716,
            "ave_precision_score": 0.8637762796389964,
            "fpr": 0.1206140350877193,
            "logloss": 0.4925543573847359,
            "mae": 0.31256983971877816,
            "precision": 0.7759674134419552,
            "recall": 0.7791411042944786
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8391547266383081,
            "auditor_fn_violation": 0.008462873127722108,
            "auditor_fp_violation": 0.011087702372103783,
            "ave_precision_score": 0.8394175792643847,
            "fpr": 0.13062568605927552,
            "logloss": 0.510595050988822,
            "mae": 0.31408035362284514,
            "precision": 0.7566462167689162,
            "recall": 0.7956989247311828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8269174002909743,
            "auditor_fn_violation": 0.012830516987765944,
            "auditor_fp_violation": 0.017110965949151843,
            "ave_precision_score": 0.8274923647053906,
            "fpr": 0.12719298245614036,
            "logloss": 0.8340439501922854,
            "mae": 0.2809843845616245,
            "precision": 0.7568134171907757,
            "recall": 0.7382413087934561
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.7913929565106637,
            "auditor_fn_violation": 0.014404589072624917,
            "auditor_fp_violation": 0.02236245588300444,
            "ave_precision_score": 0.7918004832901916,
            "fpr": 0.13721185510428102,
            "logloss": 0.895436712070274,
            "mae": 0.2850445414196775,
            "precision": 0.7357293868921776,
            "recall": 0.7483870967741936
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8771407845341892,
            "auditor_fn_violation": 0.018855612958777315,
            "auditor_fp_violation": 0.01850037327360956,
            "ave_precision_score": 0.8773497941135396,
            "fpr": 0.10197368421052631,
            "logloss": 0.6949308412837997,
            "mae": 0.24433176703819595,
            "precision": 0.8,
            "recall": 0.7607361963190185
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8381191594085342,
            "auditor_fn_violation": 0.017324693412650638,
            "auditor_fp_violation": 0.024498776784000247,
            "ave_precision_score": 0.8385098382650369,
            "fpr": 0.12843029637760703,
            "logloss": 0.7817402821053792,
            "mae": 0.2509921865020164,
            "precision": 0.7626774847870182,
            "recall": 0.8086021505376344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7807017543859649,
            "auc_prc": 0.8819860502959194,
            "auditor_fn_violation": 0.017826391131202244,
            "auditor_fp_violation": 0.012989403176973169,
            "ave_precision_score": 0.8821836903204415,
            "fpr": 0.10087719298245613,
            "logloss": 0.723332872033149,
            "mae": 0.23341366570138009,
            "precision": 0.8054968287526427,
            "recall": 0.7791411042944786
        },
        "train": {
            "accuracy": 0.7870472008781558,
            "auc_prc": 0.8496843637775259,
            "auditor_fn_violation": 0.012289460949210958,
            "auditor_fp_violation": 0.0174351350952238,
            "ave_precision_score": 0.8499984580250329,
            "fpr": 0.11855104281009879,
            "logloss": 0.8000664316900199,
            "mae": 0.23555107169685865,
            "precision": 0.7782340862422998,
            "recall": 0.8150537634408602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7763157894736842,
            "auc_prc": 0.8808968746895227,
            "auditor_fn_violation": 0.013906827395687583,
            "auditor_fp_violation": 0.012305068226120857,
            "ave_precision_score": 0.8810930564563542,
            "fpr": 0.10307017543859649,
            "logloss": 0.6953144973729316,
            "mae": 0.2374578002231075,
            "precision": 0.8012684989429175,
            "recall": 0.7750511247443763
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.84682487599215,
            "auditor_fn_violation": 0.014687865160582137,
            "auditor_fp_violation": 0.020560858072487236,
            "ave_precision_score": 0.8471773228400837,
            "fpr": 0.12294182217343579,
            "logloss": 0.77478446813647,
            "mae": 0.240772193177081,
            "precision": 0.7718940936863544,
            "recall": 0.8150537634408602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8756931062903107,
            "auditor_fn_violation": 0.01868071251749005,
            "auditor_fp_violation": 0.018153021442495133,
            "ave_precision_score": 0.8758867233131344,
            "fpr": 0.10307017543859649,
            "logloss": 0.7284621750874919,
            "mae": 0.24516484040801786,
            "precision": 0.7974137931034483,
            "recall": 0.7566462167689162
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8357757336065894,
            "auditor_fn_violation": 0.01636863661579501,
            "auditor_fp_violation": 0.022938376494563217,
            "ave_precision_score": 0.8361760668634936,
            "fpr": 0.12733260153677278,
            "logloss": 0.8235092664242742,
            "mae": 0.2513239266770462,
            "precision": 0.7603305785123967,
            "recall": 0.7913978494623656
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8430033204582325,
            "auditor_fn_violation": 0.019911742546550426,
            "auditor_fp_violation": 0.007460287835427814,
            "ave_precision_score": 0.84327378906831,
            "fpr": 0.06469298245614036,
            "logloss": 0.6615703724993107,
            "mae": 0.3069264798144322,
            "precision": 0.8396739130434783,
            "recall": 0.6319018404907976
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8242883554866834,
            "auditor_fn_violation": 0.018530977420535173,
            "auditor_fp_violation": 0.013359389228807849,
            "ave_precision_score": 0.8245516782226991,
            "fpr": 0.0801317233809001,
            "logloss": 0.6451986935243671,
            "mae": 0.2958803694103123,
            "precision": 0.8068783068783069,
            "recall": 0.6559139784946236
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7861842105263158,
            "auc_prc": 0.8836783766636405,
            "auditor_fn_violation": 0.016438399167653293,
            "auditor_fp_violation": 0.01191364937165609,
            "ave_precision_score": 0.8838518904466381,
            "fpr": 0.09429824561403509,
            "logloss": 0.7836048250694368,
            "mae": 0.23143020975141956,
            "precision": 0.8154506437768241,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7859495060373216,
            "auc_prc": 0.8498283518540161,
            "auditor_fn_violation": 0.010098792535675088,
            "auditor_fp_violation": 0.020226134982008637,
            "ave_precision_score": 0.8500684530209097,
            "fpr": 0.1119648737650933,
            "logloss": 0.8827040790647968,
            "mae": 0.23376857593832873,
            "precision": 0.7848101265822784,
            "recall": 0.8
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8730835211113035,
            "auditor_fn_violation": 0.01445619416639759,
            "auditor_fp_violation": 0.015965223342043054,
            "ave_precision_score": 0.8733205169876507,
            "fpr": 0.11074561403508772,
            "logloss": 0.7832239844006126,
            "mae": 0.24167221868946656,
            "precision": 0.7891440501043842,
            "recall": 0.7730061349693251
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8390994062311659,
            "auditor_fn_violation": 0.01774488627645386,
            "auditor_fp_violation": 0.01656879297869094,
            "ave_precision_score": 0.8393415384452472,
            "fpr": 0.13062568605927552,
            "logloss": 0.8911868579444263,
            "mae": 0.2480107734177992,
            "precision": 0.7586206896551724,
            "recall": 0.8043010752688172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.851136527749891,
            "auditor_fn_violation": 0.021896189861155962,
            "auditor_fp_violation": 0.020926651735722293,
            "ave_precision_score": 0.8514801748949274,
            "fpr": 0.11732456140350878,
            "logloss": 0.7683486170788425,
            "mae": 0.2635575893598339,
            "precision": 0.773784355179704,
            "recall": 0.7484662576687117
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8107275323113496,
            "auditor_fn_violation": 0.018283110843572585,
            "auditor_fp_violation": 0.03149350489532521,
            "ave_precision_score": 0.8111241593330538,
            "fpr": 0.14709110867178923,
            "logloss": 0.8714639570381044,
            "mae": 0.2718285795214049,
            "precision": 0.7341269841269841,
            "recall": 0.7956989247311828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7708333333333334,
            "auc_prc": 0.8753895127134534,
            "auditor_fn_violation": 0.010428999390090773,
            "auditor_fp_violation": 0.013717805151175811,
            "ave_precision_score": 0.8755937860087166,
            "fpr": 0.10855263157894737,
            "logloss": 0.7135788239858201,
            "mae": 0.24190708457104143,
            "precision": 0.7928870292887029,
            "recall": 0.7750511247443763
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8461227852155271,
            "auditor_fn_violation": 0.004005996010528432,
            "auditor_fp_violation": 0.01902014737660778,
            "ave_precision_score": 0.8464754280042975,
            "fpr": 0.1207464324917673,
            "logloss": 0.7928723908883571,
            "mae": 0.24350958897253175,
            "precision": 0.772256728778468,
            "recall": 0.8021505376344086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8046258996357466,
            "auditor_fn_violation": 0.023281939511355074,
            "auditor_fp_violation": 0.014511011571481902,
            "ave_precision_score": 0.8010278728240919,
            "fpr": 0.11842105263157894,
            "logloss": 1.3448675576360851,
            "mae": 0.2726814475177876,
            "precision": 0.7647058823529411,
            "recall": 0.7177914110429447
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7431487088335587,
            "auditor_fn_violation": 0.01614673701356184,
            "auditor_fp_violation": 0.006588630244200195,
            "ave_precision_score": 0.7392594204391411,
            "fpr": 0.16136114160263446,
            "logloss": 1.561206922174113,
            "mae": 0.2941754491798731,
            "precision": 0.7100591715976331,
            "recall": 0.7741935483870968
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8527066429683783,
            "auditor_fn_violation": 0.00966661285114628,
            "auditor_fp_violation": 0.014117000539172989,
            "ave_precision_score": 0.8529253441569296,
            "fpr": 0.08771929824561403,
            "logloss": 0.5326675533860479,
            "mae": 0.30391817006717536,
            "precision": 0.812206572769953,
            "recall": 0.7075664621676891
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8496111093806179,
            "auditor_fn_violation": 0.0025660092300792015,
            "auditor_fp_violation": 0.005786279306729411,
            "ave_precision_score": 0.8498814277069964,
            "fpr": 0.08232711306256861,
            "logloss": 0.5050263309593739,
            "mae": 0.28921475200878066,
            "precision": 0.8184019370460048,
            "recall": 0.7268817204301076
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8210771255056526,
            "auditor_fn_violation": 0.009928963513077173,
            "auditor_fp_violation": 0.013321201941022778,
            "ave_precision_score": 0.7878122225614692,
            "fpr": 0.11842105263157894,
            "logloss": 3.6570289587937577,
            "mae": 0.2600720428963141,
            "precision": 0.7711864406779662,
            "recall": 0.7443762781186094
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.7793789516461188,
            "auditor_fn_violation": 0.012952799121844135,
            "auditor_fp_violation": 0.022027732792525834,
            "ave_precision_score": 0.7355563166264222,
            "fpr": 0.13391877058177826,
            "logloss": 4.2686194863514935,
            "mae": 0.2621156935294333,
            "precision": 0.7479338842975206,
            "recall": 0.7784946236559139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 24284,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.8741477749621296,
            "auditor_fn_violation": 0.013855254188641345,
            "auditor_fp_violation": 0.010451661067562524,
            "ave_precision_score": 0.8743383151022991,
            "fpr": 0.1074561403508772,
            "logloss": 0.7599188778549579,
            "mae": 0.24066204662052582,
            "precision": 0.7932489451476793,
            "recall": 0.7689161554192229
        },
        "train": {
            "accuracy": 0.7837541163556532,
            "auc_prc": 0.8472362595835199,
            "auditor_fn_violation": 0.012818242980064449,
            "auditor_fp_violation": 0.016721387328762063,
            "ave_precision_score": 0.8474446723247162,
            "fpr": 0.12403951701427003,
            "logloss": 0.8091121703514176,
            "mae": 0.23876652559502154,
            "precision": 0.771255060728745,
            "recall": 0.8193548387096774
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8693535082129411,
            "auditor_fn_violation": 0.015323969432784421,
            "auditor_fp_violation": 0.0074265895234540275,
            "ave_precision_score": 0.8695557741499494,
            "fpr": 0.08442982456140351,
            "logloss": 0.9502636365732074,
            "mae": 0.2501208621025961,
            "precision": 0.8200934579439252,
            "recall": 0.7177914110429447
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8381913569622782,
            "auditor_fn_violation": 0.016406406760855963,
            "auditor_fp_violation": 0.023159884422085823,
            "ave_precision_score": 0.838550140833232,
            "fpr": 0.10318331503841932,
            "logloss": 0.9884820186531683,
            "mae": 0.2473786732053261,
            "precision": 0.7882882882882883,
            "recall": 0.7526881720430108
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8434312219755048,
            "auditor_fn_violation": 0.016081871345029242,
            "auditor_fp_violation": 0.016820642030608443,
            "ave_precision_score": 0.8436944049499178,
            "fpr": 0.08881578947368421,
            "logloss": 0.9947762735673674,
            "mae": 0.28421302543918936,
            "precision": 0.7995049504950495,
            "recall": 0.6605316973415133
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8022428921527345,
            "auditor_fn_violation": 0.018597075174391842,
            "auditor_fp_violation": 0.022076956776419743,
            "ave_precision_score": 0.8026367938542547,
            "fpr": 0.11306256860592755,
            "logloss": 1.034874327975713,
            "mae": 0.28770097327459515,
            "precision": 0.7604651162790698,
            "recall": 0.7032258064516129
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8528454305487639,
            "auditor_fn_violation": 0.01618501775912173,
            "auditor_fp_violation": 0.020273422918999626,
            "ave_precision_score": 0.8532023896773289,
            "fpr": 0.1206140350877193,
            "logloss": 0.7253877656094941,
            "mae": 0.26413567914198977,
            "precision": 0.7689075630252101,
            "recall": 0.7484662576687117
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.815918503807721,
            "auditor_fn_violation": 0.01750646223575653,
            "auditor_fp_violation": 0.03251982495951327,
            "ave_precision_score": 0.8162846051720485,
            "fpr": 0.1437980241492865,
            "logloss": 0.8008855898868285,
            "mae": 0.26968101753082946,
            "precision": 0.7374749498997996,
            "recall": 0.7913978494623656
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.8743637889787549,
            "auditor_fn_violation": 0.023781975388368678,
            "auditor_fp_violation": 0.01633331259591058,
            "ave_precision_score": 0.8745210679633612,
            "fpr": 0.09649122807017543,
            "logloss": 0.5388320377722707,
            "mae": 0.2794659358426767,
            "precision": 0.8040089086859689,
            "recall": 0.7382413087934561
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8528206285106859,
            "auditor_fn_violation": 0.015230810995833484,
            "auditor_fp_violation": 0.018838018636200302,
            "ave_precision_score": 0.8531510334562354,
            "fpr": 0.10537870472008781,
            "logloss": 0.5218708262834025,
            "mae": 0.271981773642865,
            "precision": 0.7857142857142857,
            "recall": 0.7569892473118279
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8647486702649767,
            "auditor_fn_violation": 0.030399042083736956,
            "auditor_fp_violation": 0.01749720044792833,
            "ave_precision_score": 0.864921882226911,
            "fpr": 0.09868421052631579,
            "logloss": 0.5887615305143256,
            "mae": 0.28313171725273234,
            "precision": 0.7972972972972973,
            "recall": 0.7239263803680982
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8357772944282627,
            "auditor_fn_violation": 0.02491649256990428,
            "auditor_fp_violation": 0.021309062627674714,
            "ave_precision_score": 0.8362635830488143,
            "fpr": 0.1163556531284303,
            "logloss": 0.5803297085114426,
            "mae": 0.28294900104422993,
            "precision": 0.7675438596491229,
            "recall": 0.7526881720430108
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7807017543859649,
            "auc_prc": 0.8809326845533377,
            "auditor_fn_violation": 0.017223208840096153,
            "auditor_fp_violation": 0.013608933681722038,
            "ave_precision_score": 0.8811296848356243,
            "fpr": 0.10197368421052631,
            "logloss": 0.7483632586410287,
            "mae": 0.23443237918230717,
            "precision": 0.8042105263157895,
            "recall": 0.7811860940695297
        },
        "train": {
            "accuracy": 0.7848518111964874,
            "auc_prc": 0.8473441669228736,
            "auditor_fn_violation": 0.013387155790045211,
            "auditor_fp_violation": 0.020629771649938723,
            "ave_precision_score": 0.8476825007883626,
            "fpr": 0.1207464324917673,
            "logloss": 0.8303591966073944,
            "mae": 0.2369039737703064,
            "precision": 0.7750511247443763,
            "recall": 0.8150537634408602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8235801421143594,
            "auditor_fn_violation": 0.041707028306963725,
            "auditor_fp_violation": 0.029607855335738877,
            "ave_precision_score": 0.8238899683692544,
            "fpr": 0.12938596491228072,
            "logloss": 0.5779134314092161,
            "mae": 0.320526628053079,
            "precision": 0.757700205338809,
            "recall": 0.754601226993865
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8124353221790498,
            "auditor_fn_violation": 0.033674444955915164,
            "auditor_fp_violation": 0.02740299183374107,
            "ave_precision_score": 0.8127643231306447,
            "fpr": 0.14270032930845225,
            "logloss": 0.5581448195307689,
            "mae": 0.3150575188888309,
            "precision": 0.7373737373737373,
            "recall": 0.7849462365591398
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7741228070175439,
            "auc_prc": 0.8802533691974779,
            "auditor_fn_violation": 0.015711889642306177,
            "auditor_fp_violation": 0.013349715897308281,
            "ave_precision_score": 0.8804570526801984,
            "fpr": 0.10635964912280702,
            "logloss": 0.7112854586375031,
            "mae": 0.23701674896348987,
            "precision": 0.7966457023060797,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8459248238512671,
            "auditor_fn_violation": 0.01385692196924094,
            "auditor_fp_violation": 0.018872475424926042,
            "ave_precision_score": 0.8461811200651461,
            "fpr": 0.12403951701427003,
            "logloss": 0.7910550825814342,
            "mae": 0.24090002743480954,
            "precision": 0.77079107505071,
            "recall": 0.8172043010752689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.8782087247667536,
            "auditor_fn_violation": 0.01714248555950203,
            "auditor_fp_violation": 0.012644643523702877,
            "ave_precision_score": 0.8784098088661755,
            "fpr": 0.10197368421052631,
            "logloss": 0.7677951487180475,
            "mae": 0.23703118109127957,
            "precision": 0.8029661016949152,
            "recall": 0.7750511247443763
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8439457828702059,
            "auditor_fn_violation": 0.01276394839653931,
            "auditor_fp_violation": 0.020235979778787418,
            "ave_precision_score": 0.8443334517875813,
            "fpr": 0.1163556531284303,
            "logloss": 0.8519700119852257,
            "mae": 0.2405986408074975,
            "precision": 0.778705636743215,
            "recall": 0.8021505376344086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8730317190507897,
            "auditor_fn_violation": 0.020079916047788186,
            "auditor_fp_violation": 0.01629442992824852,
            "ave_precision_score": 0.8732836301179778,
            "fpr": 0.1074561403508772,
            "logloss": 0.8126967292661889,
            "mae": 0.24184289026303274,
            "precision": 0.7923728813559322,
            "recall": 0.7648261758691206
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8380243078503262,
            "auditor_fn_violation": 0.021847668283701,
            "auditor_fp_violation": 0.02359305548035225,
            "ave_precision_score": 0.8383832392064046,
            "fpr": 0.13062568605927552,
            "logloss": 0.9182478736646263,
            "mae": 0.24962120774288737,
            "precision": 0.7566462167689162,
            "recall": 0.7956989247311828
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.8814746013006557,
            "auditor_fn_violation": 0.01707297384565709,
            "auditor_fp_violation": 0.014010721247563352,
            "ave_precision_score": 0.8816760786217742,
            "fpr": 0.10307017543859649,
            "logloss": 0.7191475976492979,
            "mae": 0.23413903194274246,
            "precision": 0.8021052631578948,
            "recall": 0.7791411042944786
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.8492559790228436,
            "auditor_fn_violation": 0.012329591728338234,
            "auditor_fp_violation": 0.017942142129331096,
            "ave_precision_score": 0.8495976043586174,
            "fpr": 0.11964873765093303,
            "logloss": 0.7951185565758876,
            "mae": 0.23654877171907626,
            "precision": 0.7775510204081633,
            "recall": 0.8193548387096774
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7730263157894737,
            "auc_prc": 0.8810209000399382,
            "auditor_fn_violation": 0.017651490689914975,
            "auditor_fp_violation": 0.013036062378167642,
            "ave_precision_score": 0.8812344346381402,
            "fpr": 0.10307017543859649,
            "logloss": 0.6936350244307214,
            "mae": 0.23998596156232246,
            "precision": 0.8,
            "recall": 0.7689161554192229
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8457791118757602,
            "auditor_fn_violation": 0.018797729070028205,
            "auditor_fp_violation": 0.019093983352448652,
            "ave_precision_score": 0.8461393969491975,
            "fpr": 0.1207464324917673,
            "logloss": 0.7652510449678052,
            "mae": 0.24388255036145481,
            "precision": 0.7741273100616016,
            "recall": 0.810752688172043
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7752192982456141,
            "auc_prc": 0.8779910519758187,
            "auditor_fn_violation": 0.01967181501811789,
            "auditor_fp_violation": 0.012644643523702877,
            "ave_precision_score": 0.8781925624554412,
            "fpr": 0.10197368421052631,
            "logloss": 0.7701896435740627,
            "mae": 0.23710301847277707,
            "precision": 0.8021276595744681,
            "recall": 0.7709611451942741
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8439817365695921,
            "auditor_fn_violation": 0.01276394839653931,
            "auditor_fp_violation": 0.020235979778787418,
            "ave_precision_score": 0.8442882866205638,
            "fpr": 0.1163556531284303,
            "logloss": 0.8550432158205705,
            "mae": 0.24069778265293332,
            "precision": 0.778705636743215,
            "recall": 0.8021505376344086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.8786499469987932,
            "auditor_fn_violation": 0.01276773221397051,
            "auditor_fp_violation": 0.009935817676579159,
            "ave_precision_score": 0.8788773576724273,
            "fpr": 0.10416666666666667,
            "logloss": 0.7231809778475524,
            "mae": 0.23472863176536868,
            "precision": 0.80083857442348,
            "recall": 0.7811860940695297
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8503258929580331,
            "auditor_fn_violation": 0.008842935212398055,
            "auditor_fp_violation": 0.016844447288496846,
            "ave_precision_score": 0.8506548624447272,
            "fpr": 0.11964873765093303,
            "logloss": 0.799067862489581,
            "mae": 0.23730983859102187,
            "precision": 0.7743271221532091,
            "recall": 0.8043010752688172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.8927838614063299,
            "auditor_fn_violation": 0.01876143579808417,
            "auditor_fp_violation": 0.010576085604081124,
            "ave_precision_score": 0.8929319648821632,
            "fpr": 0.09429824561403509,
            "logloss": 0.5253519608152917,
            "mae": 0.24469977397277767,
            "precision": 0.8122270742358079,
            "recall": 0.7607361963190185
        },
        "train": {
            "accuracy": 0.7980241492864983,
            "auc_prc": 0.8593092337757338,
            "auditor_fn_violation": 0.016290735691606772,
            "auditor_fp_violation": 0.01925150010090917,
            "ave_precision_score": 0.8595946245724757,
            "fpr": 0.10208562019758508,
            "logloss": 0.5647889434354659,
            "mae": 0.240103012307029,
            "precision": 0.8008565310492506,
            "recall": 0.8043010752688172
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.8820787947369422,
            "auditor_fn_violation": 0.016438399167653293,
            "auditor_fp_violation": 0.014464352370287424,
            "ave_precision_score": 0.8822828384372092,
            "fpr": 0.10197368421052631,
            "logloss": 0.7174464929653084,
            "mae": 0.23358982422813773,
            "precision": 0.8033826638477801,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7870472008781558,
            "auc_prc": 0.8496587772981951,
            "auditor_fn_violation": 0.012959881024043059,
            "auditor_fp_violation": 0.017942142129331096,
            "ave_precision_score": 0.8500039510055742,
            "fpr": 0.11964873765093303,
            "logloss": 0.7937361210365,
            "mae": 0.2359030837013736,
            "precision": 0.7770961145194274,
            "recall": 0.8172043010752689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7796052631578947,
            "auc_prc": 0.8838309580822705,
            "auditor_fn_violation": 0.014377713199153307,
            "auditor_fp_violation": 0.01685174816473809,
            "ave_precision_score": 0.884017774019763,
            "fpr": 0.10087719298245613,
            "logloss": 0.6995171405002869,
            "mae": 0.23343082012739638,
            "precision": 0.8050847457627118,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7914379802414928,
            "auc_prc": 0.8520121650476127,
            "auditor_fn_violation": 0.0106346564687275,
            "auditor_fp_violation": 0.017622186234020665,
            "ave_precision_score": 0.852337535011181,
            "fpr": 0.1207464324917673,
            "logloss": 0.7854635278743703,
            "mae": 0.2354242866671092,
            "precision": 0.7777777777777778,
            "recall": 0.8279569892473119
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7807017543859649,
            "auc_prc": 0.881016707402676,
            "auditor_fn_violation": 0.017223208840096153,
            "auditor_fp_violation": 0.013608933681722038,
            "ave_precision_score": 0.8812156464783893,
            "fpr": 0.10197368421052631,
            "logloss": 0.748297566705529,
            "mae": 0.2343464290339422,
            "precision": 0.8042105263157895,
            "recall": 0.7811860940695297
        },
        "train": {
            "accuracy": 0.7870472008781558,
            "auc_prc": 0.8473281627483686,
            "auditor_fn_violation": 0.013228993307602422,
            "auditor_fp_violation": 0.020629771649938723,
            "ave_precision_score": 0.847690178869865,
            "fpr": 0.1207464324917673,
            "logloss": 0.8304384311832654,
            "mae": 0.23680674202510668,
            "precision": 0.7759674134419552,
            "recall": 0.8193548387096774
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7763157894736842,
            "auc_prc": 0.8822257894200602,
            "auditor_fn_violation": 0.01459297528073763,
            "auditor_fp_violation": 0.015472709551656921,
            "ave_precision_score": 0.8824361538550136,
            "fpr": 0.10307017543859649,
            "logloss": 0.7447659541164945,
            "mae": 0.2330061941352317,
            "precision": 0.8012684989429175,
            "recall": 0.7750511247443763
        },
        "train": {
            "accuracy": 0.7914379802414928,
            "auc_prc": 0.850309445382028,
            "auditor_fn_violation": 0.011345207322686875,
            "auditor_fp_violation": 0.018815867843448048,
            "ave_precision_score": 0.8506746289121923,
            "fpr": 0.11745334796926454,
            "logloss": 0.8287145050905221,
            "mae": 0.23397171218541477,
            "precision": 0.7811860940695297,
            "recall": 0.821505376344086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8396381955000055,
            "auditor_fn_violation": 0.023620528827180423,
            "auditor_fp_violation": 0.024729376633072047,
            "ave_precision_score": 0.839918671182978,
            "fpr": 0.13815789473684212,
            "logloss": 0.7952732203370586,
            "mae": 0.2729860679647667,
            "precision": 0.7474949899799599,
            "recall": 0.7627811860940695
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8042353311920902,
            "auditor_fn_violation": 0.021897241599093516,
            "auditor_fp_violation": 0.03497117935743011,
            "ave_precision_score": 0.804640045195802,
            "fpr": 0.15697036223929747,
            "logloss": 0.8764221536910991,
            "mae": 0.27818759569908763,
            "precision": 0.7190569744597249,
            "recall": 0.7870967741935484
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.8456134917184935,
            "auditor_fn_violation": 0.019084328920460667,
            "auditor_fp_violation": 0.013958877690680604,
            "ave_precision_score": 0.8458586633786764,
            "fpr": 0.07785087719298246,
            "logloss": 1.0501740362693435,
            "mae": 0.28907423866726073,
            "precision": 0.8121693121693122,
            "recall": 0.6278118609406953
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8013709509157827,
            "auditor_fn_violation": 0.013172338090010982,
            "auditor_fp_violation": 0.02428219125486702,
            "ave_precision_score": 0.8017811957536649,
            "fpr": 0.10208562019758508,
            "logloss": 1.0733688461630873,
            "mae": 0.29404112935699633,
            "precision": 0.7663316582914573,
            "recall": 0.6559139784946236
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.8821260996371509,
            "auditor_fn_violation": 0.016438399167653293,
            "auditor_fp_violation": 0.014464352370287424,
            "ave_precision_score": 0.8823209198331134,
            "fpr": 0.10197368421052631,
            "logloss": 0.7157409471521722,
            "mae": 0.23368262784015362,
            "precision": 0.8033826638477801,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.8497123251745533,
            "auditor_fn_violation": 0.012959881024043059,
            "auditor_fp_violation": 0.0174351350952238,
            "ave_precision_score": 0.850054216382165,
            "fpr": 0.11855104281009879,
            "logloss": 0.791823065064206,
            "mae": 0.23598277253002822,
            "precision": 0.7786885245901639,
            "recall": 0.8172043010752689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7785087719298246,
            "auc_prc": 0.8821451418462671,
            "auditor_fn_violation": 0.016438399167653293,
            "auditor_fp_violation": 0.014464352370287424,
            "ave_precision_score": 0.8823436756377212,
            "fpr": 0.10197368421052631,
            "logloss": 0.7158236667391553,
            "mae": 0.2336749386166851,
            "precision": 0.8033826638477801,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.8500012055607299,
            "auditor_fn_violation": 0.012959881024043059,
            "auditor_fp_violation": 0.0174351350952238,
            "ave_precision_score": 0.8503404976656149,
            "fpr": 0.11855104281009879,
            "logloss": 0.7920836227388575,
            "mae": 0.2359232289406505,
            "precision": 0.7786885245901639,
            "recall": 0.8172043010752689
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7741228070175439,
            "auc_prc": 0.874317327913164,
            "auditor_fn_violation": 0.01886233989882682,
            "auditor_fp_violation": 0.012989403176973169,
            "ave_precision_score": 0.8744744747782909,
            "fpr": 0.10087719298245613,
            "logloss": 0.7821960826431458,
            "mae": 0.23975478271013212,
            "precision": 0.8029978586723768,
            "recall": 0.7668711656441718
        },
        "train": {
            "accuracy": 0.7815587266739846,
            "auc_prc": 0.8411463925401363,
            "auditor_fn_violation": 0.011824416038147849,
            "auditor_fp_violation": 0.021661014112516183,
            "ave_precision_score": 0.8414969683805382,
            "fpr": 0.11525795828759605,
            "logloss": 0.8648668082893038,
            "mae": 0.24440352679361593,
            "precision": 0.7794117647058824,
            "recall": 0.7978494623655914
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8574755683194358,
            "auditor_fn_violation": 0.0038679905284684166,
            "auditor_fp_violation": 0.01357523536974825,
            "ave_precision_score": 0.8577484068070935,
            "fpr": 0.11074561403508772,
            "logloss": 0.799851828120608,
            "mae": 0.24704749450210176,
            "precision": 0.7900207900207901,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8352860096061101,
            "auditor_fn_violation": 0.009220636663007686,
            "auditor_fp_violation": 0.009903865559455197,
            "ave_precision_score": 0.835628968627952,
            "fpr": 0.11745334796926454,
            "logloss": 0.8406194961810679,
            "mae": 0.24615505844646168,
            "precision": 0.7766179540709812,
            "recall": 0.8
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7796052631578947,
            "auc_prc": 0.8822863342896559,
            "auditor_fn_violation": 0.016438399167653293,
            "auditor_fp_violation": 0.012989403176973169,
            "ave_precision_score": 0.8824866993947929,
            "fpr": 0.10087719298245613,
            "logloss": 0.7156063737293927,
            "mae": 0.23348305022506322,
            "precision": 0.8050847457627118,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7914379802414928,
            "auc_prc": 0.8498820253264661,
            "auditor_fn_violation": 0.01150336980512966,
            "auditor_fp_violation": 0.016928128061116494,
            "ave_precision_score": 0.85022300676004,
            "fpr": 0.11745334796926454,
            "logloss": 0.791519979520397,
            "mae": 0.23580784261107646,
            "precision": 0.7811860940695297,
            "recall": 0.821505376344086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7741228070175439,
            "auc_prc": 0.8789986883038722,
            "auditor_fn_violation": 0.017528163455673956,
            "auditor_fp_violation": 0.012686118369209076,
            "ave_precision_score": 0.8792181749035594,
            "fpr": 0.10635964912280702,
            "logloss": 0.701793825918509,
            "mae": 0.24000134354183325,
            "precision": 0.7966457023060797,
            "recall": 0.7770961145194274
        },
        "train": {
            "accuracy": 0.7804610318331504,
            "auc_prc": 0.8453759896706423,
            "auditor_fn_violation": 0.015240253532098721,
            "auditor_fp_violation": 0.01934010327191821,
            "ave_precision_score": 0.8457340602035025,
            "fpr": 0.12403951701427003,
            "logloss": 0.775584399099479,
            "mae": 0.24277002362376476,
            "precision": 0.769857433808554,
            "recall": 0.8129032258064516
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7839912280701754,
            "auc_prc": 0.8799560279007502,
            "auditor_fn_violation": 0.015364331073081473,
            "auditor_fp_violation": 0.013476732611671023,
            "ave_precision_score": 0.8801690335801657,
            "fpr": 0.09758771929824561,
            "logloss": 0.7154322657540582,
            "mae": 0.2344933609786301,
            "precision": 0.8106382978723404,
            "recall": 0.7791411042944786
        },
        "train": {
            "accuracy": 0.7903402854006586,
            "auc_prc": 0.8490553083379078,
            "auditor_fn_violation": 0.013519351297758577,
            "auditor_fp_violation": 0.02026551416912377,
            "ave_precision_score": 0.8494051553135125,
            "fpr": 0.11745334796926454,
            "logloss": 0.7879629251831789,
            "mae": 0.23699709518013815,
            "precision": 0.7807377049180327,
            "recall": 0.8193548387096774
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 24284,
        "test": {
            "accuracy": 0.7752192982456141,
            "auc_prc": 0.8788944076692602,
            "auditor_fn_violation": 0.017256843540343705,
            "auditor_fp_violation": 0.01242949276263946,
            "ave_precision_score": 0.8790935951020789,
            "fpr": 0.1074561403508772,
            "logloss": 0.7518621505733362,
            "mae": 0.2377262989019672,
            "precision": 0.7958333333333333,
            "recall": 0.7811860940695297
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8446093088219873,
            "auditor_fn_violation": 0.014520260141874112,
            "auditor_fp_violation": 0.021515803360029142,
            "ave_precision_score": 0.8449731422158024,
            "fpr": 0.12623490669593854,
            "logloss": 0.8360095050386087,
            "mae": 0.23980734947234333,
            "precision": 0.7686116700201208,
            "recall": 0.821505376344086
        }
    }
]
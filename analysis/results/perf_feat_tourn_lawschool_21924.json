[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.6571687426054472,
            "auditor_fn_violation": 0.021876228248829822,
            "auditor_fp_violation": 0.030993457515522783,
            "ave_precision_score": 0.6499794495919047,
            "fpr": 0.17105263157894737,
            "logloss": 1.7418938815685947,
            "mae": 0.342307268476018,
            "precision": 0.695906432748538,
            "recall": 0.7270875763747454
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.6393080520019403,
            "auditor_fn_violation": 0.017394788438878796,
            "auditor_fp_violation": 0.03196546181590091,
            "ave_precision_score": 0.6309781230704093,
            "fpr": 0.16355653128430298,
            "logloss": 1.7198914863716919,
            "mae": 0.3461223206100885,
            "precision": 0.6829787234042554,
            "recall": 0.693304535637149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.6992947380954337,
            "auditor_fn_violation": 0.015949369350055385,
            "auditor_fp_violation": 0.028066008251031383,
            "ave_precision_score": 0.6989152621212767,
            "fpr": 0.23464912280701755,
            "logloss": 0.9236882283063872,
            "mae": 0.39111094860556095,
            "precision": 0.65814696485623,
            "recall": 0.8391038696537678
        },
        "train": {
            "accuracy": 0.6586169045005489,
            "auc_prc": 0.6644892727285818,
            "auditor_fn_violation": 0.01573520660608403,
            "auditor_fp_violation": 0.02772659557785793,
            "ave_precision_score": 0.6645082351094129,
            "fpr": 0.23600439077936333,
            "logloss": 0.8262056892865488,
            "mae": 0.4054357353052054,
            "precision": 0.6305841924398625,
            "recall": 0.7926565874730022
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 21924,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7236557714863265,
            "auditor_fn_violation": 0.041416550541322754,
            "auditor_fp_violation": 0.028654623494603498,
            "ave_precision_score": 0.7227138131876256,
            "fpr": 0.09429824561403509,
            "logloss": 0.9787953257990967,
            "mae": 0.35833275615540944,
            "precision": 0.7822784810126582,
            "recall": 0.6293279022403259
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.6971443746154045,
            "auditor_fn_violation": 0.02546746863983044,
            "auditor_fp_violation": 0.028407754429982755,
            "ave_precision_score": 0.6960825263074332,
            "fpr": 0.10208562019758508,
            "logloss": 0.9310969012650957,
            "mae": 0.36217349553830086,
            "precision": 0.7486486486486487,
            "recall": 0.5982721382289417
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.6920540902911234,
            "auditor_fn_violation": 0.014160592417908313,
            "auditor_fp_violation": 0.02894632662416135,
            "ave_precision_score": 0.6925997628528515,
            "fpr": 0.14583333333333334,
            "logloss": 1.1918196625165136,
            "mae": 0.3253093317027803,
            "precision": 0.7240663900414938,
            "recall": 0.7107942973523421
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.6925479450134173,
            "auditor_fn_violation": 0.012994525750782976,
            "auditor_fp_violation": 0.028983554179081085,
            "ave_precision_score": 0.6926084144251323,
            "fpr": 0.1350164654226125,
            "logloss": 1.0971114955377892,
            "mae": 0.3216613596456537,
            "precision": 0.7210884353741497,
            "recall": 0.6868250539956804
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.6725634207078175,
            "auditor_fn_violation": 0.01948002644084754,
            "auditor_fp_violation": 0.028670250447972667,
            "ave_precision_score": 0.6682003962317498,
            "fpr": 0.13925438596491227,
            "logloss": 1.5019225446212991,
            "mae": 0.33626887336596645,
            "precision": 0.7257019438444925,
            "recall": 0.6843177189409368
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.6595700882066429,
            "auditor_fn_violation": 0.015355873615730951,
            "auditor_fp_violation": 0.030382625058805075,
            "ave_precision_score": 0.6571391367340199,
            "fpr": 0.13172338090010977,
            "logloss": 1.3949421292839812,
            "mae": 0.33492812705682234,
            "precision": 0.719626168224299,
            "recall": 0.6652267818574514
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6469298245614035,
            "auc_prc": 0.7766128325976134,
            "auditor_fn_violation": 0.044960606710258345,
            "auditor_fp_violation": 0.012376547068383549,
            "ave_precision_score": 0.7772294380468066,
            "fpr": 0.04276315789473684,
            "logloss": 0.6669451667096716,
            "mae": 0.4055105415009485,
            "precision": 0.8421052631578947,
            "recall": 0.42362525458248473
        },
        "train": {
            "accuracy": 0.6355653128430296,
            "auc_prc": 0.7462257069042442,
            "auditor_fn_violation": 0.033065982602840736,
            "auditor_fp_violation": 0.01940077622706602,
            "ave_precision_score": 0.7469093694953952,
            "fpr": 0.054884742041712405,
            "logloss": 0.6658703205495689,
            "mae": 0.40302622567686297,
            "precision": 0.7835497835497836,
            "recall": 0.39092872570194387
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.6883228996959856,
            "auditor_fn_violation": 0.034902365383928255,
            "auditor_fp_violation": 0.03107159228236863,
            "ave_precision_score": 0.6863985750549261,
            "fpr": 0.15460526315789475,
            "logloss": 1.0635764369561709,
            "mae": 0.3737907520640364,
            "precision": 0.7116564417177914,
            "recall": 0.7087576374745418
        },
        "train": {
            "accuracy": 0.6597145993413831,
            "auc_prc": 0.656482236746559,
            "auditor_fn_violation": 0.024118465692887274,
            "auditor_fp_violation": 0.023007487846949982,
            "ave_precision_score": 0.6552169703652562,
            "fpr": 0.17014270032930845,
            "logloss": 1.0104641197919957,
            "mae": 0.3851789882005775,
            "precision": 0.6652267818574514,
            "recall": 0.6652267818574514
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6853088089967905,
            "auditor_fn_violation": 0.056329724514953376,
            "auditor_fp_violation": 0.0279462016085344,
            "ave_precision_score": 0.6849475649409231,
            "fpr": 0.07785087719298246,
            "logloss": 0.9984530752604539,
            "mae": 0.4131589724111428,
            "precision": 0.7687296416938111,
            "recall": 0.48065173116089616
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.6547278098722739,
            "auditor_fn_violation": 0.037570561863283655,
            "auditor_fp_violation": 0.031181394072447858,
            "ave_precision_score": 0.6546197196991738,
            "fpr": 0.09001097694840834,
            "logloss": 0.8986857459341755,
            "mae": 0.41457700824402516,
            "precision": 0.7191780821917808,
            "recall": 0.4535637149028078
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6337719298245614,
            "auc_prc": 0.6923541683523913,
            "auditor_fn_violation": 0.034288240969021326,
            "auditor_fp_violation": 0.02389621619369088,
            "ave_precision_score": 0.6930442009581748,
            "fpr": 0.09758771929824561,
            "logloss": 0.8267921586864394,
            "mae": 0.40751682714510123,
            "precision": 0.7343283582089553,
            "recall": 0.5010183299389002
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.691240300793154,
            "auditor_fn_violation": 0.030861109596413416,
            "auditor_fp_violation": 0.023080994197898694,
            "ave_precision_score": 0.6919465598128467,
            "fpr": 0.09001097694840834,
            "logloss": 0.7898929356603485,
            "mae": 0.39477828724489833,
            "precision": 0.7492354740061162,
            "recall": 0.5291576673866091
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.6936934509406896,
            "auditor_fn_violation": 0.009316825669060638,
            "auditor_fp_violation": 0.028026940867608454,
            "ave_precision_score": 0.6943127131178096,
            "fpr": 0.1524122807017544,
            "logloss": 1.1824217812113098,
            "mae": 0.3231499515039527,
            "precision": 0.719758064516129,
            "recall": 0.7270875763747454
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.6924015915920958,
            "auditor_fn_violation": 0.01314862977811391,
            "auditor_fp_violation": 0.031470519052846174,
            "ave_precision_score": 0.6927667526631257,
            "fpr": 0.13611416026344675,
            "logloss": 1.088349444655265,
            "mae": 0.3218227001092028,
            "precision": 0.7200902934537246,
            "recall": 0.6889848812095032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.6715784380001502,
            "auditor_fn_violation": 0.012588433915746605,
            "auditor_fp_violation": 0.032217568862774514,
            "ave_precision_score": 0.666072218428913,
            "fpr": 0.15899122807017543,
            "logloss": 1.566552101746034,
            "mae": 0.32553234083530025,
            "precision": 0.7145669291338582,
            "recall": 0.7393075356415478
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.6587375907316826,
            "auditor_fn_violation": 0.015872714815087022,
            "auditor_fp_violation": 0.03126715148188804,
            "ave_precision_score": 0.6540438005212965,
            "fpr": 0.14818880351262348,
            "logloss": 1.5008357251257,
            "mae": 0.3290196155153028,
            "precision": 0.7039473684210527,
            "recall": 0.693304535637149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.6921585643882742,
            "auditor_fn_violation": 0.009234198020509524,
            "auditor_fp_violation": 0.028128516064508065,
            "ave_precision_score": 0.6909982845605422,
            "fpr": 0.1513157894736842,
            "logloss": 1.2402180064477248,
            "mae": 0.31935693141913624,
            "precision": 0.7250996015936255,
            "recall": 0.7413441955193483
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.6908448099395128,
            "auditor_fn_violation": 0.013684437626987645,
            "auditor_fp_violation": 0.030666849615806808,
            "ave_precision_score": 0.6898347983079942,
            "fpr": 0.13830954994511527,
            "logloss": 1.1077606577389272,
            "mae": 0.3195285018002108,
            "precision": 0.720620842572062,
            "recall": 0.7019438444924406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.6713416072914613,
            "auditor_fn_violation": 0.024917818987387007,
            "auditor_fp_violation": 0.02872754927699296,
            "ave_precision_score": 0.6658374812002639,
            "fpr": 0.14144736842105263,
            "logloss": 1.5765601716914757,
            "mae": 0.3371152798634288,
            "precision": 0.7207792207792207,
            "recall": 0.6782077393075356
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.6573249062156362,
            "auditor_fn_violation": 0.020118873475851905,
            "auditor_fp_violation": 0.03251430923631802,
            "ave_precision_score": 0.6539221906777575,
            "fpr": 0.1350164654226125,
            "logloss": 1.4658105100273455,
            "mae": 0.3349806536345355,
            "precision": 0.711943793911007,
            "recall": 0.6565874730021598
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.685980566217857,
            "auditor_fn_violation": 0.05580269410797871,
            "auditor_fp_violation": 0.02776128266033254,
            "ave_precision_score": 0.6865879472412241,
            "fpr": 0.07785087719298246,
            "logloss": 0.9939568498798947,
            "mae": 0.4118999371390888,
            "precision": 0.7687296416938111,
            "recall": 0.48065173116089616
        },
        "train": {
            "accuracy": 0.6300768386388584,
            "auc_prc": 0.6550425931257141,
            "auditor_fn_violation": 0.03537517218161515,
            "auditor_fp_violation": 0.03265642151481889,
            "ave_precision_score": 0.6550815843054295,
            "fpr": 0.09220636663007684,
            "logloss": 0.8771998721250955,
            "mae": 0.4146139596076554,
            "precision": 0.7142857142857143,
            "recall": 0.4535637149028078
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.6769222191115315,
            "auditor_fn_violation": 0.01931030478436417,
            "auditor_fp_violation": 0.029394299287410924,
            "ave_precision_score": 0.6736982372356597,
            "fpr": 0.13815789473684212,
            "logloss": 1.4261108099163422,
            "mae": 0.3370953773260465,
            "precision": 0.7266811279826464,
            "recall": 0.6822810590631364
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.6657739774369918,
            "auditor_fn_violation": 0.0033855469389013127,
            "auditor_fp_violation": 0.03265642151481889,
            "ave_precision_score": 0.663582130860142,
            "fpr": 0.13830954994511527,
            "logloss": 1.3043469755789305,
            "mae": 0.33566314618552245,
            "precision": 0.7049180327868853,
            "recall": 0.6501079913606912
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6900483727813872,
            "auditor_fn_violation": 0.05277226926787438,
            "auditor_fp_violation": 0.02693565862399467,
            "ave_precision_score": 0.6898061868507097,
            "fpr": 0.07236842105263158,
            "logloss": 0.9540007853862008,
            "mae": 0.41488922952231133,
            "precision": 0.7762711864406779,
            "recall": 0.4663951120162933
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6593663640197422,
            "auditor_fn_violation": 0.03756107853852481,
            "auditor_fp_violation": 0.03198996393288381,
            "ave_precision_score": 0.6600447158709084,
            "fpr": 0.0867178924259056,
            "logloss": 0.8501914208131142,
            "mae": 0.414785110711393,
            "precision": 0.7188612099644128,
            "recall": 0.43628509719222464
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6282894736842105,
            "auc_prc": 0.6853766024550874,
            "auditor_fn_violation": 0.05732125629756672,
            "auditor_fp_violation": 0.028300412551568947,
            "ave_precision_score": 0.6858086202105755,
            "fpr": 0.0756578947368421,
            "logloss": 0.9596621850389815,
            "mae": 0.41892855962240055,
            "precision": 0.7620689655172413,
            "recall": 0.45010183299389
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.654330369392203,
            "auditor_fn_violation": 0.04268444473948122,
            "auditor_fp_violation": 0.032480006272541946,
            "ave_precision_score": 0.6550235775576088,
            "fpr": 0.09001097694840834,
            "logloss": 0.8520038021076896,
            "mae": 0.4176483562195235,
            "precision": 0.7028985507246377,
            "recall": 0.4190064794816415
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.8016583288117799,
            "auditor_fn_violation": 0.02385929181405653,
            "auditor_fp_violation": 0.01181397674709339,
            "ave_precision_score": 0.8019414615749426,
            "fpr": 0.09320175438596491,
            "logloss": 2.1674251350027047,
            "mae": 0.3101541618909571,
            "precision": 0.782608695652174,
            "recall": 0.6232179226069247
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.787781948962156,
            "auditor_fn_violation": 0.022582167081957264,
            "auditor_fp_violation": 0.004253567508232712,
            "ave_precision_score": 0.7884006448766585,
            "fpr": 0.09001097694840834,
            "logloss": 1.7270443115403196,
            "mae": 0.2984451728069761,
            "precision": 0.776566757493188,
            "recall": 0.6155507559395248
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.7059131509575984,
            "auditor_fn_violation": 0.02663513416943581,
            "auditor_fp_violation": 0.03778857773888403,
            "ave_precision_score": 0.7056546223425344,
            "fpr": 0.16337719298245615,
            "logloss": 0.8631726684979398,
            "mae": 0.3875035456169331,
            "precision": 0.702,
            "recall": 0.714867617107943
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6830993713322497,
            "auditor_fn_violation": 0.02046264399835939,
            "auditor_fp_violation": 0.038389916888819195,
            "ave_precision_score": 0.6837338220529741,
            "fpr": 0.14709110867178923,
            "logloss": 0.775662447988561,
            "mae": 0.394377272042397,
            "precision": 0.6995515695067265,
            "recall": 0.673866090712743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.6986160066069219,
            "auditor_fn_violation": 0.00534176581984493,
            "auditor_fp_violation": 0.02265126890861358,
            "ave_precision_score": 0.6981919084342034,
            "fpr": 0.1600877192982456,
            "logloss": 1.1754482865352205,
            "mae": 0.31481221961631856,
            "precision": 0.7159533073929961,
            "recall": 0.7494908350305499
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.6973543645915085,
            "auditor_fn_violation": 0.0065055607845554584,
            "auditor_fp_violation": 0.02478634153990905,
            "ave_precision_score": 0.6968580588325943,
            "fpr": 0.14818880351262348,
            "logloss": 1.078457266196674,
            "mae": 0.3166653300075154,
            "precision": 0.7039473684210527,
            "recall": 0.693304535637149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.6650606415389959,
            "auditor_fn_violation": 0.02054971951263087,
            "auditor_fp_violation": 0.02739404925615702,
            "ave_precision_score": 0.6591360142996342,
            "fpr": 0.15789473684210525,
            "logloss": 1.6093569855066594,
            "mae": 0.3348114003729438,
            "precision": 0.7061224489795919,
            "recall": 0.7046843177189409
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.6511085136853106,
            "auditor_fn_violation": 0.01964470723791055,
            "auditor_fp_violation": 0.030539438607495695,
            "ave_precision_score": 0.6474300653390442,
            "fpr": 0.14928649835345773,
            "logloss": 1.5419263361009634,
            "mae": 0.33661176588997416,
            "precision": 0.6997792494481236,
            "recall": 0.6846652267818575
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 21924,
        "test": {
            "accuracy": 0.5942982456140351,
            "auc_prc": 0.6965229722656587,
            "auditor_fn_violation": 0.04595437167256227,
            "auditor_fp_violation": 0.020132724923948825,
            "ave_precision_score": 0.6969850547987301,
            "fpr": 0.0581140350877193,
            "logloss": 1.0761758751014223,
            "mae": 0.4197514424858239,
            "precision": 0.7665198237885462,
            "recall": 0.3543788187372709
        },
        "train": {
            "accuracy": 0.6026344676180022,
            "auc_prc": 0.6753575853990892,
            "auditor_fn_violation": 0.02848079508194779,
            "auditor_fp_violation": 0.015402030735455546,
            "ave_precision_score": 0.6760875153533008,
            "fpr": 0.054884742041712405,
            "logloss": 0.9646360923931129,
            "mae": 0.4135799234731452,
            "precision": 0.7512437810945274,
            "recall": 0.326133909287257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.6789072662354071,
            "auditor_fn_violation": 0.0049040626004930865,
            "auditor_fp_violation": 0.017944951452264876,
            "ave_precision_score": 0.6715640777338354,
            "fpr": 0.21052631578947367,
            "logloss": 1.7308060449676979,
            "mae": 0.3040314533952764,
            "precision": 0.6847290640394089,
            "recall": 0.8492871690427699
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.6668777084452974,
            "auditor_fn_violation": 0.003933208943723584,
            "auditor_fp_violation": 0.030174357064450368,
            "ave_precision_score": 0.659570528360528,
            "fpr": 0.1986827661909989,
            "logloss": 1.6112512775261247,
            "mae": 0.31769328465135666,
            "precision": 0.6715063520871143,
            "recall": 0.7991360691144709
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.6647528646505363,
            "auditor_fn_violation": 0.022329563726015658,
            "auditor_fp_violation": 0.03115754052589907,
            "ave_precision_score": 0.6588701562320436,
            "fpr": 0.15899122807017543,
            "logloss": 1.6043636595271538,
            "mae": 0.33755797894536155,
            "precision": 0.7058823529411765,
            "recall": 0.7087576374745418
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.649590618512762,
            "auditor_fn_violation": 0.0190709660900015,
            "auditor_fp_violation": 0.02859152030735456,
            "ave_precision_score": 0.6460205922690923,
            "fpr": 0.1525795828759605,
            "logloss": 1.5355826077697947,
            "mae": 0.33951310261808915,
            "precision": 0.6931567328918322,
            "recall": 0.6781857451403888
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 21924,
        "test": {
            "accuracy": 0.5339912280701754,
            "auc_prc": 0.6458967857806892,
            "auditor_fn_violation": 0.12284274841890878,
            "auditor_fp_violation": 0.10860732591573947,
            "ave_precision_score": 0.6462490799673364,
            "fpr": 0.2565789473684211,
            "logloss": 1.9248679947169918,
            "mae": 0.45360747827052955,
            "precision": 0.5617977528089888,
            "recall": 0.6109979633401222
        },
        "train": {
            "accuracy": 0.5268935236004391,
            "auc_prc": 0.6104369534001909,
            "auditor_fn_violation": 0.11992138323774934,
            "auditor_fp_violation": 0.11522855574721658,
            "ave_precision_score": 0.610236956707271,
            "fpr": 0.24807903402854006,
            "logloss": 1.8781552388526377,
            "mae": 0.46163320976373085,
            "precision": 0.5330578512396694,
            "recall": 0.5572354211663066
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.6582377248262848,
            "auditor_fn_violation": 0.02278066602351092,
            "auditor_fp_violation": 0.03093615868650248,
            "ave_precision_score": 0.6523300117938405,
            "fpr": 0.1337719298245614,
            "logloss": 1.67718203744879,
            "mae": 0.36340258423824545,
            "precision": 0.7149532710280374,
            "recall": 0.6232179226069247
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.6419052173701163,
            "auditor_fn_violation": 0.022835846019255902,
            "auditor_fp_violation": 0.030769758507135026,
            "ave_precision_score": 0.6374149303488155,
            "fpr": 0.1350164654226125,
            "logloss": 1.6027377070547417,
            "mae": 0.357890126366843,
            "precision": 0.6955445544554455,
            "recall": 0.6069114470842333
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6326754385964912,
            "auc_prc": 0.6924694454379443,
            "auditor_fn_violation": 0.033691981991638975,
            "auditor_fp_violation": 0.02389621619369088,
            "ave_precision_score": 0.6931587441755698,
            "fpr": 0.09758771929824561,
            "logloss": 0.826044470372674,
            "mae": 0.4074991611557555,
            "precision": 0.7335329341317365,
            "recall": 0.4989816700610998
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.6914254321835551,
            "auditor_fn_violation": 0.030861109596413416,
            "auditor_fp_violation": 0.023080994197898694,
            "ave_precision_score": 0.6921312182941012,
            "fpr": 0.09001097694840834,
            "logloss": 0.7893349264303443,
            "mae": 0.39475145072477186,
            "precision": 0.7492354740061162,
            "recall": 0.5291576673866091
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.674973006373905,
            "auditor_fn_violation": 0.016407171186622365,
            "auditor_fp_violation": 0.030402237779722473,
            "ave_precision_score": 0.6695071989381792,
            "fpr": 0.15021929824561403,
            "logloss": 1.5448615321428123,
            "mae": 0.32632247038779433,
            "precision": 0.7221095334685599,
            "recall": 0.725050916496945
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.6611265608872722,
            "auditor_fn_violation": 0.016142989570713598,
            "auditor_fp_violation": 0.031730241492864984,
            "ave_precision_score": 0.6576408576565824,
            "fpr": 0.145993413830955,
            "logloss": 1.4349995890722518,
            "mae": 0.32975435222655264,
            "precision": 0.7037861915367484,
            "recall": 0.6825053995680346
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.6747583712899703,
            "auditor_fn_violation": 0.0003640082895630129,
            "auditor_fp_violation": 0.02695909905404843,
            "ave_precision_score": 0.6688396480073915,
            "fpr": 0.17214912280701755,
            "logloss": 1.562212499490294,
            "mae": 0.3194464287036776,
            "precision": 0.7059925093632958,
            "recall": 0.7678207739307535
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.6612744007899659,
            "auditor_fn_violation": 0.0006211577717031839,
            "auditor_fp_violation": 0.031580778579269254,
            "ave_precision_score": 0.65768117660022,
            "fpr": 0.15916575192096596,
            "logloss": 1.4653464643736083,
            "mae": 0.32610103112816957,
            "precision": 0.692144373673036,
            "recall": 0.7041036717062635
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.6755239582430126,
            "auditor_fn_violation": 0.014584896559116734,
            "auditor_fp_violation": 0.03173573780055841,
            "ave_precision_score": 0.6700999691832588,
            "fpr": 0.1611842105263158,
            "logloss": 1.5478730573219819,
            "mae": 0.32500937089995413,
            "precision": 0.7123287671232876,
            "recall": 0.7413441955193483
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.6581312236582438,
            "auditor_fn_violation": 0.011275673138245538,
            "auditor_fp_violation": 0.029348635722126398,
            "ave_precision_score": 0.654723915613809,
            "fpr": 0.1525795828759605,
            "logloss": 1.4650219318271263,
            "mae": 0.3323210601680054,
            "precision": 0.6971677559912854,
            "recall": 0.6911447084233261
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7097014679888471,
            "auditor_fn_violation": 0.011398149140672461,
            "auditor_fp_violation": 0.03231914405967413,
            "ave_precision_score": 0.7102106670026278,
            "fpr": 0.15350877192982457,
            "logloss": 1.00327314282488,
            "mae": 0.30539858632581846,
            "precision": 0.7318007662835249,
            "recall": 0.7780040733197556
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7106062116426166,
            "auditor_fn_violation": 0.005265616072338811,
            "auditor_fp_violation": 0.03277158146463855,
            "ave_precision_score": 0.710460548474217,
            "fpr": 0.1394072447859495,
            "logloss": 0.9799439739036103,
            "mae": 0.30695415715883767,
            "precision": 0.7251082251082251,
            "recall": 0.7235421166306696
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.7420138989882308,
            "auditor_fn_violation": 0.01699226426555187,
            "auditor_fp_violation": 0.024732258198941537,
            "ave_precision_score": 0.7423341233130802,
            "fpr": 0.10855263157894737,
            "logloss": 0.865488505908708,
            "mae": 0.35494988043578096,
            "precision": 0.771889400921659,
            "recall": 0.6822810590631364
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7171342612749145,
            "auditor_fn_violation": 0.021199972498358196,
            "auditor_fp_violation": 0.03648120197585071,
            "ave_precision_score": 0.717598242156358,
            "fpr": 0.11525795828759605,
            "logloss": 0.8022977493641805,
            "mae": 0.36319563461295495,
            "precision": 0.7445255474452555,
            "recall": 0.6609071274298056
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.6740202898279373,
            "auditor_fn_violation": 0.022289366491585385,
            "auditor_fp_violation": 0.02888902779514106,
            "ave_precision_score": 0.6681021678696126,
            "fpr": 0.14692982456140352,
            "logloss": 1.5513282116457627,
            "mae": 0.3327653537018581,
            "precision": 0.7190775681341719,
            "recall": 0.6985743380855397
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.6541246591865143,
            "auditor_fn_violation": 0.01298741325721385,
            "auditor_fp_violation": 0.03133330719774189,
            "ave_precision_score": 0.6507402232690231,
            "fpr": 0.14709110867178923,
            "logloss": 1.46891844079425,
            "mae": 0.3378847169629893,
            "precision": 0.6961451247165533,
            "recall": 0.6630669546436285
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.7286988527144973,
            "auditor_fn_violation": 0.010922481866580918,
            "auditor_fp_violation": 0.014590365462349464,
            "ave_precision_score": 0.7291131728566855,
            "fpr": 0.12938596491228072,
            "logloss": 1.286510083831344,
            "mae": 0.3262203500454243,
            "precision": 0.7324263038548753,
            "recall": 0.6578411405295316
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7198028789910669,
            "auditor_fn_violation": 0.012553551149497499,
            "auditor_fp_violation": 0.017979653442057396,
            "ave_precision_score": 0.7197025804772882,
            "fpr": 0.12952799121844127,
            "logloss": 1.3105672181100159,
            "mae": 0.3242181079194888,
            "precision": 0.7100737100737101,
            "recall": 0.6241900647948164
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.756746481183592,
            "auditor_fn_violation": 0.014140493800693184,
            "auditor_fp_violation": 0.028110284618910696,
            "ave_precision_score": 0.7572635222536909,
            "fpr": 0.14144736842105263,
            "logloss": 0.7948979798224435,
            "mae": 0.3535076656950496,
            "precision": 0.7048054919908466,
            "recall": 0.6272912423625254
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7385342382264877,
            "auditor_fn_violation": 0.021465505591605368,
            "auditor_fp_violation": 0.023068743139407242,
            "ave_precision_score": 0.7392582272442533,
            "fpr": 0.11964873765093303,
            "logloss": 0.8376886955348706,
            "mae": 0.33811016906756025,
            "precision": 0.72544080604534,
            "recall": 0.6220302375809935
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.6878656496739661,
            "auditor_fn_violation": 0.042441580019294683,
            "auditor_fp_violation": 0.02484164687252574,
            "ave_precision_score": 0.6868659466500996,
            "fpr": 0.0800438596491228,
            "logloss": 1.000403611135759,
            "mae": 0.41279605185059554,
            "precision": 0.7689873417721519,
            "recall": 0.49490835030549896
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.652538573197214,
            "auditor_fn_violation": 0.0361196131751831,
            "auditor_fp_violation": 0.030752607025246983,
            "ave_precision_score": 0.6524263281355835,
            "fpr": 0.09549945115257959,
            "logloss": 0.9055692522584773,
            "mae": 0.4154028388145851,
            "precision": 0.7080536912751678,
            "recall": 0.4557235421166307
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 21924,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7034500058353342,
            "auditor_fn_violation": 0.008131007253367638,
            "auditor_fp_violation": 0.030269408676084516,
            "ave_precision_score": 0.7039764551119765,
            "fpr": 0.13925438596491227,
            "logloss": 1.0697533549493783,
            "mae": 0.327176601434657,
            "precision": 0.7274678111587983,
            "recall": 0.6904276985743381
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7049615076293781,
            "auditor_fn_violation": 0.011638410310270682,
            "auditor_fp_violation": 0.02428159793006115,
            "ave_precision_score": 0.7048353022075199,
            "fpr": 0.12403951701427003,
            "logloss": 1.0775607632053799,
            "mae": 0.3176887486654027,
            "precision": 0.735981308411215,
            "recall": 0.6803455723542117
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.6861066328769092,
            "auditor_fn_violation": 0.013182459713438384,
            "auditor_fp_violation": 0.02941773971746468,
            "ave_precision_score": 0.6805859425546746,
            "fpr": 0.17324561403508773,
            "logloss": 1.5130752588037102,
            "mae": 0.3073743686986466,
            "precision": 0.710091743119266,
            "recall": 0.7881873727087576
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.672663419723369,
            "auditor_fn_violation": 0.003499346836007234,
            "auditor_fp_violation": 0.027716794731064774,
            "ave_precision_score": 0.669268752838921,
            "fpr": 0.15806805708013172,
            "logloss": 1.364025107184751,
            "mae": 0.31282383284469073,
            "precision": 0.7037037037037037,
            "recall": 0.7386609071274298
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.6686891436482376,
            "auditor_fn_violation": 0.02282756279701291,
            "auditor_fp_violation": 0.03400164603908823,
            "ave_precision_score": 0.6643541827260618,
            "fpr": 0.14035087719298245,
            "logloss": 1.5185873125147102,
            "mae": 0.34285634510271584,
            "precision": 0.7186813186813187,
            "recall": 0.6659877800407332
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.6541102161453201,
            "auditor_fn_violation": 0.02054562308999913,
            "auditor_fp_violation": 0.029221224713815278,
            "ave_precision_score": 0.6517906346610512,
            "fpr": 0.1350164654226125,
            "logloss": 1.4355492453001666,
            "mae": 0.33998863126458745,
            "precision": 0.7112676056338029,
            "recall": 0.6544276457883369
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 21924,
        "test": {
            "accuracy": 0.643640350877193,
            "auc_prc": 0.6851118793860831,
            "auditor_fn_violation": 0.055825025904884404,
            "auditor_fp_violation": 0.0279462016085344,
            "ave_precision_score": 0.6847502787733829,
            "fpr": 0.07785087719298246,
            "logloss": 0.9989032794740941,
            "mae": 0.4129460021351023,
            "precision": 0.7694805194805194,
            "recall": 0.48268839103869654
        },
        "train": {
            "accuracy": 0.6333699231613611,
            "auc_prc": 0.6540609686718506,
            "auditor_fn_violation": 0.0370228998584614,
            "auditor_fp_violation": 0.031181394072447858,
            "ave_precision_score": 0.6539540091231297,
            "fpr": 0.09001097694840834,
            "logloss": 0.90076576740146,
            "mae": 0.41465638740891175,
            "precision": 0.7201365187713311,
            "recall": 0.4557235421166307
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.6740261821954446,
            "auditor_fn_violation": 0.019600618144138358,
            "auditor_fp_violation": 0.03272544484727258,
            "ave_precision_score": 0.6691099402805436,
            "fpr": 0.14802631578947367,
            "logloss": 1.5231982140454952,
            "mae": 0.3275345290773115,
            "precision": 0.7244897959183674,
            "recall": 0.7230142566191446
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.6612969492276113,
            "auditor_fn_violation": 0.01780257140350836,
            "auditor_fp_violation": 0.030421828445977732,
            "ave_precision_score": 0.6582634069770255,
            "fpr": 0.14050493962678376,
            "logloss": 1.410832778031797,
            "mae": 0.3301100054367287,
            "precision": 0.7097505668934241,
            "recall": 0.6760259179265659
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.6731389454239356,
            "auditor_fn_violation": 0.011817986922499739,
            "auditor_fp_violation": 0.028081635204400556,
            "ave_precision_score": 0.6693598729899374,
            "fpr": 0.11951754385964912,
            "logloss": 1.4688715114548292,
            "mae": 0.3511244887915238,
            "precision": 0.7386091127098321,
            "recall": 0.6272912423625254
        },
        "train": {
            "accuracy": 0.6717892425905598,
            "auc_prc": 0.6578395592837589,
            "auditor_fn_violation": 0.01619040619450773,
            "auditor_fp_violation": 0.034645993413830956,
            "ave_precision_score": 0.6561500719764335,
            "fpr": 0.13062568605927552,
            "logloss": 1.3500440789147754,
            "mae": 0.34772725963289447,
            "precision": 0.7039800995024875,
            "recall": 0.6112311015118791
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.6881968975861491,
            "auditor_fn_violation": 0.05470620287990854,
            "auditor_fp_violation": 0.02805298578989041,
            "ave_precision_score": 0.6871984772230312,
            "fpr": 0.07894736842105263,
            "logloss": 0.9961594662758443,
            "mae": 0.4121338614102314,
            "precision": 0.7662337662337663,
            "recall": 0.48065173116089616
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.6557027244433348,
            "auditor_fn_violation": 0.039675859959743286,
            "auditor_fp_violation": 0.03135290889132821,
            "ave_precision_score": 0.6555637255309479,
            "fpr": 0.09220636663007684,
            "logloss": 0.8980743758310765,
            "mae": 0.4131865091783875,
            "precision": 0.7162162162162162,
            "recall": 0.45788336933045354
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6370614035087719,
            "auc_prc": 0.6823415626761646,
            "auditor_fn_violation": 0.04913888591131598,
            "auditor_fp_violation": 0.027357586364962293,
            "ave_precision_score": 0.6827689933867092,
            "fpr": 0.08333333333333333,
            "logloss": 0.9465496423956377,
            "mae": 0.41482202595777873,
            "precision": 0.7564102564102564,
            "recall": 0.48065173116089616
        },
        "train": {
            "accuracy": 0.6311745334796927,
            "auc_prc": 0.6555373733334602,
            "auditor_fn_violation": 0.03745913279736745,
            "auditor_fp_violation": 0.0338913282107574,
            "ave_precision_score": 0.6562793468384025,
            "fpr": 0.09659714599341383,
            "logloss": 0.841001535065555,
            "mae": 0.4148506917412008,
            "precision": 0.7095709570957096,
            "recall": 0.46436285097192226
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.6745928303851387,
            "auditor_fn_violation": 0.01529728088040876,
            "auditor_fp_violation": 0.030402237779722473,
            "ave_precision_score": 0.6696181069714879,
            "fpr": 0.15021929824561403,
            "logloss": 1.539365560338424,
            "mae": 0.32584038128583726,
            "precision": 0.7226720647773279,
            "recall": 0.7270875763747454
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.6618122336242059,
            "auditor_fn_violation": 0.016697764069104993,
            "auditor_fp_violation": 0.031029480947153836,
            "ave_precision_score": 0.6585037321091087,
            "fpr": 0.14489571899012074,
            "logloss": 1.4071011143546526,
            "mae": 0.3293556188448218,
            "precision": 0.7060133630289532,
            "recall": 0.6846652267818575
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.6764747057153255,
            "auditor_fn_violation": 0.014002036659877799,
            "auditor_fp_violation": 0.02681585198149769,
            "ave_precision_score": 0.6733448006109573,
            "fpr": 0.1206140350877193,
            "logloss": 1.432975093324446,
            "mae": 0.35332977501095203,
            "precision": 0.729064039408867,
            "recall": 0.6028513238289206
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.6633932260982015,
            "auditor_fn_violation": 0.008703321297413667,
            "auditor_fp_violation": 0.030186608122941827,
            "ave_precision_score": 0.6618953588243733,
            "fpr": 0.11855104281009879,
            "logloss": 1.3050465326221818,
            "mae": 0.3474544395708762,
            "precision": 0.7172774869109948,
            "recall": 0.591792656587473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.6643055212154252,
            "auditor_fn_violation": 0.021746703826776723,
            "auditor_fp_violation": 0.03270460890944703,
            "ave_precision_score": 0.6584093033264726,
            "fpr": 0.15789473684210525,
            "logloss": 1.6093539842278488,
            "mae": 0.336183885288408,
            "precision": 0.710261569416499,
            "recall": 0.7189409368635438
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.6461139065952978,
            "auditor_fn_violation": 0.01918713681829713,
            "auditor_fp_violation": 0.02898600439077937,
            "ave_precision_score": 0.6425863949610733,
            "fpr": 0.15148188803512624,
            "logloss": 1.5456211929948296,
            "mae": 0.34062316371888024,
            "precision": 0.695364238410596,
            "recall": 0.6803455723542117
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.6881968975861491,
            "auditor_fn_violation": 0.05470620287990854,
            "auditor_fp_violation": 0.02805298578989041,
            "ave_precision_score": 0.6871984772230312,
            "fpr": 0.07894736842105263,
            "logloss": 0.9961594774460385,
            "mae": 0.41213388898534487,
            "precision": 0.7662337662337663,
            "recall": 0.48065173116089616
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.6557027244433348,
            "auditor_fn_violation": 0.039675859959743286,
            "auditor_fp_violation": 0.03135290889132821,
            "ave_precision_score": 0.6555637255309479,
            "fpr": 0.09220636663007684,
            "logloss": 0.8980742737215726,
            "mae": 0.41318651973325954,
            "precision": 0.7162162162162162,
            "recall": 0.45788336933045354
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 21924,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.672364644037702,
            "auditor_fn_violation": 0.01862471861935899,
            "auditor_fp_violation": 0.031485706546651664,
            "ave_precision_score": 0.6668599490015666,
            "fpr": 0.14583333333333334,
            "logloss": 1.5691209667446904,
            "mae": 0.3311089398561798,
            "precision": 0.7217573221757322,
            "recall": 0.7026476578411406
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.6599967582394906,
            "auditor_fn_violation": 0.014089849760427516,
            "auditor_fp_violation": 0.030421828445977732,
            "ave_precision_score": 0.656590190877995,
            "fpr": 0.14050493962678376,
            "logloss": 1.4603652102167035,
            "mae": 0.3318665182659684,
            "precision": 0.7084282460136674,
            "recall": 0.67170626349892
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6853088089967905,
            "auditor_fn_violation": 0.056329724514953376,
            "auditor_fp_violation": 0.0279462016085344,
            "ave_precision_score": 0.6849475649409231,
            "fpr": 0.07785087719298246,
            "logloss": 0.9984539823364683,
            "mae": 0.4131587124118181,
            "precision": 0.7687296416938111,
            "recall": 0.48065173116089616
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.6547278098722739,
            "auditor_fn_violation": 0.037570561863283655,
            "auditor_fp_violation": 0.031181394072447858,
            "ave_precision_score": 0.6546197196991738,
            "fpr": 0.09001097694840834,
            "logloss": 0.8986868208269807,
            "mae": 0.41457677174620045,
            "precision": 0.7191780821917808,
            "recall": 0.4535637149028078
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 21924,
        "test": {
            "accuracy": 0.643640350877193,
            "auc_prc": 0.6883174515759776,
            "auditor_fn_violation": 0.054554346660949736,
            "auditor_fp_violation": 0.026479872484060512,
            "ave_precision_score": 0.6887705359250483,
            "fpr": 0.07346491228070176,
            "logloss": 0.9551758350122788,
            "mae": 0.4141745650522991,
            "precision": 0.7766666666666666,
            "recall": 0.4745417515274949
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.6585496160075905,
            "auditor_fn_violation": 0.03756107853852481,
            "auditor_fp_violation": 0.032460404578955625,
            "ave_precision_score": 0.6593015323871978,
            "fpr": 0.08562019758507135,
            "logloss": 0.8524570892190618,
            "mae": 0.41433639574567915,
            "precision": 0.7214285714285714,
            "recall": 0.43628509719222464
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.6661935802481322,
            "auditor_fn_violation": 0.010594204452067033,
            "auditor_fp_violation": 0.024560361711880658,
            "ave_precision_score": 0.6611631886963811,
            "fpr": 0.16885964912280702,
            "logloss": 1.5688564475211753,
            "mae": 0.3249891728359248,
            "precision": 0.7083333333333334,
            "recall": 0.7617107942973523
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.6612581838351719,
            "auditor_fn_violation": 0.01301823406268004,
            "auditor_fp_violation": 0.024303649835345782,
            "ave_precision_score": 0.6557671346803698,
            "fpr": 0.15697036223929747,
            "logloss": 1.5075654952008697,
            "mae": 0.3321985866428193,
            "precision": 0.6957446808510638,
            "recall": 0.7062634989200864
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.8221068311602056,
            "auditor_fn_violation": 0.027222460428055897,
            "auditor_fp_violation": 0.007688461057632206,
            "ave_precision_score": 0.8225466036400967,
            "fpr": 0.07017543859649122,
            "logloss": 0.5701371507468843,
            "mae": 0.36657661647462336,
            "precision": 0.8222222222222222,
            "recall": 0.6028513238289206
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7810304948592144,
            "auditor_fn_violation": 0.024464607046584465,
            "auditor_fp_violation": 0.025585110553551827,
            "ave_precision_score": 0.7817476037902671,
            "fpr": 0.0889132821075741,
            "logloss": 0.5863461705467277,
            "mae": 0.36886060979968466,
            "precision": 0.7679083094555874,
            "recall": 0.5788336933045356
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8056880867544202,
            "auditor_fn_violation": 0.03506315432164934,
            "auditor_fp_violation": 0.014491394757678046,
            "ave_precision_score": 0.8062115730076233,
            "fpr": 0.09320175438596491,
            "logloss": 0.5654343316463234,
            "mae": 0.3638590982941024,
            "precision": 0.7946859903381642,
            "recall": 0.670061099796334
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.780764257683444,
            "auditor_fn_violation": 0.025123698117322953,
            "auditor_fp_violation": 0.03303865453975224,
            "ave_precision_score": 0.781414096981325,
            "fpr": 0.1119648737650933,
            "logloss": 0.5782102930651649,
            "mae": 0.36944582643566226,
            "precision": 0.75,
            "recall": 0.6609071274298056
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.6764929221080394,
            "auditor_fn_violation": 0.006005020187944407,
            "auditor_fp_violation": 0.026576238696503728,
            "ave_precision_score": 0.6710128587946718,
            "fpr": 0.19956140350877194,
            "logloss": 1.5275516137772214,
            "mae": 0.3105716001935737,
            "precision": 0.6888888888888889,
            "recall": 0.8207739307535642
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.6561939468568492,
            "auditor_fn_violation": 0.0013442612845637528,
            "auditor_fp_violation": 0.030211110239924732,
            "ave_precision_score": 0.6539158124160296,
            "fpr": 0.18660812294182216,
            "logloss": 1.4244609269370356,
            "mae": 0.3271369603150526,
            "precision": 0.6718146718146718,
            "recall": 0.7516198704103672
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8462352313147787,
            "auditor_fn_violation": 0.014821613606317225,
            "auditor_fp_violation": 0.018236654581822734,
            "ave_precision_score": 0.8464772155764329,
            "fpr": 0.11513157894736842,
            "logloss": 0.6166559851211764,
            "mae": 0.28852377592435613,
            "precision": 0.7746781115879828,
            "recall": 0.7352342158859471
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.833138839336991,
            "auditor_fn_violation": 0.01258200112377399,
            "auditor_fp_violation": 0.012932217343578488,
            "ave_precision_score": 0.8335131495898764,
            "fpr": 0.10208562019758508,
            "logloss": 0.6134296045344606,
            "mae": 0.28542067172199864,
            "precision": 0.7742718446601942,
            "recall": 0.6889848812095032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6381578947368421,
            "auc_prc": 0.6846180121402566,
            "auditor_fn_violation": 0.054223836066745267,
            "auditor_fp_violation": 0.028594720173355005,
            "ave_precision_score": 0.6852253888706196,
            "fpr": 0.08442982456140351,
            "logloss": 0.992544101941581,
            "mae": 0.41241444328854987,
            "precision": 0.7555555555555555,
            "recall": 0.4847250509164969
        },
        "train": {
            "accuracy": 0.6289791437980241,
            "auc_prc": 0.6548648839136397,
            "auditor_fn_violation": 0.0370228998584614,
            "auditor_fp_violation": 0.03502822643876431,
            "ave_precision_score": 0.6548992080223466,
            "fpr": 0.09440175631174534,
            "logloss": 0.8748733458814014,
            "mae": 0.4141599363408514,
            "precision": 0.7104377104377104,
            "recall": 0.4557235421166307
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.861143450464251,
            "auditor_fn_violation": 0.015437971200914712,
            "auditor_fp_violation": 0.0051751260574238464,
            "ave_precision_score": 0.8613331481012177,
            "fpr": 0.0800438596491228,
            "logloss": 0.53475045161482,
            "mae": 0.29699601748539034,
            "precision": 0.8245192307692307,
            "recall": 0.6985743380855397
        },
        "train": {
            "accuracy": 0.7650933040614709,
            "auc_prc": 0.8434102527087279,
            "auditor_fn_violation": 0.013779270874575927,
            "auditor_fp_violation": 0.01051385839736553,
            "ave_precision_score": 0.8437483272854951,
            "fpr": 0.06915477497255763,
            "logloss": 0.5404271186283763,
            "mae": 0.2947327673622756,
            "precision": 0.832,
            "recall": 0.673866090712743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7072368421052632,
            "auc_prc": 0.678240020036829,
            "auditor_fn_violation": 0.0028741022617643913,
            "auditor_fp_violation": 0.026367879318248123,
            "ave_precision_score": 0.6723164277750073,
            "fpr": 0.17653508771929824,
            "logloss": 1.5459741329336432,
            "mae": 0.31299712668194507,
            "precision": 0.7051282051282052,
            "recall": 0.7841140529531568
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.6650730267970046,
            "auditor_fn_violation": 0.0015102194678432367,
            "auditor_fp_violation": 0.03506742982593697,
            "ave_precision_score": 0.6604844601411959,
            "fpr": 0.1668496158068057,
            "logloss": 1.4582875124092884,
            "mae": 0.320111743320397,
            "precision": 0.689795918367347,
            "recall": 0.7300215982721382
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.8167771527433544,
            "auditor_fn_violation": 0.00222871333118948,
            "auditor_fp_violation": 0.01275419844147185,
            "ave_precision_score": 0.8170924097962033,
            "fpr": 0.07785087719298246,
            "logloss": 0.7662413398019791,
            "mae": 0.32801328366123306,
            "precision": 0.8075880758807588,
            "recall": 0.6069246435845214
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.792567362228814,
            "auditor_fn_violation": 0.009654024604486085,
            "auditor_fp_violation": 0.013647679159479377,
            "ave_precision_score": 0.7931347854195829,
            "fpr": 0.08562019758507135,
            "logloss": 0.8190573129187385,
            "mae": 0.3242645166195301,
            "precision": 0.7685459940652819,
            "recall": 0.5593952483801296
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8281085761526307,
            "auditor_fn_violation": 0.01175545789116376,
            "auditor_fp_violation": 0.015913447514272618,
            "ave_precision_score": 0.8283789677022958,
            "fpr": 0.08991228070175439,
            "logloss": 0.7275400925209964,
            "mae": 0.3095985567007766,
            "precision": 0.7944862155388471,
            "recall": 0.6456211812627292
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.8166248448518111,
            "auditor_fn_violation": 0.015104565509622022,
            "auditor_fp_violation": 0.011790418692175005,
            "ave_precision_score": 0.8169729507399812,
            "fpr": 0.08342480790340286,
            "logloss": 0.7515319084087279,
            "mae": 0.3054984767207599,
            "precision": 0.7865168539325843,
            "recall": 0.6047516198704104
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8693820341179498,
            "auditor_fn_violation": 0.006900525243863222,
            "auditor_fp_violation": 0.014952389882068592,
            "ave_precision_score": 0.8695637721969502,
            "fpr": 0.125,
            "logloss": 0.4848672216711846,
            "mae": 0.28919713243975137,
            "precision": 0.7760314341846758,
            "recall": 0.8044806517311609
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8417587648036636,
            "auditor_fn_violation": 0.005597532438897754,
            "auditor_fp_violation": 0.015720558256233343,
            "ave_precision_score": 0.8421140633542724,
            "fpr": 0.11855104281009879,
            "logloss": 0.5080249837185518,
            "mae": 0.2951403288645591,
            "precision": 0.7707006369426752,
            "recall": 0.7840172786177105
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.6715746374709478,
            "auditor_fn_violation": 0.02124870475577947,
            "auditor_fp_violation": 0.03095959911655624,
            "ave_precision_score": 0.6661991269986607,
            "fpr": 0.14364035087719298,
            "logloss": 1.577335014999778,
            "mae": 0.33023436537582473,
            "precision": 0.7253668763102725,
            "recall": 0.7046843177189409
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.6579691227974819,
            "auditor_fn_violation": 0.013354892091618411,
            "auditor_fp_violation": 0.03282793633369923,
            "ave_precision_score": 0.6543843098324953,
            "fpr": 0.145993413830955,
            "logloss": 1.4863174525866036,
            "mae": 0.3339192757627348,
            "precision": 0.6990950226244343,
            "recall": 0.6673866090712743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6901610051620918,
            "auditor_fn_violation": 0.052187176188944875,
            "auditor_fp_violation": 0.02693565862399467,
            "ave_precision_score": 0.6899221552374076,
            "fpr": 0.07236842105263158,
            "logloss": 0.9554313131090902,
            "mae": 0.4148209615229637,
            "precision": 0.7762711864406779,
            "recall": 0.4663951120162933
        },
        "train": {
            "accuracy": 0.6256860592755215,
            "auc_prc": 0.6593051397262202,
            "auditor_fn_violation": 0.038108740543347105,
            "auditor_fp_violation": 0.03198996393288381,
            "ave_precision_score": 0.6599836713138314,
            "fpr": 0.0867178924259056,
            "logloss": 0.8511521769929753,
            "mae": 0.41479022054482634,
            "precision": 0.7178571428571429,
            "recall": 0.43412526997840173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7167850018925435,
            "auditor_fn_violation": 0.040230732125629755,
            "auditor_fp_violation": 0.028607742634495982,
            "ave_precision_score": 0.7163914622942785,
            "fpr": 0.09100877192982457,
            "logloss": 0.9559363644747213,
            "mae": 0.3752175280009907,
            "precision": 0.781578947368421,
            "recall": 0.604887983706721
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6857999523054179,
            "auditor_fn_violation": 0.0315889547716534,
            "auditor_fp_violation": 0.03227908891328211,
            "ave_precision_score": 0.6854962064243928,
            "fpr": 0.09440175631174534,
            "logloss": 0.8845183376133292,
            "mae": 0.38235307339500707,
            "precision": 0.749271137026239,
            "recall": 0.5550755939524838
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6853933747399277,
            "auditor_fn_violation": 0.05369233930038949,
            "auditor_fp_violation": 0.027805559028211864,
            "ave_precision_score": 0.6858219957978331,
            "fpr": 0.08223684210526316,
            "logloss": 0.9486005728090703,
            "mae": 0.4138245111409975,
            "precision": 0.7603833865814696,
            "recall": 0.4847250509164969
        },
        "train": {
            "accuracy": 0.6355653128430296,
            "auc_prc": 0.6555206645907647,
            "auditor_fn_violation": 0.03478246438418847,
            "auditor_fp_violation": 0.0316444840834248,
            "ave_precision_score": 0.6557796071258076,
            "fpr": 0.0889132821075741,
            "logloss": 0.8432844651500655,
            "mae": 0.4155692048530766,
            "precision": 0.7235494880546075,
            "recall": 0.45788336933045354
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 21924,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.6301333101749769,
            "auditor_fn_violation": 0.03997391646121414,
            "auditor_fp_violation": 0.03996593324165521,
            "ave_precision_score": 0.6195044614191896,
            "fpr": 0.13048245614035087,
            "logloss": 1.9923766420670892,
            "mae": 0.41809118817674745,
            "precision": 0.6925064599483204,
            "recall": 0.5458248472505092
        },
        "train": {
            "accuracy": 0.6070252469813392,
            "auc_prc": 0.5969135853059478,
            "auditor_fn_violation": 0.02821526198870063,
            "auditor_fp_violation": 0.042124039517014274,
            "ave_precision_score": 0.5865132482604412,
            "fpr": 0.14818880351262348,
            "logloss": 1.739622567259103,
            "mae": 0.42645328132506816,
            "precision": 0.64,
            "recall": 0.5183585313174947
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.7675651321003873,
            "auditor_fn_violation": 0.012463375853074648,
            "auditor_fp_violation": 0.02225278159769972,
            "ave_precision_score": 0.7681153584894813,
            "fpr": 0.1600877192982456,
            "logloss": 1.0470498983669534,
            "mae": 0.3031389944008943,
            "precision": 0.7142857142857143,
            "recall": 0.7433808553971487
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.7608472657035839,
            "auditor_fn_violation": 0.008978337715419652,
            "auditor_fp_violation": 0.015245217186764937,
            "ave_precision_score": 0.7615892850371493,
            "fpr": 0.14709110867178923,
            "logloss": 1.053951867172787,
            "mae": 0.29693097701169674,
            "precision": 0.7142857142857143,
            "recall": 0.7235421166306696
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.686049426244902,
            "auditor_fn_violation": 0.016768946296494815,
            "auditor_fp_violation": 0.028026940867608457,
            "ave_precision_score": 0.6842259950370826,
            "fpr": 0.12171052631578948,
            "logloss": 1.2843192185185286,
            "mae": 0.343313212613953,
            "precision": 0.7394366197183099,
            "recall": 0.6415478615071283
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.6759741819524678,
            "auditor_fn_violation": 0.014924382339204297,
            "auditor_fp_violation": 0.030769758507135016,
            "ave_precision_score": 0.6749865699234007,
            "fpr": 0.1251372118551043,
            "logloss": 1.1550365183449918,
            "mae": 0.3399680672170451,
            "precision": 0.7099236641221374,
            "recall": 0.6025917926565875
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.7086572499453653,
            "auditor_fn_violation": 0.008090810018937368,
            "auditor_fp_violation": 0.026024086344126358,
            "ave_precision_score": 0.709060779351957,
            "fpr": 0.16557017543859648,
            "logloss": 0.9263923736598709,
            "mae": 0.3494372673786098,
            "precision": 0.7090558766859345,
            "recall": 0.7494908350305499
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.709166702434004,
            "auditor_fn_violation": 0.00882660451927842,
            "auditor_fp_violation": 0.01783754116355653,
            "ave_precision_score": 0.7098066528691641,
            "fpr": 0.15367727771679474,
            "logloss": 0.7938606063950381,
            "mae": 0.34887483914083217,
            "precision": 0.7064989517819706,
            "recall": 0.7278617710583153
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.6855257969154709,
            "auditor_fn_violation": 0.01682477578875907,
            "auditor_fp_violation": 0.03063143309580365,
            "ave_precision_score": 0.679991353411327,
            "fpr": 0.17105263157894737,
            "logloss": 1.4959889001626008,
            "mae": 0.3090315592500917,
            "precision": 0.7148080438756855,
            "recall": 0.7963340122199593
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.664727270262026,
            "auditor_fn_violation": 0.0026363642829539667,
            "auditor_fp_violation": 0.031713090010976955,
            "ave_precision_score": 0.6609636069377086,
            "fpr": 0.15916575192096596,
            "logloss": 1.400105217344435,
            "mae": 0.32023380108259086,
            "precision": 0.7040816326530612,
            "recall": 0.7451403887688985
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.6941289440839414,
            "auditor_fn_violation": 0.01891726515882375,
            "auditor_fp_violation": 0.0312513022461141,
            "ave_precision_score": 0.6923127437291959,
            "fpr": 0.13815789473684212,
            "logloss": 1.253808306320047,
            "mae": 0.3123055857781206,
            "precision": 0.7433808553971487,
            "recall": 0.7433808553971487
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.6851616604948155,
            "auditor_fn_violation": 0.014073253942099562,
            "auditor_fp_violation": 0.037713658460090956,
            "ave_precision_score": 0.6837336755898347,
            "fpr": 0.13721185510428102,
            "logloss": 1.1097456324423378,
            "mae": 0.3178475794163814,
            "precision": 0.7203579418344519,
            "recall": 0.6954643628509719
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.652838499823534,
            "auditor_fn_violation": 0.01032175652981741,
            "auditor_fp_violation": 0.03625974080093345,
            "ave_precision_score": 0.6348153449476713,
            "fpr": 0.1524122807017544,
            "logloss": 2.575511497759493,
            "mae": 0.34936259044395246,
            "precision": 0.7067510548523207,
            "recall": 0.6822810590631364
        },
        "train": {
            "accuracy": 0.6476399560922064,
            "auc_prc": 0.6261537040981275,
            "auditor_fn_violation": 0.007498939053042608,
            "auditor_fp_violation": 0.03925239140661754,
            "ave_precision_score": 0.60413035074372,
            "fpr": 0.16465422612513722,
            "logloss": 2.756865939673399,
            "mae": 0.36048176837158263,
            "precision": 0.6606334841628959,
            "recall": 0.6306695464362851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6892091108365208,
            "auditor_fn_violation": 0.05257351627541358,
            "auditor_fp_violation": 0.02805298578989041,
            "ave_precision_score": 0.6882099097993725,
            "fpr": 0.07894736842105263,
            "logloss": 0.9950601410003432,
            "mae": 0.41095594110236944,
            "precision": 0.7669902912621359,
            "recall": 0.48268839103869654
        },
        "train": {
            "accuracy": 0.6344676180021954,
            "auc_prc": 0.6576124379639252,
            "auditor_fn_violation": 0.03812296553048536,
            "auditor_fp_violation": 0.0314092637603889,
            "ave_precision_score": 0.6575002995054636,
            "fpr": 0.09110867178924259,
            "logloss": 0.8964373394012526,
            "mae": 0.41203278141896743,
            "precision": 0.7195945945945946,
            "recall": 0.46004319654427644
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.687610625173968,
            "auditor_fn_violation": 0.013700557401650767,
            "auditor_fp_violation": 0.029542755344418054,
            "ave_precision_score": 0.6820929459970708,
            "fpr": 0.16776315789473684,
            "logloss": 1.5110874040596602,
            "mae": 0.30619448562113083,
            "precision": 0.7177121771217713,
            "recall": 0.7922606924643585
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.6722458264169895,
            "auditor_fn_violation": 0.0038905339823088617,
            "auditor_fp_violation": 0.028858593382468246,
            "ave_precision_score": 0.6688190609544287,
            "fpr": 0.15697036223929747,
            "logloss": 1.3677402726181571,
            "mae": 0.31223280075460014,
            "precision": 0.7063655030800822,
            "recall": 0.7429805615550756
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.7524718314637993,
            "auditor_fn_violation": 0.020643513059634837,
            "auditor_fp_violation": 0.014626828353544195,
            "ave_precision_score": 0.7531916560746573,
            "fpr": 0.11732456140350878,
            "logloss": 0.9659396104765524,
            "mae": 0.3415782514101789,
            "precision": 0.7390243902439024,
            "recall": 0.6171079429735234
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7581168280451085,
            "auditor_fn_violation": 0.0194906032105796,
            "auditor_fp_violation": 0.010653520464168106,
            "ave_precision_score": 0.75870286548349,
            "fpr": 0.10318331503841932,
            "logloss": 0.9968347373706165,
            "mae": 0.33352175862787387,
            "precision": 0.7466307277628033,
            "recall": 0.5982721382289417
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.6744241980857195,
            "auditor_fn_violation": 0.02290125772680173,
            "auditor_fp_violation": 0.027909738717339663,
            "ave_precision_score": 0.6685066867443427,
            "fpr": 0.14583333333333334,
            "logloss": 1.5690901270651443,
            "mae": 0.3294039515268373,
            "precision": 0.72,
            "recall": 0.6965376782077393
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.6576265734641893,
            "auditor_fn_violation": 0.016285239442096002,
            "auditor_fp_violation": 0.031426415242276934,
            "ave_precision_score": 0.6545376450908028,
            "fpr": 0.1437980241492865,
            "logloss": 1.4759948162903391,
            "mae": 0.3331044366029695,
            "precision": 0.7036199095022625,
            "recall": 0.67170626349892
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6052631578947368,
            "auc_prc": 0.6606932637572297,
            "auditor_fn_violation": 0.055170704255547216,
            "auditor_fp_violation": 0.044768616910447145,
            "ave_precision_score": 0.661747234153168,
            "fpr": 0.14802631578947367,
            "logloss": 0.7251394545622057,
            "mae": 0.4309438912330884,
            "precision": 0.6633416458852868,
            "recall": 0.5417515274949084
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.6208041400385798,
            "auditor_fn_violation": 0.049612013475804476,
            "auditor_fp_violation": 0.0485190920495531,
            "ave_precision_score": 0.6218398605734925,
            "fpr": 0.17233809001097694,
            "logloss": 0.7631317012879116,
            "mae": 0.44017801987359056,
            "precision": 0.5943152454780362,
            "recall": 0.49676025917926564
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.6656326243846127,
            "auditor_fn_violation": 0.00040197234430271194,
            "auditor_fp_violation": 0.02254969371171397,
            "ave_precision_score": 0.6584460101592982,
            "fpr": 0.18640350877192982,
            "logloss": 1.712767857298099,
            "mae": 0.3253233271151618,
            "precision": 0.6892138939670932,
            "recall": 0.7678207739307535
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.6545830417319676,
            "auditor_fn_violation": 0.0034068844196086735,
            "auditor_fp_violation": 0.02930943233495375,
            "ave_precision_score": 0.6471626246635229,
            "fpr": 0.16575192096597147,
            "logloss": 1.656059137302224,
            "mae": 0.3325801763726965,
            "precision": 0.6847599164926931,
            "recall": 0.7084233261339092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7227127999332605,
            "auditor_fn_violation": 0.027398881623610965,
            "auditor_fp_violation": 0.022195482768679418,
            "ave_precision_score": 0.7207521616712731,
            "fpr": 0.13706140350877194,
            "logloss": 1.0506517575169574,
            "mae": 0.34730603105261953,
            "precision": 0.7412008281573499,
            "recall": 0.7291242362525459
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.6894701821929239,
            "auditor_fn_violation": 0.02408764488742109,
            "auditor_fp_violation": 0.01713188019444881,
            "ave_precision_score": 0.6877300291713165,
            "fpr": 0.14928649835345773,
            "logloss": 0.9641120818556167,
            "mae": 0.3614823580173509,
            "precision": 0.7043478260869566,
            "recall": 0.6997840172786177
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6831140350877193,
            "auc_prc": 0.6656593188844383,
            "auditor_fn_violation": 0.02246355450744989,
            "auditor_fp_violation": 0.031082010251281416,
            "ave_precision_score": 0.6607912694827771,
            "fpr": 0.14583333333333334,
            "logloss": 1.5722402395783814,
            "mae": 0.3428835083095013,
            "precision": 0.7158119658119658,
            "recall": 0.6822810590631364
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.653124781000023,
            "auditor_fn_violation": 0.018985616167172044,
            "auditor_fp_violation": 0.03081876274110083,
            "ave_precision_score": 0.6503445988526896,
            "fpr": 0.1394072447859495,
            "logloss": 1.465456271612323,
            "mae": 0.3412095282600492,
            "precision": 0.7066974595842956,
            "recall": 0.6609071274298056
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.6744314106703537,
            "auditor_fn_violation": 0.02155018401400651,
            "auditor_fp_violation": 0.02756334125098971,
            "ave_precision_score": 0.6686771973274495,
            "fpr": 0.1425438596491228,
            "logloss": 1.5546765829946558,
            "mae": 0.3307960409466719,
            "precision": 0.7239915074309978,
            "recall": 0.6945010183299389
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.6573839191263635,
            "auditor_fn_violation": 0.01738056345174055,
            "auditor_fp_violation": 0.030725654696565786,
            "ave_precision_score": 0.6543940432933353,
            "fpr": 0.14270032930845225,
            "logloss": 1.4834378800854342,
            "mae": 0.3337592702932674,
            "precision": 0.7038724373576309,
            "recall": 0.6673866090712743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.6967423888132624,
            "auditor_fn_violation": 0.011489709507985852,
            "auditor_fp_violation": 0.024589011126390805,
            "ave_precision_score": 0.6950387737862294,
            "fpr": 0.14144736842105263,
            "logloss": 1.2167254214427106,
            "mae": 0.31757773354215996,
            "precision": 0.7372708757637475,
            "recall": 0.7372708757637475
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.6874845867772267,
            "auditor_fn_violation": 0.016247306143060705,
            "auditor_fp_violation": 0.02752812843029638,
            "ave_precision_score": 0.6864806861523193,
            "fpr": 0.13721185510428102,
            "logloss": 1.0966408480174434,
            "mae": 0.32270444931994513,
            "precision": 0.7191011235955056,
            "recall": 0.6911447084233261
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6853103860051192,
            "auditor_fn_violation": 0.056329724514953376,
            "auditor_fp_violation": 0.0279462016085344,
            "ave_precision_score": 0.6849489846265395,
            "fpr": 0.07785087719298246,
            "logloss": 0.9985183778235248,
            "mae": 0.413267185948299,
            "precision": 0.7687296416938111,
            "recall": 0.48065173116089616
        },
        "train": {
            "accuracy": 0.6311745334796927,
            "auc_prc": 0.654693270914259,
            "auditor_fn_violation": 0.035922834186437436,
            "auditor_fp_violation": 0.031181394072447858,
            "ave_precision_score": 0.6545853224115361,
            "fpr": 0.09001097694840834,
            "logloss": 0.8987580346219997,
            "mae": 0.4146339893465867,
            "precision": 0.718213058419244,
            "recall": 0.4514038876889849
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6546052631578947,
            "auc_prc": 0.6654177257665794,
            "auditor_fn_violation": 0.02079536927859364,
            "auditor_fp_violation": 0.03180605909071968,
            "ave_precision_score": 0.6610669043145672,
            "fpr": 0.11842105263157894,
            "logloss": 1.520931891231272,
            "mae": 0.3650292252723136,
            "precision": 0.7244897959183674,
            "recall": 0.5784114052953157
        },
        "train": {
            "accuracy": 0.6597145993413831,
            "auc_prc": 0.6409410676171762,
            "auditor_fn_violation": 0.017738558961386278,
            "auditor_fp_violation": 0.039401854320213266,
            "ave_precision_score": 0.6391899972671827,
            "fpr": 0.12403951701427003,
            "logloss": 1.3736017899803294,
            "mae": 0.36588207301672065,
            "precision": 0.7018469656992085,
            "recall": 0.5745140388768899
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6984649122807017,
            "auc_prc": 0.7234534980482534,
            "auditor_fn_violation": 0.026447547075427874,
            "auditor_fp_violation": 0.023555027711797308,
            "ave_precision_score": 0.7223139667239963,
            "fpr": 0.09649122807017543,
            "logloss": 0.9825414010479122,
            "mae": 0.3600520318192266,
            "precision": 0.7755102040816326,
            "recall": 0.6191446028513238
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.6946644162497522,
            "auditor_fn_violation": 0.019433703262026633,
            "auditor_fp_violation": 0.028179884742041715,
            "ave_precision_score": 0.6942781669528453,
            "fpr": 0.10208562019758508,
            "logloss": 0.9191626833760821,
            "mae": 0.366908431949188,
            "precision": 0.7465940054495913,
            "recall": 0.591792656587473
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6392543859649122,
            "auc_prc": 0.6957733001288551,
            "auditor_fn_violation": 0.04087835423589524,
            "auditor_fp_violation": 0.02728205609034463,
            "ave_precision_score": 0.6964245267945077,
            "fpr": 0.1162280701754386,
            "logloss": 0.8209294763248282,
            "mae": 0.40426970639964566,
            "precision": 0.7165775401069518,
            "recall": 0.5458248472505092
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.6925694782885772,
            "auditor_fn_violation": 0.037224420509586464,
            "auditor_fp_violation": 0.023492629763211544,
            "ave_precision_score": 0.6932683039790786,
            "fpr": 0.09879253567508232,
            "logloss": 0.7842052745265174,
            "mae": 0.391272926509005,
            "precision": 0.7398843930635838,
            "recall": 0.5529157667386609
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.690292134320351,
            "auditor_fn_violation": 0.05377496694894059,
            "auditor_fp_violation": 0.02831603950493812,
            "ave_precision_score": 0.6892928956785438,
            "fpr": 0.08223684210526316,
            "logloss": 0.999260608747327,
            "mae": 0.4100080675321933,
            "precision": 0.7619047619047619,
            "recall": 0.48879837067209775
        },
        "train": {
            "accuracy": 0.6388583973655324,
            "auc_prc": 0.6609538364668222,
            "auditor_fn_violation": 0.04130936264945129,
            "auditor_fp_violation": 0.03557217343578485,
            "ave_precision_score": 0.6608090503829116,
            "fpr": 0.09001097694840834,
            "logloss": 0.8978645554755728,
            "mae": 0.4094867328041881,
            "precision": 0.7248322147651006,
            "recall": 0.46652267818574517
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.6745682438776555,
            "auditor_fn_violation": 0.0011121234859041702,
            "auditor_fp_violation": 0.024698399799974995,
            "ave_precision_score": 0.6686487798343801,
            "fpr": 0.17105263157894737,
            "logloss": 1.5568711911741884,
            "mae": 0.3209456859920728,
            "precision": 0.7062146892655368,
            "recall": 0.7637474541751528
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.6590721743149399,
            "auditor_fn_violation": 0.001979644043405184,
            "auditor_fp_violation": 0.030960875019601694,
            "ave_precision_score": 0.6555833582804105,
            "fpr": 0.16245883644346873,
            "logloss": 1.4687778408231211,
            "mae": 0.3273447967194686,
            "precision": 0.6871035940803383,
            "recall": 0.7019438444924406
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6853114326887607,
            "auditor_fn_violation": 0.056329724514953376,
            "auditor_fp_violation": 0.0279462016085344,
            "ave_precision_score": 0.6849501873121322,
            "fpr": 0.07785087719298246,
            "logloss": 0.9984521218549985,
            "mae": 0.41316013563559145,
            "precision": 0.7687296416938111,
            "recall": 0.48065173116089616
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.6547284344003658,
            "auditor_fn_violation": 0.037570561863283655,
            "auditor_fp_violation": 0.031181394072447858,
            "ave_precision_score": 0.6546203437240695,
            "fpr": 0.09001097694840834,
            "logloss": 0.8986865112965101,
            "mae": 0.41457787295317944,
            "precision": 0.7191780821917808,
            "recall": 0.4535637149028078
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.6704313171323191,
            "auditor_fn_violation": 0.00452442205309608,
            "auditor_fp_violation": 0.030902300287535944,
            "ave_precision_score": 0.6632442259785756,
            "fpr": 0.2149122807017544,
            "logloss": 1.7250608521872735,
            "mae": 0.31212649997430336,
            "precision": 0.6771004942339374,
            "recall": 0.8370672097759674
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.6585038567879182,
            "auditor_fn_violation": 0.00967536208519345,
            "auditor_fp_violation": 0.03303375411635565,
            "ave_precision_score": 0.6505120832598198,
            "fpr": 0.2074643249176729,
            "logloss": 1.676650381864903,
            "mae": 0.3284575606179253,
            "precision": 0.6588447653429603,
            "recall": 0.7883369330453563
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6381578947368421,
            "auc_prc": 0.6849971495191745,
            "auditor_fn_violation": 0.05521313466966809,
            "auditor_fp_violation": 0.03096480810101263,
            "ave_precision_score": 0.6840039283926065,
            "fpr": 0.08114035087719298,
            "logloss": 1.0088014651821902,
            "mae": 0.41201986699097526,
            "precision": 0.7605177993527508,
            "recall": 0.4786150712830957
        },
        "train": {
            "accuracy": 0.6355653128430296,
            "auc_prc": 0.6520320364598651,
            "auditor_fn_violation": 0.033777231959752775,
            "auditor_fp_violation": 0.03214677748157441,
            "ave_precision_score": 0.6517489449911059,
            "fpr": 0.0889132821075741,
            "logloss": 0.9524616544140566,
            "mae": 0.4137686979862248,
            "precision": 0.7235494880546075,
            "recall": 0.45788336933045354
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6838694254191685,
            "auditor_fn_violation": 0.05683442312502233,
            "auditor_fp_violation": 0.027305496520398383,
            "ave_precision_score": 0.6835083353890992,
            "fpr": 0.07675438596491228,
            "logloss": 1.0040476875461377,
            "mae": 0.4145942606586399,
            "precision": 0.7704918032786885,
            "recall": 0.4786150712830957
        },
        "train": {
            "accuracy": 0.6300768386388584,
            "auc_prc": 0.652761805141551,
            "auditor_fn_violation": 0.03641122541151703,
            "auditor_fp_violation": 0.030546789242590557,
            "ave_precision_score": 0.6526574097125359,
            "fpr": 0.0889132821075741,
            "logloss": 0.9045382850899819,
            "mae": 0.4157668333409935,
            "precision": 0.71875,
            "recall": 0.4470842332613391
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.6853134966508899,
            "auditor_fn_violation": 0.056329724514953376,
            "auditor_fp_violation": 0.0279462016085344,
            "ave_precision_score": 0.684952248613916,
            "fpr": 0.07785087719298246,
            "logloss": 0.9984460970340518,
            "mae": 0.41315726150417487,
            "precision": 0.7687296416938111,
            "recall": 0.48065173116089616
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.6547476798686126,
            "auditor_fn_violation": 0.037570561863283655,
            "auditor_fp_violation": 0.031181394072447858,
            "ave_precision_score": 0.6546395738989224,
            "fpr": 0.09001097694840834,
            "logloss": 0.898679640798071,
            "mae": 0.4145701390668916,
            "precision": 0.7191780821917808,
            "recall": 0.4535637149028078
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.6869119866955089,
            "auditor_fn_violation": 0.05540742130274771,
            "auditor_fp_violation": 0.027123182064424718,
            "ave_precision_score": 0.6859147941501026,
            "fpr": 0.07675438596491228,
            "logloss": 0.9982824930548295,
            "mae": 0.41404783554638636,
            "precision": 0.7741935483870968,
            "recall": 0.48879837067209775
        },
        "train": {
            "accuracy": 0.6333699231613611,
            "auc_prc": 0.6554230943928061,
            "auditor_fn_violation": 0.037020529027271674,
            "auditor_fp_violation": 0.03210757409440176,
            "ave_precision_score": 0.6554576426327399,
            "fpr": 0.08781558726673985,
            "logloss": 0.878415953012972,
            "mae": 0.4150620694617849,
            "precision": 0.7231833910034602,
            "recall": 0.4514038876889849
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.726290512706675,
            "auditor_fn_violation": 0.020147747168328158,
            "auditor_fp_violation": 0.029915197733049965,
            "ave_precision_score": 0.7250764117789268,
            "fpr": 0.10635964912280702,
            "logloss": 0.9793448571091057,
            "mae": 0.3531315851024262,
            "precision": 0.7733644859813084,
            "recall": 0.6741344195519349
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.6963224859320029,
            "auditor_fn_violation": 0.01592961476363999,
            "auditor_fp_violation": 0.0318331503841932,
            "ave_precision_score": 0.6952585675556795,
            "fpr": 0.12294182217343579,
            "logloss": 0.9107360164970041,
            "mae": 0.3660618230451446,
            "precision": 0.7307692307692307,
            "recall": 0.6565874730021598
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6392543859649122,
            "auc_prc": 0.6899657105659378,
            "auditor_fn_violation": 0.053279201057633924,
            "auditor_fp_violation": 0.02693565862399467,
            "ave_precision_score": 0.6897245604283226,
            "fpr": 0.07236842105263158,
            "logloss": 0.9544451046078586,
            "mae": 0.4149677593208923,
            "precision": 0.7755102040816326,
            "recall": 0.46435845213849286
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6593266632998936,
            "auditor_fn_violation": 0.038108740543347105,
            "auditor_fp_violation": 0.03155382625058805,
            "ave_precision_score": 0.6600049998794777,
            "fpr": 0.08562019758507135,
            "logloss": 0.8505462605593934,
            "mae": 0.41483727474932347,
            "precision": 0.7204301075268817,
            "recall": 0.43412526997840173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7864431936964293,
            "auditor_fn_violation": 0.010373119662700547,
            "auditor_fp_violation": 0.01857002958703171,
            "ave_precision_score": 0.788040946739871,
            "fpr": 0.08771929824561403,
            "logloss": 0.7283291537842755,
            "mae": 0.3340319541668594,
            "precision": 0.786096256684492,
            "recall": 0.5987780040733197
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.783704029710494,
            "auditor_fn_violation": 0.01773144646781715,
            "auditor_fp_violation": 0.01888378155872667,
            "ave_precision_score": 0.7845947681108898,
            "fpr": 0.0801317233809001,
            "logloss": 0.7416083582179239,
            "mae": 0.3179712089625786,
            "precision": 0.7890173410404624,
            "recall": 0.5896328293736501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 21924,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8386810964319646,
            "auditor_fn_violation": 0.01293234358809448,
            "auditor_fp_violation": 0.018010063757969746,
            "ave_precision_score": 0.838902718447793,
            "fpr": 0.13048245614035087,
            "logloss": 1.0413012195918512,
            "mae": 0.2845001136892888,
            "precision": 0.7478813559322034,
            "recall": 0.7189409368635438
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8163780241762391,
            "auditor_fn_violation": 0.009390862342428632,
            "auditor_fp_violation": 0.018408440489258278,
            "ave_precision_score": 0.8168177237663963,
            "fpr": 0.11964873765093303,
            "logloss": 1.0691786221638868,
            "mae": 0.2779321591905653,
            "precision": 0.7505720823798627,
            "recall": 0.7084233261339092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 21924,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.6814715200240248,
            "auditor_fn_violation": 0.02153008539679138,
            "auditor_fp_violation": 0.023388340209192823,
            "ave_precision_score": 0.6820331884048416,
            "fpr": 0.09758771929824561,
            "logloss": 1.0074870642518525,
            "mae": 0.39931238932382385,
            "precision": 0.7319277108433735,
            "recall": 0.49490835030549896
        },
        "train": {
            "accuracy": 0.6432491767288694,
            "auc_prc": 0.6747457232854164,
            "auditor_fn_violation": 0.03245430815589638,
            "auditor_fp_violation": 0.02477654069311589,
            "ave_precision_score": 0.6754704863692385,
            "fpr": 0.09659714599341383,
            "logloss": 0.9630466369376136,
            "mae": 0.38227223702101926,
            "precision": 0.7197452229299363,
            "recall": 0.48812095032397407
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(0)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8475123430376934,
            "auditor_fn_violation": 0.024262401383335137,
            "auditor_fp_violation": 0.02668214654282767,
            "ave_precision_score": 0.8478452442818551,
            "fpr": 0.12828947368421054,
            "logloss": 0.7375303411216075,
            "mae": 0.26160944839062705,
            "precision": 0.7650602409638554,
            "recall": 0.7823408624229979
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8396770412798783,
            "auditor_fn_violation": 0.00718790326182255,
            "auditor_fp_violation": 0.0252420367678326,
            "ave_precision_score": 0.839951135971503,
            "fpr": 0.13062568605927552,
            "logloss": 0.7550885490236843,
            "mae": 0.25989140626724466,
            "precision": 0.7541322314049587,
            "recall": 0.7815845824411135
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(1)",
        "seed": 5740,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8329044535329302,
            "auditor_fn_violation": 0.014842033214452974,
            "auditor_fp_violation": 0.01776315789473685,
            "ave_precision_score": 0.8332136925706969,
            "fpr": 0.13048245614035087,
            "logloss": 0.806835122489863,
            "mae": 0.27501154881160444,
            "precision": 0.7576374745417516,
            "recall": 0.7638603696098563
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.81896333532462,
            "auditor_fn_violation": 0.010485688832894182,
            "auditor_fp_violation": 0.017689204022903263,
            "ave_precision_score": 0.8193306510907734,
            "fpr": 0.141602634467618,
            "logloss": 0.8375452073686365,
            "mae": 0.2733870466974819,
            "precision": 0.7378048780487805,
            "recall": 0.7773019271948608
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(2)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8294799093938707,
            "auditor_fn_violation": 0.0223576137468929,
            "auditor_fp_violation": 0.031535087719298256,
            "ave_precision_score": 0.8295134503815926,
            "fpr": 0.16776315789473684,
            "logloss": 2.1345914846792904,
            "mae": 0.28333491960840046,
            "precision": 0.720292504570384,
            "recall": 0.8090349075975359
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8327470640684513,
            "auditor_fn_violation": 0.020717521043068655,
            "auditor_fp_violation": 0.03245369408925941,
            "ave_precision_score": 0.8323684299283582,
            "fpr": 0.17014270032930845,
            "logloss": 1.5763549020414536,
            "mae": 0.28154001263722006,
            "precision": 0.7113594040968343,
            "recall": 0.8179871520342612
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(3)",
        "seed": 5740,
        "test": {
            "accuracy": 0.793859649122807,
            "auc_prc": 0.8826586733095564,
            "auditor_fn_violation": 0.014560592960841534,
            "auditor_fp_violation": 0.01068369453044376,
            "ave_precision_score": 0.8822770830744419,
            "fpr": 0.08552631578947369,
            "logloss": 0.8888710637785544,
            "mae": 0.2161725687174252,
            "precision": 0.8285714285714286,
            "recall": 0.7741273100616016
        },
        "train": {
            "accuracy": 0.7903402854006586,
            "auc_prc": 0.8841166478520448,
            "auditor_fn_violation": 0.008231536044114645,
            "auditor_fp_violation": 0.008583775872469623,
            "ave_precision_score": 0.8842719157952512,
            "fpr": 0.09330406147091108,
            "logloss": 0.8739626799531821,
            "mae": 0.2168830607691531,
            "precision": 0.8094170403587444,
            "recall": 0.7730192719486081
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(4)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.805089537670157,
            "auditor_fn_violation": 0.023546417378147635,
            "auditor_fp_violation": 0.022247162022703833,
            "ave_precision_score": 0.8057323391712948,
            "fpr": 0.22916666666666666,
            "logloss": 0.8792056386982378,
            "mae": 0.3147889464900088,
            "precision": 0.6677265500794912,
            "recall": 0.8624229979466119
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.7745336647749613,
            "auditor_fn_violation": 0.016672268749544587,
            "auditor_fp_violation": 0.013985720077926441,
            "ave_precision_score": 0.7751484480645656,
            "fpr": 0.23819978046103182,
            "logloss": 0.9203342328529677,
            "mae": 0.31974647812335416,
            "precision": 0.6494345718901454,
            "recall": 0.860813704496788
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(5)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.6727991742213129,
            "auditor_fn_violation": 0.01974584819337873,
            "auditor_fp_violation": 0.011981424148606815,
            "ave_precision_score": 0.674963065875639,
            "fpr": 0.23793859649122806,
            "logloss": 1.3140709985473173,
            "mae": 0.34210240462118446,
            "precision": 0.6544585987261147,
            "recall": 0.8439425051334702
        },
        "train": {
            "accuracy": 0.6575192096597146,
            "auc_prc": 0.6592704202385335,
            "auditor_fn_violation": 0.01615045235839854,
            "auditor_fp_violation": 0.009256237576764468,
            "ave_precision_score": 0.659906920869236,
            "fpr": 0.26125137211855104,
            "logloss": 1.4052072415815433,
            "mae": 0.36203826736059497,
            "precision": 0.6228209191759112,
            "recall": 0.841541755888651
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(6)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7916666666666666,
            "auc_prc": 0.8815078882797496,
            "auditor_fn_violation": 0.01884749090385101,
            "auditor_fp_violation": 0.017430340557275544,
            "ave_precision_score": 0.8802803725111437,
            "fpr": 0.09978070175438597,
            "logloss": 0.983109538301241,
            "mae": 0.21403283995679045,
            "precision": 0.8100208768267223,
            "recall": 0.7967145790554415
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8802588600329324,
            "auditor_fn_violation": 0.011881900257852516,
            "auditor_fp_violation": 0.014880687493201213,
            "ave_precision_score": 0.8803847123175621,
            "fpr": 0.11306256860592755,
            "logloss": 0.9503343250029254,
            "mae": 0.22276262139645242,
            "precision": 0.783157894736842,
            "recall": 0.7965738758029979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(7)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6381578947368421,
            "auc_prc": 0.6552820397337247,
            "auditor_fn_violation": 0.04848427537015024,
            "auditor_fp_violation": 0.061744066047471624,
            "ave_precision_score": 0.649316508938961,
            "fpr": 0.14583333333333334,
            "logloss": 1.9564417724555214,
            "mae": 0.36518364653858154,
            "precision": 0.6855791962174941,
            "recall": 0.5954825462012321
        },
        "train": {
            "accuracy": 0.6421514818880352,
            "auc_prc": 0.615535409147335,
            "auditor_fn_violation": 0.04186988907875902,
            "auditor_fp_violation": 0.06913005211578209,
            "ave_precision_score": 0.6057128513267973,
            "fpr": 0.15697036223929747,
            "logloss": 2.43732222559613,
            "mae": 0.3674318953167106,
            "precision": 0.6651053864168618,
            "recall": 0.6081370449678801
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(8)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8639009626931078,
            "auditor_fn_violation": 0.026763842357433627,
            "auditor_fp_violation": 0.026612487100103197,
            "ave_precision_score": 0.8642831034543893,
            "fpr": 0.1600877192982456,
            "logloss": 0.7835100428624329,
            "mae": 0.2506436991158958,
            "precision": 0.738819320214669,
            "recall": 0.8480492813141683
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8578188037027299,
            "auditor_fn_violation": 0.012201571560536577,
            "auditor_fp_violation": 0.03139802810494359,
            "ave_precision_score": 0.8580092544016169,
            "fpr": 0.1525795828759605,
            "logloss": 0.8118078532838885,
            "mae": 0.24722068008721254,
            "precision": 0.7406716417910447,
            "recall": 0.8501070663811563
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(9)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8417557448208164,
            "auditor_fn_violation": 0.011320652761266618,
            "auditor_fp_violation": 0.01665376676986584,
            "ave_precision_score": 0.8421402685975746,
            "fpr": 0.12828947368421054,
            "logloss": 0.6526292776889411,
            "mae": 0.271439437507583,
            "precision": 0.7641129032258065,
            "recall": 0.7782340862422998
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8315495176326497,
            "auditor_fn_violation": 0.006449838636507876,
            "auditor_fp_violation": 0.02075236597739343,
            "ave_precision_score": 0.8319040354988486,
            "fpr": 0.13830954994511527,
            "logloss": 0.6690133501507576,
            "mae": 0.2689112766662398,
            "precision": 0.7428571428571429,
            "recall": 0.7794432548179872
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(10)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8355188977296428,
            "auditor_fn_violation": 0.027984167297092837,
            "auditor_fp_violation": 0.029788441692466465,
            "ave_precision_score": 0.8361423449882945,
            "fpr": 0.14692982456140352,
            "logloss": 0.8054116453091746,
            "mae": 0.2679010043885996,
            "precision": 0.7452471482889734,
            "recall": 0.8049281314168378
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.831493699992325,
            "auditor_fn_violation": 0.02063760321739764,
            "auditor_fp_violation": 0.03109146468092681,
            "ave_precision_score": 0.8318389828944638,
            "fpr": 0.14489571899012074,
            "logloss": 0.7751623191724166,
            "mae": 0.26885676395436947,
            "precision": 0.7386138613861386,
            "recall": 0.7987152034261242
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(11)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8784641650128344,
            "auditor_fn_violation": 0.010665459850859187,
            "auditor_fp_violation": 0.003808049535603716,
            "ave_precision_score": 0.8780998873676096,
            "fpr": 0.07017543859649122,
            "logloss": 0.8753251795105675,
            "mae": 0.23275813012048002,
            "precision": 0.8407960199004975,
            "recall": 0.6940451745379876
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8818428673160117,
            "auditor_fn_violation": 0.009155292087900212,
            "auditor_fp_violation": 0.0046429525024475615,
            "ave_precision_score": 0.8820218382122348,
            "fpr": 0.06915477497255763,
            "logloss": 0.85871734031973,
            "mae": 0.219264646958373,
            "precision": 0.8405063291139241,
            "recall": 0.7109207708779444
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(12)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.8016157127878976,
            "auditor_fn_violation": 0.013882884830145178,
            "auditor_fp_violation": 0.014259545923632613,
            "ave_precision_score": 0.8019647867260811,
            "fpr": 0.19298245614035087,
            "logloss": 0.8881549242763039,
            "mae": 0.30387733919872734,
            "precision": 0.6912280701754386,
            "recall": 0.8090349075975359
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7814208534928244,
            "auditor_fn_violation": 0.009496118109144245,
            "auditor_fp_violation": 0.024920639629750503,
            "ave_precision_score": 0.7820606506843615,
            "fpr": 0.19758507135016465,
            "logloss": 0.9414525236778334,
            "mae": 0.29880310662413234,
            "precision": 0.6825396825396826,
            "recall": 0.828693790149893
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(13)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6162280701754386,
            "auc_prc": 0.5799895417606237,
            "auditor_fn_violation": 0.0134798623869736,
            "auditor_fp_violation": 0.01293859649122808,
            "ave_precision_score": 0.5813643154616163,
            "fpr": 0.18640350877192982,
            "logloss": 0.9041640460097868,
            "mae": 0.4369434129103115,
            "precision": 0.6436058700209644,
            "recall": 0.6303901437371663
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.5586538553056946,
            "auditor_fn_violation": 0.008161020315581394,
            "auditor_fp_violation": 0.010986837551052705,
            "ave_precision_score": 0.560042422251352,
            "fpr": 0.19319429198682767,
            "logloss": 0.9300322466465736,
            "mae": 0.4370236818643344,
            "precision": 0.6239316239316239,
            "recall": 0.6252676659528907
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(14)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.768115049026711,
            "auditor_fn_violation": 0.014407489462876906,
            "auditor_fp_violation": 0.016674406604747165,
            "ave_precision_score": 0.7685743587983447,
            "fpr": 0.20942982456140352,
            "logloss": 0.9270297474037386,
            "mae": 0.3229126693654916,
            "precision": 0.6701208981001727,
            "recall": 0.7967145790554415
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.745079408394866,
            "auditor_fn_violation": 0.006094909469557186,
            "auditor_fp_violation": 0.027538790162280835,
            "ave_precision_score": 0.7466804835095662,
            "fpr": 0.20087815587266739,
            "logloss": 0.9812025874166711,
            "mae": 0.31923800640292777,
            "precision": 0.6737967914438503,
            "recall": 0.8094218415417559
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(15)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8305922839841511,
            "auditor_fn_violation": 0.02501891278504269,
            "auditor_fp_violation": 0.024963880288957695,
            "ave_precision_score": 0.8310040777406762,
            "fpr": 0.12280701754385964,
            "logloss": 0.8099555060591722,
            "mae": 0.2708802711136355,
            "precision": 0.7666666666666667,
            "recall": 0.75564681724846
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8200239345632515,
            "auditor_fn_violation": 0.021514348775494376,
            "auditor_fp_violation": 0.0269578030280555,
            "ave_precision_score": 0.8203961903465982,
            "fpr": 0.13721185510428102,
            "logloss": 0.8294690832646583,
            "mae": 0.2732259608696869,
            "precision": 0.7395833333333334,
            "recall": 0.7601713062098501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(16)",
        "seed": 5740,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7652285282279286,
            "auditor_fn_violation": 0.014369213588385749,
            "auditor_fp_violation": 0.020650154798761613,
            "ave_precision_score": 0.7658842046175652,
            "fpr": 0.19298245614035087,
            "logloss": 0.9935785152321821,
            "mae": 0.31927912900250865,
            "precision": 0.6788321167883211,
            "recall": 0.7638603696098563
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7548859924972591,
            "auditor_fn_violation": 0.006518003840756684,
            "auditor_fp_violation": 0.021380326539492293,
            "ave_precision_score": 0.7554015027325471,
            "fpr": 0.19319429198682767,
            "logloss": 1.0570541422511712,
            "mae": 0.3179361191420205,
            "precision": 0.6764705882352942,
            "recall": 0.7880085653104925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(17)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6381578947368421,
            "auc_prc": 0.8384886634834989,
            "auditor_fn_violation": 0.01002827911668288,
            "auditor_fp_violation": 0.022610939112487103,
            "ave_precision_score": 0.8372814571028155,
            "fpr": 0.33771929824561403,
            "logloss": 2.0671881536438166,
            "mae": 0.3538932755309446,
            "precision": 0.6015523932729625,
            "recall": 0.9548254620123203
        },
        "train": {
            "accuracy": 0.6344676180021954,
            "auc_prc": 0.8305014795569776,
            "auditor_fn_violation": 0.006261796693752543,
            "auditor_fp_violation": 0.023939142215761326,
            "ave_precision_score": 0.8292857074709863,
            "fpr": 0.33699231613611413,
            "logloss": 2.0902180288899954,
            "mae": 0.35981643637637417,
            "precision": 0.589572192513369,
            "recall": 0.9443254817987152
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(18)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7905701754385965,
            "auc_prc": 0.8840303056499541,
            "auditor_fn_violation": 0.02043931697827732,
            "auditor_fp_violation": 0.009584623323013417,
            "ave_precision_score": 0.8835335762229828,
            "fpr": 0.07346491228070176,
            "logloss": 0.9458351162321897,
            "mae": 0.21609798936063496,
            "precision": 0.8441860465116279,
            "recall": 0.7453798767967146
        },
        "train": {
            "accuracy": 0.7936333699231614,
            "auc_prc": 0.8859542354096979,
            "auditor_fn_violation": 0.010774803319880502,
            "auditor_fp_violation": 0.006586169045005494,
            "ave_precision_score": 0.8860924546884352,
            "fpr": 0.08122941822173436,
            "logloss": 0.9212778770030164,
            "mae": 0.21497479367971054,
            "precision": 0.8266978922716628,
            "recall": 0.7558886509635975
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(19)",
        "seed": 5740,
        "test": {
            "accuracy": 0.42872807017543857,
            "auc_prc": 0.43689777273928865,
            "auditor_fn_violation": 0.01996199430815233,
            "auditor_fp_violation": 0.01805985552115583,
            "ave_precision_score": 0.437062129589019,
            "fpr": 0.2741228070175439,
            "logloss": 2.8430333180640006,
            "mae": 0.5739399397915111,
            "precision": 0.463519313304721,
            "recall": 0.44353182751540043
        },
        "train": {
            "accuracy": 0.41931942919868276,
            "auc_prc": 0.41526315587574847,
            "auditor_fn_violation": 0.01111797986540899,
            "auditor_fp_violation": 0.0226115248069145,
            "ave_precision_score": 0.41646461057089174,
            "fpr": 0.278814489571899,
            "logloss": 3.101723043618141,
            "mae": 0.5802732131854329,
            "precision": 0.4304932735426009,
            "recall": 0.41113490364025695
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(20)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8344654518929313,
            "auditor_fn_violation": 0.028567311502575746,
            "auditor_fp_violation": 0.02972394220846234,
            "ave_precision_score": 0.8349326842851457,
            "fpr": 0.13815789473684212,
            "logloss": 0.9062834106188322,
            "mae": 0.26592268901621285,
            "precision": 0.7519685039370079,
            "recall": 0.784394250513347
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8219463152398707,
            "auditor_fn_violation": 0.017666540521863405,
            "auditor_fp_violation": 0.029773736414790204,
            "ave_precision_score": 0.8223405877128189,
            "fpr": 0.1437980241492865,
            "logloss": 0.910164274936309,
            "mae": 0.26541575083744345,
            "precision": 0.7369477911646586,
            "recall": 0.7858672376873662
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(21)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.841886845164576,
            "auditor_fn_violation": 0.01644961994308153,
            "auditor_fp_violation": 0.015802373581011355,
            "ave_precision_score": 0.8422553863278449,
            "fpr": 0.17105263157894737,
            "logloss": 0.7549906030000146,
            "mae": 0.2743474170862541,
            "precision": 0.717391304347826,
            "recall": 0.813141683778234
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8289126669291399,
            "auditor_fn_violation": 0.01172676565507937,
            "auditor_fp_violation": 0.026972636742120828,
            "ave_precision_score": 0.8292547437430251,
            "fpr": 0.1690450054884742,
            "logloss": 0.7887412092068216,
            "mae": 0.27018103066329996,
            "precision": 0.7153419593345656,
            "recall": 0.828693790149893
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(22)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.8089759348372857,
            "auditor_fn_violation": 0.019752602759465405,
            "auditor_fp_violation": 0.021976264189886487,
            "ave_precision_score": 0.8093695975269886,
            "fpr": 0.13486842105263158,
            "logloss": 0.9825155157360159,
            "mae": 0.28545461787581106,
            "precision": 0.7426778242677824,
            "recall": 0.728952772073922
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7927321994745754,
            "auditor_fn_violation": 0.012890275175877987,
            "auditor_fp_violation": 0.022626358520979828,
            "ave_precision_score": 0.7931225376195011,
            "fpr": 0.14928649835345773,
            "logloss": 0.9946720765503247,
            "mae": 0.2905858546111063,
            "precision": 0.71900826446281,
            "recall": 0.7451820128479657
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(23)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7894736842105263,
            "auc_prc": 0.8735574249589838,
            "auditor_fn_violation": 0.02328298930076732,
            "auditor_fp_violation": 0.01794117647058824,
            "ave_precision_score": 0.8739785223576256,
            "fpr": 0.08662280701754387,
            "logloss": 0.7876291429312957,
            "mae": 0.2290945891877178,
            "precision": 0.82560706401766,
            "recall": 0.7679671457905544
        },
        "train": {
            "accuracy": 0.7892425905598244,
            "auc_prc": 0.875724642588825,
            "auditor_fn_violation": 0.015642739112959148,
            "auditor_fp_violation": 0.012816328952443114,
            "ave_precision_score": 0.8758927398612927,
            "fpr": 0.09001097694840834,
            "logloss": 0.7764950208635754,
            "mae": 0.2314075433290094,
            "precision": 0.8132118451025057,
            "recall": 0.7644539614561028
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(24)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8443436652691911,
            "auditor_fn_violation": 0.028513274973882348,
            "auditor_fp_violation": 0.02110165118679051,
            "ave_precision_score": 0.8447907697659933,
            "fpr": 0.10635964912280702,
            "logloss": 0.7798929479224942,
            "mae": 0.2633900392069392,
            "precision": 0.7872807017543859,
            "recall": 0.7371663244353183
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8329568020525724,
            "auditor_fn_violation": 0.014883519769084497,
            "auditor_fp_violation": 0.022413741952710124,
            "ave_precision_score": 0.8332793501571196,
            "fpr": 0.1119648737650933,
            "logloss": 0.7708588344949568,
            "mae": 0.2673266189377781,
            "precision": 0.7692307692307693,
            "recall": 0.728051391862955
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(25)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8209527012494395,
            "auditor_fn_violation": 0.028511023451853464,
            "auditor_fp_violation": 0.029938080495356043,
            "ave_precision_score": 0.8217337222563272,
            "fpr": 0.12390350877192982,
            "logloss": 0.9085849549708744,
            "mae": 0.275672211009739,
            "precision": 0.7631027253668763,
            "recall": 0.7474332648870636
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8131641993937075,
            "auditor_fn_violation": 0.02730839113664303,
            "auditor_fp_violation": 0.025207424768346833,
            "ave_precision_score": 0.8136011780477765,
            "fpr": 0.12294182217343579,
            "logloss": 0.8780440064528643,
            "mae": 0.2756273932977579,
            "precision": 0.7565217391304347,
            "recall": 0.7451820128479657
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(26)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7894736842105263,
            "auc_prc": 0.8811402523311798,
            "auditor_fn_violation": 0.019133434201520232,
            "auditor_fp_violation": 0.014685242518059855,
            "ave_precision_score": 0.880572088734723,
            "fpr": 0.1162280701754386,
            "logloss": 0.9746698940200614,
            "mae": 0.2162337066279023,
            "precision": 0.7909270216962525,
            "recall": 0.8234086242299795
        },
        "train": {
            "accuracy": 0.7870472008781558,
            "auc_prc": 0.8833180243495313,
            "auditor_fn_violation": 0.007451161981680015,
            "auditor_fp_violation": 0.016554424896905694,
            "ave_precision_score": 0.8834812659153681,
            "fpr": 0.11964873765093303,
            "logloss": 0.9634839735431712,
            "mae": 0.22340785932751378,
            "precision": 0.7780040733197556,
            "recall": 0.8179871520342612
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(27)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8736726819768765,
            "auditor_fn_violation": 0.016699538888288484,
            "auditor_fp_violation": 0.033266253869969044,
            "ave_precision_score": 0.8738948119588775,
            "fpr": 0.15460526315789475,
            "logloss": 0.6112295920291969,
            "mae": 0.2471577869958573,
            "precision": 0.7477638640429338,
            "recall": 0.8583162217659137
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8771785337438425,
            "auditor_fn_violation": 0.017184683043552865,
            "auditor_fp_violation": 0.031793593813352325,
            "ave_precision_score": 0.8773526118329826,
            "fpr": 0.14709110867178923,
            "logloss": 0.611606932836594,
            "mae": 0.24632100969857304,
            "precision": 0.7485928705440901,
            "recall": 0.854389721627409
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(28)",
        "seed": 5740,
        "test": {
            "accuracy": 0.3793859649122807,
            "auc_prc": 0.41892846872288353,
            "auditor_fn_violation": 0.025721387658056864,
            "auditor_fp_violation": 0.023243034055727568,
            "ave_precision_score": 0.4191090406039387,
            "fpr": 0.2730263157894737,
            "logloss": 3.8593518706768823,
            "mae": 0.6196811041732854,
            "precision": 0.40572792362768495,
            "recall": 0.3490759753593429
        },
        "train": {
            "accuracy": 0.3754116355653128,
            "auc_prc": 0.3973979877247071,
            "auditor_fn_violation": 0.01285736783589581,
            "auditor_fp_violation": 0.017259026315008762,
            "ave_precision_score": 0.39861138838093135,
            "fpr": 0.2864983534577388,
            "logloss": 4.114355420434772,
            "mae": 0.6234680048658736,
            "precision": 0.37857142857142856,
            "recall": 0.3404710920770878
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(29)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8124808572895563,
            "auditor_fn_violation": 0.024739724053460144,
            "auditor_fp_violation": 0.017198142414860687,
            "ave_precision_score": 0.8129257487003603,
            "fpr": 0.13596491228070176,
            "logloss": 0.9155069148736121,
            "mae": 0.28197949695473273,
            "precision": 0.7443298969072165,
            "recall": 0.7412731006160165
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.790919862184364,
            "auditor_fn_violation": 0.019445887405185727,
            "auditor_fp_violation": 0.02353615965031991,
            "ave_precision_score": 0.7913396772900678,
            "fpr": 0.14050493962678376,
            "logloss": 0.9522647835520256,
            "mae": 0.28984486047680363,
            "precision": 0.7264957264957265,
            "recall": 0.728051391862955
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(30)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7763157894736842,
            "auc_prc": 0.8760071003451488,
            "auditor_fn_violation": 0.008837223963399267,
            "auditor_fp_violation": 0.006738906088751292,
            "ave_precision_score": 0.8762303575004384,
            "fpr": 0.07236842105263158,
            "logloss": 0.733698074343727,
            "mae": 0.23918082914233116,
            "precision": 0.8409638554216867,
            "recall": 0.7166324435318275
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.8769121762025498,
            "auditor_fn_violation": 0.011703260412234956,
            "auditor_fp_violation": 0.009330406147091108,
            "ave_precision_score": 0.8770987425496065,
            "fpr": 0.07793633369923161,
            "logloss": 0.7246874186386097,
            "mae": 0.2293159168328841,
            "precision": 0.8293269230769231,
            "recall": 0.7387580299785867
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(31)",
        "seed": 5740,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7391589855471189,
            "auditor_fn_violation": 0.0282070679779531,
            "auditor_fp_violation": 0.022781217750257998,
            "ave_precision_score": 0.7399237360920508,
            "fpr": 0.20175438596491227,
            "logloss": 0.8613674908929011,
            "mae": 0.3463643530654767,
            "precision": 0.6737588652482269,
            "recall": 0.7802874743326489
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.7084025277868553,
            "auditor_fn_violation": 0.020724572615921987,
            "auditor_fp_violation": 0.010109176135520814,
            "ave_precision_score": 0.7097860228387112,
            "fpr": 0.21405049396267836,
            "logloss": 0.8795963306909454,
            "mae": 0.34743938511940997,
            "precision": 0.6524064171122995,
            "recall": 0.7837259100642399
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(32)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7995528147080571,
            "auditor_fn_violation": 0.016843636298137538,
            "auditor_fp_violation": 0.0149638802889577,
            "ave_precision_score": 0.7998857661795766,
            "fpr": 0.16228070175438597,
            "logloss": 0.9271689090259452,
            "mae": 0.2966155334563435,
            "precision": 0.7164750957854407,
            "recall": 0.7679671457905544
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7754620251434121,
            "auditor_fn_violation": 0.013842237511076843,
            "auditor_fp_violation": 0.028485675576784254,
            "ave_precision_score": 0.776403331879514,
            "fpr": 0.1734357848518112,
            "logloss": 0.9817304510480042,
            "mae": 0.2942176558754788,
            "precision": 0.6984732824427481,
            "recall": 0.7837259100642399
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(33)",
        "seed": 5740,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8432220129250397,
            "auditor_fn_violation": 0.03134118664217011,
            "auditor_fp_violation": 0.02871259029927761,
            "ave_precision_score": 0.843713492281688,
            "fpr": 0.13596491228070176,
            "logloss": 0.7809741265314488,
            "mae": 0.26283495286210085,
            "precision": 0.7582846003898636,
            "recall": 0.7987679671457906
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8349452668924242,
            "auditor_fn_violation": 0.016825052828033295,
            "auditor_fp_violation": 0.02880212814351125,
            "ave_precision_score": 0.8352685879232391,
            "fpr": 0.1437980241492865,
            "logloss": 0.7706073843621085,
            "mae": 0.262758941598017,
            "precision": 0.7405940594059406,
            "recall": 0.8008565310492506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(34)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8434949856335248,
            "auditor_fn_violation": 0.03164514211607047,
            "auditor_fp_violation": 0.028627450980392162,
            "ave_precision_score": 0.8439795575117288,
            "fpr": 0.13815789473684212,
            "logloss": 0.7751042593872439,
            "mae": 0.2632614626816229,
            "precision": 0.7543859649122807,
            "recall": 0.7946611909650924
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8353741894702507,
            "auditor_fn_violation": 0.016825052828033295,
            "auditor_fp_violation": 0.031724369814380794,
            "ave_precision_score": 0.8356953189313916,
            "fpr": 0.14270032930845225,
            "logloss": 0.7644985188639983,
            "mae": 0.2625736990656983,
            "precision": 0.7420634920634921,
            "recall": 0.8008565310492506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(35)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7708333333333334,
            "auc_prc": 0.8462780182345622,
            "auditor_fn_violation": 0.023447350408876407,
            "auditor_fp_violation": 0.013756449948400416,
            "ave_precision_score": 0.846689992017245,
            "fpr": 0.07017543859649122,
            "logloss": 0.8003761461121522,
            "mae": 0.28243077030234315,
            "precision": 0.8423645320197044,
            "recall": 0.702258726899384
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8635370938761187,
            "auditor_fn_violation": 0.02441959679106426,
            "auditor_fp_violation": 0.015204556916960871,
            "ave_precision_score": 0.8637162246970257,
            "fpr": 0.07683863885839737,
            "logloss": 0.9383003777861537,
            "mae": 0.2764880777350325,
            "precision": 0.8241206030150754,
            "recall": 0.702355460385439
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(36)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8251827474563738,
            "auditor_fn_violation": 0.021423232104902917,
            "auditor_fp_violation": 0.010319917440660473,
            "ave_precision_score": 0.8255672108621888,
            "fpr": 0.09868421052631579,
            "logloss": 0.9678267256984442,
            "mae": 0.293860079045179,
            "precision": 0.7877358490566038,
            "recall": 0.6858316221765913
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8183551589966441,
            "auditor_fn_violation": 0.013442648382721772,
            "auditor_fp_violation": 0.012799022952700231,
            "ave_precision_score": 0.8187536906890567,
            "fpr": 0.11306256860592755,
            "logloss": 0.8979884683847862,
            "mae": 0.2836490121369282,
            "precision": 0.7632183908045977,
            "recall": 0.7109207708779444
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(37)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8460297375618336,
            "auditor_fn_violation": 0.01686840304045535,
            "auditor_fp_violation": 0.019535603715170283,
            "ave_precision_score": 0.8464880478965033,
            "fpr": 0.17105263157894737,
            "logloss": 0.7280462860360681,
            "mae": 0.2672722940636726,
            "precision": 0.7248677248677249,
            "recall": 0.8439425051334702
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8369801060274831,
            "auditor_fn_violation": 0.007030418134764962,
            "auditor_fp_violation": 0.025736493903343524,
            "ave_precision_score": 0.8372952794298885,
            "fpr": 0.18111964873765093,
            "logloss": 0.7711476866879436,
            "mae": 0.26801178595135955,
            "precision": 0.7069271758436945,
            "recall": 0.8522483940042827
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(38)",
        "seed": 5740,
        "test": {
            "accuracy": 0.28399122807017546,
            "auc_prc": 0.3815461794469398,
            "auditor_fn_violation": 0.017622662920134025,
            "auditor_fp_violation": 0.024589783281733744,
            "ave_precision_score": 0.3826079177545219,
            "fpr": 0.2993421052631579,
            "logloss": 1.2868629114298253,
            "mae": 0.6237192999874344,
            "precision": 0.28157894736842104,
            "recall": 0.21971252566735114
        },
        "train": {
            "accuracy": 0.29527991218441274,
            "auc_prc": 0.3622117161992229,
            "auditor_fn_violation": 0.012438974513265186,
            "auditor_fp_violation": 0.024129508212933037,
            "ave_precision_score": 0.3634172999721451,
            "fpr": 0.31394072447859495,
            "logloss": 1.2834129937688694,
            "mae": 0.6234571351145796,
            "precision": 0.2795969773299748,
            "recall": 0.23768736616702354
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(39)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.8047813852538843,
            "auditor_fn_violation": 0.05569139738463202,
            "auditor_fp_violation": 0.05287667698658413,
            "ave_precision_score": 0.8054091363695306,
            "fpr": 0.18421052631578946,
            "logloss": 0.8635540888069465,
            "mae": 0.3218519243186974,
            "precision": 0.6983842010771992,
            "recall": 0.7987679671457906
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.8121882227244429,
            "auditor_fn_violation": 0.057406854598918304,
            "auditor_fp_violation": 0.05507758032456167,
            "ave_precision_score": 0.8126001867095366,
            "fpr": 0.18111964873765093,
            "logloss": 0.8156436433410491,
            "mae": 0.31291002774953874,
            "precision": 0.6915887850467289,
            "recall": 0.7922912205567452
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(40)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8321244328704731,
            "auditor_fn_violation": 0.008206797795309632,
            "auditor_fp_violation": 0.014989680082559345,
            "ave_precision_score": 0.8324987796714184,
            "fpr": 0.125,
            "logloss": 0.750925067710504,
            "mae": 0.2722980923794229,
            "precision": 0.7668711656441718,
            "recall": 0.7700205338809035
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8204006361313674,
            "auditor_fn_violation": 0.008837971309500587,
            "auditor_fp_violation": 0.017459281454890674,
            "ave_precision_score": 0.8208005866250939,
            "fpr": 0.13830954994511527,
            "logloss": 0.7770958236668264,
            "mae": 0.27051259877185746,
            "precision": 0.7428571428571429,
            "recall": 0.7794432548179872
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(41)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8246429864582098,
            "auditor_fn_violation": 0.019027612666162326,
            "auditor_fp_violation": 0.022874097007223943,
            "ave_precision_score": 0.8252438719438657,
            "fpr": 0.1875,
            "logloss": 0.8060306318074741,
            "mae": 0.28560426061630906,
            "precision": 0.7101694915254237,
            "recall": 0.8603696098562629
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8023784145243301,
            "auditor_fn_violation": 0.01647717523393593,
            "auditor_fp_violation": 0.025434875050681864,
            "ave_precision_score": 0.8028197138729124,
            "fpr": 0.19538968166849616,
            "logloss": 0.8373823272101478,
            "mae": 0.2907437141907625,
            "precision": 0.6941580756013745,
            "recall": 0.8650963597430407
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(42)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7839912280701754,
            "auc_prc": 0.8632074215903334,
            "auditor_fn_violation": 0.02182850607010339,
            "auditor_fp_violation": 0.02185500515995872,
            "ave_precision_score": 0.863597581981635,
            "fpr": 0.09649122807017543,
            "logloss": 0.7885756690872193,
            "mae": 0.2447191323458258,
            "precision": 0.8111587982832618,
            "recall": 0.7761806981519507
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8633990108709841,
            "auditor_fn_violation": 0.014354651805085128,
            "auditor_fp_violation": 0.015150166632054667,
            "ave_precision_score": 0.8635872478181839,
            "fpr": 0.10318331503841932,
            "logloss": 0.7769004147959826,
            "mae": 0.24702962821128116,
            "precision": 0.7892376681614349,
            "recall": 0.7537473233404711
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(43)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8470997825841179,
            "auditor_fn_violation": 0.012538726178896935,
            "auditor_fp_violation": 0.012631578947368424,
            "ave_precision_score": 0.8473952968424867,
            "fpr": 0.13048245614035087,
            "logloss": 0.6716688186618593,
            "mae": 0.26865572755865175,
            "precision": 0.7638888888888888,
            "recall": 0.7905544147843943
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8379243336145361,
            "auditor_fn_violation": 0.005965630633912896,
            "auditor_fp_violation": 0.016979658033445085,
            "ave_precision_score": 0.8382411690117666,
            "fpr": 0.13611416026344675,
            "logloss": 0.6855298090130701,
            "mae": 0.26819599456188864,
            "precision": 0.7489878542510121,
            "recall": 0.7922912205567452
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(44)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.800888033211127,
            "auditor_fn_violation": 0.05144052379408481,
            "auditor_fp_violation": 0.04609649122807018,
            "ave_precision_score": 0.8014179188987918,
            "fpr": 0.13048245614035087,
            "logloss": 1.141467602437467,
            "mae": 0.30695018317346734,
            "precision": 0.7325842696629213,
            "recall": 0.6694045174537988
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7839437386182798,
            "auditor_fn_violation": 0.03895288844176694,
            "auditor_fp_violation": 0.04716873844206446,
            "ave_precision_score": 0.7844736043395036,
            "fpr": 0.12403951701427003,
            "logloss": 1.060018325567951,
            "mae": 0.30322822813991257,
            "precision": 0.7402298850574712,
            "recall": 0.6895074946466809
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(45)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5975877192982456,
            "auc_prc": 0.6274231981927687,
            "auditor_fn_violation": 0.06720568104038332,
            "auditor_fp_violation": 0.0756217750257998,
            "ave_precision_score": 0.6146448040321166,
            "fpr": 0.14692982456140352,
            "logloss": 2.8139824619073264,
            "mae": 0.4044843380207937,
            "precision": 0.654639175257732,
            "recall": 0.5215605749486653
        },
        "train": {
            "accuracy": 0.5971459934138309,
            "auc_prc": 0.5973995231466127,
            "auditor_fn_violation": 0.06120295131829155,
            "auditor_fp_violation": 0.07804016969768891,
            "ave_precision_score": 0.5790787472699834,
            "fpr": 0.15477497255762898,
            "logloss": 3.2671189281254964,
            "mae": 0.40494511814428613,
            "precision": 0.6308900523560209,
            "recall": 0.5160599571734475
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(46)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8508456622072966,
            "auditor_fn_violation": 0.02480051514824021,
            "auditor_fp_violation": 0.030559855521155838,
            "ave_precision_score": 0.8511739976809407,
            "fpr": 0.1425438596491228,
            "logloss": 0.7204069798710844,
            "mae": 0.2593086959040726,
            "precision": 0.752851711026616,
            "recall": 0.813141683778234
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8418593208745606,
            "auditor_fn_violation": 0.019060401422537298,
            "auditor_fp_violation": 0.0313535269627476,
            "ave_precision_score": 0.8421514711317692,
            "fpr": 0.150384193194292,
            "logloss": 0.7251633839653323,
            "mae": 0.2589764017466486,
            "precision": 0.7339805825242719,
            "recall": 0.8094218415417559
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(47)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7845070077574278,
            "auditor_fn_violation": 0.020133109982348067,
            "auditor_fp_violation": 0.020438596491228072,
            "ave_precision_score": 0.7358310175539975,
            "fpr": 0.14912280701754385,
            "logloss": 5.3313452267418056,
            "mae": 0.3139961102163228,
            "precision": 0.7130801687763713,
            "recall": 0.6940451745379876
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.770171780463035,
            "auditor_fn_violation": 0.015440594024497175,
            "auditor_fp_violation": 0.0221319013854689,
            "ave_precision_score": 0.7134718074012644,
            "fpr": 0.16794731064763996,
            "logloss": 5.701334065022528,
            "mae": 0.31319375468894545,
            "precision": 0.6858316221765913,
            "recall": 0.715203426124197
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(48)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8135550393745667,
            "auditor_fn_violation": 0.021450250369249618,
            "auditor_fp_violation": 0.022187822497420028,
            "ave_precision_score": 0.8138775832395152,
            "fpr": 0.13706140350877194,
            "logloss": 0.9598043826538768,
            "mae": 0.28167834213977455,
            "precision": 0.742798353909465,
            "recall": 0.7412731006160165
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.7973830399947391,
            "auditor_fn_violation": 0.015985915658487625,
            "auditor_fp_violation": 0.02317026137004184,
            "ave_precision_score": 0.7977554513104543,
            "fpr": 0.14818880351262348,
            "logloss": 0.9761394943557353,
            "mae": 0.2868925821949913,
            "precision": 0.7227926078028748,
            "recall": 0.7537473233404711
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(49)",
        "seed": 5740,
        "test": {
            "accuracy": 0.8015350877192983,
            "auc_prc": 0.879723050416307,
            "auditor_fn_violation": 0.012955257754241872,
            "auditor_fp_violation": 0.010681114551083596,
            "ave_precision_score": 0.8793420804111012,
            "fpr": 0.09868421052631579,
            "logloss": 0.8862214039225776,
            "mae": 0.21547242383556742,
            "precision": 0.8148148148148148,
            "recall": 0.813141683778234
        },
        "train": {
            "accuracy": 0.7914379802414928,
            "auc_prc": 0.8795869727731863,
            "auditor_fn_violation": 0.011402393303826421,
            "auditor_fp_violation": 0.013755797509913863,
            "ave_precision_score": 0.8797364030675194,
            "fpr": 0.10757409440175632,
            "logloss": 0.8910986533745099,
            "mae": 0.21989751706961966,
            "precision": 0.7928118393234672,
            "recall": 0.8029978586723768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(50)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8476426355129909,
            "auditor_fn_violation": 0.030391044345977884,
            "auditor_fp_violation": 0.025913312693498454,
            "ave_precision_score": 0.8480684662329008,
            "fpr": 0.1162280701754386,
            "logloss": 0.7320426270684045,
            "mae": 0.2621131221559085,
            "precision": 0.7773109243697479,
            "recall": 0.7597535934291582
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8376912857422454,
            "auditor_fn_violation": 0.015635687540105823,
            "auditor_fp_violation": 0.02381552793188359,
            "ave_precision_score": 0.8379915488935434,
            "fpr": 0.11525795828759605,
            "logloss": 0.7393247728976887,
            "mae": 0.26215590002428774,
            "precision": 0.7697368421052632,
            "recall": 0.7516059957173448
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(51)",
        "seed": 5740,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.7950783594562836,
            "auditor_fn_violation": 0.0095982384091646,
            "auditor_fp_violation": 0.005018059855521153,
            "ave_precision_score": 0.7891162891057529,
            "fpr": 0.19736842105263158,
            "logloss": 1.0161885547568328,
            "mae": 0.3163796622482774,
            "precision": 0.689119170984456,
            "recall": 0.8193018480492813
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.7856803873768806,
            "auditor_fn_violation": 0.005881011759673002,
            "auditor_fp_violation": 0.007194351321683934,
            "ave_precision_score": 0.7794842648349123,
            "fpr": 0.21295279912184412,
            "logloss": 1.0545197746018637,
            "mae": 0.3156274989396579,
            "precision": 0.6700680272108843,
            "recall": 0.8436830835117773
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(52)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7894736842105263,
            "auc_prc": 0.8814277176553004,
            "auditor_fn_violation": 0.006097121654238268,
            "auditor_fp_violation": 0.005036119711042314,
            "ave_precision_score": 0.88119344400845,
            "fpr": 0.08662280701754387,
            "logloss": 0.8771671392160336,
            "mae": 0.22139418780791487,
            "precision": 0.82560706401766,
            "recall": 0.7679671457905544
        },
        "train": {
            "accuracy": 0.7969264544456641,
            "auc_prc": 0.8812826042319726,
            "auditor_fn_violation": 0.0051970091929004775,
            "auditor_fp_violation": 0.009520772144262815,
            "ave_precision_score": 0.8814637283875209,
            "fpr": 0.0845225027442371,
            "logloss": 0.8833931847888847,
            "mae": 0.21426407815007212,
            "precision": 0.823394495412844,
            "recall": 0.7687366167023555
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(53)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7949561403508771,
            "auc_prc": 0.8788471472343518,
            "auditor_fn_violation": 0.022292319608055042,
            "auditor_fp_violation": 0.013377192982456142,
            "ave_precision_score": 0.8778480819948609,
            "fpr": 0.08771929824561403,
            "logloss": 0.9462822091950962,
            "mae": 0.21544016907422273,
            "precision": 0.8260869565217391,
            "recall": 0.7802874743326489
        },
        "train": {
            "accuracy": 0.7881448957189902,
            "auc_prc": 0.881779719354431,
            "auditor_fn_violation": 0.010379915240094306,
            "auditor_fp_violation": 0.011184620405257072,
            "ave_precision_score": 0.8819379648393935,
            "fpr": 0.09220636663007684,
            "logloss": 0.8872382410252158,
            "mae": 0.2206708538730656,
            "precision": 0.8099547511312217,
            "recall": 0.7665952890792291
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(54)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.7666055056438856,
            "auditor_fn_violation": 0.014738463201123963,
            "auditor_fp_violation": 0.021271929824561407,
            "ave_precision_score": 0.767080387399301,
            "fpr": 0.2149122807017544,
            "logloss": 0.9153050848548563,
            "mae": 0.3223614061340845,
            "precision": 0.668918918918919,
            "recall": 0.813141683778234
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7459448283393428,
            "auditor_fn_violation": 0.008969600669429315,
            "auditor_fp_violation": 0.02356335479277301,
            "ave_precision_score": 0.7467488200199495,
            "fpr": 0.20965971459934138,
            "logloss": 0.978942317059422,
            "mae": 0.3234751149004822,
            "precision": 0.666083916083916,
            "recall": 0.815845824411135
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(55)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8570371165009187,
            "auditor_fn_violation": 0.025674105695450127,
            "auditor_fp_violation": 0.027763157894736847,
            "ave_precision_score": 0.8574376962422868,
            "fpr": 0.11951754385964912,
            "logloss": 0.7823656401521731,
            "mae": 0.24991898641984386,
            "precision": 0.7784552845528455,
            "recall": 0.7864476386036962
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8487022220750627,
            "auditor_fn_violation": 0.011750270897923782,
            "auditor_fp_violation": 0.020341966554919353,
            "ave_precision_score": 0.848951010095098,
            "fpr": 0.13062568605927552,
            "logloss": 0.809637569925107,
            "mae": 0.2525815619314854,
            "precision": 0.7531120331950207,
            "recall": 0.7773019271948608
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(56)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7828947368421053,
            "auc_prc": 0.8763606501209242,
            "auditor_fn_violation": 0.023125382758744914,
            "auditor_fp_violation": 0.02550051599587204,
            "ave_precision_score": 0.8759680386965344,
            "fpr": 0.125,
            "logloss": 0.972947230599319,
            "mae": 0.222596676035571,
            "precision": 0.7794970986460348,
            "recall": 0.8275154004106776
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.879515862212679,
            "auditor_fn_violation": 0.009543128594833082,
            "auditor_fp_violation": 0.018972320289554114,
            "ave_precision_score": 0.87967872455488,
            "fpr": 0.12843029637760703,
            "logloss": 0.9432282625198362,
            "mae": 0.2302798435457405,
            "precision": 0.7650602409638554,
            "recall": 0.815845824411135
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(57)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7861842105263158,
            "auc_prc": 0.8766370139919697,
            "auditor_fn_violation": 0.019592744695414107,
            "auditor_fp_violation": 0.015330237358101137,
            "ave_precision_score": 0.876137969274256,
            "fpr": 0.09539473684210527,
            "logloss": 0.8262729406005063,
            "mae": 0.22200931621287917,
            "precision": 0.8133047210300429,
            "recall": 0.7782340862422998
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8756085521803907,
            "auditor_fn_violation": 0.006644932152116532,
            "auditor_fp_violation": 0.019365413712285284,
            "ave_precision_score": 0.8757870180801204,
            "fpr": 0.11525795828759605,
            "logloss": 0.8464199260493953,
            "mae": 0.2288798320223386,
            "precision": 0.7794117647058824,
            "recall": 0.7944325481798715
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(58)",
        "seed": 5740,
        "test": {
            "accuracy": 0.44956140350877194,
            "auc_prc": 0.45742105812542316,
            "auditor_fn_violation": 0.02011509780611694,
            "auditor_fp_violation": 0.01786893704850362,
            "ave_precision_score": 0.45784683520091396,
            "fpr": 0.2598684210526316,
            "logloss": 2.151677140401044,
            "mae": 0.5443531896593081,
            "precision": 0.48366013071895425,
            "recall": 0.45585215605749485
        },
        "train": {
            "accuracy": 0.45115257958287597,
            "auc_prc": 0.435005910077527,
            "auditor_fn_violation": 0.00912003422363359,
            "auditor_fp_violation": 0.022047843672432045,
            "ave_precision_score": 0.4363007905684875,
            "fpr": 0.27442371020856204,
            "logloss": 2.2938265364564856,
            "mae": 0.5519586692529571,
            "precision": 0.46466809421841543,
            "recall": 0.46466809421841543
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(59)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5592105263157895,
            "auc_prc": 0.7016102986931289,
            "auditor_fn_violation": 0.08780035303865415,
            "auditor_fp_violation": 0.08466976264189888,
            "ave_precision_score": 0.599645672230825,
            "fpr": 0.3059210526315789,
            "logloss": 10.237635401827058,
            "mae": 0.43988991040476494,
            "precision": 0.5660964230171073,
            "recall": 0.7474332648870636
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.7129013488078897,
            "auditor_fn_violation": 0.07663884429421983,
            "auditor_fp_violation": 0.08206010620939272,
            "ave_precision_score": 0.6132712158571394,
            "fpr": 0.27552140504939626,
            "logloss": 9.32051697915258,
            "mae": 0.39479606505135934,
            "precision": 0.5898692810457516,
            "recall": 0.7730192719486081
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(60)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8281201657377126,
            "auditor_fn_violation": 0.028945567203429518,
            "auditor_fp_violation": 0.03105005159958721,
            "ave_precision_score": 0.8287476957374214,
            "fpr": 0.1206140350877193,
            "logloss": 0.9023735834754408,
            "mae": 0.2715325567940983,
            "precision": 0.7684210526315789,
            "recall": 0.7494866529774127
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8203110893276179,
            "auditor_fn_violation": 0.02688294624115909,
            "auditor_fp_violation": 0.024759941060709455,
            "ave_precision_score": 0.8207961605178367,
            "fpr": 0.11306256860592755,
            "logloss": 0.8702473857772282,
            "mae": 0.2710858862984911,
            "precision": 0.7711111111111111,
            "recall": 0.7430406852248393
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(61)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7903874854669165,
            "auditor_fn_violation": 0.020283961958283803,
            "auditor_fp_violation": 0.026839525283797735,
            "ave_precision_score": 0.7852879538032209,
            "fpr": 0.17653508771929824,
            "logloss": 1.38699568832933,
            "mae": 0.2854661558874989,
            "precision": 0.7125,
            "recall": 0.8193018480492813
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7875224181009057,
            "auditor_fn_violation": 0.020247416186180332,
            "auditor_fp_violation": 0.02271041623401668,
            "ave_precision_score": 0.7814121443552404,
            "fpr": 0.18111964873765093,
            "logloss": 1.3524679207900554,
            "mae": 0.2921876538857266,
            "precision": 0.6955719557195572,
            "recall": 0.8072805139186295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(62)",
        "seed": 5740,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8404174590545687,
            "auditor_fn_violation": 0.017705969235203005,
            "auditor_fp_violation": 0.019081527347781214,
            "ave_precision_score": 0.8408813327266366,
            "fpr": 0.12390350877192982,
            "logloss": 0.7688280193168499,
            "mae": 0.27237310143465615,
            "precision": 0.7670103092783506,
            "recall": 0.7638603696098563
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8338199803623183,
            "auditor_fn_violation": 0.0073242336703201695,
            "auditor_fp_violation": 0.020579305979964605,
            "ave_precision_score": 0.8341302270005005,
            "fpr": 0.13062568605927552,
            "logloss": 0.7386211414918306,
            "mae": 0.26957988241390896,
            "precision": 0.7510460251046025,
            "recall": 0.7687366167023555
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(63)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7850877192982456,
            "auc_prc": 0.8751101767910203,
            "auditor_fn_violation": 0.01958599012932743,
            "auditor_fp_violation": 0.015469556243550061,
            "ave_precision_score": 0.8746776165247103,
            "fpr": 0.11293859649122807,
            "logloss": 1.057351461045699,
            "mae": 0.22043701880032565,
            "precision": 0.7927565392354124,
            "recall": 0.8090349075975359
        },
        "train": {
            "accuracy": 0.7892425905598244,
            "auc_prc": 0.8798674377694679,
            "auditor_fn_violation": 0.010100202850245753,
            "auditor_fp_violation": 0.01419586436051859,
            "ave_precision_score": 0.8800243918681857,
            "fpr": 0.1119648737650933,
            "logloss": 1.004077715143645,
            "mae": 0.22339484995251854,
            "precision": 0.7870563674321504,
            "recall": 0.8072805139186295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(64)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8399578207584906,
            "auditor_fn_violation": 0.026378832090493177,
            "auditor_fp_violation": 0.03305727554179567,
            "ave_precision_score": 0.8404142689341871,
            "fpr": 0.15021929824561403,
            "logloss": 0.875874768424811,
            "mae": 0.26429354635522523,
            "precision": 0.7410207939508506,
            "recall": 0.8049281314168378
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8287051783025412,
            "auditor_fn_violation": 0.019636279872225496,
            "auditor_fp_violation": 0.02914330356701378,
            "ave_precision_score": 0.8290278181312425,
            "fpr": 0.14709110867178923,
            "logloss": 0.87469291015413,
            "mae": 0.26314004460419743,
            "precision": 0.7392996108949417,
            "recall": 0.8137044967880086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(65)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8101781097671749,
            "auditor_fn_violation": 0.020374022839439457,
            "auditor_fp_violation": 0.02120227038183695,
            "ave_precision_score": 0.8108978376653329,
            "fpr": 0.1524122807017544,
            "logloss": 0.8636150890991573,
            "mae": 0.27634043491880395,
            "precision": 0.7352380952380952,
            "recall": 0.7926078028747433
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7793263440103464,
            "auditor_fn_violation": 0.014467476970738325,
            "auditor_fp_violation": 0.028478258719751584,
            "ave_precision_score": 0.7799507578092303,
            "fpr": 0.17014270032930845,
            "logloss": 0.9350538408161798,
            "mae": 0.28583455769138344,
            "precision": 0.7086466165413534,
            "recall": 0.8072805139186295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(66)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7002571632092092,
            "auditor_fn_violation": 0.03320094383803451,
            "auditor_fp_violation": 0.05237358101135192,
            "ave_precision_score": 0.6996854132810935,
            "fpr": 0.14364035087719298,
            "logloss": 1.3266007324572433,
            "mae": 0.3319580983576818,
            "precision": 0.7120879120879121,
            "recall": 0.6652977412731006
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.6604488297060127,
            "auditor_fn_violation": 0.03414841680436821,
            "auditor_fp_violation": 0.053831548343074145,
            "ave_precision_score": 0.6575591844613887,
            "fpr": 0.1525795828759605,
            "logloss": 1.5794132480115575,
            "mae": 0.33923220611292415,
            "precision": 0.6897321428571429,
            "recall": 0.6616702355460385
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(67)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.775635351542846,
            "auditor_fn_violation": 0.03516427104722793,
            "auditor_fp_violation": 0.012474200206398351,
            "ave_precision_score": 0.7766641120822138,
            "fpr": 0.1162280701754386,
            "logloss": 0.7290741883560141,
            "mae": 0.32572931132853306,
            "precision": 0.7579908675799086,
            "recall": 0.6817248459958932
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7356268237313506,
            "auditor_fn_violation": 0.02282829185049726,
            "auditor_fp_violation": 0.010823666696334097,
            "ave_precision_score": 0.7366175133765398,
            "fpr": 0.12623490669593854,
            "logloss": 0.7237593403095121,
            "mae": 0.32445582842796794,
            "precision": 0.7368421052631579,
            "recall": 0.6895074946466809
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(68)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8403246728403537,
            "auditor_fn_violation": 0.027405526135667716,
            "auditor_fp_violation": 0.02700980392156863,
            "ave_precision_score": 0.8409081851348396,
            "fpr": 0.11732456140350878,
            "logloss": 0.8756279443383073,
            "mae": 0.26492221151714895,
            "precision": 0.7747368421052632,
            "recall": 0.75564681724846
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8367256927163136,
            "auditor_fn_violation": 0.017715901531836678,
            "auditor_fp_violation": 0.02450776792159888,
            "ave_precision_score": 0.8372029802509877,
            "fpr": 0.11964873765093303,
            "logloss": 0.8437989451326373,
            "mae": 0.2636967118546096,
            "precision": 0.7640692640692641,
            "recall": 0.7558886509635975
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(69)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8774747027236489,
            "auditor_fn_violation": 0.010242173709427572,
            "auditor_fp_violation": 0.02362229102167183,
            "ave_precision_score": 0.8765237425607945,
            "fpr": 0.21162280701754385,
            "logloss": 1.14065577734691,
            "mae": 0.25571350403023807,
            "precision": 0.7012383900928792,
            "recall": 0.9301848049281314
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8808473749465899,
            "auditor_fn_violation": 0.008875579698051651,
            "auditor_fp_violation": 0.026038112756005184,
            "ave_precision_score": 0.8809748557153125,
            "fpr": 0.2217343578485181,
            "logloss": 1.0906801579662588,
            "mae": 0.2665787831789019,
            "precision": 0.6798732171156894,
            "recall": 0.9186295503211992
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(70)",
        "seed": 5740,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7682285432855096,
            "auditor_fn_violation": 0.014585359703159337,
            "auditor_fp_violation": 0.022644478844169254,
            "ave_precision_score": 0.7686739574496626,
            "fpr": 0.26644736842105265,
            "logloss": 1.1195557336011175,
            "mae": 0.3329030094075195,
            "precision": 0.6436950146627566,
            "recall": 0.9014373716632443
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.7454484895813862,
            "auditor_fn_violation": 0.0047222032874432674,
            "auditor_fp_violation": 0.025276648767318374,
            "ave_precision_score": 0.7470713412570731,
            "fpr": 0.26344676180021953,
            "logloss": 1.1845042960616172,
            "mae": 0.3335168706390676,
            "precision": 0.6330275229357798,
            "recall": 0.8865096359743041
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(71)",
        "seed": 5740,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8306258174054769,
            "auditor_fn_violation": 0.031325425987967864,
            "auditor_fp_violation": 0.029527863777089783,
            "ave_precision_score": 0.8310172090297845,
            "fpr": 0.13157894736842105,
            "logloss": 0.881745272529434,
            "mae": 0.2725589197615344,
            "precision": 0.7510373443983402,
            "recall": 0.7433264887063655
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8204478422761332,
            "auditor_fn_violation": 0.0202286119919048,
            "auditor_fp_violation": 0.03172189752870324,
            "ave_precision_score": 0.8207976556705108,
            "fpr": 0.13721185510428102,
            "logloss": 0.8802227761715616,
            "mae": 0.27212764617308033,
            "precision": 0.7395833333333334,
            "recall": 0.7601713062098501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(72)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8191704925052627,
            "auditor_fn_violation": 0.012124446125580895,
            "auditor_fp_violation": 0.026024251805985558,
            "ave_precision_score": 0.8195035860978924,
            "fpr": 0.15679824561403508,
            "logloss": 1.6705044051048206,
            "mae": 0.2841854607369035,
            "precision": 0.7265774378585086,
            "recall": 0.7802874743326489
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8050756438367056,
            "auditor_fn_violation": 0.009992078733161438,
            "auditor_fp_violation": 0.022586801950138943,
            "ave_precision_score": 0.8054950005764296,
            "fpr": 0.15367727771679474,
            "logloss": 1.921732119329183,
            "mae": 0.2886806228778044,
            "precision": 0.7194388777555111,
            "recall": 0.7687366167023555
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(73)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5833333333333334,
            "auc_prc": 0.606553378185112,
            "auditor_fn_violation": 0.07059872473792285,
            "auditor_fp_violation": 0.07774767801857586,
            "ave_precision_score": 0.5930729690554178,
            "fpr": 0.15350877192982457,
            "logloss": 3.1332992714499635,
            "mae": 0.4238738560167477,
            "precision": 0.6382428940568475,
            "recall": 0.5071868583162218
        },
        "train": {
            "accuracy": 0.5883644346871569,
            "auc_prc": 0.5968462706424015,
            "auditor_fn_violation": 0.06093029050129631,
            "auditor_fp_violation": 0.07651229714896017,
            "ave_precision_score": 0.5774984328187376,
            "fpr": 0.15367727771679474,
            "logloss": 3.3683230929498356,
            "mae": 0.4116939474301736,
            "precision": 0.6236559139784946,
            "recall": 0.49678800856531047
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(74)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7752192982456141,
            "auc_prc": 0.8669574163923024,
            "auditor_fn_violation": 0.014425501639108038,
            "auditor_fp_violation": 0.01642156862745098,
            "ave_precision_score": 0.866978633419727,
            "fpr": 0.1206140350877193,
            "logloss": 0.7556586790299563,
            "mae": 0.2388698395758221,
            "precision": 0.7808764940239044,
            "recall": 0.8049281314168378
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8641805847887484,
            "auditor_fn_violation": 0.016119895542700803,
            "auditor_fp_violation": 0.013768158938301646,
            "ave_precision_score": 0.8644306093105746,
            "fpr": 0.12623490669593854,
            "logloss": 0.7824874149056532,
            "mae": 0.23925804192878206,
            "precision": 0.766260162601626,
            "recall": 0.8072805139186295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(75)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7850877192982456,
            "auc_prc": 0.8811318334981185,
            "auditor_fn_violation": 0.015744893548038475,
            "auditor_fp_violation": 0.011597007223942211,
            "ave_precision_score": 0.8808421956923218,
            "fpr": 0.07675438596491228,
            "logloss": 0.9176489179752622,
            "mae": 0.22092675662767466,
            "precision": 0.8375870069605569,
            "recall": 0.7412731006160165
        },
        "train": {
            "accuracy": 0.7870472008781558,
            "auc_prc": 0.8846727820006649,
            "auditor_fn_violation": 0.014982241789031046,
            "auditor_fp_violation": 0.009723499569822295,
            "ave_precision_score": 0.8848290406803507,
            "fpr": 0.0845225027442371,
            "logloss": 0.8960010831079758,
            "mae": 0.21739408932278287,
            "precision": 0.819672131147541,
            "recall": 0.7494646680942184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(76)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7905701754385965,
            "auc_prc": 0.8827959080172294,
            "auditor_fn_violation": 0.017890594041572103,
            "auditor_fp_violation": 0.010448916408668735,
            "ave_precision_score": 0.8824148945659017,
            "fpr": 0.08223684210526316,
            "logloss": 0.8947700921781782,
            "mae": 0.21686156348127475,
            "precision": 0.8318385650224215,
            "recall": 0.7618069815195072
        },
        "train": {
            "accuracy": 0.7914379802414928,
            "auc_prc": 0.8850207673598134,
            "auditor_fn_violation": 0.010993402078333573,
            "auditor_fp_violation": 0.0071102936086470695,
            "ave_precision_score": 0.8851738332136952,
            "fpr": 0.08781558726673985,
            "logloss": 0.8755354314579896,
            "mae": 0.21658523615666436,
            "precision": 0.816933638443936,
            "recall": 0.7644539614561028
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(77)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.847941680820155,
            "auditor_fn_violation": 0.02132416513563169,
            "auditor_fp_violation": 0.026359649122807034,
            "ave_precision_score": 0.8483564046123753,
            "fpr": 0.18969298245614036,
            "logloss": 0.8187717182688281,
            "mae": 0.2698690470549769,
            "precision": 0.7107023411371237,
            "recall": 0.8726899383983573
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8375625268197908,
            "auditor_fn_violation": 0.015008097556159904,
            "auditor_fp_violation": 0.03296298493883567,
            "ave_precision_score": 0.8378087292380785,
            "fpr": 0.18551042810098792,
            "logloss": 0.8315501938269633,
            "mae": 0.2696097772457024,
            "precision": 0.7065972222222222,
            "recall": 0.8715203426124197
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(78)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7949561403508771,
            "auc_prc": 0.8815657283569602,
            "auditor_fn_violation": 0.018786699809070932,
            "auditor_fp_violation": 0.013531991744066049,
            "ave_precision_score": 0.8811838385712988,
            "fpr": 0.08771929824561403,
            "logloss": 0.8705293417425913,
            "mae": 0.21669537791381627,
            "precision": 0.8260869565217391,
            "recall": 0.7802874743326489
        },
        "train": {
            "accuracy": 0.7914379802414928,
            "auc_prc": 0.8830134383731236,
            "auditor_fn_violation": 0.005989135876757313,
            "auditor_fp_violation": 0.009899031852928671,
            "ave_precision_score": 0.8831697544182904,
            "fpr": 0.09659714599341383,
            "logloss": 0.8552767908988569,
            "mae": 0.21845037691330346,
            "precision": 0.8057395143487859,
            "recall": 0.7815845824411135
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(79)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.773217464839509,
            "auditor_fn_violation": 0.00767769011852012,
            "auditor_fp_violation": 0.017884416924664602,
            "ave_precision_score": 0.7717521573159559,
            "fpr": 0.4144736842105263,
            "logloss": 3.0157116760290408,
            "mae": 0.42153420706851247,
            "precision": 0.5573770491803278,
            "recall": 0.9774127310061602
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.7482503892128867,
            "auditor_fn_violation": 0.004080510157790695,
            "auditor_fp_violation": 0.022745028233502444,
            "ave_precision_score": 0.7450768396645265,
            "fpr": 0.4270032930845225,
            "logloss": 3.143265302382945,
            "mae": 0.4298865083893006,
            "precision": 0.541813898704358,
            "recall": 0.9850107066381156
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(80)",
        "seed": 5740,
        "test": {
            "accuracy": 0.47039473684210525,
            "auc_prc": 0.5689759883182421,
            "auditor_fn_violation": 0.10324804567887894,
            "auditor_fp_violation": 0.09678534571723427,
            "ave_precision_score": 0.5191076932310433,
            "fpr": 0.16337719298245615,
            "logloss": 11.777411918373916,
            "mae": 0.5279656382038325,
            "precision": 0.5066225165562914,
            "recall": 0.3141683778234086
        },
        "train": {
            "accuracy": 0.45773874862788144,
            "auc_prc": 0.5357967029750299,
            "auditor_fn_violation": 0.09606827802941446,
            "auditor_fp_violation": 0.09591973971776387,
            "ave_precision_score": 0.48207466510124053,
            "fpr": 0.18551042810098792,
            "logloss": 11.694329977903582,
            "mae": 0.5441206135791654,
            "precision": 0.4565916398713826,
            "recall": 0.30406852248394006
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(81)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7774122807017544,
            "auc_prc": 0.8844121423050413,
            "auditor_fn_violation": 0.011192316005619797,
            "auditor_fp_violation": 0.006707946336429311,
            "ave_precision_score": 0.8796067030952177,
            "fpr": 0.07675438596491228,
            "logloss": 1.6311020319031115,
            "mae": 0.22130278467242093,
            "precision": 0.8349056603773585,
            "recall": 0.7268993839835729
        },
        "train": {
            "accuracy": 0.7859495060373216,
            "auc_prc": 0.8793184769186844,
            "auditor_fn_violation": 0.0082855981026568,
            "auditor_fp_violation": 0.01024515184778632,
            "ave_precision_score": 0.8734393145232706,
            "fpr": 0.08122941822173436,
            "logloss": 1.633596580559868,
            "mae": 0.21345324703514687,
            "precision": 0.8238095238095238,
            "recall": 0.7408993576017131
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(82)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8378549584461061,
            "auditor_fn_violation": 0.01773073597752081,
            "auditor_fp_violation": 0.014636222910216722,
            "ave_precision_score": 0.8382621157834487,
            "fpr": 0.14144736842105263,
            "logloss": 0.7080705245286374,
            "mae": 0.2736433476614584,
            "precision": 0.7465618860510805,
            "recall": 0.7802874743326489
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8233319309221665,
            "auditor_fn_violation": 0.007258418990355799,
            "auditor_fp_violation": 0.01615391461714185,
            "ave_precision_score": 0.8236994095636693,
            "fpr": 0.145993413830955,
            "logloss": 0.7284017910445345,
            "mae": 0.2718843886336335,
            "precision": 0.7392156862745098,
            "recall": 0.8072805139186295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(83)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8453428229380162,
            "auditor_fn_violation": 0.014655156886054978,
            "auditor_fp_violation": 0.024179566563467498,
            "ave_precision_score": 0.8456156890269656,
            "fpr": 0.17653508771929824,
            "logloss": 0.7335786049565607,
            "mae": 0.27142113031773013,
            "precision": 0.7175438596491228,
            "recall": 0.839835728952772
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8363055110736624,
            "auditor_fn_violation": 0.009242261486424552,
            "auditor_fp_violation": 0.023239485369013366,
            "ave_precision_score": 0.8366268839362994,
            "fpr": 0.18551042810098792,
            "logloss": 0.7795359638273868,
            "mae": 0.2705063436058143,
            "precision": 0.7035087719298245,
            "recall": 0.8586723768736617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(84)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8228280120526256,
            "auditor_fn_violation": 0.029873194279332833,
            "auditor_fp_violation": 0.03173374613003096,
            "ave_precision_score": 0.8234771268500133,
            "fpr": 0.13267543859649122,
            "logloss": 0.9043221819007315,
            "mae": 0.2740366488314542,
            "precision": 0.7545638945233266,
            "recall": 0.7638603696098563
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8138302796441483,
            "auditor_fn_violation": 0.027501134127967244,
            "auditor_fp_violation": 0.025958999614323443,
            "ave_precision_score": 0.8143337599482877,
            "fpr": 0.12843029637760703,
            "logloss": 0.8794565724615803,
            "mae": 0.27559866655165066,
            "precision": 0.75,
            "recall": 0.7516059957173448
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(85)",
        "seed": 5740,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.839103298446048,
            "auditor_fn_violation": 0.02652518102237113,
            "auditor_fp_violation": 0.029218266253869973,
            "ave_precision_score": 0.8395676723283716,
            "fpr": 0.13157894736842105,
            "logloss": 0.8303107191485389,
            "mae": 0.2640742153444846,
            "precision": 0.7595190380761523,
            "recall": 0.7782340862422998
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8292782856526607,
            "auditor_fn_violation": 0.01701309477078863,
            "auditor_fp_violation": 0.029420199562899898,
            "ave_precision_score": 0.8296037519679794,
            "fpr": 0.14270032930845225,
            "logloss": 0.8333052719760198,
            "mae": 0.26458208069424516,
            "precision": 0.7357723577235772,
            "recall": 0.7751605995717344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(86)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8245906649729507,
            "auditor_fn_violation": 0.01551974134514932,
            "auditor_fp_violation": 0.020335397316821473,
            "ave_precision_score": 0.8250659050514844,
            "fpr": 0.16557017543859648,
            "logloss": 0.8244455567647129,
            "mae": 0.2829694389589923,
            "precision": 0.7166979362101313,
            "recall": 0.784394250513347
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8045516131746817,
            "auditor_fn_violation": 0.011237856603915509,
            "auditor_fp_violation": 0.02212695681411379,
            "ave_precision_score": 0.8050052362354245,
            "fpr": 0.17233809001097694,
            "logloss": 0.8744931134200743,
            "mae": 0.2810240672179352,
            "precision": 0.7087198515769945,
            "recall": 0.8179871520342612
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(87)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8367414700285984,
            "auditor_fn_violation": 0.017525847472891674,
            "auditor_fp_violation": 0.029298245614035087,
            "ave_precision_score": 0.8370888181748961,
            "fpr": 0.125,
            "logloss": 0.8375621726088593,
            "mae": 0.2695872397163595,
            "precision": 0.7649484536082474,
            "recall": 0.7618069815195072
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8234975904882503,
            "auditor_fn_violation": 0.01351316411125502,
            "auditor_fp_violation": 0.022908199088221042,
            "ave_precision_score": 0.8239578429543483,
            "fpr": 0.132821075740944,
            "logloss": 0.8484185904524243,
            "mae": 0.2670838567233963,
            "precision": 0.7479166666666667,
            "recall": 0.7687366167023555
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(88)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7683348049476744,
            "auditor_fn_violation": 0.01838367736589935,
            "auditor_fp_violation": 0.01818885448916409,
            "ave_precision_score": 0.7079408130787462,
            "fpr": 0.16447368421052633,
            "logloss": 6.004756590236271,
            "mae": 0.3276724082947945,
            "precision": 0.6957403651115619,
            "recall": 0.704312114989733
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7576209337600708,
            "auditor_fn_violation": 0.013158234944304336,
            "auditor_fp_violation": 0.02251016109413475,
            "ave_precision_score": 0.6867389892416305,
            "fpr": 0.18551042810098792,
            "logloss": 6.514394311520772,
            "mae": 0.32499497442877257,
            "precision": 0.6692759295499021,
            "recall": 0.7323340471092077
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(89)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.7641345229043204,
            "auditor_fn_violation": 0.01567734788717173,
            "auditor_fp_violation": 0.020304437564499487,
            "ave_precision_score": 0.764611989005638,
            "fpr": 0.14802631578947367,
            "logloss": 0.9198535507619523,
            "mae": 0.3268777995249113,
            "precision": 0.7133757961783439,
            "recall": 0.6899383983572895
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.7412878172183895,
            "auditor_fn_violation": 0.00970296424617511,
            "auditor_fp_violation": 0.02678968760198179,
            "ave_precision_score": 0.7431613735213476,
            "fpr": 0.16465422612513722,
            "logloss": 0.9499024612525675,
            "mae": 0.32217153353251055,
            "precision": 0.6907216494845361,
            "recall": 0.7173447537473233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(90)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8161261322596775,
            "auditor_fn_violation": 0.017246658741309127,
            "auditor_fp_violation": 0.022337461300309606,
            "ave_precision_score": 0.8169477398694323,
            "fpr": 0.15021929824561403,
            "logloss": 0.8428453273735792,
            "mae": 0.2747304647984004,
            "precision": 0.7415094339622641,
            "recall": 0.8069815195071869
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7865902615123053,
            "auditor_fn_violation": 0.008727496668131832,
            "auditor_fp_violation": 0.029865210984859728,
            "ave_precision_score": 0.7871829182532923,
            "fpr": 0.17233809001097694,
            "logloss": 0.9181304409310321,
            "mae": 0.2836840582492447,
            "precision": 0.7059925093632958,
            "recall": 0.8072805139186295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(91)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8198534169891954,
            "auditor_fn_violation": 0.02903562808458518,
            "auditor_fp_violation": 0.027551599587203313,
            "ave_precision_score": 0.8203724589315549,
            "fpr": 0.14035087719298245,
            "logloss": 0.9732970564695231,
            "mae": 0.2767347966602605,
            "precision": 0.7445109780439122,
            "recall": 0.7659137577002053
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.808068636416919,
            "auditor_fn_violation": 0.022440455343564387,
            "auditor_fp_violation": 0.02720750388148852,
            "ave_precision_score": 0.8084480967462306,
            "fpr": 0.1394072447859495,
            "logloss": 0.9562963285313824,
            "mae": 0.27918354834927944,
            "precision": 0.7348643006263048,
            "recall": 0.7537473233404711
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(92)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7905701754385965,
            "auc_prc": 0.881511010234332,
            "auditor_fn_violation": 0.022569256817608702,
            "auditor_fp_violation": 0.018998968008255943,
            "ave_precision_score": 0.8803331548478046,
            "fpr": 0.10197368421052631,
            "logloss": 0.9834217325205384,
            "mae": 0.21387283331461687,
            "precision": 0.8070539419087137,
            "recall": 0.7987679671457906
        },
        "train": {
            "accuracy": 0.7859495060373216,
            "auc_prc": 0.881337738391092,
            "auditor_fn_violation": 0.00804819514992819,
            "auditor_fp_violation": 0.014284866644910554,
            "ave_precision_score": 0.8814934273460574,
            "fpr": 0.1119648737650933,
            "logloss": 0.9508166595782488,
            "mae": 0.22205492147603742,
            "precision": 0.7857142857142857,
            "recall": 0.8008565310492506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(93)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8137175437536158,
            "auditor_fn_violation": 0.00893854245469938,
            "auditor_fp_violation": 0.007391640866873066,
            "ave_precision_score": 0.8139539635520436,
            "fpr": 0.10416666666666667,
            "logloss": 0.7373769708725318,
            "mae": 0.28770176034679124,
            "precision": 0.7948164146868251,
            "recall": 0.75564681724846
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8018684106352152,
            "auditor_fn_violation": 0.004795069540260954,
            "auditor_fp_violation": 0.011604908970441353,
            "ave_precision_score": 0.800972332191489,
            "fpr": 0.11306256860592755,
            "logloss": 0.7200675874959591,
            "mae": 0.28875031237015386,
            "precision": 0.7716186252771619,
            "recall": 0.7451820128479657
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(94)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8134065315928551,
            "auditor_fn_violation": 0.006482131921178719,
            "auditor_fp_violation": 0.011222910216718274,
            "ave_precision_score": 0.8135469829355948,
            "fpr": 0.17763157894736842,
            "logloss": 0.8467193210469789,
            "mae": 0.28118643100425456,
            "precision": 0.7147887323943662,
            "recall": 0.8336755646817249
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.7912005472078696,
            "auditor_fn_violation": 0.009385643467775491,
            "auditor_fp_violation": 0.02089328626101403,
            "ave_precision_score": 0.7912880984325115,
            "fpr": 0.1942919868276619,
            "logloss": 0.9172689107927733,
            "mae": 0.2842820168651208,
            "precision": 0.6948275862068966,
            "recall": 0.8629550321199143
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(95)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7861842105263158,
            "auc_prc": 0.8772502403417443,
            "auditor_fn_violation": 0.016523920170034952,
            "auditor_fp_violation": 0.014816821465428278,
            "ave_precision_score": 0.876839232466075,
            "fpr": 0.10307017543859649,
            "logloss": 0.8128435404093239,
            "mae": 0.22116491500120666,
            "precision": 0.8041666666666667,
            "recall": 0.7926078028747433
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8771667941268848,
            "auditor_fn_violation": 0.005598948845539998,
            "auditor_fp_violation": 0.019758507135016472,
            "ave_precision_score": 0.8773245622494198,
            "fpr": 0.12184412733260154,
            "logloss": 0.8342821524141671,
            "mae": 0.2280976452001422,
            "precision": 0.7716049382716049,
            "recall": 0.8029978586723768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(96)",
        "seed": 5740,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8137494348011126,
            "auditor_fn_violation": 0.015278828488057929,
            "auditor_fp_violation": 0.01536119711042312,
            "ave_precision_score": 0.8139153903899952,
            "fpr": 0.14583333333333334,
            "logloss": 0.83612876799989,
            "mae": 0.2834766293820575,
            "precision": 0.7381889763779528,
            "recall": 0.7700205338809035
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7929249311399571,
            "auditor_fn_violation": 0.010490389881463062,
            "auditor_fp_violation": 0.011043700121636456,
            "ave_precision_score": 0.7933781734982178,
            "fpr": 0.15477497255762898,
            "logloss": 0.8786170300224035,
            "mae": 0.28349645048491356,
            "precision": 0.7202380952380952,
            "recall": 0.7773019271948608
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(97)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.810422069166118,
            "auditor_fn_violation": 0.015603047660218304,
            "auditor_fp_violation": 0.0216795665634675,
            "ave_precision_score": 0.8089184784649863,
            "fpr": 0.13596491228070176,
            "logloss": 0.8363396401090739,
            "mae": 0.27766122671134025,
            "precision": 0.75,
            "recall": 0.7638603696098563
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7919842078912739,
            "auditor_fn_violation": 0.013922155336747868,
            "auditor_fp_violation": 0.025662325333016888,
            "ave_precision_score": 0.7878077407913066,
            "fpr": 0.15148188803512624,
            "logloss": 0.9462799695824181,
            "mae": 0.28633583941367585,
            "precision": 0.724,
            "recall": 0.7751605995717344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(98)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8417429008524131,
            "auditor_fn_violation": 0.0294566627039879,
            "auditor_fp_violation": 0.02611197110423117,
            "ave_precision_score": 0.8423258231674216,
            "fpr": 0.12280701754385964,
            "logloss": 0.8678451045315653,
            "mae": 0.2632695245636874,
            "precision": 0.7690721649484537,
            "recall": 0.7659137577002053
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8363163264389825,
            "auditor_fn_violation": 0.01894052468403078,
            "auditor_fp_violation": 0.02768959958861167,
            "ave_precision_score": 0.836789642333428,
            "fpr": 0.12294182217343579,
            "logloss": 0.8380837884812411,
            "mae": 0.262693895249267,
            "precision": 0.7617021276595745,
            "recall": 0.7665952890792291
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_lex",
        "model": "feat_lex:archive(99)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8568456700459185,
            "auditor_fn_violation": 0.01684588782016643,
            "auditor_fp_violation": 0.019850361197110433,
            "ave_precision_score": 0.857381583518148,
            "fpr": 0.14035087719298245,
            "logloss": 0.5892911462329226,
            "mae": 0.2698971942714834,
            "precision": 0.752895752895753,
            "recall": 0.8008213552361396
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8602161913795181,
            "auditor_fn_violation": 0.010173069103063441,
            "auditor_fp_violation": 0.024297623639006734,
            "ave_precision_score": 0.8604045515698483,
            "fpr": 0.15477497255762898,
            "logloss": 0.5884953561679366,
            "mae": 0.26662157561604966,
            "precision": 0.7283236994219653,
            "recall": 0.8094218415417559
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(0)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.7077629665848868,
            "auditor_fn_violation": 0.013173851989641465,
            "auditor_fp_violation": 0.028559653193308113,
            "ave_precision_score": 0.709626045225052,
            "fpr": 0.3081140350877193,
            "logloss": 0.8715435571519855,
            "mae": 0.49133753727533314,
            "precision": 0.6091794158553546,
            "recall": 0.9106029106029107
        },
        "train": {
            "accuracy": 0.6454445664105378,
            "auc_prc": 0.6883572503157852,
            "auditor_fn_violation": 0.009533468089105903,
            "auditor_fp_violation": 0.031304352184613235,
            "ave_precision_score": 0.6902406971228285,
            "fpr": 0.3084522502744237,
            "logloss": 0.7945866079956051,
            "mae": 0.49159700921547533,
            "precision": 0.6053370786516854,
            "recall": 0.9112050739957717
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(1)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.5426163803930274,
            "auditor_fn_violation": 0.032753401174453814,
            "auditor_fp_violation": 0.028226380917490943,
            "ave_precision_score": 0.5452755578023127,
            "fpr": 0.06907894736842106,
            "logloss": 0.6982484392914301,
            "mae": 0.4994617667898797,
            "precision": 0.5116279069767442,
            "recall": 0.13721413721413722
        },
        "train": {
            "accuracy": 0.5148188803512623,
            "auc_prc": 0.559744745420508,
            "auditor_fn_violation": 0.0388045569420496,
            "auditor_fp_violation": 0.023928745069144754,
            "ave_precision_score": 0.5553726005152735,
            "fpr": 0.05817782656421515,
            "logloss": 0.6945179381375036,
            "mae": 0.4973656996741122,
            "precision": 0.6131386861313869,
            "recall": 0.17758985200845667
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(2)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.554408329523054,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5136225154417959,
            "fpr": 0.010964912280701754,
            "logloss": 0.697385883122584,
            "mae": 0.4997180726117732,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5706850760295197,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.507769714101269,
            "fpr": 0.012074643249176729,
            "logloss": 0.6946476169175971,
            "mae": 0.49804210757711975,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(3)",
        "seed": 24481,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.5107939417149943,
            "auditor_fn_violation": 0.0014726264726264838,
            "auditor_fp_violation": 0.0015747751048154034,
            "ave_precision_score": 0.5273523891944945,
            "fpr": 0.005482456140350877,
            "logloss": 0.6939043237347584,
            "mae": 0.5002899381675219,
            "precision": 0.5454545454545454,
            "recall": 0.012474012474012475
        },
        "train": {
            "accuracy": 0.48737650933040616,
            "auc_prc": 0.5797661082822418,
            "auditor_fn_violation": 0.002462271091173659,
            "auditor_fp_violation": 0.0015187284784145079,
            "ave_precision_score": 0.5256602745530068,
            "fpr": 0.0043907793633369925,
            "logloss": 0.6929369643527594,
            "mae": 0.499795141551157,
            "precision": 0.7142857142857143,
            "recall": 0.021141649048625793
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(4)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5543543586339871,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5135667016414837,
            "fpr": 0.010964912280701754,
            "logloss": 0.6973080942476612,
            "mae": 0.49984571804995076,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5706449654586428,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5077309541516417,
            "fpr": 0.012074643249176729,
            "logloss": 0.6942974560634572,
            "mae": 0.49810964539728886,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(5)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5119585103673641,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5125917934179435,
            "fpr": 0.010964912280701754,
            "logloss": 0.6995257597149876,
            "mae": 0.5007283596419975,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5336654553610125,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5344468264447888,
            "fpr": 0.012074643249176729,
            "logloss": 0.6961854453893723,
            "mae": 0.4987170636392189,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(6)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.869922759769302,
            "auditor_fn_violation": 0.009690611664295877,
            "auditor_fp_violation": 0.01431544348109252,
            "ave_precision_score": 0.8701820410167342,
            "fpr": 0.16337719298245615,
            "logloss": 0.5529315620777377,
            "mae": 0.26970330644102786,
            "precision": 0.7344028520499108,
            "recall": 0.8565488565488566
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8576117670972416,
            "auditor_fn_violation": 0.008359189887283219,
            "auditor_fp_violation": 0.02263055801993896,
            "ave_precision_score": 0.8578375474448743,
            "fpr": 0.17892425905598244,
            "logloss": 0.572672221369284,
            "mae": 0.2769736679600302,
            "precision": 0.7170138888888888,
            "recall": 0.8731501057082452
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(7)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5698275704854652,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5296759309917205,
            "fpr": 0.010964912280701754,
            "logloss": 0.6993602068522764,
            "mae": 0.5002179139930951,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5887924680501528,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5227053071021472,
            "fpr": 0.012074643249176729,
            "logloss": 0.6966451458198328,
            "mae": 0.4984878221798676,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(8)",
        "seed": 24481,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.551465913175207,
            "auditor_fn_violation": 0.0018875150454097976,
            "auditor_fp_violation": 0.0010634184068058778,
            "ave_precision_score": 0.5532607703479824,
            "fpr": 0.0021929824561403508,
            "logloss": 0.7466085604287218,
            "mae": 0.5046775672054266,
            "precision": 0.6,
            "recall": 0.006237006237006237
        },
        "train": {
            "accuracy": 0.4862788144895719,
            "auc_prc": 0.5778178609898115,
            "auditor_fn_violation": 0.0016059298728484312,
            "auditor_fp_violation": 0.0005012305209288804,
            "ave_precision_score": 0.5795066529443649,
            "fpr": 0.0010976948408342481,
            "logloss": 0.7351835876132469,
            "mae": 0.5018052039722913,
            "precision": 0.8571428571428571,
            "recall": 0.012684989429175475
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(9)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5543965002392045,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5136094715653251,
            "fpr": 0.010964912280701754,
            "logloss": 0.6970767529959399,
            "mae": 0.4997686402578103,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5706825576164675,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5077671958388413,
            "fpr": 0.012074643249176729,
            "logloss": 0.6943741448388426,
            "mae": 0.4981443814597983,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(10)",
        "seed": 24481,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.5324155487752389,
            "auditor_fn_violation": 0.005512091038406827,
            "auditor_fp_violation": 0.006271115724345669,
            "ave_precision_score": 0.5480119359158289,
            "fpr": 0.05263157894736842,
            "logloss": 0.6932085969956959,
            "mae": 0.49987874395753207,
            "precision": 0.5752212389380531,
            "recall": 0.13513513513513514
        },
        "train": {
            "accuracy": 0.47859495060373214,
            "auc_prc": 0.5200726333686986,
            "auditor_fn_violation": 0.007491245129414292,
            "auditor_fp_violation": 0.005952112436030459,
            "ave_precision_score": 0.5218755134271363,
            "fpr": 0.06476399560922064,
            "logloss": 0.6935260879713826,
            "mae": 0.5000438652237474,
            "precision": 0.49137931034482757,
            "recall": 0.12050739957716702
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(11)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8685997785782802,
            "auditor_fn_violation": 0.00705310573731627,
            "auditor_fp_violation": 0.006528066104937518,
            "ave_precision_score": 0.8688712925253173,
            "fpr": 0.02850877192982456,
            "logloss": 0.610793679427821,
            "mae": 0.32040608090768763,
            "precision": 0.9090909090909091,
            "recall": 0.5405405405405406
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.85632136196694,
            "auditor_fn_violation": 0.007816144236637944,
            "auditor_fp_violation": 0.008398117378163393,
            "ave_precision_score": 0.8565319653345573,
            "fpr": 0.03402854006586169,
            "logloss": 0.6105771940195778,
            "mae": 0.32720805891088195,
            "precision": 0.8851851851851852,
            "recall": 0.5052854122621564
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(12)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.512186798711872,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5130160191444978,
            "fpr": 0.010964912280701754,
            "logloss": 0.702428485987862,
            "mae": 0.5013915494476494,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5343497082360025,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5351404884286393,
            "fpr": 0.012074643249176729,
            "logloss": 0.6986238215672987,
            "mae": 0.49916967357029374,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(13)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5501076980807802,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5519317559188104,
            "fpr": 0.010964912280701754,
            "logloss": 0.6980885860984786,
            "mae": 0.5000690566539242,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5691572942858717,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5706799411674187,
            "fpr": 0.012074643249176729,
            "logloss": 0.6951911016138318,
            "mae": 0.4982594182606455,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(14)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8794133885063747,
            "auditor_fn_violation": 0.005325163219900063,
            "auditor_fp_violation": 0.018169699189970287,
            "ave_precision_score": 0.8796664559362324,
            "fpr": 0.17982456140350878,
            "logloss": 0.5224206939735032,
            "mae": 0.2834237500200081,
            "precision": 0.7262103505843072,
            "recall": 0.9043659043659044
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8670663676897357,
            "auditor_fn_violation": 0.006558320550100604,
            "auditor_fp_violation": 0.023051591657519223,
            "ave_precision_score": 0.8672661260715934,
            "fpr": 0.19758507135016465,
            "logloss": 0.5483917057826709,
            "mae": 0.29727755688564317,
            "precision": 0.7009966777408638,
            "recall": 0.8921775898520085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(15)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5710749717328665,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5296759309917205,
            "fpr": 0.010964912280701754,
            "logloss": 0.6974284225852073,
            "mae": 0.4999129138233369,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5923501756588733,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5245353100574315,
            "fpr": 0.012074643249176729,
            "logloss": 0.6949708807714398,
            "mae": 0.4983571810335019,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(16)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.7209775854431395,
            "auditor_fn_violation": 0.0002917897654739765,
            "auditor_fp_violation": 0.017195322994260593,
            "ave_precision_score": 0.7036352487681539,
            "fpr": 0.36403508771929827,
            "logloss": 2.998287921011428,
            "mae": 0.3768932952671951,
            "precision": 0.5855181023720349,
            "recall": 0.975051975051975
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.690993654051996,
            "auditor_fn_violation": 0.004662302188659629,
            "auditor_fp_violation": 0.017823757324230995,
            "ave_precision_score": 0.6708922276148381,
            "fpr": 0.3534577387486279,
            "logloss": 3.1791891631672136,
            "mae": 0.37431618921667215,
            "precision": 0.5855855855855856,
            "recall": 0.9619450317124736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(17)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5710749717328665,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5296759309917205,
            "fpr": 0.010964912280701754,
            "logloss": 0.6974284225852073,
            "mae": 0.4999129138233369,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5923501756588733,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5245353100574315,
            "fpr": 0.012074643249176729,
            "logloss": 0.6949708807714398,
            "mae": 0.4983571810335019,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(18)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7730263157894737,
            "auc_prc": 0.8794031297635194,
            "auditor_fn_violation": 0.006086552139183719,
            "auditor_fp_violation": 0.014618186998819567,
            "ave_precision_score": 0.8796599346180294,
            "fpr": 0.16557017543859648,
            "logloss": 0.5040496632295356,
            "mae": 0.2800354975841943,
            "precision": 0.7378472222222222,
            "recall": 0.8835758835758836
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8651194572637294,
            "auditor_fn_violation": 0.009930309141500527,
            "auditor_fp_violation": 0.022943827095519505,
            "ave_precision_score": 0.8653233454647148,
            "fpr": 0.19209659714599342,
            "logloss": 0.5342921125633747,
            "mae": 0.29651263590061755,
            "precision": 0.7048903878583473,
            "recall": 0.8837209302325582
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(19)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.6510522880170685,
            "auditor_fn_violation": 0.0011762774920669765,
            "auditor_fp_violation": 0.006879146822973909,
            "ave_precision_score": 0.6670837619266339,
            "fpr": 0.020833333333333332,
            "logloss": 0.7830513473029156,
            "mae": 0.45888062621374875,
            "precision": 0.5581395348837209,
            "recall": 0.0498960498960499
        },
        "train": {
            "accuracy": 0.4950603732162459,
            "auc_prc": 0.6591192938501408,
            "auditor_fn_violation": 0.0044511177689642535,
            "auditor_fp_violation": 0.003849450400733802,
            "ave_precision_score": 0.6656777100478444,
            "fpr": 0.013172338090010977,
            "logloss": 0.7526977515132581,
            "mae": 0.45192918417007055,
            "precision": 0.6756756756756757,
            "recall": 0.052854122621564484
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(20)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.7565589932158362,
            "auditor_fn_violation": 0.00970656891709525,
            "auditor_fp_violation": 0.005558778035576179,
            "ave_precision_score": 0.7598069032355302,
            "fpr": 0.0581140350877193,
            "logloss": 0.8822935274985868,
            "mae": 0.3768592739515793,
            "precision": 0.8239202657807309,
            "recall": 0.5155925155925156
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7923428312854929,
            "auditor_fn_violation": 0.006567603381735565,
            "auditor_fp_violation": 0.0042930394117558605,
            "ave_precision_score": 0.7776880636655077,
            "fpr": 0.04720087815587267,
            "logloss": 0.8326145801860626,
            "mae": 0.3652433625603367,
            "precision": 0.8436363636363636,
            "recall": 0.4904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(21)",
        "seed": 24481,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.6801500449718441,
            "auditor_fn_violation": 0.0010098661414450955,
            "auditor_fp_violation": 0.0020199861602963327,
            "ave_precision_score": 0.681940603734127,
            "fpr": 0.008771929824561403,
            "logloss": 0.6939608671236791,
            "mae": 0.5003110404338753,
            "precision": 0.5294117647058824,
            "recall": 0.018711018711018712
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.6320630660523365,
            "auditor_fn_violation": 0.003037806652541305,
            "auditor_fp_violation": 0.004145176408081842,
            "ave_precision_score": 0.6332100059580501,
            "fpr": 0.010976948408342482,
            "logloss": 0.6931160160634133,
            "mae": 0.49987825950859144,
            "precision": 0.5238095238095238,
            "recall": 0.023255813953488372
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(22)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5032894736842105,
            "auc_prc": 0.689087982905295,
            "auditor_fn_violation": 0.004126089652405475,
            "auditor_fp_violation": 0.006484817030976513,
            "ave_precision_score": 0.6988712093235134,
            "fpr": 0.023026315789473683,
            "logloss": 0.85636372212544,
            "mae": 0.4935422485465543,
            "precision": 0.7,
            "recall": 0.10187110187110188
        },
        "train": {
            "accuracy": 0.5115257958287596,
            "auc_prc": 0.6628087721398503,
            "auditor_fn_violation": 0.0027732459509448924,
            "auditor_fp_violation": 0.002872050884922487,
            "ave_precision_score": 0.6800809189503092,
            "fpr": 0.026344676180021953,
            "logloss": 0.755798574440739,
            "mae": 0.49286674723667057,
            "precision": 0.6842105263157895,
            "recall": 0.10993657505285412
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(23)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.7424361531504038,
            "auditor_fn_violation": 0.011824324324324334,
            "auditor_fp_violation": 0.00961910286156226,
            "ave_precision_score": 0.7505234327703509,
            "fpr": 0.051535087719298246,
            "logloss": 1.0329884286148836,
            "mae": 0.41605881211957274,
            "precision": 0.8246268656716418,
            "recall": 0.4594594594594595
        },
        "train": {
            "accuracy": 0.6717892425905598,
            "auc_prc": 0.7839534239636783,
            "auditor_fn_violation": 0.005616113139151977,
            "auditor_fp_violation": 0.004646406929010724,
            "ave_precision_score": 0.7724575142997513,
            "fpr": 0.042810098792535674,
            "logloss": 0.9910948981632544,
            "mae": 0.40465076053532967,
            "precision": 0.8452380952380952,
            "recall": 0.4503171247357294
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(24)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5516753942823522,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5527423007636325,
            "fpr": 0.010964912280701754,
            "logloss": 0.6979840761247289,
            "mae": 0.500286772062904,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5580268119180302,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5625350727808275,
            "fpr": 0.012074643249176729,
            "logloss": 0.6948667066061784,
            "mae": 0.4984505869750788,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(25)",
        "seed": 24481,
        "test": {
            "accuracy": 0.49780701754385964,
            "auc_prc": 0.5480449151585407,
            "auditor_fn_violation": 0.005149633439107148,
            "auditor_fp_violation": 0.009387593112712174,
            "ave_precision_score": 0.5496235230006483,
            "fpr": 0.05921052631578947,
            "logloss": 0.693549613117673,
            "mae": 0.5000274102612022,
            "precision": 0.5877862595419847,
            "recall": 0.1600831600831601
        },
        "train": {
            "accuracy": 0.4840834248079034,
            "auc_prc": 0.519626903996169,
            "auditor_fn_violation": 0.006335532590861531,
            "auditor_fp_violation": 0.007834233042118404,
            "ave_precision_score": 0.521347464979546,
            "fpr": 0.06586169045005488,
            "logloss": 0.6935410250425462,
            "mae": 0.5000299493057406,
            "precision": 0.5121951219512195,
            "recall": 0.1331923890063425
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(26)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.7185876333328445,
            "auditor_fn_violation": 0.0038502571397308256,
            "auditor_fp_violation": 0.02413298327024059,
            "ave_precision_score": 0.7001798387469971,
            "fpr": 0.28728070175438597,
            "logloss": 2.468317715040162,
            "mae": 0.3375349406830046,
            "precision": 0.6257142857142857,
            "recall": 0.9106029106029107
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.6835685615968174,
            "auditor_fn_violation": 0.006764863553978507,
            "auditor_fp_violation": 0.02217945055110297,
            "ave_precision_score": 0.6628273169907424,
            "fpr": 0.27332601536772777,
            "logloss": 2.7573930723545677,
            "mae": 0.3271791429079919,
            "precision": 0.6327433628318584,
            "recall": 0.9069767441860465
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(27)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.5426163803930274,
            "auditor_fn_violation": 0.032753401174453814,
            "auditor_fp_violation": 0.028226380917490943,
            "ave_precision_score": 0.5452755578023127,
            "fpr": 0.06907894736842106,
            "logloss": 0.6982479228028695,
            "mae": 0.4994609674863648,
            "precision": 0.5116279069767442,
            "recall": 0.13721413721413722
        },
        "train": {
            "accuracy": 0.5148188803512623,
            "auc_prc": 0.559744745420508,
            "auditor_fn_violation": 0.0388045569420496,
            "auditor_fp_violation": 0.023928745069144754,
            "ave_precision_score": 0.5553726005152735,
            "fpr": 0.05817782656421515,
            "logloss": 0.6945175730838052,
            "mae": 0.4973650086920557,
            "precision": 0.6131386861313869,
            "recall": 0.17758985200845667
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(28)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5133687896719046,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5144095166953111,
            "fpr": 0.010964912280701754,
            "logloss": 0.6992058341584447,
            "mae": 0.5005951261049822,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5367224346060366,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5375768615612115,
            "fpr": 0.012074643249176729,
            "logloss": 0.6959119369915242,
            "mae": 0.4986078402213548,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(29)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5021929824561403,
            "auc_prc": 0.6885862898833284,
            "auditor_fn_violation": 0.004230951599372667,
            "auditor_fp_violation": 0.006484817030976513,
            "ave_precision_score": 0.6984068544378847,
            "fpr": 0.023026315789473683,
            "logloss": 0.8541840540665159,
            "mae": 0.49295616192383723,
            "precision": 0.6956521739130435,
            "recall": 0.0997920997920998
        },
        "train": {
            "accuracy": 0.5104281009879253,
            "auc_prc": 0.6661582563414514,
            "auditor_fn_violation": 0.005128764478316477,
            "auditor_fp_violation": 0.002872050884922487,
            "ave_precision_score": 0.6830946748702693,
            "fpr": 0.026344676180021953,
            "logloss": 0.7528754936881499,
            "mae": 0.4919874345889861,
            "precision": 0.68,
            "recall": 0.10782241014799154
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(30)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.7426288261582444,
            "auditor_fn_violation": 0.011824324324324334,
            "auditor_fp_violation": 0.00961910286156226,
            "ave_precision_score": 0.7507116552563612,
            "fpr": 0.051535087719298246,
            "logloss": 1.0278171995935865,
            "mae": 0.4149962924053206,
            "precision": 0.8246268656716418,
            "recall": 0.4594594594594595
        },
        "train": {
            "accuracy": 0.6717892425905598,
            "auc_prc": 0.7840034212211704,
            "auditor_fn_violation": 0.005616113139151977,
            "auditor_fp_violation": 0.004646406929010724,
            "ave_precision_score": 0.772507273235928,
            "fpr": 0.042810098792535674,
            "logloss": 0.9857434113214436,
            "mae": 0.40367377424362433,
            "precision": 0.8452380952380952,
            "recall": 0.4503171247357294
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(31)",
        "seed": 24481,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.757505084270063,
            "auditor_fn_violation": 0.001381442170915869,
            "auditor_fp_violation": 0.00029256726503032525,
            "ave_precision_score": 0.754505283720014,
            "fpr": 0.0021929824561403508,
            "logloss": 1.181324921043757,
            "mae": 0.44012952916850817,
            "precision": 0.6,
            "recall": 0.006237006237006237
        },
        "train": {
            "accuracy": 0.4862788144895719,
            "auc_prc": 0.7817042620438174,
            "auditor_fn_violation": 0.0017521344710990743,
            "auditor_fp_violation": 0.0012781378283686449,
            "ave_precision_score": 0.7702889732363469,
            "fpr": 0.0021953896816684962,
            "logloss": 1.141913232032447,
            "mae": 0.43015410480323724,
            "precision": 0.7777777777777778,
            "recall": 0.014799154334038054
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(32)",
        "seed": 24481,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.5016028100571814,
            "auditor_fn_violation": 0.0018875150454097976,
            "auditor_fp_violation": 0.0022489518459722396,
            "ave_precision_score": 0.5118869397470183,
            "fpr": 0.0043859649122807015,
            "logloss": 0.698002417773363,
            "mae": 0.5014838337767542,
            "precision": 0.42857142857142855,
            "recall": 0.006237006237006237
        },
        "train": {
            "accuracy": 0.4829857299670692,
            "auc_prc": 0.49046731056241877,
            "auditor_fn_violation": 0.0016221748282096208,
            "auditor_fp_violation": 0.0007869319178583422,
            "ave_precision_score": 0.4984344777114033,
            "fpr": 0.005488474204171241,
            "logloss": 0.6961577253481165,
            "mae": 0.5003094247549478,
            "precision": 0.5833333333333334,
            "recall": 0.014799154334038054
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(33)",
        "seed": 24481,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.5030649095570395,
            "auditor_fn_violation": 0.0018875150454097976,
            "auditor_fp_violation": 0.0010634184068058778,
            "ave_precision_score": 0.5307688617125177,
            "fpr": 0.0021929824561403508,
            "logloss": 0.6962406132215461,
            "mae": 0.5009472068203124,
            "precision": 0.6,
            "recall": 0.006237006237006237
        },
        "train": {
            "accuracy": 0.4862788144895719,
            "auc_prc": 0.6008863691367926,
            "auditor_fn_violation": 0.0016059298728484312,
            "auditor_fp_violation": 0.0005012305209288804,
            "ave_precision_score": 0.5284615533344935,
            "fpr": 0.0010976948408342481,
            "logloss": 0.6938708530999734,
            "mae": 0.49972430113641675,
            "precision": 0.8571428571428571,
            "recall": 0.012684989429175475
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(34)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8578139766354473,
            "auditor_fn_violation": 0.008587281613597411,
            "auditor_fp_violation": 0.017905116619855905,
            "ave_precision_score": 0.8580894574881993,
            "fpr": 0.13815789473684212,
            "logloss": 0.8787270439276563,
            "mae": 0.27667751755176906,
            "precision": 0.7464788732394366,
            "recall": 0.7713097713097713
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8515284992197782,
            "auditor_fn_violation": 0.009080930046901511,
            "auditor_fp_violation": 0.021405049396267844,
            "ave_precision_score": 0.8517446558486599,
            "fpr": 0.15477497255762898,
            "logloss": 0.8646330589418901,
            "mae": 0.2765463196990445,
            "precision": 0.7309160305343512,
            "recall": 0.8097251585623678
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(35)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8500572025233719,
            "auditor_fn_violation": 0.009633621475726746,
            "auditor_fp_violation": 0.015022693043513656,
            "ave_precision_score": 0.8503588120026155,
            "fpr": 0.17543859649122806,
            "logloss": 0.5998419154049787,
            "mae": 0.3018033272208328,
            "precision": 0.7163120567375887,
            "recall": 0.83991683991684
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8402106906287263,
            "auditor_fn_violation": 0.007484283005688057,
            "auditor_fp_violation": 0.024089138835842,
            "ave_precision_score": 0.8396603226580736,
            "fpr": 0.1690450054884742,
            "logloss": 0.6410356372240097,
            "mae": 0.3053481683210995,
            "precision": 0.727433628318584,
            "recall": 0.86892177589852
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(36)",
        "seed": 24481,
        "test": {
            "accuracy": 0.5076754385964912,
            "auc_prc": 0.6818518986874275,
            "auditor_fn_violation": 0.0010167049640733895,
            "auditor_fp_violation": 0.010077034232914074,
            "ave_precision_score": 0.6861965722579344,
            "fpr": 0.041666666666666664,
            "logloss": 0.7679577936087142,
            "mae": 0.4888453028050431,
            "precision": 0.6481481481481481,
            "recall": 0.14553014553014554
        },
        "train": {
            "accuracy": 0.4983534577387486,
            "auc_prc": 0.6422882981516453,
            "auditor_fn_violation": 0.001564157130491097,
            "auditor_fp_violation": 0.013407916434847553,
            "ave_precision_score": 0.6490822750900841,
            "fpr": 0.050493962678375415,
            "logloss": 0.7349784978739206,
            "mae": 0.4923806612826598,
            "precision": 0.5740740740740741,
            "recall": 0.13107822410147993
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(37)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5754449653098301,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.539667065342741,
            "fpr": 0.010964912280701754,
            "logloss": 0.6928631017244399,
            "mae": 0.495214621289437,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5913085069486378,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.526140824301129,
            "fpr": 0.012074643249176729,
            "logloss": 0.6978889227027104,
            "mae": 0.4972872190346407,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(38)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.6649855929738805,
            "auditor_fn_violation": 0.016401776270197326,
            "auditor_fp_violation": 0.026112264419750068,
            "ave_precision_score": 0.6750673207307591,
            "fpr": 0.18859649122807018,
            "logloss": 0.6431882751104401,
            "mae": 0.4385562683373951,
            "precision": 0.6766917293233082,
            "recall": 0.7484407484407485
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.6716984940774747,
            "auditor_fn_violation": 0.019496267141328794,
            "auditor_fp_violation": 0.01847535700143853,
            "ave_precision_score": 0.6730970427432156,
            "fpr": 0.19978046103183314,
            "logloss": 0.636716635238956,
            "mae": 0.43558489712022663,
            "precision": 0.6623376623376623,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(39)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.7182015976275561,
            "auditor_fn_violation": 0.004121530437319911,
            "auditor_fp_violation": 0.02323492896975618,
            "ave_precision_score": 0.7003785809138908,
            "fpr": 0.28399122807017546,
            "logloss": 2.4520796305835417,
            "mae": 0.3364099066006677,
            "precision": 0.6289398280802292,
            "recall": 0.9126819126819127
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6842840839969317,
            "auditor_fn_violation": 0.006764863553978507,
            "auditor_fp_violation": 0.022535324220962463,
            "ave_precision_score": 0.6635560726865215,
            "fpr": 0.2722283205268935,
            "logloss": 2.7430929280132084,
            "mae": 0.3266313496375671,
            "precision": 0.6336779911373708,
            "recall": 0.9069767441860465
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(40)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.7187652437306324,
            "auditor_fn_violation": 0.004771218587008061,
            "auditor_fp_violation": 0.021937456750926038,
            "ave_precision_score": 0.7006647293477626,
            "fpr": 0.3366228070175439,
            "logloss": 2.7621787644300215,
            "mae": 0.3606024259658043,
            "precision": 0.5997392438070405,
            "recall": 0.9563409563409564
        },
        "train": {
            "accuracy": 0.6575192096597146,
            "auc_prc": 0.684969309066193,
            "auditor_fn_violation": 0.00687857824150679,
            "auditor_fp_violation": 0.025542707346535746,
            "ave_precision_score": 0.6638779009786954,
            "fpr": 0.3106476399560922,
            "logloss": 3.019551733603688,
            "mae": 0.3500613808264743,
            "precision": 0.6107290233837689,
            "recall": 0.9386892177589852
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(41)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8687818198129529,
            "auditor_fn_violation": 0.008844877265929913,
            "auditor_fp_violation": 0.006528066104937518,
            "ave_precision_score": 0.8690460647452372,
            "fpr": 0.02850877192982456,
            "logloss": 0.6085454921775297,
            "mae": 0.31923972041860044,
            "precision": 0.9094076655052264,
            "recall": 0.5426195426195426
        },
        "train": {
            "accuracy": 0.7102085620197585,
            "auc_prc": 0.8563180060138529,
            "auditor_fn_violation": 0.007702429549109671,
            "auditor_fp_violation": 0.007819196126490537,
            "ave_precision_score": 0.8565291243802926,
            "fpr": 0.03512623490669594,
            "logloss": 0.6081998180422327,
            "mae": 0.3259912774725342,
            "precision": 0.8827838827838828,
            "recall": 0.5095137420718816
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(42)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.513744463166093,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5145940592479052,
            "fpr": 0.010964912280701754,
            "logloss": 0.6997991803551317,
            "mae": 0.5008170550787135,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.535810298405565,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5365670967274583,
            "fpr": 0.012074643249176729,
            "logloss": 0.6962659341452186,
            "mae": 0.49870839525263605,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(43)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6392543859649122,
            "auc_prc": 0.6533865401456536,
            "auditor_fn_violation": 0.06476820950505162,
            "auditor_fp_violation": 0.03349767981438515,
            "ave_precision_score": 0.6723545757033345,
            "fpr": 0.1513157894736842,
            "logloss": 0.6553320582625941,
            "mae": 0.4418816800768438,
            "precision": 0.677570093457944,
            "recall": 0.6029106029106029
        },
        "train": {
            "accuracy": 0.6564215148188803,
            "auc_prc": 0.6746698618039101,
            "auditor_fn_violation": 0.05727971260353258,
            "auditor_fp_violation": 0.03925386824654527,
            "ave_precision_score": 0.6743151723207588,
            "fpr": 0.15477497255762898,
            "logloss": 0.6442363550178536,
            "mae": 0.43692161991750633,
            "precision": 0.6809954751131222,
            "recall": 0.6363636363636364
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(44)",
        "seed": 24481,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.5016028100571814,
            "auditor_fn_violation": 0.0018875150454097976,
            "auditor_fp_violation": 0.0022489518459722396,
            "ave_precision_score": 0.5118869397470183,
            "fpr": 0.0043859649122807015,
            "logloss": 0.6980024179905726,
            "mae": 0.5014838337767542,
            "precision": 0.42857142857142855,
            "recall": 0.006237006237006237
        },
        "train": {
            "accuracy": 0.4829857299670692,
            "auc_prc": 0.49046731056241877,
            "auditor_fn_violation": 0.0016221748282096208,
            "auditor_fp_violation": 0.0007869319178583422,
            "ave_precision_score": 0.4984344777114033,
            "fpr": 0.005488474204171241,
            "logloss": 0.69615772521207,
            "mae": 0.5003094245913785,
            "precision": 0.5833333333333334,
            "recall": 0.014799154334038054
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(45)",
        "seed": 24481,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8545724542854772,
            "auditor_fn_violation": 0.005145074224021599,
            "auditor_fp_violation": 0.019131355069809095,
            "ave_precision_score": 0.854929479811871,
            "fpr": 0.12171052631578948,
            "logloss": 0.7237453448371188,
            "mae": 0.27682190105548343,
            "precision": 0.7692307692307693,
            "recall": 0.7692307692307693
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8543476128391546,
            "auditor_fn_violation": 0.00919464473442979,
            "auditor_fp_violation": 0.017758597356510232,
            "ave_precision_score": 0.8545452160532921,
            "fpr": 0.14709110867178923,
            "logloss": 0.70689553808691,
            "mae": 0.2791069181868934,
            "precision": 0.7341269841269841,
            "recall": 0.7822410147991543
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(46)",
        "seed": 24481,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.6776966839369716,
            "auditor_fn_violation": 0.0018875150454097976,
            "auditor_fp_violation": 0.0022489518459722396,
            "ave_precision_score": 0.6797627443162282,
            "fpr": 0.0043859649122807015,
            "logloss": 0.6979853964553253,
            "mae": 0.5014930437073896,
            "precision": 0.42857142857142855,
            "recall": 0.006237006237006237
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.6242065924068567,
            "auditor_fn_violation": 0.0016221748282096208,
            "auditor_fp_violation": 0.00010525840939506514,
            "ave_precision_score": 0.6255402886463794,
            "fpr": 0.006586169045005488,
            "logloss": 0.6962184583459357,
            "mae": 0.5003741838275143,
            "precision": 0.5384615384615384,
            "recall": 0.014799154334038054
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(47)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6502192982456141,
            "auc_prc": 0.7127024666037701,
            "auditor_fn_violation": 0.005452821242294927,
            "auditor_fp_violation": 0.018416473317865438,
            "ave_precision_score": 0.6951300409084976,
            "fpr": 0.32127192982456143,
            "logloss": 2.6892700495616135,
            "mae": 0.35121785777778836,
            "precision": 0.6082887700534759,
            "recall": 0.9459459459459459
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.6771319461394625,
            "auditor_fn_violation": 0.007133856111468244,
            "auditor_fp_violation": 0.024886095364118906,
            "ave_precision_score": 0.6564408621672868,
            "fpr": 0.29308452250274425,
            "logloss": 2.962370724555749,
            "mae": 0.341792874100737,
            "precision": 0.6212765957446809,
            "recall": 0.9260042283298098
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(48)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.572724574427532,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5331186427192482,
            "fpr": 0.010964912280701754,
            "logloss": 0.7051339754113745,
            "mae": 0.4967887654371931,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5937222942099161,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5272795471595172,
            "fpr": 0.012074643249176729,
            "logloss": 0.6923962032244905,
            "mae": 0.4955751208636144,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(49)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8712849830180347,
            "auditor_fn_violation": 0.007055385344859031,
            "auditor_fp_violation": 0.020945272112997113,
            "ave_precision_score": 0.8717385747407687,
            "fpr": 0.17434210526315788,
            "logloss": 0.5214632104865078,
            "mae": 0.27913123874193907,
            "precision": 0.7286689419795221,
            "recall": 0.8877338877338877
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8637944067556799,
            "auditor_fn_violation": 0.009777142419523651,
            "auditor_fp_violation": 0.021151927983198763,
            "ave_precision_score": 0.8639957409965647,
            "fpr": 0.19319429198682767,
            "logloss": 0.5460873281829243,
            "mae": 0.2908184586816624,
            "precision": 0.704201680672269,
            "recall": 0.8858350951374208
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(50)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8696488190618414,
            "auditor_fn_violation": 0.009690611664295877,
            "auditor_fp_violation": 0.01431544348109252,
            "ave_precision_score": 0.8699086584604425,
            "fpr": 0.16337719298245615,
            "logloss": 0.5541552734321215,
            "mae": 0.26982017601199404,
            "precision": 0.7344028520499108,
            "recall": 0.8565488565488566
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8575847175407334,
            "auditor_fn_violation": 0.008359189887283219,
            "auditor_fp_violation": 0.02263055801993896,
            "ave_precision_score": 0.8578093382073556,
            "fpr": 0.17892425905598244,
            "logloss": 0.5739908034737863,
            "mae": 0.2772084919505259,
            "precision": 0.7170138888888888,
            "recall": 0.8731501057082452
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(51)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8417630146162394,
            "auditor_fn_violation": 0.010677681730313308,
            "auditor_fp_violation": 0.01791020474620426,
            "ave_precision_score": 0.8422105290591066,
            "fpr": 0.14473684210526316,
            "logloss": 0.8196547882541347,
            "mae": 0.2656855154501939,
            "precision": 0.7480916030534351,
            "recall": 0.814968814968815
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8224808803955357,
            "auditor_fn_violation": 0.010160059224465834,
            "auditor_fp_violation": 0.019510398027156678,
            "ave_precision_score": 0.8227938505028303,
            "fpr": 0.16575192096597147,
            "logloss": 0.8830067980433403,
            "mae": 0.2755782381072812,
            "precision": 0.7182835820895522,
            "recall": 0.813953488372093
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(52)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.7780162890923976,
            "auditor_fn_violation": 0.001381442170915869,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.7656360626209766,
            "fpr": 0.0,
            "logloss": 1.1827848123422913,
            "mae": 0.43933429091671233,
            "precision": 1.0,
            "recall": 0.006237006237006237
        },
        "train": {
            "accuracy": 0.4818880351262349,
            "auc_prc": 0.7788168206908488,
            "auditor_fn_violation": 0.000761192194066886,
            "auditor_fp_violation": 0.0012781378283686449,
            "ave_precision_score": 0.7689817402526044,
            "fpr": 0.0021953896816684962,
            "logloss": 1.1473237500916291,
            "mae": 0.43176910060612,
            "precision": 0.6,
            "recall": 0.006342494714587738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(53)",
        "seed": 24481,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.7462127087923038,
            "auditor_fn_violation": 0.0010668563300142249,
            "auditor_fp_violation": 0.0017783001587495424,
            "ave_precision_score": 0.7547680162858157,
            "fpr": 0.003289473684210526,
            "logloss": 1.2520665495847931,
            "mae": 0.4480980501743225,
            "precision": 0.4,
            "recall": 0.004158004158004158
        },
        "train": {
            "accuracy": 0.48957189901207465,
            "auc_prc": 0.7836707978735791,
            "auditor_fn_violation": 0.0008656240499602157,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.7709313857099065,
            "fpr": 0.0,
            "logloss": 1.2124850948605284,
            "mae": 0.43827018840385407,
            "precision": 1.0,
            "recall": 0.016913319238900635
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(54)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5716364716364716,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5307989307989307,
            "fpr": 0.010964912280701754,
            "logloss": 0.6972564933023977,
            "mae": 0.49921697858083713,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5928972117016652,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5256293821430155,
            "fpr": 0.012074643249176729,
            "logloss": 0.6945455115452835,
            "mae": 0.49754871811958934,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(55)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7185174983656677,
            "auditor_fn_violation": 0.0038502571397308256,
            "auditor_fp_violation": 0.023181503643098463,
            "ave_precision_score": 0.7004913255595764,
            "fpr": 0.28618421052631576,
            "logloss": 2.459589099187434,
            "mae": 0.33674574770286964,
            "precision": 0.6266094420600858,
            "recall": 0.9106029106029107
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6841316803538331,
            "auditor_fn_violation": 0.006764863553978507,
            "auditor_fp_violation": 0.022535324220962463,
            "ave_precision_score": 0.6633738314617574,
            "fpr": 0.2722283205268935,
            "logloss": 2.7493467761030983,
            "mae": 0.32668834120719514,
            "precision": 0.6336779911373708,
            "recall": 0.9069767441860465
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(56)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5889624974164448,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.567554528212423,
            "fpr": 0.010964912280701754,
            "logloss": 0.6965428940047236,
            "mae": 0.499638620992763,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.611341417994645,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5680075987985622,
            "fpr": 0.012074643249176729,
            "logloss": 0.6942858095055756,
            "mae": 0.4981883196354436,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(57)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.7203265617538737,
            "auditor_fn_violation": 0.006711164605901451,
            "auditor_fp_violation": 0.021250559693898313,
            "ave_precision_score": 0.7015572230660568,
            "fpr": 0.28618421052631576,
            "logloss": 2.505374933298809,
            "mae": 0.33771520703630475,
            "precision": 0.6282051282051282,
            "recall": 0.9168399168399168
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6848478630941115,
            "auditor_fn_violation": 0.00832437926865211,
            "auditor_fp_violation": 0.020923868096176117,
            "ave_precision_score": 0.6636461675646796,
            "fpr": 0.27552140504939626,
            "logloss": 2.816379090981541,
            "mae": 0.3280912226669942,
            "precision": 0.6325036603221084,
            "recall": 0.9133192389006343
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(58)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7916666666666666,
            "auc_prc": 0.8770240248427675,
            "auditor_fn_violation": 0.0016960280118174917,
            "auditor_fp_violation": 0.008565860707453087,
            "ave_precision_score": 0.8772877015766818,
            "fpr": 0.10526315789473684,
            "logloss": 0.46091916496403085,
            "mae": 0.28564759670195616,
            "precision": 0.8012422360248447,
            "recall": 0.8045738045738046
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.8676931031932471,
            "auditor_fn_violation": 0.0025667029470669736,
            "auditor_fp_violation": 0.016751124009443185,
            "ave_precision_score": 0.8678775786780599,
            "fpr": 0.11855104281009879,
            "logloss": 0.47036043975072966,
            "mae": 0.2984909552837488,
            "precision": 0.780040733197556,
            "recall": 0.8097251585623678
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(59)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8496240956136518,
            "auditor_fn_violation": 0.006287157602947082,
            "auditor_fp_violation": 0.020299080066756226,
            "ave_precision_score": 0.8499178267149159,
            "fpr": 0.15789473684210525,
            "logloss": 0.7543071946206473,
            "mae": 0.2665883930893372,
            "precision": 0.7348066298342542,
            "recall": 0.8295218295218295
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8266345966861373,
            "auditor_fn_violation": 0.011378430876554588,
            "auditor_fp_violation": 0.020405094507014727,
            "ave_precision_score": 0.8269611930904406,
            "fpr": 0.17453347969264543,
            "logloss": 0.8088173643180114,
            "mae": 0.27116303093820066,
            "precision": 0.7170818505338078,
            "recall": 0.8520084566596194
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(60)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6206140350877193,
            "auc_prc": 0.7216192634491467,
            "auditor_fn_violation": 0.0034057336688915643,
            "auditor_fp_violation": 0.016925652297797862,
            "ave_precision_score": 0.704217054723341,
            "fpr": 0.36293859649122806,
            "logloss": 2.968887360259564,
            "mae": 0.37472225811791904,
            "precision": 0.5846925972396487,
            "recall": 0.9688149688149689
        },
        "train": {
            "accuracy": 0.6311745334796927,
            "auc_prc": 0.6912186518650221,
            "auditor_fn_violation": 0.005256403413297194,
            "auditor_fp_violation": 0.01699923311730298,
            "ave_precision_score": 0.6714187532304841,
            "fpr": 0.34577387486278816,
            "logloss": 3.151549707479301,
            "mae": 0.37136578515667956,
            "precision": 0.5893089960886571,
            "recall": 0.9556025369978859
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(61)",
        "seed": 24481,
        "test": {
            "accuracy": 0.47478070175438597,
            "auc_prc": 0.7542666898975272,
            "auditor_fn_violation": 0.0010668563300142249,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.7551122219939634,
            "fpr": 0.0,
            "logloss": 1.2658897562453089,
            "mae": 0.4489735932016756,
            "precision": 1.0,
            "recall": 0.004158004158004158
        },
        "train": {
            "accuracy": 0.48737650933040616,
            "auc_prc": 0.7783571203277122,
            "auditor_fn_violation": 0.0021327305681325165,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.7688876392252583,
            "fpr": 0.0,
            "logloss": 1.223781008557595,
            "mae": 0.43929316818045294,
            "precision": 1.0,
            "recall": 0.012684989429175475
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(62)",
        "seed": 24481,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.5339189115800062,
            "auditor_fn_violation": 0.005512091038406827,
            "auditor_fp_violation": 0.006271115724345669,
            "ave_precision_score": 0.5447563564879497,
            "fpr": 0.05263157894736842,
            "logloss": 0.6933330709753767,
            "mae": 0.4999411463279996,
            "precision": 0.5752212389380531,
            "recall": 0.13513513513513514
        },
        "train": {
            "accuracy": 0.47859495060373214,
            "auc_prc": 0.5287545791295172,
            "auditor_fn_violation": 0.007491245129414292,
            "auditor_fp_violation": 0.005952112436030459,
            "ave_precision_score": 0.52660768865802,
            "fpr": 0.06476399560922064,
            "logloss": 0.6935395388255486,
            "mae": 0.5000506215509023,
            "precision": 0.49137931034482757,
            "recall": 0.12050739957716702
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(63)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.721649566725125,
            "auditor_fn_violation": 0.003927763796184849,
            "auditor_fp_violation": 0.02353767248748321,
            "ave_precision_score": 0.703555614893687,
            "fpr": 0.34210526315789475,
            "logloss": 2.8179309103852983,
            "mae": 0.3631317185990128,
            "precision": 0.5984555984555985,
            "recall": 0.9667359667359667
        },
        "train": {
            "accuracy": 0.6487376509330406,
            "auc_prc": 0.687197095104644,
            "auditor_fn_violation": 0.006300721972230411,
            "auditor_fp_violation": 0.027076472740578124,
            "ave_precision_score": 0.6659740174245322,
            "fpr": 0.3205268935236004,
            "logloss": 3.07569832124627,
            "mae": 0.3564584722899815,
            "precision": 0.6037991858887382,
            "recall": 0.9408033826638478
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(64)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.6648623641994625,
            "auditor_fn_violation": 0.016401776270197326,
            "auditor_fp_violation": 0.026112264419750068,
            "ave_precision_score": 0.6756029029728016,
            "fpr": 0.18859649122807018,
            "logloss": 0.6429332658768597,
            "mae": 0.4386173008303893,
            "precision": 0.6766917293233082,
            "recall": 0.7484407484407485
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.6717690599415036,
            "auditor_fn_violation": 0.019496267141328794,
            "auditor_fp_violation": 0.01847535700143853,
            "ave_precision_score": 0.6730701226240343,
            "fpr": 0.19978046103183314,
            "logloss": 0.6366024599289744,
            "mae": 0.43572247101957007,
            "precision": 0.6623376623376623,
            "recall": 0.7547568710359408
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(65)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7741228070175439,
            "auc_prc": 0.8795779154494772,
            "auditor_fn_violation": 0.005042491884597149,
            "auditor_fp_violation": 0.017213131436479834,
            "ave_precision_score": 0.8798336336279485,
            "fpr": 0.16447368421052633,
            "logloss": 0.5046760427484627,
            "mae": 0.27879385447988425,
            "precision": 0.7391304347826086,
            "recall": 0.8835758835758836
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8657306590917475,
            "auditor_fn_violation": 0.010517448242411867,
            "auditor_fp_violation": 0.022502744237102093,
            "ave_precision_score": 0.8659331148986855,
            "fpr": 0.19209659714599342,
            "logloss": 0.5347507394744728,
            "mae": 0.2953977230261566,
            "precision": 0.7048903878583473,
            "recall": 0.8837209302325582
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(66)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8502974627338684,
            "auditor_fn_violation": 0.009423897581792325,
            "auditor_fp_violation": 0.01573503073228315,
            "ave_precision_score": 0.8505987141551632,
            "fpr": 0.17324561403508773,
            "logloss": 0.5978654381959132,
            "mae": 0.3015699722289556,
            "precision": 0.7183600713012478,
            "recall": 0.8378378378378378
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8410189152439294,
            "auditor_fn_violation": 0.007790616449641801,
            "auditor_fp_violation": 0.02300648091063561,
            "ave_precision_score": 0.8404667034102073,
            "fpr": 0.1668496158068057,
            "logloss": 0.638284229503139,
            "mae": 0.30491802539551344,
            "precision": 0.7295373665480427,
            "recall": 0.8668076109936576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(67)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.7209572005578673,
            "auditor_fn_violation": 0.0002917897654739765,
            "auditor_fp_violation": 0.017195322994260593,
            "ave_precision_score": 0.7036210364141231,
            "fpr": 0.36403508771929827,
            "logloss": 2.9968930066046098,
            "mae": 0.3768374796370389,
            "precision": 0.5855181023720349,
            "recall": 0.975051975051975
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6902148986582854,
            "auditor_fn_violation": 0.004662302188659629,
            "auditor_fp_violation": 0.017823757324230995,
            "ave_precision_score": 0.6705187410120143,
            "fpr": 0.3534577387486279,
            "logloss": 3.177957072863325,
            "mae": 0.3742478012771356,
            "precision": 0.5855855855855856,
            "recall": 0.9619450317124736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(68)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8547060332607972,
            "auditor_fn_violation": 0.005145074224021599,
            "auditor_fp_violation": 0.018642894940367163,
            "ave_precision_score": 0.855062414706437,
            "fpr": 0.1206140350877193,
            "logloss": 0.723872283890963,
            "mae": 0.276851107995897,
            "precision": 0.7708333333333334,
            "recall": 0.7692307692307693
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8543224228515671,
            "auditor_fn_violation": 0.010538334613590529,
            "auditor_fp_violation": 0.017758597356510232,
            "ave_precision_score": 0.854518374942167,
            "fpr": 0.14709110867178923,
            "logloss": 0.7072424697173122,
            "mae": 0.2792205347733993,
            "precision": 0.7335984095427436,
            "recall": 0.7801268498942917
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(69)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6502192982456141,
            "auc_prc": 0.7154965897496919,
            "auditor_fn_violation": 0.005033373454426088,
            "auditor_fp_violation": 0.016717039117515365,
            "ave_precision_score": 0.6983094396928817,
            "fpr": 0.3168859649122807,
            "logloss": 2.5940991698881355,
            "mae": 0.34994669173971477,
            "precision": 0.6094594594594595,
            "recall": 0.9376299376299376
        },
        "train": {
            "accuracy": 0.6695938529088913,
            "auc_prc": 0.677648213419692,
            "auditor_fn_violation": 0.007797578573368022,
            "auditor_fp_violation": 0.02426456951816712,
            "ave_precision_score": 0.6579213133011643,
            "fpr": 0.29088913282107576,
            "logloss": 2.8948390780758175,
            "mae": 0.34047429845759497,
            "precision": 0.6225071225071225,
            "recall": 0.9238900634249472
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(70)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8418564250420884,
            "auditor_fn_violation": 0.010910201699675387,
            "auditor_fp_violation": 0.018792994667643594,
            "ave_precision_score": 0.842303996028833,
            "fpr": 0.15460526315789475,
            "logloss": 0.8202319562130556,
            "mae": 0.2659703232945609,
            "precision": 0.7369402985074627,
            "recall": 0.8212058212058212
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8227689143504662,
            "auditor_fn_violation": 0.009579882247280711,
            "auditor_fp_violation": 0.019202141256785408,
            "ave_precision_score": 0.8230806044922662,
            "fpr": 0.1690450054884742,
            "logloss": 0.8851167733330987,
            "mae": 0.2757759002012027,
            "precision": 0.7153419593345656,
            "recall": 0.8181818181818182
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(71)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6491228070175439,
            "auc_prc": 0.7220490140285443,
            "auditor_fn_violation": 0.005099482073166285,
            "auditor_fp_violation": 0.019037224732364556,
            "ave_precision_score": 0.7052901195164982,
            "fpr": 0.32127192982456143,
            "logloss": 2.5166098618901662,
            "mae": 0.35273502394071227,
            "precision": 0.607764390896921,
            "recall": 0.9438669438669439
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.6870263983841033,
            "auditor_fn_violation": 0.0075144522085016825,
            "auditor_fp_violation": 0.025525164278303248,
            "ave_precision_score": 0.6665598245917753,
            "fpr": 0.2996706915477497,
            "logloss": 2.8573757672294637,
            "mae": 0.34345496455817714,
            "precision": 0.6176470588235294,
            "recall": 0.9323467230443975
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(72)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6535087719298246,
            "auc_prc": 0.6639035106460682,
            "auditor_fn_violation": 0.04957234562497721,
            "auditor_fp_violation": 0.031202934831277732,
            "ave_precision_score": 0.6653524572522873,
            "fpr": 0.15789473684210525,
            "logloss": 0.6525711142625578,
            "mae": 0.44340438544423433,
            "precision": 0.6821192052980133,
            "recall": 0.6424116424116424
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.6804080325893805,
            "auditor_fn_violation": 0.044518139813368676,
            "auditor_fp_violation": 0.037321624588364445,
            "ave_precision_score": 0.6817260588382082,
            "fpr": 0.1602634467618002,
            "logloss": 0.6422024056220478,
            "mae": 0.43866836491452876,
            "precision": 0.683982683982684,
            "recall": 0.6680761099365751
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(73)",
        "seed": 24481,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.5012548703144679,
            "auditor_fn_violation": 0.0018875150454097976,
            "auditor_fp_violation": 0.0010634184068058778,
            "ave_precision_score": 0.5272743242170486,
            "fpr": 0.0021929824561403508,
            "logloss": 0.6961641410390741,
            "mae": 0.5009295997912424,
            "precision": 0.6,
            "recall": 0.006237006237006237
        },
        "train": {
            "accuracy": 0.4862788144895719,
            "auc_prc": 0.5994900189115592,
            "auditor_fn_violation": 0.0016059298728484312,
            "auditor_fp_violation": 0.0005012305209288804,
            "ave_precision_score": 0.5256688528840266,
            "fpr": 0.0010976948408342481,
            "logloss": 0.6937917071963303,
            "mae": 0.49970439236721537,
            "precision": 0.8571428571428571,
            "recall": 0.012684989429175475
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(74)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5754449653098301,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.539667065342741,
            "fpr": 0.010964912280701754,
            "logloss": 0.6928631015272867,
            "mae": 0.49521462102802377,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5913085069486378,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.526140824301129,
            "fpr": 0.012074643249176729,
            "logloss": 0.6978889230458455,
            "mae": 0.49728721900193745,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(75)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4725877192982456,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.216174831087763,
            "mae": 0.5274122807017544,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4807903402854007,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.932866338960213,
            "mae": 0.5192096597145993,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(76)",
        "seed": 24481,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.7539883140508155,
            "auditor_fn_violation": 0.0010668563300142249,
            "auditor_fp_violation": 0.000717425815117841,
            "ave_precision_score": 0.7549850682198029,
            "fpr": 0.0010964912280701754,
            "logloss": 1.2503055096637465,
            "mae": 0.4467087681799041,
            "precision": 0.6666666666666666,
            "recall": 0.004158004158004158
        },
        "train": {
            "accuracy": 0.48957189901207465,
            "auc_prc": 0.7792984445764248,
            "auditor_fn_violation": 0.001963318890794448,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.7692404580705041,
            "fpr": 0.0,
            "logloss": 1.2078942733847464,
            "mae": 0.4371042079466669,
            "precision": 1.0,
            "recall": 0.016913319238900635
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(77)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8418014852844882,
            "auditor_fn_violation": 0.01098542874858665,
            "auditor_fp_violation": 0.019479891724671308,
            "ave_precision_score": 0.8422487439020742,
            "fpr": 0.14802631578947367,
            "logloss": 0.81995346139146,
            "mae": 0.2657406531043093,
            "precision": 0.7443181818181818,
            "recall": 0.817047817047817
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8224721345580002,
            "auditor_fn_violation": 0.009180720486977349,
            "auditor_fp_violation": 0.0182021863675323,
            "ave_precision_score": 0.8227852431782645,
            "fpr": 0.16794731064763996,
            "logloss": 0.8834828178245355,
            "mae": 0.27560555460997693,
            "precision": 0.7161410018552876,
            "recall": 0.8160676532769556
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(78)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4725877192982456,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.216174831087763,
            "mae": 0.5274122807017544,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.4807903402854007,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.932866338960213,
            "mae": 0.5192096597145993,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(79)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6403508771929824,
            "auc_prc": 0.6630144279282928,
            "auditor_fn_violation": 0.06476820950505162,
            "auditor_fp_violation": 0.034184576871412874,
            "ave_precision_score": 0.6644668667832616,
            "fpr": 0.15021929824561403,
            "logloss": 0.656449854287378,
            "mae": 0.444577571282392,
            "precision": 0.6791569086651054,
            "recall": 0.6029106029106029
        },
        "train": {
            "accuracy": 0.6553238199780461,
            "auc_prc": 0.6797335449630814,
            "auditor_fn_violation": 0.05800841488687709,
            "auditor_fp_violation": 0.03925386824654527,
            "ave_precision_score": 0.6810533848435822,
            "fpr": 0.15477497255762898,
            "logloss": 0.6452112732152474,
            "mae": 0.43956775150367117,
            "precision": 0.6802721088435374,
            "recall": 0.6342494714587738
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(80)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8796035961738693,
            "auditor_fn_violation": 0.005630630630630631,
            "auditor_fp_violation": 0.016282004314731154,
            "ave_precision_score": 0.8798548910221189,
            "fpr": 0.1787280701754386,
            "logloss": 0.5209734527919825,
            "mae": 0.28241312327634405,
            "precision": 0.7260504201680672,
            "recall": 0.8981288981288982
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8667548921996695,
            "auditor_fn_violation": 0.00917375836325113,
            "auditor_fp_violation": 0.023753314386819647,
            "ave_precision_score": 0.8669552444923954,
            "fpr": 0.1964873765093304,
            "logloss": 0.5480901017698627,
            "mae": 0.2964762951994701,
            "precision": 0.7011686143572621,
            "recall": 0.8879492600422833
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(81)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.6801115728982106,
            "auditor_fn_violation": 0.016531713900134953,
            "auditor_fp_violation": 0.025842593723287337,
            "ave_precision_score": 0.6816913700635143,
            "fpr": 0.18421052631578946,
            "logloss": 0.6384592129330583,
            "mae": 0.4177545428504808,
            "precision": 0.681214421252372,
            "recall": 0.7463617463617463
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.6886060218895359,
            "auditor_fn_violation": 0.019846694035548604,
            "auditor_fp_violation": 0.017891423444556385,
            "ave_precision_score": 0.6899313879001452,
            "fpr": 0.19209659714599342,
            "logloss": 0.6295710504350691,
            "mae": 0.41571811000222564,
            "precision": 0.6698113207547169,
            "recall": 0.7505285412262156
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(82)",
        "seed": 24481,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.5339189115800062,
            "auditor_fn_violation": 0.005512091038406827,
            "auditor_fp_violation": 0.006271115724345669,
            "ave_precision_score": 0.5447563564879497,
            "fpr": 0.05263157894736842,
            "logloss": 0.693333068051166,
            "mae": 0.49994114557640595,
            "precision": 0.5752212389380531,
            "recall": 0.13513513513513514
        },
        "train": {
            "accuracy": 0.47859495060373214,
            "auc_prc": 0.5287545791295172,
            "auditor_fn_violation": 0.007491245129414292,
            "auditor_fp_violation": 0.005952112436030459,
            "ave_precision_score": 0.52660768865802,
            "fpr": 0.06476399560922064,
            "logloss": 0.6935395364689027,
            "mae": 0.5000506210601945,
            "precision": 0.49137931034482757,
            "recall": 0.12050739957716702
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(83)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8518123010836597,
            "auditor_fn_violation": 0.013394973921289716,
            "auditor_fp_violation": 0.024763910937436406,
            "ave_precision_score": 0.8522006533607759,
            "fpr": 0.14912280701754385,
            "logloss": 0.7518867395294303,
            "mae": 0.2602718463084467,
            "precision": 0.7448405253283302,
            "recall": 0.8253638253638254
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8340479088256048,
            "auditor_fn_violation": 0.01803422115882229,
            "auditor_fp_violation": 0.023615475993564203,
            "ave_precision_score": 0.8343349675085796,
            "fpr": 0.16794731064763996,
            "logloss": 0.8040970258830494,
            "mae": 0.27241581398705655,
            "precision": 0.7192660550458716,
            "recall": 0.828752642706131
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(84)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.7126834129871558,
            "auditor_fn_violation": 0.006088831746726486,
            "auditor_fp_violation": 0.023125534253266588,
            "ave_precision_score": 0.6948971446681078,
            "fpr": 0.32456140350877194,
            "logloss": 2.7200112358321453,
            "mae": 0.3532051143059021,
            "precision": 0.6063829787234043,
            "recall": 0.9480249480249481
        },
        "train": {
            "accuracy": 0.6663007683863886,
            "auc_prc": 0.6764787203760158,
            "auditor_fn_violation": 0.007651373975117373,
            "auditor_fp_violation": 0.023216997729425752,
            "ave_precision_score": 0.6559301527203569,
            "fpr": 0.29637760702524696,
            "logloss": 2.989192970800011,
            "mae": 0.3440176678872067,
            "precision": 0.6191819464033851,
            "recall": 0.9281183932346723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(85)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.7219321913548084,
            "auditor_fn_violation": 0.005213462450304556,
            "auditor_fp_violation": 0.020123539707738045,
            "ave_precision_score": 0.7043589278640121,
            "fpr": 0.3574561403508772,
            "logloss": 2.9266774434766383,
            "mae": 0.3719816260388448,
            "precision": 0.5873417721518988,
            "recall": 0.9646569646569647
        },
        "train": {
            "accuracy": 0.6300768386388584,
            "auc_prc": 0.6915748286239088,
            "auditor_fn_violation": 0.006437643738846099,
            "auditor_fp_violation": 0.02017452846738744,
            "ave_precision_score": 0.6724567850156586,
            "fpr": 0.3446761800219539,
            "logloss": 3.0712583304989938,
            "mae": 0.3676206515140408,
            "precision": 0.5890052356020943,
            "recall": 0.9513742071881607
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(86)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.7220860263961456,
            "auditor_fn_violation": 0.005213462450304556,
            "auditor_fp_violation": 0.020123539707738045,
            "ave_precision_score": 0.7047820770427163,
            "fpr": 0.3574561403508772,
            "logloss": 2.8693076632198515,
            "mae": 0.371095909080529,
            "precision": 0.5873417721518988,
            "recall": 0.9646569646569647
        },
        "train": {
            "accuracy": 0.6344676180021954,
            "auc_prc": 0.691373107085258,
            "auditor_fn_violation": 0.006437643738846099,
            "auditor_fp_violation": 0.01972342099855145,
            "ave_precision_score": 0.6728251302606525,
            "fpr": 0.3402854006586169,
            "logloss": 3.030060830477143,
            "mae": 0.3663943621005592,
            "precision": 0.5921052631578947,
            "recall": 0.9513742071881607
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(87)",
        "seed": 24481,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.7241784556502174,
            "auditor_fn_violation": 0.002721851406061933,
            "auditor_fp_violation": 0.01841647331786542,
            "ave_precision_score": 0.7065076672856686,
            "fpr": 0.3651315789473684,
            "logloss": 2.86493759556915,
            "mae": 0.37743354898227927,
            "precision": 0.5863354037267081,
            "recall": 0.9812889812889813
        },
        "train": {
            "accuracy": 0.6234906695938529,
            "auc_prc": 0.6923712517492788,
            "auditor_fn_violation": 0.004216726270181457,
            "auditor_fp_violation": 0.017202231478279185,
            "ave_precision_score": 0.6743854797336217,
            "fpr": 0.3578485181119649,
            "logloss": 3.0005664450675797,
            "mae": 0.37567675223226665,
            "precision": 0.5831202046035806,
            "recall": 0.9640591966173362
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(88)",
        "seed": 24481,
        "test": {
            "accuracy": 0.47149122807017546,
            "auc_prc": 0.5118478081327558,
            "auditor_fn_violation": 0.010684520552941607,
            "auditor_fp_violation": 0.009323991533357757,
            "ave_precision_score": 0.5151157034018825,
            "fpr": 0.08333333333333333,
            "logloss": 0.6922537480633945,
            "mae": 0.49913705349491355,
            "precision": 0.4966887417218543,
            "recall": 0.15592515592515593
        },
        "train": {
            "accuracy": 0.4500548847420417,
            "auc_prc": 0.49539926576663174,
            "auditor_fn_violation": 0.007020141423939976,
            "auditor_fp_violation": 0.006661353623144826,
            "ave_precision_score": 0.49826225352454445,
            "fpr": 0.09879253567508232,
            "logloss": 0.6928025625461058,
            "mae": 0.4995498720691442,
            "precision": 0.40789473684210525,
            "recall": 0.13107822410147993
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(89)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8602896030000002,
            "auditor_fn_violation": 0.006189134478608165,
            "auditor_fp_violation": 0.017167338299344654,
            "ave_precision_score": 0.8615303971107052,
            "fpr": 0.18311403508771928,
            "logloss": 0.5731648009088683,
            "mae": 0.28445758615449657,
            "precision": 0.7216666666666667,
            "recall": 0.9002079002079002
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8405531009286469,
            "auditor_fn_violation": 0.00687625753359805,
            "auditor_fp_violation": 0.021257186392593823,
            "ave_precision_score": 0.8417851059274873,
            "fpr": 0.2052689352360044,
            "logloss": 0.5986764602329697,
            "mae": 0.29743069006742334,
            "precision": 0.6924342105263158,
            "recall": 0.8900634249471459
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(90)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8577669356483608,
            "auditor_fn_violation": 0.009309917204654055,
            "auditor_fp_violation": 0.01612427239793219,
            "ave_precision_score": 0.8580582559429968,
            "fpr": 0.1337719298245614,
            "logloss": 0.8406132650160919,
            "mae": 0.2761714258760535,
            "precision": 0.7510204081632653,
            "recall": 0.7650727650727651
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8515364103820723,
            "auditor_fn_violation": 0.008633033420514595,
            "auditor_fp_violation": 0.02183360149166204,
            "ave_precision_score": 0.8517505037411399,
            "fpr": 0.15148188803512624,
            "logloss": 0.8317111590789356,
            "mae": 0.27619447049827794,
            "precision": 0.7325581395348837,
            "recall": 0.7991543340380549
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(91)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8611336039440494,
            "auditor_fn_violation": 0.0056557063136010504,
            "auditor_fp_violation": 0.012328530142060493,
            "ave_precision_score": 0.862364594124784,
            "fpr": 0.17982456140350878,
            "logloss": 0.5697213107691969,
            "mae": 0.2834311369882724,
            "precision": 0.7239057239057239,
            "recall": 0.893970893970894
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8408250144274464,
            "auditor_fn_violation": 0.007632808311847448,
            "auditor_fp_violation": 0.02241502889593954,
            "ave_precision_score": 0.8420538093767047,
            "fpr": 0.20197585071350166,
            "logloss": 0.5967906721488737,
            "mae": 0.29668980522895727,
            "precision": 0.6943521594684385,
            "recall": 0.8837209302325582
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(92)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7188647445999752,
            "auditor_fn_violation": 0.0038502571397308256,
            "auditor_fp_violation": 0.023613994382708513,
            "ave_precision_score": 0.700448865419555,
            "fpr": 0.28618421052631576,
            "logloss": 2.4625268077430977,
            "mae": 0.3370923367279393,
            "precision": 0.6266094420600858,
            "recall": 0.9106029106029107
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.6837937674652835,
            "auditor_fn_violation": 0.006764863553978507,
            "auditor_fp_violation": 0.02217945055110297,
            "ave_precision_score": 0.6630467594894104,
            "fpr": 0.27332601536772777,
            "logloss": 2.752373611096395,
            "mae": 0.3268377518730525,
            "precision": 0.6327433628318584,
            "recall": 0.9069767441860465
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(93)",
        "seed": 24481,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.718725634514995,
            "auditor_fn_violation": 0.00396879673195463,
            "auditor_fp_violation": 0.023982883542964156,
            "ave_precision_score": 0.7003225418180073,
            "fpr": 0.28728070175438597,
            "logloss": 2.468420592921232,
            "mae": 0.33737705987771194,
            "precision": 0.6251788268955651,
            "recall": 0.9085239085239085
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6835449272855454,
            "auditor_fn_violation": 0.00624502498242064,
            "auditor_fp_violation": 0.021545393942127938,
            "ave_precision_score": 0.6628224109839292,
            "fpr": 0.2711306256860593,
            "logloss": 2.758319720785727,
            "mae": 0.3269342428290114,
            "precision": 0.6340740740740741,
            "recall": 0.904862579281184
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(94)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.572724574427532,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5331186427192482,
            "fpr": 0.010964912280701754,
            "logloss": 0.6989906579723231,
            "mae": 0.4996167133774674,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5937222942099161,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5272795471595172,
            "fpr": 0.012074643249176729,
            "logloss": 0.6961045705666947,
            "mae": 0.49790227239794055,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(95)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.5443425412619544,
            "auditor_fn_violation": 0.0013062151220046107,
            "auditor_fp_violation": 0.002228599340578826,
            "ave_precision_score": 0.5332180927110683,
            "fpr": 0.01864035087719298,
            "logloss": 0.6994592747383528,
            "mae": 0.49981575462509664,
            "precision": 0.5405405405405406,
            "recall": 0.04158004158004158
        },
        "train": {
            "accuracy": 0.49176728869374314,
            "auc_prc": 0.582373312215127,
            "auditor_fn_violation": 0.001814793584635059,
            "auditor_fp_violation": 0.0017142083815767709,
            "ave_precision_score": 0.5292598306538878,
            "fpr": 0.01756311745334797,
            "logloss": 0.6960153103525883,
            "mae": 0.4978076980810134,
            "precision": 0.6190476190476191,
            "recall": 0.05496828752642706
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(96)",
        "seed": 24481,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8603523877904153,
            "auditor_fn_violation": 0.006189134478608165,
            "auditor_fp_violation": 0.016480441242316932,
            "ave_precision_score": 0.8615922786046164,
            "fpr": 0.18201754385964913,
            "logloss": 0.572712101080144,
            "mae": 0.2846007910909646,
            "precision": 0.7228714524207012,
            "recall": 0.9002079002079002
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8407074528078502,
            "auditor_fn_violation": 0.00687625753359805,
            "auditor_fp_violation": 0.021257186392593823,
            "ave_precision_score": 0.8419390003884699,
            "fpr": 0.2052689352360044,
            "logloss": 0.5981543851672836,
            "mae": 0.2975759953256351,
            "precision": 0.6924342105263158,
            "recall": 0.8900634249471459
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(97)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.572724574427532,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5331186427192482,
            "fpr": 0.010964912280701754,
            "logloss": 0.6989906579723231,
            "mae": 0.4996167133774674,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5937222942099161,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5272795471595172,
            "fpr": 0.012074643249176729,
            "logloss": 0.6961045705666947,
            "mae": 0.49790227239794055,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(98)",
        "seed": 24481,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.5572954514814924,
            "auditor_fn_violation": 0.00760932997775103,
            "auditor_fp_violation": 0.008985631131192249,
            "ave_precision_score": 0.547982856067112,
            "fpr": 0.049342105263157895,
            "logloss": 0.6939426648819552,
            "mae": 0.5001092040094367,
            "precision": 0.5794392523364486,
            "recall": 0.1288981288981289
        },
        "train": {
            "accuracy": 0.4774972557628979,
            "auc_prc": 0.5296366027242496,
            "auditor_fn_violation": 0.006307684095956633,
            "auditor_fp_violation": 0.0057065094807753035,
            "ave_precision_score": 0.5234498886002749,
            "fpr": 0.06256860592755215,
            "logloss": 0.6942179845451633,
            "mae": 0.5002573809037486,
            "precision": 0.4864864864864865,
            "recall": 0.11416490486257928
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_nsga2",
        "model": "feat_nsga2:archive(99)",
        "seed": 24481,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5114371055074194,
            "auditor_fn_violation": 0.0004992340518656516,
            "auditor_fp_violation": 0.0027908373020718857,
            "ave_precision_score": 0.5127952469624474,
            "fpr": 0.010964912280701754,
            "logloss": 0.6994865763201829,
            "mae": 0.5007099637990458,
            "precision": 0.6,
            "recall": 0.031185031185031187
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.5339520994779343,
            "auditor_fn_violation": 0.0005151971557403977,
            "auditor_fp_violation": 0.0014535685106937535,
            "ave_precision_score": 0.5350637369115854,
            "fpr": 0.012074643249176729,
            "logloss": 0.6961331129522907,
            "mae": 0.49869198627712696,
            "precision": 0.6451612903225806,
            "recall": 0.042283298097251586
        }
    }
]
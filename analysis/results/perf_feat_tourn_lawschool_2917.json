[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8444795787630706,
            "auditor_fn_violation": 0.01019238437001595,
            "auditor_fp_violation": 0.025188555500901797,
            "ave_precision_score": 0.8446670635160953,
            "fpr": 0.17763157894736842,
            "logloss": 0.794272685061526,
            "mae": 0.2779427284145147,
            "precision": 0.7152899824253075,
            "recall": 0.8409090909090909
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8397337240263743,
            "auditor_fn_violation": 0.01307424621061728,
            "auditor_fp_violation": 0.02300678778646475,
            "ave_precision_score": 0.8400213611046834,
            "fpr": 0.1877058177826564,
            "logloss": 0.7555868898455826,
            "mae": 0.28129167702024876,
            "precision": 0.6973451327433628,
            "recall": 0.8382978723404255
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8405003354680157,
            "auditor_fn_violation": 0.017632213281136733,
            "auditor_fp_violation": 0.014787260206591251,
            "ave_precision_score": 0.8409019303485814,
            "fpr": 0.16447368421052633,
            "logloss": 0.5674531835095832,
            "mae": 0.32047925366982566,
            "precision": 0.7237569060773481,
            "recall": 0.8119834710743802
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8439426136991574,
            "auditor_fn_violation": 0.008968400401709601,
            "auditor_fp_violation": 0.007875524889794932,
            "ave_precision_score": 0.8442311656981949,
            "fpr": 0.1690450054884742,
            "logloss": 0.52369458400967,
            "mae": 0.3105753228713888,
            "precision": 0.72,
            "recall": 0.8425531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8153465614694995,
            "auditor_fn_violation": 0.011248096998695093,
            "auditor_fp_violation": 0.02254467945564847,
            "ave_precision_score": 0.8157338466320571,
            "fpr": 0.1611842105263158,
            "logloss": 0.7967281096069497,
            "mae": 0.2853833861700006,
            "precision": 0.7257462686567164,
            "recall": 0.8037190082644629
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.819557367106601,
            "auditor_fn_violation": 0.015598944344536047,
            "auditor_fp_violation": 0.022085819325900872,
            "ave_precision_score": 0.8199280474951245,
            "fpr": 0.1756311745334797,
            "logloss": 0.7671265802009357,
            "mae": 0.28676364125306353,
            "precision": 0.7042513863216266,
            "recall": 0.8106382978723404
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8499429416243254,
            "auditor_fn_violation": 0.011295672031317964,
            "auditor_fp_violation": 0.023238953107066734,
            "ave_precision_score": 0.8501919760691086,
            "fpr": 0.15460526315789475,
            "logloss": 0.7035866295664893,
            "mae": 0.26703029625774766,
            "precision": 0.7403314917127072,
            "recall": 0.8305785123966942
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8442039178826074,
            "auditor_fn_violation": 0.01522759651540276,
            "auditor_fp_violation": 0.01773735472967087,
            "ave_precision_score": 0.8444600016169267,
            "fpr": 0.1602634467618002,
            "logloss": 0.6946926492911539,
            "mae": 0.27660430256839846,
            "precision": 0.7234848484848485,
            "recall": 0.8127659574468085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8524426338058545,
            "auditor_fn_violation": 0.014163766855154416,
            "auditor_fp_violation": 0.023846122315133635,
            "ave_precision_score": 0.8526905496162142,
            "fpr": 0.13596491228070176,
            "logloss": 0.653095124465152,
            "mae": 0.26856318751661035,
            "precision": 0.7578125,
            "recall": 0.8016528925619835
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8554936416450476,
            "auditor_fn_violation": 0.01563864820048112,
            "auditor_fp_violation": 0.014862439670343076,
            "ave_precision_score": 0.8557261893223879,
            "fpr": 0.13721185510428102,
            "logloss": 0.6112368687384384,
            "mae": 0.2659106316452758,
            "precision": 0.749498997995992,
            "recall": 0.7957446808510639
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8499815785373825,
            "auditor_fn_violation": 0.016460961287516316,
            "auditor_fp_violation": 0.024507091326446958,
            "ave_precision_score": 0.8502373887812238,
            "fpr": 0.13706140350877194,
            "logloss": 0.6768020548300596,
            "mae": 0.26790976165917696,
            "precision": 0.7534516765285996,
            "recall": 0.7892561983471075
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8529375469754976,
            "auditor_fn_violation": 0.014533946796833033,
            "auditor_fp_violation": 0.0162438923611889,
            "ave_precision_score": 0.8531726165804804,
            "fpr": 0.13611416026344675,
            "logloss": 0.6429984357822551,
            "mae": 0.2667734764673043,
            "precision": 0.7494949494949495,
            "recall": 0.7893617021276595
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.850506426893808,
            "auditor_fn_violation": 0.010790470494417862,
            "auditor_fp_violation": 0.024409739301524846,
            "ave_precision_score": 0.8507781661033507,
            "fpr": 0.1600877192982456,
            "logloss": 0.6959791088084777,
            "mae": 0.26680820518471504,
            "precision": 0.7359855334538878,
            "recall": 0.8409090909090909
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8497390183568329,
            "auditor_fn_violation": 0.011808393862250976,
            "auditor_fp_violation": 0.019288066488944655,
            "ave_precision_score": 0.8499740960146192,
            "fpr": 0.1690450054884742,
            "logloss": 0.6743811706937547,
            "mae": 0.2727651212443563,
            "precision": 0.7158671586715867,
            "recall": 0.825531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8447998543921378,
            "auditor_fn_violation": 0.0058721183123097015,
            "auditor_fp_violation": 0.022104033448106252,
            "ave_precision_score": 0.8450746323318723,
            "fpr": 0.16666666666666666,
            "logloss": 0.7038704642104616,
            "mae": 0.2711783196891672,
            "precision": 0.7304964539007093,
            "recall": 0.8512396694214877
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8385533213458973,
            "auditor_fn_violation": 0.011831749071630428,
            "auditor_fp_violation": 0.014825103111131027,
            "ave_precision_score": 0.8388513897753729,
            "fpr": 0.18331503841931943,
            "logloss": 0.6925457949873021,
            "mae": 0.27973311525880207,
            "precision": 0.7023172905525846,
            "recall": 0.8382978723404255
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8497026597303314,
            "auditor_fn_violation": 0.016460961287516316,
            "auditor_fp_violation": 0.024507091326446958,
            "ave_precision_score": 0.8499680246703498,
            "fpr": 0.13706140350877194,
            "logloss": 0.6790476678229559,
            "mae": 0.26781775911606853,
            "precision": 0.7534516765285996,
            "recall": 0.7892561983471075
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8529011012381612,
            "auditor_fn_violation": 0.013172338090010978,
            "auditor_fp_violation": 0.0162438923611889,
            "ave_precision_score": 0.85313381847479,
            "fpr": 0.13611416026344675,
            "logloss": 0.644643454544153,
            "mae": 0.26671115154142955,
            "precision": 0.7489878542510121,
            "recall": 0.7872340425531915
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8507420708529883,
            "auditor_fn_violation": 0.013882847614905037,
            "auditor_fp_violation": 0.026187694704049847,
            "ave_precision_score": 0.8510268214316201,
            "fpr": 0.15899122807017543,
            "logloss": 0.6854034375876698,
            "mae": 0.2674301590084882,
            "precision": 0.7363636363636363,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8495857954304311,
            "auditor_fn_violation": 0.014982366816918516,
            "auditor_fp_violation": 0.021132492514019885,
            "ave_precision_score": 0.8498148351476454,
            "fpr": 0.16465422612513722,
            "logloss": 0.6689045090343492,
            "mae": 0.27368884919981123,
            "precision": 0.719626168224299,
            "recall": 0.8191489361702128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.848849877112061,
            "auditor_fn_violation": 0.007580288531245472,
            "auditor_fp_violation": 0.02590588621085425,
            "ave_precision_score": 0.8491281180689121,
            "fpr": 0.15350877192982457,
            "logloss": 0.7387775881743591,
            "mae": 0.2669225397264008,
            "precision": 0.7383177570093458,
            "recall": 0.8161157024793388
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8489001667565828,
            "auditor_fn_violation": 0.01253941191582783,
            "auditor_fp_violation": 0.020624715308736014,
            "ave_precision_score": 0.8491354601850861,
            "fpr": 0.16465422612513722,
            "logloss": 0.7245431715161953,
            "mae": 0.26967972385509587,
            "precision": 0.7185741088180112,
            "recall": 0.8148936170212766
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.8395945687183567,
            "auditor_fn_violation": 0.015813034652747575,
            "auditor_fp_violation": 0.016895700114772916,
            "ave_precision_score": 0.8400036878099822,
            "fpr": 0.22916666666666666,
            "logloss": 0.6170396868587006,
            "mae": 0.3280843402175278,
            "precision": 0.669826224328594,
            "recall": 0.8760330578512396
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8370192432277633,
            "auditor_fn_violation": 0.01453628231777098,
            "auditor_fp_violation": 0.01676411508621012,
            "ave_precision_score": 0.8373133085918976,
            "fpr": 0.21953896816684962,
            "logloss": 0.5947867834653657,
            "mae": 0.3202998855632859,
            "precision": 0.6794871794871795,
            "recall": 0.902127659574468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.849621222183907,
            "auditor_fn_violation": 0.012278889372190809,
            "auditor_fp_violation": 0.024863194786030496,
            "ave_precision_score": 0.849884535253439,
            "fpr": 0.14802631578947367,
            "logloss": 0.6776011818128805,
            "mae": 0.26633117189379557,
            "precision": 0.7476635514018691,
            "recall": 0.8264462809917356
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8499379824266061,
            "auditor_fn_violation": 0.014466216689632624,
            "auditor_fp_violation": 0.01919845874683573,
            "ave_precision_score": 0.8501765561109301,
            "fpr": 0.15806805708013172,
            "logloss": 0.6591370304249118,
            "mae": 0.2710403448310085,
            "precision": 0.7262357414448669,
            "recall": 0.8127659574468085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7752192982456141,
            "auc_prc": 0.8576596737819107,
            "auditor_fn_violation": 0.010579781064230822,
            "auditor_fp_violation": 0.016662567634038374,
            "ave_precision_score": 0.8580487003509963,
            "fpr": 0.10964912280701754,
            "logloss": 0.49193050664789006,
            "mae": 0.3383567895977186,
            "precision": 0.791231732776618,
            "recall": 0.7830578512396694
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8469465529038268,
            "auditor_fn_violation": 0.00944017563117453,
            "auditor_fp_violation": 0.01158926797942009,
            "ave_precision_score": 0.8472034378162068,
            "fpr": 0.1251372118551043,
            "logloss": 0.5085968416434107,
            "mae": 0.344843005190034,
            "precision": 0.7615062761506276,
            "recall": 0.774468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8497435401944858,
            "auditor_fn_violation": 0.007689031462954907,
            "auditor_fp_violation": 0.025260288571897032,
            "ave_precision_score": 0.8500222216132052,
            "fpr": 0.15570175438596492,
            "logloss": 0.7021716808634334,
            "mae": 0.2672317154176695,
            "precision": 0.7389705882352942,
            "recall": 0.8305785123966942
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.842278455116131,
            "auditor_fn_violation": 0.013578718733213443,
            "auditor_fp_violation": 0.016828831788844336,
            "ave_precision_score": 0.8425284543431648,
            "fpr": 0.16355653128430298,
            "logloss": 0.6913956823973479,
            "mae": 0.27710733880758925,
            "precision": 0.71939736346516,
            "recall": 0.8127659574468085
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8490600113576737,
            "auditor_fn_violation": 0.010498223865448745,
            "auditor_fp_violation": 0.022718888342351206,
            "ave_precision_score": 0.8493383043464587,
            "fpr": 0.14692982456140352,
            "logloss": 0.7203024919179362,
            "mae": 0.26695937777179796,
            "precision": 0.7462121212121212,
            "recall": 0.8140495867768595
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8494715950714887,
            "auditor_fn_violation": 0.01183174907163043,
            "auditor_fp_violation": 0.020667030075842998,
            "ave_precision_score": 0.8496929979356758,
            "fpr": 0.15916575192096596,
            "logloss": 0.6789220642625935,
            "mae": 0.2716799442373394,
            "precision": 0.7243346007604563,
            "recall": 0.8106382978723404
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 2917,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.848474979268546,
            "auditor_fn_violation": 0.010973974191677545,
            "auditor_fp_violation": 0.026899901623216926,
            "ave_precision_score": 0.848728098749938,
            "fpr": 0.16666666666666666,
            "logloss": 0.7119869733265102,
            "mae": 0.2681205205990626,
            "precision": 0.7285714285714285,
            "recall": 0.8429752066115702
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8467429377536186,
            "auditor_fn_violation": 0.01328911413690824,
            "auditor_fp_violation": 0.020296153587669973,
            "ave_precision_score": 0.8469668378824269,
            "fpr": 0.17233809001097694,
            "logloss": 0.699555910682387,
            "mae": 0.27563714759782904,
            "precision": 0.7119266055045872,
            "recall": 0.825531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8026564174772022,
            "auditor_fn_violation": 0.007353740756850812,
            "auditor_fp_violation": 0.019168101328086575,
            "ave_precision_score": 0.8031208390016518,
            "fpr": 0.1206140350877193,
            "logloss": 0.6370330919429535,
            "mae": 0.3623299225799669,
            "precision": 0.7608695652173914,
            "recall": 0.7231404958677686
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.7897750625907262,
            "auditor_fn_violation": 0.013050891001237838,
            "auditor_fp_violation": 0.010556289841220059,
            "ave_precision_score": 0.7905231817532316,
            "fpr": 0.1141602634467618,
            "logloss": 0.6567568817584615,
            "mae": 0.35235435247785607,
            "precision": 0.7614678899082569,
            "recall": 0.7063829787234043
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8468252036112904,
            "auditor_fn_violation": 0.019276950123241997,
            "auditor_fp_violation": 0.02345159042465978,
            "ave_precision_score": 0.8471794145374241,
            "fpr": 0.12171052631578948,
            "logloss": 0.5222057717719103,
            "mae": 0.3201125763709371,
            "precision": 0.7716049382716049,
            "recall": 0.7747933884297521
        },
        "train": {
            "accuracy": 0.7749725576289791,
            "auc_prc": 0.8589266649088988,
            "auditor_fn_violation": 0.01629726510498167,
            "auditor_fp_violation": 0.013192250921590735,
            "ave_precision_score": 0.85914546404956,
            "fpr": 0.11964873765093303,
            "logloss": 0.48400675291465334,
            "mae": 0.30759553691237035,
            "precision": 0.7743271221532091,
            "recall": 0.7957446808510639
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.836156473707915,
            "auditor_fn_violation": 0.015371266492677976,
            "auditor_fp_violation": 0.01482825053287424,
            "ave_precision_score": 0.836640180532713,
            "fpr": 0.16885964912280702,
            "logloss": 0.5526971642214542,
            "mae": 0.31801808381266217,
            "precision": 0.7235188509874326,
            "recall": 0.8326446280991735
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8501782229439059,
            "auditor_fn_violation": 0.01572739799612304,
            "auditor_fp_violation": 0.014628463899280903,
            "ave_precision_score": 0.8504273229326221,
            "fpr": 0.16794731064763996,
            "logloss": 0.5178505498116275,
            "mae": 0.3060247077466191,
            "precision": 0.7248201438848921,
            "recall": 0.8574468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.8153424095628794,
            "auditor_fn_violation": 0.011248096998695093,
            "auditor_fp_violation": 0.02254467945564847,
            "ave_precision_score": 0.8157282826522726,
            "fpr": 0.1611842105263158,
            "logloss": 0.7962470710560456,
            "mae": 0.28543639733418363,
            "precision": 0.7257462686567164,
            "recall": 0.8037190082644629
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8196605008311608,
            "auditor_fn_violation": 0.015598944344536047,
            "auditor_fp_violation": 0.022085819325900872,
            "ave_precision_score": 0.8200318097490252,
            "fpr": 0.1756311745334797,
            "logloss": 0.7665526262218789,
            "mae": 0.286802657762313,
            "precision": 0.7042513863216266,
            "recall": 0.8106382978723404
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8467369552174788,
            "auditor_fn_violation": 0.013391238944468609,
            "auditor_fp_violation": 0.025890514838498118,
            "ave_precision_score": 0.847011465119076,
            "fpr": 0.1524122807017544,
            "logloss": 0.7037253079245117,
            "mae": 0.26873367717066937,
            "precision": 0.741635687732342,
            "recall": 0.8243801652892562
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8470383163211048,
            "auditor_fn_violation": 0.015299997664479061,
            "auditor_fp_violation": 0.01927313186525983,
            "ave_precision_score": 0.8472825833785796,
            "fpr": 0.16245883644346873,
            "logloss": 0.6788916646401882,
            "mae": 0.27252379519269265,
            "precision": 0.720226843100189,
            "recall": 0.8106382978723404
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8439455234167165,
            "auditor_fn_violation": 0.026007684500507472,
            "auditor_fp_violation": 0.01594011313330054,
            "ave_precision_score": 0.8443860831246871,
            "fpr": 0.11513157894736842,
            "logloss": 0.5396690603727458,
            "mae": 0.32485641259015435,
            "precision": 0.7817047817047817,
            "recall": 0.7768595041322314
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8134075790509532,
            "auditor_fn_violation": 0.016143120723077286,
            "auditor_fp_violation": 0.01551209580063273,
            "ave_precision_score": 0.8138992131928997,
            "fpr": 0.132821075740944,
            "logloss": 0.5379677446676256,
            "mae": 0.328836043255229,
            "precision": 0.7494824016563147,
            "recall": 0.7702127659574468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8501329304683068,
            "auditor_fn_violation": 0.012559808612440193,
            "auditor_fp_violation": 0.02426114936874898,
            "ave_precision_score": 0.8504036810035327,
            "fpr": 0.15679824561403508,
            "logloss": 0.6824673340579586,
            "mae": 0.2671671256741227,
            "precision": 0.74,
            "recall": 0.8409090909090909
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8500490722426114,
            "auditor_fn_violation": 0.009440175631174537,
            "auditor_fp_violation": 0.02139384842850423,
            "ave_precision_score": 0.8502867288270994,
            "fpr": 0.17233809001097694,
            "logloss": 0.6649980642758881,
            "mae": 0.27289354010043965,
            "precision": 0.7124542124542125,
            "recall": 0.8276595744680851
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8532968553327565,
            "auditor_fn_violation": 0.012577932434391764,
            "auditor_fp_violation": 0.023308124282669294,
            "ave_precision_score": 0.8535488554155748,
            "fpr": 0.15021929824561403,
            "logloss": 0.6327861540970842,
            "mae": 0.266352956824826,
            "precision": 0.7458256029684601,
            "recall": 0.8305785123966942
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8519940641212396,
            "auditor_fn_violation": 0.010537870472008786,
            "auditor_fp_violation": 0.019785887278438635,
            "ave_precision_score": 0.8522457279098385,
            "fpr": 0.15587266739846323,
            "logloss": 0.6179893396702532,
            "mae": 0.27104461956033404,
            "precision": 0.7300380228136882,
            "recall": 0.8170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 2917,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8518887930060548,
            "auditor_fn_violation": 0.010276207046541978,
            "auditor_fp_violation": 0.02499385145105756,
            "ave_precision_score": 0.8521426141620645,
            "fpr": 0.14912280701754385,
            "logloss": 0.657283664720394,
            "mae": 0.265550443495752,
            "precision": 0.7453183520599251,
            "recall": 0.8223140495867769
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8501783638653847,
            "auditor_fn_violation": 0.013637106756662076,
            "auditor_fp_violation": 0.018907233584981743,
            "ave_precision_score": 0.85043457187492,
            "fpr": 0.15148188803512624,
            "logloss": 0.6382641863170402,
            "mae": 0.27042018118893796,
            "precision": 0.7351247600767754,
            "recall": 0.8148936170212766
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8424620655811788,
            "auditor_fn_violation": 0.015878733507322027,
            "auditor_fp_violation": 0.022124528611247744,
            "ave_precision_score": 0.8429258378097377,
            "fpr": 0.12719298245614036,
            "logloss": 0.7414045083750876,
            "mae": 0.2697946879525705,
            "precision": 0.7588357588357588,
            "recall": 0.7541322314049587
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8429472438425875,
            "auditor_fn_violation": 0.01794147184529509,
            "auditor_fp_violation": 0.018623475734970172,
            "ave_precision_score": 0.8431805126545331,
            "fpr": 0.1350164654226125,
            "logloss": 0.7219785304130167,
            "mae": 0.2729646709981873,
            "precision": 0.7388535031847133,
            "recall": 0.7404255319148936
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8212849096042589,
            "auditor_fn_violation": 0.01268440988835726,
            "auditor_fp_violation": 0.02263178389899984,
            "ave_precision_score": 0.8226866175647118,
            "fpr": 0.1611842105263158,
            "logloss": 0.7746743672486963,
            "mae": 0.2672359791013995,
            "precision": 0.7346570397111913,
            "recall": 0.8409090909090909
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.800466395095841,
            "auditor_fn_violation": 0.013144311838755642,
            "auditor_fp_violation": 0.013418759380810504,
            "ave_precision_score": 0.8017461773993483,
            "fpr": 0.17453347969264543,
            "logloss": 0.8614879987889489,
            "mae": 0.2846344681402745,
            "precision": 0.708256880733945,
            "recall": 0.8212765957446808
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8541035856683754,
            "auditor_fn_violation": 0.009872952008119478,
            "auditor_fp_violation": 0.024378996556812593,
            "ave_precision_score": 0.8543314132768602,
            "fpr": 0.1513157894736842,
            "logloss": 0.6771613893095334,
            "mae": 0.2653735565138851,
            "precision": 0.7420560747663552,
            "recall": 0.8202479338842975
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8499811400178284,
            "auditor_fn_violation": 0.01274260223742906,
            "auditor_fp_violation": 0.019190991434993317,
            "ave_precision_score": 0.8502501039780257,
            "fpr": 0.15587266739846323,
            "logloss": 0.6560457199467219,
            "mae": 0.2683950137279451,
            "precision": 0.7295238095238096,
            "recall": 0.8148936170212766
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8520142162492892,
            "auditor_fn_violation": 0.012018359431636948,
            "auditor_fp_violation": 0.025475487784882775,
            "ave_precision_score": 0.8522686140206074,
            "fpr": 0.1513157894736842,
            "logloss": 0.6535622024664198,
            "mae": 0.265952113956456,
            "precision": 0.7439703153988868,
            "recall": 0.8285123966942148
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8505022552795116,
            "auditor_fn_violation": 0.010537870472008786,
            "auditor_fp_violation": 0.02036584849819913,
            "ave_precision_score": 0.850759121046528,
            "fpr": 0.15697036223929747,
            "logloss": 0.6355386724196064,
            "mae": 0.2708886661793065,
            "precision": 0.7286527514231499,
            "recall": 0.8170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8387283422924111,
            "auditor_fn_violation": 0.011687599681020741,
            "auditor_fp_violation": 0.02642338908017708,
            "ave_precision_score": 0.8399551785028258,
            "fpr": 0.15021929824561403,
            "logloss": 0.7480174855079276,
            "mae": 0.26986428520556954,
            "precision": 0.7375478927203065,
            "recall": 0.7954545454545454
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8340567730456026,
            "auditor_fn_violation": 0.014153256883947961,
            "auditor_fp_violation": 0.024572434169423348,
            "ave_precision_score": 0.8342992150041159,
            "fpr": 0.16355653128430298,
            "logloss": 0.7542080184535944,
            "mae": 0.2820970123393639,
            "precision": 0.7095516569200779,
            "recall": 0.774468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8406934443868993,
            "auditor_fn_violation": 0.023524720893141945,
            "auditor_fp_violation": 0.018499446630595183,
            "ave_precision_score": 0.8411510657851887,
            "fpr": 0.13706140350877194,
            "logloss": 0.5629579252124766,
            "mae": 0.31518685747796815,
            "precision": 0.749498997995992,
            "recall": 0.7727272727272727
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8527971276539315,
            "auditor_fn_violation": 0.01356003456570988,
            "auditor_fp_violation": 0.014352173361111737,
            "ave_precision_score": 0.8530458369628758,
            "fpr": 0.13172338090010977,
            "logloss": 0.5021514941390492,
            "mae": 0.3015910414327178,
            "precision": 0.7590361445783133,
            "recall": 0.8042553191489362
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8496487348674768,
            "auditor_fn_violation": 0.010815390749601276,
            "auditor_fp_violation": 0.025091203475979672,
            "ave_precision_score": 0.849910353959499,
            "fpr": 0.15899122807017543,
            "logloss": 0.6983591451228272,
            "mae": 0.26710967348024894,
            "precision": 0.7368421052631579,
            "recall": 0.8388429752066116
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8500730366214979,
            "auditor_fn_violation": 0.008949716234206042,
            "auditor_fp_violation": 0.019288066488944655,
            "ave_precision_score": 0.8503093570497438,
            "fpr": 0.1690450054884742,
            "logloss": 0.6804388929585083,
            "mae": 0.2726797188982456,
            "precision": 0.7153419593345656,
            "recall": 0.823404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8389448136640285,
            "auditor_fn_violation": 0.012056872553284038,
            "auditor_fp_violation": 0.026853787506148558,
            "ave_precision_score": 0.8401711645561543,
            "fpr": 0.1513157894736842,
            "logloss": 0.7443278460302916,
            "mae": 0.26980445875946457,
            "precision": 0.7366412213740458,
            "recall": 0.7975206611570248
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8342749612443083,
            "auditor_fn_violation": 0.014153256883947961,
            "auditor_fp_violation": 0.024572434169423348,
            "ave_precision_score": 0.834518358287754,
            "fpr": 0.16355653128430298,
            "logloss": 0.7504788357258785,
            "mae": 0.2821395434349152,
            "precision": 0.7095516569200779,
            "recall": 0.774468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8397564861085587,
            "auditor_fn_violation": 0.013137505437146589,
            "auditor_fp_violation": 0.025383259550746027,
            "ave_precision_score": 0.84009596363034,
            "fpr": 0.14912280701754385,
            "logloss": 0.7606578591157888,
            "mae": 0.268299093750197,
            "precision": 0.743879472693032,
            "recall": 0.8161157024793388
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8413487815798185,
            "auditor_fn_violation": 0.01739963098769181,
            "auditor_fp_violation": 0.022491543269338474,
            "ave_precision_score": 0.8415924194769486,
            "fpr": 0.15806805708013172,
            "logloss": 0.7389266575239255,
            "mae": 0.2720125753172925,
            "precision": 0.7241379310344828,
            "recall": 0.8042553191489362
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8491158313587868,
            "auditor_fn_violation": 0.011254893431926928,
            "auditor_fp_violation": 0.02467617642236432,
            "ave_precision_score": 0.8493764862068753,
            "fpr": 0.15789473684210525,
            "logloss": 0.7075324184820224,
            "mae": 0.2667574486948486,
            "precision": 0.7372262773722628,
            "recall": 0.8347107438016529
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8494676106514308,
            "auditor_fn_violation": 0.009001097694840839,
            "auditor_fp_violation": 0.021505858106140373,
            "ave_precision_score": 0.8497030161954333,
            "fpr": 0.16794731064763996,
            "logloss": 0.688350004480246,
            "mae": 0.2721428008385098,
            "precision": 0.7171903881700554,
            "recall": 0.825531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8350677161235247,
            "auditor_fn_violation": 0.004732583007104541,
            "auditor_fp_violation": 0.02340035251680603,
            "ave_precision_score": 0.8356173988099224,
            "fpr": 0.15570175438596492,
            "logloss": 0.7376832036385901,
            "mae": 0.27175545178984245,
            "precision": 0.7315689981096408,
            "recall": 0.7995867768595041
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8315724000246144,
            "auditor_fn_violation": 0.013737534156993721,
            "auditor_fp_violation": 0.017306739746758563,
            "ave_precision_score": 0.8318447730295231,
            "fpr": 0.1525795828759605,
            "logloss": 0.7202110659782083,
            "mae": 0.2788363889080253,
            "precision": 0.7290448343079922,
            "recall": 0.7957446808510639
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8485945305434057,
            "auditor_fn_violation": 0.011488237639553433,
            "auditor_fp_violation": 0.02384612231513363,
            "ave_precision_score": 0.8488631726052421,
            "fpr": 0.15570175438596492,
            "logloss": 0.6951403069771122,
            "mae": 0.26669120999741963,
            "precision": 0.7404021937842779,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8488550549510964,
            "auditor_fn_violation": 0.012240465235770841,
            "auditor_fp_violation": 0.02125943681534085,
            "ave_precision_score": 0.8490922727364202,
            "fpr": 0.16794731064763996,
            "logloss": 0.67571288509489,
            "mae": 0.27229094004037047,
            "precision": 0.7166666666666667,
            "recall": 0.823404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 2917,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8340979743240985,
            "auditor_fn_violation": 0.022493928519646225,
            "auditor_fp_violation": 0.011443986719134285,
            "ave_precision_score": 0.8345419363122242,
            "fpr": 0.09978070175438597,
            "logloss": 0.574537974980521,
            "mae": 0.31927177994450506,
            "precision": 0.795045045045045,
            "recall": 0.7293388429752066
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8490566954848032,
            "auditor_fn_violation": 0.019854263493472225,
            "auditor_fp_violation": 0.010416900020161742,
            "ave_precision_score": 0.849284630758177,
            "fpr": 0.10208562019758508,
            "logloss": 0.5047960168454246,
            "mae": 0.30420455664852103,
            "precision": 0.7942477876106194,
            "recall": 0.7638297872340426
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8377328581258127,
            "auditor_fn_violation": 0.01314656734812237,
            "auditor_fp_violation": 0.02542424987702902,
            "ave_precision_score": 0.8389655921048926,
            "fpr": 0.15789473684210525,
            "logloss": 0.7548683826435593,
            "mae": 0.2701962297951739,
            "precision": 0.7308411214953271,
            "recall": 0.8078512396694215
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.833776504639639,
            "auditor_fn_violation": 0.015577924656094545,
            "auditor_fp_violation": 0.024567455961528418,
            "ave_precision_score": 0.8340250736866717,
            "fpr": 0.1690450054884742,
            "logloss": 0.7630507716035758,
            "mae": 0.2824143400971865,
            "precision": 0.7061068702290076,
            "recall": 0.7872340425531915
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.846681725796981,
            "auditor_fn_violation": 0.008765133391329569,
            "auditor_fp_violation": 0.023449028529267096,
            "ave_precision_score": 0.8469567615920132,
            "fpr": 0.16557017543859648,
            "logloss": 0.7143029189479588,
            "mae": 0.26950736003936676,
            "precision": 0.7303571428571428,
            "recall": 0.8450413223140496
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8479809780513634,
            "auditor_fn_violation": 0.01371884998949016,
            "auditor_fp_violation": 0.023375175170690304,
            "ave_precision_score": 0.8482276508156549,
            "fpr": 0.18221734357848518,
            "logloss": 0.6978465573813499,
            "mae": 0.2759033403378473,
            "precision": 0.7035714285714286,
            "recall": 0.8382978723404255
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8505540745470132,
            "auditor_fn_violation": 0.013882847614905037,
            "auditor_fp_violation": 0.026187694704049847,
            "ave_precision_score": 0.8508444447763569,
            "fpr": 0.15899122807017543,
            "logloss": 0.6854404321772943,
            "mae": 0.26739032875722335,
            "precision": 0.7363636363636363,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8494789032328385,
            "auditor_fn_violation": 0.015582595697970434,
            "auditor_fp_violation": 0.020552531294259383,
            "ave_precision_score": 0.8497082055461119,
            "fpr": 0.16355653128430298,
            "logloss": 0.6691849283898417,
            "mae": 0.27371706056501727,
            "precision": 0.7204502814258912,
            "recall": 0.8170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8298700650963656,
            "auditor_fn_violation": 0.007890658982166161,
            "auditor_fp_violation": 0.028918675192654536,
            "ave_precision_score": 0.8311249077515639,
            "fpr": 0.17982456140350878,
            "logloss": 0.7730529249851226,
            "mae": 0.27882365618286,
            "precision": 0.7167530224525043,
            "recall": 0.8574380165289256
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8271673456796498,
            "auditor_fn_violation": 0.009276689165518371,
            "auditor_fp_violation": 0.018767843763923427,
            "ave_precision_score": 0.8284599587219719,
            "fpr": 0.18660812294182216,
            "logloss": 0.7642725143710093,
            "mae": 0.2818277612554368,
            "precision": 0.697508896797153,
            "recall": 0.8340425531914893
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8437013685130297,
            "auditor_fn_violation": 0.009159326518776281,
            "auditor_fp_violation": 0.026420827184784394,
            "ave_precision_score": 0.8440335398047499,
            "fpr": 0.15021929824561403,
            "logloss": 0.7109344352846793,
            "mae": 0.26907541590498224,
            "precision": 0.7434456928838952,
            "recall": 0.8202479338842975
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8435013325931289,
            "auditor_fn_violation": 0.015332694957610297,
            "auditor_fp_violation": 0.02220280721143196,
            "ave_precision_score": 0.8437282428478963,
            "fpr": 0.15916575192096596,
            "logloss": 0.7020317078459076,
            "mae": 0.2771057034983236,
            "precision": 0.7211538461538461,
            "recall": 0.7978723404255319
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8394555833432156,
            "auditor_fn_violation": 0.013629114107583011,
            "auditor_fp_violation": 0.023369609772093788,
            "ave_precision_score": 0.8406783782387357,
            "fpr": 0.13706140350877194,
            "logloss": 0.7394294063630448,
            "mae": 0.27133955270631105,
            "precision": 0.7484909456740443,
            "recall": 0.768595041322314
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.8358935218550416,
            "auditor_fn_violation": 0.013342831118480976,
            "auditor_fp_violation": 0.01937767423105357,
            "ave_precision_score": 0.8361307796487802,
            "fpr": 0.15148188803512624,
            "logloss": 0.7362639223879626,
            "mae": 0.28031777422479376,
            "precision": 0.7212121212121212,
            "recall": 0.7595744680851064
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8478716961860052,
            "auditor_fn_violation": 0.008590691605045678,
            "auditor_fp_violation": 0.026333722741433023,
            "ave_precision_score": 0.8481370915244303,
            "fpr": 0.15460526315789475,
            "logloss": 0.7005615725516352,
            "mae": 0.2661669054442067,
            "precision": 0.7422303473491774,
            "recall": 0.8388429752066116
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.847983392201324,
            "auditor_fn_violation": 0.01451292710839153,
            "auditor_fp_violation": 0.021311707998237717,
            "ave_precision_score": 0.8482205418196178,
            "fpr": 0.1668496158068057,
            "logloss": 0.6834286856339968,
            "mae": 0.2722337660276755,
            "precision": 0.7164179104477612,
            "recall": 0.8170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8085252641752556,
            "auditor_fn_violation": 0.010258083224590408,
            "auditor_fp_violation": 0.02254724135104116,
            "ave_precision_score": 0.8098723988735166,
            "fpr": 0.16557017543859648,
            "logloss": 0.7871811104497916,
            "mae": 0.28645368591191545,
            "precision": 0.7219152854511971,
            "recall": 0.8099173553719008
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.8122559053266071,
            "auditor_fn_violation": 0.014732466076558377,
            "auditor_fp_violation": 0.019176056811308507,
            "ave_precision_score": 0.8127768767843748,
            "fpr": 0.18111964873765093,
            "logloss": 0.7594300183573143,
            "mae": 0.2901343663013997,
            "precision": 0.698905109489051,
            "recall": 0.8148936170212766
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8485379569119157,
            "auditor_fn_violation": 0.013153363781354217,
            "auditor_fp_violation": 0.02574704869650763,
            "ave_precision_score": 0.8487940448369755,
            "fpr": 0.15679824561403508,
            "logloss": 0.7052630486344295,
            "mae": 0.26692465944451355,
            "precision": 0.7376146788990826,
            "recall": 0.8305785123966942
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8410098753282931,
            "auditor_fn_violation": 0.01431207230772824,
            "auditor_fp_violation": 0.021304240686395314,
            "ave_precision_score": 0.8412841078647376,
            "fpr": 0.17014270032930845,
            "logloss": 0.7016246532190518,
            "mae": 0.2778416635937051,
            "precision": 0.712430426716141,
            "recall": 0.8170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 2917,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8506896947485592,
            "auditor_fn_violation": 0.011254893431926928,
            "auditor_fp_violation": 0.02384612231513363,
            "ave_precision_score": 0.8509669859845262,
            "fpr": 0.15570175438596492,
            "logloss": 0.7054658772454789,
            "mae": 0.2665610311278749,
            "precision": 0.73992673992674,
            "recall": 0.8347107438016529
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8501306631115543,
            "auditor_fn_violation": 0.011808393862250976,
            "auditor_fp_violation": 0.020592356957418907,
            "ave_precision_score": 0.8503678111123693,
            "fpr": 0.1668496158068057,
            "logloss": 0.6838362067326085,
            "mae": 0.26999122510641566,
            "precision": 0.7185185185185186,
            "recall": 0.825531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.832411492243142,
            "auditor_fn_violation": 0.010203711758735684,
            "auditor_fp_violation": 0.02585208640760781,
            "ave_precision_score": 0.8327797678487925,
            "fpr": 0.15021929824561403,
            "logloss": 0.8229149483391732,
            "mae": 0.26811987293500605,
            "precision": 0.7390476190476191,
            "recall": 0.8016528925619835
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8336953191098652,
            "auditor_fn_violation": 0.010755073919237686,
            "auditor_fp_violation": 0.019382652438948503,
            "ave_precision_score": 0.8339398032422332,
            "fpr": 0.1525795828759605,
            "logloss": 0.7946579543108578,
            "mae": 0.271808397800545,
            "precision": 0.7295719844357976,
            "recall": 0.7978723404255319
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8488344985144471,
            "auditor_fn_violation": 0.011488237639553433,
            "auditor_fp_violation": 0.02384612231513363,
            "ave_precision_score": 0.8491028363031226,
            "fpr": 0.15570175438596492,
            "logloss": 0.6929940370027363,
            "mae": 0.2667754674378088,
            "precision": 0.7404021937842779,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8488001729873802,
            "auditor_fn_violation": 0.012240465235770841,
            "auditor_fp_violation": 0.02125943681534085,
            "ave_precision_score": 0.8490396242237206,
            "fpr": 0.16794731064763996,
            "logloss": 0.6734809961065674,
            "mae": 0.2723107054615756,
            "precision": 0.7166666666666667,
            "recall": 0.823404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8493068661867481,
            "auditor_fn_violation": 0.011254893431926928,
            "auditor_fp_violation": 0.02426114936874898,
            "ave_precision_score": 0.8495663093200813,
            "fpr": 0.15679824561403508,
            "logloss": 0.7073440896736014,
            "mae": 0.2667143544701805,
            "precision": 0.7385740402193784,
            "recall": 0.8347107438016529
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8495684292325267,
            "auditor_fn_violation": 0.009001097694840839,
            "auditor_fp_violation": 0.02099559179690903,
            "ave_precision_score": 0.8498032576651788,
            "fpr": 0.1690450054884742,
            "logloss": 0.6889994034486367,
            "mae": 0.27235509986952483,
            "precision": 0.7158671586715867,
            "recall": 0.825531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8500631185035256,
            "auditor_fn_violation": 0.01175782949108308,
            "auditor_fp_violation": 0.02396396950319725,
            "ave_precision_score": 0.8503231170517298,
            "fpr": 0.15021929824561403,
            "logloss": 0.6926673875722082,
            "mae": 0.2664582032578742,
            "precision": 0.7434456928838952,
            "recall": 0.8202479338842975
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.85203859097434,
            "auditor_fn_violation": 0.01488427493752482,
            "auditor_fp_violation": 0.018115698529686305,
            "ave_precision_score": 0.8522686628052742,
            "fpr": 0.1602634467618002,
            "logloss": 0.6696399174297787,
            "mae": 0.2694798438573079,
            "precision": 0.7229601518026565,
            "recall": 0.8106382978723404
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8485700075416899,
            "auditor_fn_violation": 0.011488237639553433,
            "auditor_fp_violation": 0.024112559435973117,
            "ave_precision_score": 0.8488361111042688,
            "fpr": 0.15350877192982457,
            "logloss": 0.688662750237247,
            "mae": 0.2664321339836057,
            "precision": 0.7431192660550459,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8489188389909916,
            "auditor_fn_violation": 0.013256416843777012,
            "auditor_fp_violation": 0.02032602283503962,
            "ave_precision_score": 0.8491573497173832,
            "fpr": 0.1668496158068057,
            "logloss": 0.670538513077112,
            "mae": 0.2720019476796607,
            "precision": 0.7174721189591078,
            "recall": 0.8212765957446808
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8451983555347281,
            "auditor_fn_violation": 0.01789727417717848,
            "auditor_fp_violation": 0.016980242662731605,
            "ave_precision_score": 0.8455677264130248,
            "fpr": 0.19298245614035087,
            "logloss": 0.5843231177012589,
            "mae": 0.3220610280317868,
            "precision": 0.6986301369863014,
            "recall": 0.8429752066115702
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8465682516773697,
            "auditor_fn_violation": 0.016521475115024407,
            "auditor_fp_violation": 0.01784687530335954,
            "ave_precision_score": 0.8468231316998114,
            "fpr": 0.19099890230515917,
            "logloss": 0.544868242578808,
            "mae": 0.3131039019273174,
            "precision": 0.7015437392795884,
            "recall": 0.8702127659574468
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 2917,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.7259237444511174,
            "auditor_fn_violation": 0.008572567783094096,
            "auditor_fp_violation": 0.038249098212821776,
            "ave_precision_score": 0.7251792594344286,
            "fpr": 0.26973684210526316,
            "logloss": 1.2506555414361555,
            "mae": 0.3691570389680891,
            "precision": 0.6322869955156951,
            "recall": 0.8739669421487604
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.7128545013052914,
            "auditor_fn_violation": 0.011168461125253989,
            "auditor_fp_violation": 0.03134777511443656,
            "ave_precision_score": 0.7120759823710044,
            "fpr": 0.2810098792535675,
            "logloss": 1.295392326317617,
            "mae": 0.3706829293409176,
            "precision": 0.624633431085044,
            "recall": 0.9063829787234042
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7631578947368421,
            "auc_prc": 0.852110125936662,
            "auditor_fn_violation": 0.010353233289836167,
            "auditor_fp_violation": 0.02273425971470733,
            "ave_precision_score": 0.8523687833824154,
            "fpr": 0.14144736842105263,
            "logloss": 0.6511644250868217,
            "mae": 0.2670863775977501,
            "precision": 0.7547528517110266,
            "recall": 0.8202479338842975
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8533114035663878,
            "auditor_fn_violation": 0.01600766050867646,
            "auditor_fp_violation": 0.019143698459991396,
            "ave_precision_score": 0.8535429780220024,
            "fpr": 0.1525795828759605,
            "logloss": 0.6261108821136205,
            "mae": 0.2692790296719557,
            "precision": 0.7326923076923076,
            "recall": 0.8106382978723404
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 2917,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.6853539736738796,
            "auditor_fn_violation": 0.00899847759895607,
            "auditor_fp_violation": 0.020715486145269715,
            "ave_precision_score": 0.6818407622954055,
            "fpr": 0.21710526315789475,
            "logloss": 2.0072128320039084,
            "mae": 0.30541923433900975,
            "precision": 0.6732673267326733,
            "recall": 0.8429752066115702
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7180886854814797,
            "auditor_fn_violation": 0.009351425835532614,
            "auditor_fp_violation": 0.011200967763614778,
            "ave_precision_score": 0.7124655335176426,
            "fpr": 0.21075740944017562,
            "logloss": 1.5402824015912244,
            "mae": 0.30617935330714163,
            "precision": 0.6740237691001698,
            "recall": 0.8446808510638298
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8509537282165713,
            "auditor_fn_violation": 0.009270334928229668,
            "auditor_fp_violation": 0.02396396950319725,
            "ave_precision_score": 0.851264019410428,
            "fpr": 0.15021929824561403,
            "logloss": 0.6831836244220811,
            "mae": 0.26549767390943757,
            "precision": 0.74487895716946,
            "recall": 0.8264462809917356
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8476962585169883,
            "auditor_fn_violation": 0.014076184692995774,
            "auditor_fp_violation": 0.022162981548272443,
            "ave_precision_score": 0.8479355678331959,
            "fpr": 0.16465422612513722,
            "logloss": 0.6713312986237894,
            "mae": 0.2739204198257761,
            "precision": 0.7164461247637051,
            "recall": 0.8063829787234043
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8366709336272486,
            "auditor_fn_violation": 0.023012722923010008,
            "auditor_fp_violation": 0.023992150352516815,
            "ave_precision_score": 0.8370824951284358,
            "fpr": 0.1962719298245614,
            "logloss": 0.6040275261044252,
            "mae": 0.3224248062382813,
            "precision": 0.6934931506849316,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8317885058901968,
            "auditor_fn_violation": 0.017025947637620577,
            "auditor_fp_violation": 0.01679149522963229,
            "ave_precision_score": 0.8321061700139469,
            "fpr": 0.18990120746432493,
            "logloss": 0.569035333087857,
            "mae": 0.31290362331958915,
            "precision": 0.7032590051457976,
            "recall": 0.8723404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 2917,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8428261180882772,
            "auditor_fn_violation": 0.01108271712338698,
            "auditor_fp_violation": 0.023456714215445162,
            "ave_precision_score": 0.8432429010925421,
            "fpr": 0.1425438596491228,
            "logloss": 0.7229553485725951,
            "mae": 0.2678054495437915,
            "precision": 0.7450980392156863,
            "recall": 0.7851239669421488
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.8383139849136972,
            "auditor_fn_violation": 0.013172338090010978,
            "auditor_fp_violation": 0.01728682691517881,
            "ave_precision_score": 0.8385698989168018,
            "fpr": 0.14709110867178923,
            "logloss": 0.7140150673889328,
            "mae": 0.27631053771340813,
            "precision": 0.7259713701431493,
            "recall": 0.7553191489361702
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.848360305234646,
            "auditor_fn_violation": 0.010339640423372482,
            "auditor_fp_violation": 0.023377295458271854,
            "ave_precision_score": 0.8486223422065571,
            "fpr": 0.15899122807017543,
            "logloss": 0.7175074718532548,
            "mae": 0.2662342118383718,
            "precision": 0.7358834244080146,
            "recall": 0.8347107438016529
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8483255116852431,
            "auditor_fn_violation": 0.012240465235770841,
            "auditor_fp_violation": 0.02099559179690903,
            "ave_precision_score": 0.8485585531302335,
            "fpr": 0.1690450054884742,
            "logloss": 0.6995922042478,
            "mae": 0.27243989964471277,
            "precision": 0.7153419593345656,
            "recall": 0.823404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8115810355173187,
            "auditor_fn_violation": 0.012695737277076994,
            "auditor_fp_violation": 0.014853869486801117,
            "ave_precision_score": 0.8118593387561889,
            "fpr": 0.13486842105263158,
            "logloss": 1.1003870161894358,
            "mae": 0.34423433246703034,
            "precision": 0.7515151515151515,
            "recall": 0.768595041322314
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.799844314878152,
            "auditor_fn_violation": 0.018558049372912628,
            "auditor_fp_violation": 0.011596735291262504,
            "ave_precision_score": 0.800819412021365,
            "fpr": 0.14489571899012074,
            "logloss": 1.1821040853583882,
            "mae": 0.34579888136380327,
            "precision": 0.7322515212981744,
            "recall": 0.7680851063829788
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8486944928903739,
            "auditor_fn_violation": 0.011488237639553433,
            "auditor_fp_violation": 0.024112559435973117,
            "ave_precision_score": 0.8489705367316815,
            "fpr": 0.15350877192982457,
            "logloss": 0.6878883314119156,
            "mae": 0.26622277006452816,
            "precision": 0.7431192660550459,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8491169305745183,
            "auditor_fn_violation": 0.012985496414975362,
            "auditor_fp_violation": 0.01999995021792106,
            "ave_precision_score": 0.8493544652749825,
            "fpr": 0.16575192096597147,
            "logloss": 0.6699146911987929,
            "mae": 0.2718159997633665,
            "precision": 0.7182835820895522,
            "recall": 0.8191489361702128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8535898851469638,
            "auditor_fn_violation": 0.00811041032332899,
            "auditor_fp_violation": 0.022544679455648465,
            "ave_precision_score": 0.8538233203023209,
            "fpr": 0.14802631578947367,
            "logloss": 0.6278449517225503,
            "mae": 0.2661739100494966,
            "precision": 0.75,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8489954122055846,
            "auditor_fn_violation": 0.014484900857136189,
            "auditor_fp_violation": 0.01935527229552634,
            "ave_precision_score": 0.8492588196037933,
            "fpr": 0.15806805708013172,
            "logloss": 0.6237274865454717,
            "mae": 0.27382524371076566,
            "precision": 0.7272727272727273,
            "recall": 0.8170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8511135540326149,
            "auditor_fn_violation": 0.009872952008119471,
            "auditor_fp_violation": 0.02229873749795049,
            "ave_precision_score": 0.8513743422592346,
            "fpr": 0.14364035087719298,
            "logloss": 0.6699107473379129,
            "mae": 0.26578675221784376,
            "precision": 0.7509505703422054,
            "recall": 0.8161157024793388
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8532691418761469,
            "auditor_fn_violation": 0.01097694840834248,
            "auditor_fp_violation": 0.013754788413718954,
            "ave_precision_score": 0.8534844864246469,
            "fpr": 0.15148188803512624,
            "logloss": 0.646375507301442,
            "mae": 0.26885655994772584,
            "precision": 0.7335907335907336,
            "recall": 0.8085106382978723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 2917,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8567333486384063,
            "auditor_fn_violation": 0.010457445266057706,
            "auditor_fp_violation": 0.023323495655025415,
            "ave_precision_score": 0.8570395070631341,
            "fpr": 0.17982456140350878,
            "logloss": 0.729629675769365,
            "mae": 0.2693378736225142,
            "precision": 0.7191780821917808,
            "recall": 0.8677685950413223
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8550418584970303,
            "auditor_fn_violation": 0.014337763038045638,
            "auditor_fp_violation": 0.02022894778108829,
            "ave_precision_score": 0.8552301929722836,
            "fpr": 0.18441273326015367,
            "logloss": 0.714896046916129,
            "mae": 0.2780298330081982,
            "precision": 0.7057793345008757,
            "recall": 0.8574468085106383
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8506722454846235,
            "auditor_fn_violation": 0.013882847614905037,
            "auditor_fp_violation": 0.026187694704049847,
            "ave_precision_score": 0.8509575081053444,
            "fpr": 0.15899122807017543,
            "logloss": 0.6854278137133022,
            "mae": 0.2674352695775126,
            "precision": 0.7363636363636363,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.84959725719703,
            "auditor_fn_violation": 0.014982366816918516,
            "auditor_fp_violation": 0.021132492514019885,
            "ave_precision_score": 0.8498271277304766,
            "fpr": 0.16465422612513722,
            "logloss": 0.6689511106691299,
            "mae": 0.2736767926876104,
            "precision": 0.719626168224299,
            "recall": 0.8191489361702128
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8488581511487251,
            "auditor_fn_violation": 0.011488237639553433,
            "auditor_fp_violation": 0.02384612231513363,
            "ave_precision_score": 0.8491316569105706,
            "fpr": 0.15570175438596492,
            "logloss": 0.6775496142582047,
            "mae": 0.26810682485253606,
            "precision": 0.7404021937842779,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8500086439010368,
            "auditor_fn_violation": 0.011999906579162486,
            "auditor_fp_violation": 0.02188420190615581,
            "ave_precision_score": 0.8502479642755048,
            "fpr": 0.1690450054884742,
            "logloss": 0.6588489452396075,
            "mae": 0.2727540043394053,
            "precision": 0.7158671586715867,
            "recall": 0.825531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8415881126012942,
            "auditor_fn_violation": 0.014607800492967959,
            "auditor_fp_violation": 0.022816240367273328,
            "ave_precision_score": 0.8420000134864845,
            "fpr": 0.14692982456140352,
            "logloss": 0.730953672176362,
            "mae": 0.2695763128565674,
            "precision": 0.7428023032629558,
            "recall": 0.7995867768595041
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8407306995618145,
            "auditor_fn_violation": 0.01596562113179345,
            "auditor_fp_violation": 0.016194110282239504,
            "ave_precision_score": 0.8409844405675326,
            "fpr": 0.1602634467618002,
            "logloss": 0.7162316658055049,
            "mae": 0.2755874618241534,
            "precision": 0.7181467181467182,
            "recall": 0.7914893617021277
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8525329540711672,
            "auditor_fn_violation": 0.011916412933159346,
            "auditor_fp_violation": 0.022601041154287592,
            "ave_precision_score": 0.8527935293289539,
            "fpr": 0.1524122807017544,
            "logloss": 0.6664825531966106,
            "mae": 0.26574222420387733,
            "precision": 0.7435424354243543,
            "recall": 0.8326446280991735
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.853867446943229,
            "auditor_fn_violation": 0.01215872200294276,
            "auditor_fp_violation": 0.020124405415294557,
            "ave_precision_score": 0.8540950237788598,
            "fpr": 0.16136114160263446,
            "logloss": 0.6389161448820644,
            "mae": 0.26903517789985976,
            "precision": 0.724202626641651,
            "recall": 0.8212765957446808
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8401796245555014,
            "auditor_fn_violation": 0.02778608452950558,
            "auditor_fp_violation": 0.021289350713231684,
            "ave_precision_score": 0.8405370442189459,
            "fpr": 0.19188596491228072,
            "logloss": 1.2955659218796163,
            "mae": 0.32798789393818134,
            "precision": 0.6993127147766323,
            "recall": 0.8409090909090909
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8430894811340797,
            "auditor_fn_violation": 0.0217436999322699,
            "auditor_fp_violation": 0.026747911019512093,
            "ave_precision_score": 0.8431693081921832,
            "fpr": 0.19099890230515917,
            "logloss": 1.174315120720526,
            "mae": 0.316261275652256,
            "precision": 0.6994818652849741,
            "recall": 0.8617021276595744
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.849005456686613,
            "auditor_fn_violation": 0.010611497752646078,
            "auditor_fp_violation": 0.02439949171995409,
            "ave_precision_score": 0.8492592472565755,
            "fpr": 0.15460526315789475,
            "logloss": 0.699648215226278,
            "mae": 0.2671468232144416,
            "precision": 0.7393715341959335,
            "recall": 0.8264462809917356
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8407530805896429,
            "auditor_fn_violation": 0.017121703996076328,
            "auditor_fp_violation": 0.018289935806009204,
            "ave_precision_score": 0.8410342981018686,
            "fpr": 0.15697036223929747,
            "logloss": 0.6921465722038666,
            "mae": 0.2768020338754065,
            "precision": 0.7260536398467433,
            "recall": 0.8063829787234043
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 2917,
        "test": {
            "accuracy": 0.6304824561403509,
            "auc_prc": 0.8370755281473035,
            "auditor_fn_violation": 0.03233516383935044,
            "auditor_fp_violation": 0.00528775209050664,
            "ave_precision_score": 0.8374323499674265,
            "fpr": 0.013157894736842105,
            "logloss": 1.5510999490639716,
            "mae": 0.382603050755544,
            "precision": 0.9298245614035088,
            "recall": 0.3285123966942149
        },
        "train": {
            "accuracy": 0.6311745334796927,
            "auc_prc": 0.8275337619215324,
            "auditor_fn_violation": 0.02448793703435552,
            "auditor_fp_violation": 0.004761655851510015,
            "ave_precision_score": 0.8282834413855069,
            "fpr": 0.012074643249176729,
            "logloss": 1.5166561323541874,
            "mae": 0.3752693270376361,
            "precision": 0.9294871794871795,
            "recall": 0.30851063829787234
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8479003403959391,
            "auditor_fn_violation": 0.01222451790633609,
            "auditor_fp_violation": 0.02505021314969668,
            "ave_precision_score": 0.8482383878183883,
            "fpr": 0.1513157894736842,
            "logloss": 0.7073099543545126,
            "mae": 0.26588381577809295,
            "precision": 0.7444444444444445,
            "recall": 0.8305785123966942
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.84537009755144,
            "auditor_fn_violation": 0.013522666230702758,
            "auditor_fp_violation": 0.02159297674430183,
            "ave_precision_score": 0.8456045492430599,
            "fpr": 0.16575192096597147,
            "logloss": 0.6933816964969692,
            "mae": 0.27478273352590565,
            "precision": 0.7156308851224106,
            "recall": 0.8085106382978723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.835196325939157,
            "auditor_fn_violation": 0.02172593156444831,
            "auditor_fp_violation": 0.027340547630759143,
            "ave_precision_score": 0.8355813725542596,
            "fpr": 0.12719298245614036,
            "logloss": 0.8570075424336373,
            "mae": 0.26436896721275543,
            "precision": 0.7637474541751528,
            "recall": 0.7747933884297521
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8325686912558676,
            "auditor_fn_violation": 0.02016488777821893,
            "auditor_fp_violation": 0.024104482627299004,
            "ave_precision_score": 0.8328163625365913,
            "fpr": 0.14709110867178923,
            "logloss": 0.8490356094608438,
            "mae": 0.2694759219204423,
            "precision": 0.7330677290836654,
            "recall": 0.7829787234042553
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8392279495990995,
            "auditor_fn_violation": 0.01477544584602001,
            "auditor_fp_violation": 0.014077615182816859,
            "ave_precision_score": 0.8396081476475818,
            "fpr": 0.2050438596491228,
            "logloss": 0.59539507611986,
            "mae": 0.32700735874754816,
            "precision": 0.6893687707641196,
            "recall": 0.8574380165289256
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8360277362559275,
            "auditor_fn_violation": 0.017208118270780302,
            "auditor_fp_violation": 0.017055340248064106,
            "ave_precision_score": 0.8363042954007982,
            "fpr": 0.1964873765093304,
            "logloss": 0.5609696418966923,
            "mae": 0.31725901914853755,
            "precision": 0.6981450252951096,
            "recall": 0.8808510638297873
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8638642007763356,
            "auditor_fn_violation": 0.012306075105118176,
            "auditor_fp_violation": 0.004975200852598787,
            "ave_precision_score": 0.8641305052537933,
            "fpr": 0.041666666666666664,
            "logloss": 0.5466904110949669,
            "mae": 0.34445311438921317,
            "precision": 0.8770226537216829,
            "recall": 0.5599173553719008
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.8324845263096807,
            "auditor_fn_violation": 0.008059882756848923,
            "auditor_fp_violation": 0.01057869177674729,
            "ave_precision_score": 0.8327733388023653,
            "fpr": 0.05817782656421515,
            "logloss": 0.5629751231189006,
            "mae": 0.35018475759574075,
            "precision": 0.8354037267080745,
            "recall": 0.5723404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.80328094421318,
            "auditor_fn_violation": 0.016522129186602875,
            "auditor_fp_violation": 0.022716326446958524,
            "ave_precision_score": 0.8040400804202202,
            "fpr": 0.13486842105263158,
            "logloss": 0.8109841826748524,
            "mae": 0.2901227620376166,
            "precision": 0.7458677685950413,
            "recall": 0.7458677685950413
        },
        "train": {
            "accuracy": 0.70801317233809,
            "auc_prc": 0.8061173782672106,
            "auditor_fn_violation": 0.01529065558072728,
            "auditor_fp_violation": 0.01767512713098413,
            "ave_precision_score": 0.8064101271426863,
            "fpr": 0.150384193194292,
            "logloss": 0.8078055751292428,
            "mae": 0.2981625394723781,
            "precision": 0.7133891213389121,
            "recall": 0.725531914893617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8510594276904397,
            "auditor_fn_violation": 0.013656299840510375,
            "auditor_fp_violation": 0.023620675520577145,
            "ave_precision_score": 0.851312875924259,
            "fpr": 0.1337719298245614,
            "logloss": 0.6599831623769778,
            "mae": 0.26872012848824833,
            "precision": 0.7593688362919132,
            "recall": 0.7954545454545454
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8541333891693688,
            "auditor_fn_violation": 0.01574141112175071,
            "auditor_fp_violation": 0.01393400389793679,
            "ave_precision_score": 0.8543686620648642,
            "fpr": 0.13172338090010977,
            "logloss": 0.6200506065005494,
            "mae": 0.26632347603061723,
            "precision": 0.7551020408163265,
            "recall": 0.7872340425531915
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8510329368052811,
            "auditor_fn_violation": 0.012090854719443237,
            "auditor_fp_violation": 0.022595917363502225,
            "ave_precision_score": 0.8512936566273044,
            "fpr": 0.14473684210526316,
            "logloss": 0.6813814967197838,
            "mae": 0.26555638720921326,
            "precision": 0.7504725897920604,
            "recall": 0.8202479338842975
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8523905533512248,
            "auditor_fn_violation": 0.014858584207207417,
            "auditor_fp_violation": 0.019785887278438635,
            "ave_precision_score": 0.8526187495377651,
            "fpr": 0.15587266739846323,
            "logloss": 0.6518072832086087,
            "mae": 0.2686459994240035,
            "precision": 0.7295238095238096,
            "recall": 0.8148936170212766
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8233471548793043,
            "auditor_fn_violation": 0.00982311149775265,
            "auditor_fp_violation": 0.02706386292834892,
            "ave_precision_score": 0.8237997787791208,
            "fpr": 0.14473684210526316,
            "logloss": 0.9118959059761865,
            "mae": 0.272853426616697,
            "precision": 0.7411764705882353,
            "recall": 0.78099173553719
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8114169447037768,
            "auditor_fn_violation": 0.015423780274190165,
            "auditor_fp_violation": 0.017331630786233267,
            "ave_precision_score": 0.8117117776365181,
            "fpr": 0.15148188803512624,
            "logloss": 0.927179537173296,
            "mae": 0.2854980960933498,
            "precision": 0.7217741935483871,
            "recall": 0.7617021276595745
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8480487197472215,
            "auditor_fn_violation": 0.010640948963317383,
            "auditor_fp_violation": 0.02505021314969668,
            "ave_precision_score": 0.848386435690863,
            "fpr": 0.1513157894736842,
            "logloss": 0.7048748678342296,
            "mae": 0.2659023726385538,
            "precision": 0.744916820702403,
            "recall": 0.8326446280991735
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8457109282003139,
            "auditor_fn_violation": 0.013522666230702758,
            "auditor_fp_violation": 0.02229241495354088,
            "ave_precision_score": 0.8459444378312108,
            "fpr": 0.1668496158068057,
            "logloss": 0.6910833904506095,
            "mae": 0.27484779547617644,
            "precision": 0.7142857142857143,
            "recall": 0.8085106382978723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.8044629788517089,
            "auditor_fn_violation": 0.0203507865738727,
            "auditor_fp_violation": 0.03274102311854402,
            "ave_precision_score": 0.8048259242734135,
            "fpr": 0.14035087719298245,
            "logloss": 0.5960344072163622,
            "mae": 0.33195472900443274,
            "precision": 0.7349896480331263,
            "recall": 0.7334710743801653
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8089033681052281,
            "auditor_fn_violation": 0.023682182310764414,
            "auditor_fp_violation": 0.016836299100686753,
            "ave_precision_score": 0.809607276656665,
            "fpr": 0.12733260153677278,
            "logloss": 0.5488827418517463,
            "mae": 0.3223556144239311,
            "precision": 0.7537154989384289,
            "recall": 0.7553191489361702
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8497506349231458,
            "auditor_fn_violation": 0.008604284471509354,
            "auditor_fp_violation": 0.02341572388916216,
            "ave_precision_score": 0.8499948348879185,
            "fpr": 0.1524122807017544,
            "logloss": 0.689296414502317,
            "mae": 0.26690494497305556,
            "precision": 0.7372400756143668,
            "recall": 0.8057851239669421
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8407969718417905,
            "auditor_fn_violation": 0.01536772777167948,
            "auditor_fp_violation": 0.01815303508889835,
            "ave_precision_score": 0.8410798151219555,
            "fpr": 0.14928649835345773,
            "logloss": 0.6786251044992028,
            "mae": 0.27588545859573543,
            "precision": 0.734375,
            "recall": 0.8
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.859999368343298,
            "auditor_fn_violation": 0.02057506887052342,
            "auditor_fp_violation": 0.018179209706509273,
            "ave_precision_score": 0.8602956969340455,
            "fpr": 0.18421052631578946,
            "logloss": 0.5383232206858541,
            "mae": 0.3148488149838573,
            "precision": 0.7118353344768439,
            "recall": 0.8574380165289256
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.846193605941715,
            "auditor_fn_violation": 0.016853119088212628,
            "auditor_fp_violation": 0.014337238737426917,
            "ave_precision_score": 0.8464389181415417,
            "fpr": 0.19319429198682767,
            "logloss": 0.5691913879148829,
            "mae": 0.32244905240043514,
            "precision": 0.7006802721088435,
            "recall": 0.8765957446808511
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8464065627782033,
            "auditor_fn_violation": 0.013391238944468609,
            "auditor_fp_violation": 0.025475487784882775,
            "ave_precision_score": 0.8466826595168526,
            "fpr": 0.1513157894736842,
            "logloss": 0.7039744000997028,
            "mae": 0.268924435789327,
            "precision": 0.7430167597765364,
            "recall": 0.8243801652892562
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8466557304004612,
            "auditor_fn_violation": 0.015299997664479061,
            "auditor_fp_violation": 0.018048492723104614,
            "ave_precision_score": 0.846900445488057,
            "fpr": 0.1602634467618002,
            "logloss": 0.6794630645536764,
            "mae": 0.2727248398214099,
            "precision": 0.7229601518026565,
            "recall": 0.8106382978723404
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8168779379762743,
            "auditor_fn_violation": 0.009485555313904602,
            "auditor_fp_violation": 0.023874303164453194,
            "ave_precision_score": 0.8172352044855301,
            "fpr": 0.15899122807017543,
            "logloss": 0.788493107569638,
            "mae": 0.28485951542357474,
            "precision": 0.7284644194756554,
            "recall": 0.8037190082644629
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8189359242625485,
            "auditor_fn_violation": 0.012284840133591799,
            "auditor_fp_violation": 0.02068943201137023,
            "ave_precision_score": 0.8193117002756485,
            "fpr": 0.1734357848518112,
            "logloss": 0.7580688710437447,
            "mae": 0.28647633182100796,
            "precision": 0.7063197026022305,
            "recall": 0.8085106382978723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8479171735626917,
            "auditor_fn_violation": 0.007838552994055394,
            "auditor_fp_violation": 0.026633464502377437,
            "ave_precision_score": 0.8481888228171374,
            "fpr": 0.16885964912280702,
            "logloss": 0.7271330994470666,
            "mae": 0.26816267917873654,
            "precision": 0.7264653641207816,
            "recall": 0.8450413223140496
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.846935656272104,
            "auditor_fn_violation": 0.013725856552303992,
            "auditor_fp_violation": 0.019977548282393818,
            "ave_precision_score": 0.8471564710491725,
            "fpr": 0.1756311745334797,
            "logloss": 0.7144647591782263,
            "mae": 0.27576571551030216,
            "precision": 0.7074954296160878,
            "recall": 0.823404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8389884950512003,
            "auditor_fn_violation": 0.015649920255183414,
            "auditor_fp_violation": 0.014131414986063294,
            "ave_precision_score": 0.8394493968217583,
            "fpr": 0.16228070175438597,
            "logloss": 0.5546469192607822,
            "mae": 0.3215351576781214,
            "precision": 0.7279411764705882,
            "recall": 0.8181818181818182
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8405178946646562,
            "auditor_fn_violation": 0.00797580400308289,
            "auditor_fp_violation": 0.01129804281756611,
            "ave_precision_score": 0.8408309957175722,
            "fpr": 0.16465422612513722,
            "logloss": 0.5231078628782305,
            "mae": 0.3170310397790228,
            "precision": 0.722735674676525,
            "recall": 0.8319148936170213
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8302929312376964,
            "auditor_fn_violation": 0.005731658692185014,
            "auditor_fp_violation": 0.02153016888014429,
            "ave_precision_score": 0.8316118432735753,
            "fpr": 0.14583333333333334,
            "logloss": 0.7250970561967335,
            "mae": 0.2705751007163257,
            "precision": 0.7432432432432432,
            "recall": 0.7954545454545454
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8075596946699706,
            "auditor_fn_violation": 0.01242497138986851,
            "auditor_fp_violation": 0.008716842024039769,
            "ave_precision_score": 0.8088633776096497,
            "fpr": 0.15587266739846323,
            "logloss": 0.7896988258840147,
            "mae": 0.28302987316855166,
            "precision": 0.7204724409448819,
            "recall": 0.7787234042553192
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7642543859649122,
            "auc_prc": 0.8377375067483412,
            "auditor_fn_violation": 0.012099916630419028,
            "auditor_fp_violation": 0.023559190031152654,
            "ave_precision_score": 0.8380941284311463,
            "fpr": 0.14692982456140352,
            "logloss": 0.7360271233101829,
            "mae": 0.26118297513490274,
            "precision": 0.750465549348231,
            "recall": 0.8326446280991735
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8363832428501536,
            "auditor_fn_violation": 0.014807202746572623,
            "auditor_fp_violation": 0.019350294087631403,
            "ave_precision_score": 0.8366286622022734,
            "fpr": 0.16575192096597147,
            "logloss": 0.7245170636252518,
            "mae": 0.27248657760486145,
            "precision": 0.7156308851224106,
            "recall": 0.8085106382978723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8479484973807188,
            "auditor_fn_violation": 0.008590691605045678,
            "auditor_fp_violation": 0.026333722741433023,
            "ave_precision_score": 0.8482136745526544,
            "fpr": 0.15460526315789475,
            "logloss": 0.7001773283697761,
            "mae": 0.2661606968360106,
            "precision": 0.7422303473491774,
            "recall": 0.8388429752066116
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8479794523958673,
            "auditor_fn_violation": 0.01451292710839153,
            "auditor_fp_violation": 0.021311707998237717,
            "ave_precision_score": 0.8482166026564969,
            "fpr": 0.1668496158068057,
            "logloss": 0.6831287039581526,
            "mae": 0.27224844130648557,
            "precision": 0.7164179104477612,
            "recall": 0.8170212765957446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8482011963156793,
            "auditor_fn_violation": 0.013882847614905037,
            "auditor_fp_violation": 0.027975897688145596,
            "ave_precision_score": 0.8484670624956822,
            "fpr": 0.15789473684210525,
            "logloss": 0.6923257629785626,
            "mae": 0.26697446244570994,
            "precision": 0.7377049180327869,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8476241662110086,
            "auditor_fn_violation": 0.008949716234206042,
            "auditor_fp_violation": 0.021998700687739424,
            "ave_precision_score": 0.8478584155130582,
            "fpr": 0.16575192096597147,
            "logloss": 0.670972569145752,
            "mae": 0.2742140111473184,
            "precision": 0.7193308550185874,
            "recall": 0.823404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 2917,
        "test": {
            "accuracy": 0.5986842105263158,
            "auc_prc": 0.6656991179463321,
            "auditor_fn_violation": 0.007698093373930695,
            "auditor_fp_violation": 0.021668511231349403,
            "ave_precision_score": 0.6674436171994733,
            "fpr": 0.37609649122807015,
            "logloss": 0.751116473963347,
            "mae": 0.43660336858748816,
            "precision": 0.5733830845771144,
            "recall": 0.9524793388429752
        },
        "train": {
            "accuracy": 0.5872667398463227,
            "auc_prc": 0.6716098955019737,
            "auditor_fn_violation": 0.006950510311324942,
            "auditor_fp_violation": 0.01817294792047813,
            "ave_precision_score": 0.67306583112496,
            "fpr": 0.3995609220636663,
            "logloss": 0.745007443952653,
            "mae": 0.4324345222620107,
            "precision": 0.5571776155717761,
            "recall": 0.9744680851063829
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.848570428082852,
            "auditor_fn_violation": 0.011488237639553433,
            "auditor_fp_violation": 0.02384612231513363,
            "ave_precision_score": 0.8488402815203282,
            "fpr": 0.15570175438596492,
            "logloss": 0.695757799365145,
            "mae": 0.2666510185522073,
            "precision": 0.7404021937842779,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8488201200732386,
            "auditor_fn_violation": 0.012240465235770841,
            "auditor_fp_violation": 0.02125943681534085,
            "ave_precision_score": 0.8490498340094101,
            "fpr": 0.16794731064763996,
            "logloss": 0.6763223603610798,
            "mae": 0.27227625028503966,
            "precision": 0.7166666666666667,
            "recall": 0.823404255319149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8389231554925793,
            "auditor_fn_violation": 0.017743221690590113,
            "auditor_fp_violation": 0.021724872929988526,
            "ave_precision_score": 0.8392334520976105,
            "fpr": 0.11403508771929824,
            "logloss": 0.5228250619501421,
            "mae": 0.3359162444385414,
            "precision": 0.7805907172995781,
            "recall": 0.7644628099173554
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8378695414103917,
            "auditor_fn_violation": 0.01688581638134386,
            "auditor_fp_violation": 0.011502149341258646,
            "ave_precision_score": 0.8381506510310097,
            "fpr": 0.12403951701427003,
            "logloss": 0.5041137538397334,
            "mae": 0.33081649993081025,
            "precision": 0.7640918580375783,
            "recall": 0.7787234042553192
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 2917,
        "test": {
            "accuracy": 0.6260964912280702,
            "auc_prc": 0.6738351697851894,
            "auditor_fn_violation": 0.009270334928229668,
            "auditor_fp_violation": 0.023021191998688316,
            "ave_precision_score": 0.6063198392053314,
            "fpr": 0.31359649122807015,
            "logloss": 5.284357273490208,
            "mae": 0.38193704775512166,
            "precision": 0.6,
            "recall": 0.8863636363636364
        },
        "train": {
            "accuracy": 0.5993413830954994,
            "auc_prc": 0.6507045929109891,
            "auditor_fn_violation": 0.01006609524254385,
            "auditor_fp_violation": 0.0043733556357047,
            "ave_precision_score": 0.5805269814024261,
            "fpr": 0.3380900109769484,
            "logloss": 5.595377524767056,
            "mae": 0.4058724387451577,
            "precision": 0.5728155339805825,
            "recall": 0.8787234042553191
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8215056854073485,
            "auditor_fn_violation": 0.013352725822821524,
            "auditor_fp_violation": 0.020341449417937377,
            "ave_precision_score": 0.8217748361264899,
            "fpr": 0.14912280701754385,
            "logloss": 0.7757018487579733,
            "mae": 0.286284825364343,
            "precision": 0.7328094302554028,
            "recall": 0.7706611570247934
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8261907589268663,
            "auditor_fn_violation": 0.015554569446715094,
            "auditor_fp_violation": 0.01586803766512094,
            "ave_precision_score": 0.8265190054758469,
            "fpr": 0.15148188803512624,
            "logloss": 0.7294788614907153,
            "mae": 0.28233484145837545,
            "precision": 0.7267326732673267,
            "recall": 0.7808510638297872
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 2917,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.848605789173387,
            "auditor_fn_violation": 0.011488237639553433,
            "auditor_fp_violation": 0.02384612231513363,
            "ave_precision_score": 0.8488724035690907,
            "fpr": 0.15570175438596492,
            "logloss": 0.6965017295548492,
            "mae": 0.26666865464817413,
            "precision": 0.7404021937842779,
            "recall": 0.8367768595041323
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8487879475303647,
            "auditor_fn_violation": 0.012240465235770841,
            "auditor_fp_violation": 0.02125943681534085,
            "ave_precision_score": 0.8490251916699656,
            "fpr": 0.16794731064763996,
            "logloss": 0.6769363360986069,
            "mae": 0.27225934110674277,
            "precision": 0.7166666666666667,
            "recall": 0.823404255319149
        }
    }
]
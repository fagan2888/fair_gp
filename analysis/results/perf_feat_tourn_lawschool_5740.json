[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 5740,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8345461679764532,
            "auditor_fn_violation": 0.01487580604488635,
            "auditor_fp_violation": 0.023323013415892675,
            "ave_precision_score": 0.8348335177842259,
            "fpr": 0.13596491228070176,
            "logloss": 0.8018482568702044,
            "mae": 0.27065262558896397,
            "precision": 0.7524950099800399,
            "recall": 0.7741273100616016
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8316009776362262,
            "auditor_fn_violation": 0.00635346714084577,
            "auditor_fp_violation": 0.018579226866822923,
            "ave_precision_score": 0.8318668540861709,
            "fpr": 0.141602634467618,
            "logloss": 0.7947096892921264,
            "mae": 0.26748157689231344,
            "precision": 0.7388663967611336,
            "recall": 0.7815845824411135
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.8360378606130656,
            "auditor_fn_violation": 0.01046057134623005,
            "auditor_fp_violation": 0.0030650154798761583,
            "ave_precision_score": 0.836335339367321,
            "fpr": 0.19078947368421054,
            "logloss": 0.5710999939746648,
            "mae": 0.3227748577657877,
            "precision": 0.7055837563451777,
            "recall": 0.8562628336755647
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8323294225101607,
            "auditor_fn_violation": 0.010948742116929186,
            "auditor_fp_violation": 0.010269874704561869,
            "ave_precision_score": 0.8327950311072312,
            "fpr": 0.18221734357848518,
            "logloss": 0.5510965843639536,
            "mae": 0.31771075625739825,
            "precision": 0.7072310405643739,
            "recall": 0.8586723768736617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8320236262733507,
            "auditor_fn_violation": 0.014193594870132208,
            "auditor_fp_violation": 0.018890608875129004,
            "ave_precision_score": 0.8322539036217952,
            "fpr": 0.13596491228070176,
            "logloss": 0.8309742710965865,
            "mae": 0.2727676036625487,
            "precision": 0.7489878542510121,
            "recall": 0.7597535934291582
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8208597103443861,
            "auditor_fn_violation": 0.011193196642511116,
            "auditor_fp_violation": 0.016235500044501142,
            "ave_precision_score": 0.8211791121854064,
            "fpr": 0.13721185510428102,
            "logloss": 0.8542332953611975,
            "mae": 0.26954503955512166,
            "precision": 0.7422680412371134,
            "recall": 0.7708779443254818
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8466237509893577,
            "auditor_fn_violation": 0.009924709103353867,
            "auditor_fp_violation": 0.008619711042311663,
            "ave_precision_score": 0.8468552335380143,
            "fpr": 0.09649122807017543,
            "logloss": 0.536848412310206,
            "mae": 0.3216325025304717,
            "precision": 0.7990867579908676,
            "recall": 0.7186858316221766
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.845360890188751,
            "auditor_fn_violation": 0.009251663583562317,
            "auditor_fp_violation": 0.009805084997181596,
            "ave_precision_score": 0.8458783277647245,
            "fpr": 0.10428100987925357,
            "logloss": 0.5017572993653671,
            "mae": 0.3070075199736766,
            "precision": 0.7874720357941835,
            "recall": 0.7537473233404711
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 5740,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7605142952876389,
            "auditor_fn_violation": 0.01734797723260925,
            "auditor_fp_violation": 0.0253482972136223,
            "ave_precision_score": 0.7609985863286803,
            "fpr": 0.14144736842105263,
            "logloss": 1.0620567056139971,
            "mae": 0.326014536432827,
            "precision": 0.7158590308370044,
            "recall": 0.6673511293634496
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7441354964205166,
            "auditor_fn_violation": 0.015280758373155137,
            "auditor_fp_violation": 0.02873043185886216,
            "ave_precision_score": 0.7447751358044816,
            "fpr": 0.15916575192096596,
            "logloss": 1.0991953441192472,
            "mae": 0.32230334597375176,
            "precision": 0.6940928270042194,
            "recall": 0.7044967880085653
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8305444642928728,
            "auditor_fn_violation": 0.017244407219280236,
            "auditor_fp_violation": 0.025843653250773993,
            "ave_precision_score": 0.8308625733555337,
            "fpr": 0.13925438596491227,
            "logloss": 0.8248595205924677,
            "mae": 0.2732348880102023,
            "precision": 0.75,
            "recall": 0.7823408624229979
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8212496964532523,
            "auditor_fn_violation": 0.005996187449610639,
            "auditor_fp_violation": 0.022473076808971433,
            "ave_precision_score": 0.8216267635682283,
            "fpr": 0.14709110867178923,
            "logloss": 0.8463446957387909,
            "mae": 0.2694158180628233,
            "precision": 0.7325349301397206,
            "recall": 0.7858672376873662
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8177857177659346,
            "auditor_fn_violation": 0.005513977448755359,
            "auditor_fp_violation": 0.008779669762641903,
            "ave_precision_score": 0.818243298857866,
            "fpr": 0.16885964912280702,
            "logloss": 0.5686704670187972,
            "mae": 0.328435384804483,
            "precision": 0.7194899817850637,
            "recall": 0.811088295687885
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8179007785641925,
            "auditor_fn_violation": 0.005157050280064968,
            "auditor_fp_violation": 0.0104206841308927,
            "ave_precision_score": 0.8189081136745855,
            "fpr": 0.16355653128430298,
            "logloss": 0.5434614221177008,
            "mae": 0.31914798737249034,
            "precision": 0.7214953271028037,
            "recall": 0.8265524625267666
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.7282084911431481,
            "auditor_fn_violation": 0.025462462624734325,
            "auditor_fp_violation": 0.031050051599587205,
            "ave_precision_score": 0.7165317845347356,
            "fpr": 0.17982456140350878,
            "logloss": 1.7758924715243458,
            "mae": 0.29227182073267516,
            "precision": 0.7055655296229802,
            "recall": 0.8069815195071869
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7315067438897888,
            "auditor_fn_violation": 0.012641119601727169,
            "auditor_fp_violation": 0.02972923527259422,
            "ave_precision_score": 0.7231287356542174,
            "fpr": 0.17672886937431395,
            "logloss": 1.5672383977512356,
            "mae": 0.2798233014049203,
            "precision": 0.7051282051282052,
            "recall": 0.8244111349036403
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8368668637160578,
            "auditor_fn_violation": 0.014252134442883388,
            "auditor_fp_violation": 0.022587719298245615,
            "ave_precision_score": 0.8371497583787598,
            "fpr": 0.13157894736842105,
            "logloss": 0.7771744388146233,
            "mae": 0.2684256329308666,
            "precision": 0.7590361445783133,
            "recall": 0.7761806981519507
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8328843824099256,
            "auditor_fn_violation": 0.007796689051492936,
            "auditor_fp_violation": 0.01194855667962145,
            "ave_precision_score": 0.8331492131085125,
            "fpr": 0.141602634467618,
            "logloss": 0.7699313158200703,
            "mae": 0.26780767154426965,
            "precision": 0.7399193548387096,
            "recall": 0.7858672376873662
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8230644608770953,
            "auditor_fn_violation": 0.003811826794913373,
            "auditor_fp_violation": 0.012061403508771927,
            "ave_precision_score": 0.8234170663877154,
            "fpr": 0.12609649122807018,
            "logloss": 0.583327094105625,
            "mae": 0.3247524359991692,
            "precision": 0.7599164926931107,
            "recall": 0.7474332648870636
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8295710173670329,
            "auditor_fn_violation": 0.007636853400150906,
            "auditor_fp_violation": 0.0027195142453100784,
            "ave_precision_score": 0.8301124908368118,
            "fpr": 0.12623490669593854,
            "logloss": 0.5351243542890963,
            "mae": 0.30999414519621304,
            "precision": 0.7563559322033898,
            "recall": 0.7644539614561028
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.7393311643333859,
            "auditor_fn_violation": 0.009661281025973564,
            "auditor_fp_violation": 0.018235294117647058,
            "ave_precision_score": 0.729937696685625,
            "fpr": 0.12171052631578948,
            "logloss": 1.9050561652401647,
            "mae": 0.32669409728036164,
            "precision": 0.7638297872340426,
            "recall": 0.7371663244353183
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7111904455978252,
            "auditor_fn_violation": 0.01075599912560497,
            "auditor_fp_violation": 0.02271783309104935,
            "ave_precision_score": 0.6986082493430169,
            "fpr": 0.141602634467618,
            "logloss": 2.1164956686894554,
            "mae": 0.3246900969956014,
            "precision": 0.7323651452282157,
            "recall": 0.7558886509635975
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.7565764922008892,
            "auditor_fn_violation": 0.016845887820166435,
            "auditor_fp_violation": 0.003114035087719302,
            "ave_precision_score": 0.7476706455465396,
            "fpr": 0.13048245614035087,
            "logloss": 1.9252636673636272,
            "mae": 0.2789584653373186,
            "precision": 0.7546391752577319,
            "recall": 0.7515400410677618
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7168471780075939,
            "auditor_fn_violation": 0.0011188495593942257,
            "auditor_fp_violation": 0.02013429455800477,
            "ave_precision_score": 0.7048719862179958,
            "fpr": 0.14270032930845225,
            "logloss": 2.1892607249622125,
            "mae": 0.2844694403502244,
            "precision": 0.7308488612836439,
            "recall": 0.7558886509635975
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.854046361582072,
            "auditor_fn_violation": 0.009503674483951152,
            "auditor_fp_violation": 0.008715170278637772,
            "ave_precision_score": 0.8542900183666102,
            "fpr": 0.23793859649122806,
            "logloss": 0.5779155284430998,
            "mae": 0.3427525364524244,
            "precision": 0.6692073170731707,
            "recall": 0.9014373716632443
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8634422958026386,
            "auditor_fn_violation": 0.01441576543648061,
            "auditor_fp_violation": 0.021924229388554317,
            "ave_precision_score": 0.8636852279845839,
            "fpr": 0.21075740944017562,
            "logloss": 0.5489894452040357,
            "mae": 0.3292822719701902,
            "precision": 0.6852459016393443,
            "recall": 0.8950749464668094
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.773189303325998,
            "auditor_fn_violation": 0.01665450844771066,
            "auditor_fp_violation": 0.018975748194014447,
            "ave_precision_score": 0.7744277599057616,
            "fpr": 0.15350877192982457,
            "logloss": 1.0957637099135882,
            "mae": 0.27850579834452266,
            "precision": 0.7348484848484849,
            "recall": 0.7967145790554415
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.7647116840484616,
            "auditor_fn_violation": 0.0046986980445988535,
            "auditor_fp_violation": 0.012690242382887833,
            "ave_precision_score": 0.7640903178663658,
            "fpr": 0.15477497255762898,
            "logloss": 1.1023540453088505,
            "mae": 0.2770875954260983,
            "precision": 0.7267441860465116,
            "recall": 0.8029978586723768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8377362049032557,
            "auditor_fn_violation": 0.011000936633164024,
            "auditor_fp_violation": 0.024613003095975236,
            "ave_precision_score": 0.8380118873833462,
            "fpr": 0.13157894736842105,
            "logloss": 0.7773582172236437,
            "mae": 0.2685437914322042,
            "precision": 0.7580645161290323,
            "recall": 0.7720739219712526
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.834578501165284,
            "auditor_fn_violation": 0.007077428620453794,
            "auditor_fp_violation": 0.012625962955271413,
            "ave_precision_score": 0.8348339796435286,
            "fpr": 0.1394072447859495,
            "logloss": 0.7657042197976038,
            "mae": 0.2671128980209387,
            "precision": 0.742393509127789,
            "recall": 0.7837259100642399
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.7094607991703573,
            "auditor_fn_violation": 0.012020876112251878,
            "auditor_fp_violation": 0.00456656346749226,
            "ave_precision_score": 0.7095714810172455,
            "fpr": 0.06907894736842106,
            "logloss": 6.209012745262936,
            "mae": 0.3939869540208428,
            "precision": 0.7987220447284346,
            "recall": 0.5133470225872689
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7058221147872132,
            "auditor_fn_violation": 0.01701074424650419,
            "auditor_fp_violation": 0.0065342510457768455,
            "ave_precision_score": 0.7073336149214531,
            "fpr": 0.0801317233809001,
            "logloss": 5.58546410101813,
            "mae": 0.3792243006779239,
            "precision": 0.7711598746081505,
            "recall": 0.5267665952890792
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.817879336696965,
            "auditor_fn_violation": 0.009134424871212939,
            "auditor_fp_violation": 0.002494840041279678,
            "ave_precision_score": 0.8183289484125402,
            "fpr": 0.1875,
            "logloss": 0.5773478179195275,
            "mae": 0.328836762234252,
            "precision": 0.7020905923344948,
            "recall": 0.8275154004106776
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8146238904701406,
            "auditor_fn_violation": 0.009418550807757671,
            "auditor_fp_violation": 0.0070979321802592905,
            "ave_precision_score": 0.815683549853377,
            "fpr": 0.17672886937431395,
            "logloss": 0.5528408461539128,
            "mae": 0.32013878398475215,
            "precision": 0.7040441176470589,
            "recall": 0.8201284796573876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7302631578947368,
            "auc_prc": 0.8326994763032762,
            "auditor_fn_violation": 0.014263392053027853,
            "auditor_fp_violation": 0.025090299277605786,
            "ave_precision_score": 0.8329992132098943,
            "fpr": 0.15679824561403508,
            "logloss": 0.803750681975145,
            "mae": 0.28015475922234995,
            "precision": 0.7286527514231499,
            "recall": 0.7885010266940452
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8218227685167251,
            "auditor_fn_violation": 0.004350820450501485,
            "auditor_fp_violation": 0.021652277964023307,
            "ave_precision_score": 0.8221714023868049,
            "fpr": 0.1602634467618002,
            "logloss": 0.8342827066648335,
            "mae": 0.2751642938429199,
            "precision": 0.7219047619047619,
            "recall": 0.8115631691648822
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8179257968096786,
            "auditor_fn_violation": 0.009134424871212939,
            "auditor_fp_violation": 0.0017543859649122872,
            "ave_precision_score": 0.8183751576591427,
            "fpr": 0.18640350877192982,
            "logloss": 0.5767914585333297,
            "mae": 0.3286152171600853,
            "precision": 0.7033158813263525,
            "recall": 0.8275154004106776
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8147589481832417,
            "auditor_fn_violation": 0.009418550807757671,
            "auditor_fp_violation": 0.008267323305742633,
            "ave_precision_score": 0.81581240140729,
            "fpr": 0.1756311745334797,
            "logloss": 0.5522113396158884,
            "mae": 0.3198550290648026,
            "precision": 0.7053406998158379,
            "recall": 0.8201284796573876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7291666666666666,
            "auc_prc": 0.8121766356086844,
            "auditor_fn_violation": 0.00789383623329371,
            "auditor_fp_violation": 0.00647832817337462,
            "ave_precision_score": 0.8128186459483768,
            "fpr": 0.1875,
            "logloss": 0.5799466110367648,
            "mae": 0.3328957621083726,
            "precision": 0.7061855670103093,
            "recall": 0.8439425051334702
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8103234604521047,
            "auditor_fn_violation": 0.0056718150983576885,
            "auditor_fp_violation": 0.007379772747500522,
            "ave_precision_score": 0.8114589793282181,
            "fpr": 0.17453347969264543,
            "logloss": 0.5636055747993995,
            "mae": 0.3266761310575694,
            "precision": 0.7103825136612022,
            "recall": 0.8351177730192719
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 5740,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.840128402029116,
            "auditor_fn_violation": 0.00824507366980079,
            "auditor_fp_violation": 0.010268317853457174,
            "ave_precision_score": 0.8404071752940527,
            "fpr": 0.12609649122807018,
            "logloss": 0.5747271464362351,
            "mae": 0.3114334376468673,
            "precision": 0.7676767676767676,
            "recall": 0.7802874743326489
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8491046099175452,
            "auditor_fn_violation": 0.005591897272686676,
            "auditor_fp_violation": 0.013454178657252201,
            "ave_precision_score": 0.8493508291298478,
            "fpr": 0.13721185510428102,
            "logloss": 0.5202052094187578,
            "mae": 0.30235158889426295,
            "precision": 0.7484909456740443,
            "recall": 0.7965738758029979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.7958508082470958,
            "auditor_fn_violation": 0.015711120717605106,
            "auditor_fp_violation": 0.01823013415892674,
            "ave_precision_score": 0.7613880363857575,
            "fpr": 0.12828947368421054,
            "logloss": 3.816523208170626,
            "mae": 0.28098989731387863,
            "precision": 0.7592592592592593,
            "recall": 0.757700205338809
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7809506701671347,
            "auditor_fn_violation": 0.00583400127398417,
            "auditor_fp_violation": 0.01887342886245192,
            "ave_precision_score": 0.7424325195193793,
            "fpr": 0.14270032930845225,
            "logloss": 4.014749716857761,
            "mae": 0.2821498388819596,
            "precision": 0.731404958677686,
            "recall": 0.7580299785867237
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8330287950906416,
            "auditor_fn_violation": 0.015026658020822078,
            "auditor_fp_violation": 0.025005159958720338,
            "ave_precision_score": 0.83331120220576,
            "fpr": 0.14364035087719298,
            "logloss": 0.8369985007815303,
            "mae": 0.27190552452499844,
            "precision": 0.741106719367589,
            "recall": 0.7700205338809035
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8265037204731532,
            "auditor_fn_violation": 0.007740276468666338,
            "auditor_fp_violation": 0.018359193441520552,
            "ave_precision_score": 0.8267871542105023,
            "fpr": 0.145993413830955,
            "logloss": 0.8420133404864629,
            "mae": 0.2700215870117653,
            "precision": 0.7323943661971831,
            "recall": 0.7794432548179872
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8342223436849467,
            "auditor_fn_violation": 0.0120163730681941,
            "auditor_fp_violation": 0.003921568627450985,
            "ave_precision_score": 0.8345020323676153,
            "fpr": 0.18092105263157895,
            "logloss": 0.5696283961158203,
            "mae": 0.3223210455415252,
            "precision": 0.7164948453608248,
            "recall": 0.8562628336755647
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8340558661618502,
            "auditor_fn_violation": 0.0106901844456406,
            "auditor_fp_violation": 0.008381048446910143,
            "ave_precision_score": 0.8344630660240785,
            "fpr": 0.18111964873765093,
            "logloss": 0.5469150949216925,
            "mae": 0.31726635395796254,
            "precision": 0.7048300536672629,
            "recall": 0.8436830835117773
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8509414513086123,
            "auditor_fn_violation": 0.007932112107784871,
            "auditor_fp_violation": 0.013751289989680083,
            "ave_precision_score": 0.851160131812501,
            "fpr": 0.08552631578947369,
            "logloss": 0.5435268780167573,
            "mae": 0.3199680968226411,
            "precision": 0.8133971291866029,
            "recall": 0.6981519507186859
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8545723852983955,
            "auditor_fn_violation": 0.007963576275688295,
            "auditor_fp_violation": 0.00848488444536743,
            "ave_precision_score": 0.8550665058324161,
            "fpr": 0.09659714599341383,
            "logloss": 0.4964229515086123,
            "mae": 0.30667875575960635,
            "precision": 0.7953488372093023,
            "recall": 0.7323340471092077
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8336064107593351,
            "auditor_fn_violation": 0.013338016499153432,
            "auditor_fp_violation": 0.025131578947368425,
            "ave_precision_score": 0.8338992806193164,
            "fpr": 0.14912280701754385,
            "logloss": 0.8084073874416321,
            "mae": 0.2716443940820375,
            "precision": 0.7379576107899807,
            "recall": 0.7864476386036962
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.82612161141813,
            "auditor_fn_violation": 0.007230212698942501,
            "auditor_fp_violation": 0.016398670899219755,
            "ave_precision_score": 0.8264166181800155,
            "fpr": 0.15477497255762898,
            "logloss": 0.819277057491053,
            "mae": 0.26970117449530584,
            "precision": 0.7262135922330097,
            "recall": 0.8008565310492506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8372604864377214,
            "auditor_fn_violation": 0.013378543895673481,
            "auditor_fp_violation": 0.020668214654282778,
            "ave_precision_score": 0.8375448018003142,
            "fpr": 0.14035087719298245,
            "logloss": 0.7779526401030186,
            "mae": 0.26933824467085926,
            "precision": 0.75,
            "recall": 0.7885010266940452
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8325551231493686,
            "auditor_fn_violation": 0.007159696970409251,
            "auditor_fp_violation": 0.020752365977393418,
            "ave_precision_score": 0.8328259187447673,
            "fpr": 0.150384193194292,
            "logloss": 0.7781956418996879,
            "mae": 0.2673890149519855,
            "precision": 0.7313725490196078,
            "recall": 0.7987152034261242
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8360726722071475,
            "auditor_fn_violation": 0.012414892467307901,
            "auditor_fp_violation": 0.010206398348813211,
            "ave_precision_score": 0.8363442670891402,
            "fpr": 0.19298245614035087,
            "logloss": 0.5770588187392695,
            "mae": 0.32408990203660787,
            "precision": 0.7061769616026711,
            "recall": 0.8685831622176592
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8359351586233997,
            "auditor_fn_violation": 0.010053192364556917,
            "auditor_fp_violation": 0.011342846688620567,
            "ave_precision_score": 0.8363244374516802,
            "fpr": 0.19319429198682767,
            "logloss": 0.5546169233236634,
            "mae": 0.3190448493914629,
            "precision": 0.6949740034662045,
            "recall": 0.8586723768736617
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8362432944262046,
            "auditor_fn_violation": 0.013590186966389283,
            "auditor_fp_violation": 0.01359649122807018,
            "ave_precision_score": 0.8369645002842703,
            "fpr": 0.09320175438596491,
            "logloss": 0.7002055506509395,
            "mae": 0.318708074519724,
            "precision": 0.8023255813953488,
            "recall": 0.7084188911704312
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8365793551761509,
            "auditor_fn_violation": 0.005791691836864221,
            "auditor_fp_violation": 0.010437990130635578,
            "ave_precision_score": 0.8368664101576918,
            "fpr": 0.09110867178924259,
            "logloss": 0.8029612953488457,
            "mae": 0.31315677131352826,
            "precision": 0.7970660146699267,
            "recall": 0.6980728051391863
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8361088334965179,
            "auditor_fn_violation": 0.011430977340682305,
            "auditor_fp_violation": 0.023508771929824562,
            "ave_precision_score": 0.8363973232389827,
            "fpr": 0.14035087719298245,
            "logloss": 0.7811699852612066,
            "mae": 0.27253185403901914,
            "precision": 0.747534516765286,
            "recall": 0.7782340862422998
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8317910713043704,
            "auditor_fn_violation": 0.00521111233860713,
            "auditor_fp_violation": 0.01951622313861612,
            "ave_precision_score": 0.8320802624648899,
            "fpr": 0.1437980241492865,
            "logloss": 0.7830468411827728,
            "mae": 0.2675648394116943,
            "precision": 0.7369477911646586,
            "recall": 0.7858672376873662
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7675438596491229,
            "auc_prc": 0.8605429965553405,
            "auditor_fn_violation": 0.006223206887856198,
            "auditor_fp_violation": 0.011240970072239424,
            "ave_precision_score": 0.8607722665421669,
            "fpr": 0.07785087719298246,
            "logloss": 0.5198173373180726,
            "mae": 0.29259475435528176,
            "precision": 0.829736211031175,
            "recall": 0.7104722792607803
        },
        "train": {
            "accuracy": 0.7716794731064764,
            "auc_prc": 0.8663029690176751,
            "auditor_fn_violation": 0.008217432898407996,
            "auditor_fp_violation": 0.004410557648757429,
            "ave_precision_score": 0.866550796468764,
            "fpr": 0.08781558726673985,
            "logloss": 0.4863788156791163,
            "mae": 0.2811518671444169,
            "precision": 0.8090692124105012,
            "recall": 0.7259100642398287
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.6971916023538778,
            "auditor_fn_violation": 0.05840448142944632,
            "auditor_fp_violation": 0.019287925696594436,
            "ave_precision_score": 0.6977047796822429,
            "fpr": 0.09100877192982457,
            "logloss": 1.7033159109027196,
            "mae": 0.4163040813268133,
            "precision": 0.7287581699346405,
            "recall": 0.45790554414784396
        },
        "train": {
            "accuracy": 0.6377607025246982,
            "auc_prc": 0.6956262480336164,
            "auditor_fn_violation": 0.05033412702703338,
            "auditor_fp_violation": 0.02196378595939518,
            "ave_precision_score": 0.6957565877076097,
            "fpr": 0.08342480790340286,
            "logloss": 1.5622181563742281,
            "mae": 0.3991684833898072,
            "precision": 0.7370242214532872,
            "recall": 0.45610278372591007
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8361336917589579,
            "auditor_fn_violation": 0.015841708995280816,
            "auditor_fp_violation": 0.025141898864809088,
            "ave_precision_score": 0.8364228720552371,
            "fpr": 0.1425438596491228,
            "logloss": 0.7711943371231263,
            "mae": 0.27158327333058413,
            "precision": 0.748062015503876,
            "recall": 0.7926078028747433
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8325885292670431,
            "auditor_fn_violation": 0.005048926162980654,
            "auditor_fp_violation": 0.020094737987163894,
            "ave_precision_score": 0.8328818621199179,
            "fpr": 0.150384193194292,
            "logloss": 0.7728393712938247,
            "mae": 0.26754082000659457,
            "precision": 0.7318982387475538,
            "recall": 0.8008565310492506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8307627850692874,
            "auditor_fn_violation": 0.016602723441046144,
            "auditor_fp_violation": 0.013805469556243554,
            "ave_precision_score": 0.8310731036959014,
            "fpr": 0.09429824561403509,
            "logloss": 1.0401845909138745,
            "mae": 0.2840188251425209,
            "precision": 0.7932692307692307,
            "recall": 0.6776180698151951
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8197980200538366,
            "auditor_fn_violation": 0.00731718209746684,
            "auditor_fp_violation": 0.014826297208295019,
            "ave_precision_score": 0.8201977234750513,
            "fpr": 0.10867178924259056,
            "logloss": 0.9872979513388382,
            "mae": 0.27884103425694357,
            "precision": 0.7665094339622641,
            "recall": 0.69593147751606
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8237688620148826,
            "auditor_fn_violation": 0.007297182895637452,
            "auditor_fp_violation": 0.0019633642930856595,
            "ave_precision_score": 0.8241321527738953,
            "fpr": 0.16228070175438597,
            "logloss": 0.5635261108448629,
            "mae": 0.3250563720187832,
            "precision": 0.7228464419475655,
            "recall": 0.7926078028747433
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8155656486758582,
            "auditor_fn_violation": 0.008962549096575992,
            "auditor_fp_violation": 0.008855727297000626,
            "ave_precision_score": 0.8171000867140246,
            "fpr": 0.1602634467618002,
            "logloss": 0.5384470023912925,
            "mae": 0.31638776297339455,
            "precision": 0.7186897880539499,
            "recall": 0.7987152034261242
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.7460501101355469,
            "auditor_fn_violation": 0.019896700169314455,
            "auditor_fp_violation": 0.005562435500515997,
            "ave_precision_score": 0.7366162512366974,
            "fpr": 0.13815789473684212,
            "logloss": 2.0527155978932106,
            "mae": 0.28359292831128174,
            "precision": 0.7495029821073559,
            "recall": 0.7741273100616016
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.708677809259161,
            "auditor_fn_violation": 0.0042850057705371165,
            "auditor_fp_violation": 0.019723895135530706,
            "ave_precision_score": 0.6951276229140734,
            "fpr": 0.150384193194292,
            "logloss": 2.3446554194141083,
            "mae": 0.28804209504317585,
            "precision": 0.7276341948310139,
            "recall": 0.7837259100642399
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8337733428787039,
            "auditor_fn_violation": 0.013338016499153432,
            "auditor_fp_violation": 0.026130030959752328,
            "ave_precision_score": 0.834070137593879,
            "fpr": 0.14144736842105263,
            "logloss": 0.822156169835782,
            "mae": 0.26991450426135083,
            "precision": 0.748046875,
            "recall": 0.7864476386036962
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.830390682442646,
            "auditor_fn_violation": 0.006548560656454422,
            "auditor_fp_violation": 0.02057683369428705,
            "ave_precision_score": 0.8306608128352344,
            "fpr": 0.150384193194292,
            "logloss": 0.8180289620308954,
            "mae": 0.26744164803213216,
            "precision": 0.7297830374753451,
            "recall": 0.7922912205567452
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8333377921013555,
            "auditor_fn_violation": 0.01126436471054433,
            "auditor_fp_violation": 0.02569659442724458,
            "ave_precision_score": 0.8336303496236248,
            "fpr": 0.14802631578947367,
            "logloss": 0.816430569600059,
            "mae": 0.2717059769764628,
            "precision": 0.7388781431334622,
            "recall": 0.784394250513347
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.825345692525661,
            "auditor_fn_violation": 0.008029390955652659,
            "auditor_fp_violation": 0.016826376321436697,
            "ave_precision_score": 0.8256435193276161,
            "fpr": 0.15587266739846323,
            "logloss": 0.8286062078073453,
            "mae": 0.2696678627863309,
            "precision": 0.7242718446601941,
            "recall": 0.7987152034261242
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.83526163863146,
            "auditor_fn_violation": 0.014038239850138693,
            "auditor_fp_violation": 0.024342105263157898,
            "ave_precision_score": 0.8355522980946277,
            "fpr": 0.14912280701754385,
            "logloss": 0.8095045644688644,
            "mae": 0.2701834001242007,
            "precision": 0.7394636015325671,
            "recall": 0.7926078028747433
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8272590407005384,
            "auditor_fn_violation": 0.00884032183378503,
            "auditor_fp_violation": 0.016903017177440884,
            "ave_precision_score": 0.8275489733996033,
            "fpr": 0.15697036223929747,
            "logloss": 0.8192628276570133,
            "mae": 0.2692611601388509,
            "precision": 0.7239382239382239,
            "recall": 0.8029978586723768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7127192982456141,
            "auc_prc": 0.8177081012253071,
            "auditor_fn_violation": 0.031361450340430135,
            "auditor_fp_violation": 0.0260139318885449,
            "ave_precision_score": 0.8181209395951125,
            "fpr": 0.17982456140350878,
            "logloss": 0.8515445682243231,
            "mae": 0.33146717742295756,
            "precision": 0.7034358047016275,
            "recall": 0.7987679671457906
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8145202315099359,
            "auditor_fn_violation": 0.033541981538982274,
            "auditor_fp_violation": 0.02897766042661762,
            "ave_precision_score": 0.8149713500949091,
            "fpr": 0.16355653128430298,
            "logloss": 0.9944633421850663,
            "mae": 0.3225393222557454,
            "precision": 0.7172675521821632,
            "recall": 0.8094218415417559
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7796052631578947,
            "auc_prc": 0.8674695573575477,
            "auditor_fn_violation": 0.006560935192189923,
            "auditor_fp_violation": 0.017690918472652223,
            "ave_precision_score": 0.8676947188987093,
            "fpr": 0.125,
            "logloss": 0.5312314586849165,
            "mae": 0.28249506775563993,
            "precision": 0.7782101167315175,
            "recall": 0.8213552361396304
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8782027280539715,
            "auditor_fn_violation": 0.007462914603102225,
            "auditor_fp_violation": 0.020794394833911854,
            "ave_precision_score": 0.8783935001953685,
            "fpr": 0.1394072447859495,
            "logloss": 0.48487856194190415,
            "mae": 0.2753558525568827,
            "precision": 0.7548262548262549,
            "recall": 0.8372591006423983
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8492526055283859,
            "auditor_fn_violation": 0.009674790158146911,
            "auditor_fp_violation": 0.013273993808049542,
            "ave_precision_score": 0.8494896124068432,
            "fpr": 0.09978070175438597,
            "logloss": 0.5254429468156505,
            "mae": 0.3187544653739774,
            "precision": 0.7977777777777778,
            "recall": 0.7371663244353183
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8505472239385236,
            "auditor_fn_violation": 0.01000618187886808,
            "auditor_fp_violation": 0.009634497285430326,
            "ave_precision_score": 0.8509133144493969,
            "fpr": 0.10647639956092206,
            "logloss": 0.4949585467565901,
            "mae": 0.3068896565183731,
            "precision": 0.7853982300884956,
            "recall": 0.7601713062098501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8475130295584974,
            "auditor_fn_violation": 0.009107406606866245,
            "auditor_fp_violation": 0.00890608875128999,
            "ave_precision_score": 0.8477334230156011,
            "fpr": 0.11403508771929824,
            "logloss": 0.5282034972587918,
            "mae": 0.32124501999427785,
            "precision": 0.7796610169491526,
            "recall": 0.75564681724846
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8499663337270775,
            "auditor_fn_violation": 0.008993105912273735,
            "auditor_fp_violation": 0.012349066959385294,
            "ave_precision_score": 0.8502893036451111,
            "fpr": 0.12184412733260154,
            "logloss": 0.49786114395432174,
            "mae": 0.3098580848414637,
            "precision": 0.7653276955602537,
            "recall": 0.7751605995717344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.8165719342226581,
            "auditor_fn_violation": 0.006617223242912212,
            "auditor_fp_violation": 0.0081217750257998,
            "ave_precision_score": 0.8170342769062922,
            "fpr": 0.18421052631578946,
            "logloss": 0.5779883913876627,
            "mae": 0.32944216987558766,
            "precision": 0.7047451669595782,
            "recall": 0.8234086242299795
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.813292231094815,
            "auditor_fn_violation": 0.00884502288235391,
            "auditor_fp_violation": 0.008798864726416876,
            "ave_precision_score": 0.814368468498119,
            "fpr": 0.17672886937431395,
            "logloss": 0.5535102631376911,
            "mae": 0.32070308140230863,
            "precision": 0.7034990791896869,
            "recall": 0.8179871520342612
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8177844311872677,
            "auditor_fn_violation": 0.006209697755682845,
            "auditor_fp_violation": 0.002494840041279678,
            "ave_precision_score": 0.8182341323557194,
            "fpr": 0.1875,
            "logloss": 0.5773754165031022,
            "mae": 0.3284967650340006,
            "precision": 0.7015706806282722,
            "recall": 0.8254620123203286
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8146673609698302,
            "auditor_fn_violation": 0.00884502288235391,
            "auditor_fp_violation": 0.0070979321802592905,
            "ave_precision_score": 0.8157251937115438,
            "fpr": 0.17672886937431395,
            "logloss": 0.5526566925526463,
            "mae": 0.3197118779535111,
            "precision": 0.7034990791896869,
            "recall": 0.8179871520342612
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 5740,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8361781626743112,
            "auditor_fn_violation": 0.010667711372888074,
            "auditor_fp_violation": 0.023310113519091846,
            "ave_precision_score": 0.83647163601274,
            "fpr": 0.13925438596491227,
            "logloss": 0.7501006609255905,
            "mae": 0.27104102586070117,
            "precision": 0.7524366471734892,
            "recall": 0.7926078028747433
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8361073992220622,
            "auditor_fn_violation": 0.003955932370715286,
            "auditor_fp_violation": 0.017325778028302728,
            "ave_precision_score": 0.8363850686297194,
            "fpr": 0.14489571899012074,
            "logloss": 0.7413043916917661,
            "mae": 0.2680980491700104,
            "precision": 0.7391304347826086,
            "recall": 0.8008565310492506
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8371712333200348,
            "auditor_fn_violation": 0.02221126481501496,
            "auditor_fp_violation": 0.019871001031991745,
            "ave_precision_score": 0.8374956396647688,
            "fpr": 0.09539473684210527,
            "logloss": 0.881548619961292,
            "mae": 0.2752263013427097,
            "precision": 0.7981438515081206,
            "recall": 0.7063655030800822
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8374967795782887,
            "auditor_fn_violation": 0.015313665713137315,
            "auditor_fp_violation": 0.01783506887787898,
            "ave_precision_score": 0.8377304441490814,
            "fpr": 0.10318331503841932,
            "logloss": 0.8955528346012356,
            "mae": 0.270770511684221,
            "precision": 0.7793427230046949,
            "recall": 0.7109207708779444
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8538139266448975,
            "auditor_fn_violation": 0.002427140747145071,
            "auditor_fp_violation": 0.01618679050567596,
            "ave_precision_score": 0.8540765602777122,
            "fpr": 0.08991228070175439,
            "logloss": 0.5231485024326766,
            "mae": 0.3096503605368998,
            "precision": 0.8110599078341014,
            "recall": 0.7227926078028748
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8637899642813225,
            "auditor_fn_violation": 0.0022682559344861897,
            "auditor_fp_violation": 0.017078549460547272,
            "ave_precision_score": 0.8639987217195585,
            "fpr": 0.10647639956092206,
            "logloss": 0.48327059076073625,
            "mae": 0.30275575012250894,
            "precision": 0.7834821428571429,
            "recall": 0.7516059957173448
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8365664732442601,
            "auditor_fn_violation": 0.012944000144097412,
            "auditor_fp_violation": 0.026971104231166155,
            "ave_precision_score": 0.8368562619180256,
            "fpr": 0.1611842105263158,
            "logloss": 0.8053772385837528,
            "mae": 0.2711973749592191,
            "precision": 0.7302752293577982,
            "recall": 0.8172484599589322
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8335732263455418,
            "auditor_fn_violation": 0.00852535157966985,
            "auditor_fp_violation": 0.023783388218075376,
            "ave_precision_score": 0.8338551939602137,
            "fpr": 0.16245883644346873,
            "logloss": 0.8116350282950439,
            "mae": 0.26783644472807533,
            "precision": 0.7218045112781954,
            "recall": 0.8222698072805139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8218556241578717,
            "auditor_fn_violation": 0.006707284124067873,
            "auditor_fp_violation": 0.012600619195046441,
            "ave_precision_score": 0.8222137950644539,
            "fpr": 0.09539473684210527,
            "logloss": 0.547756147659943,
            "mae": 0.3366311285277142,
            "precision": 0.7872860635696821,
            "recall": 0.6611909650924025
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.8084636541643724,
            "auditor_fn_violation": 0.0031332488711607203,
            "auditor_fp_violation": 0.010598688699676628,
            "ave_precision_score": 0.8089983463953617,
            "fpr": 0.09989023051591657,
            "logloss": 0.5433669967950304,
            "mae": 0.3346667511779181,
            "precision": 0.7753086419753087,
            "recall": 0.6723768736616702
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8300145161559875,
            "auditor_fn_violation": 0.01191730609892287,
            "auditor_fp_violation": 0.022768317853457176,
            "ave_precision_score": 0.8302861090415345,
            "fpr": 0.13925438596491227,
            "logloss": 0.7883115731504461,
            "mae": 0.2753774675253883,
            "precision": 0.7490118577075099,
            "recall": 0.7782340862422998
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.8340865545002445,
            "auditor_fn_violation": 0.0033565486781826723,
            "auditor_fp_violation": 0.019135491144272702,
            "ave_precision_score": 0.8344192849626173,
            "fpr": 0.141602634467618,
            "logloss": 0.7711534657008356,
            "mae": 0.26561368943673014,
            "precision": 0.7414829659318637,
            "recall": 0.7922912205567452
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8373323809331117,
            "auditor_fn_violation": 0.015069436939371014,
            "auditor_fp_violation": 0.021455108359133127,
            "ave_precision_score": 0.8376216232020162,
            "fpr": 0.16228070175438597,
            "logloss": 0.804705161847482,
            "mae": 0.2694717606272589,
            "precision": 0.7318840579710145,
            "recall": 0.8295687885010267
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.83169820900785,
            "auditor_fn_violation": 0.011754971946492668,
            "auditor_fp_violation": 0.019009404574717413,
            "ave_precision_score": 0.831973527604387,
            "fpr": 0.17453347969264543,
            "logloss": 0.8165171163436709,
            "mae": 0.26987975018756827,
            "precision": 0.7109090909090909,
            "recall": 0.8372591006423983
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8313530180429392,
            "auditor_fn_violation": 0.02156507799272308,
            "auditor_fp_violation": 0.021382868937048504,
            "ave_precision_score": 0.8316581516416539,
            "fpr": 0.11293859649122807,
            "logloss": 0.9076669770928557,
            "mae": 0.2731844733128359,
            "precision": 0.7736263736263737,
            "recall": 0.7227926078028748
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8261085482186675,
            "auditor_fn_violation": 0.008057597247065968,
            "auditor_fp_violation": 0.020332077412209135,
            "ave_precision_score": 0.8263831021918309,
            "fpr": 0.1207464324917673,
            "logloss": 0.9012038660598467,
            "mae": 0.2698945256833837,
            "precision": 0.7560975609756098,
            "recall": 0.7301927194860813
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8212959402423137,
            "auditor_fn_violation": 0.019444144241507262,
            "auditor_fp_violation": 0.02107843137254902,
            "ave_precision_score": 0.8216826634681973,
            "fpr": 0.13486842105263158,
            "logloss": 0.8171053632590771,
            "mae": 0.2704185381818208,
            "precision": 0.7559523809523809,
            "recall": 0.7823408624229979
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8321501369241331,
            "auditor_fn_violation": 0.011136784059684515,
            "auditor_fp_violation": 0.01886848429109681,
            "ave_precision_score": 0.8324150822386258,
            "fpr": 0.14489571899012074,
            "logloss": 0.7759259269146727,
            "mae": 0.26409251062760053,
            "precision": 0.7401574803149606,
            "recall": 0.8051391862955032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8296587295362756,
            "auditor_fn_violation": 0.009931463669440546,
            "auditor_fp_violation": 0.01935242518059856,
            "ave_precision_score": 0.8299402403622502,
            "fpr": 0.13925438596491227,
            "logloss": 0.8282028306799674,
            "mae": 0.273501245863176,
            "precision": 0.7465069860279441,
            "recall": 0.7679671457905544
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8198099248382971,
            "auditor_fn_violation": 0.006381673432259065,
            "auditor_fp_violation": 0.015797905479573982,
            "ave_precision_score": 0.8201196175786627,
            "fpr": 0.15148188803512624,
            "logloss": 0.8560163395763196,
            "mae": 0.27477147082295944,
            "precision": 0.724,
            "recall": 0.7751605995717344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7535234471251674,
            "auditor_fn_violation": 0.0026725566482942473,
            "auditor_fp_violation": 0.014736842105263163,
            "ave_precision_score": 0.7447920148642556,
            "fpr": 0.19517543859649122,
            "logloss": 2.2253979366199563,
            "mae": 0.37609717859816827,
            "precision": 0.6763636363636364,
            "recall": 0.7638603696098563
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7585653600932514,
            "auditor_fn_violation": 0.004604677073221194,
            "auditor_fp_violation": 0.017751011164842134,
            "ave_precision_score": 0.7480734401782998,
            "fpr": 0.19209659714599342,
            "logloss": 1.9964105832059282,
            "mae": 0.36573217108170186,
            "precision": 0.6673003802281369,
            "recall": 0.7516059957173448
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.8339615291271397,
            "auditor_fn_violation": 0.013770308728700605,
            "auditor_fp_violation": 0.017667698658410735,
            "ave_precision_score": 0.8342599663623714,
            "fpr": 0.12390350877192982,
            "logloss": 0.8133458522233776,
            "mae": 0.2707304478035395,
            "precision": 0.7645833333333333,
            "recall": 0.7535934291581109
        },
        "train": {
            "accuracy": 0.7420417124039517,
            "auc_prc": 0.8266882725778997,
            "auditor_fn_violation": 0.00866873356102079,
            "auditor_fp_violation": 0.01708843860325749,
            "ave_precision_score": 0.826992594481005,
            "fpr": 0.13611416026344675,
            "logloss": 0.8167651492224162,
            "mae": 0.2675630357779303,
            "precision": 0.7416666666666667,
            "recall": 0.7623126338329764
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8169486541135529,
            "auditor_fn_violation": 0.007063024604632734,
            "auditor_fp_violation": 0.006715686274509802,
            "ave_precision_score": 0.8174079168188214,
            "fpr": 0.18311403508771928,
            "logloss": 0.5781728018518445,
            "mae": 0.32882433871750355,
            "precision": 0.7065026362038664,
            "recall": 0.8254620123203286
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8137730965960442,
            "auditor_fn_violation": 0.0075310798073510285,
            "auditor_fp_violation": 0.007557777316284457,
            "ave_precision_score": 0.8148398900967282,
            "fpr": 0.17453347969264543,
            "logloss": 0.5534714398648551,
            "mae": 0.31994525803441926,
            "precision": 0.7066420664206642,
            "recall": 0.8201284796573876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.832388655326192,
            "auditor_fn_violation": 0.012097427861234203,
            "auditor_fp_violation": 0.0251780185758514,
            "ave_precision_score": 0.8326847608910355,
            "fpr": 0.13815789473684212,
            "logloss": 0.8415044850125715,
            "mae": 0.27211623402406526,
            "precision": 0.749003984063745,
            "recall": 0.7720739219712526
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8274999635947116,
            "auditor_fn_violation": 0.00645924073364564,
            "auditor_fp_violation": 0.01578307176550865,
            "ave_precision_score": 0.8277869754382151,
            "fpr": 0.14489571899012074,
            "logloss": 0.8415733002238595,
            "mae": 0.2683675690131698,
            "precision": 0.7327935222672065,
            "recall": 0.7751605995717344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8343192062801933,
            "auditor_fn_violation": 0.01723314960913578,
            "auditor_fp_violation": 0.02367389060887513,
            "ave_precision_score": 0.834614534869191,
            "fpr": 0.1513157894736842,
            "logloss": 0.8152778296455543,
            "mae": 0.26945938303878475,
            "precision": 0.7376425855513308,
            "recall": 0.7967145790554415
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8282569411648745,
            "auditor_fn_violation": 0.008854424979491673,
            "auditor_fp_violation": 0.018492696868108508,
            "ave_precision_score": 0.8285362445710835,
            "fpr": 0.15916575192096596,
            "logloss": 0.8211359204321691,
            "mae": 0.2688134053209787,
            "precision": 0.7243346007604563,
            "recall": 0.815845824411135
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7984035440508481,
            "auditor_fn_violation": 0.03105524334450088,
            "auditor_fp_violation": 0.02733230134158927,
            "ave_precision_score": 0.7991102225277475,
            "fpr": 0.14364035087719298,
            "logloss": 1.6116797400670404,
            "mae": 0.2829696929065465,
            "precision": 0.7405940594059406,
            "recall": 0.7679671457905544
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7979291273035533,
            "auditor_fn_violation": 0.024755721763739403,
            "auditor_fp_violation": 0.040248810830589093,
            "ave_precision_score": 0.7982931780856923,
            "fpr": 0.14928649835345773,
            "logloss": 1.6568563001673817,
            "mae": 0.27737184983991336,
            "precision": 0.7241379310344828,
            "recall": 0.7644539614561028
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5734649122807017,
            "auc_prc": 0.8538113119955251,
            "auditor_fn_violation": 0.0007384992254764221,
            "auditor_fp_violation": 0.006052631578947393,
            "ave_precision_score": 0.8542944705769031,
            "fpr": 0.4199561403508772,
            "logloss": 1.0760490219073335,
            "mae": 0.392132291068903,
            "precision": 0.5567129629629629,
            "recall": 0.9876796714579056
        },
        "train": {
            "accuracy": 0.5587266739846323,
            "auc_prc": 0.8607432934773576,
            "auditor_fn_violation": 0.0010647875008520653,
            "auditor_fp_violation": 0.0043067216503001535,
            "ave_precision_score": 0.860999696275996,
            "fpr": 0.43688254665203075,
            "logloss": 1.0939337439319412,
            "mae": 0.4014128478316531,
            "precision": 0.537746806039489,
            "recall": 0.9914346895074947
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8360112950827336,
            "auditor_fn_violation": 0.016147915991210057,
            "auditor_fp_violation": 0.024406604747162036,
            "ave_precision_score": 0.8362951925297144,
            "fpr": 0.13815789473684212,
            "logloss": 0.7935327697902449,
            "mae": 0.27058285843754826,
            "precision": 0.7514792899408284,
            "recall": 0.7823408624229979
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8302081081365935,
            "auditor_fn_violation": 0.005784640264010891,
            "auditor_fp_violation": 0.019906844275669747,
            "ave_precision_score": 0.8304849487084668,
            "fpr": 0.14489571899012074,
            "logloss": 0.7940692938643239,
            "mae": 0.2680621047294556,
            "precision": 0.736,
            "recall": 0.7880085653104925
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.8403833562114627,
            "auditor_fn_violation": 0.005113206527612672,
            "auditor_fp_violation": 0.01545407636738907,
            "ave_precision_score": 0.8406977744173927,
            "fpr": 0.14692982456140352,
            "logloss": 0.5921962018185071,
            "mae": 0.31019060748087296,
            "precision": 0.7490636704119851,
            "recall": 0.8213552361396304
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8519840519385609,
            "auditor_fn_violation": 0.007413553593128953,
            "auditor_fp_violation": 0.014158780075355273,
            "ave_precision_score": 0.8522137769894755,
            "fpr": 0.15477497255762898,
            "logloss": 0.5355133759413405,
            "mae": 0.30287805373139903,
            "precision": 0.7339622641509433,
            "recall": 0.8329764453961456
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8348177808567376,
            "auditor_fn_violation": 0.00145448323066393,
            "auditor_fp_violation": 0.007094943240454081,
            "ave_precision_score": 0.835120604515501,
            "fpr": 0.13706140350877194,
            "logloss": 0.5335700550067516,
            "mae": 0.33297243418596817,
            "precision": 0.7563352826510721,
            "recall": 0.7967145790554415
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8330301262501565,
            "auditor_fn_violation": 0.005888063332526322,
            "auditor_fp_violation": 0.006452665618417548,
            "ave_precision_score": 0.8338672449596581,
            "fpr": 0.13721185510428102,
            "logloss": 0.517086863727064,
            "mae": 0.3269645770854275,
            "precision": 0.7469635627530364,
            "recall": 0.7901498929336188
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8318444716663637,
            "auditor_fn_violation": 0.012009618502107423,
            "auditor_fp_violation": 0.026130030959752328,
            "ave_precision_score": 0.8321368528452875,
            "fpr": 0.14144736842105263,
            "logloss": 0.844010618733818,
            "mae": 0.27197330754247384,
            "precision": 0.7450592885375494,
            "recall": 0.7741273100616016
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8264126054995291,
            "auditor_fn_violation": 0.008751001910976245,
            "auditor_fp_violation": 0.019155269429693145,
            "ave_precision_score": 0.826700883030968,
            "fpr": 0.14709110867178923,
            "logloss": 0.8445941003664712,
            "mae": 0.26870329593688747,
            "precision": 0.7292929292929293,
            "recall": 0.7730192719486081
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8364150915966068,
            "auditor_fn_violation": 0.013220937353651074,
            "auditor_fp_violation": 0.023188854489164086,
            "ave_precision_score": 0.8367072149039709,
            "fpr": 0.15021929824561403,
            "logloss": 0.7660194211650212,
            "mae": 0.26971167270620267,
            "precision": 0.7390476190476191,
            "recall": 0.7967145790554415
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8303880610198516,
            "auditor_fn_violation": 0.009362138224931073,
            "auditor_fp_violation": 0.02189456196042366,
            "ave_precision_score": 0.8306651336045513,
            "fpr": 0.15806805708013172,
            "logloss": 0.7730352379052494,
            "mae": 0.2690343413966757,
            "precision": 0.7267552182163188,
            "recall": 0.8201284796573876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.834834883840402,
            "auditor_fn_violation": 0.01723314960913578,
            "auditor_fp_violation": 0.02248194014447885,
            "ave_precision_score": 0.8351286357169663,
            "fpr": 0.15021929824561403,
            "logloss": 0.8125273396563081,
            "mae": 0.2691091121825767,
            "precision": 0.7390476190476191,
            "recall": 0.7967145790554415
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8296587693968112,
            "auditor_fn_violation": 0.008506547385394317,
            "auditor_fp_violation": 0.018492696868108508,
            "ave_precision_score": 0.8299351352735116,
            "fpr": 0.15916575192096596,
            "logloss": 0.8148538786206916,
            "mae": 0.2682372464875397,
            "precision": 0.7238095238095238,
            "recall": 0.8137044967880086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5592105263157895,
            "auc_prc": 0.787868637220153,
            "auditor_fn_violation": 0.001085233617925718,
            "auditor_fp_violation": 0.004643962848297217,
            "ave_precision_score": 0.585611573645997,
            "fpr": 0.43859649122807015,
            "logloss": 13.731444985027764,
            "mae": 0.4411597844249782,
            "precision": 0.5480225988700564,
            "recall": 0.9958932238193019
        },
        "train": {
            "accuracy": 0.5246981339187706,
            "auc_prc": 0.7714580252835392,
            "auditor_fn_violation": 0.0037678904279599573,
            "auditor_fp_violation": 0.004153439938291748,
            "ave_precision_score": 0.559078301793314,
            "fpr": 0.4643249176728869,
            "logloss": 14.555892019895325,
            "mae": 0.47451912078411856,
            "precision": 0.5193181818181818,
            "recall": 0.9785867237687366
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7280701754385965,
            "auc_prc": 0.8120259837559384,
            "auditor_fn_violation": 0.00904211246802839,
            "auditor_fp_violation": 0.00019091847265222117,
            "ave_precision_score": 0.8125579056003933,
            "fpr": 0.18421052631578946,
            "logloss": 0.5832473075607044,
            "mae": 0.3308178590880727,
            "precision": 0.7078260869565217,
            "recall": 0.8357289527720739
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8106562760689391,
            "auditor_fn_violation": 0.010741895979898318,
            "auditor_fp_violation": 0.011308234689134809,
            "ave_precision_score": 0.8117203794746848,
            "fpr": 0.17672886937431395,
            "logloss": 0.561128844957167,
            "mae": 0.3230595274397855,
            "precision": 0.7034990791896869,
            "recall": 0.8179871520342612
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 5740,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8590987459920736,
            "auditor_fn_violation": 0.01353615043769588,
            "auditor_fp_violation": 0.015221878224974209,
            "ave_precision_score": 0.8593614910675158,
            "fpr": 0.12609649122807018,
            "logloss": 0.6359643042774816,
            "mae": 0.2733995548987265,
            "precision": 0.7676767676767676,
            "recall": 0.7802874743326489
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8570548610426915,
            "auditor_fn_violation": 0.0072325632232269395,
            "auditor_fp_violation": 0.01711810603138815,
            "ave_precision_score": 0.8573371301558337,
            "fpr": 0.12843029637760703,
            "logloss": 0.6258185382400616,
            "mae": 0.26396823059411156,
            "precision": 0.7650602409638554,
            "recall": 0.815845824411135
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8466208597973507,
            "auditor_fn_violation": 0.009924709103353867,
            "auditor_fp_violation": 0.008619711042311663,
            "ave_precision_score": 0.8468596878918024,
            "fpr": 0.09649122807017543,
            "logloss": 0.5368255290818217,
            "mae": 0.3217967854881799,
            "precision": 0.7990867579908676,
            "recall": 0.7186858316221766
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8452118411011609,
            "auditor_fn_violation": 0.009251663583562317,
            "auditor_fp_violation": 0.009805084997181596,
            "ave_precision_score": 0.845729519905229,
            "fpr": 0.10428100987925357,
            "logloss": 0.5019364539520478,
            "mae": 0.30726229265791877,
            "precision": 0.7874720357941835,
            "recall": 0.7537473233404711
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.7521985717027655,
            "auditor_fn_violation": 0.006750063042616808,
            "auditor_fp_violation": 0.011947884416924674,
            "ave_precision_score": 0.7532017725126634,
            "fpr": 0.1524122807017544,
            "logloss": 0.5765592121067354,
            "mae": 0.3614822364464073,
            "precision": 0.7377358490566037,
            "recall": 0.8028747433264887
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7634237538132573,
            "auditor_fn_violation": 0.01670047504095789,
            "auditor_fp_violation": 0.01573857062331267,
            "ave_precision_score": 0.7652131591893451,
            "fpr": 0.16575192096597147,
            "logloss": 0.5612367717590906,
            "mae": 0.360124567505261,
            "precision": 0.7112810707456979,
            "recall": 0.7965738758029979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8174623908361178,
            "auditor_fn_violation": 0.0033209949926150057,
            "auditor_fp_violation": 0.0030546955624355062,
            "ave_precision_score": 0.8179376800820286,
            "fpr": 0.14583333333333334,
            "logloss": 0.5615832752211429,
            "mae": 0.3275004935992336,
            "precision": 0.7481060606060606,
            "recall": 0.811088295687885
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.817639769436707,
            "auditor_fn_violation": 0.004388428839052551,
            "auditor_fp_violation": 0.008840893582935296,
            "ave_precision_score": 0.8187245450844877,
            "fpr": 0.14050493962678376,
            "logloss": 0.5363430212060261,
            "mae": 0.31969808177952475,
            "precision": 0.7465346534653465,
            "recall": 0.8072805139186295
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7531050664523871,
            "auditor_fn_violation": 0.021035970315933575,
            "auditor_fp_violation": 0.010147058823529414,
            "ave_precision_score": 0.7460187957227438,
            "fpr": 0.14473684210526316,
            "logloss": 1.7060769124286275,
            "mae": 0.2831645824163148,
            "precision": 0.7370517928286853,
            "recall": 0.7597535934291582
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7020103100964632,
            "auditor_fn_violation": 0.012215674706243226,
            "auditor_fp_violation": 0.022784584804343318,
            "ave_precision_score": 0.6936424156706241,
            "fpr": 0.15148188803512624,
            "logloss": 1.9306010844281458,
            "mae": 0.29534954828902554,
            "precision": 0.7154639175257732,
            "recall": 0.7430406852248393
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8275584305236562,
            "auditor_fn_violation": 0.006993227421737097,
            "auditor_fp_violation": 0.002807017543859649,
            "ave_precision_score": 0.8279671837821656,
            "fpr": 0.14912280701754385,
            "logloss": 0.5336958720841838,
            "mae": 0.32701977795569975,
            "precision": 0.746268656716418,
            "recall": 0.8213552361396304
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8223124401991974,
            "auditor_fn_violation": 0.002169533914539638,
            "auditor_fp_violation": 0.009379851860642201,
            "ave_precision_score": 0.8230459060149191,
            "fpr": 0.14709110867178923,
            "logloss": 0.5196136004858904,
            "mae": 0.3190726491271975,
            "precision": 0.7372549019607844,
            "recall": 0.8051391862955032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7379385964912281,
            "auc_prc": 0.8319519702361233,
            "auditor_fn_violation": 0.0132141827875644,
            "auditor_fp_violation": 0.024935500515995874,
            "ave_precision_score": 0.8322447408831343,
            "fpr": 0.14144736842105263,
            "logloss": 0.8290420372784415,
            "mae": 0.27372261782444784,
            "precision": 0.7450592885375494,
            "recall": 0.7741273100616016
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8283536594137311,
            "auditor_fn_violation": 0.006597921666427696,
            "auditor_fp_violation": 0.015753404337377992,
            "ave_precision_score": 0.8286506375135851,
            "fpr": 0.14050493962678376,
            "logloss": 0.8302250370422708,
            "mae": 0.2682798373793375,
            "precision": 0.7393075356415478,
            "recall": 0.7773019271948608
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.8339495386171054,
            "auditor_fn_violation": 0.030145628444828702,
            "auditor_fp_violation": 0.044520123839009294,
            "ave_precision_score": 0.834297750809604,
            "fpr": 0.21600877192982457,
            "logloss": 0.5813956656151005,
            "mae": 0.3347219699511729,
            "precision": 0.6781045751633987,
            "recall": 0.8521560574948666
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8295890577802238,
            "auditor_fn_violation": 0.0388659190432426,
            "auditor_fp_violation": 0.04613285074316908,
            "ave_precision_score": 0.8298923309190074,
            "fpr": 0.18660812294182216,
            "logloss": 0.5705853100600978,
            "mae": 0.3267331169335464,
            "precision": 0.6931407942238267,
            "recall": 0.8222698072805139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 5740,
        "test": {
            "accuracy": 0.768640350877193,
            "auc_prc": 0.8543448089487908,
            "auditor_fn_violation": 0.01235410137252783,
            "auditor_fp_violation": 0.007959236326109393,
            "ave_precision_score": 0.8546104323531547,
            "fpr": 0.07785087719298246,
            "logloss": 0.5502299301608163,
            "mae": 0.3098255046874829,
            "precision": 0.8301435406698564,
            "recall": 0.7125256673511293
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8572553954421624,
            "auditor_fn_violation": 0.005291030164278149,
            "auditor_fp_violation": 0.012717437525340927,
            "ave_precision_score": 0.8574551477488841,
            "fpr": 0.09440175631174534,
            "logloss": 0.5088534062255368,
            "mae": 0.30683264182878023,
            "precision": 0.7966903073286052,
            "recall": 0.721627408993576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.815357064697015,
            "auditor_fn_violation": 0.010122843041896324,
            "auditor_fp_violation": 0.002494840041279678,
            "ave_precision_score": 0.815839977344333,
            "fpr": 0.1875,
            "logloss": 0.5804389898251541,
            "mae": 0.3299534605756522,
            "precision": 0.7026086956521739,
            "recall": 0.8295687885010267
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8126285632877537,
            "auditor_fn_violation": 0.010295296365854406,
            "auditor_fp_violation": 0.006507055903323743,
            "ave_precision_score": 0.8137125530357943,
            "fpr": 0.1800219538968167,
            "logloss": 0.5561609892650082,
            "mae": 0.3213456054022597,
            "precision": 0.70018281535649,
            "recall": 0.8201284796573876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8296607840912014,
            "auditor_fn_violation": 0.016478889729457113,
            "auditor_fp_violation": 0.026215170278637775,
            "ave_precision_score": 0.8299663730620965,
            "fpr": 0.13486842105263158,
            "logloss": 0.8677883526333064,
            "mae": 0.27294047469417754,
            "precision": 0.7505070993914807,
            "recall": 0.7597535934291582
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8213899228002515,
            "auditor_fn_violation": 0.008508897909678757,
            "auditor_fp_violation": 0.014324423215751428,
            "ave_precision_score": 0.821694677064478,
            "fpr": 0.14270032930845225,
            "logloss": 0.8800632477059595,
            "mae": 0.2685441005122815,
            "precision": 0.7336065573770492,
            "recall": 0.7665952890792291
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 5740,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8544120515894245,
            "auditor_fn_violation": 0.02254223855326201,
            "auditor_fp_violation": 0.007017543859649126,
            "ave_precision_score": 0.8546413031918294,
            "fpr": 0.14802631578947367,
            "logloss": 0.5291920028324872,
            "mae": 0.31252889947599644,
            "precision": 0.7476635514018691,
            "recall": 0.8213552361396304
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8568773182330847,
            "auditor_fn_violation": 0.0142911876494052,
            "auditor_fp_violation": 0.013392371515313342,
            "ave_precision_score": 0.857204740825107,
            "fpr": 0.1394072447859495,
            "logloss": 0.500843905610752,
            "mae": 0.30174100768009815,
            "precision": 0.7509803921568627,
            "recall": 0.8201284796573876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.8113326699699268,
            "auditor_fn_violation": 0.006410083216254189,
            "auditor_fp_violation": 0.0012280701754386002,
            "ave_precision_score": 0.8118848445269422,
            "fpr": 0.18530701754385964,
            "logloss": 0.583610171386511,
            "mae": 0.33133967384875224,
            "precision": 0.7065972222222222,
            "recall": 0.8357289527720739
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8100248278420832,
            "auditor_fn_violation": 0.010741895979898318,
            "auditor_fp_violation": 0.010205595276945438,
            "ave_precision_score": 0.8110968634616733,
            "fpr": 0.1778265642151482,
            "logloss": 0.5611340187156769,
            "mae": 0.323412785094444,
            "precision": 0.7022058823529411,
            "recall": 0.8179871520342612
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8533902959342597,
            "auditor_fn_violation": 0.0032669584639216124,
            "auditor_fp_violation": 0.003008255933952539,
            "ave_precision_score": 0.8535887199887455,
            "fpr": 0.15679824561403508,
            "logloss": 0.5447268290336554,
            "mae": 0.32406004597203464,
            "precision": 0.731203007518797,
            "recall": 0.7987679671457906
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8468252288078779,
            "auditor_fn_violation": 0.00422389213914164,
            "auditor_fp_violation": 0.007515748459766035,
            "ave_precision_score": 0.8472054441478315,
            "fpr": 0.14709110867178923,
            "logloss": 0.5200194947919446,
            "mae": 0.31666267797715864,
            "precision": 0.7372549019607844,
            "recall": 0.8051391862955032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.817940874223066,
            "auditor_fn_violation": 0.008407183255880983,
            "auditor_fp_violation": 0.00010835913312693467,
            "ave_precision_score": 0.8183875027457482,
            "fpr": 0.17653508771929824,
            "logloss": 0.5739442300761872,
            "mae": 0.3278949616110287,
            "precision": 0.7125,
            "recall": 0.8193018480492813
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.815014020525293,
            "auditor_fn_violation": 0.009552530691970844,
            "auditor_fp_violation": 0.008277212448452843,
            "ave_precision_score": 0.8160645631149192,
            "fpr": 0.1712403951701427,
            "logloss": 0.5481545860153724,
            "mae": 0.3187169192752312,
            "precision": 0.7089552238805971,
            "recall": 0.8137044967880086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8350052254312326,
            "auditor_fn_violation": 0.011408462120393382,
            "auditor_fp_violation": 0.021550567595459248,
            "ave_precision_score": 0.8353001672848067,
            "fpr": 0.12828947368421054,
            "logloss": 0.8053552864429249,
            "mae": 0.268805225945048,
            "precision": 0.7631578947368421,
            "recall": 0.7741273100616016
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8326180602968474,
            "auditor_fn_violation": 0.007552234525911005,
            "auditor_fp_violation": 0.017701565451291024,
            "ave_precision_score": 0.8328802267213411,
            "fpr": 0.14050493962678376,
            "logloss": 0.7873108516631382,
            "mae": 0.2670302665839622,
            "precision": 0.7382413087934561,
            "recall": 0.7730192719486081
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 5740,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7514379200059038,
            "auditor_fn_violation": 0.018365665189668222,
            "auditor_fp_violation": 0.02095717234262126,
            "ave_precision_score": 0.7519497107333257,
            "fpr": 0.14364035087719298,
            "logloss": 1.0750029523907427,
            "mae": 0.32791510337513957,
            "precision": 0.7139737991266376,
            "recall": 0.6714579055441479
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.748591398057937,
            "auditor_fn_violation": 0.01599766827990983,
            "auditor_fp_violation": 0.018670701436892437,
            "ave_precision_score": 0.7492501254905947,
            "fpr": 0.150384193194292,
            "logloss": 1.077411639334102,
            "mae": 0.3210361972909234,
            "precision": 0.6989010989010989,
            "recall": 0.6809421841541756
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8363652755151639,
            "auditor_fn_violation": 0.013479862386973596,
            "auditor_fp_violation": 0.024380804953560375,
            "ave_precision_score": 0.8366493764410666,
            "fpr": 0.1425438596491228,
            "logloss": 0.7806367731651901,
            "mae": 0.2691751223579905,
            "precision": 0.7475728155339806,
            "recall": 0.7905544147843943
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8318113645418246,
            "auditor_fn_violation": 0.007958875227119412,
            "auditor_fp_violation": 0.020070015130388353,
            "ave_precision_score": 0.8320813735148866,
            "fpr": 0.15148188803512624,
            "logloss": 0.7796910199637065,
            "mae": 0.26778706770351524,
            "precision": 0.7309941520467836,
            "recall": 0.8029978586723768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8432355776391617,
            "auditor_fn_violation": 0.00946539860946,
            "auditor_fp_violation": 0.013957688338493297,
            "ave_precision_score": 0.8435507072065453,
            "fpr": 0.14364035087719298,
            "logloss": 0.5190707785156102,
            "mae": 0.3130893214349686,
            "precision": 0.7523629489603024,
            "recall": 0.8172484599589322
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.855787442114619,
            "auditor_fn_violation": 0.008915538610887163,
            "auditor_fp_violation": 0.015664402052986026,
            "ave_precision_score": 0.8560124563518028,
            "fpr": 0.13830954994511527,
            "logloss": 0.4966114304135275,
            "mae": 0.31033842263203504,
            "precision": 0.7529411764705882,
            "recall": 0.8222698072805139
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8175318662495475,
            "auditor_fn_violation": 0.009204222054108578,
            "auditor_fp_violation": 0.002494840041279678,
            "ave_precision_score": 0.8179828073145816,
            "fpr": 0.1875,
            "logloss": 0.5787206958497966,
            "mae": 0.3285836749159978,
            "precision": 0.7015706806282722,
            "recall": 0.8254620123203286
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.8139581147979296,
            "auditor_fn_violation": 0.00884502288235391,
            "auditor_fp_violation": 0.005725813629216489,
            "ave_precision_score": 0.8150235676156597,
            "fpr": 0.1778265642151482,
            "logloss": 0.5537202714083336,
            "mae": 0.31972693883678205,
            "precision": 0.7022058823529411,
            "recall": 0.8179871520342612
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7671634789674846,
            "auditor_fn_violation": 0.02016237976872367,
            "auditor_fp_violation": 0.020735294117647067,
            "ave_precision_score": 0.7676320835822241,
            "fpr": 0.14035087719298245,
            "logloss": 1.0506320092804708,
            "mae": 0.32091695559331024,
            "precision": 0.7223427331887202,
            "recall": 0.6837782340862423
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7480150961676023,
            "auditor_fn_violation": 0.013682401859734817,
            "auditor_fp_violation": 0.026055418755748065,
            "ave_precision_score": 0.7487476478594886,
            "fpr": 0.15697036223929747,
            "logloss": 1.077645067650186,
            "mae": 0.31724266133359386,
            "precision": 0.6995798319327731,
            "recall": 0.7130620985010707
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.846964777640317,
            "auditor_fn_violation": 0.005971036420620356,
            "auditor_fp_violation": 0.0061171310629514995,
            "ave_precision_score": 0.8472100553797842,
            "fpr": 0.10197368421052631,
            "logloss": 0.5243671286672528,
            "mae": 0.3208602564173226,
            "precision": 0.7924107142857143,
            "recall": 0.728952772073922
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8411662460121172,
            "auditor_fn_violation": 0.008638176745323044,
            "auditor_fp_violation": 0.009340295289801329,
            "ave_precision_score": 0.8417904620425504,
            "fpr": 0.10647639956092206,
            "logloss": 0.5009189368982401,
            "mae": 0.311661828450408,
            "precision": 0.7839643652561247,
            "recall": 0.7537473233404711
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8027324283631116,
            "auditor_fn_violation": 0.007589880759393355,
            "auditor_fp_violation": 0.01038183694530444,
            "ave_precision_score": 0.8036123203310764,
            "fpr": 0.12938596491228072,
            "logloss": 0.589680908611739,
            "mae": 0.32669757763871304,
            "precision": 0.7505285412262156,
            "recall": 0.728952772073922
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8005905061066885,
            "auditor_fn_violation": 0.012253283094794296,
            "auditor_fp_violation": 0.0075849724587375555,
            "ave_precision_score": 0.8022837472873436,
            "fpr": 0.12623490669593854,
            "logloss": 0.5552447900352769,
            "mae": 0.31825100097590225,
            "precision": 0.7472527472527473,
            "recall": 0.728051391862955
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 5740,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8353366607806553,
            "auditor_fn_violation": 0.01633479231960806,
            "auditor_fp_violation": 0.026341589267285866,
            "ave_precision_score": 0.8356261014856227,
            "fpr": 0.14802631578947367,
            "logloss": 0.7967775497805211,
            "mae": 0.26778654022763865,
            "precision": 0.7448015122873346,
            "recall": 0.8090349075975359
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8255965859446387,
            "auditor_fn_violation": 0.008706341949571853,
            "auditor_fp_violation": 0.017563117453347973,
            "ave_precision_score": 0.8259022262185411,
            "fpr": 0.16245883644346873,
            "logloss": 0.8149724058070795,
            "mae": 0.26892997073998215,
            "precision": 0.7212806026365348,
            "recall": 0.8201284796573876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8177881742330293,
            "auditor_fn_violation": 0.006209697755682845,
            "auditor_fp_violation": 0.002494840041279678,
            "ave_precision_score": 0.8182378364616739,
            "fpr": 0.1875,
            "logloss": 0.5774757261829745,
            "mae": 0.3285474060585808,
            "precision": 0.7015706806282722,
            "recall": 0.8254620123203286
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.814670364647587,
            "auditor_fn_violation": 0.00884502288235391,
            "auditor_fp_violation": 0.0070979321802592905,
            "ave_precision_score": 0.8157281996669359,
            "fpr": 0.17672886937431395,
            "logloss": 0.5527598883438665,
            "mae": 0.31977125288485153,
            "precision": 0.7034990791896869,
            "recall": 0.8179871520342612
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8334480981139558,
            "auditor_fn_violation": 0.014724954068950614,
            "auditor_fp_violation": 0.02285603715170279,
            "ave_precision_score": 0.8337407613920691,
            "fpr": 0.14473684210526316,
            "logloss": 0.828295266069603,
            "mae": 0.2716357090209271,
            "precision": 0.7416829745596869,
            "recall": 0.7782340862422998
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.8263920620925427,
            "auditor_fn_violation": 0.00583165074969972,
            "auditor_fp_violation": 0.019536001424036546,
            "ave_precision_score": 0.8266830930222326,
            "fpr": 0.14818880351262348,
            "logloss": 0.8332221036050345,
            "mae": 0.26889083907783373,
            "precision": 0.7305389221556886,
            "recall": 0.7837259100642399
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 5740,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.8316382974800007,
            "auditor_fn_violation": 0.012856190784970644,
            "auditor_fp_violation": 0.023771929824561406,
            "ave_precision_score": 0.8319267808225999,
            "fpr": 0.12719298245614036,
            "logloss": 0.8215589449204594,
            "mae": 0.27324921731248464,
            "precision": 0.7608247422680412,
            "recall": 0.757700205338809
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.8291698986019009,
            "auditor_fn_violation": 0.007082129669022679,
            "auditor_fp_violation": 0.017226886601200544,
            "ave_precision_score": 0.8294413858002223,
            "fpr": 0.13721185510428102,
            "logloss": 0.8115169310790035,
            "mae": 0.2687966738599539,
            "precision": 0.7412008281573499,
            "recall": 0.7665952890792291
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8238779624640168,
            "auditor_fn_violation": 0.010093573255520738,
            "auditor_fp_violation": 0.0033255933952528404,
            "ave_precision_score": 0.8242392015160946,
            "fpr": 0.16557017543859648,
            "logloss": 0.5646620506744211,
            "mae": 0.32592117330581494,
            "precision": 0.7198515769944341,
            "recall": 0.7967145790554415
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.8159431015886339,
            "auditor_fn_violation": 0.008962549096575992,
            "auditor_fp_violation": 0.008049762166117818,
            "ave_precision_score": 0.8174658240199433,
            "fpr": 0.16245883644346873,
            "logloss": 0.5395742396999236,
            "mae": 0.3173668154340064,
            "precision": 0.7159309021113244,
            "recall": 0.7987152034261242
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8347335864843914,
            "auditor_fn_violation": 0.015999315537303216,
            "auditor_fp_violation": 0.023916408668730658,
            "ave_precision_score": 0.8350239990999465,
            "fpr": 0.13596491228070176,
            "logloss": 0.7901727406569229,
            "mae": 0.2728150915749332,
            "precision": 0.752,
            "recall": 0.7720739219712526
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8328271296169387,
            "auditor_fn_violation": 0.006609674287849907,
            "auditor_fp_violation": 0.01696976889073486,
            "ave_precision_score": 0.8331143901727543,
            "fpr": 0.13830954994511527,
            "logloss": 0.7836699827670985,
            "mae": 0.2669016669445289,
            "precision": 0.7428571428571429,
            "recall": 0.7794432548179872
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.8096254982091944,
            "auditor_fn_violation": 0.009548704924528986,
            "auditor_fp_violation": 0.004112487100103201,
            "ave_precision_score": 0.8102974803010752,
            "fpr": 0.18859649122807018,
            "logloss": 0.5833291576645816,
            "mae": 0.329952997161487,
            "precision": 0.7013888888888888,
            "recall": 0.8295687885010267
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.8113271344901765,
            "auditor_fn_violation": 0.009747624207579506,
            "auditor_fp_violation": 0.009755639283630508,
            "ave_precision_score": 0.8124424251622204,
            "fpr": 0.18221734357848518,
            "logloss": 0.5619950245423915,
            "mae": 0.32275521772489124,
            "precision": 0.6970802919708029,
            "recall": 0.8179871520342612
        }
    }
]
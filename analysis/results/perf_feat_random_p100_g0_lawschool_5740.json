[
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(0)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8388144949578051,
            "auditor_fn_violation": 0.018151770596923522,
            "auditor_fp_violation": 0.028160474716202272,
            "ave_precision_score": 0.8392865321843241,
            "fpr": 0.12828947368421054,
            "logloss": 0.7925763340298745,
            "mae": 0.2690922696243026,
            "precision": 0.7617107942973523,
            "recall": 0.7679671457905544
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8270098171977127,
            "auditor_fn_violation": 0.013456751528428413,
            "auditor_fp_violation": 0.02409984078480237,
            "ave_precision_score": 0.8273576488130298,
            "fpr": 0.13391877058177826,
            "logloss": 0.8151784799902515,
            "mae": 0.2685720130026079,
            "precision": 0.7453027139874739,
            "recall": 0.7644539614561028
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(1)",
        "seed": 5740,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.8177387499201955,
            "auditor_fn_violation": 0.009206473576137469,
            "auditor_fp_violation": 0.0035681114551083595,
            "ave_precision_score": 0.707566586370411,
            "fpr": 0.008771929824561403,
            "logloss": 0.841035084438227,
            "mae": 0.3952307460148855,
            "precision": 0.9503105590062112,
            "recall": 0.3141683778234086
        },
        "train": {
            "accuracy": 0.6245883644346871,
            "auc_prc": 0.804880626221822,
            "auditor_fn_violation": 0.003544590620937996,
            "auditor_fp_violation": 0.0014907882635654315,
            "ave_precision_score": 0.6829454972492656,
            "fpr": 0.007683863885839737,
            "logloss": 0.716602075022439,
            "mae": 0.40050583946541596,
            "precision": 0.9496402877697842,
            "recall": 0.2826552462526767
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(2)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.6707626028976311,
            "auditor_fn_violation": 0.00992696062538276,
            "auditor_fp_violation": 0.027925696594427247,
            "ave_precision_score": 0.6390696971578655,
            "fpr": 0.3026315789473684,
            "logloss": 0.6460518378751771,
            "mae": 0.4388439139134757,
            "precision": 0.6193103448275862,
            "recall": 0.9219712525667351
        },
        "train": {
            "accuracy": 0.6355653128430296,
            "auc_prc": 0.6606223607410064,
            "auditor_fn_violation": 0.010466884638618645,
            "auditor_fp_violation": 0.03154636524559686,
            "ave_precision_score": 0.6217357324059329,
            "fpr": 0.3216245883644347,
            "logloss": 0.6554017554350755,
            "mae": 0.4384413515631637,
            "precision": 0.5936199722607489,
            "recall": 0.9164882226980728
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(3)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.8319540356971029,
            "auditor_fn_violation": 0.01667927519002847,
            "auditor_fp_violation": 0.019747162022703824,
            "ave_precision_score": 0.8322344733508881,
            "fpr": 0.1524122807017544,
            "logloss": 0.7508889528718073,
            "mae": 0.29218216591812696,
            "precision": 0.7252964426877471,
            "recall": 0.7535934291581109
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.8382612798899773,
            "auditor_fn_violation": 0.01396211424958337,
            "auditor_fp_violation": 0.031417806390364025,
            "ave_precision_score": 0.8384957340208862,
            "fpr": 0.16575192096597147,
            "logloss": 0.7266705151375359,
            "mae": 0.2849766159660551,
            "precision": 0.7073643410852714,
            "recall": 0.7815845824411135
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(4)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5964912280701754,
            "auc_prc": 0.6877283651409265,
            "auditor_fn_violation": 0.000583144205482912,
            "auditor_fp_violation": 0.009865841073271413,
            "ave_precision_score": 0.6792841197038177,
            "fpr": 0.0668859649122807,
            "logloss": 9.146478368391376,
            "mae": 0.4281822928327311,
            "precision": 0.7468879668049793,
            "recall": 0.36960985626283366
        },
        "train": {
            "accuracy": 0.6004390779363337,
            "auc_prc": 0.6484294397149406,
            "auditor_fn_violation": 0.013440297858437342,
            "auditor_fp_violation": 0.0108508618387872,
            "ave_precision_score": 0.6386973348256124,
            "fpr": 0.06915477497255763,
            "logloss": 8.920732542207329,
            "mae": 0.4227959287264289,
            "precision": 0.7248908296943232,
            "recall": 0.3554603854389722
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(5)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.8056554040162165,
            "auditor_fn_violation": 0.009496919917864478,
            "auditor_fp_violation": 0.006599587203302377,
            "ave_precision_score": 0.7080803456947784,
            "fpr": 0.06907894736842106,
            "logloss": 0.6049922441360591,
            "mae": 0.39465688472907795,
            "precision": 0.8163265306122449,
            "recall": 0.5749486652977412
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7853163532559072,
            "auditor_fn_violation": 0.006214786208063714,
            "auditor_fp_violation": 0.003975435369507818,
            "ave_precision_score": 0.7005757422364346,
            "fpr": 0.052689352360043906,
            "logloss": 0.5905374789384483,
            "mae": 0.38803889457663904,
            "precision": 0.8441558441558441,
            "recall": 0.556745182012848
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(6)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5932017543859649,
            "auc_prc": 0.6709446083600863,
            "auditor_fn_violation": 0.002215497676429272,
            "auditor_fp_violation": 0.00518575851393189,
            "ave_precision_score": 0.6614829485904661,
            "fpr": 0.04276315789473684,
            "logloss": 10.646929877558374,
            "mae": 0.4243897268443564,
            "precision": 0.7989690721649485,
            "recall": 0.3182751540041068
        },
        "train": {
            "accuracy": 0.6026344676180022,
            "auc_prc": 0.659842076450957,
            "auditor_fn_violation": 0.010022635548859198,
            "auditor_fp_violation": 0.006494694474935968,
            "ave_precision_score": 0.6494529228869388,
            "fpr": 0.04720087815587267,
            "logloss": 10.534172922241961,
            "mae": 0.4113199726093255,
            "precision": 0.774869109947644,
            "recall": 0.3169164882226981
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(7)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5394736842105263,
            "auc_prc": 0.6100855718591303,
            "auditor_fn_violation": 0.010692478115205881,
            "auditor_fp_violation": 0.024280185758513942,
            "ave_precision_score": 0.5868493370634758,
            "fpr": 0.17214912280701755,
            "logloss": 0.716593334719561,
            "mae": 0.4738520528434923,
            "precision": 0.5879265091863517,
            "recall": 0.459958932238193
        },
        "train": {
            "accuracy": 0.5345773874862788,
            "auc_prc": 0.5962027796936692,
            "auditor_fn_violation": 0.026403439287133013,
            "auditor_fp_violation": 0.020856201975850718,
            "ave_precision_score": 0.5708273530342712,
            "fpr": 0.19758507135016465,
            "logloss": 0.7204335609425914,
            "mae": 0.47582772267833107,
            "precision": 0.5533498759305211,
            "recall": 0.47751605995717344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(8)",
        "seed": 5740,
        "test": {
            "accuracy": 0.46600877192982454,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.44340362315954,
            "mae": 0.5339912280701754,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.48737650933040616,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.705388119015684,
            "mae": 0.5126234906695939,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(9)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8393404046521866,
            "auditor_fn_violation": 0.009861666486544908,
            "auditor_fp_violation": 0.007378740970072238,
            "ave_precision_score": 0.7325401690512341,
            "fpr": 0.08771929824561403,
            "logloss": 0.5487475620328328,
            "mae": 0.3576761607505512,
            "precision": 0.8126463700234192,
            "recall": 0.7125256673511293
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8360269743249765,
            "auditor_fn_violation": 0.005352143795673627,
            "auditor_fp_violation": 0.009137567864241851,
            "ave_precision_score": 0.7254542644830008,
            "fpr": 0.0889132821075741,
            "logloss": 0.5373311929612538,
            "mae": 0.35259904109685797,
            "precision": 0.8066825775656324,
            "recall": 0.7237687366167024
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(10)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5526315789473685,
            "auc_prc": 0.6254820323996905,
            "auditor_fn_violation": 0.020466335242624027,
            "auditor_fp_violation": 0.010766253869969052,
            "ave_precision_score": 0.6265070334424148,
            "fpr": 0.15021929824561403,
            "logloss": 0.7831361984546441,
            "mae": 0.47930838335576065,
            "precision": 0.6118980169971672,
            "recall": 0.44353182751540043
        },
        "train": {
            "accuracy": 0.5773874862788145,
            "auc_prc": 0.6163558737441144,
            "auditor_fn_violation": 0.011345980720999833,
            "auditor_fp_violation": 0.005997765053747492,
            "ave_precision_score": 0.6180528238710397,
            "fpr": 0.12623490669593854,
            "logloss": 0.7765172850120072,
            "mae": 0.4764866567492581,
            "precision": 0.6314102564102564,
            "recall": 0.42184154175588867
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(11)",
        "seed": 5740,
        "test": {
            "accuracy": 0.4901315789473684,
            "auc_prc": 0.6023165742193721,
            "auditor_fn_violation": 0.015483716992687076,
            "auditor_fp_violation": 0.007319401444788443,
            "ave_precision_score": 0.5550717611020513,
            "fpr": 0.051535087719298246,
            "logloss": 0.7442798115497328,
            "mae": 0.497588926329882,
            "precision": 0.5948275862068966,
            "recall": 0.14168377823408623
        },
        "train": {
            "accuracy": 0.47091108671789245,
            "auc_prc": 0.5136846859668045,
            "auditor_fn_violation": 0.009202302573589054,
            "auditor_fp_violation": 0.005555225917465217,
            "ave_precision_score": 0.5157893281345994,
            "fpr": 0.07793633369923161,
            "logloss": 0.7449895785864749,
            "mae": 0.5008374742272134,
            "precision": 0.4409448818897638,
            "recall": 0.11991434689507495
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(12)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5570175438596491,
            "auc_prc": 0.6065953505748458,
            "auditor_fn_violation": 0.07866367664541231,
            "auditor_fp_violation": 0.07924406604747163,
            "ave_precision_score": 0.5713025170902025,
            "fpr": 0.2708333333333333,
            "logloss": 0.7484745377343547,
            "mae": 0.48874014874985605,
            "precision": 0.5719237435008665,
            "recall": 0.6776180698151951
        },
        "train": {
            "accuracy": 0.5938529088913282,
            "auc_prc": 0.6419909528425413,
            "auditor_fn_violation": 0.07184377475395888,
            "auditor_fp_violation": 0.08838421297257741,
            "ave_precision_score": 0.5842385358593728,
            "fpr": 0.26125137211855104,
            "logloss": 0.7064702592735064,
            "mae": 0.4763030156280547,
            "precision": 0.5846422338568935,
            "recall": 0.7173447537473233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(13)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5855263157894737,
            "auc_prc": 0.6777389545519605,
            "auditor_fn_violation": 0.009929212147411649,
            "auditor_fp_violation": 0.007546439628482983,
            "ave_precision_score": 0.5961043589835527,
            "fpr": 0.24671052631578946,
            "logloss": 0.6817516060389979,
            "mae": 0.4785516977212147,
            "precision": 0.5974955277280859,
            "recall": 0.6858316221765913
        },
        "train": {
            "accuracy": 0.58397365532382,
            "auc_prc": 0.6601704932756208,
            "auditor_fn_violation": 0.004936100997327463,
            "auditor_fp_violation": 0.0045687839321209255,
            "ave_precision_score": 0.5744118781608407,
            "fpr": 0.2601536772777168,
            "logloss": 0.6922264618214815,
            "mae": 0.4823643393593745,
            "precision": 0.5782918149466192,
            "recall": 0.69593147751606
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(14)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5339912280701754,
            "auc_prc": 0.7669956140350878,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5339912280701754,
            "fpr": 0.46600877192982454,
            "logloss": 0.6913341405877953,
            "mae": 0.49876239242260917,
            "precision": 0.5339912280701754,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5126234906695939,
            "auc_prc": 0.756311745334797,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5126234906695939,
            "fpr": 0.48737650933040616,
            "logloss": 0.6928908109502883,
            "mae": 0.4995403835462032,
            "precision": 0.5126234906695939,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(15)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7653508771929824,
            "auc_prc": 0.8473635633968257,
            "auditor_fn_violation": 0.006473125833063153,
            "auditor_fp_violation": 0.007288441692466462,
            "ave_precision_score": 0.847673731808474,
            "fpr": 0.10964912280701754,
            "logloss": 0.5034422868277355,
            "mae": 0.32522758135261637,
            "precision": 0.7885835095137421,
            "recall": 0.7659137577002053
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.8489207499147822,
            "auditor_fn_violation": 0.004959606240171873,
            "auditor_fp_violation": 0.01187686039497236,
            "ave_precision_score": 0.84920044172754,
            "fpr": 0.11306256860592755,
            "logloss": 0.4926123903647199,
            "mae": 0.3202553725741092,
            "precision": 0.7751091703056768,
            "recall": 0.7601713062098501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(16)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5493421052631579,
            "auc_prc": 0.6272917454695373,
            "auditor_fn_violation": 0.005563510933390977,
            "auditor_fp_violation": 0.01559081527347783,
            "ave_precision_score": 0.6290987955343217,
            "fpr": 0.3081140350877193,
            "logloss": 0.8834517915346292,
            "mae": 0.460293746388695,
            "precision": 0.5595611285266457,
            "recall": 0.7330595482546202
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.6035100744227767,
            "auditor_fn_violation": 0.007596894487315396,
            "auditor_fp_violation": 0.015805322336606652,
            "ave_precision_score": 0.6044254698571774,
            "fpr": 0.3040614709110867,
            "logloss": 0.8403741539992535,
            "mae": 0.46089622602541447,
            "precision": 0.5568,
            "recall": 0.7451820128479657
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(17)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.7782645371680772,
            "auditor_fn_violation": 0.023355038005691848,
            "auditor_fp_violation": 0.021026831785345722,
            "ave_precision_score": 0.7771085486679599,
            "fpr": 0.13706140350877194,
            "logloss": 1.1518702955819742,
            "mae": 0.33443839824554233,
            "precision": 0.7252747252747253,
            "recall": 0.6776180698151951
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.765767458576696,
            "auditor_fn_violation": 0.013252255915681991,
            "auditor_fp_violation": 0.02908891328210758,
            "ave_precision_score": 0.7605223404538346,
            "fpr": 0.1525795828759605,
            "logloss": 1.3077781170708276,
            "mae": 0.33412954934620176,
            "precision": 0.7061310782241015,
            "recall": 0.715203426124197
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(18)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5679824561403509,
            "auc_prc": 0.7207955180253768,
            "auditor_fn_violation": 0.07067977953096292,
            "auditor_fp_violation": 0.09305469556243551,
            "ave_precision_score": 0.5587049307390586,
            "fpr": 0.3267543859649123,
            "logloss": 0.7858007069440232,
            "mae": 0.485527786874659,
            "precision": 0.5674891146589259,
            "recall": 0.8028747433264887
        },
        "train": {
            "accuracy": 0.5762897914379802,
            "auc_prc": 0.7216636216542716,
            "auditor_fn_violation": 0.07311070734327292,
            "auditor_fp_violation": 0.10817238753572454,
            "ave_precision_score": 0.5603656319102316,
            "fpr": 0.31833150384193193,
            "logloss": 0.7922494803090832,
            "mae": 0.4797921975442287,
            "precision": 0.5612708018154312,
            "recall": 0.7944325481798715
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(19)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.8467060105651554,
            "auditor_fn_violation": 0.010093573255520745,
            "auditor_fp_violation": 0.0122265221878225,
            "ave_precision_score": 0.8469685433199771,
            "fpr": 0.0668859649122807,
            "logloss": 0.5444662277162966,
            "mae": 0.3339877807828312,
            "precision": 0.8342391304347826,
            "recall": 0.6303901437371663
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.8284305305256009,
            "auditor_fn_violation": 0.004106365924919559,
            "auditor_fp_violation": 0.007248741606590127,
            "ave_precision_score": 0.8289348352600479,
            "fpr": 0.07025246981339188,
            "logloss": 0.5435249732077176,
            "mae": 0.33096236649005645,
            "precision": 0.819718309859155,
            "recall": 0.6231263383297645
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(20)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6228070175438597,
            "auc_prc": 0.7547243206580524,
            "auditor_fn_violation": 0.07514454771425483,
            "auditor_fp_violation": 0.0741124871001032,
            "ave_precision_score": 0.6061679374582227,
            "fpr": 0.25548245614035087,
            "logloss": 0.723662598783211,
            "mae": 0.4308184260143,
            "precision": 0.6174055829228243,
            "recall": 0.7720739219712526
        },
        "train": {
            "accuracy": 0.6608122941822173,
            "auc_prc": 0.75677717681079,
            "auditor_fn_violation": 0.07451397034108459,
            "auditor_fp_violation": 0.08115030508005261,
            "ave_precision_score": 0.6127298996824564,
            "fpr": 0.21514818880351264,
            "logloss": 0.6788880816446904,
            "mae": 0.4129269478022725,
            "precision": 0.6436363636363637,
            "recall": 0.7580299785867237
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(21)",
        "seed": 5740,
        "test": {
            "accuracy": 0.46600877192982454,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.44340362315954,
            "mae": 0.5339912280701754,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.48737650933040616,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.705388119015684,
            "mae": 0.5126234906695939,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(22)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5526315789473685,
            "auc_prc": 0.7034320816884798,
            "auditor_fn_violation": 0.0042756403328650175,
            "auditor_fp_violation": 0.0068188854489164225,
            "ave_precision_score": 0.6712307437728906,
            "fpr": 0.4155701754385965,
            "logloss": 0.7870838320919739,
            "mae": 0.4358710476948896,
            "precision": 0.5471923536439666,
            "recall": 0.9404517453798767
        },
        "train": {
            "accuracy": 0.5367727771679474,
            "auc_prc": 0.6466035190780267,
            "auditor_fn_violation": 0.003342445532476019,
            "auditor_fp_violation": 0.0031002462396535002,
            "ave_precision_score": 0.6352747466093529,
            "fpr": 0.4456641053787047,
            "logloss": 0.8201787729846489,
            "mae": 0.44581151375818984,
            "precision": 0.5262543757292882,
            "recall": 0.9657387580299786
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(23)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.6305448735600817,
            "auditor_fn_violation": 0.010334486112612142,
            "auditor_fp_violation": 0.008111455108359134,
            "ave_precision_score": 0.6504350686621636,
            "fpr": 0.039473684210526314,
            "logloss": 10.370905404648822,
            "mae": 0.45905178895707427,
            "precision": 0.7857142857142857,
            "recall": 0.27104722792607805
        },
        "train": {
            "accuracy": 0.5828759604829857,
            "auc_prc": 0.6033005869766672,
            "auditor_fn_violation": 0.008121061402745895,
            "auditor_fp_violation": 0.005473640490105913,
            "ave_precision_score": 0.625093424724128,
            "fpr": 0.03293084522502744,
            "logloss": 10.449691571144166,
            "mae": 0.44509933260434564,
            "precision": 0.7959183673469388,
            "recall": 0.2505353319057816
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(24)",
        "seed": 5740,
        "test": {
            "accuracy": 0.47368421052631576,
            "auc_prc": 0.6089055080938665,
            "auditor_fn_violation": 0.0047124356064699845,
            "auditor_fp_violation": 0.0024767801857585136,
            "ave_precision_score": 0.6100297095094678,
            "fpr": 0.005482456140350877,
            "logloss": 1.6721875163506055,
            "mae": 0.5108467738901438,
            "precision": 0.7058823529411765,
            "recall": 0.024640657084188913
        },
        "train": {
            "accuracy": 0.4950603732162459,
            "auc_prc": 0.5996420173379768,
            "auditor_fn_violation": 0.005481422631317916,
            "auditor_fp_violation": 0.0022052788243787152,
            "ave_precision_score": 0.6007889854332564,
            "fpr": 0.0043907793633369925,
            "logloss": 1.5971638338972765,
            "mae": 0.4917195590965168,
            "precision": 0.7333333333333333,
            "recall": 0.023554603854389723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(25)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6359649122807017,
            "auc_prc": 0.8066979307319198,
            "auditor_fn_violation": 0.006313267769011861,
            "auditor_fp_violation": 0.0034184726522187825,
            "ave_precision_score": 0.8043404580380538,
            "fpr": 0.01425438596491228,
            "logloss": 0.7452138520687698,
            "mae": 0.4043257339702298,
            "precision": 0.9281767955801105,
            "recall": 0.34496919917864477
        },
        "train": {
            "accuracy": 0.6421514818880352,
            "auc_prc": 0.8071332666516375,
            "auditor_fn_violation": 0.0004583522354661324,
            "auditor_fp_violation": 0.0011842248395486595,
            "ave_precision_score": 0.8037898390990574,
            "fpr": 0.012074643249176729,
            "logloss": 0.7176675370009545,
            "mae": 0.3906718006216472,
            "precision": 0.9325153374233128,
            "recall": 0.32548179871520344
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(26)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5131578947368421,
            "auc_prc": 0.6741532263995716,
            "auditor_fn_violation": 0.015744893548038475,
            "auditor_fp_violation": 0.005085139318885456,
            "ave_precision_score": 0.5378974442867046,
            "fpr": 0.26644736842105265,
            "logloss": 0.6946436966587622,
            "mae": 0.4986435360879752,
            "precision": 0.5406427221172023,
            "recall": 0.5872689938398358
        },
        "train": {
            "accuracy": 0.5422612513721186,
            "auc_prc": 0.6856315530193609,
            "auditor_fn_violation": 0.007425306214551161,
            "auditor_fp_violation": 0.008418132732073464,
            "ave_precision_score": 0.5347229913135775,
            "fpr": 0.27332601536772777,
            "logloss": 0.6889274600029445,
            "mae": 0.4956257421478351,
            "precision": 0.5456204379562044,
            "recall": 0.6402569593147751
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(27)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6239035087719298,
            "auc_prc": 0.6669749836504703,
            "auditor_fn_violation": 0.01092438488418171,
            "auditor_fp_violation": 0.019543343653250778,
            "ave_precision_score": 0.6492722998234088,
            "fpr": 0.21929824561403508,
            "logloss": 0.7001304936833264,
            "mae": 0.4431938038109557,
            "precision": 0.6323529411764706,
            "recall": 0.7063655030800822
        },
        "train": {
            "accuracy": 0.6289791437980241,
            "auc_prc": 0.6824641095443729,
            "auditor_fn_violation": 0.013059512924357784,
            "auditor_fp_violation": 0.015137805203666895,
            "ave_precision_score": 0.6694863994178567,
            "fpr": 0.20965971459934138,
            "logloss": 0.6894407299077676,
            "mae": 0.43719663908531325,
            "precision": 0.6262230919765166,
            "recall": 0.6852248394004282
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(28)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5515350877192983,
            "auc_prc": 0.797489808795348,
            "auditor_fn_violation": 0.0036812385172376523,
            "auditor_fp_violation": 0.012097523219814254,
            "ave_precision_score": 0.7988271510174978,
            "fpr": 0.44298245614035087,
            "logloss": 2.522868947214386,
            "mae": 0.4428929637047681,
            "precision": 0.5440180586907449,
            "recall": 0.9897330595482546
        },
        "train": {
            "accuracy": 0.5378704720087816,
            "auc_prc": 0.7966656217100618,
            "auditor_fn_violation": 0.00607375475099721,
            "auditor_fp_violation": 0.019800535991534893,
            "ave_precision_score": 0.7969173835768945,
            "fpr": 0.45334796926454446,
            "logloss": 2.6172359139554136,
            "mae": 0.4583181050952353,
            "precision": 0.5263761467889908,
            "recall": 0.9828693790149893
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(29)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7730263157894737,
            "auc_prc": 0.8347791430693763,
            "auditor_fn_violation": 0.0069752152455059635,
            "auditor_fp_violation": 0.006940144478844175,
            "ave_precision_score": 0.8046645845763819,
            "fpr": 0.11513157894736842,
            "logloss": 0.5099175685057766,
            "mae": 0.3231717551355822,
            "precision": 0.7857142857142857,
            "recall": 0.7905544147843943
        },
        "train": {
            "accuracy": 0.7848518111964874,
            "auc_prc": 0.8410138588954104,
            "auditor_fn_violation": 0.0035398895723691218,
            "auditor_fp_violation": 0.009790251283116272,
            "ave_precision_score": 0.8089167441112715,
            "fpr": 0.11855104281009879,
            "logloss": 0.49802120657671406,
            "mae": 0.3175847789683634,
            "precision": 0.7782340862422998,
            "recall": 0.8115631691648822
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(30)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.6952706618960132,
            "auditor_fn_violation": 0.018392683454014916,
            "auditor_fp_violation": 0.025817853457172347,
            "ave_precision_score": 0.6522199685558866,
            "fpr": 0.15679824561403508,
            "logloss": 0.6316955016081551,
            "mae": 0.43947410054112734,
            "precision": 0.7020833333333333,
            "recall": 0.6919917864476386
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.6994843968015536,
            "auditor_fn_violation": 0.012709284805975978,
            "auditor_fp_violation": 0.023311181653662447,
            "ave_precision_score": 0.6379603242269883,
            "fpr": 0.18111964873765093,
            "logloss": 0.623554333468517,
            "mae": 0.43624840223697614,
            "precision": 0.6745562130177515,
            "recall": 0.7323340471092077
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(31)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7401315789473685,
            "auc_prc": 0.8265231779763715,
            "auditor_fn_violation": 0.0032962282502972096,
            "auditor_fp_violation": 0.010639834881320951,
            "ave_precision_score": 0.812499073405649,
            "fpr": 0.09539473684210527,
            "logloss": 0.5493142027327821,
            "mae": 0.3481269501264027,
            "precision": 0.7948113207547169,
            "recall": 0.6919917864476386
        },
        "train": {
            "accuracy": 0.7541163556531284,
            "auc_prc": 0.8357745639553097,
            "auditor_fn_violation": 0.008584114686780884,
            "auditor_fp_violation": 0.009004064437653902,
            "ave_precision_score": 0.8258034338429583,
            "fpr": 0.09549945115257959,
            "logloss": 0.5304804397521559,
            "mae": 0.33845006103671343,
            "precision": 0.7913669064748201,
            "recall": 0.7066381156316917
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(32)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5855263157894737,
            "auc_prc": 0.6862450807387269,
            "auditor_fn_violation": 0.02581370006124142,
            "auditor_fp_violation": 0.008684210526315789,
            "ave_precision_score": 0.6644735356793334,
            "fpr": 0.10197368421052631,
            "logloss": 1.1503279106169457,
            "mae": 0.4557588021487295,
            "precision": 0.6847457627118644,
            "recall": 0.41478439425051333
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.6421815071552956,
            "auditor_fn_violation": 0.018390502001471434,
            "auditor_fp_violation": 0.024149286498353455,
            "ave_precision_score": 0.6194341947182028,
            "fpr": 0.12184412733260154,
            "logloss": 1.2193335018861375,
            "mae": 0.4636739812624328,
            "precision": 0.621160409556314,
            "recall": 0.3897216274089936
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(33)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.7722328401091773,
            "auditor_fn_violation": 0.019525199034547358,
            "auditor_fp_violation": 0.013911248710010322,
            "ave_precision_score": 0.7726284937406149,
            "fpr": 0.08552631578947369,
            "logloss": 1.066922038909599,
            "mae": 0.35385070313957245,
            "precision": 0.7815126050420168,
            "recall": 0.5728952772073922
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.760574991664588,
            "auditor_fn_violation": 0.021603668698303154,
            "auditor_fp_violation": 0.011481294686563621,
            "ave_precision_score": 0.7609127074509345,
            "fpr": 0.08342480790340286,
            "logloss": 1.1220496721213393,
            "mae": 0.3565890316953524,
            "precision": 0.7632398753894081,
            "recall": 0.5246252676659529
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(34)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5855263157894737,
            "auc_prc": 0.6682773217406378,
            "auditor_fn_violation": 0.007700205338809059,
            "auditor_fp_violation": 0.01565015479876161,
            "ave_precision_score": 0.6550182387653107,
            "fpr": 0.10087719298245613,
            "logloss": 0.6656620458419511,
            "mae": 0.4650555096337931,
            "precision": 0.6860068259385665,
            "recall": 0.4127310061601643
        },
        "train": {
            "accuracy": 0.5883644346871569,
            "auc_prc": 0.648103597166428,
            "auditor_fn_violation": 0.010344657375827689,
            "auditor_fp_violation": 0.019753562563661363,
            "ave_precision_score": 0.624950298891565,
            "fpr": 0.11745334796926454,
            "logloss": 0.6626352292401614,
            "mae": 0.46173849587382915,
            "precision": 0.6503267973856209,
            "recall": 0.4261241970021413
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(35)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.6611753682941449,
            "auditor_fn_violation": 0.007378237688677548,
            "auditor_fp_violation": 0.028253353973168232,
            "ave_precision_score": 0.6610230346197455,
            "fpr": 0.24890350877192982,
            "logloss": 1.2874531993608,
            "mae": 0.31896761823443753,
            "precision": 0.6539634146341463,
            "recall": 0.8809034907597536
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.6347822612546334,
            "auditor_fn_violation": 0.009362138224931075,
            "auditor_fp_violation": 0.030831874684783586,
            "ave_precision_score": 0.6363518835886246,
            "fpr": 0.2579582875960483,
            "logloss": 1.4157383177915956,
            "mae": 0.33392898548332545,
            "precision": 0.6293375394321766,
            "recall": 0.854389721627409
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(36)",
        "seed": 5740,
        "test": {
            "accuracy": 0.4407894736842105,
            "auc_prc": 0.4782642133082924,
            "auditor_fn_violation": 0.027610414640296857,
            "auditor_fp_violation": 0.006333849329205379,
            "ave_precision_score": 0.4492504983628571,
            "fpr": 0.3267543859649123,
            "logloss": 9.912039422685467,
            "mae": 0.5558605372684858,
            "precision": 0.4799301919720768,
            "recall": 0.5646817248459959
        },
        "train": {
            "accuracy": 0.4456641053787047,
            "auc_prc": 0.48655911595050433,
            "auditor_fn_violation": 0.019638630396509944,
            "auditor_fp_violation": 0.01641597689896263,
            "ave_precision_score": 0.45061732799097093,
            "fpr": 0.3336992316136114,
            "logloss": 9.605126140019108,
            "mae": 0.548858522525777,
            "precision": 0.4666666666666667,
            "recall": 0.569593147751606
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(37)",
        "seed": 5740,
        "test": {
            "accuracy": 0.46600877192982454,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.44340362315954,
            "mae": 0.5339912280701754,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.48737650933040616,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.705388119015684,
            "mae": 0.5126234906695939,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(38)",
        "seed": 5740,
        "test": {
            "accuracy": 0.4758771929824561,
            "auc_prc": 0.6073223299536997,
            "auditor_fn_violation": 0.0076664325083756766,
            "auditor_fp_violation": 0.0030753353973168264,
            "ave_precision_score": 0.5265187830022948,
            "fpr": 0.18201754385964913,
            "logloss": 0.6933353855750753,
            "mae": 0.4998996414309531,
            "precision": 0.5131964809384164,
            "recall": 0.3593429158110883
        },
        "train": {
            "accuracy": 0.5060373216245884,
            "auc_prc": 0.6081784265664667,
            "auditor_fn_violation": 0.004315562586234863,
            "auditor_fp_violation": 0.007342688462337213,
            "ave_precision_score": 0.5176078392706547,
            "fpr": 0.16794731064763996,
            "logloss": 0.6926991703150193,
            "mae": 0.4995915091521915,
            "precision": 0.5263157894736842,
            "recall": 0.3640256959314775
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(39)",
        "seed": 5740,
        "test": {
            "accuracy": 0.4725877192982456,
            "auc_prc": 0.5253138233514821,
            "auditor_fn_violation": 0.0026365322958319818,
            "auditor_fp_violation": 0.004664602683178535,
            "ave_precision_score": 0.5351564295394721,
            "fpr": 0.02850877192982456,
            "logloss": 0.6935379616999388,
            "mae": 0.5001761686514344,
            "precision": 0.5517241379310345,
            "recall": 0.06570841889117043
        },
        "train": {
            "accuracy": 0.4807903402854007,
            "auc_prc": 0.4830678146823149,
            "auditor_fn_violation": 0.0027924228499166697,
            "auditor_fp_violation": 0.005112686781182941,
            "ave_precision_score": 0.5090290983153792,
            "fpr": 0.038419319429198684,
            "logloss": 0.6935518460535781,
            "mae": 0.5001826454465136,
            "precision": 0.453125,
            "recall": 0.06209850107066381
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(40)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7740718709258473,
            "auditor_fn_violation": 0.019734590583234266,
            "auditor_fp_violation": 0.01770381836945304,
            "ave_precision_score": 0.7494487522406699,
            "fpr": 0.13596491228070176,
            "logloss": 1.981702429814438,
            "mae": 0.31836084529079434,
            "precision": 0.7292576419213974,
            "recall": 0.6858316221765913
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.760434568432155,
            "auditor_fn_violation": 0.013177039138579861,
            "auditor_fp_violation": 0.021508885394725136,
            "ave_precision_score": 0.7332499771877719,
            "fpr": 0.15806805708013172,
            "logloss": 2.108768448168141,
            "mae": 0.3236203738524038,
            "precision": 0.6981132075471698,
            "recall": 0.7130620985010707
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(41)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5339912280701754,
            "auc_prc": 0.6282942457154134,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5201490300363399,
            "fpr": 0.46600877192982454,
            "logloss": 0.6931533656109204,
            "mae": 0.49861095270566774,
            "precision": 0.5339912280701754,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5126234906695939,
            "auc_prc": 0.6410095117170431,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5199010557486665,
            "fpr": 0.48737650933040616,
            "logloss": 0.6933022474209377,
            "mae": 0.49871528030881246,
            "precision": 0.5126234906695939,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(42)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8367225687534192,
            "auditor_fn_violation": 0.008188785619078496,
            "auditor_fp_violation": 0.00753353973168215,
            "ave_precision_score": 0.8370835665995302,
            "fpr": 0.09868421052631579,
            "logloss": 0.4974803832227336,
            "mae": 0.3285207097508471,
            "precision": 0.7972972972972973,
            "recall": 0.7268993839835729
        },
        "train": {
            "accuracy": 0.7826564215148188,
            "auc_prc": 0.865664915890494,
            "auditor_fn_violation": 0.017069507353615225,
            "auditor_fp_violation": 0.01058632727128885,
            "ave_precision_score": 0.8642652428950239,
            "fpr": 0.09001097694840834,
            "logloss": 0.4749991193892403,
            "mae": 0.32027615005667726,
            "precision": 0.8106235565819861,
            "recall": 0.7516059957173448
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(43)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7437467936504041,
            "auditor_fn_violation": 0.02439298966101085,
            "auditor_fp_violation": 0.025438596491228076,
            "ave_precision_score": 0.7002482737483978,
            "fpr": 0.1513157894736842,
            "logloss": 0.630636131584263,
            "mae": 0.40606322168911757,
            "precision": 0.7057569296375267,
            "recall": 0.6796714579055442
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7318510309373166,
            "auditor_fn_violation": 0.013325122168499688,
            "auditor_fp_violation": 0.03331157721937086,
            "ave_precision_score": 0.6826191248100196,
            "fpr": 0.1690450054884742,
            "logloss": 0.620033069284701,
            "mae": 0.3989339833823569,
            "precision": 0.6863543788187373,
            "recall": 0.721627408993576
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(44)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.7232249607840501,
            "auditor_fn_violation": 0.008699881119636876,
            "auditor_fp_violation": 0.02206656346749227,
            "ave_precision_score": 0.724650285644746,
            "fpr": 0.28289473684210525,
            "logloss": 0.7068973606434498,
            "mae": 0.4550759037234147,
            "precision": 0.6282420749279539,
            "recall": 0.8952772073921971
        },
        "train": {
            "accuracy": 0.6597145993413831,
            "auc_prc": 0.6942160390448793,
            "auditor_fn_violation": 0.01444867277646279,
            "auditor_fp_violation": 0.02042355198227866,
            "ave_precision_score": 0.6960280002593464,
            "fpr": 0.2689352360043908,
            "logloss": 0.8320774280024248,
            "mae": 0.4633710783636738,
            "precision": 0.6213292117465224,
            "recall": 0.860813704496788
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(45)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5899122807017544,
            "auc_prc": 0.5805582368461739,
            "auditor_fn_violation": 0.08141953960877554,
            "auditor_fp_violation": 0.08001547987616102,
            "ave_precision_score": 0.5818228910589884,
            "fpr": 0.25109649122807015,
            "logloss": 0.8375043326442227,
            "mae": 0.467651956815139,
            "precision": 0.5989492119089317,
            "recall": 0.702258726899384
        },
        "train": {
            "accuracy": 0.6147091108671789,
            "auc_prc": 0.5769349555889414,
            "auditor_fn_violation": 0.07533430331635471,
            "auditor_fp_violation": 0.08527902216156881,
            "ave_precision_score": 0.5790663119270433,
            "fpr": 0.23161361141602635,
            "logloss": 0.8790362948437961,
            "mae": 0.45585675349432275,
            "precision": 0.6078066914498141,
            "recall": 0.7002141327623126
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(46)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7730263157894737,
            "auc_prc": 0.8667221513442809,
            "auditor_fn_violation": 0.008015418422853858,
            "auditor_fp_violation": 0.009816821465428279,
            "ave_precision_score": 0.8670480117941202,
            "fpr": 0.0712719298245614,
            "logloss": 0.5248957400312121,
            "mae": 0.28818837907540784,
            "precision": 0.8414634146341463,
            "recall": 0.7084188911704312
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8619366061927185,
            "auditor_fn_violation": 0.003401208639587071,
            "auditor_fp_violation": 0.00971608271278963,
            "ave_precision_score": 0.8622186712745561,
            "fpr": 0.07574094401756312,
            "logloss": 0.491547501922513,
            "mae": 0.2819043787173269,
            "precision": 0.8292079207920792,
            "recall": 0.7173447537473233
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(47)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6348684210526315,
            "auc_prc": 0.8071582692938428,
            "auditor_fn_violation": 0.00017111567419574988,
            "auditor_fp_violation": 0.004130546955624356,
            "ave_precision_score": 0.8074917777107419,
            "fpr": 0.02412280701754386,
            "logloss": 1.0095382947215799,
            "mae": 0.37453751066267416,
            "precision": 0.8888888888888888,
            "recall": 0.3613963039014374
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.7944792481013904,
            "auditor_fn_violation": 0.00538740165994027,
            "auditor_fp_violation": 0.0035724528040664157,
            "ave_precision_score": 0.7949025236668469,
            "fpr": 0.020856201975850714,
            "logloss": 0.914393747176645,
            "mae": 0.36617516332008526,
            "precision": 0.8950276243093923,
            "recall": 0.3468950749464668
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(48)",
        "seed": 5740,
        "test": {
            "accuracy": 0.606359649122807,
            "auc_prc": 0.7573476037448374,
            "auditor_fn_violation": 0.006783835873050183,
            "auditor_fp_violation": 0.010570175438596494,
            "ave_precision_score": 0.5838702286274934,
            "fpr": 0.3026315789473684,
            "logloss": 0.6732407833770575,
            "mae": 0.47128382566989513,
            "precision": 0.5941176470588235,
            "recall": 0.8295687885010267
        },
        "train": {
            "accuracy": 0.6158068057080132,
            "auc_prc": 0.7500918651055973,
            "auditor_fn_violation": 0.0011141485108253437,
            "auditor_fp_violation": 0.013545653227321725,
            "ave_precision_score": 0.5763823574969704,
            "fpr": 0.28869374313940727,
            "logloss": 0.6644327628514749,
            "mae": 0.465529680513786,
            "precision": 0.5909797822706065,
            "recall": 0.8137044967880086
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(49)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5899122807017544,
            "auc_prc": 0.6479020785192697,
            "auditor_fn_violation": 0.027997676429266195,
            "auditor_fp_violation": 0.014837461300309603,
            "ave_precision_score": 0.6490502701258535,
            "fpr": 0.10855263157894737,
            "logloss": 0.7614963417549445,
            "mae": 0.467708807640807,
            "precision": 0.6816720257234726,
            "recall": 0.4353182751540041
        },
        "train": {
            "accuracy": 0.605927552140505,
            "auc_prc": 0.6438865888070854,
            "auditor_fn_violation": 0.04860414115368433,
            "auditor_fp_violation": 0.023155427655976505,
            "ave_precision_score": 0.6477130544051677,
            "fpr": 0.1119648737650933,
            "logloss": 0.7308571056466093,
            "mae": 0.45791537758437295,
            "precision": 0.6730769230769231,
            "recall": 0.44967880085653106
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(50)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7845892771963485,
            "auditor_fn_violation": 0.022855200115277943,
            "auditor_fp_violation": 0.007030443756449952,
            "ave_precision_score": 0.7850182053405477,
            "fpr": 0.08223684210526316,
            "logloss": 0.6118436311349824,
            "mae": 0.39404099097586054,
            "precision": 0.7916666666666666,
            "recall": 0.5852156057494866
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7815860687246887,
            "auditor_fn_violation": 0.01964568196936327,
            "auditor_fp_violation": 0.007775338455909263,
            "ave_precision_score": 0.7819540089842635,
            "fpr": 0.0867178924259056,
            "logloss": 0.5982445753382837,
            "mae": 0.3944338167190519,
            "precision": 0.7817679558011049,
            "recall": 0.6059957173447538
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(51)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5592105263157895,
            "auc_prc": 0.48582101713549963,
            "auditor_fn_violation": 0.001085233617925718,
            "auditor_fp_violation": 0.007675438596491232,
            "ave_precision_score": 0.5002348143044831,
            "fpr": 0.43859649122807015,
            "logloss": 0.6831151685560429,
            "mae": 0.49367743227304073,
            "precision": 0.5480225988700564,
            "recall": 0.9958932238193019
        },
        "train": {
            "accuracy": 0.5301866081229418,
            "auc_prc": 0.4750888259360463,
            "auditor_fn_violation": 0.0007239614796080269,
            "auditor_fp_violation": 0.003238694237596559,
            "ave_precision_score": 0.4860508740848103,
            "fpr": 0.4632272228320527,
            "logloss": 0.6889017496546904,
            "mae": 0.49653923735595035,
            "precision": 0.522083805209513,
            "recall": 0.987152034261242
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(52)",
        "seed": 5740,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8099639959213657,
            "auditor_fn_violation": 0.019977754962354557,
            "auditor_fp_violation": 0.010570175438596494,
            "ave_precision_score": 0.8053815821928703,
            "fpr": 0.07456140350877193,
            "logloss": 0.5560919076312184,
            "mae": 0.3548118733033015,
            "precision": 0.8278481012658228,
            "recall": 0.6714579055441479
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8068479260515897,
            "auditor_fn_violation": 0.026476305539950697,
            "auditor_fp_violation": 0.01555067691181852,
            "ave_precision_score": 0.8030366356887323,
            "fpr": 0.08122941822173436,
            "logloss": 0.5546947769428746,
            "mae": 0.3591263286859353,
            "precision": 0.806282722513089,
            "recall": 0.6595289079229122
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(53)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8338480348482178,
            "auditor_fn_violation": 0.01554225656543824,
            "auditor_fp_violation": 0.01568369453044376,
            "ave_precision_score": 0.8345603144413746,
            "fpr": 0.12390350877192982,
            "logloss": 0.6997352348114957,
            "mae": 0.2781394960105612,
            "precision": 0.7650727650727651,
            "recall": 0.75564681724846
        },
        "train": {
            "accuracy": 0.743139407244786,
            "auc_prc": 0.830360894853061,
            "auditor_fn_violation": 0.011324826002439845,
            "auditor_fp_violation": 0.014722461209837725,
            "ave_precision_score": 0.8306281140902885,
            "fpr": 0.13721185510428102,
            "logloss": 0.7222032843021094,
            "mae": 0.2799837612362965,
            "precision": 0.7412008281573499,
            "recall": 0.7665952890792291
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(54)",
        "seed": 5740,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7682115297141634,
            "auditor_fn_violation": 0.020419053280017294,
            "auditor_fp_violation": 0.023952528379772964,
            "ave_precision_score": 0.6804750156506546,
            "fpr": 0.15679824561403508,
            "logloss": 0.6268580892818365,
            "mae": 0.4304830375895427,
            "precision": 0.7033195020746889,
            "recall": 0.6960985626283368
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7638519189339132,
            "auditor_fn_violation": 0.014827107186257902,
            "auditor_fp_violation": 0.02351638136489948,
            "ave_precision_score": 0.6689282235528939,
            "fpr": 0.17892425905598244,
            "logloss": 0.6129576921524298,
            "mae": 0.4234804703427722,
            "precision": 0.6772277227722773,
            "recall": 0.7323340471092077
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(55)",
        "seed": 5740,
        "test": {
            "accuracy": 0.46600877192982454,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.44340362315954,
            "mae": 0.5339912280701754,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.48737650933040616,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.705388119015684,
            "mae": 0.5126234906695939,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(56)",
        "seed": 5740,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8031478029194898,
            "auditor_fn_violation": 0.024773496883893513,
            "auditor_fp_violation": 0.020536635706914354,
            "ave_precision_score": 0.7948066678554682,
            "fpr": 0.1206140350877193,
            "logloss": 1.2401905363672625,
            "mae": 0.2713567985590941,
            "precision": 0.7731958762886598,
            "recall": 0.7700205338809035
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.7932825636821572,
            "auditor_fn_violation": 0.010767751747027179,
            "auditor_fp_violation": 0.022290127668832392,
            "ave_precision_score": 0.7819571514199313,
            "fpr": 0.13721185510428102,
            "logloss": 1.3354903740499875,
            "mae": 0.27148646576026164,
            "precision": 0.7479838709677419,
            "recall": 0.7944325481798715
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(57)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6195175438596491,
            "auc_prc": 0.757360204204667,
            "auditor_fn_violation": 0.034729727295651866,
            "auditor_fp_violation": 0.04148348813209495,
            "ave_precision_score": 0.5947183355885487,
            "fpr": 0.2708333333333333,
            "logloss": 0.6732519288831226,
            "mae": 0.4652726388160597,
            "precision": 0.610410094637224,
            "recall": 0.7946611909650924
        },
        "train": {
            "accuracy": 0.5982436882546652,
            "auc_prc": 0.7441373479557623,
            "auditor_fn_violation": 0.029689472236782418,
            "auditor_fp_violation": 0.03800397543536953,
            "ave_precision_score": 0.5645510398763618,
            "fpr": 0.30954994511525796,
            "logloss": 0.6764449866957218,
            "mae": 0.46900870208551804,
            "precision": 0.5759398496240602,
            "recall": 0.8201284796573876
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(58)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7189351896962357,
            "auditor_fn_violation": 0.025568284160092228,
            "auditor_fp_violation": 0.027476780185758515,
            "ave_precision_score": 0.7202805734174474,
            "fpr": 0.16447368421052633,
            "logloss": 2.762463181200251,
            "mae": 0.3136475230402144,
            "precision": 0.7053045186640472,
            "recall": 0.7371663244353183
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.6905715486614203,
            "auditor_fn_violation": 0.026756017929799248,
            "auditor_fp_violation": 0.038669020282631694,
            "ave_precision_score": 0.6904554599055711,
            "fpr": 0.15697036223929747,
            "logloss": 2.7833188807696536,
            "mae": 0.3043236437070621,
            "precision": 0.714,
            "recall": 0.7644539614561028
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(59)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5526315789473685,
            "auc_prc": 0.6372097514731494,
            "auditor_fn_violation": 0.09582477754962354,
            "auditor_fp_violation": 0.10036119711042311,
            "ave_precision_score": 0.5431944016906676,
            "fpr": 0.30153508771929827,
            "logloss": 0.6979727578417132,
            "mae": 0.4876083020905131,
            "precision": 0.5627980922098569,
            "recall": 0.7268993839835729
        },
        "train": {
            "accuracy": 0.5971459934138309,
            "auc_prc": 0.6568844941780279,
            "auditor_fn_violation": 0.09129671373199792,
            "auditor_fp_violation": 0.10119559735366543,
            "ave_precision_score": 0.5626542717747388,
            "fpr": 0.270032930845225,
            "logloss": 0.675063697708971,
            "mae": 0.4762708069725958,
            "precision": 0.5844594594594594,
            "recall": 0.7408993576017131
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(60)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.8411485948378667,
            "auditor_fn_violation": 0.0010919881840123925,
            "auditor_fp_violation": 0.004574303405572777,
            "ave_precision_score": 0.8414598692997783,
            "fpr": 0.4199561403508772,
            "logloss": 1.1323733641035683,
            "mae": 0.4168768810116473,
            "precision": 0.5577367205542725,
            "recall": 0.9917864476386037
        },
        "train": {
            "accuracy": 0.5521405049396267,
            "auc_prc": 0.8593007388616208,
            "auditor_fn_violation": 0.0013868093278205705,
            "auditor_fp_violation": 0.005251134779126003,
            "ave_precision_score": 0.8594881226226255,
            "fpr": 0.44127332601536773,
            "logloss": 1.1498279198374126,
            "mae": 0.43136285469329555,
            "precision": 0.5341830822711472,
            "recall": 0.987152034261242
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(61)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8440769066363493,
            "auditor_fn_violation": 0.00872464786195469,
            "auditor_fp_violation": 0.012216202270381837,
            "ave_precision_score": 0.8444444762578833,
            "fpr": 0.0668859649122807,
            "logloss": 1.1401397607015051,
            "mae": 0.31158681856010173,
            "precision": 0.8390501319261213,
            "recall": 0.6529774127310062
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.8504112769398681,
            "auditor_fn_violation": 0.0017041301062201912,
            "auditor_fp_violation": 0.00909306672204587,
            "ave_precision_score": 0.8506688114641446,
            "fpr": 0.07574094401756312,
            "logloss": 1.0205387809713369,
            "mae": 0.298577088602844,
            "precision": 0.8244274809160306,
            "recall": 0.6937901498929336
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(62)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7214912280701754,
            "auc_prc": 0.766833347776755,
            "auditor_fn_violation": 0.00041202853128715554,
            "auditor_fp_violation": 0.006865325077399379,
            "ave_precision_score": 0.7670467944320019,
            "fpr": 0.08881578947368421,
            "logloss": 1.1204015034663903,
            "mae": 0.3184849950801255,
            "precision": 0.7949367088607595,
            "recall": 0.6447638603696099
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.7523695596387395,
            "auditor_fn_violation": 0.0008955497523722651,
            "auditor_fp_violation": 0.00986689213912046,
            "ave_precision_score": 0.7525953832582235,
            "fpr": 0.07135016465422613,
            "logloss": 1.1464407848348377,
            "mae": 0.31697285804829906,
            "precision": 0.8184357541899442,
            "recall": 0.6274089935760171
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(63)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.7977276135623499,
            "auditor_fn_violation": 0.016321283187434706,
            "auditor_fp_violation": 0.023095975232198144,
            "ave_precision_score": 0.7613561330830456,
            "fpr": 0.14144736842105263,
            "logloss": 3.894588148166269,
            "mae": 0.2791667027779266,
            "precision": 0.7445544554455445,
            "recall": 0.7720739219712526
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7844472604370887,
            "auditor_fn_violation": 0.011620992062279492,
            "auditor_fp_violation": 0.02473274591825635,
            "ave_precision_score": 0.7432632876654046,
            "fpr": 0.15367727771679474,
            "logloss": 4.098042193417108,
            "mae": 0.2822770062434586,
            "precision": 0.7188755020080321,
            "recall": 0.7665952890792291
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(64)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.574972477145658,
            "auditor_fn_violation": 0.009962984977845053,
            "auditor_fp_violation": 0.011751805985552114,
            "ave_precision_score": 0.5659325982005124,
            "fpr": 0.043859649122807015,
            "logloss": 1.9227472690715537,
            "mae": 0.526803040620539,
            "precision": 0.7037037037037037,
            "recall": 0.19507186858316222
        },
        "train": {
            "accuracy": 0.5345773874862788,
            "auc_prc": 0.5615427942188157,
            "auditor_fn_violation": 0.012488335523238457,
            "auditor_fp_violation": 0.012371317530483284,
            "ave_precision_score": 0.5570956866002892,
            "fpr": 0.052689352360043906,
            "logloss": 1.8419723233260206,
            "mae": 0.5036023017115898,
            "precision": 0.6546762589928058,
            "recall": 0.1948608137044968
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(65)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5405701754385965,
            "auc_prc": 0.5097451959885264,
            "auditor_fn_violation": 0.09590808386469253,
            "auditor_fp_violation": 0.0680340557275542,
            "ave_precision_score": 0.5256573609973959,
            "fpr": 0.26864035087719296,
            "logloss": 0.7140586117446923,
            "mae": 0.5006170899427512,
            "precision": 0.5609318996415771,
            "recall": 0.6427104722792608
        },
        "train": {
            "accuracy": 0.5477497255762898,
            "auc_prc": 0.5079075758998108,
            "auditor_fn_violation": 0.08786964932528202,
            "auditor_fp_violation": 0.032377053233255224,
            "ave_precision_score": 0.511093713895123,
            "fpr": 0.26125137211855104,
            "logloss": 0.7123457388174347,
            "mae": 0.4997560571466136,
            "precision": 0.551789077212806,
            "recall": 0.6274089935760171
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(66)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.7885212579496247,
            "auditor_fn_violation": 0.010266940451745381,
            "auditor_fp_violation": 0.040116099071207445,
            "ave_precision_score": 0.6168912634508652,
            "fpr": 0.28399122807017546,
            "logloss": 0.634620201836088,
            "mae": 0.4467511139156526,
            "precision": 0.6268011527377522,
            "recall": 0.893223819301848
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.7716071672551884,
            "auditor_fn_violation": 0.007972978372826062,
            "auditor_fp_violation": 0.03746501715766263,
            "ave_precision_score": 0.5945645872126426,
            "fpr": 0.28869374313940727,
            "logloss": 0.6428733253468765,
            "mae": 0.4497451670999454,
            "precision": 0.6068759342301944,
            "recall": 0.8693790149892934
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(67)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5372807017543859,
            "auc_prc": 0.6405301567473727,
            "auditor_fn_violation": 0.084751792211535,
            "auditor_fp_violation": 0.02055211558307534,
            "ave_precision_score": 0.6366755936788018,
            "fpr": 0.07017543859649122,
            "logloss": 0.677366340237959,
            "mae": 0.47597218994378,
            "precision": 0.6683937823834197,
            "recall": 0.2648870636550308
        },
        "train": {
            "accuracy": 0.5290889132821076,
            "auc_prc": 0.6006017071016183,
            "auditor_fn_violation": 0.07410732963987618,
            "auditor_fp_violation": 0.018994570860652094,
            "ave_precision_score": 0.5982476573564263,
            "fpr": 0.09549945115257959,
            "logloss": 0.6820485397562952,
            "mae": 0.4792868851936544,
            "precision": 0.589622641509434,
            "recall": 0.2676659528907923
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(68)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.761404798891369,
            "auditor_fn_violation": 0.021607856911272018,
            "auditor_fp_violation": 0.020590815273477815,
            "ave_precision_score": 0.7618083942778304,
            "fpr": 0.11951754385964912,
            "logloss": 0.8393879695383303,
            "mae": 0.34150484816204907,
            "precision": 0.7528344671201814,
            "recall": 0.6817248459958932
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.73616266331282,
            "auditor_fn_violation": 0.01332277164421525,
            "auditor_fp_violation": 0.020989705402438663,
            "ave_precision_score": 0.7337126047886349,
            "fpr": 0.141602634467618,
            "logloss": 0.9959195299666578,
            "mae": 0.34007137394861936,
            "precision": 0.7177242888402626,
            "recall": 0.702355460385439
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(69)",
        "seed": 5740,
        "test": {
            "accuracy": 0.506578947368421,
            "auc_prc": 0.6686651010052239,
            "auditor_fn_violation": 0.004331928383587306,
            "auditor_fp_violation": 0.010010319917440655,
            "ave_precision_score": 0.5369982946574468,
            "fpr": 0.2631578947368421,
            "logloss": 1.53444984637479,
            "mae": 0.5020136088855827,
            "precision": 0.5357833655705996,
            "recall": 0.5687885010266941
        },
        "train": {
            "accuracy": 0.49066959385290887,
            "auc_prc": 0.6443069731946508,
            "auditor_fn_violation": 0.009357437176362194,
            "auditor_fp_violation": 0.013558014655709502,
            "ave_precision_score": 0.5078826126382436,
            "fpr": 0.2810098792535675,
            "logloss": 1.4810590712831033,
            "mae": 0.50100878096329,
            "precision": 0.5029126213592233,
            "recall": 0.5546038543897216
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(70)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6414473684210527,
            "auc_prc": 0.726983162284581,
            "auditor_fn_violation": 0.02165739039590763,
            "auditor_fp_violation": 0.01298245614035088,
            "ave_precision_score": 0.6635162213398729,
            "fpr": 0.0668859649122807,
            "logloss": 0.6394263155709506,
            "mae": 0.42386631069606856,
            "precision": 0.7836879432624113,
            "recall": 0.4537987679671458
        },
        "train": {
            "accuracy": 0.6641053787047201,
            "auc_prc": 0.7475839556542265,
            "auditor_fn_violation": 0.018517430312831287,
            "auditor_fp_violation": 0.007661613314741748,
            "ave_precision_score": 0.6795748557612493,
            "fpr": 0.04939626783754116,
            "logloss": 0.6130166556637581,
            "mae": 0.4134104251043726,
            "precision": 0.8207171314741036,
            "recall": 0.4411134903640257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(71)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5339912280701754,
            "auc_prc": 0.771895730084966,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5475071148096113,
            "fpr": 0.46600877192982454,
            "logloss": 0.6913332208247464,
            "mae": 0.4989134526827879,
            "precision": 0.5339912280701754,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5126234906695939,
            "auc_prc": 0.7539411130914709,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5192034594705871,
            "fpr": 0.48737650933040616,
            "logloss": 0.6926564704338728,
            "mae": 0.4995753596956984,
            "precision": 0.5126234906695939,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(72)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5350877192982456,
            "auc_prc": 0.624393859127928,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.00048761609907121246,
            "ave_precision_score": 0.5661145763748776,
            "fpr": 0.4649122807017544,
            "logloss": 7.848128060151235,
            "mae": 0.46158565466063456,
            "precision": 0.5345773874862788,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5148188803512623,
            "auc_prc": 0.6247866662052681,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0010334154132178432,
            "ave_precision_score": 0.5694715640455774,
            "fpr": 0.48518111964873767,
            "logloss": 7.638954342953633,
            "mae": 0.47967100312317895,
            "precision": 0.5137513751375138,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(73)",
        "seed": 5740,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8029731898073631,
            "auditor_fn_violation": 0.0007767750999675819,
            "auditor_fp_violation": 0.005028379772961817,
            "ave_precision_score": 0.8040736648649425,
            "fpr": 0.09539473684210527,
            "logloss": 0.6068233044697988,
            "mae": 0.3506407016773888,
            "precision": 0.8018223234624146,
            "recall": 0.7227926078028748
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8121803107927522,
            "auditor_fn_violation": 0.0006111363139548326,
            "auditor_fp_violation": 0.008252489591677301,
            "ave_precision_score": 0.8117912491065132,
            "fpr": 0.09440175631174534,
            "logloss": 0.5931122303803966,
            "mae": 0.3465957254760221,
            "precision": 0.7995337995337995,
            "recall": 0.734475374732334
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(74)",
        "seed": 5740,
        "test": {
            "accuracy": 0.4780701754385965,
            "auc_prc": 0.5007080907542182,
            "auditor_fn_violation": 0.01226404049137217,
            "auditor_fp_violation": 0.003611971104231168,
            "ave_precision_score": 0.4950411894070981,
            "fpr": 0.41776315789473684,
            "logloss": 5.581640927780774,
            "mae": 0.5152332448948777,
            "precision": 0.5071151358344114,
            "recall": 0.8049281314168378
        },
        "train": {
            "accuracy": 0.4434687156970362,
            "auc_prc": 0.47555362951203417,
            "auditor_fn_violation": 0.00810225720847035,
            "auditor_fp_violation": 0.004845679928007049,
            "ave_precision_score": 0.46610224813217777,
            "fpr": 0.43688254665203075,
            "logloss": 6.133371020184272,
            "mae": 0.5441085121119119,
            "precision": 0.47354497354497355,
            "recall": 0.7665952890792291
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(75)",
        "seed": 5740,
        "test": {
            "accuracy": 0.46600877192982454,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.44340362315954,
            "mae": 0.5339912280701754,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.48737650933040616,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.705388119015684,
            "mae": 0.5126234906695939,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(76)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7933369978667278,
            "auditor_fn_violation": 0.013777063294787277,
            "auditor_fp_violation": 0.01533281733746131,
            "ave_precision_score": 0.7499391864004161,
            "fpr": 0.15021929824561403,
            "logloss": 4.181103269423174,
            "mae": 0.281956715553885,
            "precision": 0.7350096711798839,
            "recall": 0.7802874743326489
        },
        "train": {
            "accuracy": 0.7365532381997805,
            "auc_prc": 0.779896702583742,
            "auditor_fn_violation": 0.004900843133060828,
            "auditor_fp_violation": 0.019155269429693145,
            "ave_precision_score": 0.7311649143689587,
            "fpr": 0.15367727771679474,
            "logloss": 4.398016272117261,
            "mae": 0.2846152440619905,
            "precision": 0.7238658777120316,
            "recall": 0.7858672376873662
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(77)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.8353887618561506,
            "auditor_fn_violation": 0.0022627796390359885,
            "auditor_fp_violation": 0.006836945304437579,
            "ave_precision_score": 0.835659482717211,
            "fpr": 0.41885964912280704,
            "logloss": 2.279852505192318,
            "mae": 0.42853902699425817,
            "precision": 0.5558139534883721,
            "recall": 0.9815195071868583
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.838658981145063,
            "auditor_fn_violation": 0.0012246231521940965,
            "auditor_fp_violation": 0.004976711068917449,
            "ave_precision_score": 0.8388416725780654,
            "fpr": 0.4313940724478595,
            "logloss": 2.287119906931088,
            "mae": 0.44483805189786313,
            "precision": 0.5365566037735849,
            "recall": 0.974304068522484
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(78)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7741228070175439,
            "auc_prc": 0.8543187642827774,
            "auditor_fn_violation": 0.004304910119240609,
            "auditor_fp_violation": 0.00914602683178535,
            "ave_precision_score": 0.852711051560298,
            "fpr": 0.08333333333333333,
            "logloss": 0.49843148214359506,
            "mae": 0.3118822584663959,
            "precision": 0.8244803695150116,
            "recall": 0.7330595482546202
        },
        "train": {
            "accuracy": 0.7892425905598244,
            "auc_prc": 0.8288965713798729,
            "auditor_fn_violation": 0.005789341312579774,
            "auditor_fp_violation": 0.007219074178459475,
            "ave_precision_score": 0.8351845616351014,
            "fpr": 0.08781558726673985,
            "logloss": 0.48783745144710267,
            "mae": 0.3091388210883256,
            "precision": 0.8160919540229885,
            "recall": 0.7601713062098501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(79)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8542139171184111,
            "auditor_fn_violation": 0.006786087395079072,
            "auditor_fp_violation": 0.003743550051599588,
            "ave_precision_score": 0.7979771856046702,
            "fpr": 0.08333333333333333,
            "logloss": 0.5237126914894228,
            "mae": 0.3368798424081321,
            "precision": 0.8194774346793349,
            "recall": 0.7084188911704312
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8485158889472234,
            "auditor_fn_violation": 0.007674461788701972,
            "auditor_fp_violation": 0.011891694109037686,
            "ave_precision_score": 0.7888113331225541,
            "fpr": 0.08122941822173436,
            "logloss": 0.5154286736492554,
            "mae": 0.3364715696558863,
            "precision": 0.8121827411167513,
            "recall": 0.6852248394004282
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(80)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8307331803495173,
            "auditor_fn_violation": 0.009861666486544908,
            "auditor_fp_violation": 0.007378740970072238,
            "ave_precision_score": 0.8199683282089335,
            "fpr": 0.08771929824561403,
            "logloss": 0.5145572689377655,
            "mae": 0.3357688546785268,
            "precision": 0.8126463700234192,
            "recall": 0.7125256673511293
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.7906000778212237,
            "auditor_fn_violation": 0.005352143795673627,
            "auditor_fp_violation": 0.009137567864241851,
            "ave_precision_score": 0.7781321189996845,
            "fpr": 0.0889132821075741,
            "logloss": 0.5067173196981968,
            "mae": 0.33160206502146045,
            "precision": 0.8066825775656324,
            "recall": 0.7237687366167024
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(81)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5383771929824561,
            "auc_prc": 0.6970636971112988,
            "auditor_fn_violation": 0.0006169170359162794,
            "auditor_fp_violation": 0.0014963880288957907,
            "ave_precision_score": 0.6932606820702711,
            "fpr": 0.4605263157894737,
            "logloss": 5.4408758838305475,
            "mae": 0.46148276486954737,
            "precision": 0.5364238410596026,
            "recall": 0.997946611909651
        },
        "train": {
            "accuracy": 0.5203073545554336,
            "auc_prc": 0.6976513714166412,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0033326410933436266,
            "ave_precision_score": 0.6911889897654095,
            "fpr": 0.4796926454445664,
            "logloss": 5.69490824548602,
            "mae": 0.479561137114602,
            "precision": 0.5165929203539823,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(82)",
        "seed": 5740,
        "test": {
            "accuracy": 0.46710526315789475,
            "auc_prc": 0.5571945555876956,
            "auditor_fn_violation": 0.000513347022587274,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5592580693887979,
            "fpr": 0.0,
            "logloss": 1.1370334859592632,
            "mae": 0.5226047213369033,
            "precision": 1.0,
            "recall": 0.002053388090349076
        },
        "train": {
            "accuracy": 0.4884742041712404,
            "auc_prc": 0.5520993749317605,
            "auditor_fn_violation": 0.0004936100997327491,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5528726229822291,
            "fpr": 0.0,
            "logloss": 1.0883863966083382,
            "mae": 0.5045796462695568,
            "precision": 1.0,
            "recall": 0.0021413276231263384
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(83)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5339912280701754,
            "auc_prc": 0.7669956140350878,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5339912280701754,
            "fpr": 0.46600877192982454,
            "logloss": 0.6918252215909739,
            "mae": 0.49618349289684965,
            "precision": 0.5339912280701754,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5126234906695939,
            "auc_prc": 0.756311745334797,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5126234906695939,
            "fpr": 0.48737650933040616,
            "logloss": 0.6966438428751327,
            "mae": 0.49858264486038595,
            "precision": 0.5126234906695939,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(84)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7554824561403509,
            "auc_prc": 0.8296500616912459,
            "auditor_fn_violation": 0.009861666486544908,
            "auditor_fp_violation": 0.0007920536635706935,
            "ave_precision_score": 0.8296788017869843,
            "fpr": 0.09100877192982457,
            "logloss": 0.5137268486436767,
            "mae": 0.32902578507925856,
            "precision": 0.8069767441860465,
            "recall": 0.7125256673511293
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8452019329103954,
            "auditor_fn_violation": 0.004957255715887432,
            "auditor_fp_violation": 0.00709051532322663,
            "ave_precision_score": 0.8431858011420696,
            "fpr": 0.09220636663007684,
            "logloss": 0.48785627740467474,
            "mae": 0.3193169380205915,
            "precision": 0.8009478672985783,
            "recall": 0.7237687366167024
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(85)",
        "seed": 5740,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.7482145610351045,
            "auditor_fn_violation": 0.06437776937209554,
            "auditor_fp_violation": 0.006514447884416926,
            "ave_precision_score": 0.698939918749979,
            "fpr": 0.05482456140350877,
            "logloss": 0.6608220029060983,
            "mae": 0.4810191987637888,
            "precision": 0.7959183673469388,
            "recall": 0.4004106776180698
        },
        "train": {
            "accuracy": 0.6410537870472008,
            "auc_prc": 0.734048935865089,
            "auditor_fn_violation": 0.05900051006376973,
            "auditor_fp_violation": 0.007950870739015637,
            "ave_precision_score": 0.6795724385388413,
            "fpr": 0.050493962678375415,
            "logloss": 0.6591614934293338,
            "mae": 0.4801779100190926,
            "precision": 0.8017241379310345,
            "recall": 0.39828693790149894
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(86)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5328947368421053,
            "auc_prc": 0.6263316886830883,
            "auditor_fn_violation": 0.004838520840087896,
            "auditor_fp_violation": 0.002342621259029932,
            "ave_precision_score": 0.5632364456394501,
            "fpr": 0.1162280701754386,
            "logloss": 0.687514054530363,
            "mae": 0.49294713949948027,
            "precision": 0.6117216117216118,
            "recall": 0.34291581108829566
        },
        "train": {
            "accuracy": 0.5236004390779363,
            "auc_prc": 0.5942392877334722,
            "auditor_fn_violation": 0.00894844595086936,
            "auditor_fp_violation": 0.0024203676783259753,
            "ave_precision_score": 0.5352114411845076,
            "fpr": 0.1394072447859495,
            "logloss": 0.691623525770441,
            "mae": 0.49465742474984126,
            "precision": 0.5574912891986062,
            "recall": 0.3426124197002141
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(87)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.7638736288735425,
            "auditor_fn_violation": 0.0685656003458338,
            "auditor_fp_violation": 0.04438080495356037,
            "ave_precision_score": 0.7521306187978878,
            "fpr": 0.17214912280701755,
            "logloss": 0.6034964570065164,
            "mae": 0.38546988743014243,
            "precision": 0.6951456310679611,
            "recall": 0.7351129363449692
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.794252075216043,
            "auditor_fn_violation": 0.06074224855854098,
            "auditor_fp_violation": 0.057562227430504063,
            "ave_precision_score": 0.7840362509910255,
            "fpr": 0.16794731064763996,
            "logloss": 0.5811087648932864,
            "mae": 0.3730887025840065,
            "precision": 0.6921529175050302,
            "recall": 0.7366167023554604
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(88)",
        "seed": 5740,
        "test": {
            "accuracy": 0.3717105263157895,
            "auc_prc": 0.5521157799247634,
            "auditor_fn_violation": 0.03669755754890308,
            "auditor_fp_violation": 0.012265221878224973,
            "ave_precision_score": 0.48622737806373545,
            "fpr": 0.28728070175438597,
            "logloss": 0.7603243135434405,
            "mae": 0.5230305146491319,
            "precision": 0.4018264840182648,
            "recall": 0.3613963039014374
        },
        "train": {
            "accuracy": 0.3896816684961581,
            "auc_prc": 0.5514257006022976,
            "auditor_fn_violation": 0.02602500487733789,
            "auditor_fp_violation": 0.03155130981695198,
            "ave_precision_score": 0.4694884722862268,
            "fpr": 0.29637760702524696,
            "logloss": 0.7488866387482008,
            "mae": 0.5177299569388514,
            "precision": 0.401330376940133,
            "recall": 0.3875802997858672
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(89)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7412280701754386,
            "auc_prc": 0.8355256137094174,
            "auditor_fn_violation": 0.002623023163658634,
            "auditor_fp_violation": 0.006413828689370487,
            "ave_precision_score": 0.8358686071210819,
            "fpr": 0.19078947368421054,
            "logloss": 0.5415184216502633,
            "mae": 0.3782194065820636,
            "precision": 0.7095158597662772,
            "recall": 0.8726899383983573
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8294700468795837,
            "auditor_fn_violation": 0.011536373188039595,
            "auditor_fp_violation": 0.011392292402171664,
            "ave_precision_score": 0.8298084259320713,
            "fpr": 0.1712403951701427,
            "logloss": 0.5497812846299668,
            "mae": 0.37692487452513135,
            "precision": 0.7209302325581395,
            "recall": 0.8629550321199143
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(90)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.806629291400987,
            "auditor_fn_violation": 0.009731078208869195,
            "auditor_fp_violation": 0.038015995872033026,
            "ave_precision_score": 0.6808971864980726,
            "fpr": 0.14802631578947367,
            "logloss": 0.6085147950507684,
            "mae": 0.4306691525536671,
            "precision": 0.73,
            "recall": 0.7494866529774127
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7857077242121326,
            "auditor_fn_violation": 0.008499495812540985,
            "auditor_fp_violation": 0.030587118402705674,
            "ave_precision_score": 0.6457501073413652,
            "fpr": 0.1778265642151482,
            "logloss": 0.6153974935026859,
            "mae": 0.43472717998449156,
            "precision": 0.6872586872586872,
            "recall": 0.7623126338329764
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(91)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7697368421052632,
            "auc_prc": 0.8607199669092434,
            "auditor_fn_violation": 0.0042148492380849525,
            "auditor_fp_violation": 0.007007223942208465,
            "ave_precision_score": 0.8524664041820471,
            "fpr": 0.07894736842105263,
            "logloss": 0.4996788671463736,
            "mae": 0.29923509018038186,
            "precision": 0.828978622327791,
            "recall": 0.7166324435318275
        },
        "train": {
            "accuracy": 0.7859495060373216,
            "auc_prc": 0.8625404113303856,
            "auditor_fn_violation": 0.00777788485721741,
            "auditor_fp_violation": 0.007466302746214935,
            "ave_precision_score": 0.8554761348349724,
            "fpr": 0.0801317233809001,
            "logloss": 0.47376144846322293,
            "mae": 0.29019952020336975,
            "precision": 0.8253588516746412,
            "recall": 0.7387580299785867
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(92)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.7664677903301557,
            "auditor_fn_violation": 0.03653544796282287,
            "auditor_fp_violation": 0.011044891640866876,
            "ave_precision_score": 0.6744669387143833,
            "fpr": 0.13267543859649122,
            "logloss": 0.6434460874103313,
            "mae": 0.42460006700926706,
            "precision": 0.7139479905437353,
            "recall": 0.6201232032854209
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7562103491793887,
            "auditor_fn_violation": 0.019318959093825885,
            "auditor_fp_violation": 0.02030240998407848,
            "ave_precision_score": 0.6512216092607793,
            "fpr": 0.15367727771679474,
            "logloss": 0.6271428365625036,
            "mae": 0.4175901095966607,
            "precision": 0.6956521739130435,
            "recall": 0.6852248394004282
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(93)",
        "seed": 5740,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8390137119473424,
            "auditor_fn_violation": 0.008506250225152202,
            "auditor_fp_violation": 0.006488648090815276,
            "ave_precision_score": 0.8333861549826352,
            "fpr": 0.06578947368421052,
            "logloss": 0.5577053602882344,
            "mae": 0.32516659624065813,
            "precision": 0.841688654353562,
            "recall": 0.6550308008213552
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8511790636398714,
            "auditor_fn_violation": 0.006372271335121298,
            "auditor_fp_violation": 0.011229121547453053,
            "ave_precision_score": 0.8434159624989919,
            "fpr": 0.07574094401756312,
            "logloss": 0.5191520907447746,
            "mae": 0.31685615948919704,
            "precision": 0.8226221079691517,
            "recall": 0.6852248394004282
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(94)",
        "seed": 5740,
        "test": {
            "accuracy": 0.555921052631579,
            "auc_prc": 0.7324293948171858,
            "auditor_fn_violation": 0.03927329874995497,
            "auditor_fp_violation": 0.05717234262125904,
            "ave_precision_score": 0.5565476328114299,
            "fpr": 0.3333333333333333,
            "logloss": 0.689286842847663,
            "mae": 0.4917469882782091,
            "precision": 0.5594202898550724,
            "recall": 0.7926078028747433
        },
        "train": {
            "accuracy": 0.5477497255762898,
            "auc_prc": 0.7209161624110658,
            "auditor_fn_violation": 0.04492557064853316,
            "auditor_fp_violation": 0.0592211311201432,
            "ave_precision_score": 0.5362408579654806,
            "fpr": 0.34796926454445665,
            "logloss": 0.6895519756602467,
            "mae": 0.4915921497685195,
            "precision": 0.5399129172714079,
            "recall": 0.7965738758029979
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(95)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5328947368421053,
            "auc_prc": 0.689407087304199,
            "auditor_fn_violation": 0.018003170143016688,
            "auditor_fp_violation": 0.005270897832817337,
            "ave_precision_score": 0.5774751585091451,
            "fpr": 0.03179824561403509,
            "logloss": 0.6956590954495229,
            "mae": 0.4882121107361296,
            "precision": 0.7563025210084033,
            "recall": 0.18480492813141683
        },
        "train": {
            "accuracy": 0.5488474204171241,
            "auc_prc": 0.6740561253718081,
            "auditor_fn_violation": 0.00948906653629093,
            "auditor_fp_violation": 0.004113883367450877,
            "ave_precision_score": 0.5572272482718968,
            "fpr": 0.030735455543358946,
            "logloss": 0.6903427606016872,
            "mae": 0.4856368890512657,
            "precision": 0.75,
            "recall": 0.17987152034261242
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(96)",
        "seed": 5740,
        "test": {
            "accuracy": 0.4824561403508772,
            "auc_prc": 0.6218170877287744,
            "auditor_fn_violation": 0.023978709607694812,
            "auditor_fp_violation": 0.017788957688338495,
            "ave_precision_score": 0.6193331606073438,
            "fpr": 0.03837719298245614,
            "logloss": 0.8709838187390461,
            "mae": 0.4722198312800876,
            "precision": 0.5882352941176471,
            "recall": 0.1026694045174538
        },
        "train": {
            "accuracy": 0.4972557628979144,
            "auc_prc": 0.6308667224947936,
            "auditor_fn_violation": 0.02445485465533088,
            "auditor_fp_violation": 0.019714005992820485,
            "ave_precision_score": 0.6245039431017324,
            "fpr": 0.04610318331503842,
            "logloss": 0.8413816031701625,
            "mae": 0.45449926938276847,
            "precision": 0.5483870967741935,
            "recall": 0.10920770877944326
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(97)",
        "seed": 5740,
        "test": {
            "accuracy": 0.5493421052631579,
            "auc_prc": 0.6277838121038106,
            "auditor_fn_violation": 0.0008758420692388056,
            "auditor_fp_violation": 0.00814499484004128,
            "ave_precision_score": 0.5575681917783697,
            "fpr": 0.44627192982456143,
            "logloss": 0.684732810900356,
            "mae": 0.4881113348120268,
            "precision": 0.5426966292134832,
            "recall": 0.9917864476386037
        },
        "train": {
            "accuracy": 0.5301866081229418,
            "auc_prc": 0.6356515034055605,
            "auditor_fn_violation": 0.00046070275975056244,
            "auditor_fp_violation": 0.00882605986886998,
            "ave_precision_score": 0.5484880773799972,
            "fpr": 0.46871569703622395,
            "logloss": 0.677480692283329,
            "mae": 0.4864126854789833,
            "precision": 0.5218365061590146,
            "recall": 0.9978586723768736
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(98)",
        "seed": 5740,
        "test": {
            "accuracy": 0.7576754385964912,
            "auc_prc": 0.7713577663402075,
            "auditor_fn_violation": 0.008549029143701143,
            "auditor_fp_violation": 0.005908152734778127,
            "ave_precision_score": 0.7597153228523128,
            "fpr": 0.09210526315789473,
            "logloss": 0.5496126921463165,
            "mae": 0.35854011589432494,
            "precision": 0.8064516129032258,
            "recall": 0.7186858316221766
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.7874791196490312,
            "auditor_fn_violation": 0.005199359717184918,
            "auditor_fp_violation": 0.008188210164060877,
            "ave_precision_score": 0.7754437081247743,
            "fpr": 0.09220636663007684,
            "logloss": 0.534794251706639,
            "mae": 0.3523765356478337,
            "precision": 0.8023529411764706,
            "recall": 0.7301927194860813
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(99)",
        "seed": 5740,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.7843071649881372,
            "auditor_fn_violation": 0.0611693504809251,
            "auditor_fp_violation": 0.03603715170278638,
            "ave_precision_score": 0.7191500877686403,
            "fpr": 0.16666666666666666,
            "logloss": 0.6136362831973189,
            "mae": 0.4266421623158334,
            "precision": 0.6853002070393375,
            "recall": 0.6796714579055442
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7320872591414082,
            "auditor_fn_violation": 0.059212057249369475,
            "auditor_fp_violation": 0.038003975435369514,
            "ave_precision_score": 0.7176980704095055,
            "fpr": 0.1668496158068057,
            "logloss": 0.6041946768956523,
            "mae": 0.42306594406770853,
            "precision": 0.691683569979716,
            "recall": 0.7301927194860813
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.7175311117959455,
            "auditor_fn_violation": 0.03066638370118846,
            "auditor_fp_violation": 0.07482878056438637,
            "ave_precision_score": 0.6067092559156065,
            "fpr": 0.2719298245614035,
            "logloss": 7.24421349313049,
            "mae": 0.34249345866927255,
            "precision": 0.6202143950995406,
            "recall": 0.8709677419354839
        },
        "train": {
            "accuracy": 0.7003293084522503,
            "auc_prc": 0.7395908376729732,
            "auditor_fn_violation": 0.022189598162876367,
            "auditor_fp_violation": 0.08387220959208412,
            "ave_precision_score": 0.6272993773131085,
            "fpr": 0.2513721185510428,
            "logloss": 7.2313278706534705,
            "mae": 0.31129338906950876,
            "precision": 0.6602373887240356,
            "recall": 0.9100204498977505
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 29759,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.7559709574385106,
            "auditor_fn_violation": 0.0163648368232409,
            "auditor_fp_violation": 0.020067997174143416,
            "ave_precision_score": 0.7283140930699832,
            "fpr": 0.18092105263157895,
            "logloss": 2.433165520564355,
            "mae": 0.2819530748511707,
            "precision": 0.7,
            "recall": 0.8279569892473119
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.799806493559468,
            "auditor_fn_violation": 0.016528276304831432,
            "auditor_fp_violation": 0.022546964171448493,
            "ave_precision_score": 0.776384847943085,
            "fpr": 0.15806805708013172,
            "logloss": 1.9798554684317213,
            "mae": 0.2575974809602272,
            "precision": 0.7367458866544789,
            "recall": 0.8241308793456033
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 29759,
        "test": {
            "accuracy": 0.680921052631579,
            "auc_prc": 0.7037156657309154,
            "auditor_fn_violation": 0.016407281644972644,
            "auditor_fp_violation": 0.014561011028690295,
            "ave_precision_score": 0.6897829495403343,
            "fpr": 0.08442982456140351,
            "logloss": 4.9321009683975605,
            "mae": 0.3576308784669265,
            "precision": 0.7652439024390244,
            "recall": 0.5397849462365591
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.7303589394667833,
            "auditor_fn_violation": 0.022429789058519046,
            "auditor_fp_violation": 0.010321452910972266,
            "ave_precision_score": 0.7206910699890823,
            "fpr": 0.07354555433589462,
            "logloss": 4.9839879588886955,
            "mae": 0.3629680748101222,
            "precision": 0.7912772585669782,
            "recall": 0.5194274028629857
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.704364530588242,
            "auditor_fn_violation": 0.05520892284474628,
            "auditor_fp_violation": 0.08484192864712116,
            "ave_precision_score": 0.5969776880741442,
            "fpr": 0.23026315789473684,
            "logloss": 7.184035926039615,
            "mae": 0.3627776956761834,
            "precision": 0.6263345195729537,
            "recall": 0.7569892473118279
        },
        "train": {
            "accuracy": 0.6641053787047201,
            "auc_prc": 0.726826461702399,
            "auditor_fn_violation": 0.046071756468879566,
            "auditor_fp_violation": 0.08818755494977136,
            "ave_precision_score": 0.6189346966608064,
            "fpr": 0.22941822173435786,
            "logloss": 7.182359757417545,
            "mae": 0.34760405047851184,
            "precision": 0.6522462562396006,
            "recall": 0.8016359918200409
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6447368421052632,
            "auc_prc": 0.7008332142854736,
            "auditor_fn_violation": 0.0486252593850217,
            "auditor_fp_violation": 0.0780790062404333,
            "ave_precision_score": 0.5849589454868314,
            "fpr": 0.24451754385964913,
            "logloss": 7.938407318373766,
            "mae": 0.37047031361016836,
            "precision": 0.6201022146507666,
            "recall": 0.7827956989247312
        },
        "train": {
            "accuracy": 0.6498353457738749,
            "auc_prc": 0.7139781402847225,
            "auditor_fn_violation": 0.046729475463489864,
            "auditor_fp_violation": 0.0764744746931917,
            "ave_precision_score": 0.5948276889168836,
            "fpr": 0.2502744237102086,
            "logloss": 8.300609676678869,
            "mae": 0.3600018347099228,
            "precision": 0.6357827476038339,
            "recall": 0.8139059304703476
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7216142292774661,
            "auditor_fn_violation": 0.03866723259762309,
            "auditor_fp_violation": 0.0842850975313003,
            "ave_precision_score": 0.6089867594211213,
            "fpr": 0.23464912280701755,
            "logloss": 7.2824569330156175,
            "mae": 0.33660938642721,
            "precision": 0.6433333333333333,
            "recall": 0.8301075268817204
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7411557962877725,
            "auditor_fn_violation": 0.02655344022950577,
            "auditor_fp_violation": 0.08634592474287411,
            "ave_precision_score": 0.6284116861860343,
            "fpr": 0.23380900109769484,
            "logloss": 7.2584247551315,
            "mae": 0.3137105405478223,
            "precision": 0.6697674418604651,
            "recall": 0.8834355828220859
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7260112262997075,
            "auditor_fn_violation": 0.042246745897000566,
            "auditor_fp_violation": 0.09182807410023941,
            "ave_precision_score": 0.6215821383819506,
            "fpr": 0.21271929824561403,
            "logloss": 6.714904727926895,
            "mae": 0.32694959487927544,
            "precision": 0.6608391608391608,
            "recall": 0.8129032258064516
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7397203573638824,
            "auditor_fn_violation": 0.04044186145699348,
            "auditor_fp_violation": 0.09307515828135325,
            "ave_precision_score": 0.6321363368263808,
            "fpr": 0.21514818880351264,
            "logloss": 7.011954442280374,
            "mae": 0.314723421156816,
            "precision": 0.6765676567656765,
            "recall": 0.8384458077709611
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6732456140350878,
            "auc_prc": 0.7170388496885962,
            "auditor_fn_violation": 0.041595925297113756,
            "auditor_fp_violation": 0.08527856273794106,
            "ave_precision_score": 0.6063943663110679,
            "fpr": 0.22916666666666666,
            "logloss": 7.302352359044613,
            "mae": 0.34008007290557485,
            "precision": 0.6427350427350428,
            "recall": 0.8086021505376344
        },
        "train": {
            "accuracy": 0.6893523600439078,
            "auc_prc": 0.7388409992796254,
            "auditor_fn_violation": 0.035294593011118376,
            "auditor_fp_violation": 0.08737338792327581,
            "ave_precision_score": 0.6265467833614107,
            "fpr": 0.2261251372118551,
            "logloss": 7.305541922397922,
            "mae": 0.3199920049986129,
            "precision": 0.6666666666666666,
            "recall": 0.8425357873210634
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 29759,
        "test": {
            "accuracy": 0.643640350877193,
            "auc_prc": 0.6994515303263499,
            "auditor_fn_violation": 0.07142991888322957,
            "auditor_fp_violation": 0.0975141292829389,
            "ave_precision_score": 0.5967252054553885,
            "fpr": 0.20394736842105263,
            "logloss": 8.115173541782028,
            "mae": 0.36800988145337904,
            "precision": 0.63671875,
            "recall": 0.7010752688172043
        },
        "train": {
            "accuracy": 0.6542261251372119,
            "auc_prc": 0.7223005793609107,
            "auditor_fn_violation": 0.06748466257668712,
            "auditor_fp_violation": 0.09629801114342347,
            "ave_precision_score": 0.6162087066811571,
            "fpr": 0.2074643249176729,
            "logloss": 8.219385709194766,
            "mae": 0.35534880684443054,
            "precision": 0.657608695652174,
            "recall": 0.7423312883435583
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7161818891684617,
            "auditor_fn_violation": 0.0466893039049236,
            "auditor_fp_violation": 0.08877163546450018,
            "ave_precision_score": 0.6064435225274993,
            "fpr": 0.22039473684210525,
            "logloss": 7.158907214786074,
            "mae": 0.340005287159979,
            "precision": 0.6461267605633803,
            "recall": 0.789247311827957
        },
        "train": {
            "accuracy": 0.6849615806805708,
            "auc_prc": 0.7379671371972841,
            "auditor_fn_violation": 0.03992780804482366,
            "auditor_fp_violation": 0.09077312052273165,
            "ave_precision_score": 0.6265185354893941,
            "fpr": 0.22063666300768386,
            "logloss": 7.209643452439389,
            "mae": 0.32209448052844086,
            "precision": 0.6672185430463576,
            "recall": 0.8241308793456033
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6458333333333334,
            "auc_prc": 0.7183282169077707,
            "auditor_fn_violation": 0.03304093567251462,
            "auditor_fp_violation": 0.07517955963734842,
            "ave_precision_score": 0.6053625076128605,
            "fpr": 0.28618421052631576,
            "logloss": 7.2670717606445905,
            "mae": 0.3581072220196211,
            "precision": 0.6069277108433735,
            "recall": 0.8666666666666667
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.7357596223347482,
            "auditor_fn_violation": 0.026499565636090593,
            "auditor_fp_violation": 0.07618314336102716,
            "ave_precision_score": 0.6211609825532705,
            "fpr": 0.27442371020856204,
            "logloss": 7.398216005701506,
            "mae": 0.3368571339836519,
            "precision": 0.637155297532656,
            "recall": 0.8977505112474438
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.7071780949384234,
            "auditor_fn_violation": 0.03773108847387286,
            "auditor_fp_violation": 0.08039954472310529,
            "ave_precision_score": 0.5997924394393864,
            "fpr": 0.25219298245614036,
            "logloss": 7.084205896739071,
            "mae": 0.3456637874369428,
            "precision": 0.627831715210356,
            "recall": 0.8344086021505376
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7308217260990538,
            "auditor_fn_violation": 0.02676220427898958,
            "auditor_fp_violation": 0.0865228044802597,
            "ave_precision_score": 0.6229284989792092,
            "fpr": 0.23600439077936333,
            "logloss": 7.061836829091488,
            "mae": 0.3163661980148893,
            "precision": 0.6692307692307692,
            "recall": 0.8895705521472392
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 29759,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.7491193139963326,
            "auditor_fn_violation": 0.023698358800226388,
            "auditor_fp_violation": 0.011502119392440838,
            "ave_precision_score": 0.7228760894365266,
            "fpr": 0.08552631578947369,
            "logloss": 2.680719884759474,
            "mae": 0.2859770029311088,
            "precision": 0.7984496124031008,
            "recall": 0.6645161290322581
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.795023330794999,
            "auditor_fn_violation": 0.03139541931269488,
            "auditor_fp_violation": 0.014691422893440362,
            "ave_precision_score": 0.7726200341282246,
            "fpr": 0.07683863885839737,
            "logloss": 2.1304057628703332,
            "mae": 0.27208302065045725,
            "precision": 0.8267326732673267,
            "recall": 0.6830265848670757
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 29759,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.7170172899657155,
            "auditor_fn_violation": 0.02500707413695529,
            "auditor_fp_violation": 0.07142646885670552,
            "ave_precision_score": 0.6059895035303042,
            "fpr": 0.27631578947368424,
            "logloss": 6.974551238281051,
            "mae": 0.35647278248901576,
            "precision": 0.6221889055472264,
            "recall": 0.8924731182795699
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7385150739956751,
            "auditor_fn_violation": 0.012530332518480111,
            "auditor_fp_violation": 0.0784123482866076,
            "ave_precision_score": 0.6263771517387041,
            "fpr": 0.2579582875960483,
            "logloss": 7.11344502367145,
            "mae": 0.32441175241466663,
            "precision": 0.6594202898550725,
            "recall": 0.9304703476482618
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 29759,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.7534200617330378,
            "auditor_fn_violation": 0.01614082248632334,
            "auditor_fp_violation": 0.02687997959103576,
            "ave_precision_score": 0.7267908373893737,
            "fpr": 0.17543859649122806,
            "logloss": 2.38666985572193,
            "mae": 0.2817470987330469,
            "precision": 0.7037037037037037,
            "recall": 0.8172043010752689
        },
        "train": {
            "accuracy": 0.75192096597146,
            "auc_prc": 0.798721326396291,
            "auditor_fn_violation": 0.017828000870972596,
            "auditor_fp_violation": 0.02492703710832844,
            "ave_precision_score": 0.776318543755208,
            "fpr": 0.150384193194292,
            "logloss": 1.9359846391552455,
            "mae": 0.25334783879133405,
            "precision": 0.74487895716946,
            "recall": 0.8179959100204499
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.7006672825719628,
            "auditor_fn_violation": 0.050898415393322016,
            "auditor_fp_violation": 0.07899152243023666,
            "ave_precision_score": 0.5847924420793954,
            "fpr": 0.24232456140350878,
            "logloss": 7.946844495431306,
            "mae": 0.3712429229678347,
            "precision": 0.6196213425129088,
            "recall": 0.7741935483870968
        },
        "train": {
            "accuracy": 0.6443468715697036,
            "auc_prc": 0.7129026153828251,
            "auditor_fn_violation": 0.047463516798771664,
            "auditor_fp_violation": 0.0764744746931917,
            "ave_precision_score": 0.594452128487517,
            "fpr": 0.2502744237102086,
            "logloss": 8.293870273042891,
            "mae": 0.36109638958910073,
            "precision": 0.6328502415458938,
            "recall": 0.803680981595092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 29759,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.7092121803220988,
            "auditor_fn_violation": 0.045743727598566306,
            "auditor_fp_violation": 0.08571519682876094,
            "ave_precision_score": 0.6012776678378107,
            "fpr": 0.22807017543859648,
            "logloss": 7.229432382017249,
            "mae": 0.34496651094956393,
            "precision": 0.6407599309153713,
            "recall": 0.7978494623655914
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.730738634351791,
            "auditor_fn_violation": 0.035920885159569815,
            "auditor_fp_violation": 0.08847108276411007,
            "ave_precision_score": 0.6226859747760154,
            "fpr": 0.2261251372118551,
            "logloss": 7.179594029803253,
            "mae": 0.3229153803011399,
            "precision": 0.667741935483871,
            "recall": 0.8466257668711656
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.7090872690887465,
            "auditor_fn_violation": 0.02979154876438408,
            "auditor_fp_violation": 0.07671759095725891,
            "ave_precision_score": 0.6013351503410982,
            "fpr": 0.25877192982456143,
            "logloss": 7.071406329754707,
            "mae": 0.34442487516898385,
            "precision": 0.6295133437990581,
            "recall": 0.8623655913978494
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.731431776484399,
            "auditor_fn_violation": 0.02431764460277589,
            "auditor_fp_violation": 0.08329995161818948,
            "ave_precision_score": 0.6235401245250706,
            "fpr": 0.24368825466520308,
            "logloss": 7.0488991411072455,
            "mae": 0.31185982243122023,
            "precision": 0.6646525679758308,
            "recall": 0.8997955010224948
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.7628274373751294,
            "auditor_fn_violation": 0.02170581022448595,
            "auditor_fp_violation": 0.034332195141096596,
            "ave_precision_score": 0.7236232558806915,
            "fpr": 0.16666666666666666,
            "logloss": 2.858830606945847,
            "mae": 0.27892903585524553,
            "precision": 0.7169459962756052,
            "recall": 0.8279569892473119
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.7779829191111562,
            "auditor_fn_violation": 0.02176309096500621,
            "auditor_fp_violation": 0.036863818209248736,
            "ave_precision_score": 0.7433053207107223,
            "fpr": 0.15697036223929747,
            "logloss": 2.5879373016411744,
            "mae": 0.2733429082719447,
            "precision": 0.7371323529411765,
            "recall": 0.820040899795501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.7367701040398675,
            "auditor_fn_violation": 0.018263063572910774,
            "auditor_fp_violation": 0.011575709407747563,
            "ave_precision_score": 0.7154833872690395,
            "fpr": 0.1206140350877193,
            "logloss": 1.8567066090014266,
            "mae": 0.30422456740710474,
            "precision": 0.7555555555555555,
            "recall": 0.7311827956989247
        },
        "train": {
            "accuracy": 0.7639956092206367,
            "auc_prc": 0.7743380197444674,
            "auditor_fn_violation": 0.03156377741711731,
            "auditor_fp_violation": 0.0057850078815530025,
            "ave_precision_score": 0.7577116103089706,
            "fpr": 0.10428100987925357,
            "logloss": 1.5313499324976263,
            "mae": 0.2927625098122627,
            "precision": 0.7952586206896551,
            "recall": 0.754601226993865
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7187726402895431,
            "auditor_fn_violation": 0.03922608941709112,
            "auditor_fp_violation": 0.0842850975313003,
            "ave_precision_score": 0.607779922393257,
            "fpr": 0.23464912280701755,
            "logloss": 7.250520260683541,
            "mae": 0.33745796322257204,
            "precision": 0.6433333333333333,
            "recall": 0.8301075268817204
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7393264355013479,
            "auditor_fn_violation": 0.027758884257170376,
            "auditor_fp_violation": 0.08818755494977136,
            "ave_precision_score": 0.6270350723597785,
            "fpr": 0.22941822173435786,
            "logloss": 7.25967756211418,
            "mae": 0.3148819427704997,
            "precision": 0.6724137931034483,
            "recall": 0.8773006134969326
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5350877192982456,
            "auc_prc": 0.7440000388117884,
            "auditor_fn_violation": 0.0035370684776457276,
            "auditor_fp_violation": 0.0060540052592331015,
            "ave_precision_score": 0.519069607092571,
            "fpr": 0.44956140350877194,
            "logloss": 15.538449575492667,
            "mae": 0.4710348950756541,
            "precision": 0.5238095238095238,
            "recall": 0.9698924731182795
        },
        "train": {
            "accuracy": 0.544456641053787,
            "auc_prc": 0.7509106202190922,
            "auditor_fn_violation": 0.0037644872148855507,
            "auditor_fp_violation": 0.0015476977021241182,
            "ave_precision_score": 0.5330493094861968,
            "fpr": 0.442371020856202,
            "logloss": 15.409921649759557,
            "mae": 0.46037938707517345,
            "precision": 0.5420454545454545,
            "recall": 0.9754601226993865
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.7212326878204217,
            "auditor_fn_violation": 0.031868986983588005,
            "auditor_fp_violation": 0.07765218415165431,
            "ave_precision_score": 0.6091636796928038,
            "fpr": 0.2565789473684211,
            "logloss": 7.289426912677631,
            "mae": 0.33880538997555776,
            "precision": 0.6303317535545023,
            "recall": 0.8580645161290322
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7400537443148882,
            "auditor_fn_violation": 0.023300761652064406,
            "auditor_fp_violation": 0.08450429453597683,
            "ave_precision_score": 0.6276146535776737,
            "fpr": 0.23819978046103182,
            "logloss": 7.261157776350744,
            "mae": 0.3108605060588565,
            "precision": 0.6702127659574468,
            "recall": 0.901840490797546
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.7271676755864092,
            "auditor_fn_violation": 0.027270797962648557,
            "auditor_fp_violation": 0.0822613721103654,
            "ave_precision_score": 0.6218498854175816,
            "fpr": 0.25877192982456143,
            "logloss": 6.722667888829713,
            "mae": 0.3305307699830316,
            "precision": 0.6352395672333848,
            "recall": 0.8838709677419355
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7420316347162796,
            "auditor_fn_violation": 0.024755375674274212,
            "auditor_fp_violation": 0.08549534129985797,
            "ave_precision_score": 0.6336104320777823,
            "fpr": 0.24368825466520308,
            "logloss": 6.963051310555309,
            "mae": 0.3065420482913557,
            "precision": 0.6661654135338346,
            "recall": 0.9059304703476483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.7529963897505514,
            "auditor_fn_violation": 0.01135398981324279,
            "auditor_fp_violation": 0.012414635582244208,
            "ave_precision_score": 0.7259339308517919,
            "fpr": 0.15570175438596492,
            "logloss": 2.0297211156810167,
            "mae": 0.29978420502702907,
            "precision": 0.7279693486590039,
            "recall": 0.8172043010752689
        },
        "train": {
            "accuracy": 0.7914379802414928,
            "auc_prc": 0.7926018432648202,
            "auditor_fn_violation": 0.014270032930845227,
            "auditor_fp_violation": 0.010428100987925355,
            "ave_precision_score": 0.7708807755209284,
            "fpr": 0.11306256860592755,
            "logloss": 1.7037579831706537,
            "mae": 0.2693963627612165,
            "precision": 0.7960396039603961,
            "recall": 0.8220858895705522
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7158329568787521,
            "auditor_fn_violation": 0.04000188643652142,
            "auditor_fp_violation": 0.08527856273794106,
            "ave_precision_score": 0.6055377684458596,
            "fpr": 0.22916666666666666,
            "logloss": 7.287852703048858,
            "mae": 0.3389336317297944,
            "precision": 0.6463620981387479,
            "recall": 0.821505376344086
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7377043392470214,
            "auditor_fn_violation": 0.035200312472641805,
            "auditor_fp_violation": 0.08939189786755869,
            "ave_precision_score": 0.6257204925553134,
            "fpr": 0.2239297475301866,
            "logloss": 7.293259305333254,
            "mae": 0.3188602029433058,
            "precision": 0.6709677419354839,
            "recall": 0.8507157464212679
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5833333333333334,
            "auc_prc": 0.5659585931157401,
            "auditor_fn_violation": 0.011955291454442567,
            "auditor_fp_violation": 0.03668707563091173,
            "ave_precision_score": 0.5366592847490415,
            "fpr": 0.1206140350877193,
            "logloss": 8.786884661471793,
            "mae": 0.43432241111507,
            "precision": 0.639344262295082,
            "recall": 0.41935483870967744
        },
        "train": {
            "accuracy": 0.5740944017563118,
            "auc_prc": 0.5935954909283798,
            "auditor_fn_violation": 0.02226592050354787,
            "auditor_fp_violation": 0.048147704985407436,
            "ave_precision_score": 0.5576162130381406,
            "fpr": 0.1207464324917673,
            "logloss": 9.574006942608076,
            "mae": 0.4397365326196202,
            "precision": 0.6573208722741433,
            "recall": 0.43149284253578735
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5745614035087719,
            "auc_prc": 0.7276786645096958,
            "auditor_fn_violation": 0.040386247877758914,
            "auditor_fp_violation": 0.012758055653675575,
            "ave_precision_score": 0.7048479566857537,
            "fpr": 0.0537280701754386,
            "logloss": 4.493249894415808,
            "mae": 0.42098664848881606,
            "precision": 0.72,
            "recall": 0.2709677419354839
        },
        "train": {
            "accuracy": 0.5554335894621295,
            "auc_prc": 0.7600440118085865,
            "auditor_fn_violation": 0.036506771362959885,
            "auditor_fp_violation": 0.013260777958703786,
            "ave_precision_score": 0.743113227798578,
            "fpr": 0.042810098792535674,
            "logloss": 4.260155485730533,
            "mae": 0.441608735628048,
            "precision": 0.7592592592592593,
            "recall": 0.25153374233128833
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7261368747553799,
            "auditor_fn_violation": 0.03884408602150538,
            "auditor_fp_violation": 0.08834481337572118,
            "ave_precision_score": 0.6211398283633289,
            "fpr": 0.22697368421052633,
            "logloss": 6.712890151488076,
            "mae": 0.32643245290043016,
            "precision": 0.6515151515151515,
            "recall": 0.832258064516129
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7410942118621556,
            "auditor_fn_violation": 0.02854904496059298,
            "auditor_fp_violation": 0.09187081536356591,
            "ave_precision_score": 0.6328212990856577,
            "fpr": 0.22063666300768386,
            "logloss": 6.986726551474551,
            "mae": 0.30891860327556436,
            "precision": 0.6814580031695721,
            "recall": 0.8793456032719836
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 29759,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.7206807427813754,
            "auditor_fn_violation": 0.02394831163931334,
            "auditor_fp_violation": 0.07038884964088074,
            "ave_precision_score": 0.6086109719043392,
            "fpr": 0.2807017543859649,
            "logloss": 7.05096245699918,
            "mae": 0.34805560878141095,
            "precision": 0.6207407407407407,
            "recall": 0.9010752688172043
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7398744279054,
            "auditor_fn_violation": 0.018784274904092003,
            "auditor_fp_violation": 0.0724712700485379,
            "ave_precision_score": 0.6271260941384283,
            "fpr": 0.27332601536772777,
            "logloss": 7.188003472772358,
            "mae": 0.31963429009099975,
            "precision": 0.6458036984352774,
            "recall": 0.9284253578732107
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7213226704958434,
            "auditor_fn_violation": 0.039226089417091124,
            "auditor_fp_violation": 0.08396866046548138,
            "ave_precision_score": 0.608695272814501,
            "fpr": 0.2324561403508772,
            "logloss": 7.305681080142895,
            "mae": 0.3383487901886535,
            "precision": 0.644891122278057,
            "recall": 0.8279569892473119
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7385912482879537,
            "auditor_fn_violation": 0.027911528938513383,
            "auditor_fp_violation": 0.08616904500548848,
            "ave_precision_score": 0.6263025332794562,
            "fpr": 0.23161361141602635,
            "logloss": 7.291613051323271,
            "mae": 0.319138786315099,
            "precision": 0.6677165354330709,
            "recall": 0.8670756646216768
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.7002194368494692,
            "auditor_fn_violation": 0.037068477645727224,
            "auditor_fp_violation": 0.07872905137564269,
            "ave_precision_score": 0.605657303798041,
            "fpr": 0.24561403508771928,
            "logloss": 5.967008159520658,
            "mae": 0.389474326379346,
            "precision": 0.631578947368421,
            "recall": 0.8258064516129032
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.715253790039611,
            "auditor_fn_violation": 0.035465195890266435,
            "auditor_fp_violation": 0.08191872896301654,
            "ave_precision_score": 0.6183760928339437,
            "fpr": 0.24698133918770582,
            "logloss": 6.2883852768923,
            "mae": 0.3770166731679262,
            "precision": 0.652241112828439,
            "recall": 0.8629856850715747
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7260159012285977,
            "auditor_fn_violation": 0.042246745897000566,
            "auditor_fp_violation": 0.09182807410023941,
            "ave_precision_score": 0.6215868112380625,
            "fpr": 0.21271929824561403,
            "logloss": 6.718509905963888,
            "mae": 0.3269054729009312,
            "precision": 0.6608391608391608,
            "recall": 0.8129032258064516
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7397277974029365,
            "auditor_fn_violation": 0.04044186145699348,
            "auditor_fp_violation": 0.09307515828135325,
            "ave_precision_score": 0.632143773524326,
            "fpr": 0.21514818880351264,
            "logloss": 7.012005631619371,
            "mae": 0.31468508831167086,
            "precision": 0.6765676567656765,
            "recall": 0.8384458077709611
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6535087719298246,
            "auc_prc": 0.7195316585953755,
            "auditor_fn_violation": 0.027801358234295415,
            "auditor_fp_violation": 0.07292279916794224,
            "ave_precision_score": 0.6066869729883145,
            "fpr": 0.2883771929824561,
            "logloss": 7.3905955150396405,
            "mae": 0.34704094421574094,
            "precision": 0.6103703703703703,
            "recall": 0.886021505376344
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7396830270201717,
            "auditor_fn_violation": 0.022568965091508242,
            "auditor_fp_violation": 0.075366375161923,
            "ave_precision_score": 0.6272474330754638,
            "fpr": 0.2678375411635565,
            "logloss": 7.290768441488244,
            "mae": 0.31354389692990825,
            "precision": 0.6473988439306358,
            "recall": 0.9161554192229039
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7193286251284151,
            "auditor_fn_violation": 0.038315883795510285,
            "auditor_fp_violation": 0.08473154362416109,
            "ave_precision_score": 0.6081625894676204,
            "fpr": 0.23355263157894737,
            "logloss": 7.300679483569335,
            "mae": 0.3373783455045545,
            "precision": 0.6444073455759599,
            "recall": 0.8301075268817204
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7397293102248139,
            "auditor_fn_violation": 0.02973877556517816,
            "auditor_fp_violation": 0.08680633229459842,
            "ave_precision_score": 0.6272859567050239,
            "fpr": 0.2327113062568606,
            "logloss": 7.291633416443617,
            "mae": 0.3151537135036008,
            "precision": 0.6682316118935837,
            "recall": 0.8732106339468303
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.7138948726103047,
            "auditor_fn_violation": 0.0415605546123373,
            "auditor_fp_violation": 0.08571519682876094,
            "ave_precision_score": 0.604512170503142,
            "fpr": 0.22807017543859648,
            "logloss": 7.266799914317981,
            "mae": 0.3406339258455893,
            "precision": 0.6450511945392492,
            "recall": 0.8129032258064516
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.735707168510396,
            "auditor_fn_violation": 0.03858767753362112,
            "auditor_fp_violation": 0.08893149031583437,
            "ave_precision_score": 0.6251056692010879,
            "fpr": 0.22502744237102085,
            "logloss": 7.259909500424071,
            "mae": 0.32096544609531596,
            "precision": 0.6677471636952999,
            "recall": 0.8425357873210634
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7155414782150568,
            "auditor_fn_violation": 0.042374080362195816,
            "auditor_fp_violation": 0.08655657600376782,
            "ave_precision_score": 0.6052478810388275,
            "fpr": 0.2236842105263158,
            "logloss": 7.292424891724394,
            "mae": 0.34054502655504404,
            "precision": 0.6476683937823834,
            "recall": 0.8064516129032258
        },
        "train": {
            "accuracy": 0.6937431394072447,
            "auc_prc": 0.7376431106794621,
            "auditor_fn_violation": 0.03616107605521248,
            "auditor_fp_violation": 0.08985230541928303,
            "ave_precision_score": 0.6256592785990224,
            "fpr": 0.22283205268935236,
            "logloss": 7.302126180218853,
            "mae": 0.31979652616331433,
            "precision": 0.6704545454545454,
            "recall": 0.8445807770961146
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.7213910803323347,
            "auditor_fn_violation": 0.026457272212790043,
            "auditor_fp_violation": 0.07480425055928414,
            "ave_precision_score": 0.6091571706626565,
            "fpr": 0.28289473684210525,
            "logloss": 7.224321313920976,
            "mae": 0.346754502774961,
            "precision": 0.6166419019316494,
            "recall": 0.8924731182795699
        },
        "train": {
            "accuracy": 0.6904500548847421,
            "auc_prc": 0.7404298603092785,
            "auditor_fn_violation": 0.020676619997800124,
            "auditor_fp_violation": 0.07704673266708632,
            "ave_precision_score": 0.6279846487400331,
            "fpr": 0.2678375411635565,
            "logloss": 7.215093841623742,
            "mae": 0.3120289527456202,
            "precision": 0.6489208633093525,
            "recall": 0.9222903885480572
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.724166706656032,
            "auditor_fn_violation": 0.0457838143746463,
            "auditor_fp_violation": 0.09444787864515877,
            "ave_precision_score": 0.6202253586849531,
            "fpr": 0.20614035087719298,
            "logloss": 6.675698528041392,
            "mae": 0.3283718058269722,
            "precision": 0.6624775583482945,
            "recall": 0.7935483870967742
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7392619286368906,
            "auditor_fn_violation": 0.04347904166077414,
            "auditor_fp_violation": 0.0949167884882505,
            "ave_precision_score": 0.6318269389835577,
            "fpr": 0.21075740944017562,
            "logloss": 6.986472447608932,
            "mae": 0.3173068558076643,
            "precision": 0.6762225969645869,
            "recall": 0.820040899795501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5756578947368421,
            "auc_prc": 0.6988557777738758,
            "auditor_fn_violation": 0.06476372382569327,
            "auditor_fp_violation": 0.05732907492444758,
            "ave_precision_score": 0.5958235527510977,
            "fpr": 0.28399122807017546,
            "logloss": 10.255982941818806,
            "mae": 0.4225059605310252,
            "precision": 0.5654362416107382,
            "recall": 0.7247311827956989
        },
        "train": {
            "accuracy": 0.5960482985729967,
            "auc_prc": 0.7207679303709131,
            "auditor_fn_violation": 0.07085182466513573,
            "auditor_fp_violation": 0.0627766997362411,
            "ave_precision_score": 0.6149622669854655,
            "fpr": 0.2623490669593853,
            "logloss": 10.352875609868885,
            "mae": 0.4019459829718341,
            "precision": 0.6010016694490818,
            "recall": 0.7361963190184049
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.7087957749019054,
            "auditor_fn_violation": 0.008253159781173374,
            "auditor_fp_violation": 0.014673849052160612,
            "ave_precision_score": 0.6996330606033887,
            "fpr": 0.12719298245614036,
            "logloss": 2.33949970474856,
            "mae": 0.319734924960893,
            "precision": 0.7321016166281755,
            "recall": 0.6817204301075269
        },
        "train": {
            "accuracy": 0.7793633369923162,
            "auc_prc": 0.8032683816041885,
            "auditor_fn_violation": 0.008249547116699113,
            "auditor_fp_violation": 0.008037623360610965,
            "ave_precision_score": 0.7954183841831419,
            "fpr": 0.08232711306256861,
            "logloss": 1.4305308932722163,
            "mae": 0.269767504837977,
            "precision": 0.8287671232876712,
            "recall": 0.7423312883435583
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 29759,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.7275609320818638,
            "auditor_fn_violation": 0.04842246745897001,
            "auditor_fp_violation": 0.07421553043683034,
            "ave_precision_score": 0.6442159819967033,
            "fpr": 0.17434210526315788,
            "logloss": 6.040182255434289,
            "mae": 0.3174001874353291,
            "precision": 0.6845238095238095,
            "recall": 0.7419354838709677
        },
        "train": {
            "accuracy": 0.7178924259055982,
            "auc_prc": 0.7531799663872261,
            "auditor_fn_violation": 0.044758563254384605,
            "auditor_fp_violation": 0.07310855733764782,
            "ave_precision_score": 0.6708407114561842,
            "fpr": 0.15806805708013172,
            "logloss": 6.0160064905935755,
            "mae": 0.2995536591443917,
            "precision": 0.7230769230769231,
            "recall": 0.7689161554192229
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6469298245614035,
            "auc_prc": 0.7044570281139396,
            "auditor_fn_violation": 0.05493774759479344,
            "auditor_fp_violation": 0.0871158601200989,
            "ave_precision_score": 0.5974434300473688,
            "fpr": 0.22807017543859648,
            "logloss": 7.246118119673023,
            "mae": 0.3639556940031786,
            "precision": 0.627906976744186,
            "recall": 0.7548387096774194
        },
        "train": {
            "accuracy": 0.6663007683863886,
            "auc_prc": 0.7281083177666086,
            "auditor_fn_violation": 0.04456102307852895,
            "auditor_fp_violation": 0.08772714739804705,
            "ave_precision_score": 0.6204389061008841,
            "fpr": 0.2305159165751921,
            "logloss": 7.201364356672328,
            "mae": 0.3470829730060016,
            "precision": 0.6528925619834711,
            "recall": 0.8077709611451943
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7623011287156976,
            "auditor_fn_violation": 0.02189209583097529,
            "auditor_fp_violation": 0.03111876447270302,
            "ave_precision_score": 0.6496339065450603,
            "fpr": 0.21820175438596492,
            "logloss": 6.87619206569767,
            "mae": 0.31982622295253127,
            "precision": 0.6580756013745704,
            "recall": 0.8236559139784946
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.7734378386706338,
            "auditor_fn_violation": 0.02261835013547216,
            "auditor_fp_violation": 0.028774171396465528,
            "ave_precision_score": 0.6663312265245558,
            "fpr": 0.2261251372118551,
            "logloss": 6.827340822953852,
            "mae": 0.32316296364145497,
            "precision": 0.6600660066006601,
            "recall": 0.8179959100204499
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7162187024462412,
            "auditor_fn_violation": 0.04000188643652142,
            "auditor_fp_violation": 0.08527856273794106,
            "ave_precision_score": 0.6063063654532378,
            "fpr": 0.22916666666666666,
            "logloss": 7.267781615432498,
            "mae": 0.3389564434937046,
            "precision": 0.6463620981387479,
            "recall": 0.821505376344086
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7377065426556417,
            "auditor_fn_violation": 0.035200312472641805,
            "auditor_fp_violation": 0.08939189786755869,
            "ave_precision_score": 0.6257226952339437,
            "fpr": 0.2239297475301866,
            "logloss": 7.293091699352666,
            "mae": 0.3188976786701634,
            "precision": 0.6709677419354839,
            "recall": 0.8507157464212679
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7209759730162368,
            "auditor_fn_violation": 0.04709252971137522,
            "auditor_fp_violation": 0.09182807410023941,
            "ave_precision_score": 0.6162279612298597,
            "fpr": 0.21271929824561403,
            "logloss": 6.638751087564046,
            "mae": 0.34338131483479056,
            "precision": 0.6541889483065954,
            "recall": 0.789247311827957
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7343848246593575,
            "auditor_fn_violation": 0.04632092646342476,
            "auditor_fp_violation": 0.09233122291529022,
            "ave_precision_score": 0.6270166134037252,
            "fpr": 0.21953896816684962,
            "logloss": 6.877555401743568,
            "mae": 0.3324033210351033,
            "precision": 0.6655518394648829,
            "recall": 0.8139059304703476
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7039473684210527,
            "auc_prc": 0.7587917567080668,
            "auditor_fn_violation": 0.014431239388794566,
            "auditor_fp_violation": 0.019064719965461753,
            "ave_precision_score": 0.7278827700386623,
            "fpr": 0.22587719298245615,
            "logloss": 2.6336404848822967,
            "mae": 0.2988785448639095,
            "precision": 0.6606260296540363,
            "recall": 0.8623655913978494
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.802262904700948,
            "auditor_fn_violation": 0.014319417974809142,
            "auditor_fp_violation": 0.027031385748695513,
            "ave_precision_score": 0.7786125230541157,
            "fpr": 0.1942919868276619,
            "logloss": 2.054967003055127,
            "mae": 0.2612462553784069,
            "precision": 0.7074380165289256,
            "recall": 0.8752556237218814
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.7174498599144435,
            "auditor_fn_violation": 0.04420392378796453,
            "auditor_fp_violation": 0.0874004081792849,
            "ave_precision_score": 0.6068126940118722,
            "fpr": 0.2225877192982456,
            "logloss": 7.204003795204589,
            "mae": 0.3411307294926986,
            "precision": 0.6463414634146342,
            "recall": 0.7978494623655914
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.7393951085506556,
            "auditor_fn_violation": 0.03621944019807892,
            "auditor_fp_violation": 0.08939189786755869,
            "ave_precision_score": 0.6271037205762305,
            "fpr": 0.2239297475301866,
            "logloss": 7.239382262706629,
            "mae": 0.31954032866591825,
            "precision": 0.6688311688311688,
            "recall": 0.8425357873210634
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.7132095688894555,
            "auditor_fn_violation": 0.02429494435012262,
            "auditor_fp_violation": 0.06860306526943757,
            "ave_precision_score": 0.603649449351089,
            "fpr": 0.28728070175438597,
            "logloss": 6.915115223315671,
            "mae": 0.3695170382403344,
            "precision": 0.6147058823529412,
            "recall": 0.8989247311827957
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7345036515083796,
            "auditor_fn_violation": 0.017996358975395026,
            "auditor_fp_violation": 0.075366375161923,
            "ave_precision_score": 0.6233664162675758,
            "fpr": 0.2678375411635565,
            "logloss": 7.108216201303852,
            "mae": 0.3418469932390654,
            "precision": 0.6479076479076479,
            "recall": 0.918200408997955
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7136156079605219,
            "auditor_fn_violation": 0.04665393322014714,
            "auditor_fp_violation": 0.08789836728286039,
            "ave_precision_score": 0.6044119976821516,
            "fpr": 0.2225877192982456,
            "logloss": 7.216953483265884,
            "mae": 0.34119536072401774,
            "precision": 0.6451048951048951,
            "recall": 0.7935483870967742
        },
        "train": {
            "accuracy": 0.6882546652030735,
            "auc_prc": 0.7361639318503654,
            "auditor_fn_violation": 0.03710612621470372,
            "auditor_fp_violation": 0.09013583323362172,
            "ave_precision_score": 0.625560910287105,
            "fpr": 0.21953896816684962,
            "logloss": 7.231012163555428,
            "mae": 0.3220654543225602,
            "precision": 0.6694214876033058,
            "recall": 0.8282208588957055
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.7561253306878597,
            "auditor_fn_violation": 0.011693548387096775,
            "auditor_fp_violation": 0.016955139526669032,
            "ave_precision_score": 0.7305835346143976,
            "fpr": 0.15460526315789475,
            "logloss": 2.3887813999288285,
            "mae": 0.27667950723289064,
            "precision": 0.7277992277992278,
            "recall": 0.810752688172043
        },
        "train": {
            "accuracy": 0.7672886937431395,
            "auc_prc": 0.7990929223054878,
            "auditor_fn_violation": 0.018054723118261472,
            "auditor_fp_violation": 0.015154431617773292,
            "ave_precision_score": 0.7767546954613885,
            "fpr": 0.12733260153677278,
            "logloss": 1.9630431764268819,
            "mae": 0.2522187245775696,
            "precision": 0.7721021611001965,
            "recall": 0.803680981595092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 29759,
        "test": {
            "accuracy": 0.625,
            "auc_prc": 0.7211549628099048,
            "auditor_fn_violation": 0.017579230333899264,
            "auditor_fp_violation": 0.05714264688567056,
            "ave_precision_score": 0.6082072728882583,
            "fpr": 0.33881578947368424,
            "logloss": 7.175609000788644,
            "mae": 0.3792538736448286,
            "precision": 0.582995951417004,
            "recall": 0.9290322580645162
        },
        "train": {
            "accuracy": 0.6717892425905598,
            "auc_prc": 0.7386906163049776,
            "auditor_fn_violation": 0.00982088942464179,
            "auditor_fp_violation": 0.05745470057902104,
            "ave_precision_score": 0.6255625974390642,
            "fpr": 0.3040614709110867,
            "logloss": 7.313533444657694,
            "mae": 0.3439578326308335,
            "precision": 0.6276881720430108,
            "recall": 0.9550102249488752
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.7160776174404979,
            "auditor_fn_violation": 0.04482880588568195,
            "auditor_fp_violation": 0.08871766945327525,
            "ave_precision_score": 0.6061648320623412,
            "fpr": 0.21929824561403508,
            "logloss": 7.276443172020183,
            "mae": 0.34206584375894256,
            "precision": 0.6466431095406361,
            "recall": 0.7870967741935484
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.736800924050663,
            "auditor_fn_violation": 0.039213969682072566,
            "auditor_fp_violation": 0.08829420302672446,
            "ave_precision_score": 0.624971876325273,
            "fpr": 0.2239297475301866,
            "logloss": 7.313907564654097,
            "mae": 0.32518591151438775,
            "precision": 0.6633663366336634,
            "recall": 0.8220858895705522
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5252192982456141,
            "auc_prc": 0.5448985761942258,
            "auditor_fn_violation": 0.010408413506885508,
            "auditor_fp_violation": 0.002995113622983634,
            "ave_precision_score": 0.5463833007565044,
            "fpr": 0.01864035087719298,
            "logloss": 14.33971720320729,
            "mae": 0.48134472942864365,
            "precision": 0.7424242424242424,
            "recall": 0.1053763440860215
        },
        "train": {
            "accuracy": 0.531284302963776,
            "auc_prc": 0.635585438406775,
            "auditor_fn_violation": 0.016615822519131098,
            "auditor_fp_violation": 0.0036286357890136876,
            "ave_precision_score": 0.6067677243649299,
            "fpr": 0.009879253567508232,
            "logloss": 14.614292851582164,
            "mae": 0.47607847356192723,
            "precision": 0.8875,
            "recall": 0.14519427402862986
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7177343624396311,
            "auditor_fn_violation": 0.03924731182795699,
            "auditor_fp_violation": 0.08344862435731387,
            "ave_precision_score": 0.6069129427359884,
            "fpr": 0.2324561403508772,
            "logloss": 7.29467995449456,
            "mae": 0.3391873576659349,
            "precision": 0.6436974789915967,
            "recall": 0.8236559139784946
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7388730959624011,
            "auditor_fn_violation": 0.03269963342828731,
            "auditor_fp_violation": 0.08755026766066143,
            "ave_precision_score": 0.6265819006190687,
            "fpr": 0.2283205268935236,
            "logloss": 7.295587268652645,
            "mae": 0.3177562353136926,
            "precision": 0.6682615629984051,
            "recall": 0.8568507157464212
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.7174284685078626,
            "auditor_fn_violation": 0.02657989058668176,
            "auditor_fp_violation": 0.07553524471133091,
            "ave_precision_score": 0.6066081257523572,
            "fpr": 0.2741228070175439,
            "logloss": 7.309941936128698,
            "mae": 0.3429742418903676,
            "precision": 0.622356495468278,
            "recall": 0.886021505376344
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.7399553823912396,
            "auditor_fn_violation": 0.02454885639951603,
            "auditor_fp_violation": 0.08089126578261481,
            "ave_precision_score": 0.6275096368237995,
            "fpr": 0.25466520307354557,
            "logloss": 7.269187367016977,
            "mae": 0.31213180673110036,
            "precision": 0.6562962962962963,
            "recall": 0.9059304703476483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 29759,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.7150795394303765,
            "auditor_fn_violation": 0.04251320505564988,
            "auditor_fp_violation": 0.083887711448644,
            "ave_precision_score": 0.6128607147326864,
            "fpr": 0.23135964912280702,
            "logloss": 6.605077658525219,
            "mae": 0.34322902481209544,
            "precision": 0.6393162393162393,
            "recall": 0.8043010752688172
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.7313657277448089,
            "auditor_fn_violation": 0.03517337517593422,
            "auditor_fp_violation": 0.08680633229459842,
            "ave_precision_score": 0.6271486040191087,
            "fpr": 0.2327113062568606,
            "logloss": 6.858001899196401,
            "mae": 0.32820160341164206,
            "precision": 0.6618819776714514,
            "recall": 0.8486707566462167
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7232224685070537,
            "auditor_fn_violation": 0.027707036408224864,
            "auditor_fp_violation": 0.08204796106597591,
            "ave_precision_score": 0.6170257712921602,
            "fpr": 0.26206140350877194,
            "logloss": 6.827295658648029,
            "mae": 0.3342599103665519,
            "precision": 0.6317411402157165,
            "recall": 0.8817204301075269
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7390331208809691,
            "auditor_fn_violation": 0.02454885639951603,
            "auditor_fp_violation": 0.08365371109296071,
            "ave_precision_score": 0.63013170909202,
            "fpr": 0.24807903402854006,
            "logloss": 7.04515681084301,
            "mae": 0.30828653515529664,
            "precision": 0.6621823617339312,
            "recall": 0.9059304703476483
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.7157522688151994,
            "auditor_fn_violation": 0.04797443878513488,
            "auditor_fp_violation": 0.08309784528435182,
            "ave_precision_score": 0.6051127134870533,
            "fpr": 0.23135964912280702,
            "logloss": 6.926248177758811,
            "mae": 0.35988744388420235,
            "precision": 0.6336805555555556,
            "recall": 0.7849462365591398
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7342267271200068,
            "auditor_fn_violation": 0.03551682570895598,
            "auditor_fp_violation": 0.08276931240603265,
            "ave_precision_score": 0.621550313991997,
            "fpr": 0.23710208562019758,
            "logloss": 7.1982363146361985,
            "mae": 0.34294705472269527,
            "precision": 0.6555023923444976,
            "recall": 0.8404907975460123
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.7428477673304681,
            "auditor_fn_violation": 0.0267402376910017,
            "auditor_fp_violation": 0.0721451980062012,
            "ave_precision_score": 0.622865240601372,
            "fpr": 0.2642543859649123,
            "logloss": 7.219588927777667,
            "mae": 0.34824293737530526,
            "precision": 0.6298003072196621,
            "recall": 0.8817204301075269
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.755874405848941,
            "auditor_fn_violation": 0.020070530821879373,
            "auditor_fp_violation": 0.07465885621238054,
            "ave_precision_score": 0.6376268246045383,
            "fpr": 0.25905598243688255,
            "logloss": 7.304423218986638,
            "mae": 0.3306439702036667,
            "precision": 0.6534508076358296,
            "recall": 0.9100204498977505
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.7285150192798353,
            "auditor_fn_violation": 0.040966327108092816,
            "auditor_fp_violation": 0.09160485105380901,
            "ave_precision_score": 0.6226367614574223,
            "fpr": 0.21052631578947367,
            "logloss": 6.802406590762933,
            "mae": 0.32483419612059405,
            "precision": 0.6619718309859155,
            "recall": 0.8086021505376344
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.741957896006,
            "auditor_fn_violation": 0.035575189851822425,
            "auditor_fp_violation": 0.09134017615140906,
            "ave_precision_score": 0.6335433837178813,
            "fpr": 0.21405049396267836,
            "logloss": 7.036149717670258,
            "mae": 0.314240234431169,
            "precision": 0.6792763157894737,
            "recall": 0.8445807770961146
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6469298245614035,
            "auc_prc": 0.7105159545472836,
            "auditor_fn_violation": 0.05358658743633277,
            "auditor_fp_violation": 0.08608314690529456,
            "ave_precision_score": 0.6014871195385608,
            "fpr": 0.22587719298245615,
            "logloss": 7.173136596907875,
            "mae": 0.35721594540528323,
            "precision": 0.6288288288288288,
            "recall": 0.7505376344086021
        },
        "train": {
            "accuracy": 0.6706915477497256,
            "auc_prc": 0.7328634428070062,
            "auditor_fn_violation": 0.04450041416093689,
            "auditor_fp_violation": 0.08783379547500014,
            "ave_precision_score": 0.6240289471629599,
            "fpr": 0.22502744237102085,
            "logloss": 7.168995580198747,
            "mae": 0.33765992154496577,
            "precision": 0.657762938230384,
            "recall": 0.8057259713701431
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7226829594036106,
            "auditor_fn_violation": 0.044956140350877194,
            "auditor_fp_violation": 0.09270134228187919,
            "ave_precision_score": 0.619061701111599,
            "fpr": 0.21052631578947367,
            "logloss": 6.659363465217381,
            "mae": 0.3285146694113158,
            "precision": 0.6595744680851063,
            "recall": 0.8
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.7382896562787622,
            "auditor_fn_violation": 0.041669753231914415,
            "auditor_fp_violation": 0.0944563809365262,
            "ave_precision_score": 0.6320857751304871,
            "fpr": 0.21185510428100987,
            "logloss": 6.9296183252693,
            "mae": 0.31650605279996413,
            "precision": 0.67779632721202,
            "recall": 0.8302658486707567
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.7012014579593817,
            "auditor_fn_violation": 0.04948358800226373,
            "auditor_fp_violation": 0.07944778052513835,
            "ave_precision_score": 0.585326464263718,
            "fpr": 0.2412280701754386,
            "logloss": 7.919741723887486,
            "mae": 0.3695314331833298,
            "precision": 0.6232876712328768,
            "recall": 0.7827956989247312
        },
        "train": {
            "accuracy": 0.6465422612513722,
            "auc_prc": 0.7134750938953281,
            "auditor_fn_violation": 0.0469674215844069,
            "auditor_fp_violation": 0.0764744746931917,
            "ave_precision_score": 0.594503402432145,
            "fpr": 0.2502744237102086,
            "logloss": 8.284147909618087,
            "mae": 0.35975398501285283,
            "precision": 0.6340288924558587,
            "recall": 0.8077709611451943
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7162034244191801,
            "auditor_fn_violation": 0.04099462365591398,
            "auditor_fp_violation": 0.08571519682876094,
            "ave_precision_score": 0.6062910948140124,
            "fpr": 0.22807017543859648,
            "logloss": 7.269751938880021,
            "mae": 0.33940314404289895,
            "precision": 0.645655877342419,
            "recall": 0.8150537634408602
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7376804086213191,
            "auditor_fn_violation": 0.0352474527418801,
            "auditor_fp_violation": 0.08893149031583437,
            "ave_precision_score": 0.6256965724736326,
            "fpr": 0.22502744237102085,
            "logloss": 7.295575762268597,
            "mae": 0.3194613783184866,
            "precision": 0.6688206785137318,
            "recall": 0.8466257668711656
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.7197401921532931,
            "auditor_fn_violation": 0.045863987926806266,
            "auditor_fp_violation": 0.08257045017465366,
            "ave_precision_score": 0.6056523378653026,
            "fpr": 0.23464912280701755,
            "logloss": 7.476052233293987,
            "mae": 0.35823001397652104,
            "precision": 0.6303972366148531,
            "recall": 0.7849462365591398
        },
        "train": {
            "accuracy": 0.681668496158068,
            "auc_prc": 0.7385553211429023,
            "auditor_fn_violation": 0.03292635567557618,
            "auditor_fp_violation": 0.08634592474287411,
            "ave_precision_score": 0.6249678109474982,
            "fpr": 0.23380900109769484,
            "logloss": 7.527460933380406,
            "mae": 0.3370147008681572,
            "precision": 0.6592,
            "recall": 0.8425357873210634
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7110742937834794,
            "auditor_fn_violation": 0.04329371816638371,
            "auditor_fp_violation": 0.08175605400525923,
            "ave_precision_score": 0.6170314956520516,
            "fpr": 0.22149122807017543,
            "logloss": 6.333262815560301,
            "mae": 0.3336353693666506,
            "precision": 0.648695652173913,
            "recall": 0.8021505376344086
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.73028406725186,
            "auditor_fn_violation": 0.03362448061524786,
            "auditor_fp_violation": 0.08585170194723782,
            "ave_precision_score": 0.6359644290652255,
            "fpr": 0.21405049396267836,
            "logloss": 6.3719927452618395,
            "mae": 0.3136147124501987,
            "precision": 0.6792763157894737,
            "recall": 0.8445807770961146
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.7159467723574392,
            "auditor_fn_violation": 0.0405631013016412,
            "auditor_fp_violation": 0.08344862435731387,
            "ave_precision_score": 0.6062095074206229,
            "fpr": 0.2324561403508772,
            "logloss": 7.158645370532616,
            "mae": 0.3384044269130203,
            "precision": 0.642495784148398,
            "recall": 0.8193548387096774
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7382366842821126,
            "auditor_fn_violation": 0.0327961587414895,
            "auditor_fp_violation": 0.08801067521238576,
            "ave_precision_score": 0.626252652573809,
            "fpr": 0.22722283205268934,
            "logloss": 7.220785995611481,
            "mae": 0.3170904170443799,
            "precision": 0.6703821656050956,
            "recall": 0.8609406952965235
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 29759,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.7408502520935677,
            "auditor_fn_violation": 0.03272731560083003,
            "auditor_fp_violation": 0.02391675497468504,
            "ave_precision_score": 0.7005557130581705,
            "fpr": 0.10526315789473684,
            "logloss": 3.171278497597138,
            "mae": 0.29055698734790963,
            "precision": 0.7669902912621359,
            "recall": 0.6795698924731183
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7765076864002505,
            "auditor_fn_violation": 0.03379283871967029,
            "auditor_fp_violation": 0.019503592219372496,
            "ave_precision_score": 0.744527718647609,
            "fpr": 0.09549945115257959,
            "logloss": 2.6019977152074483,
            "mae": 0.27836086916943775,
            "precision": 0.7928571428571428,
            "recall": 0.6809815950920245
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.7389209279335598,
            "auditor_fn_violation": 0.02578758724768912,
            "auditor_fp_violation": 0.023298598846108562,
            "ave_precision_score": 0.7130029255662018,
            "fpr": 0.1513157894736842,
            "logloss": 2.942583811345938,
            "mae": 0.2931056912296832,
            "precision": 0.7148760330578512,
            "recall": 0.7440860215053764
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.7865054046479919,
            "auditor_fn_violation": 0.034762581401143494,
            "auditor_fp_violation": 0.018894917828957296,
            "ave_precision_score": 0.7642386278407809,
            "fpr": 0.13172338090010977,
            "logloss": 2.4986727287474286,
            "mae": 0.26558589612130984,
            "precision": 0.7556008146639511,
            "recall": 0.7586912065439673
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7180317550942499,
            "auditor_fn_violation": 0.041482739105829086,
            "auditor_fp_violation": 0.08615183091958083,
            "ave_precision_score": 0.6072181238803984,
            "fpr": 0.22697368421052633,
            "logloss": 7.256549526838919,
            "mae": 0.3380013542265416,
            "precision": 0.6467576791808873,
            "recall": 0.8150537634408602
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7390520923412204,
            "auditor_fn_violation": 0.0352339840935263,
            "auditor_fp_violation": 0.08939189786755869,
            "ave_precision_score": 0.6267595065586257,
            "fpr": 0.2239297475301866,
            "logloss": 7.274579949547581,
            "mae": 0.31963347420742505,
            "precision": 0.6693679092382496,
            "recall": 0.8445807770961146
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6578947368421053,
            "auc_prc": 0.7043799370353873,
            "auditor_fn_violation": 0.049542539143557815,
            "auditor_fp_violation": 0.08581086384865969,
            "ave_precision_score": 0.5973663548714602,
            "fpr": 0.2324561403508772,
            "logloss": 7.229967375005817,
            "mae": 0.3604921468336943,
            "precision": 0.6325823223570191,
            "recall": 0.7849462365591398
        },
        "train": {
            "accuracy": 0.672886937431394,
            "auc_prc": 0.72874913252886,
            "auditor_fn_violation": 0.03986719912723159,
            "auditor_fp_violation": 0.08588551719114977,
            "ave_precision_score": 0.6214571147622492,
            "fpr": 0.2349066959385291,
            "logloss": 7.157817855467803,
            "mae": 0.3410274591252174,
            "precision": 0.654281098546042,
            "recall": 0.8282208588957055
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 29759,
        "test": {
            "accuracy": 0.694078947368421,
            "auc_prc": 0.6820535458307417,
            "auditor_fn_violation": 0.007394831163931334,
            "auditor_fp_violation": 0.02336728286039484,
            "ave_precision_score": 0.675264493232205,
            "fpr": 0.20833333333333334,
            "logloss": 2.1208078663033896,
            "mae": 0.3115771191853485,
            "precision": 0.6643109540636042,
            "recall": 0.8086021505376344
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7555359156951897,
            "auditor_fn_violation": 0.003957537841289942,
            "auditor_fp_violation": 0.016335363982083125,
            "ave_precision_score": 0.7490709299831548,
            "fpr": 0.1734357848518112,
            "logloss": 1.6101848646551526,
            "mae": 0.27379429229218905,
            "precision": 0.7183600713012478,
            "recall": 0.8241308793456033
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7081775494348816,
            "auditor_fn_violation": 0.029003961516694966,
            "auditor_fp_violation": 0.07553769771184113,
            "ave_precision_score": 0.6006074292082142,
            "fpr": 0.26864035087719296,
            "logloss": 7.08427149643468,
            "mae": 0.34556668521279454,
            "precision": 0.6236559139784946,
            "recall": 0.8731182795698925
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7312918834771254,
            "auditor_fn_violation": 0.025130253053454826,
            "auditor_fp_violation": 0.08457452619640934,
            "ave_precision_score": 0.6234002689233505,
            "fpr": 0.24588364434687157,
            "logloss": 7.05417843303397,
            "mae": 0.31237130275867026,
            "precision": 0.6636636636636637,
            "recall": 0.9038854805725971
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.7127748585672274,
            "auditor_fn_violation": 0.023309281267685344,
            "auditor_fp_violation": 0.06981239452097807,
            "ave_precision_score": 0.6037472434233953,
            "fpr": 0.28728070175438597,
            "logloss": 6.881458935353669,
            "mae": 0.36381832220049865,
            "precision": 0.6147058823529412,
            "recall": 0.8989247311827957
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7332693283339811,
            "auditor_fn_violation": 0.01421840311215568,
            "auditor_fp_violation": 0.07334786521764013,
            "ave_precision_score": 0.6229804660986645,
            "fpr": 0.270032930845225,
            "logloss": 7.071708278552353,
            "mae": 0.3359529742573445,
            "precision": 0.6515580736543909,
            "recall": 0.9406952965235174
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6480263157894737,
            "auc_prc": 0.6929391218164099,
            "auditor_fn_violation": 0.03731843048481419,
            "auditor_fp_violation": 0.07772822716747126,
            "ave_precision_score": 0.5922310682383223,
            "fpr": 0.2741228070175439,
            "logloss": 6.874318916602085,
            "mae": 0.35988486750784854,
            "precision": 0.6118012422360248,
            "recall": 0.8473118279569892
        },
        "train": {
            "accuracy": 0.6761800219538968,
            "auc_prc": 0.7200345467251211,
            "auditor_fn_violation": 0.03001937240588221,
            "auditor_fp_violation": 0.08096149744304733,
            "ave_precision_score": 0.6190934483941963,
            "fpr": 0.2623490669593853,
            "logloss": 6.797301517484125,
            "mae": 0.3338182926059613,
            "precision": 0.6443452380952381,
            "recall": 0.885480572597137
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5361842105263158,
            "auc_prc": 0.7430469887372325,
            "auditor_fn_violation": 0.003001792114695341,
            "auditor_fp_violation": 0.0060540052592331015,
            "ave_precision_score": 0.5191982965140847,
            "fpr": 0.44956140350877194,
            "logloss": 15.488381513047898,
            "mae": 0.4703296940077276,
            "precision": 0.5243619489559165,
            "recall": 0.9720430107526882
        },
        "train": {
            "accuracy": 0.544456641053787,
            "auc_prc": 0.7489997286396874,
            "auditor_fn_violation": 0.0037644872148855507,
            "auditor_fp_violation": 0.0015476977021241182,
            "ave_precision_score": 0.532100983822231,
            "fpr": 0.442371020856202,
            "logloss": 15.381137253813765,
            "mae": 0.4604128341965671,
            "precision": 0.5420454545454545,
            "recall": 0.9754601226993865
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.7171229861384671,
            "auditor_fn_violation": 0.0319609507640068,
            "auditor_fp_violation": 0.07794899721339142,
            "ave_precision_score": 0.6064769707438293,
            "fpr": 0.25548245614035087,
            "logloss": 7.27298191953114,
            "mae": 0.33907937077548533,
            "precision": 0.6307448494453248,
            "recall": 0.8559139784946237
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.739224993939836,
            "auditor_fn_violation": 0.02431764460277589,
            "auditor_fp_violation": 0.08312307188080388,
            "ave_precision_score": 0.6269336124649628,
            "fpr": 0.2414928649835346,
            "logloss": 7.2624987472510645,
            "mae": 0.3126633039611039,
            "precision": 0.6666666666666666,
            "recall": 0.8997955010224948
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6666666666666666,
            "auc_prc": 0.7131081287118208,
            "auditor_fn_violation": 0.048917657045840404,
            "auditor_fp_violation": 0.08674791004356529,
            "ave_precision_score": 0.6046382109951414,
            "fpr": 0.2236842105263158,
            "logloss": 6.854816107876211,
            "mae": 0.3531557516345154,
            "precision": 0.6414762741652021,
            "recall": 0.7849462365591398
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7344090144214547,
            "auditor_fn_violation": 0.03696246063226325,
            "auditor_fp_violation": 0.08489447042726862,
            "ave_precision_score": 0.6236545040990671,
            "fpr": 0.22941822173435786,
            "logloss": 7.080091273360185,
            "mae": 0.33385523273873485,
            "precision": 0.6639871382636656,
            "recall": 0.8445807770961146
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.696202030890815,
            "auditor_fn_violation": 0.05715195246179966,
            "auditor_fp_violation": 0.09073158287216924,
            "ave_precision_score": 0.5954928349740285,
            "fpr": 0.21271929824561403,
            "logloss": 6.9126448153152715,
            "mae": 0.35700517056064895,
            "precision": 0.6427255985267035,
            "recall": 0.7505376344086021
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7233145801072016,
            "auditor_fn_violation": 0.05002929431016952,
            "auditor_fp_violation": 0.09704194650948648,
            "ave_precision_score": 0.6220012301303484,
            "fpr": 0.2030735455543359,
            "logloss": 6.864899426079243,
            "mae": 0.3361310315932411,
            "precision": 0.6742957746478874,
            "recall": 0.7832310838445807
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7193273524746423,
            "auditor_fn_violation": 0.038315883795510285,
            "auditor_fp_violation": 0.08473154362416109,
            "ave_precision_score": 0.608161316879001,
            "fpr": 0.23355263157894737,
            "logloss": 7.301048067741875,
            "mae": 0.3373759396400157,
            "precision": 0.6444073455759599,
            "recall": 0.8301075268817204
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7397296680642309,
            "auditor_fn_violation": 0.02973877556517816,
            "auditor_fp_violation": 0.08680633229459842,
            "ave_precision_score": 0.6272863143500919,
            "fpr": 0.2327113062568606,
            "logloss": 7.291852670672322,
            "mae": 0.31515069193614403,
            "precision": 0.6682316118935837,
            "recall": 0.8732106339468303
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.7299671445405331,
            "auditor_fn_violation": 0.02614129409545369,
            "auditor_fp_violation": 0.07576827975980222,
            "ave_precision_score": 0.6234513024802468,
            "fpr": 0.2894736842105263,
            "logloss": 6.730031942919115,
            "mae": 0.33832439153093996,
            "precision": 0.6134699853587116,
            "recall": 0.9010752688172043
        },
        "train": {
            "accuracy": 0.6871569703622393,
            "auc_prc": 0.7432325634791827,
            "auditor_fn_violation": 0.02037806495929101,
            "auditor_fp_violation": 0.07791552431836272,
            "ave_precision_score": 0.6345185460350047,
            "fpr": 0.2722283205268935,
            "logloss": 6.965012384497245,
            "mae": 0.31218979699657307,
            "precision": 0.6457142857142857,
            "recall": 0.9243353783231084
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5383771929824561,
            "auc_prc": 0.7482584224471717,
            "auditor_fn_violation": 0.0010257498585172608,
            "auditor_fp_violation": 0.0031938066643117935,
            "ave_precision_score": 0.5232235542513588,
            "fpr": 0.4451754385964912,
            "logloss": 15.253382302419606,
            "mae": 0.4660379105424798,
            "precision": 0.5257009345794392,
            "recall": 0.967741935483871
        },
        "train": {
            "accuracy": 0.54006586169045,
            "auc_prc": 0.7551505358915253,
            "auditor_fn_violation": 0.0025074133685314016,
            "auditor_fp_violation": 0.0007491377112802639,
            "ave_precision_score": 0.5348569152503747,
            "fpr": 0.4456641053787047,
            "logloss": 15.332079527191587,
            "mae": 0.46060452805416263,
            "precision": 0.5396825396825397,
            "recall": 0.9734151329243353
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.712009416188356,
            "auditor_fn_violation": 0.03752829654782117,
            "auditor_fp_violation": 0.079433062522077,
            "ave_precision_score": 0.6026289329878202,
            "fpr": 0.24780701754385964,
            "logloss": 6.882311001629843,
            "mae": 0.3669127563919398,
            "precision": 0.6331168831168831,
            "recall": 0.8387096774193549
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.7303394724863376,
            "auditor_fn_violation": 0.02912595206508051,
            "auditor_fp_violation": 0.07943981146700933,
            "ave_precision_score": 0.6195148918093596,
            "fpr": 0.2502744237102086,
            "logloss": 7.126519487941039,
            "mae": 0.3489622579028182,
            "precision": 0.6529680365296804,
            "recall": 0.8773006134969326
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.7206461457605927,
            "auditor_fn_violation": 0.024441143180531977,
            "auditor_fp_violation": 0.07333490325365989,
            "ave_precision_score": 0.6080209173663504,
            "fpr": 0.2905701754385965,
            "logloss": 7.266746837722269,
            "mae": 0.34802522385552115,
            "precision": 0.6108663729809104,
            "recall": 0.8946236559139785
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7409594438499956,
            "auditor_fn_violation": 0.02014236361309961,
            "auditor_fp_violation": 0.07607389411146545,
            "ave_precision_score": 0.6283632105090272,
            "fpr": 0.27661909989023054,
            "logloss": 7.205210449207558,
            "mae": 0.3150144237251189,
            "precision": 0.6405135520684736,
            "recall": 0.918200408997955
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 29759,
        "test": {
            "accuracy": 0.7346491228070176,
            "auc_prc": 0.7545826086008784,
            "auditor_fn_violation": 0.02096066779852858,
            "auditor_fp_violation": 0.01305732171592292,
            "ave_precision_score": 0.7294090024367891,
            "fpr": 0.07785087719298246,
            "logloss": 2.4571371176547068,
            "mae": 0.2866376433860287,
            "precision": 0.8054794520547945,
            "recall": 0.632258064516129
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.7940264221369409,
            "auditor_fn_violation": 0.030870142026896894,
            "auditor_fp_violation": 0.012730138746546944,
            "ave_precision_score": 0.7734477514215996,
            "fpr": 0.06805708013172337,
            "logloss": 2.051708263395552,
            "mae": 0.2771645779735356,
            "precision": 0.837696335078534,
            "recall": 0.65439672801636
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6721491228070176,
            "auc_prc": 0.714990115776213,
            "auditor_fn_violation": 0.04591114883984154,
            "auditor_fp_violation": 0.08651732799560423,
            "ave_precision_score": 0.6052658937237555,
            "fpr": 0.22916666666666666,
            "logloss": 6.920874139469391,
            "mae": 0.3506187324796684,
            "precision": 0.6421232876712328,
            "recall": 0.8064516129032258
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.7341055591192152,
            "auditor_fn_violation": 0.0314246013841281,
            "auditor_fp_violation": 0.08680633229459842,
            "ave_precision_score": 0.6228167600660413,
            "fpr": 0.2327113062568606,
            "logloss": 7.13810927001404,
            "mae": 0.33336582046201674,
            "precision": 0.6682316118935837,
            "recall": 0.8732106339468303
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 29759,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.7118588684743257,
            "auditor_fn_violation": 0.03190435766836446,
            "auditor_fp_violation": 0.07697515601083246,
            "ave_precision_score": 0.6030081720156859,
            "fpr": 0.2565789473684211,
            "logloss": 7.075601082603294,
            "mae": 0.3421673466669403,
            "precision": 0.6291600633914421,
            "recall": 0.853763440860215
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7331103180662855,
            "auditor_fn_violation": 0.027983361729733616,
            "auditor_fp_violation": 0.0838670072468669,
            "ave_precision_score": 0.624746007144874,
            "fpr": 0.23710208562019758,
            "logloss": 7.059246632671781,
            "mae": 0.31449957596944916,
            "precision": 0.6676923076923077,
            "recall": 0.8875255623721882
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.7165460745600943,
            "auditor_fn_violation": 0.032118939822674976,
            "auditor_fp_violation": 0.07765218415165431,
            "ave_precision_score": 0.6066366338277458,
            "fpr": 0.2565789473684211,
            "logloss": 7.10855111408354,
            "mae": 0.33972700535173844,
            "precision": 0.629746835443038,
            "recall": 0.8559139784946237
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.7364212357968809,
            "auditor_fn_violation": 0.022485908426659845,
            "auditor_fp_violation": 0.0835834794325282,
            "ave_precision_score": 0.6249032542647774,
            "fpr": 0.24039517014270034,
            "logloss": 7.1869653192911205,
            "mae": 0.3114845695027664,
            "precision": 0.6701807228915663,
            "recall": 0.9100204498977505
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.7258676790724751,
            "auditor_fn_violation": 0.04202037351443124,
            "auditor_fp_violation": 0.08789836728286039,
            "ave_precision_score": 0.6144218818133891,
            "fpr": 0.2225877192982456,
            "logloss": 7.325766482577191,
            "mae": 0.3316511106195447,
            "precision": 0.6518010291595198,
            "recall": 0.8172043010752689
        },
        "train": {
            "accuracy": 0.7047200878155873,
            "auc_prc": 0.7431552156228081,
            "auditor_fn_violation": 0.035144193104501,
            "auditor_fp_violation": 0.09077312052273165,
            "ave_precision_score": 0.6302508907318616,
            "fpr": 0.22063666300768386,
            "logloss": 7.25978694146973,
            "mae": 0.31061634849472347,
            "precision": 0.6768488745980707,
            "recall": 0.8609406952965235
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.716220350870767,
            "auditor_fn_violation": 0.04000188643652142,
            "auditor_fp_violation": 0.08527856273794106,
            "ave_precision_score": 0.6063065269514144,
            "fpr": 0.22916666666666666,
            "logloss": 7.268287107926964,
            "mae": 0.33893899860723997,
            "precision": 0.6463620981387479,
            "recall": 0.821505376344086
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.7377043392470214,
            "auditor_fn_violation": 0.035200312472641805,
            "auditor_fp_violation": 0.08939189786755869,
            "ave_precision_score": 0.6257204925553134,
            "fpr": 0.2239297475301866,
            "logloss": 7.293322599577604,
            "mae": 0.31887209973786496,
            "precision": 0.6709677419354839,
            "recall": 0.8507157464212679
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6776315789473685,
            "auc_prc": 0.7186612005448434,
            "auditor_fn_violation": 0.041482739105829086,
            "auditor_fp_violation": 0.08571519682876094,
            "ave_precision_score": 0.60766802023285,
            "fpr": 0.22807017543859648,
            "logloss": 7.308233807730782,
            "mae": 0.33805709039499826,
            "precision": 0.645655877342419,
            "recall": 0.8150537634408602
        },
        "train": {
            "accuracy": 0.6926454445664105,
            "auc_prc": 0.7395200883815815,
            "auditor_fn_violation": 0.035007261846237425,
            "auditor_fp_violation": 0.08847108276411007,
            "ave_precision_score": 0.6270795125923597,
            "fpr": 0.2261251372118551,
            "logloss": 7.305922100763468,
            "mae": 0.3183916534758685,
            "precision": 0.6682769726247987,
            "recall": 0.8486707566462167
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.7337173419663766,
            "auditor_fn_violation": 0.021717600452744762,
            "auditor_fp_violation": 0.04747782487538758,
            "ave_precision_score": 0.6647906980833297,
            "fpr": 0.22587719298245615,
            "logloss": 4.4707067875644,
            "mae": 0.3145320474694485,
            "precision": 0.656093489148581,
            "recall": 0.8451612903225807
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7431922628770283,
            "auditor_fn_violation": 0.014579811842982501,
            "auditor_fp_violation": 0.05143558716269295,
            "ave_precision_score": 0.6732487018991147,
            "fpr": 0.20965971459934138,
            "logloss": 4.612587192531487,
            "mae": 0.2932818898174568,
            "precision": 0.6873977086743044,
            "recall": 0.8588957055214724
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6622807017543859,
            "auc_prc": 0.7261179656464848,
            "auditor_fn_violation": 0.0267402376910017,
            "auditor_fp_violation": 0.07312149220927038,
            "ave_precision_score": 0.6123150443421781,
            "fpr": 0.2774122807017544,
            "logloss": 7.194362048987478,
            "mae": 0.3428731409962091,
            "precision": 0.6184012066365008,
            "recall": 0.8817204301075269
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.745824141864418,
            "auditor_fn_violation": 0.019493623717391842,
            "auditor_fp_violation": 0.07887275583833193,
            "ave_precision_score": 0.6316407942410361,
            "fpr": 0.25686059275521406,
            "logloss": 7.228551441587528,
            "mae": 0.3099244405401084,
            "precision": 0.6583941605839416,
            "recall": 0.9222903885480572
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5416666666666666,
            "auc_prc": 0.6021120005238174,
            "auditor_fn_violation": 0.0972693831352575,
            "auditor_fp_violation": 0.05498645943718357,
            "ave_precision_score": 0.5397551945447131,
            "fpr": 0.20065789473684212,
            "logloss": 12.77866151773909,
            "mae": 0.477681110653385,
            "precision": 0.5569007263922519,
            "recall": 0.4946236559139785
        },
        "train": {
            "accuracy": 0.5543358946212953,
            "auc_prc": 0.6366454330016907,
            "auditor_fn_violation": 0.11110961459462736,
            "auditor_fp_violation": 0.043707503342506795,
            "ave_precision_score": 0.5940467421385366,
            "fpr": 0.15916575192096596,
            "logloss": 12.854007469659926,
            "mae": 0.46460934012103927,
            "precision": 0.6112600536193029,
            "recall": 0.4662576687116564
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5877192982456141,
            "auc_prc": 0.6266813449276454,
            "auditor_fn_violation": 0.021198830409356727,
            "auditor_fp_violation": 0.012502943600612272,
            "ave_precision_score": 0.6172675949020088,
            "fpr": 0.07236842105263158,
            "logloss": 8.82573117912529,
            "mae": 0.4262061154938547,
            "precision": 0.7013574660633484,
            "recall": 0.3333333333333333
        },
        "train": {
            "accuracy": 0.5916575192096597,
            "auc_prc": 0.7043588376748156,
            "auditor_fn_violation": 0.020721515492312777,
            "auditor_fp_violation": 0.009221156897529406,
            "ave_precision_score": 0.6910311926991717,
            "fpr": 0.05598243688254665,
            "logloss": 8.000013219747165,
            "mae": 0.4125721814357939,
            "precision": 0.7671232876712328,
            "recall": 0.34355828220858897
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.7149750133270332,
            "auditor_fn_violation": 0.04506225240520657,
            "auditor_fp_violation": 0.09095480591859963,
            "ave_precision_score": 0.6054163565321071,
            "fpr": 0.2149122807017544,
            "logloss": 7.250107888842246,
            "mae": 0.3427160898990246,
            "precision": 0.6512455516014235,
            "recall": 0.7870967741935484
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.736319022637331,
            "auditor_fn_violation": 0.040767353792210186,
            "auditor_fp_violation": 0.09013583323362172,
            "ave_precision_score": 0.6251798108982967,
            "fpr": 0.21953896816684962,
            "logloss": 7.275898905002162,
            "mae": 0.3245902737028184,
            "precision": 0.6666666666666666,
            "recall": 0.8179959100204499
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7162140271449632,
            "auditor_fn_violation": 0.04000188643652142,
            "auditor_fp_violation": 0.08527856273794106,
            "ave_precision_score": 0.6063002053155635,
            "fpr": 0.22916666666666666,
            "logloss": 7.267972774637908,
            "mae": 0.33893763142527017,
            "precision": 0.6463620981387479,
            "recall": 0.821505376344086
        },
        "train": {
            "accuracy": 0.6959385290889133,
            "auc_prc": 0.737705903830236,
            "auditor_fn_violation": 0.035200312472641805,
            "auditor_fp_violation": 0.08939189786755869,
            "ave_precision_score": 0.6257220568039543,
            "fpr": 0.2239297475301866,
            "logloss": 7.293092782022676,
            "mae": 0.3188552020529995,
            "precision": 0.6709677419354839,
            "recall": 0.8507157464212679
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 29759,
        "test": {
            "accuracy": 0.5087719298245614,
            "auc_prc": 0.5140801054730906,
            "auditor_fn_violation": 0.005147613657800427,
            "auditor_fp_violation": 0.0039149888143176735,
            "ave_precision_score": 0.5277920279638192,
            "fpr": 0.01644736842105263,
            "logloss": 16.042102503631494,
            "mae": 0.49582451214804973,
            "precision": 0.6808510638297872,
            "recall": 0.06881720430107527
        },
        "train": {
            "accuracy": 0.4983534577387486,
            "auc_prc": 0.6081154016301364,
            "auditor_fn_violation": 0.012678487650371866,
            "auditor_fp_violation": 0.0031474188564204744,
            "ave_precision_score": 0.5740766469797584,
            "fpr": 0.008781558726673985,
            "logloss": 16.455380692652916,
            "mae": 0.5050080847003932,
            "precision": 0.8333333333333334,
            "recall": 0.081799591002045
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 29759,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.7075434218267136,
            "auditor_fn_violation": 0.04034380305602717,
            "auditor_fp_violation": 0.08135866792260293,
            "ave_precision_score": 0.6001597631010346,
            "fpr": 0.24451754385964913,
            "logloss": 7.080696037739992,
            "mae": 0.3439769391742583,
            "precision": 0.6320132013201321,
            "recall": 0.8236559139784946
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.731067990281142,
            "auditor_fn_violation": 0.029428996653040884,
            "auditor_fp_violation": 0.08818755494977136,
            "ave_precision_score": 0.6231764569090078,
            "fpr": 0.22941822173435786,
            "logloss": 7.070000156163208,
            "mae": 0.31653333794550853,
            "precision": 0.6718995290423861,
            "recall": 0.8752556237218814
        }
    }
]
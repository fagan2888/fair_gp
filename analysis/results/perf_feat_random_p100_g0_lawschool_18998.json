[
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(0)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8397548129665542,
            "auditor_fn_violation": 0.015394159741458913,
            "auditor_fp_violation": 0.022427573868882736,
            "ave_precision_score": 0.8400012224864646,
            "fpr": 0.13706140350877194,
            "logloss": 0.6996254118319924,
            "mae": 0.26587202711508484,
            "precision": 0.7368421052631579,
            "recall": 0.7675438596491229
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8395118625643112,
            "auditor_fn_violation": 0.016359620700144153,
            "auditor_fp_violation": 0.028085572356163446,
            "ave_precision_score": 0.8399222935661006,
            "fpr": 0.1251372118551043,
            "logloss": 0.7577552884840226,
            "mae": 0.27115754724842916,
            "precision": 0.7738095238095238,
            "recall": 0.7831325301204819
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(1)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.8117809943790463,
            "auditor_fn_violation": 0.007216162665435525,
            "auditor_fp_violation": 0.01648103262542321,
            "ave_precision_score": 0.8114164978679612,
            "fpr": 0.1206140350877193,
            "logloss": 0.5269043931226889,
            "mae": 0.3378179065062125,
            "precision": 0.7505668934240363,
            "recall": 0.7258771929824561
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8204337435388692,
            "auditor_fn_violation": 0.014252399278783634,
            "auditor_fp_violation": 0.0051137164013682675,
            "ave_precision_score": 0.8207826868742202,
            "fpr": 0.10318331503841932,
            "logloss": 0.5310140819832391,
            "mae": 0.3370441434042105,
            "precision": 0.793859649122807,
            "recall": 0.7269076305220884
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(2)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7664473684210527,
            "auc_prc": 0.8429856371364804,
            "auditor_fn_violation": 0.01275873345644815,
            "auditor_fp_violation": 0.004381155740227766,
            "ave_precision_score": 0.8433027277328728,
            "fpr": 0.08662280701754387,
            "logloss": 0.5089149914060482,
            "mae": 0.30465200895508915,
            "precision": 0.8029925187032418,
            "recall": 0.706140350877193
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8584709121814398,
            "auditor_fn_violation": 0.013104007688272303,
            "auditor_fp_violation": 0.015636171304183735,
            "ave_precision_score": 0.8587785514338441,
            "fpr": 0.08342480790340286,
            "logloss": 0.5209580598751818,
            "mae": 0.30424462957563275,
            "precision": 0.8203309692671394,
            "recall": 0.6967871485943775
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(3)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7225877192982456,
            "auc_prc": 0.7624756412106386,
            "auditor_fn_violation": 0.001856340412434598,
            "auditor_fp_violation": 0.01741401200369345,
            "ave_precision_score": 0.7637561027456988,
            "fpr": 0.13486842105263158,
            "logloss": 0.9581706849307836,
            "mae": 0.3013560570507013,
            "precision": 0.7260579064587973,
            "recall": 0.7149122807017544
        },
        "train": {
            "accuracy": 0.7156970362239298,
            "auc_prc": 0.7511879838381554,
            "auditor_fn_violation": 0.02523375610014151,
            "auditor_fp_violation": 0.008515773050927194,
            "ave_precision_score": 0.7504795346667892,
            "fpr": 0.12843029637760703,
            "logloss": 1.1728429609684787,
            "mae": 0.3129710660757847,
            "precision": 0.7526427061310782,
            "recall": 0.714859437751004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(4)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5614035087719298,
            "auc_prc": 0.5502890301062406,
            "auditor_fn_violation": 0.0065621152662357655,
            "auditor_fp_violation": 0.02019852262234533,
            "ave_precision_score": 0.5488647158697755,
            "fpr": 0.4199561403508772,
            "logloss": 0.7907937107700236,
            "mae": 0.47056560121934143,
            "precision": 0.5340632603406326,
            "recall": 0.9627192982456141
        },
        "train": {
            "accuracy": 0.6092206366630076,
            "auc_prc": 0.6283571531813616,
            "auditor_fn_violation": 0.00417917553859786,
            "auditor_fp_violation": 0.025244323482430258,
            "ave_precision_score": 0.6205980387003164,
            "fpr": 0.3732162458836443,
            "logloss": 0.72859915916399,
            "mae": 0.4457471618177339,
            "precision": 0.5863746958637469,
            "recall": 0.9678714859437751
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(5)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6677631578947368,
            "auc_prc": 0.7627333871565112,
            "auditor_fn_violation": 0.06902604647583872,
            "auditor_fp_violation": 0.047822406894429056,
            "ave_precision_score": 0.7631132406783305,
            "fpr": 0.10964912280701754,
            "logloss": 0.8073986196797249,
            "mae": 0.34861307993330787,
            "precision": 0.71671388101983,
            "recall": 0.5548245614035088
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.8034982912541926,
            "auditor_fn_violation": 0.07538606676982354,
            "auditor_fp_violation": 0.041135649035330894,
            "ave_precision_score": 0.803829105554188,
            "fpr": 0.09220636663007684,
            "logloss": 0.8099831531142434,
            "mae": 0.3305165911182819,
            "precision": 0.7851662404092071,
            "recall": 0.6164658634538153
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(6)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.7379445470963645,
            "auditor_fn_violation": 0.0019116458910434023,
            "auditor_fp_violation": 0.008291012619267468,
            "ave_precision_score": 0.7347254715485134,
            "fpr": 0.07017543859649122,
            "logloss": 0.717764319113906,
            "mae": 0.37570554885621293,
            "precision": 0.8217270194986073,
            "recall": 0.6469298245614035
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8105993970627613,
            "auditor_fn_violation": 0.0019837858569293717,
            "auditor_fp_violation": 0.011606860459862376,
            "ave_precision_score": 0.7994830412056916,
            "fpr": 0.05598243688254665,
            "logloss": 0.6373129988745037,
            "mae": 0.3740753247663308,
            "precision": 0.8671875,
            "recall": 0.6686746987951807
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(7)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5087719298245614,
            "auc_prc": 0.5631578947368422,
            "auditor_fn_violation": 0.010503231763619588,
            "auditor_fp_violation": 0.005097722376115728,
            "ave_precision_score": 0.5052631578947369,
            "fpr": 0.017543859649122806,
            "logloss": 0.7139013555273225,
            "mae": 0.4973767005037843,
            "precision": 0.6,
            "recall": 0.05263157894736842
        },
        "train": {
            "accuracy": 0.4544456641053787,
            "auc_prc": 0.539807272280052,
            "auditor_fn_violation": 0.0069917430424221595,
            "auditor_fp_violation": 0.005990809131332677,
            "ave_precision_score": 0.5448223144150337,
            "fpr": 0.026344676180021953,
            "logloss": 0.7396546604275647,
            "mae": 0.5097266221020277,
            "precision": 0.5102040816326531,
            "recall": 0.050200803212851405
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(8)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5570175438596491,
            "auc_prc": 0.6544956140350877,
            "auditor_fn_violation": 0.06836238073253309,
            "auditor_fp_violation": 0.0755617112957833,
            "ave_precision_score": 0.5323099415204678,
            "fpr": 0.18530701754385964,
            "logloss": 0.7459137645648573,
            "mae": 0.4754649007268119,
            "precision": 0.5666666666666667,
            "recall": 0.48464912280701755
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.6995841249309749,
            "auditor_fn_violation": 0.0736491520417565,
            "auditor_fp_violation": 0.051498632532698284,
            "ave_precision_score": 0.5900349776426376,
            "fpr": 0.145993413830955,
            "logloss": 0.7208608554598658,
            "mae": 0.46620708713808906,
            "precision": 0.6385869565217391,
            "recall": 0.4718875502008032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(9)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.7820992464666481,
            "auditor_fn_violation": 0.002673899661434293,
            "auditor_fp_violation": 0.006170167743921211,
            "ave_precision_score": 0.7820057321025591,
            "fpr": 0.10635964912280702,
            "logloss": 0.564137447612268,
            "mae": 0.355465675151719,
            "precision": 0.7723004694835681,
            "recall": 0.7214912280701754
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.7991706553334808,
            "auditor_fn_violation": 0.005783837876202949,
            "auditor_fp_violation": 0.014809577852611215,
            "ave_precision_score": 0.8017910196955544,
            "fpr": 0.0845225027442371,
            "logloss": 0.5657855227060724,
            "mae": 0.35601091275807434,
            "precision": 0.8288888888888889,
            "recall": 0.748995983935743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(10)",
        "seed": 18998,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8452739072212435,
            "auditor_fn_violation": 0.0037944367497691595,
            "auditor_fp_violation": 0.010782163742690063,
            "ave_precision_score": 0.7866371399755114,
            "fpr": 0.10416666666666667,
            "logloss": 0.5087795315226423,
            "mae": 0.32506128591730404,
            "precision": 0.7759433962264151,
            "recall": 0.7214912280701754
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.8662284480726983,
            "auditor_fn_violation": 0.00667874571832886,
            "auditor_fp_violation": 0.011513835473350999,
            "ave_precision_score": 0.8123750289735968,
            "fpr": 0.08781558726673985,
            "logloss": 0.5049752853156446,
            "mae": 0.32044036366642503,
            "precision": 0.8226164079822617,
            "recall": 0.7449799196787149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(11)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8162191034892281,
            "auditor_fn_violation": 0.010041551246537403,
            "auditor_fp_violation": 0.008908991228070175,
            "ave_precision_score": 0.8169787616336306,
            "fpr": 0.0625,
            "logloss": 0.8584572636065186,
            "mae": 0.33268022384792184,
            "precision": 0.8347826086956521,
            "recall": 0.631578947368421
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.859213818287354,
            "auditor_fn_violation": 0.009381102896768205,
            "auditor_fp_violation": 0.006846639007237343,
            "ave_precision_score": 0.8593731032733545,
            "fpr": 0.05378704720087816,
            "logloss": 0.9891608494698456,
            "mae": 0.3291812490113008,
            "precision": 0.8693333333333333,
            "recall": 0.6546184738955824
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(12)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.48630863178227357,
            "auditor_fn_violation": 0.006324061249615266,
            "auditor_fp_violation": 0.016952331486611266,
            "ave_precision_score": 0.48810705074883054,
            "fpr": 0.4133771929824561,
            "logloss": 0.7163693859713613,
            "mae": 0.4889441102808505,
            "precision": 0.5396825396825397,
            "recall": 0.9692982456140351
        },
        "train": {
            "accuracy": 0.6081229418221734,
            "auc_prc": 0.5345635948022656,
            "auditor_fn_violation": 0.00570889485494117,
            "auditor_fp_violation": 0.026761959690944435,
            "ave_precision_score": 0.5361514051057834,
            "fpr": 0.3699231613611416,
            "logloss": 0.6972480785022661,
            "mae": 0.47925136674248686,
            "precision": 0.5865030674846625,
            "recall": 0.9598393574297188
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(13)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5975877192982456,
            "auc_prc": 0.549686431793874,
            "auditor_fn_violation": 0.04300842566943677,
            "auditor_fp_violation": 0.018895236995998772,
            "ave_precision_score": 0.5521116645699318,
            "fpr": 0.1162280701754386,
            "logloss": 1.1349312763780945,
            "mae": 0.47457637180960566,
            "precision": 0.6478405315614618,
            "recall": 0.4276315789473684
        },
        "train": {
            "accuracy": 0.5850713501646543,
            "auc_prc": 0.5991828662244671,
            "auditor_fn_violation": 0.03477356186546406,
            "auditor_fp_violation": 0.02341571803329232,
            "ave_precision_score": 0.6013839972135087,
            "fpr": 0.10976948408342481,
            "logloss": 1.1125921183762664,
            "mae": 0.4700734203722997,
            "precision": 0.6875,
            "recall": 0.44176706827309237
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(14)",
        "seed": 18998,
        "test": {
            "accuracy": 0.48903508771929827,
            "auc_prc": 0.6253384232185402,
            "auditor_fn_violation": 0.12475954139735301,
            "auditor_fp_violation": 0.12403816558941212,
            "ave_precision_score": 0.4946303515991625,
            "fpr": 0.2719298245614035,
            "logloss": 0.6992219920889641,
            "mae": 0.5009033345339591,
            "precision": 0.4897119341563786,
            "recall": 0.5219298245614035
        },
        "train": {
            "accuracy": 0.5466520307354555,
            "auc_prc": 0.7008444418251769,
            "auditor_fn_violation": 0.13115249141461566,
            "auditor_fp_violation": 0.11318748787352854,
            "ave_precision_score": 0.5683829503655575,
            "fpr": 0.2349066959385291,
            "logloss": 0.6864960208213979,
            "mae": 0.4944793258749693,
            "precision": 0.5828460038986355,
            "recall": 0.6004016064257028
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(15)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.7373948963271498,
            "auditor_fn_violation": 0.016098703447214528,
            "auditor_fp_violation": 0.022131809787626962,
            "ave_precision_score": 0.7379095200911712,
            "fpr": 0.1699561403508772,
            "logloss": 1.1665243603183792,
            "mae": 0.3202396508007174,
            "precision": 0.6770833333333334,
            "recall": 0.7127192982456141
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.7399420908012163,
            "auditor_fn_violation": 0.02022579891464872,
            "auditor_fp_violation": 0.025265586336489988,
            "ave_precision_score": 0.7414190660295603,
            "fpr": 0.16575192096597147,
            "logloss": 1.2211644865074287,
            "mae": 0.33055370542907425,
            "precision": 0.7021696252465484,
            "recall": 0.714859437751004
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(16)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.6107774466177498,
            "auditor_fn_violation": 0.017038896583564174,
            "auditor_fp_violation": 0.02728483764235149,
            "ave_precision_score": 0.6235048413858957,
            "fpr": 0.1699561403508772,
            "logloss": 0.636146086320204,
            "mae": 0.44091488666048173,
            "precision": 0.6723044397463002,
            "recall": 0.6973684210526315
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.6255401556127266,
            "auditor_fn_violation": 0.013304590480472946,
            "auditor_fp_violation": 0.028149360918342668,
            "ave_precision_score": 0.6540260473119359,
            "fpr": 0.16136114160263446,
            "logloss": 0.6325709359469983,
            "mae": 0.4388882054586442,
            "precision": 0.703030303030303,
            "recall": 0.6987951807228916
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(17)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7138157894736842,
            "auc_prc": 0.7582599482292458,
            "auditor_fn_violation": 0.015937596183441063,
            "auditor_fp_violation": 0.012984764542936289,
            "ave_precision_score": 0.756346109151122,
            "fpr": 0.13925438596491227,
            "logloss": 0.5435043147781403,
            "mae": 0.35141746325851336,
            "precision": 0.7171492204899778,
            "recall": 0.706140350877193
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7555797764365799,
            "auditor_fn_violation": 0.007653004994731951,
            "auditor_fp_violation": 0.013964379403736418,
            "ave_precision_score": 0.7598338678600285,
            "fpr": 0.1207464324917673,
            "logloss": 0.567223546800092,
            "mae": 0.3558401415159294,
            "precision": 0.7669491525423728,
            "recall": 0.7269076305220884
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(18)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.832388514470774,
            "auditor_fn_violation": 0.00630241997537704,
            "auditor_fp_violation": 0.00675207756232687,
            "ave_precision_score": 0.8290980417962185,
            "fpr": 0.10526315789473684,
            "logloss": 0.4994547221096202,
            "mae": 0.319666177246785,
            "precision": 0.777262180974478,
            "recall": 0.7346491228070176
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8598888654371338,
            "auditor_fn_violation": 0.00015870286855435134,
            "auditor_fp_violation": 0.008537035904986941,
            "ave_precision_score": 0.8579199309645412,
            "fpr": 0.09110867178924259,
            "logloss": 0.4921031705942514,
            "mae": 0.31369014731255634,
            "precision": 0.8187772925764192,
            "recall": 0.7530120481927711
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(19)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0.6832997599862849,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5124935710612035,
            "fpr": 0.5,
            "logloss": 0.6934867784329587,
            "mae": 0.4991144396756825,
            "precision": 0.5,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5466520307354555,
            "auc_prc": 0.7139567556617028,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5602924542637431,
            "fpr": 0.45334796926454446,
            "logloss": 0.6881869262554751,
            "mae": 0.4964738477728893,
            "precision": 0.5466520307354555,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(20)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6710526315789473,
            "auc_prc": 0.506475391790989,
            "auditor_fn_violation": 0.00661742074484457,
            "auditor_fp_violation": 0.013196368113265617,
            "ave_precision_score": 0.576473397299606,
            "fpr": 0.17982456140350878,
            "logloss": 0.699089856878489,
            "mae": 0.4136733339543928,
            "precision": 0.6611570247933884,
            "recall": 0.7017543859649122
        },
        "train": {
            "accuracy": 0.7069154774972558,
            "auc_prc": 0.5714465438448019,
            "auditor_fn_violation": 0.009601523547538123,
            "auditor_fp_violation": 0.013310546641399315,
            "ave_precision_score": 0.6429376417450233,
            "fpr": 0.14818880351262348,
            "logloss": 0.6374835262111473,
            "mae": 0.3877040674474446,
            "precision": 0.7305389221556886,
            "recall": 0.7349397590361446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(21)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5789473684210527,
            "auc_prc": 0.6010878311863193,
            "auditor_fn_violation": 0.007228185595567867,
            "auditor_fp_violation": 0.01192434210526316,
            "ave_precision_score": 0.5547132421995715,
            "fpr": 0.3125,
            "logloss": 0.6806236638109452,
            "mae": 0.4886361513482897,
            "precision": 0.5560747663551402,
            "recall": 0.7828947368421053
        },
        "train": {
            "accuracy": 0.5773874862788145,
            "auc_prc": 0.631222270246766,
            "auditor_fn_violation": 0.013417005012365603,
            "auditor_fp_violation": 0.010503849905513203,
            "ave_precision_score": 0.584661879572645,
            "fpr": 0.29418221734357847,
            "logloss": 0.6824658884019217,
            "mae": 0.48933296976968826,
            "precision": 0.5870570107858244,
            "recall": 0.7650602409638554
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(22)",
        "seed": 18998,
        "test": {
            "accuracy": 0.48464912280701755,
            "auc_prc": 0.48300415011587333,
            "auditor_fn_violation": 0.0142303401046476,
            "auditor_fp_violation": 0.024892274546014163,
            "ave_precision_score": 0.4970882242795487,
            "fpr": 0.13925438596491227,
            "logloss": 0.735456767574791,
            "mae": 0.493485842504653,
            "precision": 0.4708333333333333,
            "recall": 0.24780701754385964
        },
        "train": {
            "accuracy": 0.4445664105378705,
            "auc_prc": 0.5124706005313948,
            "auditor_fn_violation": 0.017730637147933135,
            "auditor_fp_violation": 0.017600327447952535,
            "ave_precision_score": 0.5357241371710767,
            "fpr": 0.12184412733260154,
            "logloss": 0.7652299583897532,
            "mae": 0.5072858791254485,
            "precision": 0.48130841121495327,
            "recall": 0.20682730923694778
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(23)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7269736842105263,
            "auc_prc": 0.7810960940344495,
            "auditor_fn_violation": 0.0016447368421052613,
            "auditor_fp_violation": 0.01528595337026778,
            "ave_precision_score": 0.7349307453345257,
            "fpr": 0.14802631578947367,
            "logloss": 0.5718736954946722,
            "mae": 0.38130402789663587,
            "precision": 0.7169811320754716,
            "recall": 0.75
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.8156010263787363,
            "auditor_fn_violation": 0.005863189310480119,
            "auditor_fp_violation": 0.014113219382154625,
            "ave_precision_score": 0.7781628319797737,
            "fpr": 0.12952799121844127,
            "logloss": 0.5430553168584824,
            "mae": 0.36470049265547194,
            "precision": 0.7672583826429981,
            "recall": 0.7811244979919679
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(24)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.81229787876604,
            "auditor_fn_violation": 0.0009161472760849488,
            "auditor_fp_violation": 0.01751500461680518,
            "ave_precision_score": 0.7875648374311813,
            "fpr": 0.18859649122807018,
            "logloss": 0.5419452074393881,
            "mae": 0.35233469791873767,
            "precision": 0.6923076923076923,
            "recall": 0.8486842105263158
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.8510967038250643,
            "auditor_fn_violation": 0.009971830240831606,
            "auditor_fp_violation": 0.0066791940315168745,
            "ave_precision_score": 0.8295833787644589,
            "fpr": 0.16355653128430298,
            "logloss": 0.5272322108666312,
            "mae": 0.3467466554816088,
            "precision": 0.7399650959860384,
            "recall": 0.8514056224899599
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(25)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6535087719298246,
            "auc_prc": 0.60369747490558,
            "auditor_fn_violation": 0.05892197599261312,
            "auditor_fp_violation": 0.05175390504770698,
            "ave_precision_score": 0.6634384916829524,
            "fpr": 0.1962719298245614,
            "logloss": 0.64652787335093,
            "mae": 0.41470776460738035,
            "precision": 0.6405622489959839,
            "recall": 0.6995614035087719
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.6725178958147421,
            "auditor_fn_violation": 0.05779429463187547,
            "auditor_fp_violation": 0.047365665274835676,
            "ave_precision_score": 0.6970103349343423,
            "fpr": 0.1734357848518112,
            "logloss": 0.627553961012864,
            "mae": 0.404623159756383,
            "precision": 0.694980694980695,
            "recall": 0.7228915662650602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(26)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7708333333333334,
            "auc_prc": 0.8469322870554392,
            "auditor_fn_violation": 0.004847645429362885,
            "auditor_fp_violation": 0.011657433056325025,
            "ave_precision_score": 0.8416622988995733,
            "fpr": 0.08881578947368421,
            "logloss": 0.513182105323448,
            "mae": 0.3146586142901102,
            "precision": 0.8019559902200489,
            "recall": 0.7192982456140351
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8594375504974143,
            "auditor_fn_violation": 0.00375596788911961,
            "auditor_fp_violation": 0.009953673556717337,
            "ave_precision_score": 0.8567418573203832,
            "fpr": 0.07354555433589462,
            "logloss": 0.5106072723669595,
            "mae": 0.31471571831582634,
            "precision": 0.8412322274881516,
            "recall": 0.7128514056224899
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(27)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8121802970541105,
            "auditor_fn_violation": 0.0037511542012927048,
            "auditor_fp_violation": 0.009599107417666978,
            "ave_precision_score": 0.6906122889190188,
            "fpr": 0.10964912280701754,
            "logloss": 0.5634964416967331,
            "mae": 0.3687975647973648,
            "precision": 0.765807962529274,
            "recall": 0.7171052631578947
        },
        "train": {
            "accuracy": 0.7760702524698134,
            "auc_prc": 0.8562560822323322,
            "auditor_fn_violation": 0.005625135007648596,
            "auditor_fp_violation": 0.009786228580996858,
            "ave_precision_score": 0.7563437407698852,
            "fpr": 0.0845225027442371,
            "logloss": 0.5298012025018259,
            "mae": 0.353578492966363,
            "precision": 0.828125,
            "recall": 0.7449799196787149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(28)",
        "seed": 18998,
        "test": {
            "accuracy": 0.49890350877192985,
            "auc_prc": 0.5189183280662029,
            "auditor_fn_violation": 0.0038785972606955993,
            "auditor_fp_violation": 0.0024045860264696856,
            "ave_precision_score": 0.5215510267871256,
            "fpr": 0.4824561403508772,
            "logloss": 0.6942854488640636,
            "mae": 0.49965439808734674,
            "precision": 0.49943117178612056,
            "recall": 0.9627192982456141
        },
        "train": {
            "accuracy": 0.5488474204171241,
            "auc_prc": 0.5980747105190538,
            "auditor_fn_violation": 0.0049197889251848224,
            "auditor_fp_violation": 0.005833995582642071,
            "ave_precision_score": 0.5995204643121865,
            "fpr": 0.4313940724478595,
            "logloss": 0.6875805972054442,
            "mae": 0.49620839191189714,
            "precision": 0.5498281786941581,
            "recall": 0.963855421686747
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(29)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5581140350877193,
            "auc_prc": 0.7051371591458389,
            "auditor_fn_violation": 0.086413608033241,
            "auditor_fp_violation": 0.10791782086795938,
            "ave_precision_score": 0.5315530012226677,
            "fpr": 0.3092105263157895,
            "logloss": 0.6880788876900816,
            "mae": 0.4956125576833361,
            "precision": 0.5429497568881686,
            "recall": 0.7346491228070176
        },
        "train": {
            "accuracy": 0.5762897914379802,
            "auc_prc": 0.7369525483445265,
            "auditor_fn_violation": 0.09442820678983772,
            "auditor_fp_violation": 0.09703037664488111,
            "ave_precision_score": 0.5779923317050665,
            "fpr": 0.2854006586169045,
            "logloss": 0.6832588339588631,
            "mae": 0.4931824691143831,
            "precision": 0.5886075949367089,
            "recall": 0.7469879518072289
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(30)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5383771929824561,
            "auc_prc": 0.5367344644148231,
            "auditor_fn_violation": 0.0355493998153278,
            "auditor_fp_violation": 0.022199138196368113,
            "ave_precision_score": 0.522591030664127,
            "fpr": 0.1425438596491228,
            "logloss": 2.25364652535176,
            "mae": 0.4799474042987353,
            "precision": 0.559322033898305,
            "recall": 0.3618421052631579
        },
        "train": {
            "accuracy": 0.5137211855104281,
            "auc_prc": 0.5746177166141188,
            "auditor_fn_violation": 0.053033208575245,
            "auditor_fp_violation": 0.036146851901563616,
            "ave_precision_score": 0.561474887536771,
            "fpr": 0.13721185510428102,
            "logloss": 2.2087786099459654,
            "mae": 0.4907596206822066,
            "precision": 0.5901639344262295,
            "recall": 0.3614457831325301
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(31)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0.5188478678397389,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5094889297133722,
            "fpr": 0.5,
            "logloss": 0.7067528243855308,
            "mae": 0.4989711477568275,
            "precision": 0.5,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5466520307354555,
            "auc_prc": 0.6444172682005376,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.6006923099647609,
            "fpr": 0.45334796926454446,
            "logloss": 0.6785619529563279,
            "mae": 0.48660557781694747,
            "precision": 0.5466520307354555,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(32)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.269388197455342,
            "mae": 0.5,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.45334796926454446,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.880692255395743,
            "mae": 0.5466520307354555,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(33)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.7393795782835444,
            "auditor_fn_violation": 0.00862044090489382,
            "auditor_fp_violation": 0.023545706371191143,
            "ave_precision_score": 0.7409614073055286,
            "fpr": 0.19188596491228072,
            "logloss": 0.8239215334154085,
            "mae": 0.3270825491340215,
            "precision": 0.6673003802281369,
            "recall": 0.7697368421052632
        },
        "train": {
            "accuracy": 0.6915477497255763,
            "auc_prc": 0.7505695239995653,
            "auditor_fn_violation": 0.019630663157569915,
            "auditor_fp_violation": 0.03317536804671449,
            "ave_precision_score": 0.7511465724162996,
            "fpr": 0.1877058177826564,
            "logloss": 0.8520335035834269,
            "mae": 0.33102313104165426,
            "precision": 0.6940966010733453,
            "recall": 0.7791164658634538
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(34)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7435793760061757,
            "auditor_fn_violation": 0.015625000000000003,
            "auditor_fp_violation": 0.01772420360110804,
            "ave_precision_score": 0.7440582571282461,
            "fpr": 0.1962719298245614,
            "logloss": 0.8250727210377623,
            "mae": 0.3430116738579918,
            "precision": 0.6564299424184261,
            "recall": 0.75
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.7804697326614329,
            "auditor_fn_violation": 0.010983561027865582,
            "auditor_fp_violation": 0.02325624662784425,
            "ave_precision_score": 0.7808518597730325,
            "fpr": 0.18441273326015367,
            "logloss": 0.8253922496180429,
            "mae": 0.3447382125134153,
            "precision": 0.6871508379888268,
            "recall": 0.7409638554216867
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(35)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5350877192982456,
            "auc_prc": 0.576549223475171,
            "auditor_fn_violation": 0.06812673130193907,
            "auditor_fp_violation": 0.041649834564481396,
            "ave_precision_score": 0.5401320978208991,
            "fpr": 0.3432017543859649,
            "logloss": 0.6838785427585877,
            "mae": 0.4837301212542674,
            "precision": 0.5243161094224924,
            "recall": 0.756578947368421
        },
        "train": {
            "accuracy": 0.5718990120746432,
            "auc_prc": 0.6303814436840891,
            "auditor_fn_violation": 0.0826379061801542,
            "auditor_fp_violation": 0.05113184830016771,
            "ave_precision_score": 0.5949115067744165,
            "fpr": 0.29527991218441274,
            "logloss": 0.687454636904666,
            "mae": 0.48295471894950426,
            "precision": 0.5835913312693498,
            "recall": 0.7570281124497992
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(36)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5175438596491229,
            "auc_prc": 0.5192702913532676,
            "auditor_fn_violation": 0.0007117574638350263,
            "auditor_fp_violation": 0.004789935364727613,
            "ave_precision_score": 0.5239159296819426,
            "fpr": 0.47368421052631576,
            "logloss": 0.6906065083340968,
            "mae": 0.4938630034754935,
            "precision": 0.509090909090909,
            "recall": 0.9824561403508771
        },
        "train": {
            "accuracy": 0.5653128430296378,
            "auc_prc": 0.5779941551491252,
            "auditor_fn_violation": 0.0013225239046195762,
            "auditor_fp_violation": 0.005419369928477074,
            "ave_precision_score": 0.5799387914596134,
            "fpr": 0.4270032930845225,
            "logloss": 0.6821716770027513,
            "mae": 0.4898821257962615,
            "precision": 0.5579545454545455,
            "recall": 0.9859437751004017
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(37)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6885964912280702,
            "auc_prc": 0.7547358852964916,
            "auditor_fn_violation": 0.009017197599261314,
            "auditor_fp_violation": 0.013961026469682983,
            "ave_precision_score": 0.7390965425228213,
            "fpr": 0.14364035087719298,
            "logloss": 0.5919900902732492,
            "mae": 0.39104300580935125,
            "precision": 0.6981566820276498,
            "recall": 0.6644736842105263
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.8012282162686233,
            "auditor_fn_violation": 0.023637910588567228,
            "auditor_fp_violation": 0.004526330057967855,
            "ave_precision_score": 0.7905908209353611,
            "fpr": 0.1119648737650933,
            "logloss": 0.5713522210207723,
            "mae": 0.3829645635325024,
            "precision": 0.7627906976744186,
            "recall": 0.6586345381526104
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(38)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5679824561403509,
            "auc_prc": 0.6546383925215946,
            "auditor_fn_violation": 0.005338180978762698,
            "auditor_fp_violation": 0.003693444136657434,
            "ave_precision_score": 0.6715196045679382,
            "fpr": 0.05043859649122807,
            "logloss": 7.061790670912461,
            "mae": 0.4537385427171264,
            "precision": 0.7012987012987013,
            "recall": 0.23684210526315788
        },
        "train": {
            "accuracy": 0.5499451152579583,
            "auc_prc": 0.7005529648382278,
            "auditor_fn_violation": 0.015085589338693986,
            "auditor_fp_violation": 0.004693775033688333,
            "ave_precision_score": 0.7039787680549225,
            "fpr": 0.043907793633369926,
            "logloss": 7.533334837117877,
            "mae": 0.48209880908006014,
            "precision": 0.7619047619047619,
            "recall": 0.2570281124497992
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(39)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.6900586009373895,
            "auditor_fn_violation": 0.0037247037550015454,
            "auditor_fp_violation": 0.006737650046168052,
            "ave_precision_score": 0.6913430482463836,
            "fpr": 0.12609649122807018,
            "logloss": 0.6200805926127242,
            "mae": 0.4209450851276256,
            "precision": 0.7110552763819096,
            "recall": 0.6206140350877193
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7966676218895397,
            "auditor_fn_violation": 0.010791795061695754,
            "auditor_fp_violation": 0.012444085338464769,
            "ave_precision_score": 0.7971686082063845,
            "fpr": 0.09330406147091108,
            "logloss": 0.5805911149900737,
            "mae": 0.406446347358656,
            "precision": 0.7858942065491183,
            "recall": 0.6265060240963856
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(40)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.7781342238743123,
            "auditor_fn_violation": 0.008488188673437988,
            "auditor_fp_violation": 0.015189769929208988,
            "ave_precision_score": 0.7786917880719436,
            "fpr": 0.11293859649122807,
            "logloss": 0.585191221708752,
            "mae": 0.38712026312285125,
            "precision": 0.7352185089974294,
            "recall": 0.6271929824561403
        },
        "train": {
            "accuracy": 0.7113062568605928,
            "auc_prc": 0.8457330379654069,
            "auditor_fn_violation": 0.020203756849571725,
            "auditor_fp_violation": 0.018474762321159462,
            "ave_precision_score": 0.8460585389270641,
            "fpr": 0.0867178924259056,
            "logloss": 0.5767506452051442,
            "mae": 0.3769012009485255,
            "precision": 0.7989821882951654,
            "recall": 0.6305220883534136
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(41)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7028508771929824,
            "auc_prc": 0.8276288215991634,
            "auditor_fn_violation": 0.006295206217297634,
            "auditor_fp_violation": 0.005042416897506926,
            "ave_precision_score": 0.8280186530976938,
            "fpr": 0.03179824561403509,
            "logloss": 0.5530233230866914,
            "mae": 0.36057934278965387,
            "precision": 0.8806584362139918,
            "recall": 0.4692982456140351
        },
        "train": {
            "accuracy": 0.6948408342480791,
            "auc_prc": 0.8632177643022236,
            "auditor_fn_violation": 0.0013225239046195847,
            "auditor_fp_violation": 0.004505067203908113,
            "ave_precision_score": 0.8633849882677214,
            "fpr": 0.019758507135016465,
            "logloss": 0.5626339155088653,
            "mae": 0.3609817646419308,
            "precision": 0.9296875,
            "recall": 0.4779116465863454
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(42)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5504385964912281,
            "auc_prc": 0.568899159322984,
            "auditor_fn_violation": 0.0061749769159741615,
            "auditor_fp_violation": 0.016298284087411515,
            "ave_precision_score": 0.5687816464633878,
            "fpr": 0.08114035087719298,
            "logloss": 8.056845327140454,
            "mae": 0.4676798410980387,
            "precision": 0.6185567010309279,
            "recall": 0.2631578947368421
        },
        "train": {
            "accuracy": 0.5214050493962679,
            "auc_prc": 0.6228051306703799,
            "auditor_fn_violation": 0.011100383972773655,
            "auditor_fp_violation": 0.007077872545137053,
            "ave_precision_score": 0.6213973345619604,
            "fpr": 0.08781558726673985,
            "logloss": 9.124818921689814,
            "mae": 0.4967964964817478,
            "precision": 0.6396396396396397,
            "recall": 0.285140562248996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(43)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5877192982456141,
            "auc_prc": 0.8009487702619702,
            "auditor_fn_violation": 0.004746652816251154,
            "auditor_fp_violation": 0.01777469990766392,
            "ave_precision_score": 0.7926425988750534,
            "fpr": 0.40131578947368424,
            "logloss": 1.7832572836034517,
            "mae": 0.39301659135258216,
            "precision": 0.5492610837438424,
            "recall": 0.9780701754385965
        },
        "train": {
            "accuracy": 0.6278814489571899,
            "auc_prc": 0.8067074484690926,
            "auditor_fn_violation": 0.0024466692235462157,
            "auditor_fp_violation": 0.026533384009802184,
            "ave_precision_score": 0.791159347774956,
            "fpr": 0.3567508232711306,
            "logloss": 1.9563948254939227,
            "mae": 0.36221196593023613,
            "precision": 0.5982694684796045,
            "recall": 0.9718875502008032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(44)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6600877192982456,
            "auc_prc": 0.6899681393026236,
            "auditor_fn_violation": 0.004347491535857187,
            "auditor_fp_violation": 0.015081563558017853,
            "ave_precision_score": 0.6905693878539054,
            "fpr": 0.3048245614035088,
            "logloss": 1.5498468655029192,
            "mae": 0.3591653806475616,
            "precision": 0.603988603988604,
            "recall": 0.9298245614035088
        },
        "train": {
            "accuracy": 0.6981339187705817,
            "auc_prc": 0.7528580392393894,
            "auditor_fn_violation": 0.007727948015993724,
            "auditor_fp_violation": 0.024021709373994997,
            "ave_precision_score": 0.7527957224785319,
            "fpr": 0.25686059275521406,
            "logloss": 1.4573067921833822,
            "mae": 0.32292603070572024,
            "precision": 0.6613603473227206,
            "recall": 0.9176706827309237
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(45)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5942982456140351,
            "auc_prc": 0.7297267141755933,
            "auditor_fn_violation": 0.004479743767313035,
            "auditor_fp_violation": 0.006607802400738688,
            "ave_precision_score": 0.7188288501638328,
            "fpr": 0.047149122807017545,
            "logloss": 0.7439126636286383,
            "mae": 0.38385309674572854,
            "precision": 0.75,
            "recall": 0.28289473684210525
        },
        "train": {
            "accuracy": 0.5598243688254665,
            "auc_prc": 0.7780103300226324,
            "auditor_fn_violation": 0.010787386648680338,
            "auditor_fp_violation": 0.0007069898974864653,
            "ave_precision_score": 0.7749931811735787,
            "fpr": 0.04610318331503842,
            "logloss": 0.7423427088926939,
            "mae": 0.39312405519301946,
            "precision": 0.7679558011049724,
            "recall": 0.2791164658634538
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(46)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0.75,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5,
            "fpr": 0.5,
            "logloss": 0.694343230401531,
            "mae": 0.5,
            "precision": 0.5,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5466520307354555,
            "auc_prc": 0.7733260153677277,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5466520307354555,
            "fpr": 0.45334796926454446,
            "logloss": 0.6897789033572932,
            "mae": 0.4977196551834843,
            "precision": 0.5466520307354555,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(47)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6140350877192983,
            "auc_prc": 0.637296397146769,
            "auditor_fn_violation": 0.07681209602954757,
            "auditor_fp_violation": 0.08669254001231148,
            "ave_precision_score": 0.6396900005995014,
            "fpr": 0.2576754385964912,
            "logloss": 0.6669332858791933,
            "mae": 0.4428990768414734,
            "precision": 0.5905923344947736,
            "recall": 0.743421052631579
        },
        "train": {
            "accuracy": 0.6267837541163557,
            "auc_prc": 0.6558821719932365,
            "auditor_fn_violation": 0.08001710464249974,
            "auditor_fp_violation": 0.07561070903644719,
            "ave_precision_score": 0.659438953416192,
            "fpr": 0.23710208562019758,
            "logloss": 0.6739767069102585,
            "mae": 0.4419175352846016,
            "precision": 0.6338983050847458,
            "recall": 0.751004016064257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(48)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6557017543859649,
            "auc_prc": 0.6954707857632295,
            "auditor_fn_violation": 0.012157586949830717,
            "auditor_fp_violation": 0.019520429362880887,
            "ave_precision_score": 0.6955872421123381,
            "fpr": 0.09868421052631579,
            "logloss": 2.300575217333242,
            "mae": 0.4522417792739553,
            "precision": 0.7204968944099379,
            "recall": 0.5087719298245614
        },
        "train": {
            "accuracy": 0.6542261251372119,
            "auc_prc": 0.7302828510860153,
            "auditor_fn_violation": 0.014455186277491969,
            "auditor_fp_violation": 0.02313664307375819,
            "ave_precision_score": 0.7305685674493201,
            "fpr": 0.07574094401756312,
            "logloss": 2.272138394242317,
            "mae": 0.46087670151935434,
            "precision": 0.7850467289719626,
            "recall": 0.5060240963855421
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(49)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5910087719298246,
            "auc_prc": 0.6679889834664372,
            "auditor_fn_violation": 0.00013706140350877583,
            "auditor_fp_violation": 0.00363573407202216,
            "ave_precision_score": 0.6686838886645923,
            "fpr": 0.07894736842105263,
            "logloss": 0.7140531664201403,
            "mae": 0.4283490139228218,
            "precision": 0.6828193832599119,
            "recall": 0.3399122807017544
        },
        "train": {
            "accuracy": 0.5850713501646543,
            "auc_prc": 0.7181005527876894,
            "auditor_fn_violation": 0.01116210175498922,
            "auditor_fp_violation": 0.007396815356033202,
            "ave_precision_score": 0.7185835330204808,
            "fpr": 0.07903402854006586,
            "logloss": 0.7226244253531178,
            "mae": 0.4328046667161597,
            "precision": 0.7272727272727273,
            "recall": 0.3855421686746988
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(50)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5975877192982456,
            "auc_prc": 0.6135131713451817,
            "auditor_fn_violation": 0.04551400430901817,
            "auditor_fp_violation": 0.08926304247460758,
            "ave_precision_score": 0.6238945335684334,
            "fpr": 0.3081140350877193,
            "logloss": 4.0672846094240995,
            "mae": 0.4393074919571814,
            "precision": 0.5683563748079877,
            "recall": 0.8114035087719298
        },
        "train": {
            "accuracy": 0.6498353457738749,
            "auc_prc": 0.6891853117940745,
            "auditor_fn_violation": 0.04062793434991338,
            "auditor_fp_violation": 0.08502749552815601,
            "ave_precision_score": 0.7059529594140632,
            "fpr": 0.27442371020856204,
            "logloss": 3.4915988326811296,
            "mae": 0.3903660611912181,
            "precision": 0.6318114874815906,
            "recall": 0.8614457831325302
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(51)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0.75,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5,
            "fpr": 0.5,
            "logloss": 0.6941609793259388,
            "mae": 0.5,
            "precision": 0.5,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5466520307354555,
            "auc_prc": 0.7733260153677277,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5466520307354555,
            "fpr": 0.45334796926454446,
            "logloss": 0.6899588946033108,
            "mae": 0.497900376989865,
            "precision": 0.5466520307354555,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(52)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6271929824561403,
            "auc_prc": 0.7615290412190617,
            "auditor_fn_violation": 0.01037338411819021,
            "auditor_fp_violation": 0.0023757309941520484,
            "ave_precision_score": 0.7631514377875316,
            "fpr": 0.020833333333333332,
            "logloss": 0.6551496250828637,
            "mae": 0.43848624034670364,
            "precision": 0.8766233766233766,
            "recall": 0.29605263157894735
        },
        "train": {
            "accuracy": 0.6114160263446762,
            "auc_prc": 0.7960598662962657,
            "auditor_fn_violation": 0.00917831589805987,
            "auditor_fp_violation": 0.001932261862679174,
            "ave_precision_score": 0.7974387530342697,
            "fpr": 0.019758507135016465,
            "logloss": 0.6735098157074214,
            "mae": 0.4485176110151505,
            "precision": 0.9,
            "recall": 0.3253012048192771
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(53)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7572351234774708,
            "auditor_fn_violation": 0.008048149430594034,
            "auditor_fp_violation": 0.028489535241612806,
            "ave_precision_score": 0.7052759782052764,
            "fpr": 0.17543859649122806,
            "logloss": 4.704908318330268,
            "mae": 0.3105078400526249,
            "precision": 0.6844181459566075,
            "recall": 0.7609649122807017
        },
        "train": {
            "accuracy": 0.686059275521405,
            "auc_prc": 0.7860741113364884,
            "auditor_fn_violation": 0.024063322444553184,
            "auditor_fp_violation": 0.029494236437621432,
            "ave_precision_score": 0.7408619398968345,
            "fpr": 0.16794731064763996,
            "logloss": 4.495475297845879,
            "mae": 0.32437096097687285,
            "precision": 0.7046332046332047,
            "recall": 0.7329317269076305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(54)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6162280701754386,
            "auc_prc": 0.6797685098090087,
            "auditor_fn_violation": 0.008810403200984925,
            "auditor_fp_violation": 0.0035058864265928077,
            "ave_precision_score": 0.5922159396502951,
            "fpr": 0.2565789473684211,
            "logloss": 0.6894327105810133,
            "mae": 0.46016956005948023,
            "precision": 0.5923344947735192,
            "recall": 0.7456140350877193
        },
        "train": {
            "accuracy": 0.6136114160263447,
            "auc_prc": 0.6901896794475548,
            "auditor_fn_violation": 0.01612156639731263,
            "auditor_fp_violation": 0.00024186496492959505,
            "ave_precision_score": 0.6133578269958075,
            "fpr": 0.2535675082327113,
            "logloss": 0.6724417857765724,
            "mae": 0.45234524930953063,
            "precision": 0.6200657894736842,
            "recall": 0.7570281124497992
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(55)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8148976369107881,
            "auditor_fn_violation": 0.0007502308402585466,
            "auditor_fp_violation": 0.010782163742690063,
            "ave_precision_score": 0.7591262366971314,
            "fpr": 0.10416666666666667,
            "logloss": 0.5527867250722109,
            "mae": 0.35335359715840275,
            "precision": 0.7732696897374701,
            "recall": 0.7105263157894737
        },
        "train": {
            "accuracy": 0.7694840834248079,
            "auc_prc": 0.8493662115834607,
            "auditor_fn_violation": 0.005898456614603313,
            "auditor_fp_violation": 0.013092602387286939,
            "ave_precision_score": 0.7981458006589632,
            "fpr": 0.08562019758507135,
            "logloss": 0.5240094364697343,
            "mae": 0.34096712341816315,
            "precision": 0.8243243243243243,
            "recall": 0.7349397590361446
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(56)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5921052631578947,
            "auc_prc": 0.6729288574794973,
            "auditor_fn_violation": 0.004061345798707301,
            "auditor_fp_violation": 0.004458102493074793,
            "ave_precision_score": 0.6733668570409677,
            "fpr": 0.04057017543859649,
            "logloss": 3.474409361904641,
            "mae": 0.42400182049342916,
            "precision": 0.7658227848101266,
            "recall": 0.26535087719298245
        },
        "train": {
            "accuracy": 0.5773874862788145,
            "auc_prc": 0.6992292044714297,
            "auditor_fn_violation": 0.005371651259263176,
            "auditor_fp_violation": 0.004085125836228183,
            "ave_precision_score": 0.699691412139251,
            "fpr": 0.03512623490669594,
            "logloss": 3.9083223105895266,
            "mae": 0.45392481073712004,
            "precision": 0.8192090395480226,
            "recall": 0.29116465863453816
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(57)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8485474888581712,
            "auditor_fn_violation": 0.00579986149584488,
            "auditor_fp_violation": 0.01311220760233918,
            "ave_precision_score": 0.8487765149529272,
            "fpr": 0.11951754385964912,
            "logloss": 0.540380334603072,
            "mae": 0.27461856269502805,
            "precision": 0.7572383073496659,
            "recall": 0.7456140350877193
        },
        "train": {
            "accuracy": 0.7552140504939627,
            "auc_prc": 0.861449070391727,
            "auditor_fn_violation": 0.008911606910628244,
            "auditor_fp_violation": 0.021725321135542725,
            "ave_precision_score": 0.861680600702475,
            "fpr": 0.11855104281009879,
            "logloss": 0.5708647522829995,
            "mae": 0.27679327312250174,
            "precision": 0.780040733197556,
            "recall": 0.7690763052208835
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(58)",
        "seed": 18998,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8164770674588196,
            "auditor_fn_violation": 0.0039964219759926235,
            "auditor_fp_violation": 0.011065904893813483,
            "ave_precision_score": 0.7534057387384314,
            "fpr": 0.10307017543859649,
            "logloss": 0.5592682061249924,
            "mae": 0.3479688580294973,
            "precision": 0.7740384615384616,
            "recall": 0.706140350877193
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8619503663055038,
            "auditor_fn_violation": 0.006288601166466088,
            "auditor_fp_violation": 0.011043394827279179,
            "ave_precision_score": 0.8105820235344736,
            "fpr": 0.08122941822173436,
            "logloss": 0.5310164356072772,
            "mae": 0.33612039470973576,
            "precision": 0.8329571106094809,
            "recall": 0.7409638554216867
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(59)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.7610485230711967,
            "auditor_fn_violation": 0.00906048014773777,
            "auditor_fp_violation": 0.007675438596491229,
            "ave_precision_score": 0.7614422826638134,
            "fpr": 0.2708333333333333,
            "logloss": 0.7828155000212919,
            "mae": 0.3863054873960975,
            "precision": 0.6146645865834633,
            "recall": 0.8640350877192983
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.7633210326829827,
            "auditor_fn_violation": 0.008887360639043553,
            "auditor_fp_violation": 0.015062074244570671,
            "ave_precision_score": 0.7640464650574728,
            "fpr": 0.23710208562019758,
            "logloss": 0.7808938476384497,
            "mae": 0.3735134831326068,
            "precision": 0.6587677725118484,
            "recall": 0.8373493975903614
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(60)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0.486370333080809,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.4828579028203098,
            "fpr": 0.5,
            "logloss": 0.6989583079814806,
            "mae": 0.4999717941932511,
            "precision": 0.5,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5466520307354555,
            "auc_prc": 0.5705640995453557,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5656790069551147,
            "fpr": 0.45334796926454446,
            "logloss": 0.7063465372093328,
            "mae": 0.4994110826864986,
            "precision": 0.5466520307354555,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(61)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6326754385964912,
            "auc_prc": 0.8009656911186045,
            "auditor_fn_violation": 0.004789935364727607,
            "auditor_fp_violation": 0.0028470298553401044,
            "ave_precision_score": 0.7803448774340733,
            "fpr": 0.01206140350877193,
            "logloss": 0.7341765930066254,
            "mae": 0.3931205117209047,
            "precision": 0.9230769230769231,
            "recall": 0.2894736842105263
        },
        "train": {
            "accuracy": 0.6169045005488474,
            "auc_prc": 0.8441073589621677,
            "auditor_fn_violation": 0.0017589567931440492,
            "auditor_fp_violation": 0.0024532017871428838,
            "ave_precision_score": 0.8246531601278031,
            "fpr": 0.012074643249176729,
            "logloss": 0.7615538200209357,
            "mae": 0.4018846784363594,
            "precision": 0.935672514619883,
            "recall": 0.321285140562249
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(62)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5241228070175439,
            "auc_prc": 0.44848450118693833,
            "auditor_fn_violation": 0.02878770390889505,
            "auditor_fp_violation": 0.028499153585718688,
            "ave_precision_score": 0.44963231564395884,
            "fpr": 0.4024122807017544,
            "logloss": 1.2658416156019134,
            "mae": 0.49038637548271385,
            "precision": 0.5145502645502645,
            "recall": 0.8530701754385965
        },
        "train": {
            "accuracy": 0.557628979143798,
            "auc_prc": 0.5052661643493743,
            "auditor_fn_violation": 0.01785407271236428,
            "auditor_fp_violation": 0.0264908583016827,
            "ave_precision_score": 0.5065079356371978,
            "fpr": 0.3699231613611416,
            "logloss": 1.4648455030755092,
            "mae": 0.49312546244100475,
            "precision": 0.5617685305591678,
            "recall": 0.8674698795180723
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(63)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.6582052386108459,
            "auditor_fn_violation": 0.015514389042782399,
            "auditor_fp_violation": 0.016012138350261617,
            "ave_precision_score": 0.6594009741178986,
            "fpr": 0.15021929824561403,
            "logloss": 0.6440696545355702,
            "mae": 0.39445698161593135,
            "precision": 0.6900452488687783,
            "recall": 0.668859649122807
        },
        "train": {
            "accuracy": 0.6739846322722283,
            "auc_prc": 0.7417733561802042,
            "auditor_fn_violation": 0.014878393926970233,
            "auditor_fp_violation": 0.01862626015633514,
            "ave_precision_score": 0.7424181024957865,
            "fpr": 0.141602634467618,
            "logloss": 0.6476379375803994,
            "mae": 0.3978211707982483,
            "precision": 0.7189542483660131,
            "recall": 0.6626506024096386
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(64)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5975877192982456,
            "auc_prc": 0.6340849301130558,
            "auditor_fn_violation": 0.024868228685749463,
            "auditor_fp_violation": 0.01783962373037859,
            "ave_precision_score": 0.6251973994798705,
            "fpr": 0.2949561403508772,
            "logloss": 0.686665178177751,
            "mae": 0.46714357682095287,
            "precision": 0.5709728867623605,
            "recall": 0.7850877192982456
        },
        "train": {
            "accuracy": 0.5938529088913282,
            "auc_prc": 0.6552779819579799,
            "auditor_fn_violation": 0.03198303642671675,
            "auditor_fp_violation": 0.030376644881100787,
            "ave_precision_score": 0.6464499537281468,
            "fpr": 0.2667398463227223,
            "logloss": 0.6702492302354562,
            "mae": 0.46173011865691743,
            "precision": 0.6042345276872965,
            "recall": 0.7449799196787149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(65)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7182017543859649,
            "auc_prc": 0.78288559814787,
            "auditor_fn_violation": 0.014432325330871036,
            "auditor_fp_violation": 0.017543859649122806,
            "ave_precision_score": 0.7698141970825807,
            "fpr": 0.125,
            "logloss": 0.5655759229914741,
            "mae": 0.37700349314081105,
            "precision": 0.7330210772833724,
            "recall": 0.6864035087719298
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8182360777935712,
            "auditor_fn_violation": 0.010884371735019114,
            "auditor_fp_violation": 0.015806274136661684,
            "ave_precision_score": 0.8072905711445401,
            "fpr": 0.11306256860592755,
            "logloss": 0.5470247417132887,
            "mae": 0.36579225727987735,
            "precision": 0.7775377969762419,
            "recall": 0.7228915662650602
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(66)",
        "seed": 18998,
        "test": {
            "accuracy": 0.4956140350877193,
            "auc_prc": 0.5267472555316061,
            "auditor_fn_violation": 0.020669821483533392,
            "auditor_fp_violation": 0.028220221606648204,
            "ave_precision_score": 0.5144404206926649,
            "fpr": 0.05263157894736842,
            "logloss": 9.729980197710416,
            "mae": 0.49928470390301494,
            "precision": 0.4782608695652174,
            "recall": 0.09649122807017543
        },
        "train": {
            "accuracy": 0.442371020856202,
            "auc_prc": 0.5448623037013657,
            "auditor_fn_violation": 0.021885566414946295,
            "auditor_fp_violation": 0.03367504511711846,
            "ave_precision_score": 0.5305803582637199,
            "fpr": 0.06256860592755215,
            "logloss": 11.377027972624225,
            "mae": 0.5500858578178847,
            "precision": 0.4519230769230769,
            "recall": 0.09437751004016064
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(67)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.6144196332599732,
            "auditor_fn_violation": 0.016098703447214528,
            "auditor_fp_violation": 0.02091268467220684,
            "ave_precision_score": 0.6157226547886723,
            "fpr": 0.17214912280701755,
            "logloss": 0.6208391648983819,
            "mae": 0.4321137000234765,
            "precision": 0.6742738589211619,
            "recall": 0.7127192982456141
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.6525712501799745,
            "auditor_fn_violation": 0.020763625302527355,
            "auditor_fp_violation": 0.025265586336489988,
            "ave_precision_score": 0.6545341527788149,
            "fpr": 0.16575192096597147,
            "logloss": 0.6215898456778675,
            "mae": 0.4315163148515965,
            "precision": 0.702755905511811,
            "recall": 0.7168674698795181
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(68)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7390350877192983,
            "auc_prc": 0.8011295550642621,
            "auditor_fn_violation": 0.008505020775623272,
            "auditor_fp_violation": 0.007685056940597108,
            "ave_precision_score": 0.8016867637061016,
            "fpr": 0.10635964912280702,
            "logloss": 0.5294963405840156,
            "mae": 0.33418427111495186,
            "precision": 0.7645631067961165,
            "recall": 0.6907894736842105
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8488977421973991,
            "auditor_fn_violation": 6.171778221558646e-05,
            "auditor_fp_violation": 0.004148914398407412,
            "ave_precision_score": 0.8491944624003094,
            "fpr": 0.0845225027442371,
            "logloss": 0.5085021893918132,
            "mae": 0.3220412615527175,
            "precision": 0.8229885057471265,
            "recall": 0.7188755020080321
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(69)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8326055668535857,
            "auditor_fn_violation": 0.0035732148353339486,
            "auditor_fp_violation": 0.014432325330871042,
            "ave_precision_score": 0.8092799747104378,
            "fpr": 0.16557017543859648,
            "logloss": 0.5111215403260463,
            "mae": 0.34081922007495896,
            "precision": 0.7084942084942085,
            "recall": 0.8048245614035088
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.8494036445142439,
            "auditor_fn_violation": 0.008067395818179413,
            "auditor_fp_violation": 0.028138729491312797,
            "ave_precision_score": 0.8250840077394616,
            "fpr": 0.15697036223929747,
            "logloss": 0.5117467632956781,
            "mae": 0.34049399010632614,
            "precision": 0.7332089552238806,
            "recall": 0.7891566265060241
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(70)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7083333333333334,
            "auc_prc": 0.7439712924869023,
            "auditor_fn_violation": 0.009387503847337641,
            "auditor_fp_violation": 0.006502000615574025,
            "ave_precision_score": 0.6851028541405046,
            "fpr": 0.12280701754385964,
            "logloss": 0.5949631010369186,
            "mae": 0.3978007195778845,
            "precision": 0.7294685990338164,
            "recall": 0.6622807017543859
        },
        "train": {
            "accuracy": 0.7376509330406147,
            "auc_prc": 0.8101418308620573,
            "auditor_fn_violation": 0.005713303267956581,
            "auditor_fp_violation": 0.013964379403736417,
            "ave_precision_score": 0.7564832507703869,
            "fpr": 0.09330406147091108,
            "logloss": 0.5593473539812907,
            "mae": 0.3804496327736113,
            "precision": 0.8018648018648019,
            "recall": 0.6907630522088354
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(71)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5263157894736842,
            "auc_prc": 0.6191105849009932,
            "auditor_fn_violation": 0.0010099261311172722,
            "auditor_fp_violation": 0.0329091643582641,
            "ave_precision_score": 0.5116265372220843,
            "fpr": 0.30043859649122806,
            "logloss": 0.8089495984247029,
            "mae": 0.48867413185929,
            "precision": 0.5209790209790209,
            "recall": 0.6535087719298246
        },
        "train": {
            "accuracy": 0.5181119648737651,
            "auc_prc": 0.6472088790573487,
            "auditor_fn_violation": 0.008312062740534037,
            "auditor_fp_violation": 0.032207908186996176,
            "ave_precision_score": 0.5567853047129965,
            "fpr": 0.270032930845225,
            "logloss": 0.8529476828260545,
            "mae": 0.5020299474703624,
            "precision": 0.5535390199637024,
            "recall": 0.6124497991967871
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(72)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.8402507864882542,
            "auditor_fn_violation": 0.015495152354570638,
            "auditor_fp_violation": 0.03579226300400125,
            "ave_precision_score": 0.840569074795757,
            "fpr": 0.31030701754385964,
            "logloss": 0.6657868426471364,
            "mae": 0.3593510505743325,
            "precision": 0.6047486033519553,
            "recall": 0.9495614035087719
        },
        "train": {
            "accuracy": 0.6827661909989023,
            "auc_prc": 0.8707337288925956,
            "auditor_fn_violation": 0.011505957970190312,
            "auditor_fp_violation": 0.04439152356322909,
            "ave_precision_score": 0.870970689786368,
            "fpr": 0.2843029637760702,
            "logloss": 0.6123476205548204,
            "mae": 0.33413356953861784,
            "precision": 0.6437414030261348,
            "recall": 0.9397590361445783
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(73)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7149122807017544,
            "auc_prc": 0.744336905632476,
            "auditor_fn_violation": 0.03962757771622037,
            "auditor_fp_violation": 0.01704370575561711,
            "ave_precision_score": 0.7382574403323685,
            "fpr": 0.09649122807017543,
            "logloss": 3.8993281671885938,
            "mae": 0.3483065228864607,
            "precision": 0.7634408602150538,
            "recall": 0.6228070175438597
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.759922498739583,
            "auditor_fn_violation": 0.04761086056630475,
            "auditor_fp_violation": 0.011155024811092831,
            "ave_precision_score": 0.7563778004289986,
            "fpr": 0.09879253567508232,
            "logloss": 4.03166190638693,
            "mae": 0.3461386642668083,
            "precision": 0.7841726618705036,
            "recall": 0.6566265060240963
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(74)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.7838209608466912,
            "auditor_fn_violation": 0.0039964219759926235,
            "auditor_fp_violation": 0.010272391505078488,
            "ave_precision_score": 0.770086235249591,
            "fpr": 0.10526315789473684,
            "logloss": 0.5783437337289864,
            "mae": 0.37230778947930065,
            "precision": 0.7703349282296651,
            "recall": 0.706140350877193
        },
        "train": {
            "accuracy": 0.7771679473106476,
            "auc_prc": 0.8272990922976122,
            "auditor_fn_violation": 0.006288601166466088,
            "auditor_fp_violation": 0.011043394827279179,
            "ave_precision_score": 0.8163122203534552,
            "fpr": 0.08122941822173436,
            "logloss": 0.5566810216494475,
            "mae": 0.36132013164746907,
            "precision": 0.8329571106094809,
            "recall": 0.7409638554216867
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(75)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5350877192982456,
            "auc_prc": 0.5436818764524318,
            "auditor_fn_violation": 0.004539858417974769,
            "auditor_fp_violation": 0.01608187134502925,
            "ave_precision_score": 0.5317814562103159,
            "fpr": 0.2719298245614035,
            "logloss": 0.7506586636877578,
            "mae": 0.48477331664527584,
            "precision": 0.5303030303030303,
            "recall": 0.6140350877192983
        },
        "train": {
            "accuracy": 0.5301866081229418,
            "auc_prc": 0.5707187862895372,
            "auditor_fn_violation": 0.005951357570788098,
            "auditor_fp_violation": 0.007739678877746572,
            "ave_precision_score": 0.5657583347732866,
            "fpr": 0.26125137211855104,
            "logloss": 0.762083026574752,
            "mae": 0.48918906340434837,
            "precision": 0.5641025641025641,
            "recall": 0.6184738955823293
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(76)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.7917211535407738,
            "auditor_fn_violation": 0.004554285934133583,
            "auditor_fp_violation": 0.007675438596491225,
            "ave_precision_score": 0.7922112510477626,
            "fpr": 0.125,
            "logloss": 0.5312005066311294,
            "mae": 0.3613234056525439,
            "precision": 0.740909090909091,
            "recall": 0.7149122807017544
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8433218330585013,
            "auditor_fn_violation": 0.003610490259611452,
            "auditor_fp_violation": 0.008853320859125624,
            "ave_precision_score": 0.8435242157722047,
            "fpr": 0.10428100987925357,
            "logloss": 0.5197869321052725,
            "mae": 0.35453212915885984,
            "precision": 0.7974413646055437,
            "recall": 0.751004016064257
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(77)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5043859649122807,
            "auc_prc": 0.6380287470524425,
            "auditor_fn_violation": 0.014918051708217911,
            "auditor_fp_violation": 0.018851954447522322,
            "ave_precision_score": 0.5327453488094469,
            "fpr": 0.37719298245614036,
            "logloss": 0.6932634729768092,
            "mae": 0.49790029283286186,
            "precision": 0.5028901734104047,
            "recall": 0.7631578947368421
        },
        "train": {
            "accuracy": 0.5422612513721186,
            "auc_prc": 0.6789549422555322,
            "auditor_fn_violation": 0.01681368724073021,
            "auditor_fp_violation": 0.01800963738860258,
            "ave_precision_score": 0.5782230541969678,
            "fpr": 0.3512623490669594,
            "logloss": 0.6867119084948389,
            "mae": 0.49456166465379275,
            "precision": 0.5561719833564494,
            "recall": 0.8052208835341366
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(78)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7785462793111461,
            "auditor_fn_violation": 0.007093528778085566,
            "auditor_fp_violation": 0.009810710987996312,
            "ave_precision_score": 0.6818076016060953,
            "fpr": 0.2236842105263158,
            "logloss": 5.2222100373340234,
            "mae": 0.3334707694030592,
            "precision": 0.659432387312187,
            "recall": 0.8662280701754386
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7899900103597302,
            "auditor_fn_violation": 0.012153994683453902,
            "auditor_fp_violation": 0.016805628277469627,
            "ave_precision_score": 0.6959343573609037,
            "fpr": 0.20636663007683864,
            "logloss": 5.474307569820006,
            "mae": 0.3294657172898953,
            "precision": 0.6938110749185668,
            "recall": 0.8554216867469879
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(79)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7357456140350878,
            "auc_prc": 0.7962570543134894,
            "auditor_fn_violation": 0.003539550630963374,
            "auditor_fp_violation": 0.010678766543551866,
            "ave_precision_score": 0.7966816739557343,
            "fpr": 0.12390350877192982,
            "logloss": 0.5440541065860619,
            "mae": 0.3591100158631442,
            "precision": 0.7437641723356009,
            "recall": 0.7192982456140351
        },
        "train": {
            "accuracy": 0.747530186608123,
            "auc_prc": 0.8595634647944457,
            "auditor_fn_violation": 0.00554578357337142,
            "auditor_fp_violation": 0.010806845575864537,
            "ave_precision_score": 0.8598646074070782,
            "fpr": 0.10647639956092206,
            "logloss": 0.5138107002994357,
            "mae": 0.3458959305102801,
            "precision": 0.79004329004329,
            "recall": 0.7329317269076305
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(80)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 0.9210837833090906,
            "mae": 0.49613980685663,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.45334796926454446,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 1.000098784259714,
            "mae": 0.5258014200170423,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(81)",
        "seed": 18998,
        "test": {
            "accuracy": 0.49451754385964913,
            "auc_prc": 0.3228473095552073,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0015605763311788245,
            "ave_precision_score": 0.4944681388945217,
            "fpr": 0.005482456140350877,
            "logloss": 17.043769884010807,
            "mae": 0.5107859530139545,
            "precision": 0.0,
            "recall": 0.0
        },
        "train": {
            "accuracy": 0.4544456641053787,
            "auc_prc": 0.39583485614911096,
            "auditor_fn_violation": 0.0012872566004964033,
            "auditor_fp_violation": 0.000584728486642941,
            "ave_precision_score": 0.5444597657687247,
            "fpr": 0.0010976948408342481,
            "logloss": 18.535437107626667,
            "mae": 0.5525109255365493,
            "precision": 0.6666666666666666,
            "recall": 0.004016064257028112
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(82)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 17.269388197455342,
            "mae": 0.5,
            "precision": 0,
            "recall": 0
        },
        "train": {
            "accuracy": 0.45334796926454446,
            "auc_prc": 0,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0,
            "fpr": 0.0,
            "logloss": 18.880692255395743,
            "mae": 0.5466520307354555,
            "precision": 0,
            "recall": 0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(83)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5109649122807017,
            "auc_prc": 0.5189454387179602,
            "auditor_fn_violation": 0.0026763042474607602,
            "auditor_fp_violation": 0.015536030317020633,
            "ave_precision_score": 0.49899269360242215,
            "fpr": 0.4133771929824561,
            "logloss": 0.7215092299218998,
            "mae": 0.49871450309690674,
            "precision": 0.506544502617801,
            "recall": 0.8486842105263158
        },
        "train": {
            "accuracy": 0.5367727771679474,
            "auc_prc": 0.6162102667162538,
            "auditor_fn_violation": 0.008054170579133225,
            "auditor_fp_violation": 0.009733071445847506,
            "ave_precision_score": 0.5635144862702209,
            "fpr": 0.37102085620197583,
            "logloss": 0.7042671165693034,
            "mae": 0.49116368658848836,
            "precision": 0.550531914893617,
            "recall": 0.8313253012048193
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(84)",
        "seed": 18998,
        "test": {
            "accuracy": 0.49122807017543857,
            "auc_prc": 0.5513157894736842,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.002366112650046169,
            "ave_precision_score": 0.618421052631579,
            "fpr": 0.008771929824561403,
            "logloss": 0.9444723590572021,
            "mae": 0.4756119941246876,
            "precision": 0.0,
            "recall": 0.0
        },
        "train": {
            "accuracy": 0.4500548847420417,
            "auc_prc": 0.629481739935251,
            "auditor_fn_violation": 0.000652445126278999,
            "auditor_fp_violation": 0.0021581796870639458,
            "ave_precision_score": 0.6650066179400216,
            "fpr": 0.0043907793633369925,
            "logloss": 1.0119273829990139,
            "mae": 0.5033123891158633,
            "precision": 0.2,
            "recall": 0.002008032128514056
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(85)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5021929824561403,
            "auc_prc": 0.7012892598318615,
            "auditor_fn_violation": 0.0006877116035703294,
            "auditor_fp_violation": 0.0017457294552169855,
            "ave_precision_score": 0.7028084994132632,
            "fpr": 0.4967105263157895,
            "logloss": 0.952332120400187,
            "mae": 0.4603922241612485,
            "precision": 0.501101321585903,
            "recall": 0.9978070175438597
        },
        "train": {
            "accuracy": 0.5488474204171241,
            "auc_prc": 0.6921424663456303,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.000940881292143651,
            "ave_precision_score": 0.6947665493369978,
            "fpr": 0.45115257958287597,
            "logloss": 0.9080016125873437,
            "mae": 0.43107449066233294,
            "precision": 0.5478547854785478,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(86)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5712719298245614,
            "auc_prc": 0.5672990774754127,
            "auditor_fn_violation": 0.0005867189904586137,
            "auditor_fp_violation": 0.012467778547245305,
            "ave_precision_score": 0.6090011299682608,
            "fpr": 0.08442982456140351,
            "logloss": 0.9669688983781949,
            "mae": 0.4378688083563343,
            "precision": 0.6484018264840182,
            "recall": 0.31140350877192985
        },
        "train": {
            "accuracy": 0.5422612513721186,
            "auc_prc": 0.5895206806647241,
            "auditor_fn_violation": 0.009826352611323468,
            "auditor_fp_violation": 0.008018753837280694,
            "ave_precision_score": 0.6428509610738387,
            "fpr": 0.08562019758507135,
            "logloss": 1.0072207784983611,
            "mae": 0.46143520783641173,
            "precision": 0.6708860759493671,
            "recall": 0.3192771084337349
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(87)",
        "seed": 18998,
        "test": {
            "accuracy": 0.5,
            "auc_prc": 0.75,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5,
            "fpr": 0.5,
            "logloss": 0.7036407831616653,
            "mae": 0.5,
            "precision": 0.5,
            "recall": 1.0
        },
        "train": {
            "accuracy": 0.5466520307354555,
            "auc_prc": 0.7733260153677277,
            "auditor_fn_violation": 0.0,
            "auditor_fp_violation": 0.0,
            "ave_precision_score": 0.5466520307354555,
            "fpr": 0.45334796926454446,
            "logloss": 0.6901002071848951,
            "mae": 0.4932768440403609,
            "precision": 0.5466520307354555,
            "recall": 1.0
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(88)",
        "seed": 18998,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.6134941278223707,
            "auditor_fn_violation": 0.0006949253616497391,
            "auditor_fp_violation": 0.012373999692212994,
            "ave_precision_score": 0.6149370491846258,
            "fpr": 0.24232456140350878,
            "logloss": 0.6180014090330873,
            "mae": 0.42329196384602447,
            "precision": 0.6492063492063492,
            "recall": 0.8969298245614035
        },
        "train": {
            "accuracy": 0.712403951701427,
            "auc_prc": 0.6655510531350654,
            "auditor_fn_violation": 0.0065729438059592945,
            "auditor_fp_violation": 0.00470706431747568,
            "ave_precision_score": 0.6668660567292594,
            "fpr": 0.2283205268935236,
            "logloss": 0.6087835696861311,
            "mae": 0.41704040040505097,
            "precision": 0.6809815950920245,
            "recall": 0.891566265060241
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(89)",
        "seed": 18998,
        "test": {
            "accuracy": 0.756578947368421,
            "auc_prc": 0.8301340358992884,
            "auditor_fn_violation": 0.0037944367497691595,
            "auditor_fp_violation": 0.010782163742690063,
            "ave_precision_score": 0.7886356649882129,
            "fpr": 0.10416666666666667,
            "logloss": 0.5425565837219559,
            "mae": 0.3529374436557032,
            "precision": 0.7759433962264151,
            "recall": 0.7214912280701754
        },
        "train": {
            "accuracy": 0.7727771679473107,
            "auc_prc": 0.871655243265784,
            "auditor_fn_violation": 0.00667874571832886,
            "auditor_fp_violation": 0.011513835473350999,
            "ave_precision_score": 0.8388457999451007,
            "fpr": 0.08781558726673985,
            "logloss": 0.5158179698371039,
            "mae": 0.3406579656409903,
            "precision": 0.8226164079822617,
            "recall": 0.7449799196787149
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(90)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7423245614035088,
            "auc_prc": 0.814488126060009,
            "auditor_fn_violation": 0.009007579255155449,
            "auditor_fp_violation": 0.013535414742997848,
            "ave_precision_score": 0.8122250046831047,
            "fpr": 0.08442982456140351,
            "logloss": 0.5407279100532941,
            "mae": 0.3455077017036577,
            "precision": 0.7946666666666666,
            "recall": 0.6535087719298246
        },
        "train": {
            "accuracy": 0.7442371020856202,
            "auc_prc": 0.8377354922121598,
            "auditor_fn_violation": 0.011142263896419927,
            "auditor_fp_violation": 0.007330368937096506,
            "ave_precision_score": 0.8412971137493132,
            "fpr": 0.06805708013172337,
            "logloss": 0.5435680725770091,
            "mae": 0.34787865354793,
            "precision": 0.8406169665809768,
            "recall": 0.6566265060240963
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(91)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8059261778762582,
            "auditor_fn_violation": 0.0038737880886426626,
            "auditor_fp_violation": 0.011065904893813483,
            "ave_precision_score": 0.7904434564696117,
            "fpr": 0.10307017543859649,
            "logloss": 0.5403890555147082,
            "mae": 0.3495329942456202,
            "precision": 0.7756563245823389,
            "recall": 0.7127192982456141
        },
        "train": {
            "accuracy": 0.7782656421514819,
            "auc_prc": 0.8430789034236179,
            "auditor_fn_violation": 0.006872715891006399,
            "auditor_fp_violation": 0.011043394827279179,
            "ave_precision_score": 0.8332445497863706,
            "fpr": 0.08122941822173436,
            "logloss": 0.5040974905085864,
            "mae": 0.33244180568912024,
            "precision": 0.8333333333333334,
            "recall": 0.7429718875502008
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(92)",
        "seed": 18998,
        "test": {
            "accuracy": 0.6765350877192983,
            "auc_prc": 0.7845054391899604,
            "auditor_fn_violation": 0.03730955678670361,
            "auditor_fp_violation": 0.009673649584487536,
            "ave_precision_score": 0.7826706105187022,
            "fpr": 0.049342105263157895,
            "logloss": 0.6445041137885195,
            "mae": 0.36566920550295007,
            "precision": 0.8207171314741036,
            "recall": 0.4517543859649123
        },
        "train": {
            "accuracy": 0.6673984632272228,
            "auc_prc": 0.8009268853167085,
            "auditor_fn_violation": 0.03796966130162803,
            "auditor_fp_violation": 0.010572954181207359,
            "ave_precision_score": 0.7991849243080907,
            "fpr": 0.04939626783754116,
            "logloss": 0.6717422420599447,
            "mae": 0.37895929158290326,
            "precision": 0.8421052631578947,
            "recall": 0.4819277108433735
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(93)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7574253497196465,
            "auditor_fn_violation": 0.001243170975684827,
            "auditor_fp_violation": 0.001500461680517082,
            "ave_precision_score": 0.7582219334951963,
            "fpr": 0.1118421052631579,
            "logloss": 0.5813721849725358,
            "mae": 0.39506946154870093,
            "precision": 0.7391304347826086,
            "recall": 0.6337719298245614
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.8399982041471203,
            "auditor_fn_violation": 0.003980796952904933,
            "auditor_fp_violation": 0.015155099231082044,
            "ave_precision_score": 0.840535324826663,
            "fpr": 0.08122941822173436,
            "logloss": 0.5489040484669807,
            "mae": 0.3813810281938046,
            "precision": 0.8131313131313131,
            "recall": 0.6465863453815262
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(94)",
        "seed": 18998,
        "test": {
            "accuracy": 0.668859649122807,
            "auc_prc": 0.7404833325680023,
            "auditor_fn_violation": 0.009320175438596492,
            "auditor_fp_violation": 0.00948849646044937,
            "ave_precision_score": 0.7082219348792175,
            "fpr": 0.15679824561403508,
            "logloss": 0.6450259020049479,
            "mae": 0.40110088001861754,
            "precision": 0.675,
            "recall": 0.6513157894736842
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.8212672103756133,
            "auditor_fn_violation": 0.00394993806179714,
            "auditor_fp_violation": 0.007269238231674743,
            "ave_precision_score": 0.7963786880494044,
            "fpr": 0.13391877058177826,
            "logloss": 0.6232386928170617,
            "mae": 0.3776681079091178,
            "precision": 0.7442348008385744,
            "recall": 0.7128514056224899
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(95)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7620614035087719,
            "auc_prc": 0.8202805500214199,
            "auditor_fn_violation": 0.009031625115420132,
            "auditor_fp_violation": 0.00986842105263158,
            "ave_precision_score": 0.814618826129507,
            "fpr": 0.08333333333333333,
            "logloss": 0.5232572992045834,
            "mae": 0.3254243619433653,
            "precision": 0.8056265984654731,
            "recall": 0.6907894736842105
        },
        "train": {
            "accuracy": 0.7628979143798024,
            "auc_prc": 0.8643234418370339,
            "auditor_fn_violation": 0.011832180533329807,
            "auditor_fp_violation": 0.00885597871588309,
            "ave_precision_score": 0.8581419884266716,
            "fpr": 0.07683863885839737,
            "logloss": 0.5167020064922668,
            "mae": 0.32557126546419674,
            "precision": 0.8341232227488151,
            "recall": 0.7068273092369478
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(96)",
        "seed": 18998,
        "test": {
            "accuracy": 0.4166666666666667,
            "auc_prc": 0.48806181739457744,
            "auditor_fn_violation": 0.010791782086795935,
            "auditor_fp_violation": 0.004636041859033556,
            "ave_precision_score": 0.4781161041218991,
            "fpr": 0.22807017543859648,
            "logloss": 0.7952781535695714,
            "mae": 0.5179143425571454,
            "precision": 0.38823529411764707,
            "recall": 0.2894736842105263
        },
        "train": {
            "accuracy": 0.3545554335894621,
            "auc_prc": 0.48174835816012,
            "auditor_fn_violation": 0.010079836359708898,
            "auditor_fp_violation": 0.005358239223055318,
            "ave_precision_score": 0.4940209967311829,
            "fpr": 0.24259055982436883,
            "logloss": 0.8349026706208269,
            "mae": 0.5380401254092823,
            "precision": 0.3721590909090909,
            "recall": 0.26305220883534136
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(97)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8334095604507142,
            "auditor_fn_violation": 0.007877423822714677,
            "auditor_fp_violation": 0.00548245614035088,
            "ave_precision_score": 0.830381995401838,
            "fpr": 0.08991228070175439,
            "logloss": 0.5087369406034332,
            "mae": 0.3211802096614663,
            "precision": 0.795,
            "recall": 0.6973684210526315
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8531745675999254,
            "auditor_fn_violation": 0.012367802714700742,
            "auditor_fp_violation": 0.012980972403473288,
            "ave_precision_score": 0.8501251987818681,
            "fpr": 0.06805708013172337,
            "logloss": 0.5145177884396518,
            "mae": 0.32064335481657286,
            "precision": 0.8484107579462102,
            "recall": 0.6967871485943775
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(98)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.834432336044463,
            "auditor_fn_violation": 0.0025007694675284703,
            "auditor_fp_violation": 0.005424746075715608,
            "ave_precision_score": 0.7945198038388832,
            "fpr": 0.10526315789473684,
            "logloss": 0.5193965417896862,
            "mae": 0.33219520636556443,
            "precision": 0.7735849056603774,
            "recall": 0.7192982456140351
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8780804405770524,
            "auditor_fn_violation": 0.003938917029258642,
            "auditor_fp_violation": 0.013419518768455494,
            "ave_precision_score": 0.8496138608464224,
            "fpr": 0.0889132821075741,
            "logloss": 0.4846246278217965,
            "mae": 0.3168493369113732,
            "precision": 0.8215859030837004,
            "recall": 0.748995983935743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_random_p100_g0",
        "model": "feat_random_p100_g0:archive(99)",
        "seed": 18998,
        "test": {
            "accuracy": 0.7116228070175439,
            "auc_prc": 0.7758324775289939,
            "auditor_fn_violation": 0.011734379809172055,
            "auditor_fp_violation": 0.02448349492151432,
            "ave_precision_score": 0.72306363456662,
            "fpr": 0.22697368421052633,
            "logloss": 3.1342460565035335,
            "mae": 0.29540029255649036,
            "precision": 0.6589785831960461,
            "recall": 0.8771929824561403
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.8062026646910125,
            "auditor_fn_violation": 0.00955743941738414,
            "auditor_fp_violation": 0.02460643786063794,
            "ave_precision_score": 0.7597371432063718,
            "fpr": 0.22063666300768386,
            "logloss": 2.940090273286157,
            "mae": 0.29345258255744383,
            "precision": 0.6819620253164557,
            "recall": 0.8654618473895582
        }
    }
]
[
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(0)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.8141757743385418,
            "auditor_fn_violation": 0.006422624790081111,
            "auditor_fp_violation": 0.006599783306246616,
            "ave_precision_score": 0.8144764946728504,
            "fpr": 0.07785087719298246,
            "logloss": 0.9405704920184401,
            "mae": 0.3411993241260453,
            "precision": 0.7947976878612717,
            "recall": 0.560081466395112
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7926474591948164,
            "auditor_fn_violation": 0.017005972123766883,
            "auditor_fp_violation": 0.005802101301552455,
            "ave_precision_score": 0.7930937975486221,
            "fpr": 0.07025246981339188,
            "logloss": 0.9831270322134873,
            "mae": 0.3300360852828016,
            "precision": 0.8128654970760234,
            "recall": 0.6004319654427646
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(1)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7598684210526315,
            "auc_prc": 0.8684514919838741,
            "auditor_fn_violation": 0.005529352913852874,
            "auditor_fp_violation": 0.01554100512564071,
            "ave_precision_score": 0.8685700377596184,
            "fpr": 0.08881578947368421,
            "logloss": 0.7269647573809792,
            "mae": 0.2853873541571853,
            "precision": 0.8133640552995391,
            "recall": 0.7189409368635438
        },
        "train": {
            "accuracy": 0.7683863885839737,
            "auc_prc": 0.8354106297087619,
            "auditor_fn_violation": 0.017326034334377294,
            "auditor_fp_violation": 0.014054414301395646,
            "ave_precision_score": 0.8358026554358691,
            "fpr": 0.09001097694840834,
            "logloss": 0.8111768672519827,
            "mae": 0.28784415147784553,
            "precision": 0.8028846153846154,
            "recall": 0.7213822894168467
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(2)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.8159369669316588,
            "auditor_fn_violation": 0.008957283738878764,
            "auditor_fp_violation": 0.013282910363795477,
            "ave_precision_score": 0.816223995756908,
            "fpr": 0.0712719298245614,
            "logloss": 0.9305179931418127,
            "mae": 0.3431295810192639,
            "precision": 0.8093841642228738,
            "recall": 0.5621181262729125
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7913797483761558,
            "auditor_fn_violation": 0.01779308807874953,
            "auditor_fp_violation": 0.007321232554492713,
            "ave_precision_score": 0.7918429800486786,
            "fpr": 0.07464324917672886,
            "logloss": 0.9721546374426676,
            "mae": 0.3322017782156084,
            "precision": 0.8005865102639296,
            "recall": 0.5896328293736501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(3)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6787280701754386,
            "auc_prc": 0.7901607977422652,
            "auditor_fn_violation": 0.004356933576303288,
            "auditor_fp_violation": 0.01721829812059841,
            "ave_precision_score": 0.790570057217132,
            "fpr": 0.10635964912280702,
            "logloss": 0.8251866282802157,
            "mae": 0.3447672773971199,
            "precision": 0.7525510204081632,
            "recall": 0.6008146639511202
        },
        "train": {
            "accuracy": 0.7091108671789242,
            "auc_prc": 0.7676271021914176,
            "auditor_fn_violation": 0.013120179803837425,
            "auditor_fp_violation": 0.006115728398933671,
            "ave_precision_score": 0.7682648087742143,
            "fpr": 0.10537870472008781,
            "logloss": 0.8838917696616747,
            "mae": 0.3376080056657347,
            "precision": 0.7538461538461538,
            "recall": 0.6349892008639308
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(4)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.7915753526332994,
            "auditor_fn_violation": 0.007552613713509844,
            "auditor_fp_violation": 0.014124161353502522,
            "ave_precision_score": 0.7929465722457031,
            "fpr": 0.07456140350877193,
            "logloss": 0.9757199245292562,
            "mae": 0.34798476032220177,
            "precision": 0.8057142857142857,
            "recall": 0.5743380855397149
        },
        "train": {
            "accuracy": 0.7145993413830956,
            "auc_prc": 0.7767726810313712,
            "auditor_fn_violation": 0.012951850789368259,
            "auditor_fp_violation": 0.009330406147091108,
            "ave_precision_score": 0.7772394456717087,
            "fpr": 0.07683863885839737,
            "logloss": 0.9804802576703319,
            "mae": 0.3400568838886214,
            "precision": 0.7959183673469388,
            "recall": 0.5896328293736501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(5)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7489035087719298,
            "auc_prc": 0.8599107899007324,
            "auditor_fn_violation": 0.006413692071318836,
            "auditor_fp_violation": 0.01641611451431429,
            "ave_precision_score": 0.8600529970537029,
            "fpr": 0.09758771929824561,
            "logloss": 0.7796260978080419,
            "mae": 0.2853590650084805,
            "precision": 0.7977272727272727,
            "recall": 0.714867617107943
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8387896713982559,
            "auditor_fn_violation": 0.013414162871361074,
            "auditor_fp_violation": 0.009448016308609067,
            "ave_precision_score": 0.8389862690566059,
            "fpr": 0.0889132821075741,
            "logloss": 0.849048461562273,
            "mae": 0.28460343481211864,
            "precision": 0.8052884615384616,
            "recall": 0.7235421166306696
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(6)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7247807017543859,
            "auc_prc": 0.8121616165883576,
            "auditor_fn_violation": 0.011442812734483875,
            "auditor_fp_violation": 0.022726799183231235,
            "ave_precision_score": 0.8124908139662111,
            "fpr": 0.13157894736842105,
            "logloss": 0.9475941053955622,
            "mae": 0.283972171341482,
            "precision": 0.75,
            "recall": 0.7331975560081466
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7937044193805892,
            "auditor_fn_violation": 0.01165737695978834,
            "auditor_fp_violation": 0.017229888662380434,
            "ave_precision_score": 0.794544681596635,
            "fpr": 0.14489571899012074,
            "logloss": 0.989417444603903,
            "mae": 0.297405894741405,
            "precision": 0.7167381974248928,
            "recall": 0.7213822894168467
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(7)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.8129513212332264,
            "auditor_fn_violation": 0.006775467181191265,
            "auditor_fp_violation": 0.0066779180730924675,
            "ave_precision_score": 0.8132750051611473,
            "fpr": 0.07675438596491228,
            "logloss": 0.9317963266592563,
            "mae": 0.34151713567323144,
            "precision": 0.7971014492753623,
            "recall": 0.560081466395112
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.794158396939758,
            "auditor_fn_violation": 0.01980118209643119,
            "auditor_fp_violation": 0.005802101301552455,
            "ave_precision_score": 0.7946033341481178,
            "fpr": 0.07025246981339188,
            "logloss": 0.9730147935491777,
            "mae": 0.3296821476876398,
            "precision": 0.8160919540229885,
            "recall": 0.6133909287257019
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(8)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.820995293893622,
            "auditor_fn_violation": 0.01215966341515705,
            "auditor_fp_violation": 0.007797849731216404,
            "ave_precision_score": 0.8213217022503504,
            "fpr": 0.07456140350877193,
            "logloss": 0.8872406992671867,
            "mae": 0.3286521708917546,
            "precision": 0.8152173913043478,
            "recall": 0.6109979633401222
        },
        "train": {
            "accuracy": 0.7387486278814489,
            "auc_prc": 0.8010258528889055,
            "auditor_fn_violation": 0.01391914991476863,
            "auditor_fp_violation": 0.009492120119178297,
            "ave_precision_score": 0.8015362239037795,
            "fpr": 0.07793633369923161,
            "logloss": 0.9323614810093345,
            "mae": 0.31869380690480553,
            "precision": 0.8065395095367848,
            "recall": 0.6393088552915767
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(9)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.7849547028443506,
            "auditor_fn_violation": 0.014975703004966597,
            "auditor_fp_violation": 0.023101846064091353,
            "ave_precision_score": 0.7852561641176845,
            "fpr": 0.13706140350877194,
            "logloss": 1.1433233520394919,
            "mae": 0.3238073989510234,
            "precision": 0.7258771929824561,
            "recall": 0.6741344195519349
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7708057672483388,
            "auditor_fn_violation": 0.017636613220228883,
            "auditor_fp_violation": 0.01958699231613612,
            "ave_precision_score": 0.7715677264305927,
            "fpr": 0.16355653128430298,
            "logloss": 1.2167448002297967,
            "mae": 0.3297789574282803,
            "precision": 0.6823027718550106,
            "recall": 0.6911447084233261
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(10)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8286604077318962,
            "auditor_fn_violation": 0.008142173151820494,
            "auditor_fp_violation": 0.015785827395091052,
            "ave_precision_score": 0.8288792641573592,
            "fpr": 0.08442982456140351,
            "logloss": 0.9033722742356337,
            "mae": 0.32424079743575357,
            "precision": 0.8010335917312662,
            "recall": 0.6313645621181263
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8041402008398767,
            "auditor_fn_violation": 0.017980383742736365,
            "auditor_fp_violation": 0.005135643719617379,
            "ave_precision_score": 0.8045310405633206,
            "fpr": 0.08342480790340286,
            "logloss": 0.9554244614138151,
            "mae": 0.3173077466184828,
            "precision": 0.7967914438502673,
            "recall": 0.6436285097192225
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(11)",
        "seed": 4719,
        "test": {
            "accuracy": 0.743421052631579,
            "auc_prc": 0.7345993797618634,
            "auditor_fn_violation": 0.004071086575910242,
            "auditor_fp_violation": 0.025938138100595916,
            "ave_precision_score": 0.7364286865286916,
            "fpr": 0.12609649122807018,
            "logloss": 0.8561521363822515,
            "mae": 0.29682464546966447,
            "precision": 0.7638603696098563,
            "recall": 0.7576374745417516
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.699966007498442,
            "auditor_fn_violation": 0.010649773704162948,
            "auditor_fp_violation": 0.03581229418221734,
            "ave_precision_score": 0.6990189216428698,
            "fpr": 0.14489571899012074,
            "logloss": 1.0933006373847007,
            "mae": 0.30719462083771193,
            "precision": 0.725,
            "recall": 0.7516198704103672
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(12)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.8112231565354815,
            "auditor_fn_violation": 0.004734340944009726,
            "auditor_fp_violation": 0.006365379005709046,
            "ave_precision_score": 0.8115436852366579,
            "fpr": 0.07346491228070176,
            "logloss": 0.9417318388989944,
            "mae": 0.3451486293062628,
            "precision": 0.8029411764705883,
            "recall": 0.5560081466395111
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.789512029880032,
            "auditor_fn_violation": 0.012757442631812289,
            "auditor_fp_violation": 0.012937117766975068,
            "ave_precision_score": 0.7900464745252868,
            "fpr": 0.06805708013172337,
            "logloss": 0.9833210670027601,
            "mae": 0.33360130640696606,
            "precision": 0.8138138138138138,
            "recall": 0.5853131749460043
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(13)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.8128412254945326,
            "auditor_fn_violation": 0.007655339979276101,
            "auditor_fp_violation": 0.007620744259699133,
            "ave_precision_score": 0.8131607797356485,
            "fpr": 0.07456140350877193,
            "logloss": 0.9574411413362083,
            "mae": 0.3433082140315946,
            "precision": 0.8,
            "recall": 0.5539714867617108
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7952312739775808,
            "auditor_fn_violation": 0.01690165555141978,
            "auditor_fp_violation": 0.0063117453347969294,
            "ave_precision_score": 0.7956732029094328,
            "fpr": 0.06915477497255763,
            "logloss": 0.995462687614426,
            "mae": 0.3297644165909058,
            "precision": 0.8147058823529412,
            "recall": 0.5982721382289417
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(14)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6535087719298246,
            "auc_prc": 0.8210927549849986,
            "auditor_fn_violation": 0.014258852324293443,
            "auditor_fp_violation": 0.004688086010751344,
            "ave_precision_score": 0.8213566403956447,
            "fpr": 0.04824561403508772,
            "logloss": 0.9489327383394692,
            "mae": 0.3691931885976437,
            "precision": 0.8326996197718631,
            "recall": 0.4460285132382892
        },
        "train": {
            "accuracy": 0.6750823271130626,
            "auc_prc": 0.794177360077005,
            "auditor_fn_violation": 0.007503680715422047,
            "auditor_fp_violation": 0.007664262192253411,
            "ave_precision_score": 0.7946628747288452,
            "fpr": 0.04171240395170143,
            "logloss": 0.9924865145246805,
            "mae": 0.35810961043980977,
            "precision": 0.8436213991769548,
            "recall": 0.4427645788336933
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(15)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7006578947368421,
            "auc_prc": 0.8195467174495659,
            "auditor_fn_violation": 0.011250759281094801,
            "auditor_fp_violation": 0.015460265866566656,
            "ave_precision_score": 0.8198439428287653,
            "fpr": 0.07236842105263158,
            "logloss": 0.912273761538458,
            "mae": 0.3364545001678188,
            "precision": 0.8114285714285714,
            "recall": 0.5784114052953157
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7968972704551842,
            "auditor_fn_violation": 0.01498839478132638,
            "auditor_fp_violation": 0.01449055198369139,
            "ave_precision_score": 0.7973361043120052,
            "fpr": 0.07244785949506037,
            "logloss": 0.9587598280704764,
            "mae": 0.3270890053651507,
            "precision": 0.811965811965812,
            "recall": 0.6155507559395248
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(16)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8588067661640166,
            "auditor_fn_violation": 0.004705309608032306,
            "auditor_fp_violation": 0.01739800808434388,
            "ave_precision_score": 0.85899883657453,
            "fpr": 0.08552631578947369,
            "logloss": 0.7694250277539261,
            "mae": 0.2865333413979666,
            "precision": 0.8173302107728337,
            "recall": 0.7107942973523421
        },
        "train": {
            "accuracy": 0.7464324917672887,
            "auc_prc": 0.8247991408252084,
            "auditor_fn_violation": 0.012067530755607611,
            "auditor_fp_violation": 0.016303708640426535,
            "ave_precision_score": 0.8252572774130362,
            "fpr": 0.10318331503841932,
            "logloss": 0.7947384458348314,
            "mae": 0.29097728948662116,
            "precision": 0.7761904761904762,
            "recall": 0.7041036717062635
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(17)",
        "seed": 4719,
        "test": {
            "accuracy": 0.5997807017543859,
            "auc_prc": 0.7872694889870063,
            "auditor_fn_violation": 0.0030840211526780292,
            "auditor_fp_violation": 0.015856148685252328,
            "ave_precision_score": 0.7541108376881911,
            "fpr": 0.39364035087719296,
            "logloss": 3.091680478112634,
            "mae": 0.382499662699717,
            "precision": 0.5746445497630331,
            "recall": 0.9877800407331976
        },
        "train": {
            "accuracy": 0.566410537870472,
            "auc_prc": 0.7640608135606927,
            "auditor_fn_violation": 0.0033096803408306924,
            "auditor_fp_violation": 0.01173651403481263,
            "ave_precision_score": 0.7242745291476239,
            "fpr": 0.42371020856201974,
            "logloss": 3.539440597393402,
            "mae": 0.4083465791126441,
            "precision": 0.5404761904761904,
            "recall": 0.980561555075594
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(18)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8788030747821773,
            "auditor_fn_violation": 0.010071640404473512,
            "auditor_fp_violation": 0.009618389798724841,
            "ave_precision_score": 0.8788961483955874,
            "fpr": 0.06578947368421052,
            "logloss": 0.6447272169060405,
            "mae": 0.31574644624142867,
            "precision": 0.8473282442748091,
            "recall": 0.6782077393075356
        },
        "train": {
            "accuracy": 0.7738748627881449,
            "auc_prc": 0.8589132375055315,
            "auditor_fn_violation": 0.007342464194521962,
            "auditor_fp_violation": 0.011545397522345933,
            "ave_precision_score": 0.8590534966297679,
            "fpr": 0.06037321624588365,
            "logloss": 0.7119348487618103,
            "mae": 0.3136548772817181,
            "precision": 0.8501362397820164,
            "recall": 0.673866090712743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(19)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7478070175438597,
            "auc_prc": 0.8555483223532288,
            "auditor_fn_violation": 0.010520509522278218,
            "auditor_fp_violation": 0.01799183231237238,
            "ave_precision_score": 0.8557505337279759,
            "fpr": 0.09320175438596491,
            "logloss": 0.6841766775547365,
            "mae": 0.2946025063394197,
            "precision": 0.802784222737819,
            "recall": 0.7046843177189409
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8331840657977329,
            "auditor_fn_violation": 0.013558783573933187,
            "auditor_fp_violation": 0.010849537400031362,
            "ave_precision_score": 0.833496033878153,
            "fpr": 0.0889132821075741,
            "logloss": 0.7068499323124434,
            "mae": 0.2912037979139245,
            "precision": 0.800982800982801,
            "recall": 0.7041036717062635
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(20)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6589912280701754,
            "auc_prc": 0.8231550131352026,
            "auditor_fn_violation": 0.010442348233108235,
            "auditor_fp_violation": 0.010282535316914615,
            "ave_precision_score": 0.8234060352868344,
            "fpr": 0.049342105263157895,
            "logloss": 0.9322441079538064,
            "mae": 0.36584081122116086,
            "precision": 0.8333333333333334,
            "recall": 0.45824847250509165
        },
        "train": {
            "accuracy": 0.6794731064763996,
            "auc_prc": 0.7970216934689855,
            "auditor_fn_violation": 0.010552569625384976,
            "auditor_fp_violation": 0.009604829857299673,
            "ave_precision_score": 0.797457703753224,
            "fpr": 0.043907793633369926,
            "logloss": 0.9775259711401502,
            "mae": 0.3552954078317446,
            "precision": 0.8406374501992032,
            "recall": 0.4557235421166307
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(21)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.8712514027307012,
            "auditor_fn_violation": 0.008993014613927906,
            "auditor_fp_violation": 0.008675563612118183,
            "ave_precision_score": 0.8713886747693864,
            "fpr": 0.06030701754385965,
            "logloss": 0.7688868824782793,
            "mae": 0.29482899430640347,
            "precision": 0.8533333333333334,
            "recall": 0.6517311608961304
        },
        "train": {
            "accuracy": 0.7508232711306256,
            "auc_prc": 0.849929238935027,
            "auditor_fn_violation": 0.014322191217018781,
            "auditor_fp_violation": 0.008070997334169676,
            "ave_precision_score": 0.8500357875101701,
            "fpr": 0.06366630076838639,
            "logloss": 0.8388207103646287,
            "mae": 0.29436296280719626,
            "precision": 0.8352272727272727,
            "recall": 0.6349892008639308
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(22)",
        "seed": 4719,
        "test": {
            "accuracy": 0.706140350877193,
            "auc_prc": 0.797219123872015,
            "auditor_fn_violation": 0.007525815557222993,
            "auditor_fp_violation": 0.013267283410426304,
            "ave_precision_score": 0.7985725129506676,
            "fpr": 0.07675438596491228,
            "logloss": 0.9821953788474604,
            "mae": 0.33767579962038025,
            "precision": 0.8071625344352618,
            "recall": 0.5967413441955194
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7802970411285131,
            "auditor_fn_violation": 0.015965177231485588,
            "auditor_fp_violation": 0.014821330562960642,
            "ave_precision_score": 0.7804089433963215,
            "fpr": 0.0867178924259056,
            "logloss": 0.9866982114285163,
            "mae": 0.3298163516673174,
            "precision": 0.7887700534759359,
            "recall": 0.6371490280777538
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(23)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6085526315789473,
            "auc_prc": 0.7021657015771134,
            "auditor_fn_violation": 0.011007342694822599,
            "auditor_fp_violation": 0.016973475851148067,
            "ave_precision_score": 0.6919847844210594,
            "fpr": 0.08333333333333333,
            "logloss": 8.519289635015843,
            "mae": 0.41176388794382657,
            "precision": 0.7342657342657343,
            "recall": 0.42769857433808556
        },
        "train": {
            "accuracy": 0.6037321624588364,
            "auc_prc": 0.648948900038806,
            "auditor_fn_violation": 0.002975393143082028,
            "auditor_fp_violation": 0.012380919711463073,
            "ave_precision_score": 0.6425495953213141,
            "fpr": 0.09110867178924259,
            "logloss": 8.801026017936795,
            "mae": 0.4171048341721243,
            "precision": 0.6902985074626866,
            "recall": 0.39956803455723544
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(24)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6524122807017544,
            "auc_prc": 0.8258716682226463,
            "auditor_fn_violation": 0.004959892092757346,
            "auditor_fp_violation": 0.0033545859899154072,
            "ave_precision_score": 0.826117934031353,
            "fpr": 0.05482456140350877,
            "logloss": 0.7578874430347532,
            "mae": 0.38024117701323384,
            "precision": 0.8175182481751825,
            "recall": 0.45621181262729127
        },
        "train": {
            "accuracy": 0.6684961580680571,
            "auc_prc": 0.808605803586977,
            "auditor_fn_violation": 0.0025462726977451048,
            "auditor_fp_violation": 0.0041310569233181755,
            "ave_precision_score": 0.809038064371536,
            "fpr": 0.04171240395170143,
            "logloss": 0.8209132300585836,
            "mae": 0.37108932521028604,
            "precision": 0.8396624472573839,
            "recall": 0.4298056155507559
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(25)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.8197137780040372,
            "auditor_fn_violation": 0.004767838639368276,
            "auditor_fp_violation": 0.007284764762261951,
            "ave_precision_score": 0.8199962787194921,
            "fpr": 0.06578947368421052,
            "logloss": 0.969125852432382,
            "mae": 0.3442536575430794,
            "precision": 0.8170731707317073,
            "recall": 0.5458248472505092
        },
        "train": {
            "accuracy": 0.716794731064764,
            "auc_prc": 0.7977202091062695,
            "auditor_fn_violation": 0.017615275739521534,
            "auditor_fp_violation": 0.006037321624588367,
            "ave_precision_score": 0.7981258091585188,
            "fpr": 0.06476399560922064,
            "logloss": 1.0081593043198824,
            "mae": 0.33313862641296194,
            "precision": 0.8173374613003096,
            "recall": 0.5701943844492441
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(26)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7521929824561403,
            "auc_prc": 0.864678410534764,
            "auditor_fn_violation": 0.010772858827312685,
            "auditor_fp_violation": 0.01651248072675751,
            "ave_precision_score": 0.8647925660040772,
            "fpr": 0.09429824561403509,
            "logloss": 0.7839818196091846,
            "mae": 0.28560763006368706,
            "precision": 0.8032036613272311,
            "recall": 0.714867617107943
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8326429216775981,
            "auditor_fn_violation": 0.01524918621219414,
            "auditor_fp_violation": 0.009460267367100521,
            "ave_precision_score": 0.8329666637252492,
            "fpr": 0.09330406147091108,
            "logloss": 0.8719147458237739,
            "mae": 0.28673104905265084,
            "precision": 0.7931873479318735,
            "recall": 0.7041036717062635
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(27)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.8055124809220007,
            "auditor_fn_violation": 0.006599045985636192,
            "auditor_fp_violation": 0.01356680001666875,
            "ave_precision_score": 0.8059864163451188,
            "fpr": 0.07346491228070176,
            "logloss": 0.8562881685256297,
            "mae": 0.3470111363906797,
            "precision": 0.804093567251462,
            "recall": 0.560081466395112
        },
        "train": {
            "accuracy": 0.7189901207464325,
            "auc_prc": 0.7745128149869718,
            "auditor_fn_violation": 0.01669539323791528,
            "auditor_fp_violation": 0.006272541947624281,
            "ave_precision_score": 0.7753528764502191,
            "fpr": 0.07025246981339188,
            "logloss": 0.8543106677719233,
            "mae": 0.33835114371981273,
            "precision": 0.808955223880597,
            "recall": 0.5853131749460043
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(28)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.8071985733164309,
            "auditor_fn_violation": 0.00303042484010434,
            "auditor_fp_violation": 0.00795672375713631,
            "ave_precision_score": 0.8076358033234022,
            "fpr": 0.07675438596491228,
            "logloss": 0.850891720500208,
            "mae": 0.34472158369031175,
            "precision": 0.803921568627451,
            "recall": 0.5845213849287169
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7894974353392787,
            "auditor_fn_violation": 0.01464699509000861,
            "auditor_fp_violation": 0.012851360357534891,
            "ave_precision_score": 0.7900595629063808,
            "fpr": 0.0801317233809001,
            "logloss": 0.8309992996442028,
            "mae": 0.33564438600705976,
            "precision": 0.7977839335180056,
            "recall": 0.6220302375809935
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(29)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8712502060475881,
            "auditor_fn_violation": 0.005768303140743923,
            "auditor_fp_violation": 0.014569529524523903,
            "ave_precision_score": 0.8713561517508999,
            "fpr": 0.08442982456140351,
            "logloss": 0.7214130880716569,
            "mae": 0.2835335673615953,
            "precision": 0.8166666666666667,
            "recall": 0.6985743380855397
        },
        "train": {
            "accuracy": 0.7661909989023051,
            "auc_prc": 0.8445657924198138,
            "auditor_fn_violation": 0.008653533842429824,
            "auditor_fp_violation": 0.004682354555433593,
            "ave_precision_score": 0.8446997175157407,
            "fpr": 0.0845225027442371,
            "logloss": 0.8000434394763368,
            "mae": 0.28571476438258464,
            "precision": 0.8094059405940595,
            "recall": 0.7062634989200864
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(30)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8476028934498469,
            "auditor_fn_violation": 0.015518365669775262,
            "auditor_fp_violation": 0.012931303912989127,
            "ave_precision_score": 0.8477582450535829,
            "fpr": 0.06469298245614036,
            "logloss": 0.6956922089821436,
            "mae": 0.3560950295599059,
            "precision": 0.834733893557423,
            "recall": 0.6069246435845214
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.8214592399462487,
            "auditor_fn_violation": 0.0018302816784536535,
            "auditor_fp_violation": 0.0035822095029010544,
            "ave_precision_score": 0.8218885559555388,
            "fpr": 0.05817782656421515,
            "logloss": 0.7636968838798079,
            "mae": 0.3519704254458093,
            "precision": 0.8374233128834356,
            "recall": 0.5896328293736501
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(31)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.6988844274249619,
            "auditor_fn_violation": 0.017048093757816133,
            "auditor_fp_violation": 0.01129047380922616,
            "ave_precision_score": 0.6963252685839012,
            "fpr": 0.14692982456140352,
            "logloss": 1.7278623261287136,
            "mae": 0.3343297039784231,
            "precision": 0.7237113402061855,
            "recall": 0.714867617107943
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.6591398379409946,
            "auditor_fn_violation": 0.01269343018969021,
            "auditor_fp_violation": 0.0052875568449114035,
            "ave_precision_score": 0.654593650238393,
            "fpr": 0.1602634467618002,
            "logloss": 2.0187751717795543,
            "mae": 0.33747887490855055,
            "precision": 0.6951983298538622,
            "recall": 0.7192224622030238
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(32)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6611842105263158,
            "auc_prc": 0.8134238277154416,
            "auditor_fn_violation": 0.006567781469968204,
            "auditor_fp_violation": 0.00360201275159395,
            "ave_precision_score": 0.8136909801443634,
            "fpr": 0.05592105263157895,
            "logloss": 0.7647539959422696,
            "mae": 0.38619809226913965,
            "precision": 0.8204225352112676,
            "recall": 0.4745417515274949
        },
        "train": {
            "accuracy": 0.6652030735455543,
            "auc_prc": 0.7915924543738229,
            "auditor_fn_violation": 0.0022759979421185395,
            "auditor_fp_violation": 0.004008546338403641,
            "ave_precision_score": 0.7921434672685124,
            "fpr": 0.04720087815587267,
            "logloss": 0.823320634738943,
            "mae": 0.37590096487012475,
            "precision": 0.8237704918032787,
            "recall": 0.43412526997840173
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(33)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6929824561403509,
            "auc_prc": 0.8160238482479932,
            "auditor_fn_violation": 0.0016257548147354188,
            "auditor_fp_violation": 0.009607971829812061,
            "ave_precision_score": 0.8163416772891708,
            "fpr": 0.0756578947368421,
            "logloss": 0.9425443918860534,
            "mae": 0.3392060791777607,
            "precision": 0.8022922636103151,
            "recall": 0.570264765784114
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.7974954460361218,
            "auditor_fn_violation": 0.019357836663956015,
            "auditor_fp_violation": 0.009933158224870633,
            "ave_precision_score": 0.7979459502496788,
            "fpr": 0.06695938529088913,
            "logloss": 0.983507989828354,
            "mae": 0.3273245850123503,
            "precision": 0.8247126436781609,
            "recall": 0.6198704103671706
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(34)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6567982456140351,
            "auc_prc": 0.8227623529985257,
            "auditor_fn_violation": 0.014165058777289451,
            "auditor_fp_violation": 0.010282535316914615,
            "ave_precision_score": 0.8230141488530867,
            "fpr": 0.049342105263157895,
            "logloss": 0.9365457685406686,
            "mae": 0.36671871158078795,
            "precision": 0.832089552238806,
            "recall": 0.45417515274949083
        },
        "train": {
            "accuracy": 0.677277716794731,
            "auc_prc": 0.7960843325051824,
            "auditor_fn_violation": 0.009021012676834372,
            "auditor_fp_violation": 0.009604829857299673,
            "ave_precision_score": 0.796558969908377,
            "fpr": 0.043907793633369926,
            "logloss": 0.9813781123274202,
            "mae": 0.3560272483986955,
            "precision": 0.8393574297188755,
            "recall": 0.4514038876889849
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(35)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.8137823635351674,
            "auditor_fn_violation": 0.006478454282345374,
            "auditor_fp_violation": 0.008363024544734761,
            "ave_precision_score": 0.8141072751064513,
            "fpr": 0.07456140350877193,
            "logloss": 0.948489886577762,
            "mae": 0.3410106056136408,
            "precision": 0.8028985507246377,
            "recall": 0.5641547861507128
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7942321723416755,
            "auditor_fn_violation": 0.017240684411547847,
            "auditor_fp_violation": 0.005361063195860125,
            "ave_precision_score": 0.7946565415923524,
            "fpr": 0.07244785949506037,
            "logloss": 0.9888974896721909,
            "mae": 0.32916816544616245,
            "precision": 0.8097982708933718,
            "recall": 0.6069114470842333
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(36)",
        "seed": 4719,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8310626336829523,
            "auditor_fn_violation": 0.009470915067710013,
            "auditor_fp_violation": 0.01600981372671584,
            "ave_precision_score": 0.8312967036711627,
            "fpr": 0.08223684210526316,
            "logloss": 0.8953626634367716,
            "mae": 0.3199091932528085,
            "precision": 0.8106060606060606,
            "recall": 0.6537678207739308
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.8091840846269025,
            "auditor_fn_violation": 0.018637103982285155,
            "auditor_fp_violation": 0.004018347185196805,
            "ave_precision_score": 0.809538449543908,
            "fpr": 0.08342480790340286,
            "logloss": 0.9491826378089991,
            "mae": 0.3129818194769607,
            "precision": 0.798941798941799,
            "recall": 0.652267818574514
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(37)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.810681147243044,
            "auditor_fn_violation": 0.008206935362847043,
            "auditor_fp_violation": 0.013019856648747765,
            "ave_precision_score": 0.8109984481257726,
            "fpr": 0.08771929824561403,
            "logloss": 0.9491241010249132,
            "mae": 0.33696534732500066,
            "precision": 0.7831978319783198,
            "recall": 0.5885947046843177
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7886318941142976,
            "auditor_fn_violation": 0.016387185183253397,
            "auditor_fp_violation": 0.005135643719617379,
            "ave_precision_score": 0.7891402757967841,
            "fpr": 0.08342480790340286,
            "logloss": 0.9904119824411861,
            "mae": 0.3257121510350052,
            "precision": 0.7929155313351499,
            "recall": 0.6285097192224622
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(38)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.8249869920790547,
            "auditor_fn_violation": 0.009734430271197345,
            "auditor_fp_violation": 0.01389236154519315,
            "ave_precision_score": 0.8251847507540628,
            "fpr": 0.07785087719298246,
            "logloss": 0.9249938039399703,
            "mae": 0.3386476509039743,
            "precision": 0.8,
            "recall": 0.5784114052953157
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.7946600704413236,
            "auditor_fn_violation": 0.015336906966213289,
            "auditor_fp_violation": 0.007068860749568768,
            "ave_precision_score": 0.7949551957646112,
            "fpr": 0.07793633369923161,
            "logloss": 0.9749691007571157,
            "mae": 0.3324820625025001,
            "precision": 0.7994350282485876,
            "recall": 0.6112311015118791
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(39)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8339240551120858,
            "auditor_fn_violation": 0.011148033015328544,
            "auditor_fp_violation": 0.021851689794557656,
            "ave_precision_score": 0.834203850823976,
            "fpr": 0.1337719298245614,
            "logloss": 0.8226816254609052,
            "mae": 0.264856094438498,
            "precision": 0.761252446183953,
            "recall": 0.7922606924643585
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.813263966854267,
            "auditor_fn_violation": 0.01302060489386974,
            "auditor_fp_violation": 0.016864807119335114,
            "ave_precision_score": 0.8138533827126861,
            "fpr": 0.1525795828759605,
            "logloss": 0.8821399440173167,
            "mae": 0.2868014203712332,
            "precision": 0.718052738336714,
            "recall": 0.7645788336933045
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(40)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7719298245614035,
            "auc_prc": 0.857432978845192,
            "auditor_fn_violation": 0.007490084682173871,
            "auditor_fp_violation": 0.012767220902612827,
            "ave_precision_score": 0.857694354729936,
            "fpr": 0.10964912280701754,
            "logloss": 0.49338909119292357,
            "mae": 0.33153505493562496,
            "precision": 0.7929606625258799,
            "recall": 0.780040733197556
        },
        "train": {
            "accuracy": 0.7607025246981339,
            "auc_prc": 0.8181258531479765,
            "auditor_fn_violation": 0.004827012302243046,
            "auditor_fp_violation": 0.004920025090167795,
            "ave_precision_score": 0.8195695843219366,
            "fpr": 0.12184412733260154,
            "logloss": 0.5088453891507494,
            "mae": 0.3353624750346837,
            "precision": 0.7623126338329764,
            "recall": 0.7688984881209503
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(41)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8655728260562661,
            "auditor_fn_violation": 0.009390520598849471,
            "auditor_fp_violation": 0.014548693586698342,
            "ave_precision_score": 0.8657416931484436,
            "fpr": 0.0712719298245614,
            "logloss": 0.729571561504536,
            "mae": 0.3013579508586613,
            "precision": 0.8333333333333334,
            "recall": 0.6619144602851323
        },
        "train": {
            "accuracy": 0.7486278814489572,
            "auc_prc": 0.8336819950908922,
            "auditor_fn_violation": 0.013153371440493335,
            "auditor_fp_violation": 0.012652893209973343,
            "ave_precision_score": 0.833963127172729,
            "fpr": 0.07244785949506037,
            "logloss": 0.70474480205396,
            "mae": 0.30364780358666793,
            "precision": 0.819672131147541,
            "recall": 0.6479481641468683
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(42)",
        "seed": 4719,
        "test": {
            "accuracy": 0.643640350877193,
            "auc_prc": 0.8001989107951745,
            "auditor_fn_violation": 0.009205166684532104,
            "auditor_fp_violation": 0.0033415635287744297,
            "ave_precision_score": 0.8005791619289142,
            "fpr": 0.05592105263157895,
            "logloss": 0.9803304377367394,
            "mae": 0.37517421355412234,
            "precision": 0.8097014925373134,
            "recall": 0.4419551934826884
        },
        "train": {
            "accuracy": 0.6575192096597146,
            "auc_prc": 0.7685499770715768,
            "auditor_fn_violation": 0.001756785911572738,
            "auditor_fp_violation": 0.0007718166849615813,
            "ave_precision_score": 0.7690287960876033,
            "fpr": 0.05378704720087816,
            "logloss": 1.0201092278524597,
            "mae": 0.3668415992043843,
            "precision": 0.8032128514056225,
            "recall": 0.4319654427645788
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(43)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.8134521889533511,
            "auditor_fn_violation": 0.008021581448529679,
            "auditor_fp_violation": 0.015694670167104224,
            "ave_precision_score": 0.8137586938290395,
            "fpr": 0.08552631578947369,
            "logloss": 0.930396534932846,
            "mae": 0.3358065122247892,
            "precision": 0.7897574123989218,
            "recall": 0.5967413441955194
        },
        "train": {
            "accuracy": 0.7343578485181119,
            "auc_prc": 0.7936063129387968,
            "auditor_fn_violation": 0.016816305628590322,
            "auditor_fp_violation": 0.007308981496001258,
            "ave_precision_score": 0.7940945185016737,
            "fpr": 0.08232711306256861,
            "logloss": 0.9720885062381743,
            "mae": 0.3241316481002941,
            "precision": 0.7978436657681941,
            "recall": 0.6393088552915767
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(44)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6699561403508771,
            "auc_prc": 0.7107688830536603,
            "auditor_fn_violation": 0.009645103083574523,
            "auditor_fp_violation": 0.022075676126182445,
            "ave_precision_score": 0.7116982939187758,
            "fpr": 0.1337719298245614,
            "logloss": 0.8944494514552476,
            "mae": 0.3769924507472624,
            "precision": 0.7188940092165899,
            "recall": 0.6354378818737271
        },
        "train": {
            "accuracy": 0.6838638858397366,
            "auc_prc": 0.700303010252203,
            "auditor_fn_violation": 0.012231118107697378,
            "auditor_fp_violation": 0.014816430139564055,
            "ave_precision_score": 0.7011751414202503,
            "fpr": 0.1525795828759605,
            "logloss": 0.9032953806704843,
            "mae": 0.3745319381145772,
            "precision": 0.6931567328918322,
            "recall": 0.6781857451403888
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(45)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.8135054061583116,
            "auditor_fn_violation": 0.005859863508057323,
            "auditor_fp_violation": 0.01596032837438013,
            "ave_precision_score": 0.8138526982856658,
            "fpr": 0.07346491228070176,
            "logloss": 0.9301606464597633,
            "mae": 0.34153136301039777,
            "precision": 0.8052325581395349,
            "recall": 0.5641547861507128
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.7925200517321592,
            "auditor_fn_violation": 0.017228830255599317,
            "auditor_fp_violation": 0.010976948408342485,
            "ave_precision_score": 0.7930428492305452,
            "fpr": 0.07025246981339188,
            "logloss": 0.9736154418738436,
            "mae": 0.330954420732743,
            "precision": 0.815028901734104,
            "recall": 0.6090712742980562
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(46)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.8078958185366285,
            "auditor_fn_violation": 0.0037606745989209366,
            "auditor_fp_violation": 0.0074514522648664455,
            "ave_precision_score": 0.8082186106948939,
            "fpr": 0.0756578947368421,
            "logloss": 0.965399207809354,
            "mae": 0.3516415858438322,
            "precision": 0.7964601769911505,
            "recall": 0.5498981670061099
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7798810476136915,
            "auditor_fn_violation": 0.01495046148229108,
            "auditor_fp_violation": 0.006637623490669593,
            "ave_precision_score": 0.7804016038308136,
            "fpr": 0.06915477497255763,
            "logloss": 1.0116863452253189,
            "mae": 0.34151633159118716,
            "precision": 0.8119402985074626,
            "recall": 0.5874730021598272
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(47)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6853070175438597,
            "auc_prc": 0.8136803621726009,
            "auditor_fn_violation": 0.007090345517561731,
            "auditor_fp_violation": 0.006881068466891698,
            "ave_precision_score": 0.8139810103207786,
            "fpr": 0.08114035087719298,
            "logloss": 0.9304978314716726,
            "mae": 0.3385991441102162,
            "precision": 0.7897727272727273,
            "recall": 0.5661914460285132
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7949898401580848,
            "auditor_fn_violation": 0.016546030872963767,
            "auditor_fp_violation": 0.0056844911400345,
            "ave_precision_score": 0.7954517131935017,
            "fpr": 0.07244785949506037,
            "logloss": 0.9713982964568159,
            "mae": 0.3261669426459442,
            "precision": 0.8125,
            "recall": 0.6177105831533477
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(48)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7456140350877193,
            "auc_prc": 0.8648215369959965,
            "auditor_fn_violation": 0.011364651445313897,
            "auditor_fp_violation": 0.014616410384631413,
            "ave_precision_score": 0.8649396727788974,
            "fpr": 0.07346491228070176,
            "logloss": 0.7547842225810577,
            "mae": 0.2947680281895293,
            "precision": 0.8295165394402035,
            "recall": 0.6639511201629328
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8338341091662926,
            "auditor_fn_violation": 0.009139554236319718,
            "auditor_fp_violation": 0.008546338403638077,
            "ave_precision_score": 0.8340900640261335,
            "fpr": 0.06805708013172337,
            "logloss": 0.8307152406343131,
            "mae": 0.2961688129480585,
            "precision": 0.8301369863013699,
            "recall": 0.6544276457883369
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(49)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6425438596491229,
            "auc_prc": 0.8233531583366942,
            "auditor_fn_violation": 0.0006498552899560851,
            "auditor_fp_violation": 0.008133829228653583,
            "ave_precision_score": 0.8236206714194448,
            "fpr": 0.039473684210526314,
            "logloss": 0.9458420810743434,
            "mae": 0.3736504807525347,
            "precision": 0.8481012658227848,
            "recall": 0.4093686354378819
        },
        "train": {
            "accuracy": 0.6619099890230515,
            "auc_prc": 0.7958541498179218,
            "auditor_fn_violation": 0.00025604976848833926,
            "auditor_fp_violation": 0.008198408342480791,
            "ave_precision_score": 0.7963608643408988,
            "fpr": 0.036223929747530186,
            "logloss": 0.993261072521239,
            "mae": 0.3639491429313727,
            "precision": 0.8506787330316742,
            "recall": 0.4060475161987041
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(50)",
        "seed": 4719,
        "test": {
            "accuracy": 0.5175438596491229,
            "auc_prc": 0.5674083992005852,
            "auditor_fn_violation": 0.008291796191088722,
            "auditor_fp_violation": 0.02468277284660583,
            "ave_precision_score": 0.5687827569942032,
            "fpr": 0.125,
            "logloss": 1.3942294790200669,
            "mae": 0.48650599882686246,
            "precision": 0.5913978494623656,
            "recall": 0.3360488798370672
        },
        "train": {
            "accuracy": 0.5170142700329309,
            "auc_prc": 0.5530311225818262,
            "auditor_fn_violation": 0.0102846657009481,
            "auditor_fp_violation": 0.01362807746589306,
            "ave_precision_score": 0.5545645791267622,
            "fpr": 0.14270032930845225,
            "logloss": 1.3973503549360995,
            "mae": 0.476456987476453,
            "precision": 0.5406360424028268,
            "recall": 0.3304535637149028
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(51)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7368421052631579,
            "auc_prc": 0.8205825473212472,
            "auditor_fn_violation": 0.011112302140279424,
            "auditor_fp_violation": 0.020369733716714588,
            "ave_precision_score": 0.8208903235194082,
            "fpr": 0.12828947368421054,
            "logloss": 0.8382368507164556,
            "mae": 0.3048379996053104,
            "precision": 0.7587628865979381,
            "recall": 0.7494908350305499
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8008601644429585,
            "auditor_fn_violation": 0.006804285514458512,
            "auditor_fp_violation": 0.012946918613768232,
            "ave_precision_score": 0.8012220200642175,
            "fpr": 0.1350164654226125,
            "logloss": 0.8890411433492759,
            "mae": 0.3024046595886083,
            "precision": 0.7349137931034483,
            "recall": 0.7365010799136069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(52)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.8167816129371778,
            "auditor_fn_violation": 0.007552613713509844,
            "auditor_fp_violation": 0.009607971829812061,
            "ave_precision_score": 0.8170982882637843,
            "fpr": 0.0756578947368421,
            "logloss": 0.9409465601421882,
            "mae": 0.3371599268855294,
            "precision": 0.8034188034188035,
            "recall": 0.5743380855397149
        },
        "train": {
            "accuracy": 0.7398463227222832,
            "auc_prc": 0.7984537606556311,
            "auditor_fn_violation": 0.016797338979072676,
            "auditor_fp_violation": 0.005895209346087504,
            "ave_precision_score": 0.7988646986030143,
            "fpr": 0.07135016465422613,
            "logloss": 0.9830843215771464,
            "mae": 0.32591901330838646,
            "precision": 0.8174157303370787,
            "recall": 0.6285097192224622
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(53)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7543859649122807,
            "auc_prc": 0.8677422920986131,
            "auditor_fn_violation": 0.008557544574266622,
            "auditor_fp_violation": 0.012337479684960625,
            "ave_precision_score": 0.8678516846869615,
            "fpr": 0.06798245614035088,
            "logloss": 0.778916471913085,
            "mae": 0.29377810073757654,
            "precision": 0.8414322250639387,
            "recall": 0.670061099796334
        },
        "train": {
            "accuracy": 0.7585071350164654,
            "auc_prc": 0.8352858283000962,
            "auditor_fn_violation": 0.01583478151605172,
            "auditor_fp_violation": 0.010847087188333073,
            "ave_precision_score": 0.8355819987028531,
            "fpr": 0.07574094401756312,
            "logloss": 0.8550063462325419,
            "mae": 0.2941301119705157,
            "precision": 0.8188976377952756,
            "recall": 0.673866090712743
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(54)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7203947368421053,
            "auc_prc": 0.825231108370259,
            "auditor_fn_violation": 0.013260621002608367,
            "auditor_fp_violation": 0.01687450514647664,
            "ave_precision_score": 0.8254467052041038,
            "fpr": 0.10197368421052631,
            "logloss": 0.9096920378422323,
            "mae": 0.32021020581926185,
            "precision": 0.7796208530805687,
            "recall": 0.670061099796334
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7906205278709628,
            "auditor_fn_violation": 0.012807230086796134,
            "auditor_fp_violation": 0.010531009879253568,
            "ave_precision_score": 0.7910815976590351,
            "fpr": 0.10208562019758508,
            "logloss": 0.9755075308004855,
            "mae": 0.31762404123608867,
            "precision": 0.7680798004987531,
            "recall": 0.6652267818574514
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(55)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6743421052631579,
            "auc_prc": 0.7957515504294517,
            "auditor_fn_violation": 0.00466287919391146,
            "auditor_fp_violation": 0.009381381005959082,
            "ave_precision_score": 0.7961323657310637,
            "fpr": 0.08881578947368421,
            "logloss": 0.7486058802127858,
            "mae": 0.3682695776040323,
            "precision": 0.7724719101123596,
            "recall": 0.560081466395112
        },
        "train": {
            "accuracy": 0.7058177826564215,
            "auc_prc": 0.7836373826662641,
            "auditor_fn_violation": 0.009047091819921154,
            "auditor_fp_violation": 0.0034327465893053163,
            "ave_precision_score": 0.784639149144133,
            "fpr": 0.07135016465422613,
            "logloss": 0.8070137417851253,
            "mae": 0.3547410416740384,
            "precision": 0.8,
            "recall": 0.5615550755939525
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(56)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.8169847127158106,
            "auditor_fn_violation": 0.0050559188194518965,
            "auditor_fp_violation": 0.009607971829812061,
            "ave_precision_score": 0.8172981213196936,
            "fpr": 0.0756578947368421,
            "logloss": 0.942624365002312,
            "mae": 0.3383248762606176,
            "precision": 0.8034188034188035,
            "recall": 0.5743380855397149
        },
        "train": {
            "accuracy": 0.7409440175631175,
            "auc_prc": 0.7986796020778706,
            "auditor_fn_violation": 0.01803491286009963,
            "auditor_fp_violation": 0.009933158224870633,
            "ave_precision_score": 0.7990877665367574,
            "fpr": 0.06695938529088913,
            "logloss": 0.9842418694287585,
            "mae": 0.32672482211377973,
            "precision": 0.8252148997134671,
            "recall": 0.6220302375809935
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(57)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7236842105263158,
            "auc_prc": 0.8358877473950795,
            "auditor_fn_violation": 0.014895308536106064,
            "auditor_fp_violation": 0.009865816560403388,
            "ave_precision_score": 0.8361393557201737,
            "fpr": 0.051535087719298246,
            "logloss": 0.9392857253414006,
            "mae": 0.3413043463793624,
            "precision": 0.8588588588588588,
            "recall": 0.5824847250509165
        },
        "train": {
            "accuracy": 0.7244785949506037,
            "auc_prc": 0.822249987194355,
            "auditor_fn_violation": 0.013487658638241994,
            "auditor_fp_violation": 0.00858554179081073,
            "ave_precision_score": 0.8224536772132958,
            "fpr": 0.052689352360043906,
            "logloss": 0.9986347525467705,
            "mae": 0.3326835314841682,
            "precision": 0.8441558441558441,
            "recall": 0.5615550755939525
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(58)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.8082894420232112,
            "auditor_fn_violation": 0.0068446957515989615,
            "auditor_fp_violation": 0.014171042213610034,
            "ave_precision_score": 0.8086109502178627,
            "fpr": 0.07675438596491228,
            "logloss": 0.9617714297932011,
            "mae": 0.35206086192327174,
            "precision": 0.7935103244837758,
            "recall": 0.5478615071283096
        },
        "train": {
            "accuracy": 0.721185510428101,
            "auc_prc": 0.7804221030605465,
            "auditor_fn_violation": 0.01495046148229108,
            "auditor_fp_violation": 0.006637623490669593,
            "ave_precision_score": 0.7809356585199912,
            "fpr": 0.06915477497255763,
            "logloss": 1.0082522254383364,
            "mae": 0.3419147557724061,
            "precision": 0.8119402985074626,
            "recall": 0.5874730021598272
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(59)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.8129304330530893,
            "auditor_fn_violation": 0.006648175938828743,
            "auditor_fp_violation": 0.006534671000541734,
            "ave_precision_score": 0.8132346066959997,
            "fpr": 0.0756578947368421,
            "logloss": 0.9431350945638745,
            "mae": 0.3429252194020545,
            "precision": 0.8005780346820809,
            "recall": 0.5641547861507128
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7909767243626118,
            "auditor_fn_violation": 0.01923929510447068,
            "auditor_fp_violation": 0.007429041869217501,
            "ave_precision_score": 0.7914200395265784,
            "fpr": 0.07244785949506037,
            "logloss": 0.9808500052120355,
            "mae": 0.33109514794375866,
            "precision": 0.8097982708933718,
            "recall": 0.6069114470842333
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(60)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7192982456140351,
            "auc_prc": 0.8231723133456144,
            "auditor_fn_violation": 0.006835763032836685,
            "auditor_fp_violation": 0.022349147810142942,
            "ave_precision_score": 0.8234064712166845,
            "fpr": 0.11074561403508772,
            "logloss": 0.8970936014755457,
            "mae": 0.3065392212224786,
            "precision": 0.7688787185354691,
            "recall": 0.6843177189409368
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.8139727107489434,
            "auditor_fn_violation": 0.01707472622826837,
            "auditor_fp_violation": 0.0033665908734514657,
            "ave_precision_score": 0.8142557347483099,
            "fpr": 0.11745334796926454,
            "logloss": 0.945438678194399,
            "mae": 0.30060180178273105,
            "precision": 0.7517401392111369,
            "recall": 0.6997840172786177
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(61)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6995614035087719,
            "auc_prc": 0.8134727520257637,
            "auditor_fn_violation": 0.011393682781291322,
            "auditor_fp_violation": 0.014176251198066429,
            "ave_precision_score": 0.8137771907671766,
            "fpr": 0.09320175438596491,
            "logloss": 0.9146328725869836,
            "mae": 0.3302212531299726,
            "precision": 0.7803617571059431,
            "recall": 0.615071283095723
        },
        "train": {
            "accuracy": 0.7299670691547749,
            "auc_prc": 0.7884429727713569,
            "auditor_fn_violation": 0.015564506760425135,
            "auditor_fp_violation": 0.003998745491610474,
            "ave_precision_score": 0.7889440542535099,
            "fpr": 0.08781558726673985,
            "logloss": 0.9640560925512797,
            "mae": 0.3217236170051222,
            "precision": 0.7877984084880637,
            "recall": 0.6414686825053996
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(62)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6754385964912281,
            "auc_prc": 0.8193516444245671,
            "auditor_fn_violation": 0.00585763032836675,
            "auditor_fp_violation": 0.004570883860482561,
            "ave_precision_score": 0.8195997882513877,
            "fpr": 0.07346491228070176,
            "logloss": 0.7327073584826252,
            "mae": 0.37480105782309253,
            "precision": 0.7963525835866262,
            "recall": 0.5336048879837068
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7932530460429177,
            "auditor_fn_violation": 0.003693754993563202,
            "auditor_fp_violation": 0.007250176415242281,
            "ave_precision_score": 0.7937672267095048,
            "fpr": 0.06037321624588365,
            "logloss": 0.8002375399978421,
            "mae": 0.3686132733756114,
            "precision": 0.8178807947019867,
            "recall": 0.5334773218142549
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(63)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7324561403508771,
            "auc_prc": 0.819210894448549,
            "auditor_fn_violation": 0.008519580519526927,
            "auditor_fp_violation": 0.020671854815185232,
            "ave_precision_score": 0.8194734449494616,
            "fpr": 0.19188596491228072,
            "logloss": 0.7244768736139016,
            "mae": 0.3266628981080614,
            "precision": 0.7068676716917923,
            "recall": 0.8594704684317719
        },
        "train": {
            "accuracy": 0.7233809001097695,
            "auc_prc": 0.7835744031447092,
            "auditor_fn_violation": 0.008523138126995948,
            "auditor_fp_violation": 0.013436960953426383,
            "ave_precision_score": 0.784135836255327,
            "fpr": 0.1942919868276619,
            "logloss": 0.8185890681353005,
            "mae": 0.3392526577153798,
            "precision": 0.6867256637168142,
            "recall": 0.838012958963283
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(64)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6962719298245614,
            "auc_prc": 0.8125571331127805,
            "auditor_fn_violation": 0.012155197055775906,
            "auditor_fp_violation": 0.014603387923490444,
            "ave_precision_score": 0.8128636437832757,
            "fpr": 0.09429824561403509,
            "logloss": 0.9236440830941697,
            "mae": 0.330509296321452,
            "precision": 0.7772020725388601,
            "recall": 0.6109979633401222
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7864185832278758,
            "auditor_fn_violation": 0.013210271389046296,
            "auditor_fp_violation": 0.003895836600282264,
            "ave_precision_score": 0.7869249682869474,
            "fpr": 0.09001097694840834,
            "logloss": 0.9729159924056646,
            "mae": 0.3223905627720241,
            "precision": 0.7842105263157895,
            "recall": 0.6436285097192225
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(65)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7160087719298246,
            "auc_prc": 0.8275926670003795,
            "auditor_fn_violation": 0.00665040911851932,
            "auditor_fp_violation": 0.017272992457390512,
            "ave_precision_score": 0.8278233687116845,
            "fpr": 0.09100877192982457,
            "logloss": 0.8978739846144229,
            "mae": 0.31841695654379454,
            "precision": 0.7914572864321608,
            "recall": 0.6415478615071283
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.8010688082939078,
            "auditor_fn_violation": 0.013703404276505302,
            "auditor_fp_violation": 0.009467618002195392,
            "ave_precision_score": 0.8014354594788182,
            "fpr": 0.09659714599341383,
            "logloss": 0.9627488767659331,
            "mae": 0.31586600251722113,
            "precision": 0.7708333333333334,
            "recall": 0.6393088552915767
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(66)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7587719298245614,
            "auc_prc": 0.8752756574405232,
            "auditor_fn_violation": 0.0008151105870582828,
            "auditor_fp_violation": 0.009634016752094013,
            "ave_precision_score": 0.8753615440336239,
            "fpr": 0.08442982456140351,
            "logloss": 0.6327413620792364,
            "mae": 0.3169632717091876,
            "precision": 0.8188235294117647,
            "recall": 0.7087576374745418
        },
        "train": {
            "accuracy": 0.7574094401756312,
            "auc_prc": 0.8529321240550991,
            "auditor_fn_violation": 0.013039571543387403,
            "auditor_fp_violation": 0.010212482358475774,
            "ave_precision_score": 0.8531525624674526,
            "fpr": 0.07903402854006586,
            "logloss": 0.7047024281593777,
            "mae": 0.3141355978403727,
            "precision": 0.8134715025906736,
            "recall": 0.6781857451403888
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(67)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6951754385964912,
            "auc_prc": 0.8153817586653089,
            "auditor_fn_violation": 0.006500786079251081,
            "auditor_fp_violation": 0.01596032837438013,
            "ave_precision_score": 0.8157017516614626,
            "fpr": 0.07346491228070176,
            "logloss": 0.9226846792375829,
            "mae": 0.3397431105502892,
            "precision": 0.8069164265129684,
            "recall": 0.570264765784114
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7935982038971575,
            "auditor_fn_violation": 0.016508097573928446,
            "auditor_fp_violation": 0.010976948408342485,
            "ave_precision_score": 0.7941172171317536,
            "fpr": 0.07025246981339188,
            "logloss": 0.9671916752037845,
            "mae": 0.3295973724781834,
            "precision": 0.8160919540229885,
            "recall": 0.6133909287257019
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(68)",
        "seed": 4719,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8332773093294055,
            "auditor_fn_violation": 0.011230660663879663,
            "auditor_fp_violation": 0.02304454723507105,
            "ave_precision_score": 0.8335570552321936,
            "fpr": 0.13486842105263158,
            "logloss": 0.8242041931223614,
            "mae": 0.266691599457674,
            "precision": 0.7583497053045186,
            "recall": 0.7861507128309573
        },
        "train": {
            "accuracy": 0.7310647639956093,
            "auc_prc": 0.8155725963181366,
            "auditor_fn_violation": 0.01619040619450774,
            "auditor_fp_violation": 0.016894209659714596,
            "ave_precision_score": 0.8164279970659118,
            "fpr": 0.14818880351262348,
            "logloss": 0.8813664401212795,
            "mae": 0.2847651646745469,
            "precision": 0.7233606557377049,
            "recall": 0.7624190064794817
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(69)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7445175438596491,
            "auc_prc": 0.84752920704109,
            "auditor_fn_violation": 0.01179565512559403,
            "auditor_fp_violation": 0.016085344001333504,
            "ave_precision_score": 0.8476905942312501,
            "fpr": 0.09758771929824561,
            "logloss": 0.8299481729419834,
            "mae": 0.2985693312930938,
            "precision": 0.7958715596330275,
            "recall": 0.7067209775967414
        },
        "train": {
            "accuracy": 0.7497255762897914,
            "auc_prc": 0.8031687014333886,
            "auditor_fn_violation": 0.011280414800624961,
            "auditor_fp_violation": 0.016426219225341068,
            "ave_precision_score": 0.8036195873293877,
            "fpr": 0.10867178924259056,
            "logloss": 0.9080290709150837,
            "mae": 0.3014230822795486,
            "precision": 0.7713625866050808,
            "recall": 0.7213822894168467
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(70)",
        "seed": 4719,
        "test": {
            "accuracy": 0.75,
            "auc_prc": 0.8381623906666987,
            "auditor_fn_violation": 0.006069782398970956,
            "auditor_fp_violation": 0.019757678043088724,
            "ave_precision_score": 0.8383805343340864,
            "fpr": 0.12390350877192982,
            "logloss": 0.8764975410728212,
            "mae": 0.26812356778698915,
            "precision": 0.7689161554192229,
            "recall": 0.7657841140529531
        },
        "train": {
            "accuracy": 0.7453347969264544,
            "auc_prc": 0.8286985015480435,
            "auditor_fn_violation": 0.008395113242751781,
            "auditor_fp_violation": 0.014593460875019613,
            "ave_precision_score": 0.8289075016236641,
            "fpr": 0.132821075740944,
            "logloss": 0.9604398255127149,
            "mae": 0.2795969506167526,
            "precision": 0.7441860465116279,
            "recall": 0.7602591792656588
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(71)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.8150264927627129,
            "auditor_fn_violation": 0.009138171293814983,
            "auditor_fp_violation": 0.013058924032170686,
            "ave_precision_score": 0.8153260517678579,
            "fpr": 0.08442982456140351,
            "logloss": 0.9246223087605927,
            "mae": 0.33595686600984553,
            "precision": 0.7884615384615384,
            "recall": 0.5845213849287169
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7909611939248777,
            "auditor_fn_violation": 0.014478666075539427,
            "auditor_fp_violation": 0.005655088599655014,
            "ave_precision_score": 0.7914078201139765,
            "fpr": 0.0801317233809001,
            "logloss": 0.9725512569804683,
            "mae": 0.3266943817518488,
            "precision": 0.7972222222222223,
            "recall": 0.6198704103671706
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(72)",
        "seed": 4719,
        "test": {
            "accuracy": 0.731359649122807,
            "auc_prc": 0.8454259326769589,
            "auditor_fn_violation": 0.009033211848358173,
            "auditor_fp_violation": 0.017236529566195778,
            "ave_precision_score": 0.8455797007131507,
            "fpr": 0.0800438596491228,
            "logloss": 0.8141412035979312,
            "mae": 0.3207129475957547,
            "precision": 0.8137755102040817,
            "recall": 0.6496945010183299
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.8049515215454076,
            "auditor_fn_violation": 0.011545947893872123,
            "auditor_fp_violation": 0.01283420887564686,
            "ave_precision_score": 0.8052080216538712,
            "fpr": 0.0889132821075741,
            "logloss": 0.8940429013396276,
            "mae": 0.3248775472565755,
            "precision": 0.7890625,
            "recall": 0.6544276457883369
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(73)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7258771929824561,
            "auc_prc": 0.8612852887417337,
            "auditor_fn_violation": 0.011255225640475939,
            "auditor_fp_violation": 0.008185919073217487,
            "ave_precision_score": 0.8614744807142218,
            "fpr": 0.06907894736842106,
            "logloss": 0.5418252747438477,
            "mae": 0.3626551196824661,
            "precision": 0.8283378746594006,
            "recall": 0.6191446028513238
        },
        "train": {
            "accuracy": 0.7530186608122942,
            "auc_prc": 0.8272125177806976,
            "auditor_fn_violation": 0.007695718041788277,
            "auditor_fp_violation": 0.001918515759761644,
            "ave_precision_score": 0.8276585779400463,
            "fpr": 0.06037321624588365,
            "logloss": 0.5397665827880092,
            "mae": 0.36171943995780614,
            "precision": 0.8419540229885057,
            "recall": 0.6328293736501079
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(74)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7510964912280702,
            "auc_prc": 0.8696983303409955,
            "auditor_fn_violation": 0.008095276378318509,
            "auditor_fp_violation": 0.012121306830020419,
            "ave_precision_score": 0.8698081881541828,
            "fpr": 0.08552631578947369,
            "logloss": 0.7383296731362959,
            "mae": 0.28575938664358835,
            "precision": 0.8142857142857143,
            "recall": 0.6965376782077393
        },
        "train": {
            "accuracy": 0.756311745334797,
            "auc_prc": 0.8409494299016363,
            "auditor_fn_violation": 0.01158625202409713,
            "auditor_fp_violation": 0.00491022424337463,
            "ave_precision_score": 0.841089332449038,
            "fpr": 0.08342480790340286,
            "logloss": 0.8186032799246801,
            "mae": 0.2895700514074511,
            "precision": 0.806615776081425,
            "recall": 0.6846652267818575
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(75)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6842105263157895,
            "auc_prc": 0.8125611513279111,
            "auditor_fn_violation": 0.007655339979276101,
            "auditor_fp_violation": 0.008691190565487355,
            "ave_precision_score": 0.8128817060827633,
            "fpr": 0.0756578947368421,
            "logloss": 0.9567422975180131,
            "mae": 0.34361720293074477,
            "precision": 0.7976539589442815,
            "recall": 0.5539714867617108
        },
        "train": {
            "accuracy": 0.7266739846322722,
            "auc_prc": 0.7950446329295875,
            "auditor_fn_violation": 0.01690165555141978,
            "auditor_fp_violation": 0.0063117453347969294,
            "ave_precision_score": 0.7954866685067052,
            "fpr": 0.06915477497255763,
            "logloss": 0.9949425203726522,
            "mae": 0.33014654428629076,
            "precision": 0.8147058823529412,
            "recall": 0.5982721382289417
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(76)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6655701754385965,
            "auc_prc": 0.8166276468628025,
            "auditor_fn_violation": 0.009176135348554696,
            "auditor_fp_violation": 0.003794745176480393,
            "ave_precision_score": 0.8168808347671895,
            "fpr": 0.06140350877192982,
            "logloss": 0.7622310717689446,
            "mae": 0.3829028470392682,
            "precision": 0.8120805369127517,
            "recall": 0.49287169042769857
        },
        "train": {
            "accuracy": 0.6805708013172338,
            "auc_prc": 0.7947067401721302,
            "auditor_fn_violation": 0.002958797324754088,
            "auditor_fp_violation": 0.004724008154304533,
            "ave_precision_score": 0.7952153718322124,
            "fpr": 0.048298572996706916,
            "logloss": 0.8227625756567551,
            "mae": 0.37332162557682996,
            "precision": 0.8307692307692308,
            "recall": 0.46652267818574517
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(77)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6875,
            "auc_prc": 0.8130268354683291,
            "auditor_fn_violation": 0.006775467181191265,
            "auditor_fp_violation": 0.008691190565487355,
            "ave_precision_score": 0.8133466691032891,
            "fpr": 0.0756578947368421,
            "logloss": 0.9323155248344634,
            "mae": 0.3416146162283613,
            "precision": 0.7994186046511628,
            "recall": 0.560081466395112
        },
        "train": {
            "accuracy": 0.7321624588364435,
            "auc_prc": 0.7941577137003697,
            "auditor_fn_violation": 0.020396260725047598,
            "auditor_fp_violation": 0.005802101301552455,
            "ave_precision_score": 0.7946027008497265,
            "fpr": 0.07025246981339188,
            "logloss": 0.9734440821332085,
            "mae": 0.3297009188681601,
            "precision": 0.8155619596541787,
            "recall": 0.6112311015118791
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(78)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7609649122807017,
            "auc_prc": 0.8713248864194699,
            "auditor_fn_violation": 0.005971522492585849,
            "auditor_fp_violation": 0.007248301871067219,
            "ave_precision_score": 0.8714310343815947,
            "fpr": 0.09320175438596491,
            "logloss": 0.6313952289071358,
            "mae": 0.3075063341433096,
            "precision": 0.8081264108352144,
            "recall": 0.7291242362525459
        },
        "train": {
            "accuracy": 0.7705817782656421,
            "auc_prc": 0.8522635230047803,
            "auditor_fn_violation": 0.003859713176842672,
            "auditor_fp_violation": 0.014108318958758043,
            "ave_precision_score": 0.8524024321622223,
            "fpr": 0.08122941822173436,
            "logloss": 0.7042650052345386,
            "mae": 0.3070710681406444,
            "precision": 0.8159203980099502,
            "recall": 0.7084233261339092
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(79)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7532894736842105,
            "auc_prc": 0.8723476737952572,
            "auditor_fn_violation": 0.020804301997355914,
            "auditor_fp_violation": 0.014712776597074631,
            "ave_precision_score": 0.8724489888279047,
            "fpr": 0.11403508771929824,
            "logloss": 0.751831006367172,
            "mae": 0.27206175072868327,
            "precision": 0.7805907172995781,
            "recall": 0.7535641547861507
        },
        "train": {
            "accuracy": 0.7596048298572997,
            "auc_prc": 0.8474051506617948,
            "auditor_fn_violation": 0.012188443146282651,
            "auditor_fp_violation": 0.009996863729026192,
            "ave_precision_score": 0.8475936517051942,
            "fpr": 0.10647639956092206,
            "logloss": 0.8450087104119346,
            "mae": 0.2753313286543609,
            "precision": 0.7785388127853882,
            "recall": 0.7365010799136069
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(80)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6392543859649122,
            "auc_prc": 0.8167181560200509,
            "auditor_fn_violation": 0.0015185621895880308,
            "auditor_fp_violation": 0.00022919531608117793,
            "ave_precision_score": 0.8169727215100441,
            "fpr": 0.03728070175438596,
            "logloss": 0.8076510433300153,
            "mae": 0.394612735563689,
            "precision": 0.8521739130434782,
            "recall": 0.39918533604887985
        },
        "train": {
            "accuracy": 0.6498353457738749,
            "auc_prc": 0.7995686470892914,
            "auditor_fn_violation": 8.060826045005224e-05,
            "auditor_fp_violation": 0.002825094088129215,
            "ave_precision_score": 0.8000867141436525,
            "fpr": 0.036223929747530186,
            "logloss": 0.8591934902020094,
            "mae": 0.38175508813982634,
            "precision": 0.8428571428571429,
            "recall": 0.38228941684665224
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(81)",
        "seed": 4719,
        "test": {
            "accuracy": 0.5208333333333334,
            "auc_prc": 0.644809516428656,
            "auditor_fn_violation": 0.00825159895665846,
            "auditor_fp_violation": 0.0028258740675917832,
            "ave_precision_score": 0.6404325060442548,
            "fpr": 0.019736842105263157,
            "logloss": 8.742437390983685,
            "mae": 0.48910175462207706,
            "precision": 0.8,
            "recall": 0.14663951120162932
        },
        "train": {
            "accuracy": 0.5411635565312843,
            "auc_prc": 0.6006075188628814,
            "auditor_fn_violation": 0.00014224987138241794,
            "auditor_fp_violation": 0.0015509840050180355,
            "ave_precision_score": 0.5960625715795664,
            "fpr": 0.018660812294182216,
            "logloss": 8.742551474677365,
            "mae": 0.47197304337922624,
            "precision": 0.7848101265822784,
            "recall": 0.13390928725701945
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(82)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7171052631578947,
            "auc_prc": 0.8126244299952232,
            "auditor_fn_violation": 0.010888984171222362,
            "auditor_fp_violation": 0.016611451431428927,
            "ave_precision_score": 0.8129393152757425,
            "fpr": 0.10087719298245613,
            "logloss": 0.8813067797859448,
            "mae": 0.32458833571933066,
            "precision": 0.7793764988009593,
            "recall": 0.6619144602851323
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7931206701472355,
            "auditor_fn_violation": 0.01280485925560643,
            "auditor_fp_violation": 0.01342225968323664,
            "ave_precision_score": 0.7935689975981715,
            "fpr": 0.10428100987925357,
            "logloss": 0.9321201648372797,
            "mae": 0.3174627743244777,
            "precision": 0.7654320987654321,
            "recall": 0.6695464362850972
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(83)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7094298245614035,
            "auc_prc": 0.7986889804596222,
            "auditor_fn_violation": 0.008421320613141816,
            "auditor_fp_violation": 0.017431866483310417,
            "ave_precision_score": 0.7990187190447989,
            "fpr": 0.11513157894736842,
            "logloss": 0.8707326345816897,
            "mae": 0.3075661480012186,
            "precision": 0.7591743119266054,
            "recall": 0.6741344195519349
        },
        "train": {
            "accuracy": 0.703622392974753,
            "auc_prc": 0.7796880499192382,
            "auditor_fn_violation": 0.00888350446783138,
            "auditor_fp_violation": 0.005238552610945591,
            "ave_precision_score": 0.7804763042012182,
            "fpr": 0.141602634467618,
            "logloss": 0.9737189680931558,
            "mae": 0.31718342560850127,
            "precision": 0.7139689578713969,
            "recall": 0.6954643628509719
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(84)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7017543859649122,
            "auc_prc": 0.8176128328214782,
            "auditor_fn_violation": 0.0077379676278272105,
            "auditor_fp_violation": 0.012967766804183859,
            "ave_precision_score": 0.8180426413616133,
            "fpr": 0.08114035087719298,
            "logloss": 0.6643928168107728,
            "mae": 0.3334605632858336,
            "precision": 0.7983651226158038,
            "recall": 0.5967413441955194
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7920737795483318,
            "auditor_fn_violation": 0.013139146453355083,
            "auditor_fp_violation": 0.016269405676650462,
            "ave_precision_score": 0.7927066549267862,
            "fpr": 0.08232711306256861,
            "logloss": 0.6540073092855787,
            "mae": 0.32487106079621403,
            "precision": 0.7972972972972973,
            "recall": 0.6371490280777538
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(85)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6907894736842105,
            "auc_prc": 0.8106874635430138,
            "auditor_fn_violation": 0.008206935362847043,
            "auditor_fp_violation": 0.013019856648747765,
            "ave_precision_score": 0.811004883205418,
            "fpr": 0.08771929824561403,
            "logloss": 0.9488046633116379,
            "mae": 0.3368927090406439,
            "precision": 0.7831978319783198,
            "recall": 0.5885947046843177
        },
        "train": {
            "accuracy": 0.7277716794731065,
            "auc_prc": 0.7885709136541227,
            "auditor_fn_violation": 0.016387185183253397,
            "auditor_fp_violation": 0.005135643719617379,
            "ave_precision_score": 0.7890794677036919,
            "fpr": 0.08342480790340286,
            "logloss": 0.9901647407779399,
            "mae": 0.32570909243113577,
            "precision": 0.7929155313351499,
            "recall": 0.6285097192224622
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(86)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7050438596491229,
            "auc_prc": 0.7983698495592715,
            "auditor_fn_violation": 0.006768767642119568,
            "auditor_fp_violation": 0.012923490436304537,
            "ave_precision_score": 0.7987520106831074,
            "fpr": 0.1118421052631579,
            "logloss": 0.7730335397402217,
            "mae": 0.30760320002752306,
            "precision": 0.7605633802816901,
            "recall": 0.659877800407332
        },
        "train": {
            "accuracy": 0.7025246981339188,
            "auc_prc": 0.7800646075088586,
            "auditor_fn_violation": 0.008191221760436997,
            "auditor_fp_violation": 0.006676826877842248,
            "ave_precision_score": 0.7807707977123308,
            "fpr": 0.1394072447859495,
            "logloss": 0.8061228637585305,
            "mae": 0.3177407513238806,
            "precision": 0.7152466367713004,
            "recall": 0.6889848812095032
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(87)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7105263157894737,
            "auc_prc": 0.8086441085867796,
            "auditor_fn_violation": 0.012443277235859508,
            "auditor_fp_violation": 0.023362295286910865,
            "ave_precision_score": 0.8091128508494987,
            "fpr": 0.09210526315789473,
            "logloss": 0.8078450189359097,
            "mae": 0.32937110013751625,
            "precision": 0.7873417721518987,
            "recall": 0.6334012219959266
        },
        "train": {
            "accuracy": 0.7354555433589463,
            "auc_prc": 0.7881864486899176,
            "auditor_fn_violation": 0.01597703138743413,
            "auditor_fp_violation": 0.015240316763368353,
            "ave_precision_score": 0.7887787494472244,
            "fpr": 0.0889132821075741,
            "logloss": 0.8091361032742721,
            "mae": 0.3245729020490976,
            "precision": 0.7890625,
            "recall": 0.6544276457883369
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(88)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7335526315789473,
            "auc_prc": 0.8286589125032513,
            "auditor_fn_violation": 0.020911494622503305,
            "auditor_fp_violation": 0.024607242571988167,
            "ave_precision_score": 0.828930150285982,
            "fpr": 0.13596491228070176,
            "logloss": 0.8058953171395307,
            "mae": 0.2809471979662344,
            "precision": 0.75,
            "recall": 0.7576374745417516
        },
        "train": {
            "accuracy": 0.7135016465422612,
            "auc_prc": 0.7990754115370129,
            "auditor_fn_violation": 0.015889310633414977,
            "auditor_fp_violation": 0.022414536615963626,
            "ave_precision_score": 0.7999142440183437,
            "fpr": 0.15587266739846323,
            "logloss": 0.8695303858890953,
            "mae": 0.2954194137104214,
            "precision": 0.7078189300411523,
            "recall": 0.7429805615550756
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(89)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.8120596590813296,
            "auditor_fn_violation": 0.004251974130846473,
            "auditor_fp_violation": 0.013460015835312748,
            "ave_precision_score": 0.8124028953043858,
            "fpr": 0.06798245614035088,
            "logloss": 0.9658198923246591,
            "mae": 0.3484158556822753,
            "precision": 0.8115501519756839,
            "recall": 0.5437881873727087
        },
        "train": {
            "accuracy": 0.7222832052689352,
            "auc_prc": 0.7953485824473988,
            "auditor_fn_violation": 0.016152472895472426,
            "auditor_fp_violation": 0.007076211384663637,
            "ave_precision_score": 0.7957611624243569,
            "fpr": 0.06366630076838639,
            "logloss": 1.004383878787603,
            "mae": 0.33636796715981443,
            "precision": 0.8220858895705522,
            "recall": 0.5788336933045356
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(90)",
        "seed": 4719,
        "test": {
            "accuracy": 0.7467105263157895,
            "auc_prc": 0.8638080713443441,
            "auditor_fn_violation": 0.009575874513166827,
            "auditor_fp_violation": 0.016087948493561696,
            "ave_precision_score": 0.863934284396603,
            "fpr": 0.08881578947368421,
            "logloss": 0.7696488871728471,
            "mae": 0.29061612021768407,
            "precision": 0.8080568720379147,
            "recall": 0.6945010183299389
        },
        "train": {
            "accuracy": 0.7618002195389681,
            "auc_prc": 0.8432382779182697,
            "auditor_fn_violation": 0.0184782582925748,
            "auditor_fp_violation": 0.01056531284302964,
            "ave_precision_score": 0.8433887500895497,
            "fpr": 0.08342480790340286,
            "logloss": 0.8440500393198918,
            "mae": 0.288837553013577,
            "precision": 0.8090452261306532,
            "recall": 0.6954643628509719
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(91)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6633771929824561,
            "auc_prc": 0.8290077513935729,
            "auditor_fn_violation": 0.01156117125808412,
            "auditor_fp_violation": 0.007696274534316791,
            "ave_precision_score": 0.8292416263590983,
            "fpr": 0.049342105263157895,
            "logloss": 0.9137145501333268,
            "mae": 0.3608086155929509,
            "precision": 0.8357664233576643,
            "recall": 0.4663951120162933
        },
        "train": {
            "accuracy": 0.6970362239297475,
            "auc_prc": 0.8019475607739307,
            "auditor_fn_violation": 0.007963621966225144,
            "auditor_fp_violation": 0.009506821389368042,
            "ave_precision_score": 0.8023433822896437,
            "fpr": 0.043907793633369926,
            "logloss": 0.9612746191082355,
            "mae": 0.35121288040168025,
            "precision": 0.850187265917603,
            "recall": 0.490280777537797
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(92)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6820175438596491,
            "auc_prc": 0.8083178704702769,
            "auditor_fn_violation": 0.0068446957515989615,
            "auditor_fp_violation": 0.006701358503146228,
            "ave_precision_score": 0.8086384129580629,
            "fpr": 0.07456140350877193,
            "logloss": 0.9654061634504739,
            "mae": 0.3528371599075742,
            "precision": 0.798219584569733,
            "recall": 0.5478615071283096
        },
        "train": {
            "accuracy": 0.7200878155872668,
            "auc_prc": 0.7802733243889419,
            "auditor_fn_violation": 0.015704385800617844,
            "auditor_fp_violation": 0.006637623490669593,
            "ave_precision_score": 0.7807907144234673,
            "fpr": 0.06915477497255763,
            "logloss": 1.0113071527271176,
            "mae": 0.34239675347559984,
            "precision": 0.811377245508982,
            "recall": 0.5853131749460043
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(93)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6798245614035088,
            "auc_prc": 0.7908154534695465,
            "auditor_fn_violation": 0.01472335369993212,
            "auditor_fp_violation": 0.02244030503812977,
            "ave_precision_score": 0.7389344049465381,
            "fpr": 0.13048245614035087,
            "logloss": 6.25051658228295,
            "mae": 0.3173046919248274,
            "precision": 0.7276887871853547,
            "recall": 0.6476578411405295
        },
        "train": {
            "accuracy": 0.6783754116355654,
            "auc_prc": 0.7590251050510315,
            "auditor_fn_violation": 0.014943348988721966,
            "auditor_fp_violation": 0.016818253097067595,
            "ave_precision_score": 0.6947959040701521,
            "fpr": 0.15806805708013172,
            "logloss": 6.798100709739883,
            "mae": 0.325049868853252,
            "precision": 0.6855895196506551,
            "recall": 0.6781857451403888
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(94)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6918859649122807,
            "auc_prc": 0.8141968802897153,
            "auditor_fn_violation": 0.005625379640547402,
            "auditor_fp_violation": 0.017231320581739387,
            "ave_precision_score": 0.8144977347943287,
            "fpr": 0.08662280701754387,
            "logloss": 0.9303881088655521,
            "mae": 0.33597894359709013,
            "precision": 0.7853260869565217,
            "recall": 0.5885947046843177
        },
        "train": {
            "accuracy": 0.725576289791438,
            "auc_prc": 0.791585272215757,
            "auditor_fn_violation": 0.01461143262216301,
            "auditor_fp_violation": 0.004576995452407093,
            "ave_precision_score": 0.7920252930990279,
            "fpr": 0.08232711306256861,
            "logloss": 0.977598881243893,
            "mae": 0.3263647480562652,
            "precision": 0.7933884297520661,
            "recall": 0.6220302375809935
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(95)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6864035087719298,
            "auc_prc": 0.8143271223883071,
            "auditor_fn_violation": 0.006203773180405191,
            "auditor_fp_violation": 0.008534921031795642,
            "ave_precision_score": 0.8146426858592214,
            "fpr": 0.07346491228070176,
            "logloss": 0.9440675752488554,
            "mae": 0.3424325466587681,
            "precision": 0.8023598820058997,
            "recall": 0.5539714867617108
        },
        "train": {
            "accuracy": 0.7288693743139407,
            "auc_prc": 0.7951516645189088,
            "auditor_fn_violation": 0.01944792824916488,
            "auditor_fp_violation": 0.005802101301552455,
            "ave_precision_score": 0.795608168035411,
            "fpr": 0.07025246981339188,
            "logloss": 0.984611575497005,
            "mae": 0.33031881695101983,
            "precision": 0.813953488372093,
            "recall": 0.6047516198704104
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(96)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6896929824561403,
            "auc_prc": 0.8378475932290006,
            "auditor_fn_violation": 0.01014756851395292,
            "auditor_fp_violation": 0.009123536275367758,
            "ave_precision_score": 0.8380322405248881,
            "fpr": 0.05921052631578947,
            "logloss": 0.8863898584793818,
            "mae": 0.343159997465958,
            "precision": 0.8291139240506329,
            "recall": 0.5336048879837068
        },
        "train": {
            "accuracy": 0.6992316136114161,
            "auc_prc": 0.8105123934818679,
            "auditor_fn_violation": 0.013224496376184532,
            "auditor_fp_violation": 0.014434197114630707,
            "ave_precision_score": 0.8108514550189027,
            "fpr": 0.06037321624588365,
            "logloss": 0.9426346596883489,
            "mae": 0.33698548616119917,
            "precision": 0.8160535117056856,
            "recall": 0.5269978401727862
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(97)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6973684210526315,
            "auc_prc": 0.8156979831401858,
            "auditor_fn_violation": 0.00286293636331154,
            "auditor_fp_violation": 0.01596032837438013,
            "ave_precision_score": 0.8160171332331424,
            "fpr": 0.07346491228070176,
            "logloss": 0.9201252359843816,
            "mae": 0.3389479361358845,
            "precision": 0.8080229226361032,
            "recall": 0.5743380855397149
        },
        "train": {
            "accuracy": 0.7332601536772777,
            "auc_prc": 0.7944451138495963,
            "auditor_fn_violation": 0.015832410684862016,
            "auditor_fp_violation": 0.010535910302650152,
            "ave_precision_score": 0.7949275214959245,
            "fpr": 0.07135016465422613,
            "logloss": 0.9649766569078418,
            "mae": 0.3288567507464461,
            "precision": 0.8142857142857143,
            "recall": 0.6155507559395248
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(98)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6392543859649122,
            "auc_prc": 0.8120388078473209,
            "auditor_fn_violation": 0.009100207239075297,
            "auditor_fp_violation": 0.003414489311163896,
            "ave_precision_score": 0.8122881346064226,
            "fpr": 0.039473684210526314,
            "logloss": 0.8400127859690053,
            "mae": 0.38645366199987047,
            "precision": 0.8461538461538461,
            "recall": 0.40325865580448067
        },
        "train": {
            "accuracy": 0.6322722283205269,
            "auc_prc": 0.7725226427802725,
            "auditor_fn_violation": 0.004841237289381301,
            "auditor_fp_violation": 0.0056256860592755215,
            "ave_precision_score": 0.7729196368143167,
            "fpr": 0.04610318331503842,
            "logloss": 0.9023470386665076,
            "mae": 0.3846246804824788,
            "precision": 0.8018867924528302,
            "recall": 0.367170626349892
        }
    },
    {
        "dataset": "lawschool",
        "method": "feat_tourn",
        "model": "feat_tourn:archive(99)",
        "seed": 4719,
        "test": {
            "accuracy": 0.6644736842105263,
            "auc_prc": 0.7988809350242628,
            "auditor_fn_violation": 0.004504323435880954,
            "auditor_fp_violation": 0.003461370171271415,
            "ave_precision_score": 0.7991716559390448,
            "fpr": 0.08662280701754387,
            "logloss": 0.7272666937665789,
            "mae": 0.3812092316179667,
            "precision": 0.7696793002915452,
            "recall": 0.5376782077393075
        },
        "train": {
            "accuracy": 0.7014270032930845,
            "auc_prc": 0.7678235702379475,
            "auditor_fn_violation": 0.006394131718639236,
            "auditor_fp_violation": 0.008781558726673988,
            "ave_precision_score": 0.7684530037580721,
            "fpr": 0.07903402854006586,
            "logloss": 0.7997656569729382,
            "mae": 0.3729949541631207,
            "precision": 0.7850746268656716,
            "recall": 0.5680345572354212
        }
    }
]